{
    "author": "milesial",
    "message": "Optimize memory usage of mllama encoder (#34930)\n\nmllama encoder memory optimization",
    "sha": "c24c79ebf91f6f04faf287997848ed6e64d78899",
    "files": [
        {
            "sha": "763ad97b1e721a75d19290babd170e97248150ae",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c24c79ebf91f6f04faf287997848ed6e64d78899/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c24c79ebf91f6f04faf287997848ed6e64d78899/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=c24c79ebf91f6f04faf287997848ed6e64d78899",
            "patch": "@@ -1608,9 +1608,8 @@ def forward(\n         hidden_state = hidden_state.reshape(batch_size, num_concurrent_media, num_tiles, num_patches, dim)\n \n         # Collect intermediate layer outputs from encoder output\n-        all_intermediate_hidden_states = output[1]\n+        all_intermediate_hidden_states = [output[1][i] for i in self.intermediate_layers_indices]\n         intermediate_hidden_states = torch.stack(all_intermediate_hidden_states, dim=-1)\n-        intermediate_hidden_states = intermediate_hidden_states[..., self.intermediate_layers_indices]\n \n         # Remove padding from intermediate hidden states\n         intermediate_hidden_states = intermediate_hidden_states.reshape("
        }
    ],
    "stats": {
        "total": 3,
        "additions": 1,
        "deletions": 2
    }
}