{
    "author": "warner-benjamin",
    "message": "Modernbert Release Fixes (#35344)\n\n* fix ForSequenceClassification\r\n\r\n* unmodularize rope layer\r\n\r\n* fix linting warning\r\n\r\n* Avoid complex PoolingHead, only one prediction head needed\r\n\r\n---------\r\n\r\nCo-authored-by: Tom Aarsen <Cubiegamedev@gmail.com>",
    "sha": "0ade1caa356dce6b70ef8293addeb0898f177206",
    "files": [
        {
            "sha": "237fba6f645fa57199987153f5772b896d43eaa7",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 14,
            "deletions": 25,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ade1caa356dce6b70ef8293addeb0898f177206/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ade1caa356dce6b70ef8293addeb0898f177206/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=0ade1caa356dce6b70ef8293addeb0898f177206",
            "patch": "@@ -610,8 +610,6 @@ def init_weight(module: nn.Module, std: float):\n             init_weight(module.Wqkv, stds[\"in\"])\n             init_weight(module.Wo, stds[\"out\"])\n         elif isinstance(module, ModernBertPredictionHead):\n-            init_weight(module.dense, stds[\"in\"])\n-        elif isinstance(module, ModernBertPoolingHead):\n             init_weight(module.dense, stds[\"out\"])\n         elif isinstance(module, ModernBertForMaskedLM):\n             init_weight(module.decoder, stds[\"out\"])\n@@ -1109,26 +1107,6 @@ def forward(\n         )\n \n \n-class ModernBertPoolingHead(nn.Module):\n-    def __init__(self, config: ModernBertConfig):\n-        super().__init__()\n-        self.config = config\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size, config.classifier_bias)\n-        self.act = ACT2FN[config.classifier_activation]\n-        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n-        self.drop = torch.nn.Dropout(config.classifier_dropout)\n-\n-    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n-        if self.config.classifier_pooling == \"cls\":\n-            hidden_states = hidden_states[:, 0]\n-        elif self.config.classifier_pooling == \"mean\":\n-            hidden_states = (hidden_states * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(\n-                dim=1, keepdim=True\n-            )\n-\n-        return self.drop(self.norm(self.act(self.dense(hidden_states))))\n-\n-\n @add_start_docstrings(\n     \"The ModernBert Model with a sequence classification head on top that performs pooling.\",\n     MODERNBERT_START_DOCSTRING,\n@@ -1140,7 +1118,8 @@ def __init__(self, config: ModernBertConfig):\n         self.config = config\n \n         self.model = ModernBertModel(config)\n-        self.head = ModernBertPoolingHead(config)\n+        self.head = ModernBertPredictionHead(config)\n+        self.drop = torch.nn.Dropout(config.classifier_dropout)\n         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n \n         # Initialize weights and apply final processing\n@@ -1194,7 +1173,15 @@ def forward(\n         )\n         last_hidden_state = outputs[0]\n \n-        pooled_output = self.head(last_hidden_state, attention_mask)\n+        if self.config.classifier_pooling == \"cls\":\n+            last_hidden_state = last_hidden_state[:, 0]\n+        elif self.config.classifier_pooling == \"mean\":\n+            last_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(\n+                dim=1, keepdim=True\n+            )\n+\n+        pooled_output = self.head(last_hidden_state)\n+        pooled_output = self.drop(pooled_output)\n         logits = self.classifier(pooled_output)\n \n         loss = None\n@@ -1242,7 +1229,8 @@ def __init__(self, config: ModernBertConfig):\n         self.num_labels = config.num_labels\n \n         self.model = ModernBertModel(config)\n-        self.drop = nn.Dropout(config.classifier_dropout)\n+        self.head = ModernBertPredictionHead(config)\n+        self.drop = torch.nn.Dropout(config.classifier_dropout)\n         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n \n         # Initialize weights and apply final processing\n@@ -1293,6 +1281,7 @@ def forward(\n         )\n         last_hidden_state = outputs[0]\n \n+        last_hidden_state = self.head(last_hidden_state)\n         last_hidden_state = self.drop(last_hidden_state)\n         logits = self.classifier(last_hidden_state)\n "
        },
        {
            "sha": "dac356146f30151a649e28b57f844a80e67ea87b",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 41,
            "deletions": 28,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ade1caa356dce6b70ef8293addeb0898f177206/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ade1caa356dce6b70ef8293addeb0898f177206/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=0ade1caa356dce6b70ef8293addeb0898f177206",
            "patch": "@@ -40,7 +40,7 @@\n     logging,\n )\n from ...utils.import_utils import is_triton_available\n-from ..gemma.modeling_gemma import GemmaRotaryEmbedding, apply_rotary_pos_emb\n+from ..gemma.modeling_gemma import apply_rotary_pos_emb\n \n \n if is_flash_attn_2_available():\n@@ -493,8 +493,32 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return self.Wo(self.drop(self.act(input) * gate))\n \n \n-class ModernBertRotaryEmbedding(GemmaRotaryEmbedding):\n-    pass\n+class ModernBertRotaryEmbedding(nn.Module):\n+    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+        super().__init__()\n+\n+        self.dim = dim\n+        self.max_position_embeddings = max_position_embeddings\n+        self.base = base\n+        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n+        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids, seq_len=None):\n+        # x: [bs, num_attention_heads, seq_len, head_size]\n+        self.inv_freq.to(x.device)\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 since bfloat16 loses precision on long contexts\n+        # See https://github.com/huggingface/transformers/pull/29285\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n def eager_attention_forward(\n@@ -811,8 +835,6 @@ def init_weight(module: nn.Module, std: float):\n             init_weight(module.Wqkv, stds[\"in\"])\n             init_weight(module.Wo, stds[\"out\"])\n         elif isinstance(module, ModernBertPredictionHead):\n-            init_weight(module.dense, stds[\"in\"])\n-        elif isinstance(module, ModernBertPoolingHead):\n             init_weight(module.dense, stds[\"out\"])\n         elif isinstance(module, ModernBertForMaskedLM):\n             init_weight(module.decoder, stds[\"out\"])\n@@ -1238,26 +1260,6 @@ def forward(\n         )\n \n \n-class ModernBertPoolingHead(nn.Module):\n-    def __init__(self, config: ModernBertConfig):\n-        super().__init__()\n-        self.config = config\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size, config.classifier_bias)\n-        self.act = ACT2FN[config.classifier_activation]\n-        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n-        self.drop = torch.nn.Dropout(config.classifier_dropout)\n-\n-    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n-        if self.config.classifier_pooling == \"cls\":\n-            hidden_states = hidden_states[:, 0]\n-        elif self.config.classifier_pooling == \"mean\":\n-            hidden_states = (hidden_states * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(\n-                dim=1, keepdim=True\n-            )\n-\n-        return self.drop(self.norm(self.act(self.dense(hidden_states))))\n-\n-\n @add_start_docstrings(\n     \"The ModernBert Model with a sequence classification head on top that performs pooling.\",\n     MODERNBERT_START_DOCSTRING,\n@@ -1269,7 +1271,8 @@ def __init__(self, config: ModernBertConfig):\n         self.config = config\n \n         self.model = ModernBertModel(config)\n-        self.head = ModernBertPoolingHead(config)\n+        self.head = ModernBertPredictionHead(config)\n+        self.drop = torch.nn.Dropout(config.classifier_dropout)\n         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n \n         # Initialize weights and apply final processing\n@@ -1323,7 +1326,15 @@ def forward(\n         )\n         last_hidden_state = outputs[0]\n \n-        pooled_output = self.head(last_hidden_state, attention_mask)\n+        if self.config.classifier_pooling == \"cls\":\n+            last_hidden_state = last_hidden_state[:, 0]\n+        elif self.config.classifier_pooling == \"mean\":\n+            last_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(\n+                dim=1, keepdim=True\n+            )\n+\n+        pooled_output = self.head(last_hidden_state)\n+        pooled_output = self.drop(pooled_output)\n         logits = self.classifier(pooled_output)\n \n         loss = None\n@@ -1371,7 +1382,8 @@ def __init__(self, config: ModernBertConfig):\n         self.num_labels = config.num_labels\n \n         self.model = ModernBertModel(config)\n-        self.drop = nn.Dropout(config.classifier_dropout)\n+        self.head = ModernBertPredictionHead(config)\n+        self.drop = torch.nn.Dropout(config.classifier_dropout)\n         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n \n         # Initialize weights and apply final processing\n@@ -1422,6 +1434,7 @@ def forward(\n         )\n         last_hidden_state = outputs[0]\n \n+        last_hidden_state = self.head(last_hidden_state)\n         last_hidden_state = self.drop(last_hidden_state)\n         logits = self.classifier(last_hidden_state)\n "
        }
    ],
    "stats": {
        "total": 108,
        "additions": 55,
        "deletions": 53
    }
}