{
    "author": "LysandreJik",
    "message": "Transformers serve VLM (#39454)\n\n* Add support for VLMs in Transformers Serve\n\n* Raushan comments\n\n* Update src/transformers/commands/serving.py\n\nCo-authored-by: Sergio Paniego Blanco <sergiopaniegoblanco@gmail.com>\n\n* Quick fix\n\n* CPU -> Auto\n\n* Update src/transformers/commands/serving.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Fixup\n\n---------\n\nCo-authored-by: Sergio Paniego Blanco <sergiopaniegoblanco@gmail.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "a0e5a7d34be27928fbc059bdcd03581ecb39cf57",
    "files": [
        {
            "sha": "ccf35dbd3b1b55503eb986d51854ab3584f811d2",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 168,
            "deletions": 74,
            "changes": 242,
            "blob_url": "https://github.com/huggingface/transformers/blob/a0e5a7d34be27928fbc059bdcd03581ecb39cf57/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a0e5a7d34be27928fbc059bdcd03581ecb39cf57/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=a0e5a7d34be27928fbc059bdcd03581ecb39cf57",
            "patch": "@@ -11,22 +11,34 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n+import base64\n import copy\n+import datetime\n+import enum\n import functools\n import gc\n import io\n import json\n import re\n+import tempfile\n import threading\n import time\n from argparse import ArgumentParser, Namespace\n from dataclasses import dataclass, field\n+from io import BytesIO\n from threading import Thread\n-from typing import Generator, Optional, Union\n+from typing import Generator, Iterable, Optional, Union\n \n-from huggingface_hub import ModelInfo, model_info\n+from huggingface_hub import model_info\n+from huggingface_hub.constants import HF_HUB_OFFLINE\n+from openai.types.chat import ChatCompletionMessageParam\n+from PIL import Image\n \n+import transformers\n+from transformers.models.auto.modeling_auto import (\n+    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n+    MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES,\n+)\n from transformers.utils.import_utils import (\n     is_fastapi_available,\n     is_librosa_available,\n@@ -35,7 +47,13 @@\n     is_uvicorn_available,\n )\n \n-from .. import LogitsProcessorList, PreTrainedTokenizerFast, ProcessorMixin, TextIteratorStreamer\n+from .. import (\n+    AutoConfig,\n+    LogitsProcessorList,\n+    PreTrainedTokenizerFast,\n+    ProcessorMixin,\n+    TextIteratorStreamer,\n+)\n from ..generation.continuous_batching import ContinuousBatchingManager, RequestStatus\n from ..utils import is_torch_available, logging\n from . import BaseTransformersCLICommand\n@@ -45,8 +63,6 @@\n     import torch\n \n     from transformers import (\n-        AutoModelForCausalLM,\n-        AutoModelForSpeechSeq2Seq,\n         AutoProcessor,\n         BitsAndBytesConfig,\n         GenerationConfig,\n@@ -187,6 +203,13 @@ class TransformersTranscriptionCreateParams(TranscriptionCreateParamsBase, total\n _MODELS_WITH_TOOL_SUPPORT = list(_TOOL_CALL_TOKENS.keys())\n \n \n+class Modality(enum.Enum):\n+    LLM = \"LLM\"\n+    VLM = \"VLM\"\n+    STT = \"STT\"\n+    TTS = \"TTS\"\n+\n+\n def serve_command_factory(args: Namespace):\n     \"\"\"\n     Factory function used to instantiate serving server from provided command line arguments.\n@@ -271,7 +294,7 @@ def reset(self):\n \n class TimedModel:\n     \"\"\"\n-    A class that holds a PreTrainedModel instance and its associated processor (tokenizer, audio processor, etc.).\n+    A class that holds a PreTrainedModel instance and its associated processor.\n     Automatically deletes the instances after a specified timeout.\n     \"\"\"\n \n@@ -325,7 +348,13 @@ class ServeArguments:\n     `transformers serve --help`\n     \"\"\"\n \n-    device: str = field(default=\"cpu\", metadata={\"help\": \"Device to use for inference.\"})\n+    device: str = field(\n+        default=\"auto\",\n+        metadata={\n+            \"help\": \"Device to use for inference; will default to `auto` and\"\n+            \"place the model on an accelerator if available.\"\n+        },\n+    )\n     torch_dtype: Optional[str] = field(\n         default=\"auto\",\n         metadata={\n@@ -438,7 +467,7 @@ def __init__(self, args: ServeArguments):\n         # cache and avoid re-running prefil\n         self.last_messages = None\n         self.last_kv_cache = None\n-        self.last_text_model = None\n+        self.last_model = None\n \n     def _validate_request(\n         self,\n@@ -632,46 +661,60 @@ async def audio_transcriptions(request: Request):\n             output = self.generate_transcription(parsed_request)\n             return StreamingResponse(output, media_type=\"text/event-stream\")\n \n+        @app.options(\"/v1/models\")\n         @app.get(\"/v1/models\")\n         def get_all_models():\n-            return JSONResponse(\n-                {\n-                    \"object\": \"list\",\n-                    \"data\": [\n-                        {\n-                            \"id\": model.id,\n-                            \"object\": \"model\",\n-                            \"created\": model.created_at.timestamp(),\n-                            \"owned_by\": model.author,\n-                        }\n-                        for model in self.get_text_gen_models()\n-                    ],\n-                }\n-            )\n+            return JSONResponse({\"object\": \"list\", \"data\": self.get_gen_models()})\n \n         uvicorn.run(app, host=self.args.host, port=self.args.port, log_level=self.args.log_level)\n \n     @functools.lru_cache(maxsize=None)\n-    def get_text_gen_models(self) -> list[ModelInfo]:\n+    def get_gen_models(self) -> list[dict[str, any]]:\n         \"\"\"\n         This is by no means a limit to which models may be instantiated with `transformers serve`: any chat-based\n         model working with generate can work.\n \n         This is a limited list of models to ensure we have a discoverable /v1/models endpoint for third-party\n         integrations.\n         \"\"\"\n-        return [\n-            model_info(\"Menlo/Jan-nano\"),\n-            model_info(\"Menlo/Jan-nano-128k\"),\n-            model_info(\"Qwen/Qwen2.5-0.5B-Instruct\"),\n-            model_info(\"Qwen/Qwen2.5-3B-Instruct\"),\n-            model_info(\"Qwen/Qwen2.5-7B-Instruct\"),\n-            model_info(\"Qwen/Qwen2.5-14B-Instruct\"),\n-            model_info(\"meta-llama/Llama-3.1-8B-Instruct\"),\n-            model_info(\"meta-llama/Llama-3.2-1B-Instruct\"),\n-            model_info(\"meta-llama/Llama-3.3-70B-Instruct\"),\n+        models = [\n+            \"Menlo/Jan-nano\",\n+            \"Menlo/Jan-nano-128k\",\n+            \"Qwen/Qwen2.5-0.5B-Instruct\",\n+            \"Qwen/Qwen2.5-3B-Instruct\",\n+            \"Qwen/Qwen2.5-7B-Instruct\",\n+            \"Qwen/Qwen2.5-14B-Instruct\",\n+            \"meta-llama/Llama-3.1-8B-Instruct\",\n+            \"meta-llama/Llama-3.2-1B-Instruct\",\n+            \"meta-llama/Llama-3.3-70B-Instruct\",\n+            \"HuggingFaceTB/SmolVLM-Instruct\",\n+            \"ibm-granite/granite-vision-3.2-2b\",\n+            \"Qwen/Qwen2.5-VL-7B-Instruct\",\n+            \"OpenGVLab/InternVL3-1B\",\n         ]\n \n+        if HF_HUB_OFFLINE:\n+            return [\n+                {\n+                    \"id\": model,\n+                    \"object\": \"model\",\n+                    \"created\": datetime.datetime.now().timestamp(),\n+                    \"owned_by\": model.split(\"/\")[0],\n+                }\n+                for model in models\n+            ]\n+        else:\n+            model_infos = [model_info(model) for model in models]\n+            return [\n+                {\n+                    \"id\": model.id,\n+                    \"object\": \"model\",\n+                    \"created\": model.created_at.timestamp(),\n+                    \"owned_by\": model.author,\n+                }\n+                for model in model_infos\n+            ]\n+\n     def continuous_batching_chat_completion(self, req: dict) -> Generator[str, None, None]:\n         \"\"\"\n         Generates an OpenAI Chat Completion using continuous batching.\n@@ -684,14 +727,16 @@ def continuous_batching_chat_completion(self, req: dict) -> Generator[str, None,\n         \"\"\"\n \n         model_id_and_revision = self.process_model_name(req[\"model\"])\n-        must_discard_cache = model_id_and_revision != self.last_text_model\n-        self.last_text_model = model_id_and_revision\n+        must_discard_cache = model_id_and_revision != self.last_model\n+        self.last_model = model_id_and_revision\n         if must_discard_cache:\n             # When switching models, terminate a continuous batching manager if it is running.\n             if self.running_continuous_batching_manager is not None:\n                 self.running_continuous_batching_manager.stop(block=True, timeout=2)\n                 self.running_continuous_batching_manager = None\n-        model, tokenizer = self.load_text_model_and_tokenizer(model_id_and_revision)\n+        model, processor = self.load_model_and_processor(model_id_and_revision)\n+\n+        tokenizer = processor.tokenizer if hasattr(processor, \"tokenizer\") else processor\n \n         generation_config = create_generation_config_from_req(\n             req,\n@@ -717,7 +762,7 @@ def continuous_batching_chat_completion(self, req: dict) -> Generator[str, None,\n             self.running_continuous_batching_manager.start()\n \n         # TODO (Joao, Lysandre): this should also work with tool support\n-        inputs = tokenizer.apply_chat_template(req[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True).to(\n+        inputs = processor.apply_chat_template(req[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True).to(\n             model.device\n         )\n \n@@ -759,6 +804,50 @@ def stream_chat_completion(_inputs):\n \n         return stream_chat_completion(inputs[0])\n \n+    @staticmethod\n+    def get_model_modality(model: PreTrainedModel) -> Modality:\n+        model_classname = model.__class__.__name__\n+        if model_classname in MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES.values():\n+            modality = Modality.VLM\n+        elif model_classname in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n+            modality = Modality.LLM\n+        else:\n+            raise ValueError(f\"Unknown modality: {model_classname}\")\n+\n+        return modality\n+\n+    @staticmethod\n+    def get_processor_inputs_from_inbound_messages(messages, modality: Modality):\n+        processor_inputs = []\n+\n+        for message in messages:\n+            parsed_message = {\"role\": message[\"role\"], \"content\": []}\n+\n+            if modality == Modality.LLM:\n+                # If we're working with LLMs, then \"content\" is a single string.\n+                content = message[\"content\"] if isinstance(message[\"content\"], str) else message[\"content\"][\"text\"]\n+                parsed_message[\"content\"] = content\n+\n+            elif modality == Modality.VLM:\n+                # If we're working with VLMs, then \"content\" is a dictionary, containing a \"type\" key indicating\n+                # which other key will be present and the type of the value of said key.\n+                if isinstance(message[\"content\"], str):\n+                    parsed_message[\"content\"].append({\"type\": \"text\", \"text\": message[\"content\"]})\n+                else:\n+                    for content in message[\"content\"]:\n+                        if content[\"type\"] == \"text\":\n+                            parsed_message[\"content\"].append(content)\n+                        elif content[\"type\"] == \"image_url\":\n+                            image_data = re.sub(\"^data:image/.+;base64,\", \"\", content[\"image_url\"][\"url\"])\n+                            image = Image.open(BytesIO(base64.b64decode(image_data)))\n+\n+                            file = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n+                            image.save(file.name)\n+\n+                            parsed_message[\"content\"].append({\"type\": \"image\", \"url\": file.name})\n+            processor_inputs.append(parsed_message)\n+        return processor_inputs\n+\n     def generate_chat_completion(self, req: dict) -> Generator[str, None, None]:\n         \"\"\"\n         Generates an OpenAI Chat Completion using `generate`.\n@@ -769,15 +858,24 @@ def generate_chat_completion(self, req: dict) -> Generator[str, None, None]:\n         Returns:\n             `Generator[str, None, None]`: A generator that yields the OpenAI Chat Completion chunks.\n         \"\"\"\n+        if self.args.force_model is not None:\n+            req[\"model\"] = self.args.force_model\n+\n+        messages: Iterable[ChatCompletionMessageParam] = req[\"messages\"]\n+\n         # HACK for tiny-agents: it sends a request after the assistant message (???). Let's assume we can't have a\n         # request whose last message is from the assistant.\n-        if req[\"messages\"][-1][\"role\"] == \"assistant\":\n+        if messages[-1][\"role\"] == \"assistant\":\n             return\n \n         model_id_and_revision = self.process_model_name(req[\"model\"])\n-        must_discard_cache = model_id_and_revision != self.last_text_model\n-        self.last_text_model = model_id_and_revision\n-        model, tokenizer = self.load_text_model_and_tokenizer(model_id_and_revision)\n+        must_discard_cache = model_id_and_revision != self.last_model\n+\n+        self.last_model = model_id_and_revision\n+        model, processor = self.load_model_and_processor(model_id_and_revision)\n+\n+        modality = self.get_model_modality(model)\n+        processor_inputs = self.get_processor_inputs_from_inbound_messages(messages, modality)\n \n         # ====== TOOL PREPROCESSING LOGIC ======\n         tool_model_family = None\n@@ -790,25 +888,26 @@ def generate_chat_completion(self, req: dict) -> Generator[str, None, None]:\n         # 2. force generation to pick from that tool's arguments\n         # ====== END OF TOOL PREPROCESSING LOGIC ======\n \n-        if tool_model_family is not None:\n-            text = tokenizer.apply_chat_template(\n-                req[\"messages\"], add_generation_prompt=True, tokenize=False, tools=req.get(\"tools\")\n-            )\n-        else:\n-            text = tokenizer.apply_chat_template(req[\"messages\"], add_generation_prompt=True, tokenize=False)\n-        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)[\"input_ids\"]\n+        inputs = processor.apply_chat_template(\n+            processor_inputs,\n+            add_generation_prompt=True,\n+            tools=req.get(\"tools\", None),\n+            return_tensors=\"pt\",\n+            return_dict=True,\n+            tokenize=True,\n+        )\n+        inputs = inputs.to(model.device)\n         request_id = req.get(\"request_id\", \"req_0\")\n \n-        generation_streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n+        generation_streamer = TextIteratorStreamer(processor, skip_special_tokens=True, skip_prompt=True)\n         generation_config = create_generation_config_from_req(req, model_generation_config=model.generation_config)\n \n         last_kv_cache = None\n         if self.is_continuation(req) and not must_discard_cache:\n             last_kv_cache = self.last_kv_cache\n \n         generation_kwargs = {\n-            \"inputs\": inputs,\n-            \"attention_mask\": torch.ones_like(inputs),\n+            **inputs,\n             \"streamer\": generation_streamer,\n             \"generation_config\": generation_config,\n             \"return_dict_in_generate\": True,\n@@ -929,15 +1028,14 @@ def generate_response(self, req: dict) -> Generator[str, None, None]:\n         \"\"\"\n         # TODO -- Implement non-streaming mode\n         model_id_and_revision = self.process_model_name(req[\"model\"])\n-        must_discard_cache = model_id_and_revision != self.last_text_model\n-        self.last_text_model = model_id_and_revision\n-        model, tokenizer = self.load_text_model_and_tokenizer(model_id_and_revision)\n+        must_discard_cache = model_id_and_revision != self.last_model\n+        self.last_model = model_id_and_revision\n+        model, processor = self.load_model_and_processor(model_id_and_revision)\n \n-        text = tokenizer.apply_chat_template(req[\"input\"], add_generation_prompt=True, tokenize=False)\n-        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)[\"input_ids\"]\n+        inputs = processor.apply_chat_template(req[\"input\"], add_generation_prompt=True).to(model.device)\n         request_id = req.get(\"previous_response_id\", \"req_0\")\n \n-        generation_streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n+        generation_streamer = TextIteratorStreamer(processor, skip_special_tokens=True, skip_prompt=True)\n         generation_config = create_generation_config_from_req(req, model_generation_config=model.generation_config)\n \n         last_kv_cache = None\n@@ -1282,9 +1380,7 @@ def process_model_name(self, model_id: str) -> str:\n             return model_id\n         return f\"{model_id}@main\"\n \n-    def _load_model_and_data_processor(\n-        self, model_id_and_revision: str, model_cls: type[PreTrainedModel]\n-    ) -> tuple[PreTrainedModel, Union[ProcessorMixin, PreTrainedTokenizerFast]]:\n+    def _load_model_and_data_processor(self, model_id_and_revision: str):\n         \"\"\"\n         Generic method to load a model and a data processor from a model ID and revision, making use of the serve CLI\n         arguments.\n@@ -1325,7 +1421,9 @@ def _load_model_and_data_processor(\n             \"trust_remote_code\": args.trust_remote_code,\n         }\n \n-        model = model_cls.from_pretrained(model_id, **model_kwargs)\n+        config = AutoConfig.from_pretrained(model_id, **model_kwargs)\n+        architecture = getattr(transformers, config.architectures[0])\n+        model = architecture.from_pretrained(model_id, **model_kwargs)\n \n         if getattr(model, \"hf_device_map\", None) is None:\n             model = model.to(args.device)\n@@ -1342,32 +1440,30 @@ def _load_model_and_data_processor(\n         logger.info(f\"Loaded model {model_id_and_revision}\")\n         return model, data_processor\n \n-    def load_text_model_and_tokenizer(\n-        self, model_id_and_revision: str\n-    ) -> tuple[PreTrainedModel, PreTrainedTokenizerFast]:\n+    def load_model_and_processor(self, model_id_and_revision: str) -> tuple[PreTrainedModel, PreTrainedTokenizerFast]:\n         \"\"\"\n-        Loads the text model and tokenizer from the given model ID and revision into the ServeCommand instance.\n+        Loads the text model and processor from the given model ID and revision into the ServeCommand instance.\n \n         Args:\n             model_id_and_revision (`str`):\n                 The model ID and revision to load.\n \n         Returns:\n-            `tuple[PreTrainedModel, PreTrainedTokenizerFast]`: The loaded text model and tokenizer.\n+            `tuple[PreTrainedModel, PreTrainedTokenizerFast]`: The loaded text model and processor.\n         \"\"\"\n         if model_id_and_revision not in self.loaded_models or self.loaded_models[model_id_and_revision].is_deleted():\n-            model, tokenizer = self._load_model_and_data_processor(model_id_and_revision, AutoModelForCausalLM)\n+            model, processor = self._load_model_and_data_processor(model_id_and_revision)\n             self.loaded_models[model_id_and_revision] = TimedModel(\n                 model,\n                 timeout_seconds=self.args.model_timeout,\n-                processor=tokenizer,\n+                processor=processor,\n             )\n         else:\n             self.loaded_models[model_id_and_revision].reset_timer()\n             model = self.loaded_models[model_id_and_revision].model\n-            tokenizer = self.loaded_models[model_id_and_revision].processor\n+            processor = self.loaded_models[model_id_and_revision].processor\n \n-        return model, tokenizer\n+        return model, processor\n \n     def load_audio_model_and_processor(self, model_id_and_revision: str) -> tuple[PreTrainedModel, ProcessorMixin]:\n         \"\"\"\n@@ -1381,9 +1477,7 @@ def load_audio_model_and_processor(self, model_id_and_revision: str) -> tuple[Pr\n             `tuple[PreTrainedModel, ProcessorMixin]`: The loaded audio model and processor.\n         \"\"\"\n         if model_id_and_revision not in self.loaded_models or self.loaded_models[model_id_and_revision].is_deleted():\n-            audio_model, audio_processor = self._load_model_and_data_processor(\n-                model_id_and_revision, AutoModelForSpeechSeq2Seq\n-            )\n+            audio_model, audio_processor = self._load_model_and_data_processor(model_id_and_revision)\n             self.loaded_models[model_id_and_revision] = TimedModel(\n                 audio_model,\n                 timeout_seconds=self.args.model_timeout,"
        },
        {
            "sha": "f0592686e8a834951007c847bb52890fabdb6221",
            "filename": "tests/commands/test_serving.py",
            "status": "modified",
            "additions": 100,
            "deletions": 1,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/a0e5a7d34be27928fbc059bdcd03581ecb39cf57/tests%2Fcommands%2Ftest_serving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a0e5a7d34be27928fbc059bdcd03581ecb39cf57/tests%2Fcommands%2Ftest_serving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcommands%2Ftest_serving.py?ref=a0e5a7d34be27928fbc059bdcd03581ecb39cf57",
            "patch": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import asyncio\n+import os\n import time\n import unittest\n from threading import Thread\n@@ -23,7 +24,7 @@\n \n import transformers.commands.transformers_cli as cli\n from transformers import GenerationConfig\n-from transformers.commands.serving import ServeArguments, ServeCommand\n+from transformers.commands.serving import Modality, ServeArguments, ServeCommand\n from transformers.testing_utils import CaptureStd, require_openai, slow\n from transformers.utils.import_utils import is_openai_available\n \n@@ -258,6 +259,104 @@ def test_generation_config_in_request(self):\n     # TODO: speed-based test to confirm that KV cache is working across requests\n \n \n+class ServeCompletionsGenerateMockTests(unittest.TestCase):\n+    def test_processor_inputs_from_inbound_messages_llm(self):\n+        modality = Modality.LLM\n+        messages = expected_outputs = [\n+            {\"role\": \"user\", \"content\": \"How are you doing?\"},\n+            {\"role\": \"assistant\", \"content\": \"I'm doing great, thank you for asking! How can I assist you today?\"},\n+            {\"role\": \"user\", \"content\": \"Can you help me write tests?\"},\n+        ]\n+        outputs = ServeCommand.get_processor_inputs_from_inbound_messages(messages, modality)\n+        self.assertListEqual(expected_outputs, outputs)\n+\n+    def test_processor_inputs_from_inbound_messages_vlm_text_only(self):\n+        modality = Modality.VLM\n+        messages = [\n+            {\"role\": \"user\", \"content\": \"How are you doing?\"},\n+            {\"role\": \"assistant\", \"content\": \"I'm doing great, thank you for asking! How can I assist you today?\"},\n+            {\"role\": \"user\", \"content\": \"Can you help me write tests?\"},\n+        ]\n+\n+        expected_outputs = [\n+            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"How are you doing?\"}]},\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"I'm doing great, thank you for asking! How can I assist you today?\"}\n+                ],\n+            },\n+            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Can you help me write tests?\"}]},\n+        ]\n+\n+        outputs = ServeCommand.get_processor_inputs_from_inbound_messages(messages, modality)\n+        self.assertListEqual(expected_outputs, outputs)\n+\n+    def test_processor_inputs_from_inbound_messages_vlm_text_and_image_in_base_64(self):\n+        modality = Modality.VLM\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"How many pixels are in the image?\"},\n+                    {\n+                        \"type\": \"image_url\",\n+                        \"image_url\": {\n+                            \"url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAASABIAAD/4QBARXhpZgAATU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAABaADAAQAAAABAAAABQAAAAD/7QA4UGhvdG9zaG9wIDMuMAA4QklNBAQAAAAAAAA4QklNBCUAAAAAABDUHYzZjwCyBOmACZjs+EJ+/8AAEQgABQAFAwEiAAIRAQMRAf/EAB8AAAEFAQEBAQEBAAAAAAAAAAABAgMEBQYHCAkKC//EALUQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5+v/EAB8BAAMBAQEBAQEBAQEAAAAAAAABAgMEBQYHCAkKC//EALURAAIBAgQEAwQHBQQEAAECdwABAgMRBAUhMQYSQVEHYXETIjKBCBRCkaGxwQkjM1LwFWJy0QoWJDThJfEXGBkaJicoKSo1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoKDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uLj5OXm5+jp6vLz9PX29/j5+v/bAEMAAQEBAQEBAgEBAgICAgICAwICAgIDBAMDAwMDBAUEBAQEBAQFBQUFBQUFBQYGBgYGBgcHBwcHCAgICAgICAgICP/bAEMBAQEBAgICAwICAwgFBAUICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICP/dAAQAAf/aAAwDAQACEQMRAD8A/v4ooooA/9k=\"\n+                        },\n+                    },\n+                ],\n+            },\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": \"The number of pixels in the image cannot be determined from the provided information.\",\n+            },\n+            {\"role\": \"user\", \"content\": \"Alright\"},\n+        ]\n+\n+        expected_outputs = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"How many pixels are in the image?\"},\n+                    {\"type\": \"image\", \"url\": \"/var/folders/4v/64sxdhsd3gz3r8vhhnyc0mqw0000gn/T/tmp50oyghk6.png\"},\n+                ],\n+            },\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"text\",\n+                        \"text\": \"The number of pixels in the image cannot be determined from the provided information.\",\n+                    }\n+                ],\n+            },\n+            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Alright\"}]},\n+        ]\n+\n+        outputs = ServeCommand.get_processor_inputs_from_inbound_messages(messages, modality)\n+\n+        for expected_output, output in zip(expected_outputs, outputs):\n+            expected_output_content = expected_output[\"content\"]\n+            output_content = output[\"content\"]\n+\n+            self.assertEqual(type(expected_output_content), type(output_content))\n+\n+            if isinstance(expected_output_content, list):\n+                for expected_output_content_item, output_content_item in zip(expected_output_content, output_content):\n+                    self.assertIn(\"type\", expected_output_content_item)\n+                    self.assertIn(\"type\", output_content_item)\n+                    self.assertTrue(expected_output_content_item[\"type\"] == output_content_item[\"type\"])\n+\n+                    if expected_output_content_item[\"type\"] == \"text\":\n+                        self.assertEqual(expected_output_content_item[\"text\"], output_content_item[\"text\"])\n+\n+                    if expected_output_content_item[\"type\"] == \"image\":\n+                        self.assertTrue(os.path.exists(output_content_item[\"url\"]))\n+            else:\n+                raise ValueError(\"VLMs should only receive content as lists.\")\n+\n+\n @slow  # server startup time is slow on our push CI\n @require_openai\n class ServeCompletionsGenerateIntegrationTest(ServeCompletionsMixin, unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 343,
        "additions": 268,
        "deletions": 75
    }
}