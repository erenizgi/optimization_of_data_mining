{
    "author": "iMountTai",
    "message": "fix qwen25-vl grad acc (#40333)\n\n* fix qwen25â€”vl grad acc\n\n* fix Qwen2_5_VLForConditionalGeneration for accepts_loss_kwargs\n\n* fix ci\n\n* fix ci\n\n* fix typo\n\n* fix CI",
    "sha": "64ae6e6b1de2c6822a53be46aba9db68f75ec595",
    "files": [
        {
            "sha": "24975bc0a66960e9565dd3f5154f3a8949cf8cf1",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=64ae6e6b1de2c6822a53be46aba9db68f75ec595",
            "patch": "@@ -893,6 +893,8 @@ def forward(\n class Glm4vModel(Glm4vPreTrainedModel):\n     base_model_prefix = \"\"\n     _checkpoint_conversion_mapping = {}\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n     config: Glm4vConfig\n     _no_split_modules = [\"Glm4vTextDecoderLayer\", \"Glm4vVisionBlock\"]\n \n@@ -1329,6 +1331,8 @@ class Glm4vCausalLMOutputWithPast(ModelOutput):\n class Glm4vForConditionalGeneration(Glm4vPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {}\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "18c8d04ed1514eb987aedf8d388038d01ca89548",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=64ae6e6b1de2c6822a53be46aba9db68f75ec595",
            "patch": "@@ -1009,6 +1009,8 @@ def forward(\n class Glm4vMoeModel(Glm4vMoePreTrainedModel):\n     base_model_prefix = \"\"\n     _checkpoint_conversion_mapping = {}\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n     config: Glm4vMoeConfig\n     _no_split_modules = [\"Glm4vMoeTextDecoderLayer\", \"Glm4vMoeVisionBlock\"]\n \n@@ -1445,6 +1447,8 @@ class Glm4vMoeCausalLMOutputWithPast(ModelOutput):\n class Glm4vMoeForConditionalGeneration(Glm4vMoePreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {}\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "336b9d77ca5ff1ecdb7d7c17ba8a1b1d1753c40b",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=64ae6e6b1de2c6822a53be46aba9db68f75ec595",
            "patch": "@@ -938,6 +938,8 @@ def forward(\n class Qwen2_5_VLModel(Qwen2_5_VLPreTrainedModel):\n     base_model_prefix = \"\"\n     _checkpoint_conversion_mapping = {\"^model\": \"language_model\"}\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n     config: Qwen2_5_VLConfig\n     _no_split_modules = [\"Qwen2_5_VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n \n@@ -1368,6 +1370,8 @@ class Qwen2_5_VLForConditionalGeneration(Qwen2_5_VLPreTrainedModel, GenerationMi\n         r\"^model(?!\\.(language_model|visual))\": \"model.language_model\",\n     }\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "7b47fdb199d69799ff996de1cb503a478636fda9",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=64ae6e6b1de2c6822a53be46aba9db68f75ec595",
            "patch": "@@ -346,6 +346,8 @@ class Qwen2_5_VLModel(Qwen2VLModel):\n     config: Qwen2_5_VLConfig\n     base_model_prefix = \"\"\n     _no_split_modules = [\"Qwen2_5_VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -651,6 +653,9 @@ class Qwen2_5_VLCausalLMOutputWithPast(Qwen2VLCausalLMOutputWithPast):\n \n \n class Qwen2_5_VLForConditionalGeneration(Qwen2VLForConditionalGeneration):\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n+\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,"
        },
        {
            "sha": "a7b79e7257a4359e5f604ca22603488d52025880",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64ae6e6b1de2c6822a53be46aba9db68f75ec595/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=64ae6e6b1de2c6822a53be46aba9db68f75ec595",
            "patch": "@@ -911,6 +911,8 @@ def forward(\n class Qwen2VLModel(Qwen2VLPreTrainedModel):\n     base_model_prefix = \"\"\n     _checkpoint_conversion_mapping = {\"^model\": \"language_model\"}\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n \n     def __init__(self, config: Qwen2VLConfig):\n         super().__init__(config)"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 19,
        "deletions": 0
    }
}