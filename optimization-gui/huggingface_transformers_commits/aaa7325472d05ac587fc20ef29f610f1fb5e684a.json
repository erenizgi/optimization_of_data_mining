{
    "author": "juliendenize",
    "message": "Patch Mistral Common Tokenizer 2 (#41962)\n\n* Patch Mistral Common Tokenizer 2: fix add_special_tokens\n\n* Fix typos\n\n* Fix typing issue\n\n* Make _get_validation_mode static\n\n* wip\n\n* Add int support to decode\n\n* Add edge test cases\n\n* fix\n\n* Support batch for decode\n\n* fix\n\n* Refactor to private logic of batch_decode\n\n* Add special properties",
    "sha": "aaa7325472d05ac587fc20ef29f610f1fb5e684a",
    "files": [
        {
            "sha": "714cf59d185c7d855ff7e8af34df78179128862e",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 153,
            "deletions": 61,
            "changes": 214,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaa7325472d05ac587fc20ef29f610f1fb5e684a/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaa7325472d05ac587fc20ef29f610f1fb5e684a/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=aaa7325472d05ac587fc20ef29f610f1fb5e684a",
            "patch": "@@ -61,7 +61,7 @@\n                 Whether or not to add special tokens when encoding the sequences. This will use the underlying\n                 `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n                 automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n-                automatically.\n+                automatically. When Tokenizer is loading with `finetuning` mode it adds both `bos` and `eos`. Else, for \"test\" mode it only adds `bos`.\n             padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n                 Activates and controls padding. Accepts the following values:\n \n@@ -215,7 +215,7 @@ def __init__(\n         \"\"\"\n         Constructs a `MistralCommonBackend`.\n \n-        - **model_input_names** (`List[str]`) -- A list of inputs expected in the forward pass of the model.\n+        - **model_input_names** (`list[str]`) -- A list of inputs expected in the forward pass of the model.\n         - **padding_side** (`str`) -- The default value for the side on which the model should have padding applied.\n             Should be `'right'` or `'left'`.\n         - **truncation_side** (`str`) -- The default value for the side on which the model should have truncation\n@@ -224,8 +224,11 @@ def __init__(\n         Args:\n             tokenizer_path (`str` or `os.PathLike` or `Path`):\n                 Path to the tokenizer file to load the `MistralTokenizer`.\n-            mode (`ValidationMode`, *optional*, defaults to `ValidationMode.test`):\n-                The mode to use for the tokenizer. This will be passed to the `MistralTokenizer` constructor.\n+            mode (`Union[str, ValidationMode]`, *optional*, defaults to `ValidationMode.test`):\n+                The mode to use for the tokenizer. This will be passed to the `MistralTokenizer` constructor. Possible values are:\n+                - `\"finetuning\"` or `ValidationMode.finetuning`: The finetuning mode.\n+                - `\"test\"` or `ValidationMode.test`: The test mode.\n+                It changes how the tokenizer validates the input and prepares the request to the model.\n             model_max_length (`int`, *optional*):\n                 The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is\n                 loaded with [`~tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`], this will be set to the\n@@ -248,7 +251,8 @@ def __init__(\n             raise ValueError(f\"Kwargs {list(kwargs.keys())} are not supported to init `MistralCommonBackend`.\")\n \n         self._tokenizer_path = Path(tokenizer_path)\n-        self.tokenizer: MistralTokenizer = MistralTokenizer.from_file(str(self._tokenizer_path), mode=mode)\n+        self._mode = self._get_validation_mode(mode)\n+        self.tokenizer: MistralTokenizer = MistralTokenizer.from_file(str(self._tokenizer_path), mode=self._mode)\n         self._tokenizer_type = (\n             MistralTokenizerType.tekken\n             if isinstance(self.tokenizer.instruct_tokenizer.tokenizer, Tekkenizer)\n@@ -259,6 +263,7 @@ def __init__(\n         self.model_max_length = model_max_length\n         self.cleanup_tokenization_spaces = clean_up_tokenization_spaces\n         self.deprecation_warnings = {}  # Use to store when we have already noticed a deprecation warning (avoid overlogging).\n+        self._all_special_tokens_ids = self._get_all_special_ids()\n \n         if model_input_names is not None:\n             if (\n@@ -291,6 +296,16 @@ def clean_up_tokenization(text: str) -> str:\n             .replace(\" 're\", \"'re\")\n         )\n \n+    @property\n+    def mode(self) -> ValidationMode:\n+        \"\"\"\n+        `ValidationMode`: The mode used by the tokenizer. Possible values are:\n+            - `\"finetuning\"` or `ValidationMode.finetuning`: The finetuning mode.\n+            - `\"test\"` or `ValidationMode.test`: The test mode.\n+            It changes how the tokenizer validates the input and prepares the request to the model.\n+        \"\"\"\n+        return self._mode\n+\n     @property\n     def bos_token_id(self) -> int:\n         \"\"\"\n@@ -347,6 +362,20 @@ def pad_token(self) -> str:\n         \"\"\"\n         return self.convert_ids_to_tokens(self.pad_token_id)\n \n+    @property\n+    def all_special_ids(self) -> list[int]:\n+        \"\"\"\n+        `list[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.).\n+        \"\"\"\n+        return sorted(self._all_special_tokens_ids)\n+\n+    @property\n+    def all_special_tokens(self) -> list[str]:\n+        \"\"\"\n+        `list[str]`: A list of all unique special tokens.\n+        \"\"\"\n+        return self.convert_ids_to_tokens(self.all_special_ids)\n+\n     @property\n     def vocab_size(self) -> int:\n         \"\"\"\n@@ -390,7 +419,7 @@ def __len__(self):\n         \"\"\",\n         \"\"\"\n         Returns:\n-            `List[int]`, `torch.Tensor`: The tokenized ids of the text.\n+            `list[int]`, `torch.Tensor`: The tokenized ids of the text.\n         \"\"\",\n     )\n     def encode(\n@@ -412,7 +441,7 @@ def encode(\n         Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n \n         Args:\n-            text (`str` or `List[int]`):\n+            text (`str` or `list[int]`):\n                 The first sequence to be encoded. This can be a string or a list of integers (tokenized string ids).\n             text_pair (`None`, *optional*):\n                 Not supported by `MistralCommonBackend.encode`. Kept to match `PreTrainedTokenizerBase.encode` signature.\n@@ -451,18 +480,19 @@ def encode(\n \n     def decode(\n         self,\n-        token_ids: Union[int, list[int], np.ndarray, \"torch.Tensor\"],\n+        token_ids: Union[int, list[int], list[list[int]], np.ndarray, \"torch.Tensor\"],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: bool | None = None,\n         **kwargs,\n-    ) -> str:\n+    ) -> Union[str, list[str]]:\n         \"\"\"\n         Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n         tokens and clean up tokenization spaces.\n \n         Args:\n-            token_ids (`Union[int, List[int], np.ndarray, torch.Tensor]`):\n-                List of tokenized input ids. Can be obtained using the `__call__` method.\n+            token_ids (`Union[int, list[int], list[list[int]], np.ndarray, torch.Tensor]`):\n+                A single sequence or a batch (list of sequences) of tokenized input ids. Can be obtained using the\n+                `__call__` method.\n             skip_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not to remove special tokens in the decoding.\n             clean_up_tokenization_spaces (`bool`, *optional*):\n@@ -473,29 +503,26 @@ def decode(\n                 Will raise an error if used.\n \n         Returns:\n-            `str`: The decoded sentence.\n+            `Union[str, list[str]]`: The decoded string for a single sequence, or a list of decoded strings for a\n+            batch of sequences.\n         \"\"\"\n         if kwargs:\n             raise ValueError(f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonBackend.decode`.\")\n \n-        clean_up_tokenization_spaces = clean_up_tokenization_spaces or self.cleanup_tokenization_spaces\n-\n-        # Convert inputs to python lists\n         token_ids = to_py_obj(token_ids)\n \n-        special_token_policy = SpecialTokenPolicy.IGNORE if skip_special_tokens else SpecialTokenPolicy.KEEP\n-\n-        decoded_string = self.tokenizer.decode(token_ids, special_token_policy=special_token_policy)\n-        if clean_up_tokenization_spaces:\n-            decoded_string = self.clean_up_tokenization(decoded_string)\n-\n-        # in the specific case of Voxtral, the added f\"lang:xx\" (always a two char language code since it follows ISO 639-1 alpha-2 format)\n-        # is not considered as a special token by mistral-common and is encoded/ decoded as normal text.\n-        # Nevertheless we should remove it to ease users life.\n-        if skip_special_tokens:\n-            decoded_string = re.sub(r\"^lang:[a-z]{2}\", \"\", decoded_string)\n+        if isinstance(token_ids, (list, tuple)) and len(token_ids) > 0 and isinstance(token_ids[0], (list, tuple)):\n+            return self._batch_decode(\n+                sequences=token_ids,\n+                skip_special_tokens=skip_special_tokens,\n+                clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+            )\n \n-        return decoded_string\n+        return self._decode(\n+            token_ids=token_ids,\n+            skip_special_tokens=skip_special_tokens,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+        )\n \n     def batch_decode(\n         self,\n@@ -507,8 +534,11 @@ def batch_decode(\n         \"\"\"\n         Convert a list of lists of token ids into a list of strings by calling decode.\n \n+        This method is provided for backwards compatibility. The `decode` method now handles batched input natively,\n+        so you can use `decode` directly instead of `batch_decode`.\n+\n         Args:\n-            sequences (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor]`):\n+            sequences (`Union[list[int], list[list[int]], np.ndarray, torch.Tensor]`):\n                 List of tokenized input ids. Can be obtained using the `__call__` method.\n             skip_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not to remove special tokens in the decoding.\n@@ -520,14 +550,56 @@ def batch_decode(\n                 Will raise an error if used.\n \n         Returns:\n-            `List[str]`: The list of decoded sentences.\n+            `list[str]`: The list of decoded sentences.\n         \"\"\"\n+        if kwargs:\n+            raise ValueError(f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonBackend.batch_decode`.\")\n+\n+        return self._batch_decode(\n+            sequences=sequences,\n+            skip_special_tokens=skip_special_tokens,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+        )\n+\n+    def _decode(\n+        self,\n+        token_ids: Union[int, list[int], list[list[int]], np.ndarray, \"torch.Tensor\"],\n+        skip_special_tokens: bool = False,\n+        clean_up_tokenization_spaces: bool | None = None,\n+    ) -> str:\n+        clean_up_tokenization_spaces = clean_up_tokenization_spaces or self.cleanup_tokenization_spaces\n+\n+        # Convert inputs to python lists\n+        if isinstance(token_ids, int):\n+            token_ids = [token_ids]\n+\n+        token_ids = to_py_obj(token_ids)\n+\n+        special_token_policy = SpecialTokenPolicy.IGNORE if skip_special_tokens else SpecialTokenPolicy.KEEP\n+\n+        decoded_string = self.tokenizer.decode(token_ids, special_token_policy=special_token_policy)\n+        if clean_up_tokenization_spaces:\n+            decoded_string = self.clean_up_tokenization(decoded_string)\n+\n+        # in the specific case of Voxtral, the added f\"lang:xx\" (always a two char language code since it follows ISO 639-1 alpha-2 format)\n+        # is not considered as a special token by mistral-common and is encoded/ decoded as normal text.\n+        # Nevertheless we should remove it to ease users life.\n+        if skip_special_tokens:\n+            decoded_string = re.sub(r\"^lang:[a-z]{2}\", \"\", decoded_string)\n+\n+        return decoded_string\n+\n+    def _batch_decode(\n+        self,\n+        sequences: Union[list[int], list[list[int]], np.ndarray, \"torch.Tensor\"],\n+        skip_special_tokens: bool = False,\n+        clean_up_tokenization_spaces: bool | None = None,\n+    ) -> list[str]:\n         return [\n-            self.decode(\n+            self._decode(\n                 seq,\n                 skip_special_tokens=skip_special_tokens,\n                 clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n-                **kwargs,\n             )\n             for seq in sequences\n         ]\n@@ -550,13 +622,13 @@ def convert_ids_to_tokens(self, ids: int | list[int], skip_special_tokens: bool\n         added tokens.\n \n         Args:\n-            ids (`int` or `List[int]`):\n+            ids (`int` or `list[int]`):\n                 The token id (or token ids) to convert to tokens.\n             skip_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not to remove special tokens in the decoding.\n \n         Returns:\n-            `str` or `List[str]`: The decoded token(s).\n+            `str` or `list[str]`: The decoded token(s).\n         \"\"\"\n \n         if isinstance(ids, int):\n@@ -608,10 +680,10 @@ def convert_tokens_to_ids(self, tokens: str | list[str]) -> int | list[int]:\n         vocabulary.\n \n         Args:\n-            tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).\n+            tokens (`str` or `list[str]`): One or several token(s) to convert to token id(s).\n \n         Returns:\n-            `int` or `List[int]`: The token id or list of token ids.\n+            `int` or `list[int]`: The token id or list of token ids.\n         \"\"\"\n \n         if isinstance(tokens, str):\n@@ -632,9 +704,8 @@ def _text_to_ids(self, text: TextInput, add_special_tokens: bool) -> list[int]:\n         \"\"\"\n         Converts a string into a sequence of tokens ids, using the tokenizer.\n         \"\"\"\n-        tokens_ids = self.tokenizer.instruct_tokenizer.tokenizer.encode(\n-            text, bos=add_special_tokens, eos=add_special_tokens\n-        )\n+        add_eos = add_special_tokens and self._mode == ValidationMode.finetuning\n+        tokens_ids = self.tokenizer.instruct_tokenizer.tokenizer.encode(text, bos=add_special_tokens, eos=add_eos)\n         return tokens_ids\n \n     def tokenize(self, text: TextInput, **kwargs) -> list[str]:\n@@ -651,7 +722,7 @@ def tokenize(self, text: TextInput, **kwargs) -> list[str]:\n                 Will raise an error if used.\n \n         Returns:\n-            `List[str]`: The list of tokens.\n+            `list[str]`: The list of tokens.\n         \"\"\"\n         if kwargs:\n             raise ValueError(f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonBackend.tokenize`.\")\n@@ -751,7 +822,7 @@ def get_input_ids(text):\n \n         return BatchEncoding(batch_outputs)\n \n-    def _all_special_ids(self) -> set[int]:\n+    def _get_all_special_ids(self) -> set[int]:\n         if self._tokenizer_type == MistralTokenizerType.tekken:\n             return {t[\"rank\"] for t in self.tokenizer.instruct_tokenizer.tokenizer._all_special_tokens}\n         elif self._tokenizer_type == MistralTokenizerType.spm:\n@@ -767,9 +838,9 @@ def get_special_tokens_mask(\n         special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of ids of the sequence.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Not supported by `MistralCommonBackend`. Kept to match the interface of `PreTrainedTokenizerBase`.\n             already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not the token list is already formatted with special tokens for the model.\n@@ -786,9 +857,7 @@ def get_special_tokens_mask(\n                 \"`already_has_special_tokens` is not supported by `MistralCommonBackend` and should be `False`.\"\n             )\n \n-        all_special_ids = self._all_special_ids()  # cache the ids\n-\n-        special_tokens_mask = [1 if token in all_special_ids else 0 for token in token_ids_0]\n+        special_tokens_mask = [1 if token in self._all_special_tokens_ids else 0 for token in token_ids_0]\n         return special_tokens_mask\n \n     def _batch_prepare_for_model(\n@@ -882,7 +951,7 @@ def prepare_for_model(\n         manages a moving window (with user defined stride) for overflowing tokens.\n \n         Args:\n-            ids (`List[int]`):\n+            ids (`list[int]`):\n                 Tokenized input ids of the first sequence.\n             pair_ids (`None`, *optional*):\n                 Not supported by `MistralCommonBackend`. Kept to match the interface of `PreTrainedTokenizerBase`.\n@@ -1081,7 +1150,7 @@ def _pad(\n \n         Args:\n             encoded_inputs:\n-                Dictionary of tokenized inputs (`List[int]`) or batch of tokenized inputs (`List[List[int]]`).\n+                Dictionary of tokenized inputs (`list[int]`) or batch of tokenized inputs (`list[list[int]]`).\n             max_length: maximum length of the returned list and optionally padding length (see below).\n                 Will truncate by taking into account the special tokens.\n             padding_strategy: PaddingStrategy to use for padding.\n@@ -1171,13 +1240,13 @@ def pad(\n         </Tip>\n \n         Args:\n-            encoded_inputs ([`BatchEncoding`], list of [`BatchEncoding`], `Dict[str, List[int]]`, `Dict[str, List[List[int]]` or `List[Dict[str, List[int]]]`):\n-                Tokenized inputs. Can represent one input ([`BatchEncoding`] or `Dict[str, List[int]]`) or a batch of\n-                tokenized inputs (list of [`BatchEncoding`], *Dict[str, List[List[int]]]* or *List[Dict[str,\n-                List[int]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader\n+            encoded_inputs ([`BatchEncoding`], list of [`BatchEncoding`], `Dict[str, list[int]]`, `Dict[str, list[list[int]]` or `List[Dict[str, list[int]]]`):\n+                Tokenized inputs. Can represent one input ([`BatchEncoding`] or `Dict[str, list[int]]`) or a batch of\n+                tokenized inputs (list of [`BatchEncoding`], *Dict[str, list[list[int]]]* or *List[Dict[str,\n+                list[int]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader\n                 collate function.\n \n-                Instead of `List[int]` you can have tensors (numpy arrays, PyTorch tensors), see\n+                Instead of `list[int]` you can have tensors (numpy arrays, PyTorch tensors), see\n                 the note above for the return type.\n             padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n                  Select a strategy to pad the returned sequences (according to the model's padding side and padding\n@@ -1316,7 +1385,7 @@ def truncate_sequences(\n         Truncates a sequence pair in-place following the strategy.\n \n         Args:\n-            ids (`List[int]`):\n+            ids (`list[int]`):\n                 Tokenized input ids. Can be obtained from a string by chaining the `tokenize` and\n                 `convert_tokens_to_ids` methods.\n             pair_ids (`None`, *optional*):\n@@ -1335,7 +1404,7 @@ def truncate_sequences(\n                 sequence returned. The value of this argument defines the number of additional tokens.\n \n         Returns:\n-            `Tuple[List[int], None, List[int]]`: The truncated `ids` and the list of\n+            `Tuple[list[int], None, list[int]]`: The truncated `ids` and the list of\n             overflowing tokens. `None` is returned to match Transformers signature.\n         \"\"\"\n         if kwargs:\n@@ -1442,7 +1511,7 @@ def apply_chat_template(\n                 Will raise an error if used.\n \n         Returns:\n-            `Union[str, List[int], List[str], List[List[int]], BatchEncoding]`: A list of token ids representing the tokenized chat so far, including control\n+            `Union[str, list[int], list[str], list[list[int]], BatchEncoding]`: A list of token ids representing the tokenized chat so far, including control\n             tokens. This output is ready to pass to the model, either directly or via methods like `generate()`.\n         \"\"\"\n         if kwargs:\n@@ -1617,7 +1686,7 @@ def __call__(\n         sequences.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n+            text (`str`, `list[str]`, `list[list[str]]`, *optional*):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of int\n                 (encoded strings).\n             text_pair (`None`, *optional*):\n@@ -1657,8 +1726,8 @@ def _is_valid_text_input(t):\n \n         if not _is_valid_text_input(text):\n             raise ValueError(\n-                \"text input must be of type `str` (single example), `List[str]` (batch or single encoded example) \"\n-                \"or `List[List[int]]` (batch of encoded examples).\"\n+                \"text input must be of type `str` (single example), `list[str]` (batch or single encoded example) \"\n+                \"or `list[list[int]]` (batch of encoded examples).\"\n             )\n \n         is_batched = isinstance(text, (list, tuple)) and isinstance(text[0], (str, list, tuple))\n@@ -1712,7 +1781,7 @@ def from_pretrained(\n         cls,\n         pretrained_model_name_or_path: str | os.PathLike,\n         *init_inputs,\n-        mode: ValidationMode = ValidationMode.test,\n+        mode: Union[str, ValidationMode] = ValidationMode.test,\n         cache_dir: str | os.PathLike | None = None,\n         force_download: bool = False,\n         local_files_only: bool = False,\n@@ -1737,8 +1806,11 @@ def from_pretrained(\n                 - A path to a *directory* containing the tokenizer config, for instance saved\n                   using the [`MistralCommonBackend.tokenization_mistral_common.save_pretrained`] method, e.g.,\n                   `./my_model_directory/`.\n-            mode (`ValidationMode`, *optional*, defaults to `ValidationMode.test`):\n-                Validation mode for the `MistralTokenizer` tokenizer.\n+            mode (`Union[str, ValidationMode]`, *optional*, defaults to `ValidationMode.test`):\n+                Validation mode for the `MistralTokenizer` tokenizer. Possible values are:\n+                - `\"finetuning\"` or `ValidationMode.finetuning`: The finetuning mode.\n+                - `\"test\"` or `ValidationMode.test`: The test mode.\n+                It changes how the tokenizer validates the input and prepare the request to the model.\n             cache_dir (`str` or `os.PathLike`, *optional*):\n                 Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n                 standard cache should not be used.\n@@ -1784,6 +1856,8 @@ def from_pretrained(\n         ):\n             raise ValueError(f\"Some kwargs in {kwargs} are not supported by `MistralCommonBackend.from_pretrained`.\")\n \n+        mode = cls._get_validation_mode(mode)\n+\n         if not os.path.isdir(pretrained_model_name_or_path):\n             tokenizer_path = download_tokenizer_from_hf_hub(\n                 repo_id=pretrained_model_name_or_path,\n@@ -1894,3 +1968,21 @@ def save_pretrained(\n             )\n \n         return (str(save_directory / self._tokenizer_path.name),)\n+\n+    @staticmethod\n+    def _get_validation_mode(mode: Union[str, ValidationMode]) -> ValidationMode:\n+        \"\"\"Get the validation mode from a string or a ValidationMode.\"\"\"\n+        _invalid_mode_msg = (\n+            f\"Invalid `mistral-common` tokenizer mode: {mode}. Possible values are 'finetuning' or 'test'.\"\n+        )\n+        if isinstance(mode, str):\n+            try:\n+                mode = ValidationMode[mode]\n+            except KeyError:\n+                raise ValueError(_invalid_mode_msg)\n+        elif not isinstance(mode, (str, ValidationMode)):\n+            raise ValueError(_invalid_mode_msg)\n+\n+        if mode not in [ValidationMode.finetuning, ValidationMode.test]:\n+            raise ValueError(_invalid_mode_msg)\n+        return mode"
        },
        {
            "sha": "1a82a07d1a6cc3691fe4096bdfb26a7cfa9da94e",
            "filename": "tests/test_tokenization_mistral_common.py",
            "status": "modified",
            "additions": 207,
            "deletions": 37,
            "changes": 244,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaa7325472d05ac587fc20ef29f610f1fb5e684a/tests%2Ftest_tokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaa7325472d05ac587fc20ef29f610f1fb5e684a/tests%2Ftest_tokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_mistral_common.py?ref=aaa7325472d05ac587fc20ef29f610f1fb5e684a",
            "patch": "@@ -17,6 +17,7 @@\n import io\n import tempfile\n import unittest\n+from unittest.mock import patch\n \n import numpy as np\n import torch\n@@ -33,6 +34,9 @@\n     import mistral_common.tokens.tokenizers\n     from mistral_common.exceptions import InvalidMessageStructureException\n     from mistral_common.protocol.instruct.request import ChatCompletionRequest\n+    from mistral_common.protocol.instruct.validator import (\n+        ValidationMode,\n+    )\n     from mistral_common.protocol.transcription.request import TranscriptionRequest\n     from mistral_common.tokens.tokenizers.base import SpecialTokenPolicy\n     from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n@@ -206,7 +210,7 @@ def test_encode(self):\n \n         # Test 1:\n         # encode with add_special_tokens\n-        expected_with_special = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(string, bos=True, eos=True)\n+        expected_with_special = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(string, bos=True, eos=False)\n         tokens_with_special = self.tokenizer.encode(string, add_special_tokens=True)\n         self.assertEqual(tokens_with_special, expected_with_special)\n \n@@ -216,6 +220,13 @@ def test_encode(self):\n         tokens_without_special = self.tokenizer.encode(string, add_special_tokens=False)\n         self.assertEqual(tokens_without_special, expected_without_special)\n \n+        # Test 3:\n+        # encode with add_special_tokens and mode finetuning\n+        expected_with_special_ft = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(string, bos=True, eos=True)\n+        with patch.object(self.tokenizer, \"_mode\", ValidationMode.finetuning):\n+            tokens_with_special_ft = self.tokenizer.encode(string, add_special_tokens=True)\n+        self.assertEqual(tokens_with_special_ft, expected_with_special_ft)\n+\n         # Test 3:\n         # encode with return_tensors\n         tokens_with_return_tensors = self.tokenizer.encode(string, add_special_tokens=False, return_tensors=\"pt\")\n@@ -273,7 +284,10 @@ def test_encode(self):\n         ) + expected_long\n         self.assertEqual(tokens_with_padding_and_truncation, expected_long_padding)\n \n-        # Test encode with unsupported kwargs\n+        # Test 8:\n+        # encode empty string\n+        self.assertEqual(self.tokenizer.encode(\"\", add_special_tokens=False), [])\n+\n         with self.assertRaises(\n             ValueError, msg=\"Kwargs [unk_args] are not supported by `MistralCommonBackend.encode`.\"\n         ):\n@@ -302,12 +316,57 @@ def test_decode(self):\n         )\n \n         # Test 3:\n+        # decode one token\n+        self.assertEqual(self.tokenizer.decode(tokens_ids[0], skip_special_tokens=False), \"<s>\")\n+\n+        # Test 4:\n+        # decode numpy\n+        self.assertEqual(self.tokenizer.decode(np.array(tokens_ids), skip_special_tokens=True), string)\n+\n+        # Test 5:\n+        # decode empty string\n+        self.assertEqual(self.tokenizer.decode([], skip_special_tokens=True), \"\")\n+\n+        # Test 6:\n         # decode with unsupported kwargs\n         with self.assertRaises(\n             ValueError, msg=\"Kwargs [unk_args] are not supported by `MistralCommonBackend.decode`.\"\n         ):\n             self.tokenizer.decode(tokens_ids, skip_special_tokens=False, unk_args=\"\")\n \n+    def test_decode_on_batch(self):\n+        string = \"Hello, world!\"\n+        string_with_space = \"Hello, world !\"\n+\n+        batch_tokens_ids = [\n+            self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(string, bos=True, eos=True),\n+            self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(string_with_space, bos=True, eos=True),\n+        ]\n+\n+        # Test 1:\n+        # batch_decode with and without skip_special_tokens\n+        self.assertEqual(\n+            self.tokenizer.decode(batch_tokens_ids, skip_special_tokens=True),\n+            [string, string_with_space],\n+        )\n+        self.assertEqual(\n+            self.tokenizer.decode(batch_tokens_ids, skip_special_tokens=False),\n+            [\"<s>\" + string + \"</s>\", \"<s>\" + string_with_space + \"</s>\"],\n+        )\n+        self.assertEqual(\n+            self.tokenizer.decode(batch_tokens_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True),\n+            [\"Hello, world!\", \"Hello, world!\"],\n+        )\n+\n+        # Test 3:\n+        # decode numpy\n+        self.assertEqual(\n+            self.tokenizer.decode(\n+                np.array(batch_tokens_ids), skip_special_tokens=True, clean_up_tokenization_spaces=True\n+            ),\n+            [\"Hello, world!\", \"Hello, world!\"],\n+        )\n+\n     def test_decode_transcription_mode(self):\n         # in the specific case of Voxtral, the added f\"lang:xx\" (always a two char language code since it follows ISO 639-1 alpha-2 format)\n         # is not considered as a special token by mistral-common and is encoded/ decoded as normal text.\n@@ -357,7 +416,23 @@ def test_batch_decode(self):\n             [\"Hello, world!\", \"Hello, world!\"],\n         )\n \n-        # Test 2:\n+        # Test 3:\n+        # decode numpy\n+        self.assertEqual(\n+            self.tokenizer.batch_decode(\n+                np.array(batch_tokens_ids), skip_special_tokens=True, clean_up_tokenization_spaces=True\n+            ),\n+            [\"Hello, world!\", \"Hello, world!\"],\n+        )\n+\n+        # Test 4:\n+        # decode empty list\n+        self.assertEqual(self.tokenizer.batch_decode([], skip_special_tokens=True), [])\n+        self.assertEqual(\n+            self.tokenizer.batch_decode([batch_tokens_ids[0], []], skip_special_tokens=True), [string, \"\"]\n+        )\n+\n+        # Test 5:\n         # batch_decode with unsupported kwargs\n         with self.assertRaises(\n             ValueError, msg=\"Kwargs [unk_args] are not supported by `MistralCommonBackend.batch_decode`.\"\n@@ -382,6 +457,11 @@ def test_convert_ids_to_tokens(self):\n         tokens = self.tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=True)\n         self.assertEqual(tokens, expected_tokens)\n \n+        # Test 3:\n+        # with empty list\n+        tokens = self.tokenizer.convert_ids_to_tokens([])\n+        self.assertEqual(tokens, [])\n+\n         with self.assertRaises(ValueError):\n             self.tokenizer.convert_ids_to_tokens(ids[0], skip_special_tokens=True)\n         token = self.tokenizer.convert_ids_to_tokens(ids[1], skip_special_tokens=True)\n@@ -401,15 +481,27 @@ def test_convert_tokens_to_ids(self):\n         self.assertEqual(id, expected_ids[0])\n         self.assertEqual(id, self.tokenizer.convert_tokens_to_ids(tokens[0]))\n \n+        # Test 3:\n+        # with empty list\n+        ids = self.tokenizer.convert_tokens_to_ids([])\n+        self.assertEqual(ids, [])\n+\n     def test_tokenize(self):\n         string = \"Hello world!\"\n+        # Test 1:\n+        # with string\n         expected_tokens = [\n             self.ref_tokenizer.instruct_tokenizer.tokenizer.id_to_piece(id)\n             for id in self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(string, bos=False, eos=False)\n         ]\n         tokens = self.tokenizer.tokenize(string)\n         self.assertEqual(tokens, expected_tokens)\n \n+        # Test 2:\n+        # with empty string\n+        tokens = self.tokenizer.tokenize(\"\")\n+        self.assertEqual(tokens, [])\n+\n         with self.assertRaises(\n             ValueError, msg=\"Kwargs [add_special_tokens] are not supported by `MistralCommonBackend.tokenize`.\"\n         ):\n@@ -1473,7 +1565,7 @@ def test_call(self):\n         # Test 1:\n         # default case\n         text = \"Hello world!\"\n-        expected_tokens = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(text, bos=True, eos=True)\n+        expected_tokens = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(text, bos=True, eos=False)\n         tokens = self.tokenizer(text)\n         self.assertIsInstance(tokens, BatchEncoding)\n         self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n@@ -1498,7 +1590,7 @@ def test_call(self):\n         tokens = self.tokenizer(text, return_special_tokens_mask=True)\n         self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n         self.assertEqual(tokens[\"attention_mask\"], [1] * len(expected_tokens))\n-        self.assertEqual(tokens[\"special_tokens_mask\"], [1] + [0] * (len(expected_tokens) - 2) + [1])\n+        self.assertEqual(tokens[\"special_tokens_mask\"], [1] + [0] * (len(expected_tokens) - 1))\n \n         # Test 5:\n         # add_special_tokens=False\n@@ -1509,6 +1601,24 @@ def test_call(self):\n         self.assertEqual(tokens[\"attention_mask\"], [1] * len(expected_tokens))\n         self.assertEqual(tokens[\"special_tokens_mask\"], [0] * len(expected_tokens))\n \n+        # Test 6:\n+        # add_special_tokens=False and mode finetuning\n+        text = \"Hello world!\"\n+        expected_tokens = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(text, bos=True, eos=True)\n+        with patch.object(self.tokenizer, \"_mode\", ValidationMode.finetuning):\n+            tokens = self.tokenizer(text, return_special_tokens_mask=True)\n+        self.assertIsInstance(tokens, BatchEncoding)\n+        self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+        self.assertEqual(tokens[\"attention_mask\"], [1] * len(expected_tokens))\n+        self.assertEqual(tokens[\"special_tokens_mask\"], [1] + [0] * (len(expected_tokens) - 2) + [1])\n+\n+        # Test 7:\n+        # empty string\n+        tokens = self.tokenizer(\"\", add_special_tokens=False)\n+        self.assertIsInstance(tokens, BatchEncoding)\n+        self.assertEqual(tokens[\"input_ids\"], [])\n+        self.assertEqual(tokens[\"attention_mask\"], [])\n+\n         with self.assertRaises(\n             ValueError, msg=\"Kwargs [wrong_kwarg] are not supported by `MistralCommonBackend.__call__`.\"\n         ):\n@@ -1534,7 +1644,7 @@ def test_call_with_truncation(self):\n         # Test 1:\n         # truncation=True or \"longest_first\" or TruncationStrategy.LONGEST_FIRST\n         text = \"Hello world!\" * 10\n-        expected_tokens = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(text, bos=True, eos=True)\n+        expected_tokens = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(text, bos=True, eos=False)\n \n         for truncation in [True, \"longest_first\", TruncationStrategy.LONGEST_FIRST]:\n             tokens = self.tokenizer(text, truncation=True, max_length=10, return_special_tokens_mask=True)\n@@ -1550,7 +1660,7 @@ def test_call_with_truncation(self):\n             self.assertIsInstance(tokens, BatchEncoding)\n             self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n             self.assertEqual(tokens[\"attention_mask\"], [1] * len(expected_tokens))\n-            self.assertEqual(tokens[\"special_tokens_mask\"], [1] + [0] * (len(expected_tokens) - 2) + [1])\n+            self.assertEqual(tokens[\"special_tokens_mask\"], [1] + [0] * (len(expected_tokens) - 1))\n \n         # Test 3:\n         # truncation=True or \"longest_first\" or TruncationStrategy.LONGEST_FIRST with return_overflowing_tokens=True and stride\n@@ -1583,7 +1693,7 @@ def test_call_with_truncation(self):\n \n     def test_call_with_padding(self):\n         text = \"Hello world!\"\n-        expected_tokens = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(text, bos=True, eos=True)\n+        expected_tokens = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(text, bos=True, eos=False)\n \n         # Test 1:\n         # padding=False or padding=True or \"do_not_pad\" or PaddingStrategy.DO_NOT_PAD or padding=\"longest\" or PaddingStrategy.LONGEST\n@@ -1592,7 +1702,7 @@ def test_call_with_padding(self):\n             self.assertIsInstance(tokens, BatchEncoding)\n             self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n             self.assertEqual(tokens[\"attention_mask\"], [1] * len(expected_tokens))\n-            self.assertEqual(tokens[\"special_tokens_mask\"], [1] + [0] * (len(expected_tokens) - 2) + [1])\n+            self.assertEqual(tokens[\"special_tokens_mask\"], [1] + [0] * (len(expected_tokens) - 1))\n \n         # Test 2:\n         # padding=\"max_length\" or PaddingStrategy.MAX_LENGTH\n@@ -1602,9 +1712,7 @@ def test_call_with_padding(self):\n             num_padding = 20 - len(expected_tokens)\n             self.assertEqual(tokens[\"input_ids\"], num_padding * [self.tokenizer.pad_token_id] + expected_tokens)\n             self.assertEqual(tokens[\"attention_mask\"], num_padding * [0] + [1] * len(expected_tokens))\n-            self.assertEqual(\n-                tokens[\"special_tokens_mask\"], num_padding * [1] + [1] + [0] * (len(expected_tokens) - 2) + [1]\n-            )\n+            self.assertEqual(tokens[\"special_tokens_mask\"], num_padding * [1] + [1] + [0] * (len(expected_tokens) - 1))\n \n         # Test 3:\n         # pad_to_multiple_of\n@@ -1615,9 +1723,7 @@ def test_call_with_padding(self):\n         num_padding = 16 - len(expected_tokens)\n         self.assertEqual(tokens[\"input_ids\"], num_padding * [self.tokenizer.pad_token_id] + expected_tokens)\n         self.assertEqual(tokens[\"attention_mask\"], num_padding * [0] + [1] * len(expected_tokens))\n-        self.assertEqual(\n-            tokens[\"special_tokens_mask\"], num_padding * [1] + [1] + [0] * (len(expected_tokens) - 2) + [1]\n-        )\n+        self.assertEqual(tokens[\"special_tokens_mask\"], num_padding * [1] + [1] + [0] * (len(expected_tokens) - 1))\n \n         # Test 4:\n         # padding=\"max_length\" and padding_side=\"right\"\n@@ -1628,15 +1734,15 @@ def test_call_with_padding(self):\n         num_padding = 20 - len(expected_tokens)\n         self.assertEqual(tokens[\"input_ids\"], expected_tokens + num_padding * [self.tokenizer.pad_token_id])\n         self.assertEqual(tokens[\"attention_mask\"], [1] * len(expected_tokens) + num_padding * [0])\n-        self.assertEqual(\n-            tokens[\"special_tokens_mask\"], [1] + [0] * (len(expected_tokens) - 2) + [1] + num_padding * [1]\n-        )\n+        self.assertEqual(tokens[\"special_tokens_mask\"], [1] + [0] * (len(expected_tokens) - 1) + num_padding * [1])\n \n     def test_batch_call(self):\n         # Test 1:\n         # default case\n         text = [\"Hello world!\", \"Hello world! Longer\"]\n-        expected_tokens = [self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=True, eos=True) for t in text]\n+        expected_tokens = [\n+            self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=True, eos=False) for t in text\n+        ]\n         tokens = self.tokenizer(text)\n         self.assertIsInstance(tokens, BatchEncoding)\n         self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n@@ -1682,15 +1788,12 @@ def test_batch_call(self):\n                 torch.Tensor(\n                     (len(expected_tokens[1]) - len(expected_tokens[0])) * [1]\n                     + [1]\n-                    + [0] * (len(expected_tokens[0]) - 2)\n-                    + [1]\n+                    + [0] * (len(expected_tokens[0]) - 1)\n                 ),\n             )\n         )\n         self.assertTrue(\n-            torch.equal(\n-                tokens[\"special_tokens_mask\"][1], torch.Tensor([1] + [0] * (len(expected_tokens[1]) - 2) + [1])\n-            )\n+            torch.equal(tokens[\"special_tokens_mask\"][1], torch.Tensor([1] + [0] * (len(expected_tokens[1]) - 1)))\n         )\n \n         # Test 4:\n@@ -1704,11 +1807,45 @@ def test_batch_call(self):\n         self.assertEqual(tokens[\"attention_mask\"], [[1] * len(t) for t in expected_tokens])\n         self.assertEqual(tokens[\"special_tokens_mask\"], [[0] * len(t) for t in expected_tokens])\n \n+        # Test 5:\n+        # add_special_tokens=True and mode = finetuning\n+        expected_tokens = [self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=True, eos=True) for t in text]\n+        with patch.object(self.tokenizer, \"_mode\", ValidationMode.finetuning):\n+            tokens = self.tokenizer(text, add_special_tokens=True, return_special_tokens_mask=True)\n+        self.assertIsInstance(tokens, BatchEncoding)\n+        self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+        self.assertEqual(\n+            tokens[\"special_tokens_mask\"],\n+            [[1] + [0] * (len(expected_tokens[0]) - 2) + [1], [1] + [0] * (len(expected_tokens[1]) - 2) + [1]],\n+        )\n+\n+        # Test 6:\n+        # empty string in batch\n+        expected_tokens = [\n+            self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=False, eos=False) for t in text\n+        ]\n+        expected_tokens.append([])\n+        tokens = self.tokenizer(text + [\"\"], add_special_tokens=False, return_special_tokens_mask=True)\n+        self.assertIsInstance(tokens, BatchEncoding)\n+        self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+        self.assertEqual(tokens[\"attention_mask\"], [[1] * len(t) for t in expected_tokens])\n+        self.assertEqual(tokens[\"special_tokens_mask\"], [[0] * len(t) for t in expected_tokens])\n+\n+        # Test 7:\n+        # empty batch\n+        tokens = self.tokenizer([\"\"], add_special_tokens=False, return_special_tokens_mask=True)\n+        self.assertIsInstance(tokens, BatchEncoding)\n+        self.assertEqual(tokens[\"input_ids\"], [[]])\n+        self.assertEqual(tokens[\"attention_mask\"], [[]])\n+        self.assertEqual(tokens[\"special_tokens_mask\"], [[]])\n+\n     def test_batch_call_with_truncation(self):\n         # Test 1:\n         # truncation=True\n         text = [\"Hello world!\", \"Hello world! Longer\" * 10]\n-        expected_tokens = [self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=True, eos=True) for t in text]\n+        expected_tokens = [\n+            self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=True, eos=False) for t in text\n+        ]\n \n         for truncation in [True, \"longest_first\", TruncationStrategy.LONGEST_FIRST]:\n             tokens = self.tokenizer(text, truncation=True, max_length=10, return_special_tokens_mask=True)\n@@ -1729,7 +1866,7 @@ def test_batch_call_with_truncation(self):\n             self.assertEqual(tokens[\"attention_mask\"], [[1] * len(t) for t in expected_tokens])\n             self.assertEqual(\n                 tokens[\"special_tokens_mask\"],\n-                [[1] + [0] * (len(t) - 2) + [1] for t in expected_tokens],\n+                [[1] + [0] * (len(t) - 1) for t in expected_tokens],\n             )\n \n         # Test 3:\n@@ -1764,15 +1901,17 @@ def test_batch_call_with_padding(self):\n         # Test 1:\n         # padding=False or padding=True or \"do_not_pad\" or PaddingStrategy.DO_NOT_PAD or padding=\"longest\" or PaddingStrategy.LONGEST\n         text = [\"Hello world!\", \"Hello world! Longer\"]\n-        expected_tokens = [self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=True, eos=True) for t in text]\n+        expected_tokens = [\n+            self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=True, eos=False) for t in text\n+        ]\n         for padding in [False, \"do_not_pad\", PaddingStrategy.DO_NOT_PAD]:\n             tokens = self.tokenizer(text, padding=padding, return_special_tokens_mask=True)\n             self.assertIsInstance(tokens, BatchEncoding)\n             self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n             self.assertEqual(tokens[\"attention_mask\"], [[1] * len(t) for t in expected_tokens])\n             self.assertEqual(\n                 tokens[\"special_tokens_mask\"],\n-                [[1] + [0] * (len(t) - 2) + [1] for t in expected_tokens],\n+                [[1] + [0] * (len(t) - 1) for t in expected_tokens],\n             )\n \n         # Test 2:\n@@ -1798,8 +1937,8 @@ def test_batch_call_with_padding(self):\n             self.assertEqual(\n                 tokens[\"special_tokens_mask\"],\n                 [\n-                    num_padding[0] * [1] + [1] + [0] * (len(expected_tokens[0]) - 2) + [1],\n-                    num_padding[1] * [1] + [1] + [0] * (len(expected_tokens[1]) - 2) + [1],\n+                    num_padding[0] * [1] + [1] + [0] * (len(expected_tokens[0]) - 1),\n+                    num_padding[1] * [1] + [1] + [0] * (len(expected_tokens[1]) - 1),\n                 ],\n             )\n \n@@ -1826,8 +1965,8 @@ def test_batch_call_with_padding(self):\n             self.assertEqual(\n                 tokens[\"special_tokens_mask\"],\n                 [\n-                    num_padding[0] * [1] + [1] + [0] * (len(expected_tokens[0]) - 2) + [1],\n-                    num_padding[1] * [1] + [1] + [0] * (len(expected_tokens[1]) - 2) + [1],\n+                    num_padding[0] * [1] + [1] + [0] * (len(expected_tokens[0]) - 1),\n+                    num_padding[1] * [1] + [1] + [0] * (len(expected_tokens[1]) - 1),\n                 ],\n             )\n \n@@ -1855,8 +1994,8 @@ def test_batch_call_with_padding(self):\n         self.assertEqual(\n             tokens[\"special_tokens_mask\"],\n             [\n-                num_padding[0] * [1] + [1] + [0] * (len(expected_tokens[0]) - 2) + [1],\n-                num_padding[1] * [1] + [1] + [0] * (len(expected_tokens[1]) - 2) + [1],\n+                num_padding[0] * [1] + [1] + [0] * (len(expected_tokens[0]) - 1),\n+                num_padding[1] * [1] + [1] + [0] * (len(expected_tokens[1]) - 1),\n             ],\n         )\n \n@@ -1885,8 +2024,8 @@ def test_batch_call_with_padding(self):\n             self.assertEqual(\n                 tokens[\"special_tokens_mask\"],\n                 [\n-                    [1] + [0] * (len(expected_tokens[0]) - 2) + [1] + num_padding[0] * [1],\n-                    [1] + [0] * (len(expected_tokens[1]) - 2) + [1] + num_padding[1] * [1],\n+                    [1] + [0] * (len(expected_tokens[0]) - 1) + num_padding[0] * [1],\n+                    [1] + [0] * (len(expected_tokens[1]) - 1) + num_padding[1] * [1],\n                 ],\n             )\n \n@@ -1896,7 +2035,9 @@ def test_batch_call_with_padding_and_truncation(self):\n         # and truncation=True or \"longest_first\" or TruncationStrategy.LONGEST_FIRST\n         # and max_length\n         text = [\"Hello world!\", \"Hello world! Longer\" * 10]\n-        expected_tokens = [self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=True, eos=True) for t in text]\n+        expected_tokens = [\n+            self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=True, eos=False) for t in text\n+        ]\n         for padding in [True, \"longest\", PaddingStrategy.LONGEST, \"max_length\", PaddingStrategy.MAX_LENGTH]:\n             for truncation in [True, \"longest_first\", TruncationStrategy.LONGEST_FIRST]:\n                 tokens = self.tokenizer(\n@@ -1956,3 +2097,32 @@ def test_get_vocab(self):\n             self.assertEqual(\n                 self.ref_tokenizer.decode([id_token], special_token_policy=SpecialTokenPolicy.KEEP), token\n             )\n+\n+    def test_get_validation_mode(self):\n+        for mode, expected in [\n+            (\"test\", ValidationMode.test),\n+            (ValidationMode.test, ValidationMode.test),\n+            (\"finetuning\", ValidationMode.finetuning),\n+            (ValidationMode.finetuning, ValidationMode.finetuning),\n+        ]:\n+            self.assertEqual(MistralCommonBackend._get_validation_mode(mode), expected)\n+\n+        for invalid_mode in [(\"serving\", ValidationMode.serving, \"invalid\", 1)]:\n+            with self.assertRaises(ValueError):\n+                MistralCommonBackend._get_validation_mode(invalid_mode)\n+\n+    def test_all_special_ids(self):\n+        with patch.object(self.tokenizer, \"_all_special_tokens_ids\", {1, 0}):\n+            self.assertEqual(self.tokenizer.all_special_ids, [0, 1])\n+\n+        spm_tokenizer = self._get_spm_tokenizer()\n+        with patch.object(spm_tokenizer, \"_all_special_tokens_ids\", {1, 0}):\n+            self.assertEqual(spm_tokenizer.all_special_ids, [0, 1])\n+\n+    def test_all_special_tokens(self):\n+        with patch.object(self.tokenizer, \"_all_special_tokens_ids\", {1, 0}):\n+            self.assertEqual(self.tokenizer.all_special_tokens, [\"<unk>\", \"<s>\"])\n+\n+        spm_tokenizer = self._get_spm_tokenizer()\n+        with patch.object(spm_tokenizer, \"_all_special_tokens_ids\", {1, 0}):\n+            self.assertEqual(spm_tokenizer.all_special_tokens, [\"<unk>\", \"<s>\"])"
        }
    ],
    "stats": {
        "total": 458,
        "additions": 360,
        "deletions": 98
    }
}