{
    "author": "Rocketknight1",
    "message": "Fix Prefill docs (#33352)\n\nlast -> final",
    "sha": "d7b04ea14ddfef04fc110f360e8ea81cfcd1c86a",
    "files": [
        {
            "sha": "f65dbc016e16d058f6e481dc8f06718d9f048bb4",
            "filename": "docs/source/en/chat_templating.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7b04ea14ddfef04fc110f360e8ea81cfcd1c86a/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7b04ea14ddfef04fc110f360e8ea81cfcd1c86a/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=d7b04ea14ddfef04fc110f360e8ea81cfcd1c86a",
            "patch": "@@ -196,7 +196,7 @@ Not all models require generation prompts. Some models, like LLaMA, don't have a\n special tokens before bot responses. In these cases, the `add_generation_prompt` argument will have no effect. The exact\n effect that `add_generation_prompt` has will depend on the template being used.\n \n-## What does \"continue_last_message\" do?\n+## What does \"continue_final_message\" do?\n \n When passing a list of messages to `apply_chat_template` or `TextGenerationPipeline`, you can choose\n to format the chat so the model will continue the final message in the chat instead of starting a new one. This is done\n@@ -211,15 +211,15 @@ chat = [\n     {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n ]\n \n-formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_last_message=True)\n+formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True)\n model.generate(**formatted_chat)\n ```\n \n The model will generate text that continues the JSON string, rather than starting a new message. This approach\n can be very useful for improving the accuracy of the model's instruction-following when you know how you want\n it to start its replies.\n \n-Because `add_generation_prompt` adds the tokens that start a new message, and `continue_last_message` removes any\n+Because `add_generation_prompt` adds the tokens that start a new message, and `continue_final_message` removes any\n end-of-message tokens from the final message, it does not make sense to use them together. As a result, you'll\n get an error if you try!\n \n@@ -228,7 +228,7 @@ get an error if you try!\n The default behaviour of `TextGenerationPipeline` is to set `add_generation_prompt=True` so that it starts a new\n message. However, if the final message in the input chat has the \"assistant\" role, it will assume that this message is \n a prefill and switch to `continue_final_message=True` instead, because most models do not support multiple \n-consecutive assistant messages. You can override this behaviour by explicitly passing the `continue_last_message` \n+consecutive assistant messages. You can override this behaviour by explicitly passing the `continue_final_message` \n argument when calling the pipeline.\n \n </Tip>"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}