{
    "author": "Cyrilvallez",
    "message": "[Backend support] Allow `num_logits_to_keep` as Tensor + add flag (#35757)\n\n* support\r\n\r\n* Update modeling_utils.py\r\n\r\n* style\r\n\r\n* most models\r\n\r\n* Other models\r\n\r\n* fix-copies\r\n\r\n* tests + generation utils",
    "sha": "d3af76df58476830eb5b5981decc64af15e369f5",
    "files": [
        {
            "sha": "9689ca2b5203eba2697b49184a9932e81c52a757",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -129,9 +129,9 @@ def __init__(\n                     value.detach().to(device) if isinstance(value, torch.Tensor) else copy.deepcopy(value)\n                 )\n \n-        # Remove potential default \"num_logits_to_keep\" key\n-        if \"num_logits_to_keep\" in assistant_kwargs.keys() and not assistant_model._supports_num_logits_to_keep():\n-            del assistant_kwargs[\"num_logits_to_keep\"]\n+        # Remove potential default \"logits_to_keep\" key\n+        if \"logits_to_keep\" in assistant_kwargs.keys() and not assistant_model._supports_logits_to_keep():\n+            del assistant_kwargs[\"logits_to_keep\"]\n \n         if \"assistant_encoder_outputs\" in model_kwargs:\n             assistant_kwargs[\"encoder_outputs\"] = model_kwargs[\"assistant_encoder_outputs\"]"
        },
        {
            "sha": "94230d1b72fb01ce6f7a346020ddebae89a00651",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -1780,12 +1780,12 @@ def _prepare_cache_for_generation(\n                 else EncoderDecoderCache(DynamicCache(), DynamicCache())\n             )\n \n-    def _supports_num_logits_to_keep(self) -> bool:\n+    def _supports_logits_to_keep(self) -> bool:\n         \"\"\"\n-        Return True if the current model supports the keyword argument `num_logits_to_keep` in forward()\n+        Return True if the current model supports the keyword argument `logits_to_keep` in forward()\n         to save memory. Checking it in this way allows to avoid using a new model attribute.\n         \"\"\"\n-        return \"num_logits_to_keep\" in set(inspect.signature(self.forward).parameters.keys())\n+        return \"logits_to_keep\" in set(inspect.signature(self.forward).parameters.keys())\n \n     def _prepare_special_tokens(\n         self,\n@@ -2066,11 +2066,11 @@ def generate(\n             input_ids_length=input_ids_length,\n         )\n \n-        # If the model supports `num_logits_to_keep` in forward(), set it to 1 to avoid computing the whole\n+        # If the model supports `logits_to_keep` in forward(), set it to 1 to avoid computing the whole\n         # logit matrix. This can save a lot of memory during the first forward pass. Note that assisted decoding\n         # dynamically overrides this value as it can need more than the last token logits\n-        if self._supports_num_logits_to_keep() and \"num_logits_to_keep\" not in model_kwargs:\n-            model_kwargs[\"num_logits_to_keep\"] = 1\n+        if self._supports_logits_to_keep() and \"logits_to_keep\" not in model_kwargs:\n+            model_kwargs[\"logits_to_keep\"] = 1\n \n         self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n \n@@ -4236,8 +4236,8 @@ def _assisted_decoding(\n                 )\n \n             model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n-            if \"num_logits_to_keep\" in model_inputs:\n-                model_inputs[\"num_logits_to_keep\"] = candidate_length + 1\n+            if \"logits_to_keep\" in model_inputs:\n+                model_inputs[\"logits_to_keep\"] = candidate_length + 1\n \n             # 2.2. Run a forward pass on the candidate sequence\n             # prepare variable output controls (note: some models won't accept all output controls)\n@@ -4575,7 +4575,7 @@ def _split_model_inputs(\n     # ModelOutput object.\n     # bool should not be split but replicated for each split\n     bool_keys = [k for k in keys if isinstance(model_input[k], bool) or k == \"cache_position\"]\n-    keys_to_ignore = [\"cache_position\", \"encoder_outputs\", \"num_logits_to_keep\"]\n+    keys_to_ignore = [\"cache_position\", \"encoder_outputs\", \"logits_to_keep\"]\n     non_bool_keys = [k for k in keys if not isinstance(model_input[k], bool) and k not in keys_to_ignore]\n \n     num_hidden_layers = config.get_text_config().num_hidden_layers\n@@ -4595,10 +4595,10 @@ def _split_model_inputs(\n         data_split_list = [\n             {**data_split, \"encoder_outputs\": encoder_outputs_split[i]} for i, data_split in enumerate(data_split_list)\n         ]\n-    # num_logits_to_keep should be replicated for each split, similar to bool values\n-    if \"num_logits_to_keep\" in model_input:\n+    # logits_to_keep should be replicated for each split, similar to bool values\n+    if \"logits_to_keep\" in model_input:\n         data_split_list = [\n-            {**data_split, \"num_logits_to_keep\": model_input[\"num_logits_to_keep\"]} for data_split in data_split_list\n+            {**data_split, \"logits_to_keep\": model_input[\"logits_to_keep\"]} for data_split in data_split_list\n         ]\n \n     # Convert each dictionary in the list to an object of the inferred class"
        },
        {
            "sha": "d869229af9c8a5ede7470b0996437b14f9338e4e",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -1292,6 +1292,11 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix\n     # `config.base_model_tp_plan` during `post_init`.\n     _tp_plan = None\n \n+    # This flag signal that the model can be used as an efficient backend in TGI and vLLM\n+    # In practice, it means that they support attention interface functions, fully pass the kwargs\n+    # through all modules up to the Attention layer, and can slice logits with Tensor\n+    _supports_attention_backend = False\n+\n     @property\n     def dummy_inputs(self) -> Dict[str, torch.Tensor]:\n         \"\"\"\n@@ -5188,6 +5193,10 @@ def get_compiled_call(self, compile_config: CompileConfig):\n             self._compiled_call = torch.compile(self.__call__, **compile_config.to_dict())\n         return self._compiled_call\n \n+    @classmethod\n+    def is_backend_compatible(cls):\n+        return cls._supports_attention_backend\n+\n \n PreTrainedModel.push_to_hub = copy_func(PreTrainedModel.push_to_hub)\n if PreTrainedModel.push_to_hub.__doc__ is not None:"
        },
        {
            "sha": "41430167355235a9d3cb5885dc2c3ac0caa8e778",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 18,
            "deletions": 10,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -37,6 +37,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_torch_available\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_aria import AriaConfig, AriaTextConfig\n@@ -708,6 +709,7 @@ class AriaPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = False\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -1168,6 +1170,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(ARIA_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1183,7 +1186,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1193,10 +1196,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1239,7 +1244,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n@@ -1324,8 +1330,9 @@ class AriaCausalLMOutputWithPast(ModelOutput):\n             Whether to output hidden states.\n         return_dict (`bool`, *optional*):\n             Whether to return a `ModelOutput` object.\n-        num_logits_to_keep (`int`, *optional*, defaults to 0):\n-            Calculate logits for the last `num_logits_to_keep` tokens, or all `input_ids` if `0`.\n+        logits_to_keep (`int` or `torch.Tensor`, *optional*, defaults to 0):\n+            If an `int`, calculate logits for the last `logits_to_keep` tokens, or all `input_ids` if `0`.\n+            Otherwise, slice according to the 1D tensor in the sequence length dimension\n         cache_position (`torch.LongTensor`, *optional*):\n             Cache positions.\n         **loss_kwargs:\n@@ -1426,6 +1433,7 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature, attn_mask=image_attn_mask)\n         return image_features\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=AriaConfig)\n     def forward(\n@@ -1442,7 +1450,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         cache_position: Optional[torch.LongTensor] = None,\n         **loss_kwargs,\n     ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n@@ -1552,7 +1560,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -1584,7 +1592,7 @@ def prepare_inputs_for_generation(\n         pixel_mask=None,\n         attention_mask=None,\n         cache_position=None,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         model_inputs = self.language_model.prepare_inputs_for_generation(\n@@ -1593,7 +1601,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "5c40473a18f7edb3fccc336c18c8730603d97739",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -45,6 +45,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_torch_available\n from ..auto import CONFIG_MAPPING, AutoConfig, AutoModel, AutoModelForCausalLM, AutoTokenizer\n from ..llama.configuration_llama import LlamaConfig\n@@ -1222,6 +1223,8 @@ def _init_weights(self, module):\n \n \n class AriaPreTrainedModel(LlamaPreTrainedModel):\n+    _supports_attention_backend = False\n+\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n@@ -1301,8 +1304,9 @@ class AriaCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n             Whether to output hidden states.\n         return_dict (`bool`, *optional*):\n             Whether to return a `ModelOutput` object.\n-        num_logits_to_keep (`int`, *optional*, defaults to 0):\n-            Calculate logits for the last `num_logits_to_keep` tokens, or all `input_ids` if `0`.\n+        logits_to_keep (`int` or `torch.Tensor`, *optional*, defaults to 0):\n+            If an `int`, calculate logits for the last `logits_to_keep` tokens, or all `input_ids` if `0`.\n+            Otherwise, slice according to the 1D tensor in the sequence length dimension\n         cache_position (`torch.LongTensor`, *optional*):\n             Cache positions.\n         **loss_kwargs:\n@@ -1403,6 +1407,7 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature, attn_mask=image_attn_mask)\n         return image_features\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=AriaConfig)\n     def forward(\n@@ -1419,7 +1424,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         cache_position: Optional[torch.LongTensor] = None,\n         **loss_kwargs,\n     ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n@@ -1529,7 +1534,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -1561,7 +1566,7 @@ def prepare_inputs_for_generation(\n         pixel_mask=None,\n         attention_mask=None,\n         cache_position=None,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         model_inputs = self.language_model.prepare_inputs_for_generation(\n@@ -1570,7 +1575,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "edfc162a032a5d9426ee3b38f35e74bea54b54ae",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 12,
            "deletions": 7,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n     is_mamba_2_ssm_available,\n@@ -1466,6 +1467,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1481,7 +1483,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1491,10 +1493,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int` or `None`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `None`, calculate logits for all\n-                `input_ids`. Only last token logits are needed for generation, and calculating them only for that token\n-                can save memory, which becomes pretty significant for long sequences.\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1537,7 +1541,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n@@ -1602,7 +1607,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": self.config.num_logits_to_keep,\n+                \"logits_to_keep\": self.config.num_logits_to_keep,\n                 \"cache_position\": cache_position,\n             }\n         )"
        },
        {
            "sha": "93fb274e4d4d6bf3f66304d686d7437ca3a65c9c",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -54,6 +54,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n     is_flash_attn_2_available,\n@@ -1182,6 +1183,7 @@ def _update_mamba_mask(self, attention_mask, cache_position):\n \n \n class BambaForCausalLM(LlamaForCausalLM):\n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1197,7 +1199,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1207,10 +1209,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int` or `None`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `None`, calculate logits for all\n-                `input_ids`. Only last token logits are needed for generation, and calculating them only for that token\n-                can save memory, which becomes pretty significant for long sequences.\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1242,7 +1246,7 @@ def forward(\n             output_hidden_states,\n             return_dict,\n             cache_position,\n-            num_logits_to_keep,\n+            logits_to_keep,\n             **kwargs,\n         )\n \n@@ -1293,7 +1297,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": self.config.num_logits_to_keep,\n+                \"logits_to_keep\": self.config.num_logits_to_keep,\n                 \"cache_position\": cache_position,\n             }\n         )"
        },
        {
            "sha": "7337ae6acf4916cecdbc12c92bb82d3caede701d",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -48,6 +48,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_cohere import CohereConfig\n \n \n@@ -421,6 +422,7 @@ class CoherePreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -808,6 +810,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(COHERE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -823,7 +826,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -833,10 +836,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -879,7 +884,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n         logits = logits * self.logit_scale  # main diff from Llama\n \n         loss = None"
        },
        {
            "sha": "17eb3f6a343410bea8c42507c74472e692991602",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -317,7 +317,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -327,10 +327,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -373,7 +375,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n         logits = logits * self.logit_scale  # main diff from Llama\n \n         loss = None"
        },
        {
            "sha": "2353601d91f2928c9923bf92fc639ce87ced8a12",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 7,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -39,6 +39,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_cohere2 import Cohere2Config\n \n \n@@ -421,6 +422,7 @@ class Cohere2PreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -779,6 +781,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(COHERE2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -794,7 +797,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -804,10 +807,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -850,7 +855,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n         logits = logits * self.logit_scale  # main diff from Llama\n \n         loss = None\n@@ -878,7 +884,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten: has a special cache type, `HybridCache`\n@@ -933,8 +939,8 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+        if logits_to_keep is not None:\n+            model_inputs[\"logits_to_keep\"] = logits_to_keep\n \n         model_inputs.update(\n             {"
        },
        {
            "sha": "1e3295fce30751b0ce00b97c6b0e480b86013820",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -544,7 +544,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten: has a special cache type, `HybridCache`\n@@ -599,8 +599,8 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+        if logits_to_keep is not None:\n+            model_inputs[\"logits_to_keep\"] = logits_to_keep\n \n         model_inputs.update(\n             {"
        },
        {
            "sha": "5ad827689b41344a9f4e056f11fa3165471cb483",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -35,6 +35,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_dbrx import DbrxConfig\n \n \n@@ -1257,6 +1258,7 @@ def set_decoder(self, decoder: DbrxModel):\n     def get_decoder(self) -> DbrxModel:\n         return self.transformer\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(DBRX_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1273,7 +1275,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"Forward function for causal language modeling.\n \n@@ -1283,10 +1285,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1333,7 +1337,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # No upscaling to float was ever done for Dbrx\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "c262340aacf9aa38d708e4163b49aa298eee30cb",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -51,6 +51,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_diffllama import DiffLlamaConfig\n \n \n@@ -599,6 +600,7 @@ class DiffLlamaPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = False\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -1045,6 +1047,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(DIFFLLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1060,7 +1063,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1070,10 +1073,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1116,7 +1121,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "c6bdf18093d40037215c6d21187dc0197870b70c",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -432,6 +432,7 @@ def __init__(self, config: DiffLlamaConfig, layer_idx: int):\n \n class DiffLlamaPreTrainedModel(LlamaPreTrainedModel):\n     _supports_flex_attn = False\n+    _supports_attention_backend = False\n \n \n class DiffLlamaModel(LlamaModel):"
        },
        {
            "sha": "6944f91b97580115680d6a26b5038949d3f7f07f",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 17,
            "deletions": 8,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -44,6 +44,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_emu3 import Emu3Config, Emu3TextConfig, Emu3VQVAEConfig\n \n \n@@ -1626,6 +1627,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(EMU3_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"Emu3TextConfig\")\n     def forward(\n@@ -1641,7 +1643,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1650,10 +1652,13 @@ def forward(\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1696,7 +1701,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n@@ -1865,18 +1871,21 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1949,7 +1958,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n         )\n \n         return outputs"
        },
        {
            "sha": "01d09b703d8e3bb941ea72d50ddc3d113bced5f0",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 14,
            "deletions": 6,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -36,6 +36,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..chameleon.modeling_chameleon import (\n     ChameleonPreTrainedModel,\n     ChameleonVQVAEEncoderConvDownsample,\n@@ -1071,6 +1072,7 @@ def __init__(self, config):\n         super().__init__(config)\n         self.model = Emu3TextModel(config)\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(EMU3_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"Emu3TextConfig\")\n     def forward(**super_kwargs):\n@@ -1080,10 +1082,13 @@ def forward(**super_kwargs):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1177,18 +1182,21 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1261,7 +1269,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n         )\n \n         return outputs"
        },
        {
            "sha": "f499801d217048bab89b16434da99397d5b69760",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -46,6 +46,7 @@\n     is_flash_attn_greater_or_equal_2_10,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_falcon import FalconConfig\n \n \n@@ -1176,6 +1177,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings: torch.Tensor):\n         self.lm_head = new_embeddings\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(FALCON_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1196,18 +1198,20 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n \n-        num_logits_to_keep (`int`, *optional*):\n-            Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+        logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+            If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n             `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n             token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+            If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+            This is useful when using packed tensor format (single dimension for batch and sequence length).\n         \"\"\"\n \n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n@@ -1227,7 +1231,8 @@ def forward(\n         )\n         hidden_states = transformer_outputs[0]\n \n-        lm_logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        lm_logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "caaf2c60f519b37023424b6a32057beba3e5de6e",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -46,6 +46,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_gemma import GemmaConfig\n \n \n@@ -387,6 +388,7 @@ class GemmaPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -777,6 +779,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GEMMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -792,7 +795,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -802,10 +805,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -848,7 +853,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "9c015d37c2f4c1f5c0309780340415fd80f359d7",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -474,10 +474,12 @@ def forward(**super_kwargs):\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n "
        },
        {
            "sha": "9bc0d278166b012f497cc2d1968cf998afca7e58",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 7,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -44,6 +44,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_gemma2 import Gemma2Config\n \n \n@@ -417,6 +418,7 @@ class Gemma2PreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -781,6 +783,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GEMMA2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -796,7 +799,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -806,10 +809,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -856,7 +861,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n         if self.config.final_logit_softcapping is not None:\n             logits = logits / self.config.final_logit_softcapping\n             logits = torch.tanh(logits)\n@@ -887,7 +893,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten: has a special cache type, `HybridCache`\n@@ -942,8 +948,8 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+        if logits_to_keep is not None:\n+            model_inputs[\"logits_to_keep\"] = logits_to_keep\n \n         model_inputs.update(\n             {"
        },
        {
            "sha": "ffd75fa2f06482a7e1a1961140390adca7426c58",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -539,7 +539,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -584,7 +584,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n         if self.config.final_logit_softcapping is not None:\n             logits = logits / self.config.final_logit_softcapping\n             logits = torch.tanh(logits)\n@@ -615,7 +616,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten: has a special cache type, `HybridCache`\n@@ -670,8 +671,8 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+        if logits_to_keep is not None:\n+            model_inputs[\"logits_to_keep\"] = logits_to_keep\n \n         model_inputs.update(\n             {"
        },
        {
            "sha": "a3461ffd71cb112df45b96cafb2bb564952b0d15",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -46,6 +46,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_glm import GlmConfig\n \n \n@@ -402,6 +403,7 @@ class GlmPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -787,6 +789,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GLM_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -802,7 +805,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -812,10 +815,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -858,7 +863,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "4549cdd5d70b4058a5cb2aa527390af8e9e9e7fa",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -40,6 +40,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_granite import GraniteConfig\n \n \n@@ -402,6 +403,7 @@ class GranitePreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -790,6 +792,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GRANITE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -805,7 +808,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -815,10 +818,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -861,7 +866,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n         logits = logits / self.config.logits_scaling  # main diff with Llama\n \n         loss = None"
        },
        {
            "sha": "f23ae4a673c3e7dbb05aa819973106395976906d",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -245,7 +245,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -271,7 +271,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n         logits = logits / self.config.logits_scaling  # main diff with Llama\n \n         loss = None"
        },
        {
            "sha": "71518c4a9aa85457db158f9f10b5888b7f7fead1",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -47,6 +47,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_helium import HeliumConfig\n \n \n@@ -389,6 +390,7 @@ class HeliumPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -774,6 +776,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(HELIUM_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -789,7 +792,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -799,10 +802,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -845,7 +850,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "3aaf46d63df2a6d0001a03d6f69eb8d4def978ad",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 7,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -37,6 +37,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel\n from .configuration_idefics2 import Idefics2Config, Idefics2PerceiverConfig, Idefics2VisionConfig\n \n@@ -1508,6 +1509,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(IDEFICS2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Idefics2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1525,7 +1527,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, Idefics2CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1535,10 +1537,12 @@ def forward(\n                 Tokens with indices set to `model.image_token_id` are ignored (masked), the loss is only\n                 computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1604,7 +1608,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n@@ -1648,7 +1653,7 @@ def prepare_inputs_for_generation(\n         pixel_values=None,\n         pixel_attention_mask=None,\n         image_hidden_states=None,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten -- there are mutually exclusive inputs (if the logic to make `image_hidden_states` take\n@@ -1677,8 +1682,8 @@ def prepare_inputs_for_generation(\n             # The clone here is for the same reason as for `position_ids`.\n             model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n \n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+        if logits_to_keep is not None:\n+            model_inputs[\"logits_to_keep\"] = logits_to_keep\n \n         if image_hidden_states is not None:\n             pixel_values = None"
        },
        {
            "sha": "e4cc8bda569fadd3d6cc6f78d15968340c1edd96",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -1242,7 +1242,7 @@ def prepare_inputs_for_generation(\n         pixel_values=None,\n         pixel_attention_mask=None,\n         image_hidden_states=None,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten -- there are mutually exclusive inputs (if the logic to make `image_hidden_states` take\n@@ -1271,8 +1271,8 @@ def prepare_inputs_for_generation(\n             # The clone here is for the same reason as for `position_ids`.\n             model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n \n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+        if logits_to_keep is not None:\n+            model_inputs[\"logits_to_keep\"] = logits_to_keep\n \n         if image_hidden_states is not None:\n             pixel_values = None"
        },
        {
            "sha": "24aeb9890b9fed9bb0994b4f4d2b5126ae870f1c",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -45,6 +45,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n     is_flash_attn_2_available,\n@@ -1433,9 +1434,9 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(JAMBA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n-    # Ignore copy\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1450,7 +1451,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: Optional[Union[int, None]] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1460,10 +1461,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int` or `None`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `None`, calculate logits for all\n-                `input_ids`. Only last token logits are needed for generation, and calculating them only for that token\n-                can save memory, which becomes pretty significant for long sequences.\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1510,10 +1513,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if num_logits_to_keep is None:\n-            logits = self.lm_head(hidden_states)\n-        else:\n-            logits = self.lm_head(hidden_states[..., -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n@@ -1595,7 +1596,7 @@ def prepare_inputs_for_generation(\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n                 \"output_router_logits\": output_router_logits,\n-                \"num_logits_to_keep\": self.config.num_logits_to_keep,\n+                \"logits_to_keep\": self.config.num_logits_to_keep,\n                 \"cache_position\": cache_position,\n             }\n         )"
        },
        {
            "sha": "fca47eb3fa0da3d6b7fb089634efb37c29e64c87",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -42,6 +42,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_jetmoe import JetMoeConfig\n \n \n@@ -1274,6 +1275,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(JETMOE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1290,7 +1292,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1299,10 +1301,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n         \"\"\"\n@@ -1329,7 +1333,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "361ae15c31277ef6a5369aed98bc3498ac5ed0ad",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -47,6 +47,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_llama import LlamaConfig\n \n \n@@ -391,6 +392,7 @@ class LlamaPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -776,6 +778,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -791,7 +794,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -801,10 +804,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -847,7 +852,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "fcf016f28f81ba39686b5a304a9e1aa07d581b0e",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -31,6 +31,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_llava import LlavaConfig\n \n@@ -380,6 +381,7 @@ def _merge_input_ids_with_image_features(self, image_features, inputs_embeds, in\n \n         return final_embedding, final_attention_mask, final_labels, position_ids\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(LLAVA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=LlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -398,7 +400,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -407,10 +409,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n \n         Returns:\n@@ -490,7 +494,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -534,7 +538,7 @@ def prepare_inputs_for_generation(\n         pixel_values=None,\n         attention_mask=None,\n         cache_position=None,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -545,7 +549,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "8bff9dc90061e6a3c32d4495de0f544bb10b8109",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -34,6 +34,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_llava_next import LlavaNextConfig\n \n@@ -752,6 +753,7 @@ def get_image_features(\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n         return image_features\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(LLAVA_NEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=LlavaNextCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -771,7 +773,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, LlavaNextCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -780,10 +782,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -871,7 +875,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -916,7 +920,7 @@ def prepare_inputs_for_generation(\n         image_sizes=None,\n         attention_mask=None,\n         cache_position=None,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -927,7 +931,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "c82d52bfdaa9d884efc01984239c079098760ec9",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -33,6 +33,7 @@\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_llava_next_video import LlavaNextVideoConfig\n \n@@ -787,6 +788,7 @@ def get_image_features(\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n         return image_features\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(LLAVA_NEXT_VIDEO_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=LlavaNextVideoCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -807,7 +809,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -819,10 +821,12 @@ def forward(\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -967,7 +971,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -1014,7 +1018,7 @@ def prepare_inputs_for_generation(\n         image_sizes=None,\n         attention_mask=None,\n         cache_position=None,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten -- extra custom processing\n@@ -1025,7 +1029,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "580f890b42668b7f056e0d786a2e8a9685b1b5a9",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -335,7 +335,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -347,10 +347,12 @@ def forward(\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -495,7 +497,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -542,7 +544,7 @@ def prepare_inputs_for_generation(\n         image_sizes=None,\n         attention_mask=None,\n         cache_position=None,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten -- extra custom processing\n@@ -553,7 +555,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "f1cf7a6c2dcadf753df81ec2e4b9307908f7f9a3",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -32,6 +32,7 @@\n     add_start_docstrings,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_llava_onevision import LlavaOnevisionConfig\n \n@@ -568,6 +569,7 @@ def get_video_features(\n \n         return video_features\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings(LLAVA_ONEVISION_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -589,7 +591,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, LlavaOnevisionCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -598,10 +600,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n \n         Returns:\n@@ -734,7 +738,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -782,7 +786,7 @@ def prepare_inputs_for_generation(\n         image_sizes_videos=None,\n         attention_mask=None,\n         cache_position=None,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -793,7 +797,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "cc62d378ebaef7c41a4a703beb01ae88434bc927",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -32,6 +32,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_mistral import MistralConfig\n \n \n@@ -363,6 +364,7 @@ class MistralPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -777,6 +779,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -792,7 +795,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -802,10 +805,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -848,7 +853,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "034ddba8c48441792811bc9d40ef49779710438f",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -55,6 +55,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_mixtral import MixtralConfig\n \n \n@@ -485,6 +486,7 @@ class MixtralPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -996,6 +998,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(MIXTRAL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1012,7 +1015,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1022,10 +1025,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1074,7 +1079,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "a16e4c5a16d9e417f847442b168cbb8e3b1a535b",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -466,7 +466,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -476,10 +476,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -528,7 +530,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "d1f83e13d8d479f4ac7d75503d5d276bff5779bb",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 19,
            "deletions": 11,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -35,6 +35,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_mllama import MllamaConfig, MllamaTextConfig, MllamaVisionConfig\n \n \n@@ -1872,6 +1873,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(MLLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"MllamaTextConfig\")\n     def forward(\n@@ -1890,7 +1892,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1900,10 +1902,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1950,7 +1954,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :]).float()\n \n         loss = None\n         if labels is not None:\n@@ -2014,6 +2019,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(MLLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"MllamaConfig\")\n     def forward(\n@@ -2034,7 +2040,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -2043,10 +2049,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n \n         Returns:\n@@ -2140,7 +2148,7 @@ def forward(\n             output_attentions=output_attentions,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n         )\n \n         return outputs\n@@ -2158,7 +2166,7 @@ def prepare_inputs_for_generation(\n         past_key_values=None,\n         use_cache=False,\n         cache_position=None,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -2190,8 +2198,8 @@ def prepare_inputs_for_generation(\n             # The clone here is for the same reason as for `position_ids`.\n             model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n \n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+        if logits_to_keep is not None:\n+            model_inputs[\"logits_to_keep\"] = logits_to_keep\n \n         model_inputs.update(\n             {"
        },
        {
            "sha": "3796e2dc5f3525cda1fdd7ffe58f370bdc9cda07",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -48,6 +48,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto.modeling_auto import AutoModel\n from .configuration_moshi import MoshiConfig, MoshiDepthConfig\n \n@@ -1788,6 +1789,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(MOSHI_DECODER_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoshiCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1803,7 +1805,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, MoshiCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1812,10 +1814,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1861,7 +1865,8 @@ def forward(\n                 \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n             )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n@@ -2446,7 +2451,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         user_delay_pattern_mask=None,\n         moshi_delay_pattern_mask=None,\n         kwargs_depth_decoder=None,\n@@ -2463,7 +2468,7 @@ def prepare_inputs_for_generation(\n             cache_position=cache_position,\n             position_ids=position_ids,\n             use_cache=use_cache,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "8ae6e9c77fac5f1b2fdb176b9ecd88a3fe59813e",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -46,6 +46,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_nemotron import NemotronConfig\n \n \n@@ -1023,6 +1024,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(NEMOTRON_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     # Ignore copy (doc string different)\n@@ -1039,7 +1041,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1049,10 +1051,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1094,7 +1098,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "c2e1ae15b4b5f6e16b5d488a7f4b1cc411c7cb40",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -26,6 +26,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_olmo import OlmoConfig\n \n \n@@ -367,6 +368,7 @@ class OlmoPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -752,6 +754,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(OLMO_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -767,7 +770,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -777,10 +780,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -823,7 +828,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "163956d61a22983b3b44bfa0e650ea2bf894c326",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -25,6 +25,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_olmo2 import Olmo2Config\n \n \n@@ -368,6 +369,7 @@ class Olmo2PreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -753,6 +755,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(OLMO2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -768,7 +771,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -778,10 +781,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -824,7 +829,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "47126da9564778923b3575b0dad53ec258f4ef82",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -38,6 +38,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_olmoe import OlmoeConfig\n \n \n@@ -756,7 +757,6 @@ def forward(\n     \"The bare Olmoe Model outputting raw hidden-states without any specific head on top.\",\n     OLMOE_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel with Llama->Olmoe\n class OlmoePreTrainedModel(PreTrainedModel):\n     config_class = OlmoeConfig\n     base_model_prefix = \"model\"\n@@ -765,7 +765,6 @@ class OlmoePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n@@ -1186,6 +1185,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(OLMOE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1202,7 +1202,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1212,10 +1212,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1262,7 +1264,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "5889f92f3c0de55e523c82b9d977c912ca4ae2b0",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -32,6 +32,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_paligemma import PaliGemmaConfig\n \n \n@@ -412,6 +413,7 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n         image_features = image_features / (self.config.text_config.hidden_size**0.5)\n         return image_features\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PALIGEMMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=PaliGemmaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -429,7 +431,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, PaliGemmaCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -438,10 +440,12 @@ def forward(\n                 config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -532,7 +536,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n         )\n \n         logits = outputs.logits\n@@ -581,7 +585,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         token_type_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         labels=None,\n         **kwargs,\n     ):\n@@ -594,7 +598,7 @@ def prepare_inputs_for_generation(\n             position_ids=position_ids,\n             cache_position=cache_position,\n             use_cache=use_cache,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             token_type_ids=token_type_ids,\n             **kwargs,\n         )"
        },
        {
            "sha": "d1cb49529428e3f5abd55dd1ca22bab9e4608e93",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -46,6 +46,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_persimmon import PersimmonConfig\n \n \n@@ -830,6 +831,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PERSIMMON_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -845,7 +847,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -854,10 +856,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -900,7 +904,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # No upscaling to float was ever done for Persimmon\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "7d360b1ed41e72b21d9f4d834c84d73d6d4aff1e",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -31,6 +31,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_phi import PhiConfig\n \n \n@@ -363,6 +364,7 @@ class PhiPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -750,6 +752,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PHI_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -765,7 +768,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -775,10 +778,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -821,7 +826,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "e86e028b402715d5845cb047c6902eece0461c81",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -47,6 +47,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_phi3 import Phi3Config\n \n \n@@ -432,6 +433,7 @@ class Phi3PreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n     _version = \"0.0.5\"\n \n     def _init_weights(self, module):\n@@ -847,6 +849,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PHI3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -862,7 +865,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -872,10 +875,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -918,7 +923,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n@@ -945,7 +951,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten -- this model may need to switch between short and long rope, invalidating the cache in the\n@@ -970,7 +976,7 @@ def prepare_inputs_for_generation(\n             cache_position=cache_position,\n             position_ids=position_ids,\n             use_cache=use_cache,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n         return model_inputs"
        },
        {
            "sha": "27f7c42f5bb8823f457346e5cd498ec72569c5ff",
            "filename": "src/transformers/models/phi3/modular_phi3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -275,7 +275,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten -- this model may need to switch between short and long rope, invalidating the cache in the\n@@ -300,7 +300,7 @@ def prepare_inputs_for_generation(\n             cache_position=cache_position,\n             position_ids=position_ids,\n             use_cache=use_cache,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n         return model_inputs"
        },
        {
            "sha": "ba4b766507301c6e2254b976ebb2b235e9483c87",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -40,6 +40,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_torch_fx_available\n from .configuration_phimoe import PhimoeConfig\n \n@@ -901,7 +902,6 @@ def forward(\n     \"The bare Phimoe Model outputting raw hidden-states without any specific head on top.\",\n     PHIMOE_START_DOCSTRING,\n )\n-# Copied from transformers.models.mixtral.modeling_mixtral.MixtralPreTrainedModel with Mixtral->Phimoe\n class PhimoePreTrainedModel(PreTrainedModel):\n     config_class = PhimoeConfig\n     base_model_prefix = \"model\"\n@@ -910,7 +910,6 @@ class PhimoePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n@@ -1365,6 +1364,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PHIMOE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     # Ignore copy\n@@ -1382,7 +1382,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1392,10 +1392,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n         Returns:\n         Example:\n         ```python\n@@ -1445,7 +1447,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n@@ -1488,7 +1491,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten -- this model may need to switch between short and long rope, invalidating the cache in the\n@@ -1513,7 +1516,7 @@ def prepare_inputs_for_generation(\n             cache_position=cache_position,\n             position_ids=position_ids,\n             use_cache=use_cache,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n         return model_inputs"
        },
        {
            "sha": "96cd6a6aa32ebc5ab0ec9e8ce409d3d0b7d4c3b3",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -32,6 +32,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_qwen2 import Qwen2Config\n \n \n@@ -376,6 +377,7 @@ class Qwen2PreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -761,6 +763,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -776,7 +779,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -786,10 +789,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -832,7 +837,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "ad61003c8602b0d93c00552fac1dbf0d7ed36a8e",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -49,6 +49,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_qwen2_moe import Qwen2MoeConfig\n \n \n@@ -1247,6 +1248,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(QWEN2MOE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1263,7 +1265,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1273,10 +1275,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1323,7 +1327,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "55a85a9a1fa21ff64ef18edf184e6658c1bbaee0",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -48,6 +48,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_stablelm import StableLmConfig\n \n \n@@ -1086,6 +1087,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(STABLELM_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     # Ignore copy\n@@ -1102,7 +1104,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1111,10 +1113,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1156,7 +1160,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # No upscaling to float was ever done for StableLm\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "57898bc8d61687452f3a3ff310c811c3fb36babd",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -51,6 +51,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_starcoder2 import Starcoder2Config\n \n \n@@ -368,6 +369,7 @@ class Starcoder2PreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -773,6 +775,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(STARCODER2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -788,7 +791,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -798,10 +801,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -844,7 +849,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "f592da818549d3d43e468f670211b8ba689697e0",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -31,6 +31,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_video_llava import VideoLlavaConfig\n \n@@ -409,6 +410,7 @@ def get_video_features(self, pixel_values_videos: torch.FloatTensor, vision_feat\n \n         return video_features, num_frames\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(VIDEO_LLAVA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=VideoLlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -428,7 +430,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, VideoLlavaCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -437,10 +439,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -579,7 +583,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -625,7 +629,7 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         attention_mask=None,\n         cache_position=None,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -636,7 +640,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "8ef881b771cbc714a8ead003de781caa604fe2f7",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -31,6 +31,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_vipllava import VipLlavaConfig\n \n@@ -373,6 +374,7 @@ def _merge_input_ids_with_image_features(self, image_features, inputs_embeds, in\n \n         return final_embedding, final_attention_mask, final_labels, position_ids\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(VIPLLAVA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=VipLlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     # Ignore copy\n@@ -391,7 +393,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, VipLlavaCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -400,10 +402,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n \n         Returns:\n@@ -479,7 +483,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n         )\n \n         logits = outputs[0]\n@@ -521,7 +525,7 @@ def prepare_inputs_for_generation(\n         pixel_values=None,\n         attention_mask=None,\n         cache_position=None,\n-        num_logits_to_keep=None,\n+        logits_to_keep=None,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -532,7 +536,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n-            num_logits_to_keep=num_logits_to_keep,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "a25cfbc42862b7f39193c8d83e0da02860949caa",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 12,
            "deletions": 7,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -48,6 +48,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n     is_mamba_ssm_available,\n@@ -1217,6 +1218,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(ZAMBA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1232,7 +1234,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1242,10 +1244,12 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-            num_logits_to_keep (`int` or `None`, *optional*):\n-                Calculate logits for the last `num_logits_to_keep` tokens. If `None`, calculate logits for all\n-                `input_ids`. Only last token logits are needed for generation, and calculating them only for that token\n-                can save memory, which becomes pretty significant for long sequences.\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n \n         Returns:\n \n@@ -1289,7 +1293,8 @@ def forward(\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n@@ -1355,7 +1360,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": self.config.num_logits_to_keep,\n+                \"logits_to_keep\": self.config.num_logits_to_keep,\n                 \"cache_position\": cache_position,\n             }\n         )"
        },
        {
            "sha": "064decb14dba8f7a0a9eb860b450823c425ca43a",
            "filename": "src/transformers/utils/deprecation.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Futils%2Fdeprecation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/src%2Ftransformers%2Futils%2Fdeprecation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdeprecation.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -19,7 +19,12 @@\n import packaging.version\n \n from .. import __version__\n-from . import ExplicitEnum\n+from . import ExplicitEnum, is_torch_available, is_torchdynamo_compiling\n+\n+\n+# This is needed in case we deprecate a kwarg of a function/method being compiled\n+if is_torch_available():\n+    import torch  # noqa: F401\n \n \n class Action(ExplicitEnum):\n@@ -40,6 +45,7 @@ def deprecate_kwarg(\n ):\n     \"\"\"\n     Function or method decorator to notify users about deprecated keyword arguments, replacing them with a new name if specified.\n+    Note that is decorator is `torch.compile`-safe, i.e. it will not cause graph breaks (but no warning will be displayed if compiling).\n \n     This decorator allows you to:\n     - Notify users when a keyword argument is deprecated.\n@@ -158,7 +164,8 @@ def wrapped_func(*args, **kwargs):\n             # raise error or notify user\n             if minimum_action == Action.RAISE:\n                 raise ValueError(message)\n-            elif minimum_action in (Action.NOTIFY, Action.NOTIFY_ALWAYS):\n+            # If we are compiling, we do not raise the warning as it would break compilation\n+            elif minimum_action in (Action.NOTIFY, Action.NOTIFY_ALWAYS) and not is_torchdynamo_compiling():\n                 # DeprecationWarning is ignored by default, so we use FutureWarning instead\n                 warnings.warn(message, FutureWarning, stacklevel=2)\n "
        },
        {
            "sha": "b47566354b44b461894bf2bd5006cbadc09e7b98",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -2029,10 +2029,10 @@ def test_generate_compile_model_forward(self):\n                 self._check_similar_generate_outputs(dynamic_result, compiled_result)\n \n     @pytest.mark.generate\n-    def test_generate_methods_with_num_logits_to_keep(self):\n+    def test_generate_methods_with_logits_to_keep(self):\n         for model_class in self.all_generative_model_classes:\n-            if \"num_logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n-                self.skipTest(reason=\"This model does not support `num_logits_to_keep` argument.\")\n+            if \"logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n+                self.skipTest(reason=\"This model does not support `logits_to_keep` argument.\")\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n             config.use_cache = True\n@@ -2047,17 +2047,17 @@ def test_generate_methods_with_num_logits_to_keep(self):\n                 \"do_sample\": False,\n             }\n \n-            # Setting num_logits_to_keep at 0 keeps all logits (old behavior)\n-            with_all_logits = model.generate(**generation_kwargs, **inputs_dict, num_logits_to_keep=0)\n-            # By default, num_logits_to_keep is automatically set to 1 if not provided (new behavior)\n+            # Setting logits_to_keep at 0 keeps all logits (old behavior)\n+            with_all_logits = model.generate(**generation_kwargs, **inputs_dict, logits_to_keep=0)\n+            # By default, logits_to_keep is automatically set to 1 if not provided (new behavior)\n             without_all_logits = model.generate(**inputs_dict, **generation_kwargs)\n             self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n \n     @pytest.mark.generate\n-    def test_assisted_decoding_with_num_logits_to_keep(self):\n+    def test_assisted_decoding_with_logits_to_keep(self):\n         for model_class in self.all_generative_model_classes:\n-            if \"num_logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n-                self.skipTest(reason=\"This model does not support `num_logits_to_keep` argument.\")\n+            if \"logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n+                self.skipTest(reason=\"This model does not support `logits_to_keep` argument.\")\n             if model_class._is_stateful:\n                 self.skipTest(reason=\"Stateful models don't support assisted generation\")\n \n@@ -2081,9 +2081,9 @@ def test_assisted_decoding_with_num_logits_to_keep(self):\n                 \"output_scores\": True,\n             }\n \n-            # Setting num_logits_to_keep at 0 keeps all logits (old behavior)\n-            with_all_logits = model.generate(**generation_kwargs, **inputs_dict, num_logits_to_keep=0)\n-            # By default, num_logits_to_keep is automatically set to 1 if not provided (new behavior)\n+            # Setting logits_to_keep at 0 keeps all logits (old behavior)\n+            with_all_logits = model.generate(**generation_kwargs, **inputs_dict, logits_to_keep=0)\n+            # By default, logits_to_keep is automatically set to 1 if not provided (new behavior)\n             without_all_logits = model.generate(**inputs_dict, **generation_kwargs)\n \n             self._check_similar_generate_outputs(with_all_logits, without_all_logits)"
        },
        {
            "sha": "16be88f94949bfa6b8961bb3f1645c59a88597cd",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -531,7 +531,7 @@ def test_simple_generate(self):\n         # TODO: there are significant differences in the logits across major cuda versions, which shouldn't exist\n         if self.cuda_compute_capability_major_version == 8:\n             with torch.no_grad():\n-                logits = self.model(input_ids=input_ids, num_logits_to_keep=40).logits\n+                logits = self.model(input_ids=input_ids, logits_to_keep=40).logits\n \n             EXPECTED_LOGITS_NO_GRAD = torch.tensor(\n                 ["
        },
        {
            "sha": "8a14ba666900b8b4499f61f3bc6f8e1b4ee53a9f",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -4759,21 +4759,21 @@ def test_torch_compile_for_training(self):\n         for name, param in model._orig_mod.named_parameters():\n             torch.testing.assert_close(param.grad.detach().cpu(), params[name], rtol=1e-4, atol=1e-4)\n \n-    def test_forward_with_num_logits_to_keep(self):\n+    def test_forward_with_logits_to_keep(self):\n         for model_class in self.all_generative_model_classes:\n-            if \"num_logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n-                self.skipTest(reason=\"This model does not support `num_logits_to_keep` argument.\")\n+            if \"logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n+                self.skipTest(reason=\"This model does not support `logits_to_keep` argument.\")\n \n             config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n             batch_size, sequence_length = inputs[\"input_ids\"].shape\n             vocab_size = config.get_text_config().vocab_size\n             model = model_class(config).to(device=torch_device).eval()\n-            # some models have labels but `num_logits_to_keep` should not be used in train mode\n+            # some models have labels but `logits_to_keep` should not be used in train mode\n             _ = inputs.pop(\"labels\", None)\n \n-            # num_logits_to_keep=0 is a special case meaning \"keep all logits\"\n-            all_logits = model(**inputs, num_logits_to_keep=0).logits\n-            last_token_logits = model(**inputs, num_logits_to_keep=1).logits\n+            # logits_to_keep=0 is a special case meaning \"keep all logits\"\n+            all_logits = model(**inputs, logits_to_keep=0).logits\n+            last_token_logits = model(**inputs, logits_to_keep=1).logits\n \n             # Assert all shapes are correct\n             self.assertEqual(tuple(all_logits.shape), (batch_size, sequence_length, vocab_size))"
        },
        {
            "sha": "bf9f63e070b93a650d32c8d404473e1ebc3448e0",
            "filename": "tests/utils/test_deprecation.py",
            "status": "modified",
            "additions": 26,
            "deletions": 1,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3af76df58476830eb5b5981decc64af15e369f5/tests%2Futils%2Ftest_deprecation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3af76df58476830eb5b5981decc64af15e369f5/tests%2Futils%2Ftest_deprecation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_deprecation.py?ref=d3af76df58476830eb5b5981decc64af15e369f5",
            "patch": "@@ -17,10 +17,15 @@\n \n from parameterized import parameterized\n \n-from transformers import __version__\n+from transformers import __version__, is_torch_available\n+from transformers.testing_utils import require_torch_gpu\n from transformers.utils.deprecation import deprecate_kwarg\n \n \n+if is_torch_available():\n+    import torch\n+\n+\n INFINITE_VERSION = \"9999.0.0\"\n \n \n@@ -168,3 +173,23 @@ def dummy_function(new_name=None, **kwargs):\n         with self.assertWarns(FutureWarning):\n             result = dummy_function(deprecated_name=\"old_value\", new_name=\"new_value\")\n         self.assertEqual(result, \"new_value\")\n+\n+    @require_torch_gpu\n+    def test_compile_safe(self):\n+        @deprecate_kwarg(\"deprecated_factor\", new_name=\"new_factor\", version=INFINITE_VERSION)\n+        def dummy_function(new_factor=None, **kwargs):\n+            return new_factor * torch.ones(1, device=\"cuda\")\n+\n+        compiled_function = torch.compile(dummy_function, fullgraph=True)\n+\n+        # Check that we can correctly call the compiled function with the old name, without raising errors\n+        out = compiled_function(deprecated_factor=2)\n+        self.assertEqual(out.item(), 2)\n+\n+        # Check that we can correctly call the compiled function with the new name, without raising errors\n+        out = compiled_function(new_factor=2)\n+        self.assertEqual(out.item(), 2)\n+\n+        # Check that we can correctly call the compiled function with both names, without raising errors\n+        out = compiled_function(new_factor=2, deprecated_factor=10)\n+        self.assertEqual(out.item(), 2)"
        }
    ],
    "stats": {
        "total": 918,
        "additions": 603,
        "deletions": 315
    }
}