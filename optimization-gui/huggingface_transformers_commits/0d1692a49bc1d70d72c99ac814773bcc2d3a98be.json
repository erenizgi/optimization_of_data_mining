{
    "author": "zhenglongjiepheonix",
    "message": "Fix attn mask ignore logic in training-time trace (#32613)\n\n* fix attn mask logic for training-time trace\r\n\r\n* add test\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* format\r\n\r\n* [run-slow] llama\r\n\r\n* avoid accelearate\r\n\r\n* [run-slow] llama",
    "sha": "0d1692a49bc1d70d72c99ac814773bcc2d3a98be",
    "files": [
        {
            "sha": "0dbf0cc682d7a3fede08468c41d4208d949474de",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=0d1692a49bc1d70d72c99ac814773bcc2d3a98be",
            "patch": "@@ -1284,7 +1284,6 @@ def __init__(\n         max_batch_size: Optional[int] = None,\n         layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n-        super().__init__()\n         if not hasattr(config, \"sliding_window\") or config.sliding_window is None:\n             raise ValueError(\n                 \"Setting `cache_implementation` to 'sliding_window' requires the model config supporting \""
        },
        {
            "sha": "4319c021cb2bc3fe4fac723837b2f94fdb1c52e2",
            "filename": "src/transformers/modeling_attn_mask_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/src%2Ftransformers%2Fmodeling_attn_mask_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/src%2Ftransformers%2Fmodeling_attn_mask_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_attn_mask_utils.py?ref=0d1692a49bc1d70d72c99ac814773bcc2d3a98be",
            "patch": "@@ -281,7 +281,7 @@ def _ignore_causal_mask_sdpa(\n         elif sliding_window is None or key_value_length < sliding_window:\n             if len(attention_mask.shape) == 4:\n                 return False\n-            elif (is_training or not is_tracing) and torch.all(attention_mask == 1):\n+            elif not is_tracing and torch.all(attention_mask == 1):\n                 if query_length == 1 or key_value_length == query_length:\n                     # For query_length == 1, causal attention and bi-directional attention are the same.\n                     ignore_causal_mask = True"
        },
        {
            "sha": "67828259f470336eceb1029b8673e9aa66fa466c",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=0d1692a49bc1d70d72c99ac814773bcc2d3a98be",
            "patch": "@@ -321,6 +321,9 @@ class GemmaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     # used in `test_torch_compile`\n     _torch_compile_test_ckpt = \"google/gemma-2b\"\n \n+    # used in `test_torch_compile_for_training`\n+    _torch_compile_train_cls = GemmaForCausalLM if is_torch_available() else None\n+\n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n         self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n@@ -808,7 +811,7 @@ def test_compile_static_cache(self):\n \n         prompts = [\"Hello I am doing\", \"Hi today\"]\n         tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", pad_token=\"</s>\", padding_side=\"right\")\n-        model = GemmaForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"sequential\", torch_dtype=torch.float16)\n+        model = GemmaForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=torch_device, torch_dtype=torch.float16)\n         inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n \n         # Dynamic Cache"
        },
        {
            "sha": "3a103f3efa9eb39cab9cc6f80632647e447c1492",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=0d1692a49bc1d70d72c99ac814773bcc2d3a98be",
            "patch": "@@ -319,6 +319,9 @@ class LlamaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     # used in `test_torch_compile`\n     _torch_compile_test_ckpt = \"meta-llama/Llama-2-7b-hf\"\n \n+    # used in `test_torch_compile_for_training`\n+    _torch_compile_train_cls = LlamaForCausalLM if is_torch_available() else None\n+\n     def setUp(self):\n         self.model_tester = LlamaModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=LlamaConfig, hidden_size=37)\n@@ -874,7 +877,7 @@ def test_compile_static_cache(self):\n         ]\n         tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", pad_token=\"</s>\", padding_side=\"right\")\n         model = LlamaForCausalLM.from_pretrained(\n-            \"meta-llama/Llama-2-7b-hf\", device_map=\"sequential\", torch_dtype=torch.float16\n+            \"meta-llama/Llama-2-7b-hf\", device_map=torch_device, torch_dtype=torch.float16\n         )\n         inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n "
        },
        {
            "sha": "01dd3030956d9903bef74e992d3a929c321a1363",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=0d1692a49bc1d70d72c99ac814773bcc2d3a98be",
            "patch": "@@ -677,7 +677,7 @@ def test_compile_static_cache(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", use_fast=False)\n         tokenizer.pad_token = tokenizer.eos_token\n         model = MistralForCausalLM.from_pretrained(\n-            \"mistralai/Mistral-7B-v0.1\", device_map=\"sequential\", torch_dtype=torch.float16\n+            \"mistralai/Mistral-7B-v0.1\", device_map=torch_device, torch_dtype=torch.float16\n         )\n         inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n "
        },
        {
            "sha": "13adfe1e579489596b3fab76eb4c930ae2ac218e",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=0d1692a49bc1d70d72c99ac814773bcc2d3a98be",
            "patch": "@@ -94,6 +94,8 @@ class NemotronModelTest(GemmaModelTest):\n \n     # used in `test_torch_compile`\n     _torch_compile_test_ckpt = \"nvidia/nemotron-3-8b-base-4k-hf\"\n+    # used in `test_torch_compile_for_training`\n+    _torch_compile_train_cls = NemotronForCausalLM if is_torch_available() else None\n \n     def setUp(self):\n         self.model_tester = NemotronModelTester(self)"
        },
        {
            "sha": "1f5b232f0db1ef67214f673635162e6d8d744bbf",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d1692a49bc1d70d72c99ac814773bcc2d3a98be/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=0d1692a49bc1d70d72c99ac814773bcc2d3a98be",
            "patch": "@@ -4937,6 +4937,49 @@ def test_torch_compile(self):\n         for i in range(n_iter):\n             _ = model.generate(**input_ids, do_sample=False)\n \n+    @slow\n+    @require_torch_gpu\n+    def test_torch_compile_for_training(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        if not hasattr(self, \"_torch_compile_train_cls\"):\n+            self.skipTest(f\"{self.__class__.__name__} doesn't have the attribute `_torch_compile_train_cls`.\")\n+\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        cls = self._torch_compile_train_cls\n+        model = cls(config).to(torch_device)\n+\n+        inputs = {\n+            \"input_ids\": torch.randint(low=1, high=model.config.vocab_size, size=(2, 10), device=torch_device),\n+            \"attention_mask\": torch.tensor(\n+                [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n+                dtype=torch.int64,\n+                device=torch_device,\n+            ),\n+            \"position_ids\": torch.arange(0, 10, device=torch_device).unsqueeze(0),\n+            \"labels\": torch.randint(low=1, high=model.config.vocab_size, size=(2, 10), device=torch_device),\n+        }\n+\n+        # eager backward\n+        set_seed(42)\n+        loss = model(**inputs).loss\n+        loss.backward()\n+\n+        params = {name: param.grad.clone().detach().cpu() for name, param in model.named_parameters()}\n+        model.zero_grad()\n+        del loss\n+\n+        model = torch.compile(model, fullgraph=True, mode=\"reduce-overhead\")\n+        # forward compilation\n+        set_seed(42)\n+        loss = model(**inputs).loss\n+        # backward compilation\n+        loss.backward()\n+        # check grad matches\n+        for name, param in model._orig_mod.named_parameters():\n+            torch.testing.assert_close(param.grad.detach().cpu(), params[name], rtol=1e-4, atol=1e-4)\n+\n     @slow\n     @require_torch_gpu  # Testing cuda graphs.\n     @require_read_token"
        }
    ],
    "stats": {
        "total": 60,
        "additions": 55,
        "deletions": 5
    }
}