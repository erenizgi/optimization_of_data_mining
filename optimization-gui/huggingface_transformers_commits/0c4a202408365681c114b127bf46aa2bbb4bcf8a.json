{
    "author": "kashif",
    "message": "[tests] Add Context-parallel CI tests (#41860)\n\n* intial\n\n* simplify tests\n\n* add test_cp_equivalence\n\n* removed fsdp_transformer_layer_cls_to_wrap\n\n* use DataCollatorForLanguageModeling\n\n* remove use_cache=False.\n\n* changes from review\n\n* make script self contained\n\n* moved to fsdp folder\n\n* fix class name",
    "sha": "0c4a202408365681c114b127bf46aa2bbb4bcf8a",
    "files": [
        {
            "sha": "8e0b58a321871a6900b86560becde3a647c16b6e",
            "filename": "tests/fsdp/test_context_parallel.py",
            "status": "added",
            "additions": 224,
            "deletions": 0,
            "changes": 224,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c4a202408365681c114b127bf46aa2bbb4bcf8a/tests%2Ffsdp%2Ftest_context_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c4a202408365681c114b127bf46aa2bbb4bcf8a/tests%2Ffsdp%2Ftest_context_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffsdp%2Ftest_context_parallel.py?ref=0c4a202408365681c114b127bf46aa2bbb4bcf8a",
            "patch": "@@ -0,0 +1,224 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import json\n+import sys\n+from pathlib import Path\n+\n+from transformers import is_torch_available\n+from transformers.testing_utils import (\n+    TestCasePlus,\n+    execute_subprocess_async,\n+    require_accelerate,\n+    require_torch_multi_accelerator,\n+    run_first,\n+    slow,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        AutoModelForCausalLM,\n+        AutoTokenizer,\n+        DataCollatorForLanguageModeling,\n+        HfArgumentParser,\n+        Trainer,\n+        TrainingArguments,\n+    )\n+\n+\n+class TestContextParallel(TestCasePlus):\n+    \"\"\"Test Trainer with Torch context parallelism enabled via accelerate's ParallelismConfig.\"\"\"\n+\n+    @require_torch_multi_accelerator\n+    @require_accelerate\n+    @slow\n+    @run_first\n+    def test_cp_equivalence(self):\n+        \"\"\"Test that CP produces the same losses as without CP.\"\"\"\n+\n+        # Shared setup\n+        world_size = 2\n+        script_path = __file__\n+\n+        # Step 1: Run with CP enabled (cp_size=world_size)\n+        cp_yes_output_dir = Path(self.get_auto_remove_tmp_dir()).resolve()\n+        cp_yes_config_path = cp_yes_output_dir / \"context_parallel_config.yaml\"\n+        cp_yes_losses_path = cp_yes_output_dir / \"cp_yes_losses.json\"\n+\n+        # Write config file inline (self-contained test)\n+        with open(cp_yes_config_path, \"w\") as f:\n+            f.write(\n+                f\"\"\"distributed_type: FSDP\n+fsdp_config:\n+  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n+  fsdp_state_dict_type: SHARDED_STATE_DICT\n+  fsdp_version: 2\n+mixed_precision: bf16\n+num_processes: {world_size}\n+parallelism_config:\n+  parallelism_config_dp_replicate_size: 1\n+  parallelism_config_dp_shard_size: 1\n+  parallelism_config_tp_size: 1\n+  parallelism_config_cp_size: {world_size}\n+  parallelism_config_cp_comm_strategy: alltoall\n+\"\"\"\n+            )\n+\n+        cmd_cp_yes = f\"\"\"\n+            accelerate launch\n+            --config_file {cp_yes_config_path}\n+            {script_path}\n+            --output_dir {cp_yes_output_dir}\n+            --report_to none\n+            --max_steps 10\n+            --per_device_train_batch_size 1\n+            --gradient_accumulation_steps 1\n+            --logging_steps 1\n+            --remove_unused_columns False\n+            --seed 42\n+            --loss_output_file {cp_yes_losses_path}\n+        \"\"\".split()\n+\n+        execute_subprocess_async(cmd_cp_yes, env=self.get_env())\n+\n+        # Step 2: Run without CP (FSDP with num_processes=1, no parallelism_config)\n+        cp_no_output_dir = Path(self.get_auto_remove_tmp_dir()).resolve()\n+        cp_no_config_path = cp_no_output_dir / \"context_parallel_config.yaml\"\n+        cp_no_losses_path = cp_no_output_dir / \"cp_no_losses.json\"\n+\n+        # Write config file inline (self-contained test)\n+        with open(cp_no_config_path, \"w\") as f:\n+            f.write(\n+                \"\"\"distributed_type: FSDP\n+fsdp_config:\n+  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n+  fsdp_state_dict_type: SHARDED_STATE_DICT\n+  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer\n+  fsdp_version: 2\n+mixed_precision: bf16\n+num_processes: 1\n+\"\"\"\n+            )\n+\n+        cmd_cp_no = f\"\"\"\n+            accelerate launch\n+            --config_file {cp_no_config_path}\n+            {script_path}\n+            --output_dir {cp_no_output_dir}\n+            --report_to none\n+            --max_steps 10\n+            --per_device_train_batch_size 1\n+            --gradient_accumulation_steps 1\n+            --logging_steps 1\n+            --remove_unused_columns False\n+            --seed 42\n+            --loss_output_file {cp_no_losses_path}\n+        \"\"\".split()\n+\n+        execute_subprocess_async(cmd_cp_no, env=self.get_env())\n+\n+        # Compare losses - should be very close since CP just splits sequence computation\n+        with open(cp_yes_losses_path) as f:\n+            cp_yes_losses = json.load(f)\n+        with open(cp_no_losses_path) as f:\n+            cp_no_losses = json.load(f)\n+\n+        assert len(cp_yes_losses) == len(cp_no_losses), (\n+            f\"Different number of losses: CP has {len(cp_yes_losses)}, no-CP has {len(cp_no_losses)}\"\n+        )\n+\n+        # CP should produce very similar results (small numerical differences expected)\n+        # The differences come from:\n+        # - Different gradient reduction patterns in distributed training\n+        # - BF16 mixed precision accumulated differences\n+        # - Sequence splitting and gathering in CP mode\n+        cp_yes_losses_tensor = torch.tensor(cp_yes_losses)\n+        cp_no_losses_tensor = torch.tensor(cp_no_losses)\n+\n+        # Use torch.testing.assert_close with rtol=2% and atol=0.02\n+        # Testing shows actual differences are typically <1.5%\n+        torch.testing.assert_close(\n+            cp_yes_losses_tensor,\n+            cp_no_losses_tensor,\n+            rtol=2e-2,  # 2% relative tolerance\n+            atol=2e-2,  # 0.02 absolute tolerance\n+            msg=f\"CP losses {cp_yes_losses} do not match non-CP losses {cp_no_losses}\",\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    # Parse custom arguments (not TrainingArguments parameters)\n+    loss_output_file = None\n+\n+    if \"--loss_output_file\" in sys.argv:\n+        idx = sys.argv.index(\"--loss_output_file\")\n+        loss_output_file = sys.argv[idx + 1]\n+        sys.argv.pop(idx)\n+        sys.argv.pop(idx)\n+\n+    parser = HfArgumentParser((TrainingArguments,))\n+    training_args = parser.parse_args_into_dataclasses()[0]\n+\n+    # Use SmolLM (small Llama-based model that works with CP)\n+    model_name = \"HuggingFaceTB/SmolLM-135M\"\n+    tokenizer = AutoTokenizer.from_pretrained(model_name)\n+    if tokenizer.pad_token is None:\n+        tokenizer.pad_token = tokenizer.eos_token\n+\n+    model = AutoModelForCausalLM.from_pretrained(\n+        model_name,\n+        attn_implementation=\"sdpa\",  # CP requires SDPA\n+    )\n+\n+    # Create simple dataset: just tokenize some text\n+    texts = [\n+        \"The quick brown fox jumps over the lazy dog. \" * 10,\n+        \"Hello world, this is a test sentence for training. \" * 10,\n+    ] * 4  # 8 samples total\n+\n+    def tokenize_function(examples):\n+        return tokenizer(examples, max_length=128, truncation=True, padding=\"max_length\")\n+\n+    train_dataset = [tokenize_function(text) for text in texts]\n+\n+    # Use standard DataCollatorForLanguageModeling for causal LM\n+    # pad_to_multiple_of=4 ensures sequences are divisible by cp_size * 2 (for cp_size=2)\n+    # Trainer will automatically generate position_ids and shift_labels as needed\n+    data_collator = DataCollatorForLanguageModeling(\n+        tokenizer=tokenizer,\n+        mlm=False,  # Causal language modeling\n+        pad_to_multiple_of=4,\n+    )\n+\n+    trainer = Trainer(\n+        model=model,\n+        args=training_args,\n+        train_dataset=train_dataset,\n+        data_collator=data_collator,\n+    )\n+\n+    # Train for a few steps\n+    trainer.train()\n+\n+    # Verify training completed\n+    assert trainer.state.global_step > 0, \"Training should have completed at least one step\"\n+\n+    # Save losses to file if requested (for equivalence testing)\n+    if loss_output_file and training_args.process_index == 0:\n+        losses = [log[\"loss\"] for log in trainer.state.log_history if \"loss\" in log]\n+        with open(loss_output_file, \"w\") as f:\n+            json.dump(losses, f)"
        }
    ],
    "stats": {
        "total": 224,
        "additions": 224,
        "deletions": 0
    }
}