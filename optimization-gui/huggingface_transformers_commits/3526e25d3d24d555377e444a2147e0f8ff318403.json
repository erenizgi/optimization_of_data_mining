{
    "author": "yao-matrix",
    "message": "enable misc test cases on XPU (#38852)\n\n* enable misc test cases on XPU\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* tweak bamba ground truth on XPU\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* remove print\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* one more\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>",
    "sha": "3526e25d3d24d555377e444a2147e0f8ff318403",
    "files": [
        {
            "sha": "03ace47468a47b575f2597c7f46eea2f153f6d49",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3526e25d3d24d555377e444a2147e0f8ff318403/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3526e25d3d24d555377e444a2147e0f8ff318403/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=3526e25d3d24d555377e444a2147e0f8ff318403",
            "patch": "@@ -391,7 +391,7 @@ class TrainingArguments:\n             installation](https://github.com/intel/intel-extension-for-pytorch).\n         bf16 (`bool`, *optional*, defaults to `False`):\n             Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\n-            NVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\n+            NVIDIA architecture or Intel XPU or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\n         fp16 (`bool`, *optional*, defaults to `False`):\n             Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n         fp16_opt_level (`str`, *optional*, defaults to 'O1'):"
        },
        {
            "sha": "e92d1e1ec77a85bdce021db8b36396a5282b6b4e",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=3526e25d3d24d555377e444a2147e0f8ff318403",
            "patch": "@@ -4892,7 +4892,7 @@ def test_cache_device_map_with_vision_layer_device_map(self):\n         # If the generate doesn't infer the DECODER device map correctly, this will fail\n         _ = model.generate(**inputs, max_new_tokens=2, do_sample=False)\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_cpu_offload_doesnt_compile(self):\n         \"\"\"Test that CPU offload doesn't trigger compilation\"\"\"\n         tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")"
        },
        {
            "sha": "0afc7bdbf40cf86c0ec57977bbfaf89ecc6c7fca",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 8,
            "deletions": 13,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=3526e25d3d24d555377e444a2147e0f8ff318403",
            "patch": "@@ -610,22 +610,15 @@ def setUpClass(cls):\n         cls.device_properties = get_device_properties()\n \n     def test_simple_generate(self):\n+        # fmt: off\n         expectations = Expectations(\n             {\n-                (\n-                    \"cuda\",\n-                    8,\n-                ): \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are all having a good time.\",\n-                (\n-                    \"rocm\",\n-                    9,\n-                ): \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are doing well. I am here\",\n-                (\n-                    \"xpu\",\n-                    3,\n-                ): \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are all doing well. Today I\",\n+                (\"cuda\", 8): \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are all having a good time.\",\n+                (\"rocm\", 9): \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are doing well. I am here\",\n+                (\"xpu\", 3): \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are all doing well. I am\",\n             }\n         )\n+        # fmt: on\n \n         self.model.to(torch_device)\n \n@@ -659,6 +652,7 @@ def test_simple_batched_generate_with_padding(self):\n         #\n         # Note: Key 9 is currently set for MI300, but may need potential future adjustments for H100s,\n         # considering differences in hardware processing and potential deviations in generated text.\n+        # fmt: off\n         EXPECTED_TEXTS = Expectations(\n             {\n                 (\"cuda\", 7): [],\n@@ -671,11 +665,12 @@ def test_simple_batched_generate_with_padding(self):\n                     \"!!!<|begin_of_text|>I am late! I need to be at the airport in 20 minutes! I\",\n                 ],\n                 (\"xpu\", 3): [\n-                    \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are all doing well. Today I\",\n+                    \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are all doing well. I am\",\n                     \"!!!<|begin_of_text|>I am late! I need to get to work! I have to get to the\",\n                 ],\n             }\n         )\n+        # fmt: on\n         EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n \n         self.model.to(torch_device)"
        },
        {
            "sha": "2b517572bb2dac9417348cd301041aa2d52723c3",
            "filename": "tests/models/d_fine/test_modeling_d_fine.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py?ref=3526e25d3d24d555377e444a2147e0f8ff318403",
            "patch": "@@ -27,7 +27,13 @@\n     is_torch_available,\n     is_vision_available,\n )\n-from transformers.testing_utils import require_torch, require_torch_gpu, require_vision, slow, torch_device\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils import cached_property\n \n \n@@ -680,7 +686,7 @@ def test_initialization(self):\n         self.assertTrue(not failed_cases, message)\n \n     @parameterized.expand([\"float32\", \"float16\", \"bfloat16\"])\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_inference_with_different_dtypes(self, torch_dtype_str):\n         torch_dtype = {\n@@ -702,7 +708,7 @@ def test_inference_with_different_dtypes(self, torch_dtype_str):\n                 _ = model(**self._prepare_for_class(inputs_dict, model_class))\n \n     @parameterized.expand([\"float32\", \"float16\", \"bfloat16\"])\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_inference_equivalence_for_static_and_dynamic_anchors(self, torch_dtype_str):\n         torch_dtype = {"
        },
        {
            "sha": "1500098325d2df2debe7c29da806ebdb72260d41",
            "filename": "tests/models/glm4/test_modeling_glm4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py?ref=3526e25d3d24d555377e444a2147e0f8ff318403",
            "patch": "@@ -119,7 +119,7 @@ def test_model_9b_bf16(self):\n             {\n                 (\"xpu\", 3): [\n                     \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n-                    \"Hi today I am going to tell you about the most common disease in the world. This disease is called diabetes\",\n+                    \"Hi today I am going to tell you about the most common mistakes that people make when they are learning English.\",\n                 ],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", 8): [\n@@ -177,7 +177,7 @@ def test_model_9b_sdpa(self):\n             {\n                 (\"xpu\", 3): [\n                     \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n-                    \"Hi today I am going to tell you about the most common disease in the world. This disease is called diabetes\",\n+                    \"Hi today I am going to tell you about the most common mistakes that people make when they are learning English.\",\n                 ],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", 8): ["
        },
        {
            "sha": "3de8b482d70911bfe226553199d648f8211537ae",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=3526e25d3d24d555377e444a2147e0f8ff318403",
            "patch": "@@ -1035,7 +1035,7 @@ def test_flash_attn_2_conversion(self):\n         self.skipTest(reason=\"Musicgen doesn't use the MusicgenFlashAttention2 class method.\")\n \n     @require_torch_sdpa\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_sdpa_can_dispatch_on_flash(self):\n         if not self.has_attentions:\n@@ -1046,8 +1046,8 @@ def test_sdpa_can_dispatch_on_flash(self):\n             self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n         elif device_type == \"rocm\" and major < 9:\n             self.skipTest(reason=\"This test requires an AMD GPU with compute capability >= 9.0\")\n-        else:\n-            self.skipTest(reason=\"This test requires a Nvidia or AMD GPU\")\n+        elif device_type not in [\"cuda\", \"rocm\", \"xpu\"]:\n+            self.skipTest(reason=\"This test requires a Nvidia or AMD GPU or an Intel XPU\")\n \n         torch.compiler.reset()\n "
        },
        {
            "sha": "eef833750ba7768e394cbb8ba4553ba5edeb4845",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=3526e25d3d24d555377e444a2147e0f8ff318403",
            "patch": "@@ -1035,7 +1035,7 @@ def test_flash_attn_2_conversion(self):\n         self.skipTest(reason=\"MusicgenMelody doesn't use the MusicgenMelodyFlashAttention2 class method.\")\n \n     @require_torch_sdpa\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_sdpa_can_dispatch_on_flash(self):\n         if not self.has_attentions:\n@@ -1046,8 +1046,8 @@ def test_sdpa_can_dispatch_on_flash(self):\n             self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n         elif device_type == \"rocm\" and major < 9:\n             self.skipTest(reason=\"This test requires an AMD GPU with compute capability >= 9.0\")\n-        else:\n-            self.skipTest(reason=\"This test requires a Nvidia or AMD GPU\")\n+        elif device_type not in [\"cuda\", \"rocm\", \"xpu\"]:\n+            self.skipTest(reason=\"This test requires a Nvidia or AMD GPU or an Intel XPU\")\n \n         torch.compiler.reset()\n "
        },
        {
            "sha": "455d94794e126ebaf8a7386f9edbd213c2c5ba5c",
            "filename": "tests/models/vitmatte/test_image_processing_vitmatte.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py?ref=3526e25d3d24d555377e444a2147e0f8ff318403",
            "patch": "@@ -21,7 +21,14 @@\n import requests\n from packaging import version\n \n-from transformers.testing_utils import is_flaky, require_torch, require_torch_gpu, require_vision, slow, torch_device\n+from transformers.testing_utils import (\n+    is_flaky,\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n@@ -337,7 +344,7 @@ def test_slow_fast_equivalence_batched(self):\n         )\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_vision\n     def test_can_compile_fast_image_processor(self):\n         # override as trimaps are needed for the image processor"
        },
        {
            "sha": "a11ff9bcbc29facaca658e5eac19bc475dcd495c",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=3526e25d3d24d555377e444a2147e0f8ff318403",
            "patch": "@@ -1623,9 +1623,10 @@ def is_any_loss_nan_or_inf(log_history):\n         self.assertFalse(is_any_loss_nan_or_inf(log_history_filter))\n \n     def test_train_and_eval_dataloaders(self):\n-        if torch_device in [\"cuda\", \"xpu\"]:\n+        if torch_device in [\"cuda\"]:\n             n_gpu = max(1, backend_device_count(torch_device))\n         else:\n+            # DP is decprecated by PyTorch, accelerators like XPU doesn't support DP\n             n_gpu = 1\n \n         tmp_dir = self.get_auto_remove_tmp_dir()\n@@ -3940,7 +3941,7 @@ def test_torchdynamo_memory(self):\n         from torch import _dynamo as torchdynamo\n \n         class CustomTrainer(Trainer):\n-            def compute_loss(self, model, inputs, return_outputs=False):\n+            def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n                 x = inputs[\"x\"]\n                 output = model(x)\n                 if self.args.n_gpu == 1:"
        },
        {
            "sha": "b1f41153e223d8b0a02efa28606746f3085a83cc",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3526e25d3d24d555377e444a2147e0f8ff318403/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=3526e25d3d24d555377e444a2147e0f8ff318403",
            "patch": "@@ -556,7 +556,7 @@ def test_static_cache_multi_accelerator(self):\n         _ = model(**inputs)\n         _ = model.generate(**inputs, max_new_tokens=2, cache_implementation=\"hybrid\")\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @parameterized.expand(TEST_CACHE_IMPLEMENTATIONS)\n     def test_cache_gptj_model(self, cache_implementation):\n         \"\"\"Tests caches with GPT-J model. Regression test for https://github.com/huggingface/transformers/pull/34799\"\"\""
        }
    ],
    "stats": {
        "total": 71,
        "additions": 40,
        "deletions": 31
    }
}