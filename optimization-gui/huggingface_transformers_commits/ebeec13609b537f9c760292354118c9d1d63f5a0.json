{
    "author": "ydshieh",
    "message": "Fix `InternVL` integration test (#38612)\n\n* fix\n\n* fix\n\n* fix OOM\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "ebeec13609b537f9c760292354118c9d1d63f5a0",
    "files": [
        {
            "sha": "ff7848add78fd568eb0325a9b02ac2231a50286a",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 29,
            "deletions": 12,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebeec13609b537f9c760292354118c9d1d63f5a0/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebeec13609b537f9c760292354118c9d1d63f5a0/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=ebeec13609b537f9c760292354118c9d1d63f5a0",
            "patch": "@@ -284,6 +284,7 @@ class InternVLQwen2IntegrationTest(unittest.TestCase):\n     def setUp(self):\n         self.small_model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n         self.medium_model_checkpoint = \"OpenGVLab/InternVL3-2B-hf\"\n+        cleanup(torch_device, gc_collect=True)\n \n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n@@ -329,7 +330,8 @@ def test_qwen2_small_model_integration_forward(self):\n         expected_logits_all = Expectations(\n             {\n                 (\"xpu\", 3): torch.tensor([11.7500, 14.7500, 14.1250, 10.5625, 6.7812], dtype=torch.bfloat16),\n-                (\"cuda\", 7): torch.tensor([11.9375, 14.8750, 14.0625, 10.7500, 6.9062], dtype=torch.bfloat16),\n+                (\"cuda\", 7): torch.tensor([11.9375, 14.7500, 14.4375, 10.8125,  7.0938], dtype=torch.bfloat16),\n+                (\"cuda\", 8): torch.tensor([11.8750, 14.8125, 14.3125, 10.8125,  6.9375], dtype=torch.bfloat16),\n             }\n         )  # fmt: skip\n         expected_logits = expected_logits_all.get_expectation()\n@@ -358,7 +360,8 @@ def test_qwen2_small_model_integration_generate_text_only(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"Whispers of dawn,\\nSilent whispers of the night,\\nNew day's light.\",\n-                (\"cuda\", 7): \"Whispers of dawn,\\nSilent whispers of the night,\\nNew day's light begins.\",\n+                (\"cuda\", 7): \"Whispers of dawn,\\nSilent whispers of the night,\\nNew day's light.\",\n+                (\"cuda\", 8): \"Whispers of dawn,\\nSilent whispers of the night,\\nNew day's light begins.\",\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -425,7 +428,7 @@ def test_qwen2_small_model_integration_batched_generate(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Gate\"',\n-                (\"cuda\", 7): 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Gate of',\n+                (\"cuda\", 7): 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Arch,\"',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -483,7 +486,7 @@ def test_qwen2_small_model_integration_batched_generate_multi_image(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"user\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images show the Statue of Liberty and the Golden Gate Bridge from different angles. Here are the differences:\\n\\n1. **Foreground\",\n-                (\"cuda\", 7): \"user\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images show the Statue of Liberty and the Golden Gate Bridge from different angles. Here are the differences:\\n\\n1. **Angle\",\n+                (\"cuda\", 7): \"user\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images show the Statue of Liberty and the Golden Gate Bridge from different angles. Here are the differences:\\n\\n1. **Foreground\",\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -606,7 +609,7 @@ def test_qwen2_small_model_integration_interleaved_images_videos(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"user\\n\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images depict two distinct scenes:\\n\\n1. **Left Image:**\\n   - The Statue of Liberty is prominently featured on an\",\n-                (\"cuda\", 7): \"user\\n\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images depict two distinct scenes:\\n\\n1. **Left Image**: This shows the Statue of Liberty on Liberty Island, with the\",\n+                (\"cuda\", 7): \"user\\n\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images depict two distinct scenes:\\n\\n1. **Left Image:**\\n   - The Statue of Liberty is prominently featured on an\",\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -620,7 +623,7 @@ def test_qwen2_small_model_integration_interleaved_images_videos(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot.\",\n-                (\"cuda\", 7): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nA forehand shot\",\n+                (\"cuda\", 7): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot.\",\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -646,6 +649,7 @@ class InternVLLlamaIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         self.small_model_checkpoint = \"OpenGVLab/InternVL2_5-2B-MPO-hf\"\n         self.medium_model_checkpoint = \"OpenGVLab/InternVL2_5-8B-MPO-hf\"\n+        cleanup(torch_device, gc_collect=True)\n \n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n@@ -688,13 +692,22 @@ def test_llama_small_model_integration_forward(self):\n             output = model(**inputs)\n \n         actual_logits = output.logits[0, -1, :5].cpu()\n-        expected_logits = torch.tensor([-9.8750, -0.4258, 1.4844, -10.3125, -10.3125], dtype=torch.bfloat16)\n+\n+        expected_logits_all = Expectations(\n+            {\n+                (\"xpu\", 3): torch.tensor([-9.8750, -0.5703, 1.4297, -10.3125, -10.3125], dtype=torch.bfloat16),\n+                (\"cuda\", 7): torch.tensor([-9.8750, -0.5703, 1.4297, -10.3125, -10.3125], dtype=torch.bfloat16),\n+                (\"cuda\", 8): torch.tensor([-9.8750,  -0.5117,   1.4297, -10.3750, -10.3750], dtype=torch.bfloat16),\n+            }\n+        )  # fmt: skip\n+        expected_logits = torch.tensor(expected_logits_all.get_expectation(), dtype=torch.bfloat16)\n+\n         # The original implementation and the transformers implementation do not match exactly, hence the higher tolerance.\n         # The difference is likely due to the different implementations of the attention mechanism (different order of operations)\n         # between the transformers Llama model and the original InternLM model.\n         # The difference has almost no effect on the output tokens, but it does affect the logits a lot more.\n         self.assertTrue(\n-            torch.allclose(actual_logits, expected_logits, atol=1),\n+            torch.allclose(actual_logits, expected_logits, atol=1e-3),\n             f\"Actual logits: {actual_logits}\"\n             f\"\\nExpected logits: {expected_logits}\"\n             f\"\\nDifference: {torch.abs(actual_logits - expected_logits)}\",\n@@ -765,7 +778,8 @@ def test_llama_small_model_integration_batched_generate(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden path leads to calm lake,\\nNature's peaceful grace.\",\n-                (\"cuda\", 7): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.\",\n+                (\"cuda\", 7): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden path leads to calm lake,\\nNature's peaceful grace.\",\n+                (\"cuda\", 8): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nA wooden path leads to the sea,\\nPeaceful, still waters.\",\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -819,7 +833,7 @@ def test_llama_small_model_integration_batched_generate_multi_image(self):\n         # Check first output\n         decoded_output = processor.decode(output[0], skip_special_tokens=True)\n         # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n-        expected_output = 'user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nA wooden path leads to the sea,\\nPeaceful, still waters.'  # fmt: skip\n+        expected_output = \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden path leads to calm lake,\\nNature's peaceful grace.\"  # fmt: skip\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -940,7 +954,8 @@ def test_llama_small_model_integration_interleaved_images_videos(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"user\\n\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. After re-examining the images, I can see that they are actually\",\n-                (\"cuda\", 7): \"user\\n\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. Upon closer inspection, the differences between the two images are:\\n\\n1. **\",\n+                (\"cuda\", 7): \"user\\n\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. After re-examining the images, I can see that they are actually\",\n+                (\"cuda\", 8): \"user\\n\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. After closely examining the images again, I can see that there are several differences\",\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -956,6 +971,7 @@ def test_llama_small_model_integration_interleaved_images_videos(self):\n             {\n                 (\"xpu\", 3): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot. This is a common shot in tennis where the player swings the racket across their\",\n                 (\"cuda\", 7): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot. This is a common shot in tennis where the player swings the racket across their\",\n+                (\"cuda\", 8): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot. This is a common shot in tennis where the player swings the racket across their\",\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -970,7 +986,8 @@ def test_llama_small_model_integration_interleaved_images_videos(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.\",\n-                (\"cuda\", 7): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nA wooden path leads to the sea,\\nPeaceful, untouched dreams.\",\n+                (\"cuda\", 7): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.\",\n+                (\"cuda\", 8): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.\",\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()"
        }
    ],
    "stats": {
        "total": 41,
        "additions": 29,
        "deletions": 12
    }
}