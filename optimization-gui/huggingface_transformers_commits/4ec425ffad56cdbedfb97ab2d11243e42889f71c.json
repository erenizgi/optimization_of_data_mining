{
    "author": "timjeffrey10",
    "message": "Fix GA loss for Deepspeed (#35808)\n\n* Fix GA loss for Deepspeed\n\n* Turn off loss scaling in DeepSpeed engine by scale_wrt_gas\n\n* Add comment linking to PR",
    "sha": "4ec425ffad56cdbedfb97ab2d11243e42889f71c",
    "files": [
        {
            "sha": "00938a630764d0e9b58360d8234fd0676c251610",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4ec425ffad56cdbedfb97ab2d11243e42889f71c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4ec425ffad56cdbedfb97ab2d11243e42889f71c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=4ec425ffad56cdbedfb97ab2d11243e42889f71c",
            "patch": "@@ -3722,6 +3722,11 @@ def training_step(\n             if not self.model_accepts_loss_kwargs and self.compute_loss_func is None:\n                 loss = loss / self.args.gradient_accumulation_steps\n \n+            # Turning off loss scaling w.r.t. gradient accumulation when DeepSpeed is enabled\n+            # https://github.com/huggingface/transformers/pull/35808\n+            if self.accelerator.distributed_type == DistributedType.DEEPSPEED:\n+                kwargs[\"scale_wrt_gas\"] = False\n+\n             self.accelerator.backward(loss, **kwargs)\n \n             return loss.detach()"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 5,
        "deletions": 0
    }
}