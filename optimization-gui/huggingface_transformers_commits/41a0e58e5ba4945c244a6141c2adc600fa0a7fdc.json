{
    "author": "cyyever",
    "message": "Set weights_only in torch.load (#36991)",
    "sha": "41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
    "files": [
        {
            "sha": "4b50352f580f1d74df6ea0d1df28e02f585a6717",
            "filename": "examples/flax/vision/run_image_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Fflax%2Fvision%2Frun_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Fflax%2Fvision%2Frun_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fvision%2Frun_image_classification.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -36,12 +36,12 @@\n # for dataset and preprocessing\n import torch\n import torchvision\n-import torchvision.transforms as transforms\n from flax import jax_utils\n from flax.jax_utils import pad_shard_unpad, unreplicate\n from flax.training import train_state\n from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n from huggingface_hub import HfApi\n+from torchvision import transforms\n from tqdm import tqdm\n \n import transformers"
        },
        {
            "sha": "2ee901c1bc805e858caa12e3bc5076d426e2b091",
            "filename": "examples/legacy/multiple_choice/utils_multiple_choice.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -113,7 +113,7 @@ def __init__(\n             with FileLock(lock_path):\n                 if os.path.exists(cached_features_file) and not overwrite_cache:\n                     logger.info(f\"Loading features from cached file {cached_features_file}\")\n-                    self.features = torch.load(cached_features_file)\n+                    self.features = torch.load(cached_features_file, weights_only=True)\n                 else:\n                     logger.info(f\"Creating features from dataset file at {data_dir}\")\n                     label_list = processor.get_labels()"
        },
        {
            "sha": "00302c5061c4e71f1002e7b7e73bd9fa319bbdd6",
            "filename": "examples/legacy/pytorch-lightning/run_glue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Fpytorch-lightning%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Fpytorch-lightning%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Frun_glue.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -81,7 +81,7 @@ def get_dataloader(self, mode: str, batch_size: int, shuffle: bool = False) -> D\n \n         cached_features_file = self._feature_file(mode)\n         logger.info(\"Loading features from cached file %s\", cached_features_file)\n-        features = torch.load(cached_features_file)\n+        features = torch.load(cached_features_file, weights_only=True)\n         all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n         all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n         all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)"
        },
        {
            "sha": "144759d36aac50bd65a5777017b8299c8b55b33b",
            "filename": "examples/legacy/pytorch-lightning/run_ner.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Fpytorch-lightning%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Fpytorch-lightning%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Frun_ner.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -63,7 +63,7 @@ def prepare_data(self):\n             cached_features_file = self._feature_file(mode)\n             if os.path.exists(cached_features_file) and not args.overwrite_cache:\n                 logger.info(\"Loading features from cached file %s\", cached_features_file)\n-                features = torch.load(cached_features_file)\n+                features = torch.load(cached_features_file, weights_only=True)\n             else:\n                 logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n                 examples = self.token_classification_task.read_examples_from_file(args.data_dir, mode)\n@@ -89,7 +89,7 @@ def get_dataloader(self, mode: int, batch_size: int, shuffle: bool = False) -> D\n         \"Load datasets. Called after prepare data.\"\n         cached_features_file = self._feature_file(mode)\n         logger.info(\"Loading features from cached file %s\", cached_features_file)\n-        features = torch.load(cached_features_file)\n+        features = torch.load(cached_features_file, weights_only=True)\n         all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n         all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n         if features[0].token_type_ids is not None:"
        },
        {
            "sha": "007e9e7ab2783f827d9f25e5267371d23bd617c2",
            "filename": "examples/legacy/question-answering/run_squad.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -105,8 +105,8 @@ def train(args, train_dataset, model, tokenizer):\n         os.path.join(args.model_name_or_path, \"scheduler.pt\")\n     ):\n         # Load in optimizer and scheduler states\n-        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n-        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n+        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\"), weights_only=True))\n+        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\"), weights_only=True))\n \n     if args.fp16:\n         try:\n@@ -417,7 +417,7 @@ def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=Fal\n     # Init features and dataset from cache if it exists\n     if os.path.exists(cached_features_file) and not args.overwrite_cache:\n         logger.info(\"Loading features from cached file %s\", cached_features_file)\n-        features_and_dataset = torch.load(cached_features_file)\n+        features_and_dataset = torch.load(cached_features_file, weights_only=True)\n         features, dataset, examples = (\n             features_and_dataset[\"features\"],\n             features_and_dataset[\"dataset\"],"
        },
        {
            "sha": "55fd0aa05205013901b6b0ff0c7ee240aa97c624",
            "filename": "examples/legacy/run_swag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_swag.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -244,7 +244,7 @@ def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=Fal\n     )\n     if os.path.exists(cached_features_file) and not args.overwrite_cache and not output_examples:\n         logger.info(\"Loading features from cached file %s\", cached_features_file)\n-        features = torch.load(cached_features_file)\n+        features = torch.load(cached_features_file, weights_only=True)\n     else:\n         logger.info(\"Creating features from dataset file at %s\", input_file)\n         examples = read_swag_examples(input_file)"
        },
        {
            "sha": "8d568a7e4af0a8b15fea3580a1f580225cb9f639",
            "filename": "examples/legacy/seq2seq/convert_model_to_fp16.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Fseq2seq%2Fconvert_model_to_fp16.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Fseq2seq%2Fconvert_model_to_fp16.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fconvert_model_to_fp16.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -22,7 +22,7 @@\n \n def convert(src_path: str, map_location: str = \"cpu\", save_path: Union[str, None] = None) -> None:\n     \"\"\"Convert a pytorch_model.bin or model.pt file to torch.float16 for faster downloads, less disk space.\"\"\"\n-    state_dict = torch.load(src_path, map_location=map_location)\n+    state_dict = torch.load(src_path, map_location=map_location, weights_only=True)\n     for k, v in tqdm(state_dict.items()):\n         if not isinstance(v, torch.Tensor):\n             raise TypeError(\"FP16 conversion only works on paths that are saved state dicts, like pytorch_model.bin\")"
        },
        {
            "sha": "9167ce15161805b16c308ec692f7de9cd8708841",
            "filename": "examples/legacy/token-classification/utils_ner.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -242,7 +242,7 @@ def __init__(\n             with FileLock(lock_path):\n                 if os.path.exists(cached_features_file) and not overwrite_cache:\n                     logger.info(f\"Loading features from cached file {cached_features_file}\")\n-                    self.features = torch.load(cached_features_file)\n+                    self.features = torch.load(cached_features_file, weights_only=True)\n                 else:\n                     logger.info(f\"Creating features from dataset file at {data_dir}\")\n                     examples = token_classification_task.read_examples_from_file(data_dir, mode)"
        },
        {
            "sha": "fad6463e982667433c1235c470199d2a74031e84",
            "filename": "src/transformers/convert_pytorch_checkpoint_to_tf2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fconvert_pytorch_checkpoint_to_tf2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fconvert_pytorch_checkpoint_to_tf2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_pytorch_checkpoint_to_tf2.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -277,12 +277,7 @@ def convert_pt_checkpoint_to_tf(\n     if compare_with_pt_model:\n         tfo = tf_model(tf_model.dummy_inputs, training=False)  # build the network\n \n-        weights_only_kwarg = {\"weights_only\": True}\n-        state_dict = torch.load(\n-            pytorch_checkpoint_path,\n-            map_location=\"cpu\",\n-            **weights_only_kwarg,\n-        )\n+        state_dict = torch.load(pytorch_checkpoint_path, map_location=\"cpu\", weights_only=True)\n         pt_model = pt_model_class.from_pretrained(\n             pretrained_model_name_or_path=None, config=config, state_dict=state_dict\n         )"
        },
        {
            "sha": "7546d7b49ed0eb730a8d5b00959a7bfc1f4d09cf",
            "filename": "src/transformers/data/datasets/squad.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -148,7 +148,7 @@ def __init__(\n         with FileLock(lock_path):\n             if os.path.exists(cached_features_file) and not args.overwrite_cache:\n                 start = time.time()\n-                self.old_features = torch.load(cached_features_file)\n+                self.old_features = torch.load(cached_features_file, weights_only=True)\n \n                 # Legacy cache files have only features, while new cache files\n                 # will have dataset and examples also."
        },
        {
            "sha": "072850657725fb41e54eb33d0a7ad9d489968b00",
            "filename": "src/transformers/modeling_flax_pytorch_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -71,8 +71,7 @@ def load_pytorch_checkpoint_in_flax_state_dict(\n                 )\n                 raise\n \n-            weights_only_kwarg = {\"weights_only\": True}\n-            pt_state_dict = torch.load(pt_path, map_location=\"cpu\", **weights_only_kwarg)\n+            pt_state_dict = torch.load(pt_path, map_location=\"cpu\", weights_only=True)\n             logger.info(f\"PyTorch checkpoint contains {sum(t.numel() for t in pt_state_dict.values()):,} parameters.\")\n \n         flax_state_dict = convert_pytorch_state_dict_to_flax(pt_state_dict, flax_model)\n@@ -248,8 +247,7 @@ def convert_pytorch_sharded_state_dict_to_flax(shard_filenames, flax_model):\n     flax_state_dict = {}\n     for shard_file in shard_filenames:\n         # load using msgpack utils\n-        weights_only_kwarg = {\"weights_only\": True}\n-        pt_state_dict = torch.load(shard_file, **weights_only_kwarg)\n+        pt_state_dict = torch.load(shard_file, weights_only=True)\n         weight_dtypes = {k: v.dtype for k, v in pt_state_dict.items()}\n         pt_state_dict = {\n             k: v.numpy() if v.dtype != torch.bfloat16 else v.float().numpy() for k, v in pt_state_dict.items()"
        },
        {
            "sha": "84a6ddaebcc46c12a2a08e4612fde2f4f20cf648",
            "filename": "src/transformers/modeling_tf_pytorch_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -198,8 +198,7 @@ def load_pytorch_checkpoint_in_tf2_model(\n         if pt_path.endswith(\".safetensors\"):\n             state_dict = safe_load_file(pt_path)\n         else:\n-            weights_only_kwarg = {\"weights_only\": True}\n-            state_dict = torch.load(pt_path, map_location=\"cpu\", **weights_only_kwarg)\n+            state_dict = torch.load(pt_path, map_location=\"cpu\", weights_only=True)\n \n         pt_state_dict.update(state_dict)\n "
        },
        {
            "sha": "3c4526eb1e6b6d102c9494704cbf578fb8dcb339",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -504,8 +504,7 @@ def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n             error_message += f\"\\nMissing key(s): {str_unexpected_keys}.\"\n         raise RuntimeError(error_message)\n \n-    weights_only_kwarg = {\"weights_only\": True}\n-    loader = safe_load_file if load_safe else partial(torch.load, map_location=\"cpu\", **weights_only_kwarg)\n+    loader = safe_load_file if load_safe else partial(torch.load, map_location=\"cpu\", weights_only=True)\n \n     for shard_file in shard_files:\n         state_dict = loader(os.path.join(folder, shard_file))\n@@ -598,11 +597,10 @@ def load_state_dict(\n             and is_zipfile(checkpoint_file)\n         ):\n             extra_args = {\"mmap\": True}\n-        weights_only_kwarg = {\"weights_only\": weights_only}\n         return torch.load(\n             checkpoint_file,\n             map_location=map_location,\n-            **weights_only_kwarg,\n+            weights_only=weights_only,\n             **extra_args,\n         )\n     except Exception as e:\n@@ -1216,7 +1214,7 @@ def _get_torch_dtype(\n     weights_only: bool,\n ) -> Tuple[PretrainedConfig, Optional[torch.dtype], Optional[torch.dtype]]:\n     \"\"\"Find the correct `torch_dtype` to use based on provided arguments. Also update the `config` based on the\n-    infered dtype. We do the following:\n+    inferred dtype. We do the following:\n     1. If torch_dtype is not None, we use that dtype\n     2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\n         weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype"
        },
        {
            "sha": "4ecc33355145577a81dc33b4f421a80559ed93ef",
            "filename": "src/transformers/models/data2vec/convert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -207,7 +207,7 @@ def convert_wav2vec2_checkpoint(\n         hf_wav2vec = Data2VecAudioModel(config)\n         data2vec_checkpoint_dir = os.path.dirname(checkpoint_path)\n \n-        state_dict = torch.load(checkpoint_path)\n+        state_dict = torch.load(checkpoint_path, weights_only=True)\n         state_dict[\"model\"][\"final_proj.weight\"] = state_dict[\"model\"].pop(\"final_proj.0.weight\")\n         state_dict[\"model\"][\"final_proj.bias\"] = state_dict[\"model\"].pop(\"final_proj.0.bias\")\n         converted_ckpt = os.path.join(data2vec_checkpoint_dir, \"converted.pt\")"
        },
        {
            "sha": "097423366115f22ffb0d05bba859840d91dc38ad",
            "filename": "src/transformers/models/phi/convert_phi_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fmodels%2Fphi%2Fconvert_phi_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fmodels%2Fphi%2Fconvert_phi_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fconvert_phi_weights_to_hf.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -121,7 +121,7 @@ def convert_phi_weights(\n             if model_path.endswith(\"safetensors\"):\n                 loaded_weights = safetensors.torch.load_file(model_path, device=device)\n             else:\n-                loaded_weights = torch.load(model_path, map_location=device)\n+                loaded_weights = torch.load(model_path, map_location=device, weights_only=True)\n             model_checkpoint.update(**loaded_weights)\n \n         model_type = model_name.split(\"/\")[1]  # phi-1 or phi-1_5 or phi-2"
        },
        {
            "sha": "b7b37e47a67086e1e5bf5673f325d8c44237b002",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -1589,11 +1589,10 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n                     cache_dir=cache_dir,\n                 )\n \n-                weights_only_kwarg = {\"weights_only\": True}\n                 state_dict = torch.load(\n                     weight_path,\n                     map_location=\"cpu\",\n-                    **weights_only_kwarg,\n+                    weights_only=True,\n                 )\n \n             except EnvironmentError:"
        },
        {
            "sha": "db8a5615cc013097bfc996ef6168c7544e8afa24",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 27,
            "deletions": 30,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -2820,7 +2820,6 @@ def _load_from_checkpoint(self, resume_from_checkpoint, model=None):\n                 )\n \n         if os.path.isfile(weights_file) or os.path.isfile(safe_weights_file) or is_fsdp_ckpt:\n-            weights_only_kwarg = {\"weights_only\": True}\n             # If the model is on the GPU, it still works!\n             if is_sagemaker_mp_enabled():\n                 if os.path.isfile(os.path.join(resume_from_checkpoint, \"user_content.pt\")):\n@@ -2836,11 +2835,7 @@ def _load_from_checkpoint(self, resume_from_checkpoint, model=None):\n                         logger.warning(\n                             \"Enabling FP16 and loading from smp < 1.10 checkpoint together is not supported.\"\n                         )\n-                    state_dict = torch.load(\n-                        weights_file,\n-                        map_location=\"cpu\",\n-                        **weights_only_kwarg,\n-                    )\n+                    state_dict = torch.load(weights_file, map_location=\"cpu\", weights_only=True)\n                     # Required for smp to not auto-translate state_dict from hf to smp (is already smp).\n                     state_dict[\"_smp_is_partial\"] = False\n                     load_result = model.load_state_dict(state_dict, strict=True)\n@@ -2859,11 +2854,7 @@ def _load_from_checkpoint(self, resume_from_checkpoint, model=None):\n                 if self.args.save_safetensors and os.path.isfile(safe_weights_file):\n                     state_dict = safetensors.torch.load_file(safe_weights_file, device=\"cpu\")\n                 else:\n-                    state_dict = torch.load(\n-                        weights_file,\n-                        map_location=\"cpu\",\n-                        **weights_only_kwarg,\n-                    )\n+                    state_dict = torch.load(weights_file, map_location=\"cpu\", weights_only=True)\n \n                 # workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963\n                 # which takes *args instead of **kwargs\n@@ -2941,7 +2932,6 @@ def _load_best_model(self):\n             or os.path.exists(best_safe_adapter_model_path)\n         ):\n             has_been_loaded = True\n-            weights_only_kwarg = {\"weights_only\": True}\n             if is_sagemaker_mp_enabled():\n                 if os.path.isfile(os.path.join(self.state.best_model_checkpoint, \"user_content.pt\")):\n                     # If the 'user_content.pt' file exists, load with the new smp api.\n@@ -2958,11 +2948,7 @@ def _load_best_model(self):\n                     if self.args.save_safetensors and os.path.isfile(best_safe_model_path):\n                         state_dict = safetensors.torch.load_file(best_safe_model_path, device=\"cpu\")\n                     else:\n-                        state_dict = torch.load(\n-                            best_model_path,\n-                            map_location=\"cpu\",\n-                            **weights_only_kwarg,\n-                        )\n+                        state_dict = torch.load(best_model_path, map_location=\"cpu\", weights_only=True)\n \n                     state_dict[\"_smp_is_partial\"] = False\n                     load_result = model.load_state_dict(state_dict, strict=True)\n@@ -3017,11 +3003,7 @@ def _load_best_model(self):\n                     if self.args.save_safetensors and os.path.isfile(best_safe_model_path):\n                         state_dict = safetensors.torch.load_file(best_safe_model_path, device=\"cpu\")\n                     else:\n-                        state_dict = torch.load(\n-                            best_model_path,\n-                            map_location=\"cpu\",\n-                            **weights_only_kwarg,\n-                        )\n+                        state_dict = torch.load(best_model_path, map_location=\"cpu\", weights_only=True)\n \n                     # If the model is on the GPU, it still works!\n                     # workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963\n@@ -3142,7 +3124,7 @@ def _load_rng_state(self, checkpoint):\n                 return\n \n         with safe_globals():\n-            checkpoint_rng_state = torch.load(rng_file)\n+            checkpoint_rng_state = torch.load(rng_file, weights_only=True)\n         random.setstate(checkpoint_rng_state[\"python\"])\n         np.random.set_state(checkpoint_rng_state[\"numpy\"])\n         torch.random.set_rng_state(checkpoint_rng_state[\"cpu\"])\n@@ -3375,7 +3357,9 @@ def _load_optimizer_and_scheduler(self, checkpoint):\n             # deepspeed loads optimizer/lr_scheduler together with the model in deepspeed_init\n             if not isinstance(self.lr_scheduler, DeepSpeedSchedulerWrapper):\n                 with warnings.catch_warnings(record=True) as caught_warnings:\n-                    self.lr_scheduler.load_state_dict(torch.load(os.path.join(checkpoint, SCHEDULER_NAME)))\n+                    self.lr_scheduler.load_state_dict(\n+                        torch.load(os.path.join(checkpoint, SCHEDULER_NAME), weights_only=True)\n+                    )\n                 reissue_pt_warnings(caught_warnings)\n             return\n \n@@ -3410,13 +3394,18 @@ def _load_optimizer_and_scheduler(self, checkpoint):\n                             checkpoint, f\"rank{self.args.process_index}-of-{self.args.world_size}-{OPTIMIZER_NAME}\"\n                         ),\n                         map_location=\"cpu\",\n+                        weights_only=True,\n                     )\n                     # We only need `optimizer` when resuming from checkpoint\n                     optimizer_state = optimizer_state[\"optimizer\"]\n                 else:\n-                    optimizer_state = torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=\"cpu\")\n+                    optimizer_state = torch.load(\n+                        os.path.join(checkpoint, OPTIMIZER_NAME), map_location=\"cpu\", weights_only=True\n+                    )\n                 with warnings.catch_warnings(record=True) as caught_warnings:\n-                    lr_scheduler_state = torch.load(os.path.join(checkpoint, SCHEDULER_NAME), map_location=\"cpu\")\n+                    lr_scheduler_state = torch.load(\n+                        os.path.join(checkpoint, SCHEDULER_NAME), map_location=\"cpu\", weights_only=True\n+                    )\n                 reissue_pt_warnings(caught_warnings)\n \n                 xm.send_cpu_data_to_device(optimizer_state, self.args.device)\n@@ -3458,10 +3447,14 @@ def opt_load_hook(mod, opt):\n                         )\n                     else:\n                         self.optimizer.load_state_dict(\n-                            torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n+                            torch.load(\n+                                os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location, weights_only=True\n+                            )\n                         )\n                 with warnings.catch_warnings(record=True) as caught_warnings:\n-                    self.lr_scheduler.load_state_dict(torch.load(os.path.join(checkpoint, SCHEDULER_NAME)))\n+                    self.lr_scheduler.load_state_dict(\n+                        torch.load(os.path.join(checkpoint, SCHEDULER_NAME), weights_only=True)\n+                    )\n                 reissue_pt_warnings(caught_warnings)\n \n     def _save_scaler(self, output_dir):\n@@ -3496,13 +3489,17 @@ def _load_scaler(self, checkpoint):\n             # Load in scaler states\n             if is_torch_xla_available():\n                 with warnings.catch_warnings(record=True) as caught_warnings:\n-                    scaler_state = torch.load(os.path.join(checkpoint, SCALER_NAME), map_location=\"cpu\")\n+                    scaler_state = torch.load(\n+                        os.path.join(checkpoint, SCALER_NAME), map_location=\"cpu\", weights_only=True\n+                    )\n                 reissue_pt_warnings(caught_warnings)\n                 xm.send_cpu_data_to_device(scaler_state, self.args.device)\n                 self.accelerator.scaler.load_state_dict(scaler_state)\n             else:\n                 with warnings.catch_warnings(record=True) as caught_warnings:\n-                    self.accelerator.scaler.load_state_dict(torch.load(os.path.join(checkpoint, SCALER_NAME)))\n+                    self.accelerator.scaler.load_state_dict(\n+                        torch.load(os.path.join(checkpoint, SCALER_NAME), weights_only=True)\n+                    )\n                 reissue_pt_warnings(caught_warnings)\n \n     def _load_callback_state(self):"
        },
        {
            "sha": "cca8f3b3ac87b08bb715748d99e82f61089d125b",
            "filename": "tests/models/autoformer/test_modeling_autoformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -415,7 +415,7 @@ def test_model_get_set_embeddings(self):\n \n def prepare_batch(filename=\"train-batch.pt\"):\n     file = hf_hub_download(repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=filename, repo_type=\"dataset\")\n-    batch = torch.load(file, map_location=torch_device)\n+    batch = torch.load(file, map_location=torch_device, weights_only=True)\n     return batch\n \n "
        },
        {
            "sha": "5b40a0393e5cde29138f5a51cd92eeb312eb8bba",
            "filename": "tests/models/idefics/test_image_processing_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Fidefics%2Ftest_image_processing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Fidefics%2Ftest_image_processing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_image_processing_idefics.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -28,7 +28,7 @@\n     import torch\n \n if is_torchvision_available():\n-    import torchvision.transforms as transforms\n+    from torchvision import transforms\n \n if is_vision_available():\n     from PIL import Image"
        },
        {
            "sha": "b5e35490f6f6b205585610ed5bd883a2563d7837",
            "filename": "tests/models/informer/test_modeling_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -476,7 +476,7 @@ def test_model_get_set_embeddings(self):\n \n def prepare_batch(filename=\"train-batch.pt\"):\n     file = hf_hub_download(repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=filename, repo_type=\"dataset\")\n-    batch = torch.load(file, map_location=torch_device)\n+    batch = torch.load(file, map_location=torch_device, weights_only=True)\n     return batch\n \n "
        },
        {
            "sha": "a9758bd4230fd3679ef94d0cc5de79c0538c8f3f",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -408,7 +408,7 @@ def test_small_model_integration_test(self):\n             filename=\"llava_1_6_input_ids.pt\",\n             repo_type=\"dataset\",\n         )\n-        original_input_ids = torch.load(filepath, map_location=\"cpu\")\n+        original_input_ids = torch.load(filepath, map_location=\"cpu\", weights_only=True)\n         # replace -200 by image_token_index (since we use token ID = 32000 for the image token)\n         # remove image token indices because HF impl expands image tokens `image_seq_length` times\n         original_input_ids = original_input_ids[original_input_ids != -200]\n@@ -420,7 +420,7 @@ def test_small_model_integration_test(self):\n             filename=\"llava_1_6_pixel_values.pt\",\n             repo_type=\"dataset\",\n         )\n-        original_pixel_values = torch.load(filepath, map_location=\"cpu\")\n+        original_pixel_values = torch.load(filepath, map_location=\"cpu\", weights_only=True)\n         assert torch.allclose(original_pixel_values, inputs.pixel_values.half())\n \n         # verify generation"
        },
        {
            "sha": "2f39b9e68006354f9afa27687ee75178c0280723",
            "filename": "tests/models/patchtsmixer/test_modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -452,7 +452,7 @@ def test_model_get_set_embeddings(self):\n def prepare_batch(repo_id=\"ibm/patchtsmixer-etth1-test-data\", file=\"pretrain_batch.pt\"):\n     # TODO: Make repo public\n     file = hf_hub_download(repo_id=repo_id, filename=file, repo_type=\"dataset\")\n-    batch = torch.load(file, map_location=torch_device)\n+    batch = torch.load(file, map_location=torch_device, weights_only=True)\n     return batch\n \n "
        },
        {
            "sha": "8f48a6f904a28190d1e6aef48fc602e6aceb9427",
            "filename": "tests/models/patchtst/test_modeling_patchtst.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -303,7 +303,7 @@ def test_model_get_set_embeddings(self):\n \n def prepare_batch(repo_id=\"hf-internal-testing/etth1-hourly-batch\", file=\"train-batch.pt\"):\n     file = hf_hub_download(repo_id=repo_id, filename=file, repo_type=\"dataset\")\n-    batch = torch.load(file, map_location=torch_device)\n+    batch = torch.load(file, map_location=torch_device, weights_only=True)\n     return batch\n \n "
        },
        {
            "sha": "f9dfefe376724b4135a4725ff9ca6a308063aedb",
            "filename": "tests/models/time_series_transformer/test_modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -481,7 +481,7 @@ def test_model_get_set_embeddings(self):\n \n def prepare_batch(filename=\"train-batch.pt\"):\n     file = hf_hub_download(repo_id=\"hf-internal-testing/tourism-monthly-batch\", filename=filename, repo_type=\"dataset\")\n-    batch = torch.load(file, map_location=torch_device)\n+    batch = torch.load(file, map_location=torch_device, weights_only=True)\n     return batch\n \n "
        },
        {
            "sha": "8f1ab7de053cedcefa05615864a115283dda4425",
            "filename": "tests/models/videomae/test_modeling_videomae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -456,7 +456,7 @@ def test_inference_for_pretraining(self):\n \n         # add boolean mask, indicating which patches to mask\n         local_path = hf_hub_download(repo_id=\"hf-internal-testing/bool-masked-pos\", filename=\"bool_masked_pos.pt\")\n-        inputs[\"bool_masked_pos\"] = torch.load(local_path)\n+        inputs[\"bool_masked_pos\"] = torch.load(local_path, weights_only=True)\n \n         # forward pass\n         with torch.no_grad():"
        },
        {
            "sha": "22282aa07f32f00f2709fa2a88cac14d0209773b",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -554,7 +554,7 @@ def test_peft_add_adapter_with_state_dict(self):\n \n                 state_dict_path = hf_hub_download(peft_model_id, \"adapter_model.bin\")\n \n-                dummy_state_dict = torch.load(state_dict_path)\n+                dummy_state_dict = torch.load(state_dict_path, weights_only=True)\n \n                 model.load_adapter(adapter_state_dict=dummy_state_dict, peft_config=peft_config)\n                 with self.assertRaises(ValueError):\n@@ -579,7 +579,7 @@ def test_peft_add_adapter_with_state_dict_low_cpu_mem_usage(self):\n \n                 peft_config = LoraConfig()\n                 state_dict_path = hf_hub_download(peft_model_id, \"adapter_model.bin\")\n-                dummy_state_dict = torch.load(state_dict_path)\n+                dummy_state_dict = torch.load(state_dict_path, weights_only=True)\n \n                 # this should always work\n                 model.load_adapter(\n@@ -647,7 +647,7 @@ def test_peft_from_pretrained_unexpected_keys_warning(self):\n \n                 peft_config = LoraConfig()\n                 state_dict_path = hf_hub_download(peft_model_id, \"adapter_model.bin\")\n-                dummy_state_dict = torch.load(state_dict_path)\n+                dummy_state_dict = torch.load(state_dict_path, weights_only=True)\n \n                 # add unexpected key\n                 dummy_state_dict[\"foobar\"] = next(iter(dummy_state_dict.values()))\n@@ -674,7 +674,7 @@ def test_peft_from_pretrained_missing_keys_warning(self):\n \n                 peft_config = LoraConfig()\n                 state_dict_path = hf_hub_download(peft_model_id, \"adapter_model.bin\")\n-                dummy_state_dict = torch.load(state_dict_path)\n+                dummy_state_dict = torch.load(state_dict_path, weights_only=True)\n \n                 # remove a key so that we have missing keys\n                 key = next(iter(dummy_state_dict.keys()))"
        },
        {
            "sha": "f7e16926f2d38d70050ab326704c7081cb1d78b0",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -648,7 +648,7 @@ def check_best_model_has_been_loaded(\n         else:\n             best_model = RegressionModel()\n             if not safe_weights:\n-                state_dict = torch.load(os.path.join(checkpoint, WEIGHTS_NAME))\n+                state_dict = torch.load(os.path.join(checkpoint, WEIGHTS_NAME), weights_only=True)\n             else:\n                 state_dict = safetensors.torch.load_file(os.path.join(checkpoint, SAFE_WEIGHTS_NAME))\n             best_model.load_state_dict(state_dict)"
        },
        {
            "sha": "212969ce150ad64c6317c2609a65e915ba32434e",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41a0e58e5ba4945c244a6141c2adc600fa0a7fdc/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=41a0e58e5ba4945c244a6141c2adc600fa0a7fdc",
            "patch": "@@ -765,7 +765,7 @@ def test_checkpoint_sharding_local_bin(self):\n                     # Note: pickle adds some junk so the weight of the file can end up being slightly bigger than\n                     # the size asked for (since we count parameters)\n                     if size >= max_size_int + 50000:\n-                        state_dict = torch.load(shard_file)\n+                        state_dict = torch.load(shard_file, weights_only=True)\n                         self.assertEqual(len(state_dict), 1)\n \n                 # Check the index and the shard files found match"
        }
    ],
    "stats": {
        "total": 142,
        "additions": 64,
        "deletions": 78
    }
}