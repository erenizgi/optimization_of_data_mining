{
    "author": "gpetho",
    "message": "Fix format mistake in string repr of tokenizer objects (#34493)\n\n* fix repr string format for tokenizer objects\r\n\r\nThe repr of tokenizer tokens looks confusing and just stupid, like this: `Tokenizer(...), added_tokens_decoder={1: ..., 2: ...}`. The dict that is the value of the added_tokens_decoder attribute is outside of the parentheses of the tokenizer object, whereas all other attributes are inside the parentheses like they should be.\r\n\r\nThis commit fixes this bug.\r\n\r\n* cos: add newline before closing parenthesis of repr string",
    "sha": "25a9fc584acb09afecb08b6cfd74e705058bf2ac",
    "files": [
        {
            "sha": "89ab2dc9260819237f731fae8eae52da79338c0a",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/25a9fc584acb09afecb08b6cfd74e705058bf2ac/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25a9fc584acb09afecb08b6cfd74e705058bf2ac/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=25a9fc584acb09afecb08b6cfd74e705058bf2ac",
            "patch": "@@ -1687,8 +1687,8 @@ def __repr__(self) -> str:\n             f\"{self.__class__.__name__}(name_or_path='{self.name_or_path}',\"\n             f\" vocab_size={self.vocab_size}, model_max_length={self.model_max_length}, is_fast={self.is_fast},\"\n             f\" padding_side='{self.padding_side}', truncation_side='{self.truncation_side}',\"\n-            f\" special_tokens={self.special_tokens_map}, clean_up_tokenization_spaces={self.clean_up_tokenization_spaces}), \"\n-            \" added_tokens_decoder={\\n\\t\" + added_tokens_decoder_rep + \"\\n}\"\n+            f\" special_tokens={self.special_tokens_map}, clean_up_tokenization_spaces={self.clean_up_tokenization_spaces},\"\n+            \" added_tokens_decoder={\\n\\t\" + added_tokens_decoder_rep + \"\\n}\\n)\"\n         )\n \n     def __len__(self) -> int:"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}