{
    "author": "gante",
    "message": "[docs] flax/jax purge (#40372)\n\nflax/jax purge",
    "sha": "0031c044f8b96775a91a3cb6da366d09e94b76f1",
    "files": [
        {
            "sha": "7728546633b94e421438230ad0353ff602e960fe",
            "filename": "CONTRIBUTING.md",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0031c044f8b96775a91a3cb6da366d09e94b76f1/CONTRIBUTING.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0031c044f8b96775a91a3cb6da366d09e94b76f1/CONTRIBUTING.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/CONTRIBUTING.md?ref=0031c044f8b96775a91a3cb6da366d09e94b76f1",
            "patch": "@@ -68,8 +68,7 @@ already reported** (use the search bar on GitHub under Issues). Your issue shoul\n \n Once you've confirmed the bug hasn't already been reported, please include the following information in your issue so we can quickly resolve it:\n \n-* Your **OS type and version** and **Python**, **PyTorch** and\n-  **TensorFlow** versions when applicable.\n+* Your **OS type and version** and **Python**, and **PyTorch** versions when applicable.\n * A short, self-contained, code snippet that allows us to reproduce the bug in\n   less than 30s.\n * The *full* traceback if an exception is raised.\n@@ -165,8 +164,7 @@ You'll need **[Python 3.9](https://github.com/huggingface/transformers/blob/main\n    mode with the `-e` flag.\n \n    Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\n-   failure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n-   (PyTorch, TensorFlow and/or Flax) then do:\n+   failure with this command. If that's the case make sure to install Pytorch then do:\n \n    ```bash\n    pip install -e \".[quality]\""
        },
        {
            "sha": "9527ca4ab2207605f17ed64bc974f53157a10424",
            "filename": "docs/source/en/installation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 33,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/0031c044f8b96775a91a3cb6da366d09e94b76f1/docs%2Fsource%2Fen%2Finstallation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0031c044f8b96775a91a3cb6da366d09e94b76f1/docs%2Fsource%2Fen%2Finstallation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finstallation.md?ref=0031c044f8b96775a91a3cb6da366d09e94b76f1",
            "patch": "@@ -20,7 +20,7 @@ rendered properly in your Markdown viewer.\n \n # Installation\n \n-Transformers works with [PyTorch](https://pytorch.org/get-started/locally/), [TensorFlow 2.0](https://www.tensorflow.org/install/pip), and [Flax](https://flax.readthedocs.io/en/latest/). It has been tested on Python 3.9+, PyTorch 2.1+, TensorFlow 2.6+, and Flax 0.4.1+.\n+Transformers works with [PyTorch](https://pytorch.org/get-started/locally/). It has been tested on Python 3.9+ and PyTorch 2.2+.\n \n ## Virtual environment\n \n@@ -74,7 +74,7 @@ uv pip install transformers\n </hfoption>\n </hfoptions>\n \n-For GPU acceleration, install the appropriate CUDA drivers for [PyTorch](https://pytorch.org/get-started/locally) and [TensorFlow](https://www.tensorflow.org/install/pip).\n+For GPU acceleration, install the appropriate CUDA drivers for [PyTorch](https://pytorch.org/get-started/locally).\n \n Run the command below to check if your system detects an NVIDIA GPU.\n \n@@ -84,42 +84,11 @@ nvidia-smi\n \n To install a CPU-only version of Transformers and a machine learning framework, run the following command.\n \n-<hfoptions id=\"cpu-only\">\n-<hfoption id=\"PyTorch\">\n-\n ```bash\n pip install 'transformers[torch]'\n uv pip install 'transformers[torch]'\n ```\n \n-</hfoption>\n-<hfoption id=\"TensorFlow\">\n-\n-For Apple M1 hardware, you need to install CMake and pkg-config first.\n-\n-```bash\n-brew install cmake\n-brew install pkg-config\n-```\n-\n-Install TensorFlow 2.0.\n-\n-```bash\n-pip install 'transformers[tf-cpu]'\n-uv pip install 'transformers[tf-cpu]'\n-```\n-\n-</hfoption>\n-<hfoption id=\"Flax\">\n-\n-```bash\n-pip install 'transformers[flax]'\n-uv pip install 'transformers[flax]'\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n Test whether the install was successful with the following command. It should return a label and score for the provided text.\n \n ```bash"
        },
        {
            "sha": "d1283db75b205dccb4228c074238179fe783607e",
            "filename": "docs/source/en/model_sharing.md",
            "status": "modified",
            "additions": 2,
            "deletions": 59,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/0031c044f8b96775a91a3cb6da366d09e94b76f1/docs%2Fsource%2Fen%2Fmodel_sharing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0031c044f8b96775a91a3cb6da366d09e94b76f1/docs%2Fsource%2Fen%2Fmodel_sharing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_sharing.md?ref=0031c044f8b96775a91a3cb6da366d09e94b76f1",
            "patch": "@@ -73,53 +73,9 @@ A model repository also includes an inference [widget](https://hf.co/docs/hub/mo\n \n Check out the Hub [Models](https://hf.co/docs/hub/models) documentation to for more information.\n \n-## Model framework conversion\n-\n-Reach a wider audience by making a model available in PyTorch, TensorFlow, and Flax. While users can still load a model if they're using a different framework, it is slower because Transformers needs to convert the checkpoint on the fly. It is faster to convert the checkpoint first.\n-\n-<hfoptions id=\"convert\">\n-<hfoption id=\"PyTorch\">\n-\n-Set `from_tf=True` to convert a checkpoint from TensorFlow to PyTorch and then save it.\n-\n-```py\n-from transformers import DistilBertForSequenceClassification\n-\n-pt_model = DistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_tf=True)\n-pt_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n-```\n-\n-</hfoption>\n-<hfoption id=\"TensorFlow\">\n-\n-Set `from_pt=True` to convert a checkpoint from PyTorch to TensorFlow and then save it.\n-\n-```py\n-from transformers import TFDistilBertForSequenceClassification\n-\n-tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_pt=True)\n-tf_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n-```\n-\n-</hfoption>\n-<hfoption id=\"Flax\">\n-\n-Set `from_pt=True` to convert a checkpoint from PyTorch to Flax and then save it.\n-\n-```py\n-from transformers import FlaxDistilBertForSequenceClassification\n-flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\n-    \"path/to/awesome-name-you-picked\", from_pt=True\n-)\n-flax_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n ## Uploading a model\n \n-There are several ways to upload a model to the Hub depending on your workflow preference. You can push a model with [`Trainer`], a callback for TensorFlow models, call [`~PreTrainedModel.push_to_hub`] directly on a model, or use the Hub web interface.\n+There are several ways to upload a model to the Hub depending on your workflow preference. You can push a model with [`Trainer`], call [`~PreTrainedModel.push_to_hub`] directly on a model, or use the Hub web interface.\n \n <Youtube id=\"Z1-XMy-GNLQ\"/>\n \n@@ -143,19 +99,6 @@ trainer = Trainer(\n trainer.push_to_hub()\n ```\n \n-### PushToHubCallback\n-\n-For TensorFlow models, add the [`PushToHubCallback`] to the [fit](https://keras.io/api/models/model_training_apis/#fit-method) method.\n-\n-```py\n-from transformers import PushToHubCallback\n-\n-push_to_hub_callback = PushToHubCallback(\n-    output_dir=\"./your_model_save_path\", tokenizer=tokenizer, hub_model_id=\"your-username/my-awesome-model\"\n-)\n-model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)\n-```\n-\n ### PushToHubMixin\n \n The [`~utils.PushToHubMixin`] provides functionality for pushing a model or tokenizer to the Hub.\n@@ -166,7 +109,7 @@ Call [`~utils.PushToHubMixin.push_to_hub`] directly on a model to upload it to t\n model.push_to_hub(\"my-awesome-model\")\n ```\n \n-Other objects like a tokenizer or TensorFlow model are also pushed to the Hub in the same way.\n+Other objects like a tokenizer are also pushed to the Hub in the same way.\n \n ```py\n tokenizer.push_to_hub(\"my-awesome-model\")"
        },
        {
            "sha": "fdfcfba6585a7662345267f46aca57fa0e0f87a4",
            "filename": "docs/source/en/models.md",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/0031c044f8b96775a91a3cb6da366d09e94b76f1/docs%2Fsource%2Fen%2Fmodels.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0031c044f8b96775a91a3cb6da366d09e94b76f1/docs%2Fsource%2Fen%2Fmodels.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodels.md?ref=0031c044f8b96775a91a3cb6da366d09e94b76f1",
            "patch": "@@ -45,43 +45,6 @@ There are two general types of models you can load:\n 1. A barebones model, like [`AutoModel`] or [`LlamaModel`], that outputs hidden states.\n 2. A model with a specific *head* attached, like [`AutoModelForCausalLM`] or [`LlamaForCausalLM`], for performing specific tasks.\n \n-For each model type, there is a separate class for each machine learning framework (PyTorch, TensorFlow, Flax). Pick the corresponding prefix for the framework you're using.\n-\n-<hfoptions id=\"backend\">\n-<hfoption id=\"PyTorch\">\n-\n-```py\n-from transformers import AutoModelForCausalLM, MistralForCausalLM\n-\n-# load with AutoClass or model-specific class\n-model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", dtype=\"auto\", device_map=\"auto\")\n-model = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", dtype=\"auto\", device_map=\"auto\")\n-```\n-\n-</hfoption>\n-<hfoption id=\"TensorFlow\">\n-\n-```py\n-from transformers import TFAutoModelForCausalLM, TFMistralForCausalLM\n-\n-# load with AutoClass or model-specific class\n-model = TFAutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n-model = TFMistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n-```\n-\n-</hfoption>\n-<hfoption id=\"Flax\">\n-\n-```py\n-from transformers import FlaxAutoModelForCausalLM, FlaxMistralForCausalLM\n-\n-# load with AutoClass or model-specific class\n-model = FlaxAutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n-model = FlaxMistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n-```\n-\n-</hfoption>\n-</hfoptions>\n \n ## Model classes\n "
        },
        {
            "sha": "7cfa46458b7579511516b269bc307b9995d1a649",
            "filename": "docs/source/en/philosophy.md",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0031c044f8b96775a91a3cb6da366d09e94b76f1/docs%2Fsource%2Fen%2Fphilosophy.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0031c044f8b96775a91a3cb6da366d09e94b76f1/docs%2Fsource%2Fen%2Fphilosophy.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fphilosophy.md?ref=0031c044f8b96775a91a3cb6da366d09e94b76f1",
            "patch": "@@ -34,17 +34,17 @@ The library was designed with two strong goals in mind:\n     loads the related class instance and associated data (configurations' hyperparameters, tokenizers' vocabulary,\n     and models' weights) from a pretrained checkpoint provided on [Hugging Face Hub](https://huggingface.co/models) or your own saved checkpoint.\n   - On top of those three base classes, the library provides two APIs: [`pipeline`] for quickly\n-    using a model for inference on a given task and [`Trainer`] to quickly train or fine-tune a PyTorch model (all TensorFlow models are compatible with `Keras.fit`).\n+    using a model for inference on a given task and [`Trainer`] to quickly train or fine-tune a PyTorch model.\n   - As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to\n-    extend or build upon the library, just use regular Python, PyTorch, TensorFlow, Keras modules and inherit from the base\n+    extend or build upon the library, just use regular Python or PyTorch and inherit from the base\n     classes of the library to reuse functionalities like model loading and saving. If you'd like to learn more about our coding philosophy for models, check out our [Repeat Yourself](https://huggingface.co/blog/transformers-design-philosophy) blog post.\n \n 2. Provide state-of-the-art models with performances as close as possible to the original models:\n \n   - We provide at least one example for each architecture which reproduces a result provided by the official authors\n     of said architecture.\n   - The code is usually as close to the original code base as possible which means some PyTorch code may be not as\n-    *pytorchic* as it could be as a result of being converted TensorFlow code and vice versa.\n+    *pytorchic* as it could be as a result of being converted from other Deep Learning frameworks.\n \n A few other goals:\n \n@@ -58,13 +58,11 @@ A few other goals:\n   - A simple and consistent way to add new tokens to the vocabulary and embeddings for fine-tuning.\n   - Simple ways to mask and prune Transformer heads.\n \n-- Easily switch between PyTorch, TensorFlow 2.0 and Flax, allowing training with one framework and inference with another.\n-\n ## Main concepts\n \n The library is built around three types of classes for each model:\n \n-- **Model classes** can be PyTorch models ([torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)), Keras models ([tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)) or JAX/Flax models ([flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)) that work with the pretrained weights provided in the library.\n+- **Model classes** are be PyTorch models ([torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)).\n - **Configuration classes** store the hyperparameters required to build a model (such as the number of layers and hidden size). You don't always need to instantiate these yourself. In particular, if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model).\n - **Preprocessing classes** convert the raw data into a format accepted by the model. A [tokenizer](main_classes/tokenizer) stores the vocabulary for each model and provide methods for encoding and decoding strings in a list of token embedding indices to be fed to a model. [Image processors](main_classes/image_processor) preprocess vision inputs, [feature extractors](main_classes/feature_extractor) preprocess audio inputs, and a [processor](main_classes/processors) handles multimodal inputs.\n \n@@ -76,4 +74,3 @@ All these classes can be instantiated from pretrained instances, saved locally,\n - `save_pretrained()` lets you save a model, configuration, and preprocessing class locally so that it can be reloaded using\n   `from_pretrained()`.\n - `push_to_hub()` lets you share a model, configuration, and a preprocessing class to the Hub, so it is easily accessible to everyone.\n-"
        },
        {
            "sha": "a5634c29ee49c10a1c572613be965c2adbe633f9",
            "filename": "docs/source/en/pr_checks.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0031c044f8b96775a91a3cb6da366d09e94b76f1/docs%2Fsource%2Fen%2Fpr_checks.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0031c044f8b96775a91a3cb6da366d09e94b76f1/docs%2Fsource%2Fen%2Fpr_checks.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fpr_checks.md?ref=0031c044f8b96775a91a3cb6da366d09e94b76f1",
            "patch": "@@ -40,7 +40,7 @@ or for an editable install:\n pip install -e .[dev]\n ```\n \n-inside the Transformers repo. Since the number of optional dependencies of Transformers has grown a lot, it's possible you don't manage to get all of them. If the dev install fails, make sure to install the Deep Learning framework you are working with (PyTorch, TensorFlow and/or Flax) then do\n+inside the Transformers repo. Since the number of optional dependencies of Transformers has grown a lot, it's possible you don't manage to get all of them. If the dev install fails, make sure to install PyTorch then do\n \n ```bash\n pip install transformers[quality]\n@@ -55,7 +55,7 @@ pip install -e .[quality]\n \n ## Tests\n \n-All the jobs that begin with `ci/circleci: run_tests_` run parts of the Transformers testing suite. Each of those jobs focuses on a part of the library in a certain environment: for instance `ci/circleci: run_tests_pipelines_tf` runs the pipelines test in an environment where TensorFlow only is installed.\n+All the jobs that begin with `ci/circleci: run_tests_` run parts of the Transformers testing suite. Each of those jobs focuses on a part of the library in a certain environment: for instance `ci/circleci: run_tests_pipelines` runs the pipeline tests in an environment where all pipeline-related requirements are installed.\n \n Note that to avoid running tests when there is no real change in the modules they are testing, only part of the test suite is run each time: a utility is run to determine the differences in the library between before and after the PR (what GitHub shows you in the \"Files changes\" tab) and picks the tests impacted by that diff. That utility can be run locally with:\n "
        },
        {
            "sha": "c3a4787575c04e3deb83500285e69ae07f4284d3",
            "filename": "docs/source/en/run_scripts.md",
            "status": "modified",
            "additions": 4,
            "deletions": 41,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/0031c044f8b96775a91a3cb6da366d09e94b76f1/docs%2Fsource%2Fen%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0031c044f8b96775a91a3cb6da366d09e94b76f1/docs%2Fsource%2Fen%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Frun_scripts.md?ref=0031c044f8b96775a91a3cb6da366d09e94b76f1",
            "patch": "@@ -16,13 +16,13 @@ rendered properly in your Markdown viewer.\n \n # Training scripts\n \n-Transformers provides many example training scripts for deep learning frameworks (PyTorch, TensorFlow, Flax) and tasks in [transformers/examples](https://github.com/huggingface/transformers/tree/main/examples). There are additional scripts in [transformers/research projects](https://github.com/huggingface/transformers-research-projects/) and [transformers/legacy](https://github.com/huggingface/transformers/tree/main/examples/legacy), but these aren't actively maintained and requires a specific version of Transformers.\n+Transformers provides many example training scripts for PyTorch and tasks in [transformers/examples](https://github.com/huggingface/transformers/tree/main/examples). There are additional scripts in [transformers/research projects](https://github.com/huggingface/transformers-research-projects/) and [transformers/legacy](https://github.com/huggingface/transformers/tree/main/examples/legacy), but these aren't actively maintained and requires a specific version of Transformers.\n \n Example scripts are only examples and you may need to adapt the script to your use-case. To help you with this, most scripts are very transparent in how data is preprocessed, allowing you to edit it as necessary.\n \n For any feature you'd like to implement in an example script, please discuss it on the [forum](https://discuss.huggingface.co/) or in an [issue](https://github.com/huggingface/transformers/issues) before submitting a pull request. While we welcome contributions, it is unlikely a pull request that adds more functionality is added at the cost of readability.\n \n-This guide will show you how to run an example summarization training script in [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) and [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization).\n+This guide will show you how to run an example summarization training script in [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization).\n \n ## Setup\n \n@@ -58,10 +58,7 @@ Start with a smaller dataset by including the `max_train_samples`, `max_eval_sam\n \n The example below fine-tunes [T5-small](https://huggingface.co/google-t5/t5-small) on the [CNN/DailyMail](https://huggingface.co/datasets/abisee/cnn_dailymail) dataset. T5 requires an additional `source_prefix` parameter to prompt it to summarize.\n \n-<hfoptions id=\"script\">\n-<hfoption id=\"PyTorch\">\n-\n-The example script downloads and preprocesses a dataset, and then fine-tunes it with [`Trainer`] with a supported model architecture. \n+The example script downloads and preprocesses a dataset, and then fine-tunes it with [`Trainer`] with a supported model architecture.\n \n Resuming training from a checkpoint is very useful if training is interrupted because you don't have to start over again. There are two ways to resume training from a checkpoint.\n \n@@ -116,40 +113,6 @@ python xla_spawn.py --num_cores 8 pytorch/summarization/run_summarization.py \\\n     ...\n ```\n \n-</hfoption>\n-<hfoption id=\"TensorFlow\">\n-\n-```bash\n-python examples/tensorflow/summarization/run_summarization.py  \\\n-    --model_name_or_path google-t5/t5-small \\\n-    # remove the `max_train_samples`, `max_eval_samples` and `max_predict_samples` if everything works\n-    --max_train_samples 50 \\\n-    --max_eval_samples 50 \\\n-    --max_predict_samples 50 \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval \\\n-```\n-\n-TensorFlow uses the [MirroredStrategy](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy) for distributed training and doesn't require adding any additional parameters. The script uses multiple GPUs by default if they are available.\n-\n-For TPU training, TensorFlow scripts use the [TPUStrategy](https://www.tensorflow.org/guide/distributed_training#tpustrategy). Pass the TPU resource name to the `--tpu` parameter.\n-\n-```bash\n-python run_summarization.py  \\\n-    --tpu name_of_tpu_resource \\\n-    ...\n-    ...\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n ## Accelerate\n \n [Accelerate](https://huggingface.co/docs/accelerate) is designed to simplify distributed training while offering complete visibility into the PyTorch training loop. If you're planning on training with a script with Accelerate, use the `_no_trainer.py` version of the script.\n@@ -160,7 +123,7 @@ Install Accelerate from source to ensure you have the latest version.\n pip install git+https://github.com/huggingface/accelerate\n ```\n \n-Run the [accelerate config](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config) command to answer a few questions about your training setup. This creates and saves a config file about your system. \n+Run the [accelerate config](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config) command to answer a few questions about your training setup. This creates and saves a config file about your system.\n \n ```bash\n accelerate config"
        }
    ],
    "stats": {
        "total": 199,
        "additions": 16,
        "deletions": 183
    }
}