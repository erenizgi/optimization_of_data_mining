{
    "author": "omahs",
    "message": "Fix typos (#36910)\n\n* fix typos\n\n* fix typos\n\n* fix typos\n\n* fix typos",
    "sha": "cbf924b76c03828101a34069a96d209314114fd5",
    "files": [
        {
            "sha": "856ba546b977693c261939ca8ac19296b764740c",
            "filename": "docs/source/de/quicktour.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fde%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fde%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fquicktour.md?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -156,7 +156,7 @@ Die [`pipeline`] kann jedes Modell aus dem [Model Hub](https://huggingface.co/mo\n \n <frameworkcontent>\n <pt>\n-Use the [`AutoModelForSequenceClassification`] and [`AutoTokenizer`] to load the pretrained model and it's associated tokenizer (more on an `AutoClass` below):\n+Use the [`AutoModelForSequenceClassification`] and [`AutoTokenizer`] to load the pretrained model and its associated tokenizer (more on an `AutoClass` below):\n \n ```py\n >>> from transformers import AutoTokenizer, AutoModelForSequenceClassification\n@@ -166,7 +166,7 @@ Use the [`AutoModelForSequenceClassification`] and [`AutoTokenizer`] to load the\n ```\n </pt>\n <tf>\n-Use the [`TFAutoModelForSequenceClassification`] and [`AutoTokenizer`] to load the pretrained model and it's associated tokenizer (more on an `TFAutoClass` below):\n+Use the [`TFAutoModelForSequenceClassification`] and [`AutoTokenizer`] to load the pretrained model and its associated tokenizer (more on an `TFAutoClass` below):\n \n ```py\n >>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n@@ -222,7 +222,7 @@ Anschließend wandelt der Tokenizer die Token in Zahlen um, um einen Tensor als\n Der Tokenizer gibt ein Wörterbuch zurück, das Folgendes enthält:\n \n * [input_ids](./glossary#input-ids): numerische Repräsentationen Ihrer Token.\n-* [atttention_mask](.glossary#attention-mask): gibt an, welche Token beachtet werden sollen.\n+* [attention_mask](.glossary#attention-mask): gibt an, welche Token beachtet werden sollen.\n \n Genau wie die [`pipeline`] akzeptiert der Tokenizer eine Liste von Eingaben. Darüber hinaus kann der Tokenizer den Text auch auffüllen und kürzen, um einen Stapel mit einheitlicher Länge zurückzugeben:\n "
        },
        {
            "sha": "59496e4298fc602a1684b37fe34c652edd4baea5",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -9,7 +9,7 @@ Unless required by applicable law or agreed to in writing, software distributed\n an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n specific language governing permissions and limitations under the License.\n \n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+⚠️ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\n rendered properly in your Markdown viewer.\n \n -->\n@@ -62,7 +62,7 @@ for _ in range(max_new_tokens):\n     # Greedily sample one next token\n     next_token_ids = outputs.logits[:, -1:].argmax(-1)\n     generated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)\n-    # Prepare inputs for the next generation step by leaaving unprocessed tokens, in our case we have only one new token\n+    # Prepare inputs for the next generation step by leaving unprocessed tokens, in our case we have only one new token\n     # and expanding attn mask for the new token, as explained above\n     attention_mask = inputs[\"attention_mask\"]\n     attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\n@@ -88,7 +88,7 @@ model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", to\n inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n \n # `return_dict_in_generate=True` is required to return the cache and `return_legacy_cache` forces the returned cache\n-# in the the legacy format\n+# in the legacy format\n generation_outputs = model.generate(**inputs, return_dict_in_generate=True, return_legacy_cache=True, max_new_tokens=5)\n \n cache = DynamicCache.from_legacy_cache(generation_outputs.past_key_values)"
        },
        {
            "sha": "6319f00b97b9f7f465b7b80c17dc08d22668a6fe",
            "filename": "docs/source/en/chat_templating_multimodal.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -9,7 +9,7 @@ Unless required by applicable law or agreed to in writing, software distributed\n an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n specific language governing permissions and limitations under the License.\n \n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+⚠️ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\n rendered properly in your Markdown viewer.\n \n -->\n@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n Multimodal model chat templates expect a similar [template](./chat_templating) as text-only models. It needs `messages` that includes a dictionary of the `role` and `content`.\n \n-Multimodal templates are included in the [Processor](./processors) class and requires an additional `type` key for specifying whether the included content is an image, video, or text.\n+Multimodal templates are included in the [Processor](./processors) class and require an additional `type` key for specifying whether the included content is an image, video, or text.\n \n This guide will show you how to format chat templates for multimodal models as well as some best practices for configuring the template\n \n@@ -109,7 +109,7 @@ These inputs are now ready to be used in [`~GenerationMixin.generate`].\n \n Some vision models also support video inputs. The message format is very similar to the format for [image inputs](#image-inputs).\n \n-- The content `\"type\"` should be `\"video\"` to indicate the the content is a video.\n+- The content `\"type\"` should be `\"video\"` to indicate the content is a video.\n - For videos, it can be a link to the video (`\"url\"`) or it could be a file path (`\"path\"`). Videos loaded from a URL can only be decoded with [PyAV](https://pyav.basswood-io.com/docs/stable/) or [Decord](https://github.com/dmlc/decord).\n \n > [!WARNING]\n@@ -141,7 +141,7 @@ Pass `messages` to [`~ProcessorMixin.apply_chat_template`] to tokenize the input\n \n The `video_load_backend` parameter refers to a specific framework to load a video. It supports [PyAV](https://pyav.basswood-io.com/docs/stable/), [Decord](https://github.com/dmlc/decord), [OpenCV](https://github.com/opencv/opencv), and [torchvision](https://pytorch.org/vision/stable/index.html).\n \n-The examples below uses Decord as the backend because it is a bit faster than PyAV.\n+The examples below use Decord as the backend because it is a bit faster than PyAV.\n \n <hfoptions id=\"sampling\">\n <hfoption id=\"fixed number of frames\">"
        },
        {
            "sha": "592aa9aa105f15a0248f04fe0e81d750195c0840",
            "filename": "docs/source/en/custom_models.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fen%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fen%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcustom_models.md?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -131,7 +131,7 @@ class ResnetModel(PreTrainedModel):\n </hfoption>\n <hfoption id=\"ResnetModelForImageClassification\">\n \n-The `forward` method needs to be rewrittten to calculate the loss for each logit if labels are available. Otherwise, the ResNet model class is the same.\n+The `forward` method needs to be rewritten to calculate the loss for each logit if labels are available. Otherwise, the ResNet model class is the same.\n \n > [!TIP]\n > Add `config_class` to the model class to enable [AutoClass](#autoclass-support) support."
        },
        {
            "sha": "57623ed74a14f7b71ce995785e131e220e8d7d85",
            "filename": "docs/source/en/gpu_selection.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fen%2Fgpu_selection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fen%2Fgpu_selection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgpu_selection.md?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -9,7 +9,7 @@ Unless required by applicable law or agreed to in writing, software distributed\n an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n specific language governing permissions and limitations under the License.\n \n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+⚠️ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\n rendered properly in your Markdown viewer.\n \n -->\n@@ -56,7 +56,7 @@ deepspeed --num_gpus 2 trainer-program.py ...\n \n ### Order of GPUs\n \n-To select specific GPUs to use and their order, configure the the `CUDA_VISIBLE_DEVICES` environment variable. It is easiest to set the environment variable in `~/bashrc` or another startup config file. `CUDA_VISIBLE_DEVICES` is used to map which GPUs are used. For example, if there are 4 GPUs (0, 1, 2, 3) and you only want to run GPUs 0 and 2:\n+To select specific GPUs to use and their order, configure the `CUDA_VISIBLE_DEVICES` environment variable. It is easiest to set the environment variable in `~/bashrc` or another startup config file. `CUDA_VISIBLE_DEVICES` is used to map which GPUs are used. For example, if there are 4 GPUs (0, 1, 2, 3) and you only want to run GPUs 0 and 2:\n \n ```bash\n CUDA_VISIBLE_DEVICES=0,2 torchrun trainer-program.py ..."
        },
        {
            "sha": "41b9c54002822cc7c04a476534e162bfc87757d3",
            "filename": "docs/source/es/quicktour.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fes%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fes%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fquicktour.md?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -220,7 +220,7 @@ Pasa tu texto al tokenizador:\n El tokenizador devolverá un diccionario conteniendo:\n \n * [input_ids](./glossary#input-ids): representaciones numéricas de los tokens.\n-* [atttention_mask](.glossary#attention-mask): indica cuáles tokens deben ser atendidos.\n+* [attention_mask](.glossary#attention-mask): indica cuáles tokens deben ser atendidos.\n \n Como con el [`pipeline`], el tokenizador aceptará una lista de inputs. Además, el tokenizador también puede rellenar (pad, en inglés) y truncar el texto para devolver un lote (batch, en inglés) de longitud uniforme:\n "
        },
        {
            "sha": "5bf48e4737d9412b7d3ca257709451f80d3928ad",
            "filename": "docs/source/it/perf_infer_cpu.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fit%2Fperf_infer_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fit%2Fperf_infer_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fperf_infer_cpu.md?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -23,7 +23,7 @@ Abbiamo integrato di recente `BetterTransformer` per fare inferenza più rapidam\n \n ## PyTorch JIT-mode (TorchScript)\n \n-TorchScript è un modo di creare modelli serializzabili e ottimizzabili da codice PyTorch. Ogni programmma TorchScript può esere salvato da un processo Python  e caricato in un processo dove non ci sono dipendenze Python.\n+TorchScript è un modo di creare modelli serializzabili e ottimizzabili da codice PyTorch. Ogni programma TorchScript può esere salvato da un processo Python  e caricato in un processo dove non ci sono dipendenze Python.\n Comparandolo con l'eager mode di default, jit mode in PyTorch normalmente fornisce prestazioni migliori per l'inferenza del modello da parte di metodologie di ottimizzazione come la operator fusion.\n \n Per una prima introduzione a TorchScript, vedi la Introduction to [PyTorch TorchScript tutorial](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html#tracing-modules)."
        },
        {
            "sha": "5ccdd63376e3984885c234cd4bab24283e9d5d6e",
            "filename": "docs/source/pt/quicktour.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fpt%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/docs%2Fsource%2Fpt%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Fquicktour.md?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -222,7 +222,7 @@ Passe o texto para o tokenizer:\n O tokenizer retornará um dicionário contendo:\n \n * [input_ids](./glossary#input-ids): representações numéricas de seus tokens.\n-* [atttention_mask](.glossary#attention-mask): indica quais tokens devem ser atendidos.\n+* [attention_mask](.glossary#attention-mask): indica quais tokens devem ser atendidos.\n \n Assim como o [`pipeline`], o tokenizer aceitará uma lista de entradas. Além disso, o tokenizer também pode preencher e truncar o texto para retornar um lote com comprimento uniforme:\n "
        },
        {
            "sha": "bfb812340e72bdc4543581d1e53bafb6730eb622",
            "filename": "src/transformers/commands/add_new_model_like.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -918,7 +918,7 @@ def add_model_to_main_init(\n         new_model_patterns (`ModelPatterns`): The patterns for the new model.\n         frameworks (`List[str]`, *optional*):\n             If specified, only the models implemented in those frameworks will be added.\n-        with_processsing (`bool`, *optional*, defaults to `True`):\n+        with_processing (`bool`, *optional*, defaults to `True`):\n             Whether the tokenizer/feature extractor/processor of the model should also be added to the init or not.\n     \"\"\"\n     with open(TRANSFORMERS_PATH / \"__init__.py\", \"r\", encoding=\"utf-8\") as f:"
        },
        {
            "sha": "490491778a4558432dd2b8115dfc10db380444ec",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -94,7 +94,7 @@\n     list[\"np.ndarray\"],\n     list[\"torch.Tensor\"],\n     list[list[\"PIL.Image.Image\"]],\n-    list[list[\"np.ndarrray\"]],\n+    list[list[\"np.ndarray\"]],\n     list[list[\"torch.Tensor\"]],\n ]  # noqa\n "
        },
        {
            "sha": "628f05b2e01bd6e12f0904c54f6c43b10ece33c0",
            "filename": "src/transformers/models/align/processing_align.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -83,7 +83,7 @@ def __call__(\n         arguments to BertTokenizerFast's [`~BertTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` arguments to\n         EfficientNetImageProcessor's [`~EfficientNetImageProcessor.__call__`] if `images` is not `None`. Please refer\n-        to the doctsring of the above two methods for more information.\n+        to the docstring of the above two methods for more information.\n \n         Args:\n             images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):"
        },
        {
            "sha": "3ce4f2481d4905c651d1abcbe7fc020e92d2c001",
            "filename": "src/transformers/models/altclip/processing_altclip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -68,7 +68,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to XLMRobertaTokenizerFast's [`~XLMRobertaTokenizerFast.__call__`] if `text` is not\n         `None` to encode the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "74b3d66167d5b56a1cfce28c86af8f376b9cc711",
            "filename": "src/transformers/models/auto/modeling_flax_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_flax_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_flax_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_flax_auto.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -123,7 +123,7 @@\n \n FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES = OrderedDict(\n     [\n-        # Model for Image-classsification\n+        # Model for Image-classification\n         (\"beit\", \"FlaxBeitForImageClassification\"),\n         (\"dinov2\", \"FlaxDinov2ForImageClassification\"),\n         (\"regnet\", \"FlaxRegNetForImageClassification\"),"
        },
        {
            "sha": "36ac30ccca430c30315006df12341bea2dab1a09",
            "filename": "src/transformers/models/bamba/configuration_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -39,7 +39,7 @@ class BambaConfig(PretrainedConfig):\n             `inputs_ids` passed when calling [`BambaModel`]\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the\n-            model has a output word embedding layer.\n+            model has an output word embedding layer.\n         hidden_size (`int`, *optional*, defaults to 4096):\n             Dimension of the hidden representations.\n         intermediate_size (`int`, *optional*, defaults to 14336):\n@@ -85,7 +85,7 @@ class BambaConfig(PretrainedConfig):\n         mamba_n_heads (`int`, *optional*, defaults to 128):\n             The number of mamba heads used in the v2 implementation.\n         mamba_d_head (`int`, *optional*, defaults to `\"auto\"`):\n-            Head embeddding dimension size\n+            Head embedding dimension size\n         mamba_n_groups (`int`, *optional*, defaults to 1):\n             The number of the mamba groups used in the v2 implementation.\n         mamba_d_state (`int`, *optional*, defaults to 256):"
        },
        {
            "sha": "af1b000a3708a043356227eebd316d08dc029bc6",
            "filename": "src/transformers/models/bark/convert_suno_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fbark%2Fconvert_suno_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fbark%2Fconvert_suno_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fconvert_suno_to_hf.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -190,12 +190,12 @@ def load_model(pytorch_dump_folder_path, use_small=False, model_type=\"text\"):\n         output_new_model = output_new_model_total.logits[:, [-1], :]\n \n     else:\n-        prediction_codeboook_channel = 3\n+        prediction_codebook_channel = 3\n         n_codes_total = 8\n         vec = torch.randint(256, (batch_size, sequence_length, n_codes_total), dtype=torch.int)\n \n-        output_new_model_total = model(prediction_codeboook_channel, vec)\n-        output_old_model = bark_model(prediction_codeboook_channel, vec)\n+        output_new_model_total = model(prediction_codebook_channel, vec)\n+        output_old_model = bark_model(prediction_codebook_channel, vec)\n \n         output_new_model = output_new_model_total.logits\n "
        },
        {
            "sha": "5c80c7c6c401b1a594eb7ce9a75a54a464fab56b",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -87,7 +87,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "958adfdd0aea45b7d27e23097b1a76e347a16061",
            "filename": "src/transformers/models/chinese_clip/processing_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -78,7 +78,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to BertTokenizerFast's [`~BertTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "126fc384ebfbfb53a55c08237ed1e951968bed10",
            "filename": "src/transformers/models/clap/processing_clap.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -46,7 +46,7 @@ def __call__(self, text=None, audios=None, return_tensors=None, **kwargs):\n         and `kwargs` arguments to RobertaTokenizerFast's [`~RobertaTokenizerFast.__call__`] if `text` is not `None` to\n         encode the text. To prepare the audio(s), this method forwards the `audios` and `kwrags` arguments to\n         ClapFeatureExtractor's [`~ClapFeatureExtractor.__call__`] if `audios` is not `None`. Please refer to the\n-        doctsring of the above two methods for more information.\n+        docstring of the above two methods for more information.\n \n         Args:\n             text (`str`, `List[str]`, `List[List[str]]`):"
        },
        {
            "sha": "6f835fb313b731153a964844aa305a1bfec253c5",
            "filename": "src/transformers/models/clip/processing_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -63,7 +63,7 @@ def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "5a4c10930f727e12de8eb32ee3e14c8654927ae2",
            "filename": "src/transformers/models/clipseg/processing_clipseg.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -63,7 +63,7 @@ def __call__(self, text=None, images=None, visual_prompt=None, return_tensors=No\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        ViTImageProcessor's [`~ViTImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring of\n+        ViTImageProcessor's [`~ViTImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring of\n         the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "6a733030ee463901425b098423ad8224080d1717",
            "filename": "src/transformers/models/clvp/processing_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -48,7 +48,7 @@ def __init__(self, feature_extractor, tokenizer):\n     def __call__(self, *args, **kwargs):\n         \"\"\"\n         Forwards the `audio` and `sampling_rate` arguments to [`~ClvpFeatureExtractor.__call__`] and the `text`\n-        argument to [`~ClvpTokenizer.__call__`]. Please refer to the doctsring of the above two methods for more\n+        argument to [`~ClvpTokenizer.__call__`]. Please refer to the docstring of the above two methods for more\n         information.\n         \"\"\"\n "
        },
        {
            "sha": "e5bd804abd046a7a047f6d2003f83cdc88fb4e26",
            "filename": "src/transformers/models/colpali/modular_colpali.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -100,11 +100,11 @@ def __call__(\n         wrapper around the PaliGemmaProcessor's [`~PaliGemmaProcessor.__call__`] method adapted for the ColPali model. It cannot process\n         both text and images at the same time.\n \n-        When preparing the the text(s), this method forwards the `text` and `kwargs` arguments to LlamaTokenizerFast's\n+        When preparing the text(s), this method forwards the `text` and `kwargs` arguments to LlamaTokenizerFast's\n         [`~LlamaTokenizerFast.__call__`].\n-        When preparing the the image(s), this method forwards the `images` and `kwargs` arguments to SiglipImageProcessor's\n+        When preparing the image(s), this method forwards the `images` and `kwargs` arguments to SiglipImageProcessor's\n         [`~SiglipImageProcessor.__call__`].\n-        Please refer to the doctsring of the above two methods for more information.\n+        Please refer to the docstring of the above two methods for more information.\n \n         Args:\n             images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):"
        },
        {
            "sha": "eeb14901f7d3ca6e787e0b6ce67b26a7bbf82b9b",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -140,11 +140,11 @@ def __call__(\n         wrapper around the PaliGemmaProcessor's [`~PaliGemmaProcessor.__call__`] method adapted for the ColPali model. It cannot process\n         both text and images at the same time.\n \n-        When preparing the the text(s), this method forwards the `text` and `kwargs` arguments to LlamaTokenizerFast's\n+        When preparing the text(s), this method forwards the `text` and `kwargs` arguments to LlamaTokenizerFast's\n         [`~LlamaTokenizerFast.__call__`].\n-        When preparing the the image(s), this method forwards the `images` and `kwargs` arguments to SiglipImageProcessor's\n+        When preparing the image(s), this method forwards the `images` and `kwargs` arguments to SiglipImageProcessor's\n         [`~SiglipImageProcessor.__call__`].\n-        Please refer to the doctsring of the above two methods for more information.\n+        Please refer to the docstring of the above two methods for more information.\n \n         Args:\n             images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):"
        },
        {
            "sha": "299757545e67ffa83b4441c0e0a4ee193311f075",
            "filename": "src/transformers/models/deprecated/jukebox/modeling_jukebox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -1303,7 +1303,7 @@ def __init__(\n             n_ctx (`int`, *optional*):\n                 Number of tokens or lyrics tokens provided in a single pass.\n             embed_dim (`int`, *optional*):\n-                Either equals to the dimension of the codebook, or the sum of n_vocab (lyrics) and codeboook dimension,\n+                Either equals to the dimension of the codebook, or the sum of n_vocab (lyrics) and codebook dimension,\n                 if the model combines lyrics and music tokens, or simply n_vocab if the model is a seperate encoder\n             audio_conditioning (`bool`, *optional*, defaults to `False`):\n                 Whether or not the prior supports conditionning on audio.\n@@ -1921,7 +1921,7 @@ def get_metadata(self, labels, start, total_length, offset, get_indices=False):\n \n     def set_metadata_lyric_tokens(self, labels):\n         \"\"\"\n-        Processes the full labels to only retreive the relevant lyric tokens and keep the metadata conditioning tokens.\n+        Processes the full labels to only retrieve the relevant lyric tokens and keep the metadata conditioning tokens.\n         \"\"\"\n         if self.nb_relevant_lyric_tokens > 0:\n             tokens_list = torch.zeros(\n@@ -2147,7 +2147,7 @@ def sample(\n \n     def get_encoder_states(self, lyric_tokens, sample=False):\n         \"\"\"\n-        Retreive the last hidden_states of the lyric encoder that will be attended to by the decoder. Forwards through\n+        Retrieve the last hidden_states of the lyric encoder that will be attended to by the decoder. Forwards through\n         the lyric encoder.\n         \"\"\"\n         if self.nb_relevant_lyric_tokens != 0 and self.lyric_conditioning:"
        },
        {
            "sha": "7dcbefe1018d4fb9275e3bfe06cd17f4b13667fd",
            "filename": "src/transformers/models/deprecated/mctct/processing_mctct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fprocessing_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fprocessing_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fprocessing_mctct.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -49,7 +49,7 @@ def __call__(self, *args, **kwargs):\n         When used in normal mode, this method forwards all its arguments to MCTCTFeatureExtractor's\n         [`~MCTCTFeatureExtractor.__call__`] and returns its output. If used in the context\n         [`~MCTCTProcessor.as_target_processor`] this method forwards all its arguments to AutoTokenizer's\n-        [`~AutoTokenizer.__call__`]. Please refer to the doctsring of the above two methods for more information.\n+        [`~AutoTokenizer.__call__`]. Please refer to the docstring of the above two methods for more information.\n         \"\"\"\n         # For backward compatibility\n         if self._in_target_context_manager:"
        },
        {
            "sha": "f3eb696f893dac3e0e854c18f53feae00c51440a",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/processing_speech_to_text_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fprocessing_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fprocessing_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fprocessing_speech_to_text_2.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -50,7 +50,7 @@ def __call__(self, *args, **kwargs):\n         When used in normal mode, this method forwards all its arguments to AutoFeatureExtractor's\n         [`~AutoFeatureExtractor.__call__`] and returns its output. If used in the context\n         [`~Speech2Text2Processor.as_target_processor`] this method forwards all its arguments to\n-        Speech2Text2Tokenizer's [`~Speech2Text2Tokenizer.__call__`]. Please refer to the doctsring of the above two\n+        Speech2Text2Tokenizer's [`~Speech2Text2Tokenizer.__call__`]. Please refer to the docstring of the above two\n         methods for more information.\n         \"\"\"\n         # For backward compatibility"
        },
        {
            "sha": "04ddf901c611687addbdbf8bfcbf4d3e5c12f332",
            "filename": "src/transformers/models/donut/processing_donut.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -86,7 +86,7 @@ def __call__(\n         When used in normal mode, this method forwards all its arguments to AutoImageProcessor's\n         [`~AutoImageProcessor.__call__`] and returns its output. If used in the context\n         [`~DonutProcessor.as_target_processor`] this method forwards all its arguments to DonutTokenizer's\n-        [`~DonutTokenizer.__call__`]. Please refer to the doctsring of the above two methods for more information.\n+        [`~DonutTokenizer.__call__`]. Please refer to the docstring of the above two methods for more information.\n         \"\"\"\n         # For backward compatibility\n         legacy = kwargs.pop(\"legacy\", True)"
        },
        {
            "sha": "ff0e68162321dbec09754748329834bf8814b398",
            "filename": "src/transformers/models/emu3/processing_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -95,7 +95,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to Emu3TokenizerFast's [`~Emu3TokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "c295d94a9df968661c6a985b366d883d39b534c3",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -481,7 +481,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to\n         encode the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        FuyuImageProcessor's [`~FuyuImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        FuyuImageProcessor's [`~FuyuImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "2f1b35cb7cb909dac6f2edeecbf59fe81accd467",
            "filename": "src/transformers/models/git/processing_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -66,7 +66,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to BertTokenizerFast's [`~BertTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "c477938250372a6db93db39883e95a2e32fee18c",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -103,7 +103,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "3c2c469e2fd3f52ce72316bfae522ae3e52a5087",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -106,7 +106,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "ffa7b2cceb6eb9ef4ddd17c064961713f5086083",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -123,7 +123,7 @@ def __call__(\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n         LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. To prepare the video(s),\n         this method forwards the `videos` and `kwrags` arguments to LlavaNextVideoImageProcessor's\n-        [`~LlavaNextVideoImageProcessor.__call__`] if `videos` is not `None`. Please refer to the doctsring\n+        [`~LlavaNextVideoImageProcessor.__call__`] if `videos` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "502a514871723e675bc45b6d98577949911340ec",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -114,7 +114,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "f15c7af6d423ffcaa5ac07c73baec7aebe3d9313",
            "filename": "src/transformers/models/mgp_str/processing_mgp_str.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -81,7 +81,7 @@ def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n         When used in normal mode, this method forwards all its arguments to ViTImageProcessor's\n         [`~ViTImageProcessor.__call__`] and returns its output. This method also forwards the `text` and `kwargs`\n         arguments to MgpstrTokenizer's [`~MgpstrTokenizer.__call__`] if `text` is not `None` to encode the text. Please\n-        refer to the doctsring of the above methods for more information.\n+        refer to the docstring of the above methods for more information.\n         \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You need to specify either an `images` or `text` input to process.\")"
        },
        {
            "sha": "82671e4bf4cc25573528a9a849780fa7b3b8a5f8",
            "filename": "src/transformers/models/musicgen/processing_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -53,7 +53,7 @@ def get_decoder_prompt_ids(self, task=None, language=None, no_timestamps=True):\n     def __call__(self, *args, **kwargs):\n         \"\"\"\n         Forwards the `audio` argument to EncodecFeatureExtractor's [`~EncodecFeatureExtractor.__call__`] and the `text`\n-        argument to [`~T5Tokenizer.__call__`]. Please refer to the doctsring of the above two methods for more\n+        argument to [`~T5Tokenizer.__call__`]. Please refer to the docstring of the above two methods for more\n         information.\n         \"\"\"\n         # For backward compatibility"
        },
        {
            "sha": "8cf11e67d46d4d111cdf7a02ba045126060fc5f5",
            "filename": "src/transformers/models/musicgen_melody/processing_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -54,7 +54,7 @@ def __call__(self, audio=None, text=None, **kwargs):\n         Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `audio`\n         and `kwargs` arguments to MusicgenMelodyFeatureExtractor's [`~MusicgenMelodyFeatureExtractor.__call__`] if `audio` is not\n         `None` to pre-process the audio. It also forwards the `text` and `kwargs` arguments to\n-        PreTrainedTokenizer's [`~PreTrainedTokenizer.__call__`] if `text` is not `None`. Please refer to the doctsring of the above two methods for more information.\n+        PreTrainedTokenizer's [`~PreTrainedTokenizer.__call__`] if `text` is not `None`. Please refer to the docstring of the above two methods for more information.\n \n         Args:\n             audio (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):"
        },
        {
            "sha": "d3e02f50d8170833c54cd77f85669c7511227c03",
            "filename": "src/transformers/models/oneformer/processing_oneformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Foneformer%2Fprocessing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Foneformer%2Fprocessing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fprocessing_oneformer.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -82,7 +82,7 @@ def __call__(self, images=None, task_inputs=None, segmentation_maps=None, **kwar\n         `task_inputs` and `kwargs` arguments to CLIPTokenizer's [`~CLIPTokenizer.__call__`] if `task_inputs` is not\n         `None` to encode. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         OneFormerImageProcessor's [`~OneFormerImageProcessor.__call__`] if `images` is not `None`. Please refer to the\n-        doctsring of the above two methods for more information.\n+        docstring of the above two methods for more information.\n \n         Args:\n             task_inputs (`str`, `List[str]`):"
        },
        {
            "sha": "4996cae7ab9694e2114cdc046ca95837a870158f",
            "filename": "src/transformers/models/owlv2/processing_owlv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -96,7 +96,7 @@ def __call__(\n         Main method to prepare for the model one or several text(s) and image(s). This method forwards the `text` and\n         `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode:\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "859e28bfcc8b335a61f7dd78aa486b368f2f7f8e",
            "filename": "src/transformers/models/owlvit/processing_owlvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -110,7 +110,7 @@ def __call__(\n         Main method to prepare for the model one or several text(s) and image(s). This method forwards the `text` and\n         `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode:\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "f988d43583f69494023c7bf48ede40ee27646ee6",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -162,7 +162,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to GemmaTokenizerFast's [`~GemmaTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        SiglipImageProcessor's [`~SiglipImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        SiglipImageProcessor's [`~SiglipImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         The usage for PaliGemma fine-tuning preparation is slightly different than usual. suffix passed are suffixes to"
        },
        {
            "sha": "853d12e6fea0d1ad783ed19f23304fc79e2ad816",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -119,7 +119,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "934332d7ac6f80846a13864482d0e685daa98cd4",
            "filename": "src/transformers/models/qwen2_audio/processing_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -80,7 +80,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `text`\n         and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the audio(s), this method forwards the `audios` and `kwrags` arguments to\n-        WhisperFeatureExtractor's [`~WhisperFeatureExtractor.__call__`] if `audios` is not `None`. Please refer to the doctsring\n+        WhisperFeatureExtractor's [`~WhisperFeatureExtractor.__call__`] if `audios` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "9fd0a4c634d5921417fda15c3fd02e1c01d5e000",
            "filename": "src/transformers/models/regnet/modeling_regnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -80,7 +80,7 @@ def forward(self, hidden_state):\n \n class RegNetEmbeddings(nn.Module):\n     \"\"\"\n-    RegNet Embedddings (stem) composed of a single aggressive convolution.\n+    RegNet Embeddings (stem) composed of a single aggressive convolution.\n     \"\"\"\n \n     def __init__(self, config: RegNetConfig):"
        },
        {
            "sha": "90d96b61c99c1f27f4d9d81189bee43c8be5974a",
            "filename": "src/transformers/models/seamless_m4t/processing_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -47,7 +47,7 @@ def __call__(self, text=None, audios=None, src_lang=None, tgt_lang=None, **kwarg\n         and `kwargs` arguments to SeamlessM4TTokenizerFast's [`~SeamlessM4TTokenizerFast.__call__`] if `text` is not\n         `None` to encode the text. To prepare the audio(s), this method forwards the `audios` and `kwrags` arguments to\n         SeamlessM4TFeatureExtractor's [`~SeamlessM4TFeatureExtractor.__call__`] if `audios` is not `None`. Please refer\n-        to the doctsring of the above two methods for more information.\n+        to the docstring of the above two methods for more information.\n \n         Args:\n             text (`str`, `List[str]`, `List[List[str]]`):"
        },
        {
            "sha": "4ff30ffb23a04e99ba9890354466754dfaa5c5dc",
            "filename": "src/transformers/models/siglip/processing_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -59,7 +59,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to SiglipTokenizer's [`~SiglipTokenizer.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` argument to\n-        SiglipImageProcessor's [`~SiglipImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        SiglipImageProcessor's [`~SiglipImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "16e0ea1b6b8df5ef74dc9f1472d779407dc16c08",
            "filename": "src/transformers/models/siglip2/processing_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -79,7 +79,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to GemmaTokenizerFast's [`~GemmaTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` argument to\n-        Siglip2ImageProcessor's [`~Siglip2ImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        Siglip2ImageProcessor's [`~Siglip2ImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "724c0a6ed0f77877b9de500cc212c8da5c7fcc25",
            "filename": "src/transformers/models/speech_to_text/processing_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fprocessing_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fprocessing_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fprocessing_speech_to_text.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -51,7 +51,7 @@ def __call__(self, *args, **kwargs):\n         When used in normal mode, this method forwards all its arguments to Speech2TextFeatureExtractor's\n         [`~Speech2TextFeatureExtractor.__call__`] and returns its output. If used in the context\n         [`~Speech2TextProcessor.as_target_processor`] this method forwards all its arguments to Speech2TextTokenizer's\n-        [`~Speech2TextTokenizer.__call__`]. Please refer to the doctsring of the above two methods for more\n+        [`~Speech2TextTokenizer.__call__`]. Please refer to the docstring of the above two methods for more\n         information.\n         \"\"\"\n         # For backward compatibility"
        },
        {
            "sha": "6fb5f281ecdfa81994e0e45cbcfb49e7f65df30d",
            "filename": "src/transformers/models/trocr/processing_trocr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -81,7 +81,7 @@ def __call__(\n         When used in normal mode, this method forwards all its arguments to AutoImageProcessor's\n         [`~AutoImageProcessor.__call__`] and returns its output. If used in the context\n         [`~TrOCRProcessor.as_target_processor`] this method forwards all its arguments to TrOCRTokenizer's\n-        [`~TrOCRTokenizer.__call__`]. Please refer to the doctsring of the above two methods for more information.\n+        [`~TrOCRTokenizer.__call__`]. Please refer to the docstring of the above two methods for more information.\n         \"\"\"\n         # For backward compatibility\n         if self._in_target_context_manager:"
        },
        {
            "sha": "76baae91346cfe21ddc506d2bd61347cdc0d82c3",
            "filename": "src/transformers/models/tvp/processing_tvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -51,15 +51,15 @@ def __call__(self, text=None, videos=None, return_tensors=None, **kwargs):\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to BertTokenizerFast's [`~BertTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `videos` and `kwargs` arguments to\n-        TvpImageProcessor's [`~TvpImageProcessor.__call__`] if `videos` is not `None`. Please refer to the doctsring of\n+        TvpImageProcessor's [`~TvpImageProcessor.__call__`] if `videos` is not `None`. Please refer to the docstring of\n         the above two methods for more information.\n \n         Args:\n             text (`str`, `List[str]`, `List[List[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`, `List[List[PIL.Image.Image]]`, `List[List[np.ndarrray]]`,:\n+            videos (`List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`, `List[List[PIL.Image.Image]]`, `List[List[np.ndarray]]`,:\n                 `List[List[torch.Tensor]]`): The video or batch of videos to be prepared. Each video should be a list\n                 of frames, which can be either PIL images or NumPy arrays. In case of NumPy arrays/PyTorch tensors,\n                 each frame should be of shape (H, W, C), where H and W are frame height and width, and C is a number of"
        },
        {
            "sha": "4806720b3701b9d3c6753a6645b2314e0b0beb77",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -103,7 +103,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        VideoLlavaImageProcessor's [`~VideoLlavaImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        VideoLlavaImageProcessor's [`~VideoLlavaImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "0b12bae8f7ef2aea1002048719a0883edbc76aad",
            "filename": "src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -66,7 +66,7 @@ def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to VisionTextDualEncoderTokenizer's [`~PreTrainedTokenizer.__call__`] if `text` is not\n         `None` to encode the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        AutoImageProcessor's [`~AutoImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        AutoImageProcessor's [`~AutoImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "dbd40c94ea5b99c2334075ecf4fa5e49843ff140",
            "filename": "src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -81,7 +81,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `audio`\n         and `kwargs` arguments to SeamlessM4TFeatureExtractor's [`~SeamlessM4TFeatureExtractor.__call__`] if `audio` is not\n         `None` to pre-process the audio. To prepare the target sequences(s), this method forwards the `text` and `kwargs` arguments to\n-        PreTrainedTokenizer's [`~PreTrainedTokenizer.__call__`] if `text` is not `None`. Please refer to the doctsring of the above two methods for more information.\n+        PreTrainedTokenizer's [`~PreTrainedTokenizer.__call__`] if `text` is not `None`. Please refer to the docstring of the above two methods for more information.\n \n         Args:\n             audio (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n@@ -127,7 +127,7 @@ def pad(self, input_features=None, labels=None, **kwargs):\n         \"\"\"\n         If `input_features` is not `None`, this method forwards the `input_features` and `kwargs` arguments to SeamlessM4TFeatureExtractor's [`~SeamlessM4TFeatureExtractor.pad`] to pad the input features.\n         If `labels` is not `None`, this method forwards the `labels` and `kwargs` arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.pad`] to pad the label(s).\n-        Please refer to the doctsring of the above two methods for more information.\n+        Please refer to the docstring of the above two methods for more information.\n         \"\"\"\n         if input_features is None and labels is None:\n             raise ValueError(\"You need to specify either an `input_features` or `labels` input to pad.\")"
        },
        {
            "sha": "b5f703c6cae070b574a13439ffc64674fc294d2b",
            "filename": "src/transformers/models/whisper/processing_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fwhisper%2Fprocessing_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fwhisper%2Fprocessing_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fprocessing_whisper.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -48,7 +48,7 @@ def get_decoder_prompt_ids(self, task=None, language=None, no_timestamps=True):\n     def __call__(self, *args, **kwargs):\n         \"\"\"\n         Forwards the `audio` argument to WhisperFeatureExtractor's [`~WhisperFeatureExtractor.__call__`] and the `text`\n-        argument to [`~WhisperTokenizer.__call__`]. Please refer to the doctsring of the above two methods for more\n+        argument to [`~WhisperTokenizer.__call__`]. Please refer to the docstring of the above two methods for more\n         information.\n         \"\"\"\n         # For backward compatibility"
        },
        {
            "sha": "66568a4fee270eed0c2f08a422617b2d3789a6f3",
            "filename": "src/transformers/models/x_clip/processing_x_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -65,14 +65,14 @@ def __call__(self, text=None, videos=None, return_tensors=None, **kwargs):\n         and `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `videos` and `kwargs` arguments to\n         VideoMAEImageProcessor's [`~VideoMAEImageProcessor.__call__`] if `videos` is not `None`. Please refer to the\n-        doctsring of the above two methods for more information.\n+        docstring of the above two methods for more information.\n \n         Args:\n             text (`str`, `List[str]`, `List[List[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`, `List[List[PIL.Image.Image]]`, `List[List[np.ndarrray]]`,:\n+            videos (`List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`, `List[List[PIL.Image.Image]]`, `List[List[np.ndarray]]`,:\n                 `List[List[torch.Tensor]]`): The video or batch of videos to be prepared. Each video should be a list\n                 of frames, which can be either PIL images or NumPy arrays. In case of NumPy arrays/PyTorch tensors,\n                 each frame should be of shape (H, W, C), where H and W are frame height and width, and C is a number of"
        },
        {
            "sha": "1e510b5483a7aed8a1b2fad276e1cb0d28b9d47f",
            "filename": "src/transformers/onnx/config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fonnx%2Fconfig.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fonnx%2Fconfig.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2Fconfig.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -337,7 +337,7 @@ def generate_dummy_inputs(\n                 \" `preprocessor` instead.\",\n                 FutureWarning,\n             )\n-            logger.warning(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummmy inputs.\")\n+            logger.warning(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummy inputs.\")\n             preprocessor = tokenizer\n         if isinstance(preprocessor, PreTrainedTokenizerBase):\n             # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX"
        },
        {
            "sha": "a73b6b927d0f6a76091580e90c0c90e662c17664",
            "filename": "src/transformers/onnx/convert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fonnx%2Fconvert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fonnx%2Fconvert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2Fconvert.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -118,7 +118,7 @@ def export_pytorch(\n             \" `preprocessor` instead.\",\n             FutureWarning,\n         )\n-        logger.info(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummmy inputs.\")\n+        logger.info(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummy inputs.\")\n         preprocessor = tokenizer\n \n     if issubclass(type(model), PreTrainedModel):\n@@ -221,7 +221,7 @@ def export_tensorflow(\n             \" `preprocessor` instead.\",\n             FutureWarning,\n         )\n-        logger.info(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummmy inputs.\")\n+        logger.info(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummy inputs.\")\n         preprocessor = tokenizer\n \n     model.config.return_dict = True\n@@ -296,7 +296,7 @@ def export(\n             \" `preprocessor` instead.\",\n             FutureWarning,\n         )\n-        logger.info(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummmy inputs.\")\n+        logger.info(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummy inputs.\")\n         preprocessor = tokenizer\n \n     if is_torch_available():\n@@ -335,7 +335,7 @@ def validate_model_outputs(\n             \" `preprocessor` instead.\",\n             FutureWarning,\n         )\n-        logger.info(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummmy inputs.\")\n+        logger.info(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummy inputs.\")\n         preprocessor = tokenizer\n \n     # generate inputs with a different batch_size and seq_len that was used for conversion to properly test"
        },
        {
            "sha": "4adc323f958ebfcd7de8fd5d62d8fdcb99606582",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -193,7 +193,7 @@ def create_quantized_param(\n         unexpected_keys: List[str],\n     ):\n         \"\"\"\n-        Each nn.Linear layer is processsed here.\n+        Each nn.Linear layer is processed here.\n         We first check if the corresponding module state_dict contains already HQQ quantized parameters.\n         If not, we create a temp linear layer with the module state_dict params and use it for quantization\n         \"\"\""
        },
        {
            "sha": "4af6f2d5b9547cd1898cff2d60374b40efe7862c",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -355,7 +355,7 @@ def __init__(self, *args, **kwargs):\n \n         if is_modeloutput_subclass and not is_dataclass(self):\n             raise TypeError(\n-                f\"{self.__module__}.{self.__class__.__name__} is not a dataclasss.\"\n+                f\"{self.__module__}.{self.__class__.__name__} is not a dataclass.\"\n                 \" This is a subclass of ModelOutput and so must use the @dataclass decorator.\"\n             )\n "
        },
        {
            "sha": "106ebbfa93ec922a1701a71a69b7a8ba47573333",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -241,19 +241,19 @@ def test_inputs_embeds_matches_input_ids(self):\n             torch.testing.assert_close(out_embeds, out_ids)\n \n     @unittest.skip(\n-        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seems to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seems to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seems to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -311,7 +311,7 @@ def tearDown(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n+        # Let's make sure we test the preprocessing to replace what is used\n         model = AriaForConditionalGeneration.from_pretrained(\"rhymes-ai/Aria\", load_in_4bit=True)\n \n         prompt = \"<image>\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT:\"\n@@ -333,7 +333,7 @@ def test_small_model_integration_test(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_single(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n+        # Let's make sure we test the preprocessing to replace what is used\n         model_id = \"rhymes-ai/Aria\"\n \n         model = AriaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n@@ -355,7 +355,7 @@ def test_small_model_integration_test_llama_single(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_batched(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n+        # Let's make sure we test the preprocessing to replace what is used\n         model_id = \"rhymes-ai/Aria\"\n \n         model = AriaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n@@ -382,7 +382,7 @@ def test_small_model_integration_test_llama_batched(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_batch(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n+        # Let's make sure we test the preprocessing to replace what is used\n         model = AriaForConditionalGeneration.from_pretrained(\"rhymes-ai/Aria\", load_in_4bit=True)\n         # The first batch is longer in terms of text, but only has 1 image. The second batch will be padded in text, but the first will be padded because images take more space!.\n         prompts = [\n@@ -408,7 +408,7 @@ def test_small_model_integration_test_batch(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_batched_regression(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n+        # Let's make sure we test the preprocessing to replace what is used\n         model_id = \"rhymes-ai/Aria\"\n \n         # Multi-image & multi-prompt (e.g. 3 images and 2 prompts now fails with SDPA, this tests if \"eager\" works as before)\n@@ -442,7 +442,7 @@ def test_batched_generation(self):\n \n         processor = AutoProcessor.from_pretrained(\"rhymes-ai/Aria\")\n \n-        prompt1 = \"<image>\\n<image>\\nUSER: What's the the difference of two images?\\nASSISTANT:\"\n+        prompt1 = \"<image>\\n<image>\\nUSER: What's the difference of two images?\\nASSISTANT:\"\n         prompt2 = \"<image>\\nUSER: Describe the image.\\nASSISTANT:\"\n         prompt3 = \"<image>\\nUSER: Describe the image.\\nASSISTANT:\"\n         url1 = \"https://images.unsplash.com/photo-1552053831-71594a27632d?q=80&w=3062&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n@@ -460,7 +460,7 @@ def test_batched_generation(self):\n         model = model.eval()\n \n         EXPECTED_OUTPUT = [\n-            \"\\n \\nUSER: What's the the difference of two images?\\nASSISTANT: The difference between the two images is that one shows a dog standing on a grassy field, while\",\n+            \"\\n \\nUSER: What's the difference of two images?\\nASSISTANT: The difference between the two images is that one shows a dog standing on a grassy field, while\",\n             \"\\nUSER: Describe the image.\\nASSISTANT: The image features a brown and white dog sitting on a sidewalk. The dog is holding a small\",\n             \"\\nUSER: Describe the image.\\nASSISTANT: The image features a lone llama standing on a grassy hill. The llama is the\",\n         ]"
        },
        {
            "sha": "cc042e9f661555056c4d3531e7088775db1e7be9",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -253,7 +253,7 @@ def test_inputs_embeds_matches_input_ids(self):\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong\n-        when number of images don't match number of image tokens in the text.\n+        when number of images doesn't match number of image tokens in the text.\n         Also we need to test multi-image cases when one prompr has multiple image tokens.\n         \"\"\"\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -306,19 +306,19 @@ def test_vision_feature_layers(self, vision_feature_layer):\n             model(**input_dict)\n \n     @unittest.skip(\n-        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seems to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seems to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seems to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -345,7 +345,7 @@ def tearDown(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n+        # Let's make sure we test the preprocessing to replace what is used\n         model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/bakLlava-v1-hf\", load_in_4bit=True)\n \n         prompt = \"<image>\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT:\"\n@@ -364,7 +364,7 @@ def test_small_model_integration_test(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_single(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n+        # Let's make sure we test the preprocessing to replace what is used\n         model_id = \"llava-hf/llava-1.5-7b-hf\"\n \n         model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", load_in_4bit=True)\n@@ -386,7 +386,7 @@ def test_small_model_integration_test_llama_single(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_batched(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n+        # Let's make sure we test the preprocessing to replace what is used\n         model_id = \"llava-hf/llava-1.5-7b-hf\"\n \n         model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", load_in_4bit=True)\n@@ -413,7 +413,7 @@ def test_small_model_integration_test_llama_batched(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_batch(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n+        # Let's make sure we test the preprocessing to replace what is used\n         model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/bakLlava-v1-hf\", load_in_4bit=True)\n         # The first batch is longer in terms of text, but only has 1 image. The second batch will be padded in text, but the first will be padded because images take more space!.\n         prompts = [\n@@ -441,7 +441,7 @@ def test_small_model_integration_test_batch(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_batched_regression(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n+        # Let's make sure we test the preprocessing to replace what is used\n         model_id = \"llava-hf/llava-1.5-7b-hf\"\n \n         # Multi-image & multi-prompt (e.g. 3 images and 2 prompts now fails with SDPA, this tests if \"eager\" works as before)\n@@ -478,7 +478,7 @@ def test_batched_generation(self):\n \n         processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n \n-        prompt1 = \"<image>\\n<image>\\nUSER: What's the the difference of two images?\\nASSISTANT:\"\n+        prompt1 = \"<image>\\n<image>\\nUSER: What's the difference of two images?\\nASSISTANT:\"\n         prompt2 = \"<image>\\nUSER: Describe the image.\\nASSISTANT:\"\n         prompt3 = \"<image>\\nUSER: Describe the image.\\nASSISTANT:\"\n         url1 = \"https://images.unsplash.com/photo-1552053831-71594a27632d?q=80&w=3062&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n@@ -496,7 +496,7 @@ def test_batched_generation(self):\n         model = model.eval()\n \n         EXPECTED_OUTPUT = [\n-            \"\\n \\nUSER: What's the the difference of two images?\\nASSISTANT: The difference between the two images is that one shows a dog standing on a grassy field, while\",\n+            \"\\n \\nUSER: What's the difference of two images?\\nASSISTANT: The difference between the two images is that one shows a dog standing on a grassy field, while\",\n             \"\\nUSER: Describe the image.\\nASSISTANT: The image features a brown and white dog sitting on a sidewalk. The dog is holding a small\",\n             \"\\nUSER: Describe the image.\\nASSISTANT: The image features a lone llama standing on a grassy hill. The llama is the\",\n         ]\n@@ -617,7 +617,7 @@ def test_pixtral_4bit(self):\n         generate_ids = model.generate(**inputs, max_new_tokens=50)\n         output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n \n-        EXPECTED_GENERATION = \"Describe the images.The image showcases a dog, which is prominently positioned in the center, taking up a significant portion of the frame. The dog is situated against a backdrop of a wooden surface, which spans the entire image. The dog appears to be a black Labrador\"  # fmt: skip\n+        EXPECTED_GENERATION = \"Describe the images. The image showcases a dog, which is prominently positioned in the center, taking up a significant portion of the frame. The dog is situated against a backdrop of a wooden surface, which spans the entire image. The dog appears to be a black Labrador\"  # fmt: skip\n         self.assertEqual(output, EXPECTED_GENERATION)\n \n     @slow"
        },
        {
            "sha": "8fa19cb411824fbc90ad2798129eb3c81301e018",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -237,7 +237,7 @@ def test_inputs_embeds_matches_input_ids(self):\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong\n-        when number of images don't match number of image tokens in the text.\n+        when number of images doesn't match number of image tokens in the text.\n         Also we need to test multi-image cases when one prompr has multiple image tokens.\n         \"\"\"\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "140006d9f7282c88468bd296b1e86e7d1c153192",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -234,7 +234,7 @@ def test_inputs_embeds_matches_input_ids(self):\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong\n-        when number of images don't match number of image tokens in the text.\n+        when number of images doesn't match number of image tokens in the text.\n         Also we need to test multi-image cases when one prompr has multiple image tokens.\n         \"\"\"\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "47495b2ce348974195f5fa1219402537623f5999",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -231,7 +231,7 @@ def test_inputs_embeds_matches_input_ids(self):\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong\n-        when number of images don't match number of image tokens in the text.\n+        when number of images doesn't match number of image tokens in the text.\n         Also we need to test multi-image cases when one prompr has multiple image tokens.\n         \"\"\"\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "d2b3ddf85373692ecdf9b2962bf062d6f7fc95be",
            "filename": "tests/quantization/gptq/test_gptq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/tests%2Fquantization%2Fgptq%2Ftest_gptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/tests%2Fquantization%2Fgptq%2Ftest_gptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fgptq%2Ftest_gptq.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -458,7 +458,7 @@ def check_inference_correctness(self, model):\n \n     def test_generate_quality(self):\n         \"\"\"\n-        Simple test to check the quality of the model by comparing the the generated tokens with the expected tokens\n+        Simple test to check the quality of the model by comparing the generated tokens with the expected tokens\n         \"\"\"\n         self.check_inference_correctness(self.quantized_model)\n "
        },
        {
            "sha": "ebb3fae32591c79aaa451dc6110b77a23aca3c64",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -1090,7 +1090,7 @@ def test_chat_template_video_custom_sampling(self):\n             ]\n         ]\n \n-        def dummmy_sample_indices_fn(metadata, **fn_kwargs):\n+        def dummy_sample_indices_fn(metadata, **fn_kwargs):\n             # sample only the first two frame always\n             return [0, 1]\n \n@@ -1099,7 +1099,7 @@ def dummmy_sample_indices_fn(metadata, **fn_kwargs):\n             add_generation_prompt=True,\n             tokenize=True,\n             return_dict=True,\n-            sample_indices_fn=dummmy_sample_indices_fn,\n+            sample_indices_fn=dummy_sample_indices_fn,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)"
        },
        {
            "sha": "47dc0dc46cb8951022f1b3056322013ef93470c0",
            "filename": "tests/utils/test_image_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbf924b76c03828101a34069a96d209314114fd5/tests%2Futils%2Ftest_image_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbf924b76c03828101a34069a96d209314114fd5/tests%2Futils%2Ftest_image_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_image_utils.py?ref=cbf924b76c03828101a34069a96d209314114fd5",
            "patch": "@@ -429,7 +429,7 @@ def test_make_batched_videos_numpy(self):\n         self.assertEqual(len(videos_list), 1)\n         self.assertTrue(np.array_equal(videos_list[0][0], images))\n \n-        # Test a 4d array of images is converted to a a list of 1 video\n+        # Test a 4d array of images is converted to a list of 1 video\n         images = np.random.randint(0, 256, (4, 16, 32, 3))\n         videos_list = make_batched_videos(images)\n         self.assertIsInstance(videos_list[0], list)"
        }
    ],
    "stats": {
        "total": 222,
        "additions": 111,
        "deletions": 111
    }
}