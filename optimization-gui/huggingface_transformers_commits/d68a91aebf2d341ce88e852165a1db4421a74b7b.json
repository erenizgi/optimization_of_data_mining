{
    "author": "cyyever",
    "message": "Remove extra tensor clone in PyTorch code (#36748)\n\n* Use detach().clone()\n\n* Eliminate continuous()\n\n* Merge clone and other calls with to\n\n* Merge clone and other calls with to",
    "sha": "d68a91aebf2d341ce88e852165a1db4421a74b7b",
    "files": [
        {
            "sha": "ce040ea87c7122d387873b952974d5f3200940eb",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/d68a91aebf2d341ce88e852165a1db4421a74b7b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d68a91aebf2d341ce88e852165a1db4421a74b7b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=d68a91aebf2d341ce88e852165a1db4421a74b7b",
            "patch": "@@ -2697,7 +2697,7 @@ def _dola_decoding(\n             )\n \n             # .float() is needed to retain precision for later logits manipulations\n-            final_layer_next_token_logits = outputs.logits[:, -1, :].detach().clone().float()\n+            final_layer_next_token_logits = outputs.logits[:, -1, :].detach().to(copy=True, dtype=torch.float32)\n             final_logits = outputs.logits[:, -1, :].float()\n             candidate_premature_logits = {}\n             for candidate_premature_layer in candidate_premature_layers:\n@@ -2885,11 +2885,12 @@ def _contrastive_search(\n                     last_hidden_states = outputs.hidden_states[-1]\n \n                 # next logit for contrastive search to select top-k candidate tokens\n-                # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for this first iteration\n+                # Copy is needed to avoid keeping a hanging ref to outputs.logits which may be very large for this first iteration\n                 # (the clone itself is always small)\n-                # .float() is needed to retain precision for later logits manipulations\n-                logit_for_next_step = outputs.logits[:, -1, :].clone().float()\n-                logit_for_next_step = logit_for_next_step.to(input_ids.device)\n+                # torch.float32 is needed to retain precision for later logits manipulations\n+                logit_for_next_step = outputs.logits[:, -1, :].to(\n+                    copy=True, dtype=torch.float32, device=input_ids.device\n+                )\n \n                 model_kwargs = self._update_model_kwargs_for_generation(\n                     outputs,\n@@ -3297,10 +3298,9 @@ def _sample(\n             if synced_gpus and this_peer_finished:\n                 continue\n \n-            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n+            # Copy is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n             # (the clone itself is always small)\n-            next_token_logits = outputs.logits[:, -1, :].clone().float()\n-            next_token_logits = next_token_logits.to(input_ids.device)\n+            next_token_logits = outputs.logits[:, -1, :].to(copy=True, dtype=torch.float32, device=input_ids.device)\n \n             # pre-process distribution\n             next_token_scores = logits_processor(input_ids, next_token_logits)\n@@ -3768,8 +3768,8 @@ def _beam_search(\n             if synced_gpus and this_peer_finished:\n                 continue\n \n-            logits = model_outputs.logits[:, -1, :].clone().float()  # Clone is needed to avoid keeping a hanging ref\n-            logits = logits.to(input_ids.device)\n+            # Copy is needed to avoid keeping a hanging ref\n+            logits = model_outputs.logits[:, -1, :].to(copy=True, dtype=torch.float32, device=input_ids.device)\n \n             # b. Compute log probs -- get log probabilities from logits, process logits with processors (*e.g.*\n             # `temperature`, ...), and add new logprobs to existing running logprobs scores.\n@@ -4045,10 +4045,9 @@ def _group_beam_search(\n             if output_scores:\n                 processed_score = torch.zeros_like(outputs.logits[:, -1, :])\n             if output_logits:\n-                # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n+                # Copy is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n                 # (the clone itself is always small)\n-                raw_logit_score = outputs.logits[:, -1, :].clone()\n-                raw_logit_score = raw_logit_score.to(input_ids.device)\n+                raw_logit_score = outputs.logits[:, -1, :].to(copy=True, device=input_ids.device)\n \n             for beam_group_idx in range(num_beam_groups):\n                 group_start_idx = beam_group_idx * num_sub_beams\n@@ -4067,8 +4066,9 @@ def _group_beam_search(\n                 # select outputs of beams of current group only\n                 # No need to clone() the logits here as they will not retain outputs.logits at the end of the loop\n                 # .float() is needed to retain precision for later logits manipulations\n-                next_token_logits = outputs.logits[batch_group_indices, -1, :].float()\n-                next_token_logits = next_token_logits.to(input_ids.device)\n+                next_token_logits = outputs.logits[batch_group_indices, -1, :].to(\n+                    dtype=torch.float32, device=input_ids.device\n+                )\n \n                 next_token_scores = nn.functional.log_softmax(\n                     next_token_logits, dim=-1\n@@ -4322,11 +4322,10 @@ def _constrained_beam_search(\n                 cur_len = cur_len + 1\n                 continue\n \n-            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n+            # Copy is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n             # (the clone itself is always small)\n             # .float() is needed to retain precision for later logits manipulations\n-            next_token_logits = outputs.logits[:, -1, :].clone().float()\n-            next_token_logits = next_token_logits.to(input_ids.device)\n+            next_token_logits = outputs.logits[:, -1, :].to(copy=True, dtype=torch.float32, device=input_ids.device)\n             next_token_scores = nn.functional.log_softmax(\n                 next_token_logits, dim=-1\n             )  # (batch_size * num_beams, vocab_size)\n@@ -4574,8 +4573,9 @@ def _assisted_decoding(\n \n             # 2.3. Process the new logits\n             # .float() is needed to retain precision for later logits manipulations\n-            new_logits = outputs.logits[:, -candidate_length - 1 :].float()  # excludes the input prompt if present\n-            new_logits = new_logits.to(input_ids.device)\n+            new_logits = outputs.logits[:, -candidate_length - 1 :].to(\n+                dtype=torch.float32, device=input_ids.device\n+            )  # excludes the input prompt if present\n             next_token_logits = new_logits.clone()\n             if len(logits_processor) > 0:\n                 for i in range(candidate_length + 1):"
        },
        {
            "sha": "dd31764dfe0ce731077a3fca9c6e35579c65b50b",
            "filename": "src/transformers/integrations/higgs.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d68a91aebf2d341ce88e852165a1db4421a74b7b/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d68a91aebf2d341ce88e852165a1db4421a74b7b/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhiggs.py?ref=d68a91aebf2d341ce88e852165a1db4421a74b7b",
            "patch": "@@ -446,7 +446,7 @@ def quantize_with_higgs(weight, bits: int = 4, p: int = 2, group_size: int = 256\n \n     device = weight.device\n     dtype = weight.dtype\n-    weight = weight.clone().float()\n+    weight = weight.to(copy=True, dtype=torch.float32)\n     # Pad to Hadamard transform size\n     weight = pad_to_block(weight, [1], hadamard_size)\n "
        },
        {
            "sha": "b5ab4cea1bf8935f849ef98c66308b740c5a4da5",
            "filename": "src/transformers/models/deprecated/jukebox/modeling_jukebox.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d68a91aebf2d341ce88e852165a1db4421a74b7b/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d68a91aebf2d341ce88e852165a1db4421a74b7b/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py?ref=d68a91aebf2d341ce88e852165a1db4421a74b7b",
            "patch": "@@ -2205,12 +2205,12 @@ def forward_tokens(\n         loss += next_token_prediction_loss * self.next_token_prediction_loss_dims / self.total_loss_dims\n \n         metrics = {\n-            \"bpd\": next_token_prediction_loss.clone().detach(),\n-            \"encoder_loss\": encoder_loss.clone().detach(),\n-            \"next_token_prediction_loss\": next_token_prediction_loss.clone().detach(),\n+            \"bpd\": next_token_prediction_loss.detach().clone(),\n+            \"encoder_loss\": encoder_loss.detach().clone(),\n+            \"next_token_prediction_loss\": next_token_prediction_loss.detach().clone(),\n         }\n         if get_preds:\n-            metrics[\"preds\"] = preds.clone().detach()\n+            metrics[\"preds\"] = preds.detach().clone()\n         if get_attn_weights:\n             saved_attn_weights = self.prior.transformer.saved_attn_weights\n             self.prior.transformer.set_record_attn(False)"
        },
        {
            "sha": "3309627b2786ffdd808b4ee1eec7e4f076de362b",
            "filename": "src/transformers/models/mixtral/convert_mixtral_weights_to_hf.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d68a91aebf2d341ce88e852165a1db4421a74b7b/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconvert_mixtral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d68a91aebf2d341ce88e852165a1db4421a74b7b/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconvert_mixtral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconvert_mixtral_weights_to_hf.py?ref=d68a91aebf2d341ce88e852165a1db4421a74b7b",
            "patch": "@@ -148,7 +148,7 @@ def permute(w, n_heads=n_heads, dim1=dim, dim2=dim):\n         w3 = merged_state_dict[f\"layers.{layer_i}.block_sparse_moe.w3\"]\n \n         experts_w1 = [\n-            w1[ffn_dim * expert_idx : ffn_dim * (expert_idx + 1), :].contiguous().clone()\n+            w1[ffn_dim * expert_idx : ffn_dim * (expert_idx + 1), :].clone(memory_format=torch.contiguous_format)\n             for expert_idx in range(num_local_experts)\n         ]\n \n@@ -157,16 +157,16 @@ def permute(w, n_heads=n_heads, dim1=dim, dim2=dim):\n             state_dict[expert_key + \".weight\"] = expert_block.clone()\n \n         experts_w2 = [\n-            w2[ffn_dim * expert_idx : ffn_dim * (expert_idx + 1), :].contiguous().clone()\n+            w2[ffn_dim * expert_idx : ffn_dim * (expert_idx + 1), :].clone(memory_format=torch.contiguous_format)\n             for expert_idx in range(num_local_experts)\n         ]\n \n         for idx, expert_block in enumerate(experts_w2):\n             expert_key = f\"model.layers.{layer_i}.block_sparse_moe.experts.{idx}.w2\"\n-            state_dict[expert_key + \".weight\"] = expert_block.T.clone().contiguous()\n+            state_dict[expert_key + \".weight\"] = expert_block.T.clone(memory_format=torch.contiguous_format)\n \n         experts_w3 = [\n-            w3[ffn_dim * expert_idx : ffn_dim * (expert_idx + 1), :].contiguous().clone()\n+            w3[ffn_dim * expert_idx : ffn_dim * (expert_idx + 1), :].clone(memory_format=torch.contiguous_format)\n             for expert_idx in range(num_local_experts)\n         ]\n "
        },
        {
            "sha": "4807372816b2fe31b455ee733044a72b57ed5cec",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d68a91aebf2d341ce88e852165a1db4421a74b7b/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d68a91aebf2d341ce88e852165a1db4421a74b7b/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=d68a91aebf2d341ce88e852165a1db4421a74b7b",
            "patch": "@@ -131,7 +131,7 @@ def forward(self, pixel_values, bool_masked_pos):\n         embeddings = self.patch_embeddings(pixel_values)\n \n         # add position embeddings\n-        embeddings = embeddings + self.position_embeddings.type_as(embeddings).to(embeddings.device).clone().detach()\n+        embeddings = embeddings + self.position_embeddings.type_as(embeddings).to(embeddings.device).detach().clone()\n         # only keep visible patches\n         # ~bool_masked_pos means visible\n         if bool_masked_pos is not None:\n@@ -856,7 +856,7 @@ def forward(\n         if bool_masked_pos is None:\n             raise ValueError(\"One must provided a boolean mask \")\n         expanded_position_embeddings = self.position_embeddings.expand(batch_size, -1, -1).type_as(pixel_values)\n-        expanded_position_embeddings = expanded_position_embeddings.to(pixel_values.device).clone().detach()\n+        expanded_position_embeddings = expanded_position_embeddings.to(pixel_values.device).detach().clone()\n         pos_emb_visible = expanded_position_embeddings[~bool_masked_pos].reshape(batch_size, -1, num_channels)\n         pos_emb_mask = expanded_position_embeddings[bool_masked_pos].reshape(batch_size, -1, num_channels)\n "
        },
        {
            "sha": "cf8a1209589818556349298389a8c59434f010da",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d68a91aebf2d341ce88e852165a1db4421a74b7b/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d68a91aebf2d341ce88e852165a1db4421a74b7b/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=d68a91aebf2d341ce88e852165a1db4421a74b7b",
            "patch": "@@ -73,12 +73,12 @@ def prune_linear_layer(layer: nn.Linear, index: torch.LongTensor, dim: int = 0)\n         `torch.nn.Linear`: The pruned layer as a new layer with `requires_grad=True`.\n     \"\"\"\n     index = index.to(layer.weight.device)\n-    W = layer.weight.index_select(dim, index).clone().detach()\n+    W = layer.weight.index_select(dim, index).detach().clone()\n     if layer.bias is not None:\n         if dim == 1:\n-            b = layer.bias.clone().detach()\n+            b = layer.bias.detach().clone()\n         else:\n-            b = layer.bias[index].clone().detach()\n+            b = layer.bias[index].detach().clone()\n     new_size = list(layer.weight.size())\n     new_size[dim] = len(index)\n     new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\n@@ -137,11 +137,11 @@ def prune_conv1d_layer(layer: Conv1D, index: torch.LongTensor, dim: int = 1) ->\n         [`~pytorch_utils.Conv1D`]: The pruned layer as a new layer with `requires_grad=True`.\n     \"\"\"\n     index = index.to(layer.weight.device)\n-    W = layer.weight.index_select(dim, index).clone().detach()\n+    W = layer.weight.index_select(dim, index).detach().clone()\n     if dim == 0:\n-        b = layer.bias.clone().detach()\n+        b = layer.bias.detach().clone()\n     else:\n-        b = layer.bias[index].clone().detach()\n+        b = layer.bias[index].detach().clone()\n     new_size = list(layer.weight.size())\n     new_size[dim] = len(index)\n     new_layer = Conv1D(new_size[1], new_size[0]).to(layer.weight.device)"
        }
    ],
    "stats": {
        "total": 74,
        "additions": 37,
        "deletions": 37
    }
}