{
    "author": "justinchuby",
    "message": "Simplify GQA conditions in sdpa_attention.py (#41699)\n\nRemoved unnecessary checks for key being a torch.fx.Proxy in GQA conditions because fx tracing is no longer supported, and torch.export supports enable_gqa.",
    "sha": "347a0f9e836386e508d76eecfb644e15a5b49a5a",
    "files": [
        {
            "sha": "db36dfc30332e988b31f564f914812a1826ba80b",
            "filename": "src/transformers/integrations/sdpa_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/347a0f9e836386e508d76eecfb644e15a5b49a5a/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/347a0f9e836386e508d76eecfb644e15a5b49a5a/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py?ref=347a0f9e836386e508d76eecfb644e15a5b49a5a",
            "patch": "@@ -32,13 +32,11 @@ def use_gqa_in_sdpa(attention_mask: Optional[torch.Tensor], key: torch.Tensor) -\n     # 1.cuda or Ascend NPU\n     #   - torch version >= 2.5\n     #   - attention_mask is None (otherwise it will fall back to the math kernel)\n-    #   - key is not a torch.fx.Proxy (otherwise it will fail with a tracing error)\n     # 2.xpu\n     #   - torch version >= 2.8\n-    #   - key is not a torch.fx.Proxy (otherwise it will fail with a tracing error)\n     if _is_torch_xpu_available:\n-        return _is_torch_greater_or_equal_than_2_8 and not isinstance(key, torch.fx.Proxy)\n-    return _is_torch_greater_or_equal_than_2_5 and attention_mask is None and not isinstance(key, torch.fx.Proxy)\n+        return _is_torch_greater_or_equal_than_2_8\n+    return _is_torch_greater_or_equal_than_2_5 and attention_mask is None\n \n \n def sdpa_attention_forward("
        }
    ],
    "stats": {
        "total": 6,
        "additions": 2,
        "deletions": 4
    }
}