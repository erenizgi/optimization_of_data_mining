{
    "author": "gante",
    "message": "[tests] remove `test_sdpa_equivalence` (redundant) (#37911)\n\n* rm test_sdpa_equivalence\n\n* make fixup\n\n---------\n\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "40a493c7ed4f19f08eadb0639cf26d49bfa5e180",
    "files": [
        {
            "sha": "d0afad7d17d8c0c3a8c95a436571ba87e379ba74",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=40a493c7ed4f19f08eadb0639cf26d49bfa5e180",
            "patch": "@@ -297,10 +297,6 @@ def test_generate_continue_from_inputs_embeds(self):\n     def test_multi_gpu_data_parallel_forward(self):\n         pass\n \n-    @unittest.skip(\"Cohere2's eager attn/sdpa attn outputs are expected to be different\")\n-    def test_sdpa_equivalence(self):\n-        pass\n-\n     @unittest.skip(reason=\"SiglipVisionModel does not support standalone training\")\n     def test_training(self):\n         pass"
        },
        {
            "sha": "195be1c23d86e8d72c4f6be30ee8986284845672",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=40a493c7ed4f19f08eadb0639cf26d49bfa5e180",
            "patch": "@@ -127,10 +127,6 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_generate_continue_from_inputs_embeds(self):\n         pass\n \n-    @unittest.skip(\"Cohere2's eager attn/sdpa attn outputs are expected to be different\")\n-    def test_sdpa_equivalence(self):\n-        pass\n-\n \n @slow\n @require_read_token"
        },
        {
            "sha": "9b796937b08c7da81245a04239202cc7dace9f70",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=40a493c7ed4f19f08eadb0639cf26d49bfa5e180",
            "patch": "@@ -300,10 +300,6 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_generate_continue_from_inputs_embeds(self):\n         pass\n \n-    @unittest.skip(\"DeepseekV3's eager attn/sdpa attn outputs are expected to be different\")\n-    def test_sdpa_equivalence(self):\n-        pass\n-\n     @unittest.skip(\"Deepseek-V3 uses MLA so it is not compatible with the standard cache format\")\n     def test_beam_search_generate_dict_outputs_use_cache(self):\n         pass"
        },
        {
            "sha": "0a4cff3c7ff8873b8f06c1fe851b0624360e3f68",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=40a493c7ed4f19f08eadb0639cf26d49bfa5e180",
            "patch": "@@ -303,38 +303,6 @@ def test_Gemma_token_classification_model(self):\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         self.skipTest(reason=\"Gemma flash attention does not support right padding\")\n \n-    @require_torch_sdpa\n-    @require_torch_accelerator\n-    @slow\n-    def test_sdpa_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(reason=\"Model does not support SDPA\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_sdpa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.float16, attn_implementation=\"sdpa\"\n-                )\n-                model_sdpa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, attn_implementation=\"eager\")\n-                model.to(torch_device)\n-\n-                dummy_input = inputs_dict[model_class.main_input_name]\n-                dummy_input = dummy_input.to(torch_device)\n-                outputs = model(dummy_input, output_hidden_states=True)\n-                outputs_sdpa = model_sdpa(dummy_input, output_hidden_states=True)\n-\n-                logits = outputs.hidden_states[-1]\n-                logits_sdpa = outputs_sdpa.hidden_states[-1]\n-\n-                # gemma sdpa needs a high tolerance\n-                assert torch.allclose(logits_sdpa, logits, atol=3e-3)\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "6ee4e8f2327d86c469bd588ee706e2fac2c0a4e0",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=40a493c7ed4f19f08eadb0639cf26d49bfa5e180",
            "patch": "@@ -143,10 +143,6 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_generate_continue_from_inputs_embeds(self):\n         pass\n \n-    @unittest.skip(\"Gemma2's eager attn/sdpa attn outputs are expected to be different\")\n-    def test_sdpa_equivalence(self):\n-        pass\n-\n     @unittest.skip(\n         reason=\"HybridCache can't be gathered because it is not iterable. Adding a simple iter and dumping `distributed_iterator`\"\n         \" as in Dynamic Cache doesn't work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\""
        },
        {
            "sha": "6dd2fb5cd65172e314a76c8292219b2a48034937",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/40a493c7ed4f19f08eadb0639cf26d49bfa5e180/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=40a493c7ed4f19f08eadb0639cf26d49bfa5e180",
            "patch": "@@ -28,7 +28,6 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -102,38 +101,6 @@ def setUp(self):\n     def test_model_outputs_equivalence(self, **kwargs):\n         pass\n \n-    @require_torch_sdpa\n-    @require_torch_accelerator\n-    @slow\n-    def test_sdpa_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(reason=\"Model does not support SDPA\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_sdpa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.float16, attn_implementation=\"sdpa\"\n-                )\n-                model_sdpa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, attn_implementation=\"eager\")\n-                model.to(torch_device)\n-\n-                dummy_input = inputs_dict[model_class.main_input_name]\n-                dummy_input = dummy_input.to(torch_device)\n-                outputs = model(dummy_input, output_hidden_states=True)\n-                outputs_sdpa = model_sdpa(dummy_input, output_hidden_states=True)\n-\n-                logits = outputs.hidden_states[-1]\n-                logits_sdpa = outputs_sdpa.hidden_states[-1]\n-\n-                # nemotron sdpa needs a high tolerance\n-                assert torch.allclose(logits_sdpa, logits, atol=1e-2)\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        }
    ],
    "stats": {
        "total": 81,
        "additions": 0,
        "deletions": 81
    }
}