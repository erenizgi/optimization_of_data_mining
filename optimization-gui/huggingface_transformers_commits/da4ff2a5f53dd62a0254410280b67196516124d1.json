{
    "author": "cyyever",
    "message": "Add Optional to remaining types (#37808)\n\nMore Optional typing\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "da4ff2a5f53dd62a0254410280b67196516124d1",
    "files": [
        {
            "sha": "c491b01af9465c2a1e17dfe344b468e4cc8be9a1",
            "filename": "examples/legacy/seq2seq/run_distributed_eval.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -19,6 +19,7 @@\n from json import JSONDecodeError\n from logging import getLogger\n from pathlib import Path\n+from typing import Optional\n \n import torch\n from torch.utils.data import DataLoader\n@@ -54,7 +55,7 @@ def eval_data_dir(\n     task=\"summarization\",\n     local_rank=None,\n     num_return_sequences=1,\n-    dataset_kwargs: dict = None,\n+    dataset_kwargs: Optional[dict] = None,\n     prefix=\"\",\n     **generate_kwargs,\n ) -> dict:"
        },
        {
            "sha": "94274bb8f22d2150e5687b116bc631c5eb88c015",
            "filename": "examples/modular-transformers/image_processing_new_imgproc_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -74,7 +74,7 @@ class ImgprocModelImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: dict[str, int] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n@@ -159,7 +159,7 @@ def preprocess(\n         image_mean: Optional[Union[float, list[float]]] = None,\n         image_std: Optional[Union[float, list[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> PIL.Image.Image:"
        },
        {
            "sha": "7b6ba5ee3f519cef92508e0e98160c5e24264c21",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -359,7 +359,7 @@ class DynamicCache(Cache):\n         ```\n     \"\"\"\n \n-    def __init__(self, _distributed_cache_data: Iterable = None) -> None:\n+    def __init__(self, _distributed_cache_data: Optional[Iterable] = None) -> None:\n         super().__init__()\n         self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n         self.key_cache: List[torch.Tensor] = []"
        },
        {
            "sha": "a16a02c4621f23a4bf496935a39a7174af62d90e",
            "filename": "src/transformers/commands/add_new_model_like.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -512,7 +512,7 @@ def duplicate_module(\n     new_model_patterns: ModelPatterns,\n     dest_file: Optional[str] = None,\n     add_copied_from: bool = True,\n-    attrs_to_remove: List[str] = None,\n+    attrs_to_remove: Optional[List[str]] = None,\n ):\n     \"\"\"\n     Create a new module from an existing one and adapting all function and classes names from old patterns to new ones."
        },
        {
            "sha": "5716ee4bf5cfb719e15cc779a860d09a987d457e",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -19,6 +19,7 @@\n \"\"\"\n \n import warnings\n+from typing import Optional\n \n from packaging import version\n from tokenizers import AddedToken, Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n@@ -326,7 +327,9 @@ def converted(self) -> Tokenizer:\n \n \n class GPT2Converter(Converter):\n-    def converted(self, vocab: dict[str, int] = None, merges: list[tuple[str, str]] = None) -> Tokenizer:\n+    def converted(\n+        self, vocab: Optional[dict[str, int]] = None, merges: Optional[list[tuple[str, str]]] = None\n+    ) -> Tokenizer:\n         if not vocab:\n             vocab = self.original_tokenizer.encoder\n         if not merges:\n@@ -395,7 +398,9 @@ def converted(self) -> Tokenizer:\n \n \n class Qwen2Converter(Converter):\n-    def converted(self, vocab: dict[str, int] = None, merges: list[tuple[str, str]] = None) -> Tokenizer:\n+    def converted(\n+        self, vocab: Optional[dict[str, int]] = None, merges: Optional[list[tuple[str, str]]] = None\n+    ) -> Tokenizer:\n         if not vocab:\n             vocab = self.original_tokenizer.encoder\n         if not merges:"
        },
        {
            "sha": "b3acbb3feb708f37482aed9d15db92354c025622",
            "filename": "src/transformers/image_processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fimage_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fimage_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -209,7 +209,7 @@ def convert_to_size_dict(\n \n \n def get_size_dict(\n-    size: Union[int, Iterable[int], dict[str, int]] = None,\n+    size: Optional[Union[int, Iterable[int], dict[str, int]]] = None,\n     max_size: Optional[int] = None,\n     height_width_order: bool = True,\n     default_to_square: bool = True,"
        },
        {
            "sha": "2f590bce0e5bdc76ea9419505da1ab9086a89e91",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -755,7 +755,7 @@ def to_dict(self):\n \n \n class SemanticSegmentationMixin:\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n         \"\"\"\n         Converts the output of [`MobileNetV2ForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n "
        },
        {
            "sha": "8a1652748f5a2106978593cef99269ba64f1a86e",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -79,7 +79,7 @@ def load_adapter(\n         max_memory: Optional[str] = None,\n         offload_folder: Optional[str] = None,\n         offload_index: Optional[int] = None,\n-        peft_config: Dict[str, Any] = None,\n+        peft_config: Optional[Dict[str, Any]] = None,\n         adapter_state_dict: Optional[Dict[str, \"torch.Tensor\"]] = None,\n         low_cpu_mem_usage: bool = False,\n         is_trainable: bool = False,"
        },
        {
            "sha": "f7e5f2220704181622c9318edb1a2410b431d1d4",
            "filename": "src/transformers/models/albert/modeling_flax_albert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -558,7 +558,7 @@ def __call__(\n         attention_mask=None,\n         token_type_ids=None,\n         position_ids=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "5843e726d646f5b4000df50088c9d80a55b65a90",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -18,7 +18,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Dict\n+from typing import Dict, Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n@@ -268,7 +268,7 @@ def __init__(\n         vision_config=None,\n         vision_feature_layer: int = -1,\n         text_config: AriaTextConfig = None,\n-        projector_patch_to_query_dict: Dict = None,\n+        projector_patch_to_query_dict: Optional[Dict] = None,\n         image_token_index: int = 9,\n         initializer_range: float = 0.02,\n         **kwargs,"
        },
        {
            "sha": "d1a722e9054ca92f7e4febee9b0bc77946bf683f",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -124,8 +124,8 @@ class AriaImageProcessor(BaseImageProcessor):\n \n     def __init__(\n         self,\n-        image_mean: List[float] = None,\n-        image_std: List[float] = None,\n+        image_mean: Optional[List[float]] = None,\n+        image_std: Optional[List[float]] = None,\n         max_image_size: int = 980,\n         min_image_size: int = 336,\n         split_resolutions: Optional[List[Tuple[int, int]]] = None,"
        },
        {
            "sha": "51e203b07b2e1ec5d5d4c5134a6f4485d8ee9d7b",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -276,7 +276,7 @@ def __init__(\n         vision_config=None,\n         vision_feature_layer: int = -1,\n         text_config: AriaTextConfig = None,\n-        projector_patch_to_query_dict: Dict = None,\n+        projector_patch_to_query_dict: Optional[Dict] = None,\n         image_token_index: int = 9,\n         initializer_range: float = 0.02,\n         **kwargs,\n@@ -514,8 +514,8 @@ class AriaImageProcessor(BaseImageProcessor):\n \n     def __init__(\n         self,\n-        image_mean: List[float] = None,\n-        image_std: List[float] = None,\n+        image_mean: Optional[List[float]] = None,\n+        image_std: Optional[List[float]] = None,\n         max_image_size: int = 980,\n         min_image_size: int = 336,\n         split_resolutions: Optional[List[Tuple[int, int]]] = None,"
        },
        {
            "sha": "e8e304d218a4eee6f37c98edf0ef7f871f00ef03",
            "filename": "src/transformers/models/bark/configuration_bark.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"BARK model configuration\"\"\"\n \n-from typing import Dict\n+from typing import Dict, Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import add_start_docstrings, logging\n@@ -243,10 +243,10 @@ class BarkConfig(PretrainedConfig):\n \n     def __init__(\n         self,\n-        semantic_config: Dict = None,\n-        coarse_acoustics_config: Dict = None,\n-        fine_acoustics_config: Dict = None,\n-        codec_config: Dict = None,\n+        semantic_config: Optional[Dict] = None,\n+        coarse_acoustics_config: Optional[Dict] = None,\n+        fine_acoustics_config: Optional[Dict] = None,\n+        codec_config: Optional[Dict] = None,\n         initializer_range=0.02,\n         **kwargs,\n     ):"
        },
        {
            "sha": "bb1fc26655041e04951c6a224178a603faeb3fe5",
            "filename": "src/transformers/models/bark/generation_configuration_bark.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"BARK model generation configuration\"\"\"\n \n import copy\n-from typing import Dict\n+from typing import Dict, Optional\n \n from ...generation.configuration_utils import GenerationConfig\n from ...utils import logging\n@@ -245,9 +245,9 @@ class BarkGenerationConfig(GenerationConfig):\n \n     def __init__(\n         self,\n-        semantic_config: Dict = None,\n-        coarse_acoustics_config: Dict = None,\n-        fine_acoustics_config: Dict = None,\n+        semantic_config: Optional[Dict] = None,\n+        coarse_acoustics_config: Optional[Dict] = None,\n+        fine_acoustics_config: Optional[Dict] = None,\n         sample_rate=24_000,\n         codebook_size=1024,\n         **kwargs,"
        },
        {
            "sha": "f04ab551e39e09bbb1ec8ab9ed1bfc520cb2e8c1",
            "filename": "src/transformers/models/bart/modeling_flax_bart.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1007,7 +1007,7 @@ def encode(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1068,12 +1068,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1186,7 +1186,7 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1335,12 +1335,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1807,8 +1807,8 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n-        past_key_values: dict = None,\n+        params: Optional[dict] = None,\n+        past_key_values: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions"
        },
        {
            "sha": "a83cf10aad73fedbb17611241c6f0d783f34427e",
            "filename": "src/transformers/models/beit/image_processing_beit.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -106,10 +106,10 @@ class BeitImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_rescale: bool = True,\n         do_normalize: bool = True,\n@@ -194,10 +194,10 @@ def _preprocess(\n         image: ImageInput,\n         do_reduce_labels: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -226,10 +226,10 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -271,10 +271,10 @@ def _preprocess_segmentation_map(\n         self,\n         segmentation_map: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_reduce_labels: Optional[bool] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ):\n@@ -320,10 +320,10 @@ def preprocess(\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -470,7 +470,7 @@ def preprocess(\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n-    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[List[Tuple]] = None):\n         \"\"\"\n         Converts the output of [`BeitForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n "
        },
        {
            "sha": "b51ff9fd094f8f8ff3a8c5922d8b8a1754ba9e9f",
            "filename": "src/transformers/models/beit/modeling_flax_beit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_flax_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_flax_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_flax_beit.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -634,7 +634,7 @@ def __call__(\n         self,\n         pixel_values,\n         bool_masked_pos=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "48b72193fa22b82696bcf4141f0c4d6ad2042b9e",
            "filename": "src/transformers/models/bert/modeling_flax_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -864,13 +864,13 @@ def __call__(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "18913e930d557ae4c3c88584df99ad96066517fe",
            "filename": "src/transformers/models/big_bird/modeling_flax_big_bird.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1725,14 +1725,14 @@ def __call__(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: Optional[jax.random.PRNGKey] = None,\n         indices_rng: Optional[jax.random.PRNGKey] = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -2442,7 +2442,7 @@ def __call__(\n         position_ids=None,\n         head_mask=None,\n         question_lengths=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: Optional[jax.random.PRNGKey] = None,\n         indices_rng: Optional[jax.random.PRNGKey] = None,\n         train: bool = False,"
        },
        {
            "sha": "aa2eb3795519d8365903fa02d5049d2f6282fa14",
            "filename": "src/transformers/models/bit/image_processing_bit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -92,10 +92,10 @@ class BitImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -177,7 +177,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,"
        },
        {
            "sha": "835cb6814a738ae42e8b41cba7b473471b6544d3",
            "filename": "src/transformers/models/blenderbot/modeling_flax_blenderbot.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_flax_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_flax_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_flax_blenderbot.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -980,7 +980,7 @@ def encode(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1043,12 +1043,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1161,7 +1161,7 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1311,12 +1311,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\""
        },
        {
            "sha": "1e6a3a727a68da570c10f1ed6dae5621bbeca5d0",
            "filename": "src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_flax_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_flax_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_flax_blenderbot_small.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -977,7 +977,7 @@ def encode(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1040,12 +1040,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1157,7 +1157,7 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1308,12 +1308,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         deterministic: bool = True,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\""
        },
        {
            "sha": "ace61142ec8e0ef115738e50b7535ed097ab0472",
            "filename": "src/transformers/models/blip/image_processing_blip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -83,7 +83,7 @@ class BlipImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,"
        },
        {
            "sha": "fb6fc00b94d1215656cc1ea1206684ecc9c8e1ae",
            "filename": "src/transformers/models/bloom/configuration_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -148,7 +148,7 @@ def __init__(\n         self,\n         config: PretrainedConfig,\n         task: str = \"default\",\n-        patching_specs: List[PatchingSpec] = None,\n+        patching_specs: Optional[List[PatchingSpec]] = None,\n         use_past: bool = False,\n     ):\n         super().__init__(config, task=task, patching_specs=patching_specs, use_past=use_past)"
        },
        {
            "sha": "d0b2f084d37d9e054efdd98e42474c291cf9fad8",
            "filename": "src/transformers/models/bloom/modeling_flax_bloom.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_flax_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_flax_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_flax_bloom.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -463,8 +463,8 @@ def __call__(\n         self,\n         input_ids,\n         attention_mask=None,\n-        past_key_values: dict = None,\n-        params: dict = None,\n+        past_key_values: Optional[dict] = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "1f651eba1d0786f519f963162ff93be0a653593f",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -172,7 +172,7 @@ class BridgeTowerImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n@@ -181,7 +181,7 @@ def __init__(\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_pad: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -385,7 +385,7 @@ def preprocess(\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_pad: Optional[bool] = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "f1ab1565038808c76b7064efb7c2235fa2ba21f0",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1581,7 +1581,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "5955ef48940ff80ef045dfe1a88e071530852e2c",
            "filename": "src/transformers/models/chameleon/configuration_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"chameleon model configuration\"\"\"\n \n-from typing import List\n+from typing import List, Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n@@ -75,7 +75,7 @@ def __init__(\n         base_channels: int = 128,\n         channel_multiplier: List[int] = [1, 1, 2, 2, 4],\n         num_res_blocks: int = 2,\n-        attn_resolutions: List[int] = None,\n+        attn_resolutions: Optional[List[int]] = None,\n         dropout: float = 0.0,\n         attn_type: str = \"vanilla\",\n         initializer_range=0.02,"
        },
        {
            "sha": "e694cee7bb842cd0b5a9b7c6b2bfa5df709c9ca7",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -88,10 +88,10 @@ class ChameleonImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PIL.Image.LANCZOS,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 0.0078,\n         do_normalize: bool = True,\n@@ -173,7 +173,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,"
        },
        {
            "sha": "e8f8ba1e8d52285b598b83359e0a582a4c4b80b1",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -96,10 +96,10 @@ class ChineseCLIPImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -170,7 +170,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,"
        },
        {
            "sha": "a506da423de1b15a5cb4f54665d86bd6c2a3843e",
            "filename": "src/transformers/models/clip/image_processing_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -95,10 +95,10 @@ class CLIPImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -203,7 +203,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,"
        },
        {
            "sha": "c8eb6cf02ed9740c3a926a3c320ed6a0289b51c2",
            "filename": "src/transformers/models/clip/modeling_flax_clip.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_flax_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_flax_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_flax_clip.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -667,7 +667,7 @@ def __call__(\n         input_ids,\n         attention_mask=None,\n         position_ids=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,\n@@ -745,7 +745,7 @@ def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: Froz\n     def __call__(\n         self,\n         pixel_values,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,\n@@ -823,7 +823,7 @@ def __call__(\n         pixel_values,\n         attention_mask=None,\n         position_ids=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,\n@@ -867,7 +867,7 @@ def get_text_features(\n         input_ids,\n         attention_mask=None,\n         position_ids=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train=False,\n     ):\n@@ -930,7 +930,7 @@ def _get_features(module, input_ids, attention_mask, position_ids, deterministic\n         )\n \n     def get_image_features(\n-        self, pixel_values, params: dict = None, dropout_rng: jax.random.PRNGKey = None, train=False\n+        self, pixel_values, params: Optional[dict] = None, dropout_rng: jax.random.PRNGKey = None, train=False\n     ):\n         r\"\"\"\n         Args:"
        },
        {
            "sha": "7ed03ab3f6521e2756ce9025757456d49da76254",
            "filename": "src/transformers/models/codegen/configuration_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -151,7 +151,7 @@ def __init__(\n         self,\n         config: PretrainedConfig,\n         task: str = \"default\",\n-        patching_specs: List[PatchingSpec] = None,\n+        patching_specs: Optional[List[PatchingSpec]] = None,\n         use_past: bool = False,\n     ):\n         super().__init__(config, task=task, patching_specs=patching_specs, use_past=use_past)"
        },
        {
            "sha": "83dc0f2c8b636e8bc25be03dd6d1aa52f1cfc8f5",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -749,7 +749,7 @@ def compute_segments(\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n     label_ids_to_fuse: Optional[Set[int]] = None,\n-    target_size: Tuple[int, int] = None,\n+    target_size: Optional[Tuple[int, int]] = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -863,13 +863,13 @@ def __init__(\n         self,\n         format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Union[float, List[float]] = None,\n-        image_std: Union[float, List[float]] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n         do_convert_annotations: Optional[bool] = None,\n         do_pad: bool = True,\n         pad_size: Optional[Dict[str, int]] = None,\n@@ -1633,7 +1633,7 @@ def post_process_object_detection(\n         return results\n \n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_semantic_segmentation with Detr->ConditionalDetr\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple[int, int]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[List[Tuple[int, int]]] = None):\n         \"\"\"\n         Converts the output of [`ConditionalDetrForSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n "
        },
        {
            "sha": "0566eb39477b1893a817e363dc765e46584467db",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -850,7 +850,7 @@ def post_process_object_detection(\n \n         return results\n \n-    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple[int, int]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[List[Tuple[int, int]]] = None):\n         \"\"\"\n         Converts the output of [`ConditionalDetrForSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n "
        },
        {
            "sha": "5093c9d33b9f32f5557bab16203e5f35a9d1aa2c",
            "filename": "src/transformers/models/convnext/image_processing_convnext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -91,7 +91,7 @@ class ConvNextImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         crop_pct: Optional[float] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n@@ -190,7 +190,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         crop_pct: Optional[float] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,"
        },
        {
            "sha": "e5cc353cc572ede85747153c9cb63f09c178d77f",
            "filename": "src/transformers/models/cpmant/tokenization_cpmant.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -222,7 +222,9 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n                 index += 1\n         return (vocab_file,)\n \n-    def build_inputs_with_special_tokens(self, token_ids_0: List[int], token_ids_1: List[int] = None) -> List[int]:\n+    def build_inputs_with_special_tokens(\n+        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n+    ) -> List[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A CPMAnt sequence has the following format:"
        },
        {
            "sha": "3d8cf3e2795c93a505d1e89185af2a2611faa257",
            "filename": "src/transformers/models/dab_detr/convert_dab_detr_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconvert_dab_detr_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconvert_dab_detr_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconvert_dab_detr_original_pytorch_checkpoint_to_pytorch.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -19,6 +19,7 @@\n import json\n import re\n from pathlib import Path\n+from typing import Optional\n \n import torch\n from huggingface_hub import hf_hub_download\n@@ -87,7 +88,7 @@\n \n \n # Copied from transformers.models.mllama.convert_mllama_weights_to_hf.convert_old_keys_to_new_keys\n-def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None):\n     \"\"\"\n     This function should be applied only once, on the concatenated keys to efficiently rename using\n     the key mappings."
        },
        {
            "sha": "36d48380b8c1cbc050fdd94c0ce6a55b75be8052",
            "filename": "src/transformers/models/dbrx/configuration_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -89,7 +89,7 @@ class DbrxFFNConfig(PretrainedConfig):\n \n     def __init__(\n         self,\n-        ffn_act_fn: dict = None,\n+        ffn_act_fn: Optional[dict] = None,\n         ffn_hidden_size: int = 3584,\n         moe_num_experts: int = 4,\n         moe_top_k: int = 1,"
        },
        {
            "sha": "81cc3b8f33cb0e1bc7a27f384637196ea7898c7f",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -747,7 +747,7 @@ def compute_segments(\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n     label_ids_to_fuse: Optional[Set[int]] = None,\n-    target_size: Tuple[int, int] = None,\n+    target_size: Optional[Tuple[int, int]] = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -861,13 +861,13 @@ def __init__(\n         self,\n         format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Union[float, List[float]] = None,\n-        image_std: Union[float, List[float]] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n         do_convert_annotations: Optional[bool] = None,\n         do_pad: bool = True,\n         pad_size: Optional[Dict[str, int]] = None,"
        },
        {
            "sha": "7b198f5200a7a952c5fd1ca3d2fc804a26a55137",
            "filename": "src/transformers/models/deit/image_processing_deit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -84,10 +84,10 @@ class DeiTImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PIL.Image.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_rescale: bool = True,\n         do_normalize: bool = True,\n@@ -166,10 +166,10 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample=None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "e76228fb6bf0248d61351eca24d95d0b9a8ca489",
            "filename": "src/transformers/models/deprecated/deta/image_processing_deta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -553,13 +553,13 @@ def __init__(\n         self,\n         format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Union[float, List[float]] = None,\n-        image_std: Union[float, List[float]] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n         do_convert_annotations: bool = True,\n         do_pad: bool = True,\n         pad_size: Optional[Dict[str, int]] = None,"
        },
        {
            "sha": "83a78ac65fab91979edf1082d012c935a4ca4b0b",
            "filename": "src/transformers/models/deprecated/efficientformer/image_processing_efficientformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -91,7 +91,7 @@ def __init__(\n         do_center_crop: bool = True,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_normalize: bool = True,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n@@ -179,7 +179,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,"
        },
        {
            "sha": "2c4a848df5c59878bb8712029c5b649dc9b72afd",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1684,7 +1684,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "719d3c59f32412fabf5c90b1076bc3539678bb4d",
            "filename": "src/transformers/models/deprecated/tapex/tokenization_tapex.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -497,7 +497,7 @@ def __call__(\n         self,\n         table: Union[\"pd.DataFrame\", List[\"pd.DataFrame\"]] = None,\n         query: Optional[Union[TextInput, List[TextInput]]] = None,\n-        answer: Union[str, List[str]] = None,\n+        answer: Optional[Union[str, List[str]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n         truncation: Union[bool, str, TruncationStrategy] = None,\n@@ -574,7 +574,7 @@ def source_call_func(\n         self,\n         table: Union[\"pd.DataFrame\", List[\"pd.DataFrame\"]],\n         query: Optional[Union[TextInput, List[TextInput]]] = None,\n-        answer: Union[str, List[str]] = None,\n+        answer: Optional[Union[str, List[str]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n         truncation: Union[bool, str, TruncationStrategy] = None,\n@@ -662,10 +662,10 @@ def batch_encode_plus(\n         self,\n         table: Union[\"pd.DataFrame\", List[\"pd.DataFrame\"]],\n         query: Optional[List[TextInput]] = None,\n-        answer: List[str] = None,\n+        answer: Optional[List[str]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str] = None,\n+        truncation: Optional[Union[bool, str]] = None,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n@@ -884,7 +884,7 @@ def encode_plus(\n         answer: Optional[str] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str] = None,\n+        truncation: Optional[Union[bool, str]] = None,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n@@ -1053,7 +1053,7 @@ def target_batch_encode_plus(\n         answer: List[str],\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str] = None,\n+        truncation: Optional[Union[bool, str]] = None,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n@@ -1197,7 +1197,7 @@ def target_encode_plus(\n         answer: str,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str] = None,\n+        truncation: Optional[Union[bool, str]] = None,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "a10b9b3b2116c47bdf41f7f41b1a332056c5f4f0",
            "filename": "src/transformers/models/deprecated/tvlt/image_processing_tvlt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -121,12 +121,12 @@ class TvltImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         patch_size: List[int] = [16, 16],\n         num_frames: int = 8,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -221,10 +221,10 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -278,12 +278,12 @@ def preprocess(\n         self,\n         videos: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n-        patch_size: List[int] = None,\n+        size: Optional[Dict[str, int]] = None,\n+        patch_size: Optional[List[int]] = None,\n         num_frames: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "9f644bfc5635f7b972027574698a7cd070f9f5b9",
            "filename": "src/transformers/models/deprecated/vit_hybrid/image_processing_vit_hybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -93,10 +93,10 @@ class ViTHybridImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -193,7 +193,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,"
        },
        {
            "sha": "655bbdc0230f654d5fdb913380bfc780f48896d0",
            "filename": "src/transformers/models/depth_pro/convert_depth_pro_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconvert_depth_pro_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconvert_depth_pro_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconvert_depth_pro_weights_to_hf.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -15,6 +15,7 @@\n import argparse\n import gc\n import os\n+from typing import Optional\n \n import regex as re\n import torch\n@@ -93,7 +94,7 @@\n # fmt: on\n \n \n-def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None):\n     output_dict = {}\n     if state_dict_keys is not None:\n         old_text = \"\\n\".join(state_dict_keys)"
        },
        {
            "sha": "0b365eafa17f86732947e9ab0cd8cb8637a5265c",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -732,7 +732,7 @@ def compute_segments(\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n     label_ids_to_fuse: Optional[Set[int]] = None,\n-    target_size: Tuple[int, int] = None,\n+    target_size: Optional[Tuple[int, int]] = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -845,13 +845,13 @@ def __init__(\n         self,\n         format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Union[float, List[float]] = None,\n-        image_std: Union[float, List[float]] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n         do_convert_annotations: Optional[bool] = None,\n         do_pad: bool = True,\n         pad_size: Optional[Dict[str, int]] = None,\n@@ -1824,7 +1824,7 @@ def post_process_object_detection(\n \n         return results\n \n-    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple[int, int]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[List[Tuple[int, int]]] = None):\n         \"\"\"\n         Converts the output of [`DetrForSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n "
        },
        {
            "sha": "419d099e913e0883b736c624c6fc104e3b5efd57",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1088,7 +1088,7 @@ def post_process_object_detection(\n         return results\n \n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_semantic_segmentation\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple[int, int]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[List[Tuple[int, int]]] = None):\n         \"\"\"\n         Converts the output of [`DetrForSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n "
        },
        {
            "sha": "48afecde5e128a8f3ee16544f5ebe99d9c9ca5e6",
            "filename": "src/transformers/models/dinov2/modeling_flax_dinov2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -592,7 +592,7 @@ def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: Froz\n     def __call__(\n         self,\n         pixel_values,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "e9c12c4b088238b6a371d82e71326fb74091e1e4",
            "filename": "src/transformers/models/distilbert/modeling_flax_distilbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -459,7 +459,7 @@ def __call__(\n         input_ids,\n         attention_mask=None,\n         head_mask=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "c45e11430ff41f3ac85d38dfd7e77e8b4031a019",
            "filename": "src/transformers/models/donut/image_processing_donut.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -94,7 +94,7 @@ class DonutImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_thumbnail: bool = True,\n         do_align_long_axis: bool = False,\n@@ -313,7 +313,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_thumbnail: Optional[bool] = None,\n         do_align_long_axis: Optional[bool] = None,"
        },
        {
            "sha": "a22548f5cd92d71200bb65f71da2a773356fee04",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -154,7 +154,7 @@ class DPTImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         keep_aspect_ratio: bool = False,\n         ensure_multiple_of: int = 1,\n@@ -299,7 +299,7 @@ def _preprocess(\n         image: ImageInput,\n         do_reduce_labels: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         keep_aspect_ratio: Optional[bool] = None,\n         ensure_multiple_of: Optional[int] = None,\n@@ -340,7 +340,7 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         keep_aspect_ratio: Optional[bool] = None,\n         ensure_multiple_of: Optional[int] = None,\n@@ -391,7 +391,7 @@ def _preprocess_segmentation_map(\n         self,\n         segmentation_map: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         keep_aspect_ratio: Optional[bool] = None,\n         ensure_multiple_of: Optional[int] = None,\n@@ -592,7 +592,7 @@ def preprocess(\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->DPT\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[List[Tuple]] = None):\n         \"\"\"\n         Converts the output of [`DPTForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n "
        },
        {
            "sha": "6aa42f18ce91acbb0d3a04edd979d5406023c2a6",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -87,10 +87,10 @@ class EfficientNetImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PIL.Image.NEAREST,\n         do_center_crop: bool = False,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         rescale_factor: Union[int, float] = 1 / 255,\n         rescale_offset: bool = False,\n         do_rescale: bool = True,\n@@ -213,10 +213,10 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample=None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         rescale_offset: Optional[bool] = None,"
        },
        {
            "sha": "7cc20ec27fcb59dbcc50366157db6ca50861679a",
            "filename": "src/transformers/models/electra/modeling_flax_electra.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -777,13 +777,13 @@ def __call__(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "60e5e55ab448dc9e3fd48ed534adef189c4911e1",
            "filename": "src/transformers/models/emu3/configuration_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -304,7 +304,7 @@ def __init__(\n         self,\n         vq_config: Union[Dict, Emu3VQVAEConfig] = None,\n         text_config: Union[Dict, Emu3TextConfig] = None,\n-        vocabulary_map: Dict[int, int] = None,\n+        vocabulary_map: Optional[Dict[int, int]] = None,\n         **kwargs,\n     ):\n         if vq_config is None:"
        },
        {
            "sha": "3780de93c36012aa143ee5aaadbeaf909dc1fa01",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -309,7 +309,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "1ed7a2a5ce2aac7c8298c96c5efb7e0497dc2754",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -550,7 +550,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n         encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n-        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "c37e7d3537d3ec287c12e8021f175a98bbfc1fa8",
            "filename": "src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -436,7 +436,7 @@ def encode(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -508,12 +508,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -638,7 +638,7 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\""
        },
        {
            "sha": "5bb3f150f6b901022612a2db29bf445729957d39",
            "filename": "src/transformers/models/fastspeech2_conformer/configuration_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"FastSpeech2Conformer model configuration\"\"\"\n \n-from typing import Dict\n+from typing import Dict, Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n@@ -459,8 +459,8 @@ class FastSpeech2ConformerWithHifiGanConfig(PretrainedConfig):\n \n     def __init__(\n         self,\n-        model_config: Dict = None,\n-        vocoder_config: Dict = None,\n+        model_config: Optional[Dict] = None,\n+        vocoder_config: Optional[Dict] = None,\n         **kwargs,\n     ):\n         if model_config is None:"
        },
        {
            "sha": "4f9a47b4d152d674bb60016fde501a406bacd733",
            "filename": "src/transformers/models/flava/configuration_flava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"FLAVA model configurations\"\"\"\n \n-from typing import Any, Dict\n+from typing import Any, Dict, Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n@@ -472,10 +472,10 @@ class FlavaConfig(PretrainedConfig):\n \n     def __init__(\n         self,\n-        image_config: Dict[str, Any] = None,\n-        text_config: Dict[str, Any] = None,\n-        multimodal_config: Dict[str, Any] = None,\n-        image_codebook_config: Dict[str, Any] = None,\n+        image_config: Optional[Dict[str, Any]] = None,\n+        text_config: Optional[Dict[str, Any]] = None,\n+        multimodal_config: Optional[Dict[str, Any]] = None,\n+        image_codebook_config: Optional[Dict[str, Any]] = None,\n         hidden_size: int = 768,\n         layer_norm_eps: float = 1e-12,\n         projection_dim: int = 768,"
        },
        {
            "sha": "caa03dca8cf0595bdf5b2495f0c5d631cf2bd41f",
            "filename": "src/transformers/models/flava/image_processing_flava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -228,10 +228,10 @@ class FlavaImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -392,10 +392,10 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -457,7 +457,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[Dict[str, int]] = None,"
        },
        {
            "sha": "2984aac67da459c01201ee38557ac5fde92bf2ca",
            "filename": "src/transformers/models/fuyu/image_processing_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -537,7 +537,7 @@ def preprocess(\n         }\n         return FuyuBatchFeature(data=data, tensor_type=return_tensors)\n \n-    def get_num_patches(self, image_height: int, image_width: int, patch_size: Dict[str, int] = None) -> int:\n+    def get_num_patches(self, image_height: int, image_width: int, patch_size: Optional[Dict[str, int]] = None) -> int:\n         \"\"\"\n         Calculate number of patches required to encode an image.\n "
        },
        {
            "sha": "237e92d9492c58e623f0f9529ce7e87a90efdd22",
            "filename": "src/transformers/models/gemma/modeling_flax_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_flax_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_flax_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_flax_gemma.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -485,8 +485,8 @@ def __call__(\n         input_ids,\n         attention_mask=None,\n         position_ids=None,\n-        params: dict = None,\n-        past_key_values: dict = None,\n+        params: Optional[dict] = None,\n+        past_key_values: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "91e2d0c66a49b36c61e9ec965003bbdeb3f016b8",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -95,7 +95,7 @@ class Gemma3ImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n@@ -241,7 +241,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "2bf4b3ac7d0addcc5ba7c3f0fdde5bcc23aedf84",
            "filename": "src/transformers/models/got_ocr2/convert_got_ocr2_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -61,7 +61,7 @@\n CONTEXT_LENGTH = 8000\n \n \n-def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None):\n     \"\"\"\n     This function should be applied only once, on the concatenated keys to efficiently rename using\n     the key mappings."
        },
        {
            "sha": "dc06f1ef391b7701d811ddcaa659da66d5436410",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -172,7 +172,7 @@ class GotOcr2ImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         crop_to_patches: bool = False,\n         min_patches: int = 1,\n         max_patches: int = 12,\n@@ -419,7 +419,7 @@ def crop_image_to_patches(\n         min_patches: int,\n         max_patches: int,\n         use_thumbnail: bool = True,\n-        patch_size: Union[Tuple, int, dict] = None,\n+        patch_size: Optional[Union[Tuple, int, dict]] = None,\n         data_format: ChannelDimension = None,\n     ):\n         \"\"\""
        },
        {
            "sha": "e8b17c4ed368dbf7a22fd040507b61435aa868f0",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -114,7 +114,7 @@ def crop_image_to_patches(\n         min_patches: int,\n         max_patches: int,\n         use_thumbnail: bool = True,\n-        patch_size: Union[Tuple, int, dict] = None,\n+        patch_size: Optional[Union[Tuple, int, dict]] = None,\n         interpolation: Optional[\"F.InterpolationMode\"] = None,\n     ):\n         \"\"\""
        },
        {
            "sha": "fb582998bf8686a8e825250d9aa80a84447162d2",
            "filename": "src/transformers/models/gpt2/configuration_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -194,7 +194,7 @@ def __init__(\n         self,\n         config: PretrainedConfig,\n         task: str = \"default\",\n-        patching_specs: List[PatchingSpec] = None,\n+        patching_specs: Optional[List[PatchingSpec]] = None,\n         use_past: bool = False,\n     ):\n         super().__init__(config, task=task, patching_specs=patching_specs, use_past=use_past)"
        },
        {
            "sha": "1382a25561240cac22b979cbbd829d9b2bb222bf",
            "filename": "src/transformers/models/gpt2/modeling_flax_gpt2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_flax_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_flax_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_flax_gpt2.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -461,8 +461,8 @@ def __call__(\n         position_ids=None,\n         encoder_hidden_states: Optional[jnp.ndarray] = None,\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n-        params: dict = None,\n-        past_key_values: dict = None,\n+        params: Optional[dict] = None,\n+        past_key_values: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "f7371d08b03da94d69747fcf46a09acef9686e68",
            "filename": "src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_flax_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_flax_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_flax_gpt_neo.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -404,8 +404,8 @@ def __call__(\n         input_ids,\n         attention_mask=None,\n         position_ids=None,\n-        params: dict = None,\n-        past_key_values: dict = None,\n+        params: Optional[dict] = None,\n+        past_key_values: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "5b59d309c25a3cb4919955f95d84d3b77dc611b9",
            "filename": "src/transformers/models/gptj/configuration_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgptj%2Fconfiguration_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgptj%2Fconfiguration_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fconfiguration_gptj.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -140,7 +140,7 @@ def __init__(\n         self,\n         config: PretrainedConfig,\n         task: str = \"default\",\n-        patching_specs: List[PatchingSpec] = None,\n+        patching_specs: Optional[List[PatchingSpec]] = None,\n         use_past: bool = False,\n     ):\n         super().__init__(config, task=task, patching_specs=patching_specs, use_past=use_past)"
        },
        {
            "sha": "ee88f69cc18ddc7f3b4894a6865e3334558f6455",
            "filename": "src/transformers/models/gptj/modeling_flax_gptj.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_flax_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_flax_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_flax_gptj.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -438,8 +438,8 @@ def __call__(\n         input_ids,\n         attention_mask=None,\n         position_ids=None,\n-        params: dict = None,\n-        past_key_values: dict = None,\n+        params: Optional[dict] = None,\n+        past_key_values: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "f32d3095ffa6bc59cca94c8bab71fca5d56329c5",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -756,7 +756,7 @@ def compute_segments(\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n     label_ids_to_fuse: Optional[Set[int]] = None,\n-    target_size: Tuple[int, int] = None,\n+    target_size: Optional[Tuple[int, int]] = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -899,13 +899,13 @@ def __init__(\n         self,\n         format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Union[float, List[float]] = None,\n-        image_std: Union[float, List[float]] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n         do_convert_annotations: Optional[bool] = None,\n         do_pad: bool = True,\n         pad_size: Optional[Dict[str, int]] = None,"
        },
        {
            "sha": "a741539a4050208a99b3c97f6afd77dbac217999",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -2554,7 +2554,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        labels: List[Dict[str, Union[torch.LongTensor, torch.FloatTensor]]] = None,\n+        labels: Optional[List[Dict[str, Union[torch.LongTensor, torch.FloatTensor]]]] = None,\n     ):\n         r\"\"\"\n         labels (`List[Dict]` of len `(batch_size,)`, *optional*):"
        },
        {
            "sha": "17b7fb4f39f090484f64d856e802c01425252903",
            "filename": "src/transformers/models/idefics/image_processing_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -101,7 +101,7 @@ def preprocess(\n         image_size: Optional[Dict[str, int]] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        transform: Callable = None,\n+        transform: Optional[Callable] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,"
        },
        {
            "sha": "239a266d9bb46480f8772daebc8bf3fc217f7565",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -190,7 +190,7 @@ def __init__(\n         self,\n         do_convert_rgb: bool = True,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,"
        },
        {
            "sha": "b2f049e998adf131cb1fbb53e77ab9f201044d10",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -295,10 +295,10 @@ def __init__(\n         self,\n         do_convert_rgb: bool = True,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.LANCZOS,\n         do_image_splitting: bool = True,\n-        max_image_size: Dict[str, int] = None,\n+        max_image_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,"
        },
        {
            "sha": "25d97df6ce8f3d3cfaf021b69b2540e7602cf9f7",
            "filename": "src/transformers/models/ijepa/convert_ijepa_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fijepa%2Fconvert_ijepa_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fijepa%2Fconvert_ijepa_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fconvert_ijepa_to_hf.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -21,6 +21,7 @@\n import gc\n import re\n from pathlib import Path\n+from typing import Optional\n \n import requests\n import torch\n@@ -63,7 +64,7 @@\n # fmt: on\n \n \n-def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None):\n     \"\"\"\n     Converts old keys to new keys using the mapping and dynamically removes the 'ijepa.' prefix if necessary.\n "
        },
        {
            "sha": "5b941a6c779b9437b1b4f77b301765f235cc212d",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -89,7 +89,7 @@ def __init__(\n         # clusters is a first argument to maintain backwards compatibility with the old ImageGPTImageProcessor\n         clusters: Optional[Union[List[List[int]], np.ndarray]] = None,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_normalize: bool = True,\n         do_color_quantize: bool = True,\n@@ -180,7 +180,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_normalize: Optional[bool] = None,\n         do_color_quantize: Optional[bool] = None,"
        },
        {
            "sha": "5cb34dc897081036bb508bc4788d3a2430157f99",
            "filename": "src/transformers/models/informer/configuration_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Finformer%2Fconfiguration_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Finformer%2Fconfiguration_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fconfiguration_informer.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -141,7 +141,7 @@ def __init__(\n         distribution_output: str = \"student_t\",\n         loss: str = \"nll\",\n         input_size: int = 1,\n-        lags_sequence: List[int] = None,\n+        lags_sequence: Optional[List[int]] = None,\n         scaling: Optional[Union[str, bool]] = \"mean\",\n         num_dynamic_real_features: int = 0,\n         num_static_real_features: int = 0,"
        },
        {
            "sha": "32018a79542c57e358926182f3ee06e8889b9c49",
            "filename": "src/transformers/models/instructblipvideo/image_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -84,7 +84,7 @@ class InstructBlipVideoImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,"
        },
        {
            "sha": "e0f9ef60f6c5aeb7d2ca37d34a4721d21584ae9f",
            "filename": "src/transformers/models/layoutlm/configuration_layoutlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fconfiguration_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fconfiguration_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fconfiguration_layoutlm.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -139,7 +139,7 @@ def __init__(\n         self,\n         config: PretrainedConfig,\n         task: str = \"default\",\n-        patching_specs: List[PatchingSpec] = None,\n+        patching_specs: Optional[List[PatchingSpec]] = None,\n     ):\n         super().__init__(config, task=task, patching_specs=patching_specs)\n         self.max_2d_positions = config.max_2d_position_embeddings - 1"
        },
        {
            "sha": "8a73e443de5768f86757fca819bfee7be762d688",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -129,7 +129,7 @@ class LayoutLMv2ImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         apply_ocr: bool = True,\n         ocr_lang: Optional[str] = None,\n@@ -201,7 +201,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         apply_ocr: Optional[bool] = None,\n         ocr_lang: Optional[str] = None,"
        },
        {
            "sha": "a5ac6681c28208dde9b7f0f3cecf0547089c5db3",
            "filename": "src/transformers/models/layoutlmv2/processing_layoutlmv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -71,7 +71,7 @@ def __call__(\n         images,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n+        boxes: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "2466bdc80d204c9b6e0193429f0186e50227f888",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -406,7 +406,7 @@ def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n+        boxes: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "32d38be09bb4c7b328fc0d6838a5a7f2722b79aa",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -157,7 +157,7 @@ def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n+        boxes: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "705a5e512322cce76f587827e0c516994b0ac829",
            "filename": "src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -146,13 +146,13 @@ class LayoutLMv3ImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_value: float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Union[float, Iterable[float]] = None,\n-        image_std: Union[float, Iterable[float]] = None,\n+        image_mean: Optional[Union[float, Iterable[float]]] = None,\n+        image_std: Optional[Union[float, Iterable[float]]] = None,\n         apply_ocr: bool = True,\n         ocr_lang: Optional[str] = None,\n         tesseract_config: Optional[str] = \"\",\n@@ -228,13 +228,13 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample=None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        image_mean: Union[float, Iterable[float]] = None,\n-        image_std: Union[float, Iterable[float]] = None,\n+        image_mean: Optional[Union[float, Iterable[float]]] = None,\n+        image_std: Optional[Union[float, Iterable[float]]] = None,\n         apply_ocr: Optional[bool] = None,\n         ocr_lang: Optional[str] = None,\n         tesseract_config: Optional[str] = None,"
        },
        {
            "sha": "209272ca354fd656ea1873cfbf021b312c9cc8b8",
            "filename": "src/transformers/models/layoutlmv3/processing_layoutlmv3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -71,7 +71,7 @@ def __call__(\n         images,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n+        boxes: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "7758ba0acc6cd4cf693bb2134c4393dc93ead5e9",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -535,7 +535,7 @@ def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n+        boxes: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "3d0cd26d80c37cc0e3fea3bbe50f6d64a348fafb",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -201,7 +201,7 @@ def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n+        boxes: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "b325221d9f4ec1b2f3f497bbb90077ac2d87c4ea",
            "filename": "src/transformers/models/layoutxlm/processing_layoutxlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -70,7 +70,7 @@ def __call__(\n         images,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n+        boxes: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "8aa85b4f318d17105c9fdf5f75b26c113ea2aa8e",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -441,7 +441,7 @@ def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n+        boxes: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "66d972f4482cff1f1fcff4647b09dfa42831dd3a",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -269,7 +269,7 @@ def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n+        boxes: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "8b3e4e4cf1f7e5bcf4c91d48ba61b467978da6a6",
            "filename": "src/transformers/models/levit/image_processing_levit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -90,10 +90,10 @@ class LevitImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,"
        },
        {
            "sha": "14bc16ede672fd6add008b7420243877810f3a3b",
            "filename": "src/transformers/models/llama/modeling_flax_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_flax_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_flax_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_flax_llama.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -467,8 +467,8 @@ def __call__(\n         input_ids,\n         attention_mask=None,\n         position_ids=None,\n-        params: dict = None,\n-        past_key_values: dict = None,\n+        params: Optional[dict] = None,\n+        past_key_values: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "bce62169a4e84bf57ed4c522779ed41ba4007681",
            "filename": "src/transformers/models/llama4/convert_llama4_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -90,7 +90,7 @@\n # fmt: on\n \n \n-def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None):\n     \"\"\"\n     This function should be applied only once, on the concatenated keys to efficiently rename using\n     the key mappings."
        },
        {
            "sha": "bc56310f189ddc6238acb3029dcf957057b1ceea",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1287,7 +1287,7 @@ def forward(\n         hidden_state: torch.Tensor,\n         freqs_ci: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = None,\n+        output_attentions: Optional[bool] = None,\n     ):\n         # Self Attention\n         residual = hidden_state"
        },
        {
            "sha": "2940ed5c8013a7a60b8d17bae1b4923cf298e32c",
            "filename": "src/transformers/models/llava/image_processing_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -99,10 +99,10 @@ def __init__(\n         self,\n         do_pad: bool = False,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,"
        },
        {
            "sha": "e1afee3192885c15e177b75a63be22cd019ac478",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -168,11 +168,11 @@ class LlavaNextImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n-        image_grid_pinpoints: List = None,\n+        size: Optional[Dict[str, int]] = None,\n+        image_grid_pinpoints: Optional[List] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -330,7 +330,7 @@ def _preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n@@ -559,8 +559,8 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n-        image_grid_pinpoints: List = None,\n+        size: Optional[Dict[str, int]] = None,\n+        image_grid_pinpoints: Optional[List] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,"
        },
        {
            "sha": "06ee0fbdaec8d8d8c4d475d40f5921178b41d7a1",
            "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -93,11 +93,11 @@ class LlavaNextVideoImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n-        image_grid_pinpoints: List = None,\n+        size: Optional[Dict[str, int]] = None,\n+        image_grid_pinpoints: Optional[List] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -180,7 +180,7 @@ def _preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n@@ -280,7 +280,7 @@ def preprocess(\n         self,\n         images: VideoInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,"
        },
        {
            "sha": "8cfdfee1f4af469845b04561ea6b862aa2623fbb",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -162,8 +162,8 @@ class LlavaOnevisionImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n-        image_grid_pinpoints: List = None,\n+        size: Optional[Dict[str, int]] = None,\n+        image_grid_pinpoints: Optional[List] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n@@ -454,7 +454,7 @@ def _preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n@@ -529,8 +529,8 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n-        image_grid_pinpoints: List = None,\n+        size: Optional[Dict[str, int]] = None,\n+        image_grid_pinpoints: Optional[List] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "b9ac7a6e4bc3df2660414abae37417ad5db47a2f",
            "filename": "src/transformers/models/llava_onevision/video_processing_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -84,7 +84,7 @@ class LlavaOnevisionVideoProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n@@ -112,7 +112,7 @@ def _preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n@@ -203,7 +203,7 @@ def preprocess(\n         self,\n         videos: VideoInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "16bcba9fbbfd88ad20434806ffecdafbad52913f",
            "filename": "src/transformers/models/longformer/configuration_longformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flongformer%2Fconfiguration_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flongformer%2Fconfiguration_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fconfiguration_longformer.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -139,7 +139,9 @@ def __init__(\n \n \n class LongformerOnnxConfig(OnnxConfig):\n-    def __init__(self, config: \"PretrainedConfig\", task: str = \"default\", patching_specs: \"List[PatchingSpec]\" = None):\n+    def __init__(\n+        self, config: \"PretrainedConfig\", task: str = \"default\", patching_specs: \"Optional[List[PatchingSpec]]\" = None\n+    ):\n         super().__init__(config, task, patching_specs)\n         config.onnx_export = True\n "
        },
        {
            "sha": "b9a341349f713fe0d48ae40c1f43d26c2da58dd7",
            "filename": "src/transformers/models/longt5/modeling_flax_longt5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_flax_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_flax_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_flax_longt5.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1731,7 +1731,7 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1816,7 +1816,7 @@ def encode(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1872,12 +1872,12 @@ def decode(\n         encoder_outputs,\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -2260,12 +2260,12 @@ def decode(\n         encoder_outputs,\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\""
        },
        {
            "sha": "2436158806ad0351dc5b3141887bd008a1d78b69",
            "filename": "src/transformers/models/marian/modeling_flax_marian.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_flax_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_flax_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_flax_marian.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -970,7 +970,7 @@ def encode(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1032,12 +1032,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1150,7 +1150,7 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1299,12 +1299,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\""
        },
        {
            "sha": "9884b6d7e9e20009f168fb817965888de476007f",
            "filename": "src/transformers/models/marian/modeling_tf_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_tf_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_tf_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_tf_marian.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1155,7 +1155,7 @@ def call(\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n         encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None,\n-        past_key_values: Tuple[Tuple[tf.Tensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "6d1edf2bbb3dc6262e89fec479a92760012adbc0",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -495,7 +495,7 @@ def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        xpaths: Union[List[List[int]], List[List[List[int]]]] = None,\n+        xpaths: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         node_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "e9e9a11953f16f246df08dbd104ed36aa5bab465",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -270,7 +270,7 @@ def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        xpaths: Union[List[List[int]], List[List[List[int]]]] = None,\n+        xpaths: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         node_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "2a3faf183174802523909a0278806fe904475aee",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -207,7 +207,7 @@ def compute_segments(\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n     label_ids_to_fuse: Optional[Set[int]] = None,\n-    target_size: Tuple[int, int] = None,\n+    target_size: Optional[Tuple[int, int]] = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -404,14 +404,14 @@ class Mask2FormerImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Union[float, List[float]] = None,\n-        image_std: Union[float, List[float]] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n         ignore_index: Optional[int] = None,\n         do_reduce_labels: bool = False,\n         num_labels: Optional[int] = None,\n@@ -576,7 +576,7 @@ def _preprocess(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n@@ -600,7 +600,7 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n@@ -642,7 +642,7 @@ def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         size_divisor: int = 0,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:"
        },
        {
            "sha": "bf0bedd2917c0dc64a91a5382471d89f1185e103",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1804,7 +1804,7 @@ def forward(\n         pixel_embeddings: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         query_position_embeddings: Optional[torch.Tensor] = None,\n-        feature_size_list: List = None,\n+        feature_size_list: Optional[List] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "32fc423f08c326d78d9446c4738e6d4f18008ac2",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -213,7 +213,7 @@ def compute_segments(\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n     label_ids_to_fuse: Optional[Set[int]] = None,\n-    target_size: Tuple[int, int] = None,\n+    target_size: Optional[Tuple[int, int]] = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -410,14 +410,14 @@ class MaskFormerImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Union[float, List[float]] = None,\n-        image_std: Union[float, List[float]] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n         ignore_index: Optional[int] = None,\n         do_reduce_labels: bool = False,\n         num_labels: Optional[int] = None,\n@@ -579,7 +579,7 @@ def _preprocess(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n@@ -603,7 +603,7 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n@@ -645,7 +645,7 @@ def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         size_divisor: int = 0,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:\n@@ -973,7 +973,7 @@ def encode_inputs(\n         return encoded_inputs\n \n     def post_process_segmentation(\n-        self, outputs: \"MaskFormerForInstanceSegmentationOutput\", target_size: Tuple[int, int] = None\n+        self, outputs: \"MaskFormerForInstanceSegmentationOutput\", target_size: Optional[Tuple[int, int]] = None\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into image segmentation predictions. Only"
        },
        {
            "sha": "1e019f5199e3ab73e81b39b0ed9a8aa5fe84db92",
            "filename": "src/transformers/models/mbart/modeling_flax_mbart.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_flax_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_flax_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_flax_mbart.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1045,7 +1045,7 @@ def encode(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1106,12 +1106,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1223,7 +1223,7 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1371,12 +1371,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\""
        },
        {
            "sha": "16c53caa3f238168c722a3bfc1ce1b389a80084c",
            "filename": "src/transformers/models/mbart/modeling_tf_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_tf_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_tf_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_tf_mbart.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1430,7 +1430,7 @@ def call(\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n         encoder_outputs: Optional[TFBaseModelOutput] = None,\n-        past_key_values: Tuple[Tuple[tf.Tensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "c90bf25a95707e46b945b05a3513a05a7bf832fe",
            "filename": "src/transformers/models/mistral/modeling_flax_mistral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -461,8 +461,8 @@ def __call__(\n         input_ids,\n         attention_mask=None,\n         position_ids=None,\n-        params: dict = None,\n-        past_key_values: dict = None,\n+        params: Optional[dict] = None,\n+        past_key_values: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "8cb9e78daa54de1917a5d7960b06e82970017351",
            "filename": "src/transformers/models/mllama/convert_mllama_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -90,7 +90,7 @@\n CONTEXT_LENGTH = 131072\n \n \n-def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None):\n     \"\"\"\n     This function should be applied only once, on the concatenated keys to efficiently rename using\n     the key mappings."
        },
        {
            "sha": "2342979f0de00f5ab45e480b152b69d543af4d6d",
            "filename": "src/transformers/models/mobilenet_v1/image_processing_mobilenet_v1.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -92,7 +92,7 @@ def __init__(\n         size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -171,10 +171,10 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "e6b909a8e49fda0261666a5dc1fc017a006d43b5",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -98,7 +98,7 @@ def __init__(\n         size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -177,10 +177,10 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -309,7 +309,7 @@ def preprocess(\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->MobileNetV2\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[List[Tuple]] = None):\n         \"\"\"\n         Converts the output of [`MobileNetV2ForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n "
        },
        {
            "sha": "c23de20ee52b8f0a20c3eab3c40f8a44447063a6",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -90,12 +90,12 @@ class MobileViTImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_flip_channel_order: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -223,12 +223,12 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_flip_channel_order: Optional[bool] = None,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -265,9 +265,9 @@ def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single mask.\"\"\"\n@@ -305,12 +305,12 @@ def preprocess(\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_flip_channel_order: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n@@ -440,7 +440,7 @@ def preprocess(\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->MobileViT\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[List[Tuple]] = None):\n         \"\"\"\n         Converts the output of [`MobileViTForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n "
        },
        {
            "sha": "a7387004a6e8eb5e1bb80f9f47324bc3b88464aa",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1067,7 +1067,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         last_hidden_state: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.BoolTensor] = None,\n-        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1931,7 +1931,7 @@ def forward(\n         user_audio_codes: Optional[torch.Tensor] = None,\n         moshi_input_values: Optional[torch.FloatTensor] = None,\n         moshi_audio_codes: Optional[torch.Tensor] = None,\n-        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         text_labels: Optional[torch.LongTensor] = None,\n         audio_labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "e18c09a11bfd1ce8cd086a0a2ec1c996419690c9",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -2018,7 +2018,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n         encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n-        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -2439,7 +2439,7 @@ def _maybe_initialize_input_ids_for_generation(\n         return torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * bos_token_id\n \n     def _get_decoder_start_token_id(\n-        self, decoder_start_token_id: Union[int, List[int]] = None, bos_token_id: Optional[int] = None\n+        self, decoder_start_token_id: Optional[Union[int, List[int]]] = None, bos_token_id: Optional[int] = None\n     ) -> int:\n         decoder_start_token_id = (\n             decoder_start_token_id"
        },
        {
            "sha": "2489ec9a388d1c0e81273ad730f48c476229cc77",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1917,7 +1917,7 @@ def forward(\n         input_features: Optional[torch.FloatTensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n-        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -2310,7 +2310,7 @@ def freeze_text_encoder(self):\n \n     # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenForConditionalGeneration._get_decoder_start_token_id\n     def _get_decoder_start_token_id(\n-        self, decoder_start_token_id: Union[int, List[int]] = None, bos_token_id: Optional[int] = None\n+        self, decoder_start_token_id: Optional[Union[int, List[int]]] = None, bos_token_id: Optional[int] = None\n     ) -> int:\n         decoder_start_token_id = (\n             decoder_start_token_id"
        },
        {
            "sha": "9d38a0afafd21effbaeb1ef88953bf360f13355c",
            "filename": "src/transformers/models/nougat/image_processing_nougat.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -96,7 +96,7 @@ def __init__(\n         self,\n         do_crop_margin: bool = True,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_thumbnail: bool = True,\n         do_align_long_axis: bool = False,\n@@ -373,13 +373,13 @@ def preprocess(\n         images: ImageInput,\n         do_crop_margin: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_thumbnail: Optional[bool] = None,\n         do_align_long_axis: Optional[bool] = None,\n         do_pad: Optional[bool] = None,\n         do_rescale: Optional[bool] = None,\n-        rescale_factor: Union[int, float] = None,\n+        rescale_factor: Optional[Union[int, float]] = None,\n         do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,"
        },
        {
            "sha": "6f48f23d581a54311dcf5430f564db8dd7097df8",
            "filename": "src/transformers/models/nougat/processing_nougat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -52,13 +52,13 @@ def __call__(\n         text=None,\n         do_crop_margin: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: \"PILImageResampling\" = None,  # noqa: F821\n         do_thumbnail: Optional[bool] = None,\n         do_align_long_axis: Optional[bool] = None,\n         do_pad: Optional[bool] = None,\n         do_rescale: Optional[bool] = None,\n-        rescale_factor: Union[int, float] = None,\n+        rescale_factor: Optional[Union[int, float]] = None,\n         do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,"
        },
        {
            "sha": "618c67c783d829f74ba853b67721b755c21afd3e",
            "filename": "src/transformers/models/omdet_turbo/processing_omdet_turbo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -227,7 +227,7 @@ def __init__(self, image_processor, tokenizer):\n     def __call__(\n         self,\n         images: ImageInput = None,\n-        text: Union[List[str], List[List[str]]] = None,\n+        text: Optional[Union[List[str], List[List[str]]]] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[OmDetTurboProcessorKwargs],"
        },
        {
            "sha": "068d6afd21b61eafe7c4754eb3f771be45103e8f",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -210,7 +210,7 @@ def compute_segments(\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n     label_ids_to_fuse: Optional[Set[int]] = None,\n-    target_size: Tuple[int, int] = None,\n+    target_size: Optional[Tuple[int, int]] = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -430,13 +430,13 @@ class OneFormerImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Union[float, List[float]] = None,\n-        image_std: Union[float, List[float]] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n         ignore_index: Optional[int] = None,\n         do_reduce_labels: bool = False,\n         repo_path: Optional[str] = \"shi-labs/oneformer_demo\",\n@@ -583,7 +583,7 @@ def _preprocess(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n@@ -604,7 +604,7 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n@@ -644,7 +644,7 @@ def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single mask.\"\"\""
        },
        {
            "sha": "97637a83d1a5113e760f5f2ad969807cb3dab9bc",
            "filename": "src/transformers/models/opt/modeling_flax_opt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_flax_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_flax_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_flax_opt.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -585,8 +585,8 @@ def __call__(\n         input_ids: jnp.ndarray,\n         attention_mask: Optional[jnp.ndarray] = None,\n         position_ids: Optional[jnp.ndarray] = None,\n-        params: dict = None,\n-        past_key_values: dict = None,\n+        params: Optional[dict] = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "b7c7785bc68f4428af2104409047e0d16bd92199",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -248,7 +248,7 @@ def __init__(\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_pad: bool = True,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_normalize: bool = True,\n         image_mean: Optional[Union[float, List[float]]] = None,\n@@ -371,7 +371,7 @@ def preprocess(\n         images: ImageInput,\n         do_pad: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "856553336f368875380e8c34cfc4ca71198dd734",
            "filename": "src/transformers/models/patchtsmixer/configuration_patchtsmixer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fconfiguration_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fconfiguration_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fconfiguration_patchtsmixer.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -185,10 +185,10 @@ def __init__(\n         distribution_output: str = \"student_t\",\n         # Prediction head configuration\n         prediction_length: int = 16,\n-        prediction_channel_indices: list = None,\n+        prediction_channel_indices: Optional[list] = None,\n         # Classification/Regression configuration\n         num_targets: int = 3,\n-        output_range: list = None,\n+        output_range: Optional[list] = None,\n         head_aggregation: str = \"max_pool\",\n         **kwargs,\n     ):"
        },
        {
            "sha": "23363380897fdd175eb292217444a052181080af",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -816,7 +816,7 @@ def forward(self, hidden_features):\n def random_masking(\n     inputs: torch.Tensor,\n     mask_ratio: float,\n-    unmasked_channel_indices: list = None,\n+    unmasked_channel_indices: Optional[list] = None,\n     channel_consistent_masking: bool = False,\n     mask_value: int = 0,\n ):\n@@ -875,7 +875,7 @@ def random_masking(\n def forecast_masking(\n     inputs: torch.Tensor,\n     num_forecast_mask_patches: Union[list, int],\n-    unmasked_channel_indices: list = None,\n+    unmasked_channel_indices: Optional[list] = None,\n     mask_value: int = 0,\n ):\n     \"\"\"Forecast masking that masks the last K patches where K is from the num_forecast_mask_patches."
        },
        {
            "sha": "7ee66bec70cac6de7256a5fb45881b827af1803d",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -218,7 +218,7 @@ def forward(self, inputs: torch.Tensor):\n def random_masking(\n     inputs: torch.Tensor,\n     mask_ratio: float,\n-    unmasked_channel_indices: list = None,\n+    unmasked_channel_indices: Optional[list] = None,\n     channel_consistent_masking: bool = False,\n     mask_value: int = 0,\n ):\n@@ -276,7 +276,7 @@ def random_masking(\n def forecast_masking(\n     inputs: torch.Tensor,\n     num_forecast_mask_patches: Union[list, int],\n-    unmasked_channel_indices: list = None,\n+    unmasked_channel_indices: Optional[list] = None,\n     mask_value: int = 0,\n ):\n     \"\"\"Forecast masking that masks the last K patches where K is from the num_forecast_mask_patches."
        },
        {
            "sha": "89b8450312f5448707b70a8d8e86888a93ef39fb",
            "filename": "src/transformers/models/pegasus/modeling_flax_pegasus.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_flax_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_flax_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_flax_pegasus.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -988,7 +988,7 @@ def encode(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1049,12 +1049,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1167,7 +1167,7 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1317,12 +1317,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         deterministic: bool = True,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\""
        },
        {
            "sha": "15176c92b01d82cb0c970e884fec22d9a2738f2d",
            "filename": "src/transformers/models/pegasus/modeling_tf_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_tf_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_tf_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_tf_pegasus.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -933,7 +933,7 @@ def call(\n         encoder_attention_mask: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        past_key_values: Tuple[Tuple[tf.Tensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1170,7 +1170,7 @@ def call(\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n         encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None,\n-        past_key_values: Tuple[Tuple[tf.Tensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "2edd4bef93e1c5cbb1c0f4a21edc6938a0052d24",
            "filename": "src/transformers/models/perceiver/image_processing_perceiver.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -88,9 +88,9 @@ class PerceiverImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,"
        },
        {
            "sha": "f0525212364bc1f34b897ab626282e81262f947b",
            "filename": "src/transformers/models/pix2struct/image_processing_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -213,7 +213,7 @@ def __init__(\n         self,\n         do_convert_rgb: bool = True,\n         do_normalize: bool = True,\n-        patch_size: Dict[str, int] = None,\n+        patch_size: Optional[Dict[str, int]] = None,\n         max_patches: int = 2048,\n         is_vqa: bool = False,\n         **kwargs,"
        },
        {
            "sha": "7cd9149e385ce301e58f4842c02a6ba6e172e98a",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -175,8 +175,8 @@ class PixtralImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n-        patch_size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n+        patch_size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n@@ -320,8 +320,8 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n-        patch_size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n+        patch_size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "25c00cc5fef36bcbddcfd1369860b263579efaf8",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -103,11 +103,11 @@ class PoolFormerImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         crop_pct: int = 0.9,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_rescale: bool = True,\n         do_normalize: bool = True,\n@@ -214,11 +214,11 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         crop_pct: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "437d4efaef2f3dc35b1637f347c5b908f9997a83",
            "filename": "src/transformers/models/pop2piano/processing_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -52,7 +52,7 @@ def __init__(self, feature_extractor, tokenizer):\n     def __call__(\n         self,\n         audio: Union[np.ndarray, List[float], List[np.ndarray]] = None,\n-        sampling_rate: Union[int, List[int]] = None,\n+        sampling_rate: Optional[Union[int, List[int]]] = None,\n         steps_per_beat: int = 2,\n         resample: Optional[bool] = True,\n         notes: Union[List, TensorType] = None,"
        },
        {
            "sha": "6ae239ab137ee91d68adf54c88dc7a126717bb5c",
            "filename": "src/transformers/models/prompt_depth_anything/convert_prompt_depth_anything_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconvert_prompt_depth_anything_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconvert_prompt_depth_anything_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconvert_prompt_depth_anything_to_hf.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -18,6 +18,7 @@\n import argparse\n import re\n from pathlib import Path\n+from typing import Optional\n \n import requests\n import torch\n@@ -130,7 +131,7 @@ def transform_qkv_weights(key, value, config):\n }\n \n \n-def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None):\n     \"\"\"\n     Convert old state dict keys to new keys using regex patterns.\n     \"\"\""
        },
        {
            "sha": "00fcd2b17a1cc8132035d5961c0ea0475dfa00fc",
            "filename": "src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -142,7 +142,7 @@ class PromptDepthAnythingImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         keep_aspect_ratio: bool = False,\n         ensure_multiple_of: int = 1,"
        },
        {
            "sha": "6915fff6f9dbfdb5d21b8d74debcef6128ee1ba7",
            "filename": "src/transformers/models/pvt/image_processing_pvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -150,7 +150,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "830f15df18402194ae3f375c3494af0f50e8b10c",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -124,7 +124,7 @@ class Qwen2VLImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n@@ -170,7 +170,7 @@ def _preprocess(\n         self,\n         images: Union[ImageInput, VideoInput],\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n@@ -303,7 +303,7 @@ def preprocess(\n         images: ImageInput,\n         videos: VideoInput = None,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         min_pixels: Optional[int] = None,\n         max_pixels: Optional[int] = None,\n         resample: PILImageResampling = None,"
        },
        {
            "sha": "350d12ca8710ba65e20b0da961efb40fb16dc1b5",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -256,7 +256,7 @@ def preprocess(\n         images: ImageInput,\n         videos: VideoInput = None,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "1adce9f7b06a49811e8c5ae837dbf1375044b38a",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1375,7 +1375,7 @@ def generate(\n         doc_scores: Optional[torch.FloatTensor] = None,\n         n_docs: Optional[int] = None,\n         generation_config: Optional[GenerationConfig] = None,\n-        prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]] = None,\n+        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n         logits_processor: Optional[LogitsProcessorList] = LogitsProcessorList(),\n         stopping_criteria: Optional[StoppingCriteriaList] = StoppingCriteriaList(),\n         **kwargs,"
        },
        {
            "sha": "8d2921ea149c76fb71d4e80e1baaa4da26ce14b4",
            "filename": "src/transformers/models/regnet/modeling_flax_regnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_flax_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_flax_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_flax_regnet.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -598,7 +598,7 @@ def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: Froz\n     def __call__(\n         self,\n         pixel_values,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         train: bool = False,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "ac4ee0a970f8352f3adcaffa9710d746cb267b93",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -239,7 +239,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Tuple[Tuple[torch.FloatTensor]] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         output_attentions: bool = False,\n     ) -> Tuple:\n         mixed_query_layer = self.query(hidden_states)"
        },
        {
            "sha": "e6aba34cbee5bad4f0e0a8582b707e57ca9e7c8d",
            "filename": "src/transformers/models/resnet/modeling_flax_resnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_flax_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_flax_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_flax_resnet.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -489,7 +489,7 @@ def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: Froz\n     def __call__(\n         self,\n         pixel_values,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         train: bool = False,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "3eb6d539291a64d898c718fd84a337cb3494db0a",
            "filename": "src/transformers/models/roberta/modeling_flax_roberta.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_flax_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_flax_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_flax_roberta.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -824,13 +824,13 @@ def __call__(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "8b77f2bd63b59c5099048fa44e031b5d38868519",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1040,7 +1040,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "ca90ce96fb426d12f11dca1025a767e0e6307dfb",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_flax_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_flax_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_flax_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_flax_roberta_prelayernorm.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -828,13 +828,13 @@ def __call__(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "35c6550ff9f241a1a19e59c94e8df571390f33ed",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -894,7 +894,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "c320407813e44a9a6e4edd59a439518d60e27696",
            "filename": "src/transformers/models/roformer/modeling_flax_roformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_flax_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_flax_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_flax_roformer.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -648,7 +648,7 @@ def __call__(\n         attention_mask=None,\n         token_type_ids=None,\n         head_mask=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "cdefc9ff947f06b1e728a127a16addc31a5fe702",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -440,13 +440,13 @@ def __init__(\n         self,\n         format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = False,\n-        image_mean: Union[float, List[float]] = None,\n-        image_std: Union[float, List[float]] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n         do_convert_annotations: bool = True,\n         do_pad: bool = False,\n         pad_size: Optional[Dict[str, int]] = None,"
        },
        {
            "sha": "1a86ac02af51bd7750b755641f0b4cc90503bf34",
            "filename": "src/transformers/models/rt_detr_v2/convert_rt_detr_v2_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconvert_rt_detr_v2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconvert_rt_detr_v2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconvert_rt_detr_v2_weights_to_hf.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -18,6 +18,7 @@\n import json\n import re\n from pathlib import Path\n+from typing import Optional\n \n import requests\n import torch\n@@ -159,7 +160,7 @@ def get_rt_detr_v2_config(model_name: str) -> RTDetrV2Config:\n }\n \n \n-def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None):\n     # Use the mapping to rename keys\n     for original_key, converted_key in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n         for key in list(state_dict_keys.keys()):"
        },
        {
            "sha": "3142d9d19810a14776c2ecda5445b5d17a253037",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -118,8 +118,8 @@ class SamImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n-        mask_size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n+        mask_size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n@@ -294,7 +294,7 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n@@ -349,7 +349,7 @@ def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n         do_resize: Optional[bool] = None,\n-        mask_size: Dict[str, int] = None,\n+        mask_size: Optional[Dict[str, int]] = None,\n         do_pad: Optional[bool] = None,\n         mask_pad_size: Optional[Dict[str, int]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "84fec78a2ad6bdf1e7cc6b1e1637c2881233e450",
            "filename": "src/transformers/models/segformer/image_processing_segformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -99,7 +99,7 @@ class SegformerImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n@@ -222,7 +222,7 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n@@ -264,7 +264,7 @@ def _preprocess_mask(\n         segmentation_map: ImageInput,\n         do_reduce_labels: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:\n         \"\"\"Preprocesses a single mask.\"\"\"\n@@ -437,7 +437,7 @@ def preprocess(\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->Segformer\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[List[Tuple]] = None):\n         \"\"\"\n         Converts the output of [`SegformerForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n "
        },
        {
            "sha": "b469586de83340834b88bbee65e16766f11a9dc9",
            "filename": "src/transformers/models/seggpt/image_processing_seggpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -247,7 +247,7 @@ def _preprocess_step(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n@@ -394,7 +394,7 @@ def preprocess(\n         prompt_images: Optional[ImageInput] = None,\n         prompt_masks: Optional[ImageInput] = None,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "ae9dabb3ed9faaea986d921e7bb1017e00f3bb6c",
            "filename": "src/transformers/models/siglip/image_processing_siglip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -82,7 +82,7 @@ class SiglipImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n@@ -112,7 +112,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "92b432de0ece6c4b41cf724ec6651562a156bca7",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -292,10 +292,10 @@ def __init__(\n         self,\n         do_convert_rgb: bool = True,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.LANCZOS,\n         do_image_splitting: bool = True,\n-        max_image_size: Dict[str, int] = None,\n+        max_image_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_normalize: bool = True,"
        },
        {
            "sha": "e266460346c315b29abbdf7b32a45a636216d72e",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -474,7 +474,7 @@ def encode(\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n         freeze_feature_encoder: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -542,12 +542,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -671,7 +671,7 @@ def __call__(\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n         freeze_feature_encoder: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\""
        },
        {
            "sha": "caebe86d9c733284e8ffa43d5a693fddd3180744",
            "filename": "src/transformers/models/superglue/configuration_superglue.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, List\n+from typing import TYPE_CHECKING, List, Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n@@ -73,8 +73,8 @@ def __init__(\n         self,\n         keypoint_detector_config: \"SuperPointConfig\" = None,\n         hidden_size: int = 256,\n-        keypoint_encoder_sizes: List[int] = None,\n-        gnn_layers_types: List[str] = None,\n+        keypoint_encoder_sizes: Optional[List[int]] = None,\n+        gnn_layers_types: Optional[List[str]] = None,\n         num_attention_heads: int = 4,\n         sinkhorn_iterations: int = 100,\n         matching_threshold: float = 0.0,"
        },
        {
            "sha": "c2e1f936269a85b5408eb9beebe4251fcf4436c9",
            "filename": "src/transformers/models/superglue/image_processing_superglue.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -161,7 +161,7 @@ class SuperGlueImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n@@ -223,7 +223,7 @@ def preprocess(\n         self,\n         images,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "e0835934df34cfdb6599a5220aa7475b6ac04b2b",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -122,7 +122,7 @@ class SuperPointImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n         do_grayscale: bool = False,\n@@ -181,7 +181,7 @@ def preprocess(\n         self,\n         images,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_grayscale: Optional[bool] = None,"
        },
        {
            "sha": "1fa8da5c2d0bfb2286a79ffd1317f5dfbcda7927",
            "filename": "src/transformers/models/t5/modeling_flax_t5.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_flax_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_flax_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_flax_t5.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -993,7 +993,7 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1078,7 +1078,7 @@ def encode(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1134,12 +1134,12 @@ def decode(\n         encoder_outputs,\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1462,7 +1462,7 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1612,12 +1612,12 @@ def decode(\n         encoder_outputs,\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\""
        },
        {
            "sha": "f75d1db097a6377c9600d43fd08da1b2823a81ea",
            "filename": "src/transformers/models/textnet/image_processing_textnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -94,11 +94,11 @@ class TextNetImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_center_crop: bool = False,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -204,7 +204,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,"
        },
        {
            "sha": "81129a54f1ddd10d2e057e0852a5d68d1b2f862a",
            "filename": "src/transformers/models/tvp/image_processing_tvp.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -136,14 +136,14 @@ class TvpImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_pad: bool = True,\n-        pad_size: Dict[str, int] = None,\n+        pad_size: Optional[Dict[str, int]] = None,\n         constant_values: Union[float, Iterable[float]] = 0,\n         pad_mode: PaddingMode = PaddingMode.CONSTANT,\n         do_normalize: bool = True,\n@@ -219,7 +219,7 @@ def resize(\n     def pad_image(\n         self,\n         image: np.ndarray,\n-        pad_size: Dict[str, int] = None,\n+        pad_size: Optional[Dict[str, int]] = None,\n         constant_values: Union[float, Iterable[float]] = 0,\n         pad_mode: PaddingMode = PaddingMode.CONSTANT,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -267,15 +267,15 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_pad: bool = True,\n-        pad_size: Dict[str, int] = None,\n-        constant_values: Union[float, Iterable[float]] = None,\n+        pad_size: Optional[Dict[str, int]] = None,\n+        constant_values: Optional[Union[float, Iterable[float]]] = None,\n         pad_mode: PaddingMode = None,\n         do_normalize: Optional[bool] = None,\n         do_flip_channel_order: Optional[bool] = None,\n@@ -341,15 +341,15 @@ def preprocess(\n         self,\n         videos: Union[ImageInput, List[ImageInput], List[List[ImageInput]]],\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_pad: Optional[bool] = None,\n-        pad_size: Dict[str, int] = None,\n-        constant_values: Union[float, Iterable[float]] = None,\n+        pad_size: Optional[Dict[str, int]] = None,\n+        constant_values: Optional[Union[float, Iterable[float]]] = None,\n         pad_mode: PaddingMode = None,\n         do_normalize: Optional[bool] = None,\n         do_flip_channel_order: Optional[bool] = None,"
        },
        {
            "sha": "d7e205815208c3d020af9b8326c98cd1c4474071",
            "filename": "src/transformers/models/tvp/modeling_tvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -919,7 +919,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n-        labels: Tuple[torch.Tensor] = None,\n+        labels: Optional[Tuple[torch.Tensor]] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "5fb3c0ce8d5764efd1c324cc2feb1cb66a90de65",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1716,9 +1716,9 @@ def forward(\n         self,\n         input_ids: Optional[Tensor] = None,\n         attention_mask: Optional[Tensor] = None,\n-        bbox: Dict[str, Any] = None,\n+        bbox: Optional[Dict[str, Any]] = None,\n         pixel_values: Optional[Tensor] = None,\n-        visual_bbox: Dict[str, Any] = None,\n+        visual_bbox: Optional[Dict[str, Any]] = None,\n         decoder_input_ids: Optional[Tensor] = None,\n         decoder_attention_mask: Optional[Tensor] = None,\n         inputs_embeds: Optional[Tensor] = None,\n@@ -1892,9 +1892,9 @@ def forward(\n         self,\n         input_ids: Optional[Tensor] = None,\n         attention_mask: Optional[Tensor] = None,\n-        bbox: Dict[str, Any] = None,\n+        bbox: Optional[Dict[str, Any]] = None,\n         pixel_values: Optional[Tensor] = None,\n-        visual_bbox: Dict[str, Any] = None,\n+        visual_bbox: Optional[Dict[str, Any]] = None,\n         decoder_input_ids: Optional[Tensor] = None,\n         decoder_attention_mask: Optional[Tensor] = None,\n         inputs_embeds: Optional[Tensor] = None,\n@@ -2104,10 +2104,10 @@ class PreTrainedModel\n     def forward(\n         self,\n         input_ids: Optional[Tensor] = None,\n-        bbox: Dict[str, Any] = None,\n+        bbox: Optional[Dict[str, Any]] = None,\n         attention_mask: Optional[Tensor] = None,\n         pixel_values: Optional[Tensor] = None,\n-        visual_bbox: Dict[str, Any] = None,\n+        visual_bbox: Optional[Dict[str, Any]] = None,\n         head_mask: Optional[Tensor] = None,\n         inputs_embeds: Optional[Tensor] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "86ae0744d540a8cc3c2db7352324b19a551ee35e",
            "filename": "src/transformers/models/udop/tokenization_udop.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -511,7 +511,7 @@ def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n+        boxes: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         text_target: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         text_pair_target: Optional[\n@@ -545,7 +545,7 @@ def call_boxes(\n         self,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n+        boxes: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "337617f7216099fdf45bc21be2a0fb7c204de873",
            "filename": "src/transformers/models/udop/tokenization_udop_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -243,7 +243,7 @@ def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n+        boxes: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         text_target: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         text_pair_target: Optional[\n@@ -278,7 +278,7 @@ def call_boxes(\n         self,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n         text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n+        boxes: Optional[Union[List[List[int]], List[List[List[int]]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,"
        },
        {
            "sha": "659724168332e7d33f718702ac2c8337b21100c8",
            "filename": "src/transformers/models/video_llava/image_processing_video_llava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -90,10 +90,10 @@ class VideoLlavaImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -172,10 +172,10 @@ def resize(\n     @filter_out_non_signature_kwargs()\n     def preprocess(\n         self,\n-        images: List[ImageInput] = None,\n-        videos: List[VideoInput] = None,\n+        images: Optional[List[ImageInput]] = None,\n+        videos: Optional[List[VideoInput]] = None,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,"
        },
        {
            "sha": "fa0459fabbd6ca8b68cf04c1f2f4f71ebbe9b602",
            "filename": "src/transformers/models/videomae/image_processing_videomae.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fimage_processing_videomae.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -105,10 +105,10 @@ class VideoMAEImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -181,10 +181,10 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n@@ -239,10 +239,10 @@ def preprocess(\n         self,\n         videos: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "4bd7ac55ee7c01c875ba51bd43fc9ce1bfa6c54d",
            "filename": "src/transformers/models/vilt/image_processing_vilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -165,7 +165,7 @@ class ViltImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,"
        },
        {
            "sha": "4d96a68bc1c698168d6606b9328002a343d01966",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_flax_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_flax_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_flax_vision_encoder_decoder.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -400,7 +400,7 @@ def encode(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -473,12 +473,12 @@ def decode(\n         encoder_outputs,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -607,7 +607,7 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\""
        },
        {
            "sha": "020efb3c5c1cd41e73d24cee898ca2094efd2959",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -273,7 +273,7 @@ def __call__(\n         attention_mask=None,\n         position_ids=None,\n         token_type_ids=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,\n@@ -322,7 +322,7 @@ def get_text_features(\n         attention_mask=None,\n         position_ids=None,\n         token_type_ids=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train=False,\n     ):\n@@ -379,7 +379,7 @@ def _get_features(module, input_ids, attention_mask, position_ids, token_type_id\n         )\n \n     def get_image_features(\n-        self, pixel_values, params: dict = None, dropout_rng: jax.random.PRNGKey = None, train=False\n+        self, pixel_values, params: Optional[dict] = None, dropout_rng: jax.random.PRNGKey = None, train=False\n     ):\n         r\"\"\"\n         Args:"
        },
        {
            "sha": "654e56ab910431ed55ea4edfde1f6f0ce68c427a",
            "filename": "src/transformers/models/vit/image_processing_vit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fimage_processing_vit.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -155,7 +155,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "8e86248cde1cf070be550ad8b9cd42e0b680575a",
            "filename": "src/transformers/models/vit/modeling_flax_vit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_flax_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_flax_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_flax_vit.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -474,7 +474,7 @@ def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: Froz\n     def __call__(\n         self,\n         pixel_values,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "e9bbad20354f5467eeba6359c1a117d2dfbb5268",
            "filename": "src/transformers/models/vitpose/convert_vitpose_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -22,6 +22,7 @@\n import argparse\n import os\n import re\n+from typing import Optional\n \n import requests\n import torch\n@@ -160,7 +161,7 @@ def get_config(model_name):\n     return config\n \n \n-def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None):\n     \"\"\"\n     This function should be applied only once, on the concatenated keys to efficiently rename using\n     the key mappings."
        },
        {
            "sha": "fc1a8719391e1c0cd041963725342e223e8027b5",
            "filename": "src/transformers/models/vitpose/image_processing_vitpose.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -353,7 +353,7 @@ class VitPoseImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_affine_transform: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n@@ -425,7 +425,7 @@ def preprocess(\n         images: ImageInput,\n         boxes: Union[List[List[float]], np.ndarray],\n         do_affine_transform: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "7e984f025194b469f86dd85c7bb4fdbe3e1fa96d",
            "filename": "src/transformers/models/vivit/image_processing_vivit.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvivit%2Fimage_processing_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fvivit%2Fimage_processing_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fimage_processing_vivit.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -109,10 +109,10 @@ class VivitImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_center_crop: bool = True,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 127.5,\n         offset: bool = True,\n@@ -228,10 +228,10 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         offset: Optional[bool] = None,\n@@ -291,10 +291,10 @@ def preprocess(\n         self,\n         videos: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Dict[str, int] = None,\n+        crop_size: Optional[Dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         offset: Optional[bool] = None,"
        },
        {
            "sha": "ee188888d7c27c475985158465e8c305d3bd92f3",
            "filename": "src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -891,7 +891,7 @@ def __call__(\n         input_values,\n         attention_mask=None,\n         mask_time_indices=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,\n@@ -1327,7 +1327,7 @@ def __call__(\n         attention_mask=None,\n         mask_time_indices=None,\n         gumbel_temperature: int = 1,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         gumbel_rng: jax.random.PRNGKey = None,\n         train: bool = False,"
        },
        {
            "sha": "decc393dfcada24b418356a47c7a4f3df8922112",
            "filename": "src/transformers/models/whisper/modeling_flax_whisper.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_flax_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_flax_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_flax_whisper.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -867,7 +867,7 @@ class FlaxWhisperPreTrainedModel(FlaxPreTrainedModel):\n     def __init__(\n         self,\n         config: WhisperConfig,\n-        input_shape: Tuple[int] = None,\n+        input_shape: Optional[Tuple[int]] = None,\n         seed: int = 0,\n         dtype: jnp.dtype = jnp.float32,\n         _do_init: bool = True,\n@@ -970,7 +970,7 @@ def encode(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n         **kwargs,\n     ):\n@@ -1025,12 +1025,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1144,7 +1144,7 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1278,12 +1278,12 @@ def decode(\n         encoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_attention_mask: Optional[jnp.ndarray] = None,\n         decoder_position_ids: Optional[jnp.ndarray] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         r\"\"\"\n@@ -1631,7 +1631,7 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n         **kwargs,\n     ):"
        },
        {
            "sha": "96f797ea58cefe7371f27f5d4e9e759575176452",
            "filename": "src/transformers/models/xglm/modeling_flax_xglm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_flax_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_flax_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_flax_xglm.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -619,8 +619,8 @@ def __call__(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         train: bool = False,\n-        params: dict = None,\n-        past_key_values: dict = None,\n+        params: Optional[dict] = None,\n+        past_key_values: Optional[dict] = None,\n         dropout_rng: PRNGKey = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions"
        },
        {
            "sha": "b7fdeda1b23b40f295854e8cf16f4101cc797c24",
            "filename": "src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_flax_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_flax_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_flax_xlm_roberta.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -831,13 +831,13 @@ def __call__(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        params: dict = None,\n+        params: Optional[dict] = None,\n         dropout_rng: jax.random.PRNGKey = None,\n         train: bool = False,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        past_key_values: dict = None,\n+        past_key_values: Optional[dict] = None,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "dcc9bf2344cb4e9f68d4d39a2100be4afa7e3e83",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1043,7 +1043,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "9465bf7a65a5212dd9cf85d667c6ddbafa7386a4",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -996,7 +996,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "681ed17f2fb1938761eee2ae1c26cad7c2b4ee16",
            "filename": "src/transformers/models/yolos/image_processing_yolos.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -668,7 +668,7 @@ def compute_segments(\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n     label_ids_to_fuse: Optional[Set[int]] = None,\n-    target_size: Tuple[int, int] = None,\n+    target_size: Optional[Tuple[int, int]] = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n@@ -777,13 +777,13 @@ def __init__(\n         self,\n         format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Union[float, List[float]] = None,\n-        image_std: Union[float, List[float]] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n         do_convert_annotations: Optional[bool] = None,\n         do_pad: bool = True,\n         pad_size: Optional[Dict[str, int]] = None,"
        },
        {
            "sha": "47920c29c68c1228c97a3914d8c3c98e613ec0eb",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -154,7 +154,7 @@ def __init__(\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_resize: bool = True,\n-        size: Dict[str, int] = None,\n+        size: Optional[Dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         keep_aspect_ratio: bool = True,\n         ensure_multiple_of: int = 32,"
        },
        {
            "sha": "bba2592f3cee0ece3ca78e5ec23e84ce6be9922c",
            "filename": "src/transformers/onnx/config.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fonnx%2Fconfig.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fonnx%2Fconfig.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2Fconfig.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -108,7 +108,9 @@ class OnnxConfig(ABC):\n         \"speech2seq-lm\": OrderedDict({\"logits\": {0: \"batch\", 1: \"sequence\"}}),\n     }\n \n-    def __init__(self, config: \"PretrainedConfig\", task: str = \"default\", patching_specs: List[PatchingSpec] = None):\n+    def __init__(\n+        self, config: \"PretrainedConfig\", task: str = \"default\", patching_specs: Optional[List[PatchingSpec]] = None\n+    ):\n         self._config = config\n \n         if task not in self._tasks_to_common_outputs:"
        },
        {
            "sha": "96b0565d78481a247a4de5ee22c4c1443f464aae",
            "filename": "src/transformers/pipelines/document_question_answering.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -202,7 +202,7 @@ def __call__(\n         self,\n         image: Union[\"Image.Image\", str],\n         question: Optional[str] = None,\n-        word_boxes: Tuple[str, List[float]] = None,\n+        word_boxes: Optional[Tuple[str, List[float]]] = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -283,7 +283,7 @@ def preprocess(\n         padding=\"do_not_pad\",\n         doc_stride=None,\n         max_seq_len=None,\n-        word_boxes: Tuple[str, List[float]] = None,\n+        word_boxes: Optional[Tuple[str, List[float]]] = None,\n         lang=None,\n         tesseract_config=\"\",\n         timeout=None,"
        },
        {
            "sha": "940dea10d76cc7366d7f26fac1652e2a9b3d409b",
            "filename": "src/transformers/pipelines/video_classification.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n import warnings\n from io import BytesIO\n-from typing import List, Union\n+from typing import List, Optional, Union\n \n import requests\n \n@@ -77,7 +77,7 @@ def _sanitize_parameters(self, top_k=None, num_frames=None, frame_sampling_rate=\n             postprocess_params[\"function_to_apply\"] = \"softmax\"\n         return preprocess_params, {}, postprocess_params\n \n-    def __call__(self, inputs: Union[str, List[str]] = None, **kwargs):\n+    def __call__(self, inputs: Optional[Union[str, List[str]]] = None, **kwargs):\n         \"\"\"\n         Assign labels to the video(s) passed as inputs.\n "
        },
        {
            "sha": "83dbd8f21512695ccccbe30a3d2fe3a7f8c41c91",
            "filename": "src/transformers/pipelines/visual_question_answering.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fpipelines%2Fvisual_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fpipelines%2Fvisual_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fvisual_question_answering.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import List, Union\n+from typing import List, Optional, Union\n \n from ..utils import add_end_docstrings, is_torch_available, is_vision_available, logging\n from .base import Pipeline, build_pipeline_init_args\n@@ -79,7 +79,7 @@ def _sanitize_parameters(self, top_k=None, padding=None, truncation=None, timeou\n     def __call__(\n         self,\n         image: Union[\"Image.Image\", str, List[\"Image.Image\"], List[str], \"KeyDataset\"],\n-        question: Union[str, List[str]] = None,\n+        question: Optional[Union[str, List[str]]] = None,\n         **kwargs,\n     ):\n         r\"\"\""
        },
        {
            "sha": "5385d84db769c28068da6ca74ed3116467203505",
            "filename": "src/transformers/pipelines/zero_shot_object_detection.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fpipelines%2Fzero_shot_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Fpipelines%2Fzero_shot_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_object_detection.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import Any, Dict, List, Union\n+from typing import Any, Dict, List, Optional, Union\n \n from ..utils import add_end_docstrings, is_torch_available, is_vision_available, logging, requires_backends\n from .base import ChunkPipeline, build_pipeline_init_args\n@@ -65,7 +65,7 @@ def __init__(self, **kwargs):\n     def __call__(\n         self,\n         image: Union[str, \"Image.Image\", List[Dict[str, Any]]],\n-        candidate_labels: Union[str, List[str]] = None,\n+        candidate_labels: Optional[Union[str, List[str]]] = None,\n         **kwargs,\n     ):\n         \"\"\""
        },
        {
            "sha": "49feecf694d4e590a696388ab9ecd428ae97ca40",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -792,7 +792,7 @@ def number_of_arguments(func):\n \n \n def find_executable_batch_size(\n-    function: callable = None, starting_batch_size: int = 128, auto_find_batch_size: bool = False\n+    function: Optional[callable] = None, starting_batch_size: int = 128, auto_find_batch_size: bool = False\n ):\n     \"\"\"\n     Args:"
        },
        {
            "sha": "9654d5d1fff342e75d2e84b5d3bba9c498ac9486",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1877,7 +1877,7 @@ def __init__(\n         module_file: str,\n         import_structure: IMPORT_STRUCTURE_T,\n         module_spec: Optional[importlib.machinery.ModuleSpec] = None,\n-        extra_objects: Dict[str, object] = None,\n+        extra_objects: Optional[Dict[str, object]] = None,\n     ):\n         super().__init__(name)\n \n@@ -2412,7 +2412,7 @@ def flatten_dict(_dict, previous_key=None):\n \n \n @lru_cache()\n-def define_import_structure(module_path: str, prefix: str = None) -> IMPORT_STRUCTURE_T:\n+def define_import_structure(module_path: str, prefix: Optional[str] = None) -> IMPORT_STRUCTURE_T:\n     \"\"\"\n     This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\n "
        },
        {
            "sha": "72b3837142c37a956cbc81ebadc93d5b5cbafe67",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -1308,13 +1308,13 @@ class CompressedTensorsConfig(QuantizationConfigMixin):\n \n     def __init__(\n         self,\n-        config_groups: Dict[str, Union[\"QuantizationScheme\", List[str]]] = None,  # noqa: F821\n+        config_groups: Optional[Dict[str, Union[\"QuantizationScheme\", List[str]]]] = None,  # noqa: F821\n         format: str = \"dense\",\n         quantization_status: \"QuantizationStatus\" = \"initialized\",  # noqa: F821\n         kv_cache_scheme: Optional[\"QuantizationArgs\"] = None,  # noqa: F821\n         global_compression_ratio: Optional[float] = None,\n         ignore: Optional[List[str]] = None,\n-        sparsity_config: Dict[str, Any] = None,\n+        sparsity_config: Optional[Dict[str, Any]] = None,\n         quant_method: str = \"compressed-tensors\",\n         run_compressed: bool = True,\n         **kwargs,"
        },
        {
            "sha": "b2b96613e63d08d6f4034ff7d48a1c0df60e464a",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -19,6 +19,7 @@\n import unittest\n from copy import deepcopy\n from functools import partial\n+from typing import Optional\n \n import datasets\n from parameterized import parameterized\n@@ -1252,8 +1253,8 @@ def run_and_check(\n         do_eval: bool = True,\n         quality_checks: bool = True,\n         fp32: bool = False,\n-        extra_args_str: str = None,\n-        remove_args_str: str = None,\n+        extra_args_str: Optional[str] = None,\n+        remove_args_str: Optional[str] = None,\n     ):\n         # we are doing quality testing so using a small real model\n         output_dir = self.run_trainer(\n@@ -1285,8 +1286,8 @@ def run_trainer(\n         do_eval: bool = True,\n         distributed: bool = True,\n         fp32: bool = False,\n-        extra_args_str: str = None,\n-        remove_args_str: str = None,\n+        extra_args_str: Optional[str] = None,\n+        remove_args_str: Optional[str] = None,\n     ):\n         max_len = 32\n         data_dir = self.test_file_dir / \"../fixtures/tests_samples/wmt_en_ro\""
        },
        {
            "sha": "dc087559d45bfa92a3d5e353e054e4b39ab4793d",
            "filename": "tests/extended/test_trainer_ext.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/tests%2Fextended%2Ftest_trainer_ext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/tests%2Fextended%2Ftest_trainer_ext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fextended%2Ftest_trainer_ext.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -17,6 +17,7 @@\n import re\n import sys\n from pathlib import Path\n+from typing import Optional\n from unittest.mock import patch\n \n from parameterized import parameterized\n@@ -270,13 +271,13 @@ def run_trainer(\n         learning_rate: float = 3e-3,\n         optim: str = \"adafactor\",\n         distributed: bool = False,\n-        extra_args_str: str = None,\n+        extra_args_str: Optional[str] = None,\n         eval_steps: int = 0,\n         predict_with_generate: bool = True,\n         do_train: bool = True,\n         do_eval: bool = True,\n         do_predict: bool = True,\n-        n_gpus_to_use: int = None,\n+        n_gpus_to_use: Optional[int] = None,\n     ):\n         data_dir = self.test_file_dir / \"../fixtures/tests_samples/wmt_en_ro\"\n         output_dir = self.get_auto_remove_tmp_dir()"
        },
        {
            "sha": "12d2d03b7755c85dd8613ccc0ed4d5df371692c9",
            "filename": "tests/models/bridgetower/test_image_processing_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -41,7 +41,7 @@ def __init__(\n         self,\n         parent,\n         do_resize: bool = True,\n-        size: dict[str, int] = None,\n+        size: Optional[dict[str, int]] = None,\n         size_divisor: int = 32,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,"
        },
        {
            "sha": "ad00eab111f6679c17f5c94277c10e88a87ab6bc",
            "filename": "tests/models/patchtsmixer/test_modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -92,12 +92,12 @@ def __init__(\n         head_dropout: float = 0.2,\n         # forecast related\n         prediction_length: int = 16,\n-        out_channels: int = None,\n+        out_channels: Optional[int] = None,\n         # Classification/regression related\n         # num_labels: int = 3,\n         num_targets: int = 3,\n-        output_range: list = None,\n-        head_aggregation: str = None,\n+        output_range: Optional[list] = None,\n+        head_aggregation: Optional[str] = None,\n         # Trainer related\n         batch_size=13,\n         is_training=True,"
        },
        {
            "sha": "20f4ed7369771fd64496c8891ce4692c9d7663e9",
            "filename": "tests/models/tvp/test_image_processing_tvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -41,12 +41,12 @@ def __init__(\n         do_resize: bool = True,\n         size: dict[str, int] = {\"longest_edge\": 40},\n         do_center_crop: bool = False,\n-        crop_size: dict[str, int] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n         do_rescale: bool = False,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_pad: bool = True,\n         pad_size: dict[str, int] = {\"height\": 80, \"width\": 80},\n-        fill: int = None,\n+        fill: Optional[int] = None,\n         pad_mode: PaddingMode = None,\n         do_normalize: bool = True,\n         image_mean: Optional[Union[float, list[float]]] = [0.48145466, 0.4578275, 0.40821073],"
        },
        {
            "sha": "356473d11adf4a45064e61b6405f560c5c652e48",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -28,7 +28,7 @@\n from functools import lru_cache\n from itertools import takewhile\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n from parameterized import parameterized\n \n@@ -173,7 +173,7 @@ def _test_subword_regularization_tokenizer(in_queue, out_queue, timeout):\n \n def check_subword_sampling(\n     tokenizer: PreTrainedTokenizer,\n-    text: str = None,\n+    text: Optional[str] = None,\n     test_sentencepiece_ignore_case: bool = True,\n ) -> None:\n     \"\"\"\n@@ -321,9 +321,9 @@ def tokenizer_integration_test_util(\n         self,\n         expected_encoding: dict,\n         model_name: str,\n-        revision: str = None,\n-        sequences: list[str] = None,\n-        decode_kwargs: dict[str, Any] = None,\n+        revision: Optional[str] = None,\n+        sequences: Optional[list[str]] = None,\n+        decode_kwargs: Optional[dict[str, Any]] = None,\n         padding: bool = True,\n     ):\n         \"\"\""
        },
        {
            "sha": "9b392f13673347762ea23dd936a687c8726fcf42",
            "filename": "utils/check_copies.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/utils%2Fcheck_copies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/utils%2Fcheck_copies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_copies.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -390,7 +390,7 @@ def split_code_into_blocks(\n \n \n def find_code_in_transformers(\n-    object_name: str, base_path: str = None, return_indices: bool = False\n+    object_name: str, base_path: Optional[str] = None, return_indices: bool = False\n ) -> Union[str, Tuple[List[str], int, int]]:\n     \"\"\"\n     Find and return the source code of an object.\n@@ -491,7 +491,7 @@ def replace_code(code: str, replace_pattern: str) -> str:\n     return code\n \n \n-def find_code_and_splits(object_name: str, base_path: str, buffer: dict = None):\n+def find_code_and_splits(object_name: str, base_path: str, buffer: Optional[dict] = None):\n     \"\"\"Find the code of an object (specified by `object_name`) and split it into blocks.\n \n     Args:\n@@ -638,7 +638,9 @@ def check_codes_match(observed_code: str, theoretical_code: str) -> Optional[int\n         diff_index += 1\n \n \n-def is_copy_consistent(filename: str, overwrite: bool = False, buffer: dict = None) -> Optional[List[Tuple[str, int]]]:\n+def is_copy_consistent(\n+    filename: str, overwrite: bool = False, buffer: Optional[dict] = None\n+) -> Optional[List[Tuple[str, int]]]:\n     \"\"\"\n     Check if the code commented as a copy in a file matches the original.\n \n@@ -831,7 +833,7 @@ def is_copy_consistent(filename: str, overwrite: bool = False, buffer: dict = No\n     return diffs\n \n \n-def check_copies(overwrite: bool = False, file: str = None):\n+def check_copies(overwrite: bool = False, file: Optional[str] = None):\n     \"\"\"\n     Check every file is copy-consistent with the original. Also check the model list in the main README and other\n     READMEs are consistent."
        },
        {
            "sha": "17e3b9d498228bc7215f04b70bb2f8da5266e591",
            "filename": "utils/notification_service.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/utils%2Fnotification_service.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/utils%2Fnotification_service.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -107,7 +107,7 @@ def __init__(\n         ci_title: str,\n         model_results: Dict,\n         additional_results: Dict,\n-        selected_warnings: List = None,\n+        selected_warnings: Optional[List] = None,\n         prev_ci_artifacts=None,\n     ):\n         self.title = title\n@@ -856,7 +856,7 @@ def __init__(self, name: str, single_gpu: bool = False, multi_gpu: bool = False)\n         def __str__(self):\n             return self.name\n \n-        def add_path(self, path: str, gpu: str = None):\n+        def add_path(self, path: str, gpu: Optional[str] = None):\n             self.paths.append({\"name\": self.name, \"path\": path, \"gpu\": gpu})\n \n     _available_artifacts: Dict[str, Artifact] = {}"
        },
        {
            "sha": "afdd35b9a8dedc6ede047f3c39548bd23a4e8cd6",
            "filename": "utils/tests_fetcher.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ff2a5f53dd62a0254410280b67196516124d1/utils%2Ftests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ff2a5f53dd62a0254410280b67196516124d1/utils%2Ftests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftests_fetcher.py?ref=da4ff2a5f53dd62a0254410280b67196516124d1",
            "patch": "@@ -59,7 +59,7 @@\n import tempfile\n from contextlib import contextmanager\n from pathlib import Path\n-from typing import Dict, List, Tuple, Union\n+from typing import Dict, List, Optional, Tuple, Union\n \n from git import Repo\n \n@@ -621,7 +621,7 @@ def get_doctest_files(diff_with_last_commit: bool = False) -> List[str]:\n _re_multi_line_direct_imports = re.compile(r\"(?:^|\\n)\\s*from\\s+transformers(\\S*)\\s+import\\s+\\(([^\\)]+)\\)\")\n \n \n-def extract_imports(module_fname: str, cache: Dict[str, List[str]] = None) -> List[str]:\n+def extract_imports(module_fname: str, cache: Optional[Dict[str, List[str]]] = None) -> List[str]:\n     \"\"\"\n     Get the imports a given module makes.\n \n@@ -703,7 +703,7 @@ def extract_imports(module_fname: str, cache: Dict[str, List[str]] = None) -> Li\n     return result\n \n \n-def get_module_dependencies(module_fname: str, cache: Dict[str, List[str]] = None) -> List[str]:\n+def get_module_dependencies(module_fname: str, cache: Optional[Dict[str, List[str]]] = None) -> List[str]:\n     \"\"\"\n     Refines the result of `extract_imports` to remove subfolders and get a proper list of module filenames: if a file\n     as an import `from utils import Foo, Bar`, with `utils` being a subfolder containing many files, this will traverse\n@@ -953,7 +953,7 @@ def create_reverse_dependency_map() -> Dict[str, List[str]]:\n \n \n def create_module_to_test_map(\n-    reverse_map: Dict[str, List[str]] = None, filter_models: bool = False\n+    reverse_map: Optional[Dict[str, List[str]]] = None, filter_models: bool = False\n ) -> Dict[str, List[str]]:\n     \"\"\"\n     Extract the tests from the reverse_dependency_map and potentially filters the model tests."
        }
    ],
    "stats": {
        "total": 1084,
        "additions": 553,
        "deletions": 531
    }
}