{
    "author": "Cyrilvallez",
    "message": "Remove deprecated cache-related objects (#40035)\n\nremove them",
    "sha": "68eb1a9a6353911f491b1c8139eb73d052a8e9b9",
    "files": [
        {
            "sha": "5f29fabd4f397a2811615f953f80503a3ade06f5",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/68eb1a9a6353911f491b1c8139eb73d052a8e9b9/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/68eb1a9a6353911f491b1c8139eb73d052a8e9b9/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=68eb1a9a6353911f491b1c8139eb73d052a8e9b9",
            "patch": "@@ -426,15 +426,6 @@ A [`Constraint`] can be used to force the generation to include specific tokens\n     - to_legacy_cache\n     - from_legacy_cache\n \n-[[autodoc]] MambaCache\n-    - update_conv_state\n-    - update_ssm_state\n-    - reset\n-\n-[[autodoc]] CacheConfig\n-\n-[[autodoc]] QuantizedCacheConfig\n-\n \n ## Watermark Utils\n "
        },
        {
            "sha": "bf567920610cec6ed7d0d23178773c84f7860a32",
            "filename": "docs/source/ko/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/68eb1a9a6353911f491b1c8139eb73d052a8e9b9/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/68eb1a9a6353911f491b1c8139eb73d052a8e9b9/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md?ref=68eb1a9a6353911f491b1c8139eb73d052a8e9b9",
            "patch": "@@ -405,15 +405,6 @@ generation_output[:2]\n     - to_legacy_cache\n     - from_legacy_cache\n \n-[[autodoc]] MambaCache\n-    - update_conv_state\n-    - update_ssm_state\n-    - reset\n-\n-[[autodoc]] CacheConfig\n-\n-[[autodoc]] QuantizedCacheConfig\n-\n ## 워터마크 유틸리티 (Watermark Utils) [[transformers.WatermarkDetector]]\n \n [[autodoc]] WatermarkDetector"
        },
        {
            "sha": "82db94a3474703bdf579d704d81f02e320ebd7dc",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/68eb1a9a6353911f491b1c8139eb73d052a8e9b9/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68eb1a9a6353911f491b1c8139eb73d052a8e9b9/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=68eb1a9a6353911f491b1c8139eb73d052a8e9b9",
            "patch": "@@ -380,7 +380,6 @@\n         \"QuantoQuantizedLayer\",\n         \"HQQQuantizedLayer\",\n         \"Cache\",\n-        \"CacheConfig\",\n         \"DynamicCache\",\n         \"EncoderDecoderCache\",\n         \"HQQQuantizedCache\",\n@@ -389,7 +388,6 @@\n         \"OffloadedCache\",\n         \"OffloadedStaticCache\",\n         \"QuantizedCache\",\n-        \"QuantizedCacheConfig\",\n         \"QuantoQuantizedCache\",\n         \"SinkCache\",\n         \"SlidingWindowCache\",\n@@ -580,7 +578,6 @@\n if TYPE_CHECKING:\n     # All modeling imports\n     from .cache_utils import Cache as Cache\n-    from .cache_utils import CacheConfig as CacheConfig\n     from .cache_utils import ChunkedSlidingLayer as ChunkedSlidingLayer\n     from .cache_utils import DynamicCache as DynamicCache\n     from .cache_utils import DynamicLayer as DynamicLayer\n@@ -592,7 +589,6 @@\n     from .cache_utils import OffloadedCache as OffloadedCache\n     from .cache_utils import OffloadedStaticCache as OffloadedStaticCache\n     from .cache_utils import QuantizedCache as QuantizedCache\n-    from .cache_utils import QuantizedCacheConfig as QuantizedCacheConfig\n     from .cache_utils import QuantoQuantizedCache as QuantoQuantizedCache\n     from .cache_utils import QuantoQuantizedLayer as QuantoQuantizedLayer\n     from .cache_utils import SinkCache as SinkCache"
        },
        {
            "sha": "9b6e14c6f1952d974d3162c6984c5ab794608231",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 392,
            "changes": 393,
            "blob_url": "https://github.com/huggingface/transformers/blob/68eb1a9a6353911f491b1c8139eb73d052a8e9b9/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68eb1a9a6353911f491b1c8139eb73d052a8e9b9/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=68eb1a9a6353911f491b1c8139eb73d052a8e9b9",
            "patch": "@@ -1,10 +1,6 @@\n-import copy\n-import json\n-import os\n from abc import ABC, abstractmethod\n from collections.abc import Iterable\n-from dataclasses import dataclass\n-from typing import Any, Optional, Union\n+from typing import Any, Optional\n \n import torch\n \n@@ -686,38 +682,6 @@ def _dequantize(self, qtensor):\n }\n \n \n-class KeyValuesWrapper:\n-    \"\"\"Helper class for Cache that simulates layer-indexed key/value lists from a layered cache.\n-    This allows for BC access and writing, e.g., cache.key_cache[idx] = ...\n-    Deprecated in favor of Cache.layers[idx].keys/values. TODO: remove in v4.56.0\"\"\"\n-\n-    def __init__(self, layers, cache_type=\"keys\"):\n-        self.layers = layers\n-        self.cache_type = cache_type\n-\n-    def __getitem__(self, idx):\n-        if isinstance(idx, slice):\n-            return [getattr(layer, self.cache_type) for layer in self.layers[idx]]\n-        return getattr(self.layers[idx], self.cache_type)\n-\n-    def __setitem__(self, idx, value):\n-        if isinstance(idx, slice):\n-            for layer, val in zip(self.layers[idx], value):\n-                setattr(layer, self.cache_type, val)\n-        else:\n-            setattr(self.layers[idx], self.cache_type, value)\n-\n-    def __len__(self):\n-        return len(self.layers)\n-\n-    def __iter__(self):\n-        for layer in self.layers:\n-            yield getattr(layer, self.cache_type)\n-\n-    def __bool__(self):\n-        return bool(self.layers)\n-\n-\n class Cache:\n     \"\"\"\n     A `Cache` is mostly a list of `CacheLayerMixin` objects, one per model layer. It serves as a container for\n@@ -956,22 +920,6 @@ def __len__(self):\n         # forward through all the layers\n         return len(self.layers)\n \n-    @property\n-    def key_cache(self) -> KeyValuesWrapper:\n-        \"\"\"List-like object of key cache tensors indexed by layer. Deprecated in favor of `cache.layers[idx].keys`\"\"\"\n-        logger.warning_once(\n-            \"`cache.key_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].keys` instead.\"\n-        )\n-        return KeyValuesWrapper(self.layers, \"keys\")\n-\n-    @property\n-    def value_cache(self) -> KeyValuesWrapper:\n-        \"\"\"List-like object of value cache tensors indexed by layer. Deprecated in favor of `cache.layers[idx].values`\"\"\"\n-        logger.warning_once(\n-            \"`cache.value_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].values` instead.\"\n-        )\n-        return KeyValuesWrapper(self.layers, \"values\")\n-\n \n class DynamicCache(Cache):\n     \"\"\"\n@@ -1602,342 +1550,3 @@ def __init__(self, **kwargs) -> None:\n             \"`SinkCache` has been moved as a `custom_generate` repository on the Hub: \"\n             \"https://huggingface.co/transformers-community/sink_cache. See the repository for usage examples.\"\n         )\n-\n-\n-@dataclass\n-class CacheConfig:\n-    \"\"\"\n-    Base class for cache configs. Deprecated in favor of a simpler dictionary.\n-    \"\"\"\n-\n-    cache_implementation: None\n-\n-    def __post_init__(self):\n-        logger.warning_once(\n-            \"CacheConfig is deprecated and will be removed in v4.55.0 in favor of a simpler dictionary.\"\n-        )\n-\n-    @classmethod\n-    def from_dict(cls, config_dict, **kwargs):\n-        \"\"\"\n-        Constructs a CacheConfig instance from a dictionary of parameters.\n-        Args:\n-            config_dict (dict[str, Any]): Dictionary containing configuration parameters.\n-            **kwargs: Additional keyword arguments to override dictionary values.\n-\n-        Returns:\n-            CacheConfig: Instance of CacheConfig constructed from the dictionary.\n-        \"\"\"\n-        logger.warning_once(\n-            \"CacheConfig is deprecated and will be removed in v4.55.0 in favor of a simpler dictionary.\"\n-        )\n-        config = cls(**config_dict)\n-        to_remove = []\n-        for key, value in kwargs.items():\n-            if hasattr(config, key):\n-                setattr(config, key, value)\n-                to_remove.append(key)\n-        for key in to_remove:\n-            kwargs.pop(key, None)\n-        return config\n-\n-    # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.to_json_file\n-    def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n-        \"\"\"\n-        Save this instance to a JSON file.\n-\n-        Args:\n-            json_file_path (`str` or `os.PathLike`):\n-                Path to the JSON file in which this configuration instance's parameters will be saved.\n-            use_diff (`bool`, *optional*, defaults to `True`):\n-                If set to `True`, only the difference between the config instance and the default\n-                `QuantizationConfig()` is serialized to JSON file.\n-        \"\"\"\n-        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n-            config_dict = self.to_dict()\n-            json_string = json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n-\n-            writer.write(json_string)\n-\n-    # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.to_dict\n-    def to_dict(self) -> dict[str, Any]:\n-        \"\"\"\n-        Serializes this instance to a Python dictionary. Returns:\n-            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n-        \"\"\"\n-        return copy.deepcopy(self.__dict__)\n-\n-    # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.__iter__\n-    def __iter__(self):\n-        \"\"\"allows `dict(obj)` for situations where obj may be a dict or QuantizationConfigMixin\"\"\"\n-        for attr, value in copy.deepcopy(self.__dict__).items():\n-            yield attr, value\n-\n-    # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.__repr__\n-    def __repr__(self):\n-        return f\"{self.__class__.__name__} {self.to_json_string()}\"\n-\n-    def to_json_string(self):\n-        \"\"\"\n-        Serializes this instance to a JSON formatted string.\n-        Returns:\n-            str: JSON formatted string representing the configuration instance.\n-        \"\"\"\n-        return json.dumps(self.__dict__, indent=2) + \"\\n\"\n-\n-    # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.update\n-    def update(self, **kwargs):\n-        \"\"\"\n-        Updates attributes of this class instance with attributes from `kwargs` if they match existing attributes,\n-        returning all the unused kwargs.\n-\n-        Args:\n-            kwargs (`dict[str, Any]`):\n-                Dictionary of attributes to tentatively update this class.\n-\n-        Returns:\n-            `dict[str, Any]`: Dictionary containing all the key-value pairs that were not used to update the instance.\n-        \"\"\"\n-        to_remove = []\n-        for key, value in kwargs.items():\n-            if hasattr(self, key):\n-                setattr(self, key, value)\n-                to_remove.append(key)\n-\n-        # Remove all the attributes that were updated, without modifying the input dict\n-        unused_kwargs = {key: value for key, value in kwargs.items() if key not in to_remove}\n-        return unused_kwargs\n-\n-\n-@dataclass\n-class QuantizedCacheConfig(CacheConfig):\n-    \"\"\"\n-    Configuration class for quantized cache settings. Deprecated in favor of a simpler dictionary.\n-\n-    Attributes:\n-        backend (`str`, *optional*, defaults to `\"quanto\"`):\n-            Backend to use when performing quantization, Can be one of [`quanto`, `HQQ`]\n-        nbits (`Optional[int]`, *optional*, defaults to 4):\n-            Number of bits, can be 2 or 4 for the `quanto` backend and one of [1, 2, 3, 4, 8] for the `HQQ` backend. Defaults to 2.\n-        axis_key (`int`, *optional*, defaults to 0):\n-            Axis over which to perform grouping for the key tensors. Can be [0, -1] for `quanto` backend and [0, 1] for `HQQ` backend.\n-        axis_value (`int`, *optional*, defaults to 0):\n-            Axis over which to perform grouping for the value tensors. Can be [0, -1] for `quanto` backend and [0, 1] for `HQQ` backend.\n-        q_group_size (`Optional[int]`, *optional*, defaults to 64):\n-            Size of the quantization group, should be a divisor of the model's hidden dimension.\n-            Defaults to 64.\n-        residual_length (`Optional[int]`, *optional*, defaults to 128):\n-            Length of the residual cache which will always be stored in original precision.\n-            Defaults to 128.\n-        compute_dtype (`torch.dtype`, *optional*, defaults to `torch.float16`):\n-            The default dtype used for computations in the model. Keys and Values will be cast to this dtype after dequantization.\n-        device (`str`, *optional*, defaults to `\"cpu\"`):\n-            Device on which to perform computations, should be same as the model's device.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        backend: str = \"quanto\",\n-        nbits: Optional[int] = 4,\n-        axis_key: Optional[int] = 0,\n-        axis_value: Optional[int] = 0,\n-        q_group_size: Optional[int] = 64,\n-        residual_length: Optional[int] = 128,\n-        compute_dtype: Optional[torch.dtype] = torch.float16,\n-        device: Optional[str] = \"cpu\",\n-    ):\n-        logger.warning_once(\n-            \"CacheConfig is deprecated and will be removed in v4.55.0 in favor of a simpler dictionary.\"\n-        )\n-        self.backend = backend\n-        self.nbits = nbits\n-        self.axis_key = axis_key\n-        self.axis_value = axis_value\n-        self.q_group_size = q_group_size\n-        self.residual_length = residual_length\n-        self.compute_dtype = compute_dtype\n-        self.device = device\n-\n-    def validate(self):\n-        \"\"\"Validates if the arguments passed are correct\"\"\"\n-\n-        incorrect_arg_msg = (\n-            \"Some of the keys in `cache_config` are defined incorrectly. `{key}` should be {correct_value}` \"\n-            \"but found {found_value}\"\n-        )\n-        # Check that the values are reasonable in general (nbits, axis)\n-        # Later in QuantizedCache init we check if they are supported for that particular backend\n-        if self.nbits not in [1, 2, 3, 4, 8]:\n-            raise ValueError(\n-                incorrect_arg_msg.format(\n-                    key=\"nbits\",\n-                    correct_value=\"2 or 4 or 8\",\n-                    found_value=self.nbits,\n-                ),\n-            )\n-        if self.q_group_size <= 0:\n-            raise ValueError(\n-                incorrect_arg_msg.format(\n-                    key=\"q_group_size\",\n-                    correct_value=\"a positive integer\",\n-                    found_value=self.q_group_size,\n-                ),\n-            )\n-        if self.residual_length < 0:\n-            raise ValueError(\n-                incorrect_arg_msg.format(\n-                    key=\"residual_length\",\n-                    correct_value=\"a positive integer\",\n-                    found_value=self.residual_length,\n-                ),\n-            )\n-\n-        if self.axis_key not in [0, 1, -1]:\n-            raise ValueError(\n-                incorrect_arg_msg.format(\n-                    key=\"axis_key\",\n-                    correct_value=\"`1` or `0`, `-1`\",\n-                    found_value=self.axis_key,\n-                ),\n-            )\n-\n-        if self.axis_value not in [0, 1, -1]:\n-            raise ValueError(\n-                incorrect_arg_msg.format(\n-                    key=\"axis_value\",\n-                    correct_value=\"`1` or `0` or `-1`\",\n-                    found_value=self.axis_value,\n-                ),\n-            )\n-\n-\n-@dataclass\n-class StaticCacheConfig(CacheConfig):\n-    \"\"\"\n-    Configuration class for static cache settings.\n-    \"\"\"\n-\n-    cache_implementation = \"static\"\n-\n-    def __init__(self, batch_size: int, max_cache_len: int, device=\"cpu\"):\n-        logger.warning_once(\n-            \"CacheConfig is deprecated and will be removed in v4.55.0 in favor of a simpler dictionary.\"\n-        )\n-        self.batch_size = batch_size\n-        self.max_cache_len = max_cache_len\n-        self.device = device\n-\n-\n-# TODO (manuel, joao): remove this class, it is here only for backwards compatibility\n-# PEP 562: Lazy loading for deprecated location of MambaCache\n-def __getattr__(name: str) -> Any:\n-    if name == \"MambaCache\":\n-        logger.warning_once(\n-            \"Importing `MambaCache` from `transformers.cache_utils` is deprecated and will be removed \"\n-            \"in a future version. Please import it from `transformers` or `transformers.models.mamba.cache_mamba` instead.\"\n-        )\n-\n-        class MambaCache:\n-            \"\"\"\n-            Importing `MambaCache` from `transformers.cache_utils` is deprecated and will be removed\n-            in a future version. Please import it from `transformers` or `transformers.models.mamba.cache_mamba` instead.\n-\n-            Cache for mamba model which does not have attention mechanism and key value states.\n-\n-            Arguments:\n-                config (`PretrainedConfig):\n-                    The configuration file defining the shape-related attributes required to initialize the static cache.\n-                max_batch_size (`int`):\n-                    The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a smaller batch size is used.\n-                dtype (`torch.dtype`, *optional*, defaults to `torch.float16`):\n-                    The default `dtype` to use when initializing the layer.\n-                device (`torch.device` or `str`, *optional*):\n-                    The device on which the cache should be initialized. Should be the same as the layer.\n-\n-            Example:\n-\n-                ```python\n-                >>> from transformers import AutoTokenizer, MambaForCausalLM, MambaCache\n-\n-                >>> model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n-                >>> tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n-\n-                >>> inputs = tokenizer(text=\"My name is Mamba\", return_tensors=\"pt\")\n-\n-                >>> # Prepare a cache class and pass it to model's forward\n-                >>> past_key_values = MambaCache(config=model.config, max_batch_size=1, device=model.device, dtype=model.dtype)\n-                >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-                >>> outputs.past_key_values\n-                MambaCache()\n-                ```\n-            \"\"\"\n-\n-            is_compileable = True\n-\n-            # TODO (joao): add layer_device_map arg and update code in `generate` accordingly\n-            def __init__(\n-                self,\n-                config,\n-                max_batch_size: int,\n-                dtype: torch.dtype = torch.float16,\n-                device: Union[torch.device, str, None] = None,\n-            ):\n-                self.max_batch_size = max_batch_size\n-                self._dtype = dtype\n-                self.intermediate_size = config.intermediate_size\n-                self.ssm_state_size = config.state_size\n-                self.conv_kernel_size = config.conv_kernel\n-\n-                self.conv_states: list[torch.Tensor] = []\n-                self.ssm_states: list[torch.Tensor] = []\n-                device = torch.device(device) if device is not None else None\n-                for _ in range(config.num_hidden_layers):\n-                    conv_state: torch.Tensor = torch.zeros(\n-                        self.max_batch_size,\n-                        self.intermediate_size,\n-                        self.conv_kernel_size,\n-                        device=device,\n-                        dtype=self._dtype,\n-                    )\n-                    ssm_state: torch.Tensor = torch.zeros(\n-                        self.max_batch_size,\n-                        self.intermediate_size,\n-                        self.ssm_state_size,\n-                        device=device,\n-                        dtype=self._dtype,\n-                    )\n-\n-                    torch._dynamo.mark_static_address(conv_state)\n-                    torch._dynamo.mark_static_address(ssm_state)\n-                    self.conv_states.append(conv_state)\n-                    self.ssm_states.append(ssm_state)\n-\n-            def update_conv_state(\n-                self, layer_idx: int, new_conv_state: torch.Tensor, cache_position: torch.LongTensor\n-            ) -> torch.Tensor:\n-                # This `if` blocks is only reached in multigpu and if `layer_device_map` is not passed. It is used\n-                # when the cache is initialized in the forward pass (e.g. Mamba)\n-                if self.conv_states[layer_idx].device != new_conv_state.device:\n-                    self.conv_states[layer_idx] = self.conv_states[layer_idx].to(new_conv_state.device)\n-\n-                conv_state = self.conv_states[layer_idx]\n-                cache_position = cache_position.clamp(0, self.conv_kernel_size - 1)\n-\n-                conv_state = conv_state.roll(shifts=-1, dims=-1)\n-                conv_state[:, :, cache_position] = new_conv_state.to(device=conv_state.device, dtype=conv_state.dtype)\n-                self.conv_states[layer_idx].zero_()\n-                self.conv_states[layer_idx] += conv_state\n-                return self.conv_states[layer_idx]\n-\n-            def update_ssm_state(self, layer_idx: int, new_ssm_state: torch.Tensor):\n-                self.ssm_states[layer_idx] = new_ssm_state.to(self.ssm_states[layer_idx].device)\n-                return self.ssm_states[layer_idx]\n-\n-            def reset(self):\n-                for layer_idx in range(len(self.conv_states)):\n-                    # In-place ops prevent breaking the static address\n-                    self.conv_states[layer_idx].zero_()\n-                    self.ssm_states[layer_idx].zero_()\n-\n-        return MambaCache\n-    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")"
        },
        {
            "sha": "85fd051db3b6bda11b2468ae1a1498a84d4afd08",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/68eb1a9a6353911f491b1c8139eb73d052a8e9b9/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68eb1a9a6353911f491b1c8139eb73d052a8e9b9/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=68eb1a9a6353911f491b1c8139eb73d052a8e9b9",
            "patch": "@@ -396,15 +396,6 @@ def __init__(self, **kwargs):\n         self.use_cache = kwargs.pop(\"use_cache\", True)\n         self.cache_implementation = kwargs.pop(\"cache_implementation\", None)\n         self.cache_config = kwargs.pop(\"cache_config\", None)\n-        if self.cache_config is not None and not isinstance(self.cache_config, dict):\n-            warnings.warn(\n-                (\n-                    \"Passing a CacheConfig object is deprecated and will be removed in v4.55.0 in favor of a simpler dictionary.\"\n-                ),\n-                FutureWarning,\n-                stacklevel=2,\n-            )\n-            self.cache_config = self.cache_config.to_dict()\n \n         self.return_legacy_cache = kwargs.pop(\"return_legacy_cache\", None)\n         self.prefill_chunk_size = kwargs.pop(\"prefill_chunk_size\", None)"
        },
        {
            "sha": "c892fb365cbcad3325dde322c4f7289359ec1649",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/68eb1a9a6353911f491b1c8139eb73d052a8e9b9/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68eb1a9a6353911f491b1c8139eb73d052a8e9b9/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=68eb1a9a6353911f491b1c8139eb73d052a8e9b9",
            "patch": "@@ -9,13 +9,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class CacheConfig(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class DynamicCache(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -65,13 +58,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class QuantizedCacheConfig(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class QuantoQuantizedCache(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        }
    ],
    "stats": {
        "total": 438,
        "additions": 1,
        "deletions": 437
    }
}