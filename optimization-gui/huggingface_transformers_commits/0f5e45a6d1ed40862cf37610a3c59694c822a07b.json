{
    "author": "yevvonlim",
    "message": "fix: gas for gemma fixed (#40591)\n\n* fix: gas for gemma fixed\n\n* feat: run fix-copies\n\n* feat: added issue label",
    "sha": "0f5e45a6d1ed40862cf37610a3c59694c822a07b",
    "files": [
        {
            "sha": "c00b22156467a0db10595bfbcff5739d5acb816e",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f5e45a6d1ed40862cf37610a3c59694c822a07b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f5e45a6d1ed40862cf37610a3c59694c822a07b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=0f5e45a6d1ed40862cf37610a3c59694c822a07b",
            "patch": "@@ -969,6 +969,9 @@ class Gemma3ForConditionalGeneration(Gemma3PreTrainedModel, GenerationMixin):\n         \"^language_model.lm_head\": \"lm_head\",\n     }\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    # we are filtering the logits/labels so we shouldn't divide the loss based on num_items_in_batch\n+    # Fix: https://github.com/huggingface/transformers/issues/40564\n+    accepts_loss_kwargs = False\n \n     def __init__(self, config: Gemma3Config):\n         super().__init__(config)"
        },
        {
            "sha": "18f10fc3ad3dcba77c2913ab8834eefec6af5fff",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f5e45a6d1ed40862cf37610a3c59694c822a07b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f5e45a6d1ed40862cf37610a3c59694c822a07b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=0f5e45a6d1ed40862cf37610a3c59694c822a07b",
            "patch": "@@ -869,6 +869,10 @@ def forward(\n \n \n class Gemma3ForConditionalGeneration(PaliGemmaForConditionalGeneration):\n+    # we are filtering the logits/labels so we shouldn't divide the loss based on num_items_in_batch\n+    # Fix: https://github.com/huggingface/transformers/issues/40564\n+    accepts_loss_kwargs = False\n+\n     @auto_docstring\n     def forward(\n         self,"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 7,
        "deletions": 0
    }
}