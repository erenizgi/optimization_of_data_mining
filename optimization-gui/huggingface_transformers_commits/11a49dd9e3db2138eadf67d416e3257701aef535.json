{
    "author": "ahadnagy",
    "message": "T5 test and target device fixes (#40313)\n\n* Fix cache setup related issues\n\n* Fix target-device-related issues\n\n* Ruff\n\n* Address review comments",
    "sha": "11a49dd9e3db2138eadf67d416e3257701aef535",
    "files": [
        {
            "sha": "49b666912246146691f1da3a67c0a3ebc2c06069",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 26,
            "deletions": 10,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/11a49dd9e3db2138eadf67d416e3257701aef535/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11a49dd9e3db2138eadf67d416e3257701aef535/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=11a49dd9e3db2138eadf67d416e3257701aef535",
            "patch": "@@ -820,11 +820,15 @@ def __init__(self, model, max_static_cache_length, batch_size):\n         self.lm_head = model.lm_head\n         self.config = model.config\n \n+        # Detect the device of the exported models by checking a parameter\n+        # We'll use the model's device as the target device\n+        model_device = next(model.parameters()).device\n+\n         # Initialize static cache for decoder and DynamicCache for encoder\n         self.static_cache = StaticCache(config=self.config, max_cache_len=max_static_cache_length)\n         head_dim = getattr(self.config, \"head_dim\", self.config.hidden_size // self.config.num_attention_heads)\n         num_heads = getattr(self.config, \"num_key_value_heads\", self.config.num_attention_heads)\n-        self.static_cache.early_initialization(batch_size, num_heads, head_dim, torch.float32, \"cpu\")\n+        self.static_cache.early_initialization(batch_size, num_heads, head_dim, torch.float32, model_device)\n         self.cache = EncoderDecoderCache(self.static_cache, DynamicCache())\n \n         register_dynamic_cache_export_support()\n@@ -887,16 +891,22 @@ def _export_encoder(self, encoder_input_ids):\n         return exported_encoder\n \n     def _export_decoder(self, decoder_input_ids, encoder_hidden_states, cache_position):\n+        target_device = self.full_model.device\n         wrapped_decoder = (\n             Seq2SeqLMDecoderExportableModuleWithStaticCache(\n                 model=self.full_model,\n-                max_static_cache_length=self.generation_config.cache_config.max_cache_len,\n-                batch_size=self.generation_config.cache_config.batch_size,\n+                max_static_cache_length=self.generation_config.cache_config.get(\"max_cache_len\"),\n+                batch_size=self.generation_config.cache_config.get(\"batch_size\"),\n             )\n-            .to(\"cpu\")\n+            .to(target_device)\n             .eval()\n         )\n \n+        # Move input tensors to the same device as the wrapped decoder\n+        decoder_input_ids = decoder_input_ids.to(target_device)\n+        encoder_hidden_states = encoder_hidden_states.to(target_device)\n+        cache_position = cache_position.to(target_device)\n+\n         # Define dynamic dimension for encoder output sequence length\n         encoder_seq_len_dim = torch.export.Dim(\"encoder_hidden_seq_length\", max=self.max_hidden_seq_length)\n \n@@ -934,7 +944,7 @@ def export(self, encoder_input_ids=None, decoder_input_ids=None, encoder_hidden_\n             encoder_hidden_states\n             if encoder_hidden_states is not None\n             else torch.zeros(\n-                (self.generation_config.cache_config.batch_size, 10, self.config.d_model),\n+                (self.generation_config.cache_config.get(\"batch_size\"), 10, self.config.d_model),\n                 dtype=torch.float32,\n                 device=device,\n             )\n@@ -949,26 +959,32 @@ def export(self, encoder_input_ids=None, decoder_input_ids=None, encoder_hidden_\n \n     def generate(self, prompt_token_ids, max_new_tokens):\n         with torch.no_grad():\n+            model_device = self.full_model.device\n+\n+            # Move input to the model's device if it's on a different device\n+            if prompt_token_ids.device != model_device:\n+                prompt_token_ids = prompt_token_ids.to(model_device)\n+\n             # Run encoder\n             encoder_output = self.exported_encoder.module()(prompt_token_ids)\n \n-            # Initialize with start token (0 for T5)\n-            decoder_input_ids = torch.tensor([[0]], dtype=torch.long)\n+            # Initialize with start token (0 for T5) on the correct device\n+            decoder_input_ids = torch.tensor([[0]], dtype=torch.long, device=model_device)\n             generated_ids = [0]\n \n             # Generate tokens one by one\n             for i in range(max_new_tokens - 1):\n                 # Run decoder for next token prediction\n                 logits = self.exported_decoder.module()(\n-                    decoder_input_ids, encoder_output, torch.tensor([i], dtype=torch.long)\n+                    decoder_input_ids, encoder_output, torch.tensor([i], dtype=torch.long, device=model_device)\n                 )\n \n                 # Get next token\n                 next_token = torch.argmax(logits[:, -1, :], dim=-1).item()\n                 generated_ids.append(next_token)\n \n-                # Update input for next iteration\n-                decoder_input_ids = torch.tensor([[next_token]], dtype=torch.long)\n+                # Update input for next iteration on the correct device\n+                decoder_input_ids = torch.tensor([[next_token]], dtype=torch.long, device=model_device)\n \n                 # Check if EOS token\n                 if next_token == self.config.eos_token_id:"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 26,
        "deletions": 10
    }
}