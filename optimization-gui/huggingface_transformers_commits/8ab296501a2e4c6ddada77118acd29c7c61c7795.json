{
    "author": "Cyrilvallez",
    "message": "Remove deprecation warning for `num_logits_to_keep` (#37149)\n\n* remove everything\n\n* style",
    "sha": "8ab296501a2e4c6ddada77118acd29c7c61c7795",
    "files": [
        {
            "sha": "672adcf149dcf98a74b5be0f41da2a6549192ed1",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -19,7 +19,6 @@\n     add_start_docstrings_to_model_forward,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_new_task_model import NewTaskModelConfig\n \n@@ -328,7 +327,6 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n         image_features = image_features / (self.config.text_config.hidden_size**0.5)\n         return image_features\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(NEW_TASK_MODEL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=NewTaskModelCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "7f88875f3e7c5bfb49146c10f8317c8b30dbd17b",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -41,7 +41,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_torch_available\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_aria import AriaConfig, AriaTextConfig\n@@ -1160,7 +1159,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(ARIA_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1417,7 +1415,6 @@ def get_image_features(\n         return image_features\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=AriaConfig)\n     def forward("
        },
        {
            "sha": "d331789354604de2c484d51fa814b3892733f8dc",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -49,7 +49,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_torch_available\n from ..auto import CONFIG_MAPPING, AutoConfig, AutoModel, AutoModelForCausalLM, AutoTokenizer\n from ..llama.configuration_llama import LlamaConfig\n@@ -1452,7 +1451,6 @@ def get_image_features(\n         return image_features\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=AriaConfig)\n     def forward("
        },
        {
            "sha": "13e3dfdb43b7d9ab81b2cd9cb9a7eea20dfe5bd0",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -35,7 +35,6 @@\n     is_torchdynamo_compiling,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_aya_vision import AyaVisionConfig\n \n@@ -343,7 +342,6 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature)\n         return image_features\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(AYA_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=AyaVisionCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "62496ba4ebdc08edb08257b86215b1168681b10c",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -48,7 +48,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_bamba import BambaConfig\n \n@@ -1451,7 +1450,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "8d64fdd989feaec6d0f11e41d050961fd369a8b1",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -55,7 +55,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n     is_flash_attn_2_available,\n@@ -1188,7 +1187,6 @@ def _update_mamba_mask(self, attention_mask, cache_position):\n \n class BambaForCausalLM(LlamaForCausalLM):\n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "a250d47809f918717494254f4dbfb5f6f4bec8ae",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -51,7 +51,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_cohere import CohereConfig\n \n \n@@ -799,7 +798,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(COHERE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "a7189a1a212b6fd695a6b9bbb2f071ca7da19ef2",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -42,7 +42,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_cohere2 import Cohere2Config\n \n \n@@ -797,7 +796,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(COHERE2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "729e877cc8f3baf1fbe476f55c7a0a312003674e",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -35,7 +35,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_dbrx import DbrxConfig\n \n \n@@ -1273,7 +1272,6 @@ def set_decoder(self, decoder: DbrxModel):\n     def get_decoder(self) -> DbrxModel:\n         return self.transformer\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(DBRX_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "546c7fed3d374d8fe62e29d11cb702494d4504fe",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -31,7 +31,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_deepseek_v3 import DeepseekV3Config\n \n \n@@ -942,7 +941,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(DEEPSEEK_V3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "1959fac86e04f5d35ca01bd1cd4e31d44dc3df9b",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -58,7 +58,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_diffllama import DiffLlamaConfig\n \n \n@@ -1045,7 +1044,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(DIFFLLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "f74b0cacb01ee2a87fd3394fdbc9364c0954f6c4",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -47,7 +47,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_emu3 import Emu3Config, Emu3TextConfig, Emu3VQVAEConfig\n \n \n@@ -1631,7 +1630,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(EMU3_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"Emu3TextConfig\")\n     def forward("
        },
        {
            "sha": "c4e35e71d21baacd46310d029b9caf7d5925457a",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -36,7 +36,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..chameleon.modeling_chameleon import (\n     ChameleonPreTrainedModel,\n     ChameleonVQVAEEncoderConvDownsample,\n@@ -1085,7 +1084,6 @@ def __init__(self, config):\n         self.model = Emu3TextModel(config)\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(EMU3_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"Emu3TextConfig\")\n     def forward(**super_kwargs):"
        },
        {
            "sha": "86b974570d68cd9ac3735a27e3d834ae5fadb425",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -45,7 +45,6 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_falcon import FalconConfig\n \n \n@@ -1151,7 +1150,6 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings: torch.Tensor):\n         self.lm_head = new_embeddings\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(FALCON_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,"
        },
        {
            "sha": "df66bc36e42d1593e4c0206c6c07c497df51ce5f",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -48,7 +48,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_gemma import GemmaConfig\n \n \n@@ -764,7 +763,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GEMMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "f0d340048fb777c9cd53b1bfe60a052547fe32e2",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -47,7 +47,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_gemma2 import Gemma2Config\n \n \n@@ -804,7 +803,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GEMMA2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "50ca08a3f10af594774330112df61adbf5629876",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -45,7 +45,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_gemma3 import Gemma3Config, Gemma3TextConfig\n \n@@ -891,7 +890,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GEMMA3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1201,7 +1199,6 @@ def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return image_features\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GEMMA3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Gemma3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "f2e716f21628e4e9f776febd42887b6f0097d1db",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -41,7 +41,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n     Gemma2Attention,\n@@ -847,7 +846,6 @@ def _update_causal_mask(\n         return causal_mask\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GEMMA3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Gemma3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "a12057cbb245d93e00a97e65ed6c7469180b09ae",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -50,7 +50,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_glm import GlmConfig\n \n \n@@ -780,7 +779,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GLM_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "8eb015ac00385600c1fb85caf522ff044701e458",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -50,7 +50,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_glm4 import Glm4Config\n \n \n@@ -421,12 +420,15 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n+            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n+            but you can also pass a `BlockMask` object directly here.\n+\n             [What are attention masks?](../glossary#attention-mask)\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n@@ -623,7 +625,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -636,8 +638,7 @@ def _update_causal_mask(\n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n                 attention_mask = make_flex_block_causal_mask(attention_mask)\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -783,7 +784,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GLM4_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "1db20f8624ff9d1a99cd5c998f60cf6837974f2d",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -44,7 +44,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_granite import GraniteConfig\n \n \n@@ -780,7 +779,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GRANITE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "16fcdc3a77ef570502b82857e76b74efa577fc55",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -50,7 +50,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_helium import HeliumConfig\n \n \n@@ -765,7 +764,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(HELIUM_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "186c7be6bbc9dcc55763af10052fb075eed8576a",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -34,7 +34,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel\n from .configuration_idefics2 import Idefics2Config, Idefics2PerceiverConfig, Idefics2VisionConfig\n \n@@ -1293,7 +1292,6 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(IDEFICS2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Idefics2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "64d939e7b4b1ca6c9ef06fa2046d5261dd4bf16b",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -47,7 +47,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n     is_mamba_ssm_available,\n@@ -1434,7 +1433,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(JAMBA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "bca39ba83e2f39a687ec78e99ec36ac2597328f4",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -42,7 +42,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_jetmoe import JetMoeConfig\n \n \n@@ -1259,7 +1258,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(JETMOE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "a8bebd2a365b2b55b620a3c35c347396c0e7d239",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -50,7 +50,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_llama import LlamaConfig\n \n \n@@ -770,7 +769,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "ba5277d4ff33d000a4c31663821f34f2e25b81ce",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -32,7 +32,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_llava import LlavaConfig\n \n@@ -313,7 +312,6 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature)\n         return image_features\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(LLAVA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=LlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "b3eae9c4431bfc0720b193c4fb22f5264cdaf0c7",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -35,7 +35,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_llava_next import LlavaNextConfig\n \n@@ -525,7 +524,6 @@ def get_image_features(\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n         return image_features\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(LLAVA_NEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=LlavaNextCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "1965f08bb460980ee2e4ac3cc10153ff746e8692",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -39,7 +39,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_llava_next_video import LlavaNextVideoConfig\n \n@@ -565,7 +564,6 @@ def get_image_features(\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n         return image_features\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(LLAVA_NEXT_VIDEO_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=LlavaNextVideoCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "ec928380914475e75c9cb4853f3da9c4e71d188f",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -33,7 +33,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_llava_onevision import LlavaOnevisionConfig\n \n@@ -586,7 +585,6 @@ def get_video_features(\n \n         return video_features\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings(LLAVA_ONEVISION_INPUTS_DOCSTRING)\n     def forward(\n         self,"
        },
        {
            "sha": "20e8aca6222d2c0fe0d65b10f13205b6d9b3a413",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -36,7 +36,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_mistral import MistralConfig\n \n \n@@ -769,7 +768,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "c27a646dd68f6ecf8e93d3658faf03b5994794dd",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -36,7 +36,6 @@\n     is_torchdynamo_compiling,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_mistral3 import Mistral3Config\n \n@@ -364,7 +363,6 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature.squeeze(0), image_sizes)\n         return image_features\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(MISTRAL3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Mistral3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "377e18b875ebf20ef63d17c4e2f755f1bd0b1c3a",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -59,7 +59,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_mixtral import MixtralConfig\n \n \n@@ -983,7 +982,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(MIXTRAL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "db961a30dc82cdd4f37604b5d5ba4fcd27ee2e95",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -36,7 +36,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_mllama import MllamaConfig, MllamaTextConfig, MllamaVisionConfig\n \n \n@@ -1873,7 +1872,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(MLLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"MllamaTextConfig\")\n     def forward(\n@@ -2018,7 +2016,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(MLLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"MllamaConfig\")\n     def forward("
        },
        {
            "sha": "6e990c44028dafc91f9e4c2fb0b74c68d97df2ed",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -48,7 +48,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto.modeling_auto import AutoModel\n from .configuration_moshi import MoshiConfig, MoshiDepthConfig\n \n@@ -1777,7 +1776,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(MOSHI_DECODER_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoshiCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "d33cb3a24b0e79f8831a4abcce3321fa04e55ef5",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -47,7 +47,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_nemotron import NemotronConfig\n \n \n@@ -1011,7 +1010,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(NEMOTRON_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     # Ignore copy (doc string different)"
        },
        {
            "sha": "aa7d6e7445ea1da880f5c32f8310295951f6a7e2",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -29,7 +29,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_olmo import OlmoConfig\n \n \n@@ -740,7 +739,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(OLMO_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "999b2ded0525b5d9e464981382df88e0eb6243b0",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -29,7 +29,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_olmo2 import Olmo2Config\n \n \n@@ -746,7 +745,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(OLMO2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "429ea28413a4cb18c47d4badb29558ee0b6e5efb",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -37,7 +37,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_olmoe import OlmoeConfig\n \n \n@@ -1159,7 +1158,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(OLMOE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "b7ac3b751b52f48c26a219ce67a3e427adc43083",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -33,7 +33,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_paligemma import PaliGemmaConfig\n \n@@ -405,7 +404,6 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n         image_features = image_features / (self.config.text_config.hidden_size**0.5)\n         return image_features\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PALIGEMMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=PaliGemmaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "564865ddef6d664b3455a8811f448280ecebc9ac",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -47,7 +47,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_persimmon import PersimmonConfig\n \n \n@@ -817,7 +816,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PERSIMMON_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "8b8bd4b8e82c4d5b546f45f9f9b27a4036ee56b3",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -34,7 +34,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_phi import PhiConfig\n \n \n@@ -738,7 +737,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PHI_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "4955857247fe943decb69d713896383e357eeaa1",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -51,7 +51,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_phi3 import Phi3Config\n \n \n@@ -824,7 +823,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PHI3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "683aa431cef3b26b6843b8390a292e4aaf7e09dc",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -42,7 +42,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_phimoe import PhimoeConfig\n \n \n@@ -1372,7 +1371,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PHIMOE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     # Ignore copy"
        },
        {
            "sha": "2d24e228c227de9af37d95dcd0fa2eed1a40e12a",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -36,7 +36,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_qwen2 import Qwen2Config\n \n \n@@ -782,7 +781,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "718b719e58a5bc067804fbeb9975dbb20ae764de",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -43,6 +43,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n@@ -68,6 +69,12 @@\n     apply_rotary_emb = None\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n@@ -2035,7 +2042,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -2053,6 +2060,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n@@ -2747,7 +2758,7 @@ def forward(\n \n     def _update_causal_mask(\n         self,\n-        attention_mask: torch.Tensor,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n@@ -2765,6 +2776,10 @@ def _update_causal_mask(\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "d780d6051ab82ac75385544579bd7bf5f5ddea28",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -50,7 +50,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_qwen2_moe import Qwen2MoeConfig\n \n \n@@ -1230,7 +1229,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(QWEN2MOE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "8416478e5ad60778dea256ac767c5682eed8b021",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -51,7 +51,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_qwen3 import Qwen3Config\n \n \n@@ -809,7 +808,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "daf8335acd8639264aa38d5779ae7d73077c78f5",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -54,7 +54,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_qwen3_moe import Qwen3MoeConfig\n \n \n@@ -997,7 +996,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(QWEN3_MOE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "a0a65ea37c8dfa6e6e6cc50860fb937f78bf75e6",
            "filename": "src/transformers/models/shieldgemma2/modeling_shieldgemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -26,7 +26,6 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModelForImageTextToText\n from .configuration_shieldgemma2 import ShieldGemma2Config\n \n@@ -150,7 +149,6 @@ def get_decoder(self):\n     def tie_weights(self):\n         return self.model.language_model.tie_weights()\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(SHIELDGEMMA2_INPUTS_DOCSTRING)\n     def forward(\n         self,"
        },
        {
            "sha": "85e20b44932b898867f3eaffdbd4124e9093aa0f",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -48,7 +48,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_stablelm import StableLmConfig\n \n \n@@ -1072,7 +1071,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(STABLELM_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     # Ignore copy"
        },
        {
            "sha": "0299cab66f1359634b904c373e8e8c6eace589cc",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -53,7 +53,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_starcoder2 import Starcoder2Config\n \n \n@@ -759,7 +758,6 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(STARCODER2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "f092bba196d3d9f93516613beae6545303e741b2",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -32,7 +32,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_video_llava import VideoLlavaConfig\n \n@@ -359,7 +358,6 @@ def get_video_features(\n \n         return video_features, num_frames\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(VIDEO_LLAVA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=VideoLlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "c6060b756eb8bf7797d36a256c7d69f60eec876d",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -32,7 +32,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_vipllava import VipLlavaConfig\n \n@@ -288,7 +287,6 @@ def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_lay\n         image_features = self.multi_modal_projector(image_features)\n         return image_features\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(VIPLLAVA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=VipLlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     # Ignore copy"
        },
        {
            "sha": "29c54fbdf48ce5f73538e4d15e1b77fbc1bb72b3",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -48,7 +48,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n     is_mamba_ssm_available,\n@@ -1208,7 +1207,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(ZAMBA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "7854b04bc63d6ce4bd77062d7d2c281d8fee712d",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ab296501a2e4c6ddada77118acd29c7c61c7795/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=8ab296501a2e4c6ddada77118acd29c7c61c7795",
            "patch": "@@ -43,7 +43,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n from .configuration_zamba2 import Zamba2Config\n \n@@ -1617,7 +1616,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n-    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(ZAMBA2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        }
    ],
    "stats": {
        "total": 142,
        "additions": 23,
        "deletions": 119
    }
}