{
    "author": "stevhliu",
    "message": "[docs] Update docs moved to the course (#38800)\n\n* update\n\n* update\n\n* update not_doctested.txt\n\n* slow_documentation_tests.txt",
    "sha": "fdb5da59ddf0e4f79623adc2941dd34bea59225b",
    "files": [
        {
            "sha": "7f9dbaea0570cf22310026d94ff2ef9390ea1bfd",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/fdb5da59ddf0e4f79623adc2941dd34bea59225b/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/fdb5da59ddf0e4f79623adc2941dd34bea59225b/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=fdb5da59ddf0e4f79623adc2941dd34bea59225b",
            "patch": "@@ -23,14 +23,6 @@\n       title: Modular Transformers\n     - local: auto_docstring\n       title: Document your models\n-    - local: task_summary\n-      title: What ðŸ¤— Transformers can do\n-    - local: tasks_explained\n-      title: How ðŸ¤— Transformers solve tasks\n-    - local: model_summary\n-      title: The Transformer model family\n-    - local: attention\n-      title: Attention mechanisms\n     - local: attention_interface\n       title: Customizing attention function\n     title: Models"
        },
        {
            "sha": "02e4db58f5bea0c144f51e29eaf5cab77789ce11",
            "filename": "docs/source/en/attention.md",
            "status": "removed",
            "additions": 0,
            "deletions": 61,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b73799500a69242d0afa73c1256bcaf7a3fff37/docs%2Fsource%2Fen%2Fattention.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b73799500a69242d0afa73c1256bcaf7a3fff37/docs%2Fsource%2Fen%2Fattention.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fattention.md?ref=8b73799500a69242d0afa73c1256bcaf7a3fff37",
            "patch": "@@ -1,61 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Attention mechanisms\n-\n-Most transformer models use full attention in the sense that the attention matrix is square. It can be a big\n-computational bottleneck when you have long texts. Longformer and reformer are models that try to be more efficient and\n-use a sparse version of the attention matrix to speed up training.\n-\n-## LSH attention\n-\n-[Reformer](model_doc/reformer) uses LSH attention. In the softmax(QK^t), only the biggest elements (in the softmax\n-dimension) of the matrix QK^t are going to give useful contributions. So for each query q in Q, we can consider only\n-the keys k in K that are close to q. A hash function is used to determine if q and k are close. The attention mask is\n-modified to mask the current token (except at the first position), because it will give a query and a key equal (so\n-very similar to each other). Since the hash can be a bit random, several hash functions are used in practice\n-(determined by a n_rounds parameter) and then are averaged together.\n-\n-## Local attention\n-\n-[Longformer](model_doc/longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\n-left and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\n-window, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\n-representation of the whole sentence.\n-\n-Some preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\n-all tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\n-their local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:\n-\n-<div class=\"flex justify-center\">\n-    <img scale=\"50 %\" align=\"center\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png\"/>\n-</div>\n-\n-Using those attention matrices with less parameters then allows the model to have inputs having a bigger sequence\n-length.\n-\n-## Other tricks\n-\n-### Axial positional encodings\n-\n-[Reformer](model_doc/reformer) uses axial positional encodings: in traditional transformer models, the positional encoding\n-E is a matrix of size \\\\(l\\\\) by \\\\(d\\\\), \\\\(l\\\\) being the sequence length and \\\\(d\\\\) the dimension of the\n-hidden state. If you have very long texts, this matrix can be huge and take way too much space on the GPU. To alleviate\n-that, axial positional encodings consist of factorizing that big matrix E in two smaller matrices E1 and E2, with\n-dimensions \\\\(l_{1} \\times d_{1}\\\\) and \\\\(l_{2} \\times d_{2}\\\\), such that \\\\(l_{1} \\times l_{2} = l\\\\) and\n-\\\\(d_{1} + d_{2} = d\\\\) (with the product for the lengths, this ends up being way smaller). The embedding for time\n-step \\\\(j\\\\) in E is obtained by concatenating the embeddings for timestep \\\\(j \\% l1\\\\) in E1 and \\\\(j // l1\\\\)\n-in E2."
        },
        {
            "sha": "ab0677b5a54e0e061039c63942195bf333fe79f4",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/fdb5da59ddf0e4f79623adc2941dd34bea59225b/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fdb5da59ddf0e4f79623adc2941dd34bea59225b/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=fdb5da59ddf0e4f79623adc2941dd34bea59225b",
            "patch": "@@ -59,3 +59,6 @@ Transformers is designed for developers and machine learning engineers and resea\n   </a>\n </div>\n \n+## Learn\n+\n+If you're new to Transformers or want to learn more about transformer models, we recommend starting with the [LLM course](https://huggingface.co/learn/llm-course/chapter1/1?fw=pt). This comprehensive course covers everything from the fundamentals of how transformer models work to practical applications across various tasks. You'll learn the complete workflow, from curating high-quality datasets to fine-tuning large language models and implementing reasoning capabilities. The course contains both theoretical and hands-on exercises to build a solid foundational knowledge of transformer models as you learn.\n\\ No newline at end of file"
        },
        {
            "sha": "0836f27283b3b4cd8f66f51b3253060185997ba0",
            "filename": "docs/source/en/model_summary.md",
            "status": "removed",
            "additions": 0,
            "deletions": 107,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b73799500a69242d0afa73c1256bcaf7a3fff37/docs%2Fsource%2Fen%2Fmodel_summary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b73799500a69242d0afa73c1256bcaf7a3fff37/docs%2Fsource%2Fen%2Fmodel_summary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_summary.md?ref=8b73799500a69242d0afa73c1256bcaf7a3fff37",
            "patch": "@@ -1,107 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# The Transformer model family\n-\n-Since its introduction in 2017, the [original Transformer](https://huggingface.co/papers/1706.03762) model (see the [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) blog post for a gentle technical introduction) has inspired many new and exciting models that extend beyond natural language processing (NLP) tasks. There are models for [predicting the folded structure of proteins](https://huggingface.co/blog/deep-learning-with-proteins), [training a cheetah to run](https://huggingface.co/blog/train-decision-transformers), and [time series forecasting](https://huggingface.co/blog/time-series-transformers). With so many Transformer variants available, it can be easy to miss the bigger picture. What all these models have in common is they're based on the original Transformer architecture. Some models only use the encoder or decoder, while others use both. This provides a useful taxonomy to categorize and examine the high-level differences within models in the Transformer family, and it'll help you understand Transformers you haven't encountered before.\n-\n-If you aren't familiar with the original Transformer model or need a refresher, check out the [How do Transformers work](https://huggingface.co/course/chapter1/4?fw=pt) chapter from the Hugging Face course.\n-\n-<div align=\"center\">\n-    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/H39Z_720T5s\" title=\"YouTube video player\"\n-    frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope;\n-    picture-in-picture\" allowfullscreen></iframe>\n-</div>\n-\n-## Computer vision\n-\n-<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1\" allowfullscreen></iframe> \n-\n-### Convolutional network\n-\n-For a long time, convolutional networks (CNNs) were the dominant paradigm for computer vision tasks until the [Vision Transformer](https://huggingface.co/papers/2010.11929) demonstrated its scalability and efficiency. Even then, some of a CNN's best qualities, like translation invariance, are so powerful (especially for certain tasks) that some Transformers incorporate convolutions in their architecture. [ConvNeXt](model_doc/convnext) flipped this exchange around and incorporated design choices from Transformers to modernize a CNN. For example, ConvNeXt uses non-overlapping sliding windows to patchify an image and a larger kernel to increase its global receptive field. ConvNeXt also makes several layer design choices to be more memory-efficient and improve performance, so it competes favorably with Transformers!\n-\n-### Encoder[[cv-encoder]]\n-\n-The [Vision Transformer (ViT)](model_doc/vit) opened the door to computer vision tasks without convolutions. ViT uses a standard Transformer encoder, but its main breakthrough was how it treated an image. It splits an image into fixed-size patches and uses them to create an embedding, just like how a sentence is split into tokens. ViT capitalized on the Transformers' efficient architecture to demonstrate competitive results with the CNNs at the time while requiring fewer resources to train. ViT was soon followed by other vision models that could also handle dense vision tasks like segmentation as well as detection.\n-\n-One of these models is the [Swin](model_doc/swin) Transformer. It builds hierarchical feature maps (like a CNN ðŸ‘€ and unlike ViT) from smaller-sized patches and merges them with neighboring patches in deeper layers. Attention is only computed within a local window, and the window is shifted between attention layers to create connections to help the model learn better. Since the Swin Transformer can produce hierarchical feature maps, it is a good candidate for dense prediction tasks like segmentation and detection. The [SegFormer](model_doc/segformer) also uses a Transformer encoder to build hierarchical feature maps, but it adds a simple multilayer perceptron (MLP) decoder on top to combine all the feature maps and make a prediction.\n-\n-Other vision models, like BeIT and ViTMAE, drew inspiration from BERT's pretraining objective. [BeIT](model_doc/beit) is pretrained by *masked image modeling (MIM)*; the image patches are randomly masked, and the image is also tokenized into visual tokens. BeIT is trained to predict the visual tokens corresponding to the masked patches. [ViTMAE](model_doc/vitmae) has a similar pretraining objective, except it must predict the pixels instead of visual tokens. What's unusual is 75% of the image patches are masked! The decoder reconstructs the pixels from the masked tokens and encoded patches. After pretraining, the decoder is thrown away, and the encoder is ready to be used in downstream tasks.\n-\n-### Decoder[[cv-decoder]]\n-\n-Decoder-only vision models are rare because most vision models rely on an encoder to learn an image representation. But for use cases like image generation, the decoder is a natural fit, as we've seen from text generation models like GPT-2. [ImageGPT](model_doc/imagegpt) uses the same architecture as GPT-2, but instead of predicting the next token in a sequence, it predicts the next pixel in an image. In addition to image generation, ImageGPT could also be finetuned for image classification.\n-\n-### Encoder-decoder[[cv-encoder-decoder]]\n-\n-Vision models commonly use an encoder (also known as a backbone) to extract important image features before passing them to a Transformer decoder. [DETR](model_doc/detr) has a pretrained backbone, but it also uses the complete Transformer encoder-decoder architecture for object detection. The encoder learns image representations and combines them with object queries (each object query is a learned embedding that focuses on a region or object in an image) in the decoder. DETR predicts the bounding box coordinates and class label for each object query.\n-\n-## Natural language processing\n-\n-<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1\" allowfullscreen></iframe>\n-\n-### Encoder[[nlp-encoder]]\n-\n-[BERT](model_doc/bert) is an encoder-only Transformer that randomly masks certain tokens in the input to avoid seeing other tokens, which would allow it to \"cheat\". The pretraining objective is to predict the masked token based on the context. This allows BERT to fully use the left and right contexts to help it learn a deeper and richer representation of the inputs. However, there was still room for improvement in BERT's pretraining strategy. [RoBERTa](model_doc/roberta) improved upon this by introducing a new pretraining recipe that includes training for longer and on larger batches, randomly masking tokens at each epoch instead of just once during preprocessing, and removing the next-sentence prediction objective. \n-\n-The dominant strategy to improve performance is to increase the model size. But training large models is computationally expensive. One way to reduce computational costs is using a smaller model like [DistilBERT](model_doc/distilbert). DistilBERT uses [knowledge distillation](https://huggingface.co/papers/1503.02531) - a compression technique - to create a smaller version of BERT while keeping nearly all of its language understanding capabilities. \n-\n-However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\n-\n-### Decoder[[nlp-decoder]]\n-\n-[GPT-2](model_doc/gpt2) is a decoder-only Transformer that predicts the next word in the sequence. It masks tokens to the right so the model can't \"cheat\" by looking ahead. By pretraining on a massive body of text, GPT-2 became really good at generating text, even if the text is only sometimes accurate or true. But GPT-2 lacked the bidirectional context from BERT's pretraining, which made it unsuitable for certain tasks. [XLNET](model_doc/xlnet) combines the best of both BERT and GPT-2's pretraining objectives by using a permutation language modeling objective (PLM) that allows it to learn bidirectionally.\n-\n-After GPT-2, language models grew even bigger and are now known as *large language models (LLMs)*. LLMs demonstrate few- or even zero-shot learning if pretrained on a large enough dataset. [GPT-J](model_doc/gptj) is an LLM with 6B parameters and trained on 400B tokens. GPT-J was followed by [OPT](model_doc/opt), a family of decoder-only models, the largest of which is 175B and trained on 180B tokens. [BLOOM](model_doc/bloom) was released around the same time, and the largest model in the family has 176B parameters and is trained on 366B tokens in 46 languages and 13 programming languages.\n-\n-### Encoder-decoder[[nlp-encoder-decoder]]\n-\n-[BART](model_doc/bart) keeps the original Transformer architecture, but it modifies the pretraining objective with *text infilling* corruption, where some text spans are replaced with a single `mask` token. The decoder predicts the uncorrupted tokens (future tokens are masked) and uses the encoder's hidden states to help it. [Pegasus](model_doc/pegasus) is similar to BART, but Pegasus masks entire sentences instead of text spans. In addition to masked language modeling, Pegasus is pretrained by gap sentence generation (GSG). The GSG objective masks whole sentences important to a document, replacing them with a `mask` token. The decoder must generate the output from the remaining sentences. [T5](model_doc/t5) is a more unique model that casts all NLP tasks into a text-to-text problem using specific prefixes. For example, the prefix `Summarize:` indicates a summarization task. T5 is pretrained by supervised (GLUE and SuperGLUE) training and self-supervised training (randomly sample and drop out 15% of tokens).\n-\n-## Audio\n-\n-<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1\" allowfullscreen></iframe>\n-\n-### Encoder[[audio-encoder]]\n-\n-[Wav2Vec2](model_doc/wav2vec2) uses a Transformer encoder to learn speech representations directly from raw audio waveforms. It is pretrained with a contrastive task to determine the true speech representation from a set of false ones. [HuBERT](model_doc/hubert) is similar to Wav2Vec2 but has a different training process. Target labels are created by a clustering step in which segments of similar audio are assigned to a cluster which becomes a hidden unit. The hidden unit is mapped to an embedding to make a prediction.\n-\n-### Encoder-decoder[[audio-encoder-decoder]]\n-\n-[Speech2Text](model_doc/speech_to_text) is a speech model designed for automatic speech recognition (ASR) and speech translation. The model accepts log mel-filter bank features extracted from the audio waveform and pretrained autoregressively to generate a transcript or translation. [Whisper](model_doc/whisper) is also an ASR model, but unlike many other speech models, it is pretrained on a massive amount of âœ¨ labeled âœ¨ audio transcription data for zero-shot performance. A large chunk of the dataset also contains non-English languages, meaning Whisper can also be used for low-resource languages. Structurally, Whisper is similar to Speech2Text. The audio signal is converted to a log-mel spectrogram encoded by the encoder. The decoder generates the transcript autoregressively from the encoder's hidden states and the previous tokens.\n-\n-## Multimodal\n-\n-<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1\" allowfullscreen></iframe>\n-\n-### Encoder[[mm-encoder]]\n-\n-[VisualBERT](model_doc/visual_bert) is a multimodal model for vision-language tasks released shortly after BERT. It combines BERT and a pretrained object detection system to extract image features into visual embeddings, passed alongside text embeddings to BERT. VisualBERT predicts the masked text based on the unmasked text and the visual embeddings, and it also has to predict whether the text is aligned with the image. When ViT was released, [ViLT](model_doc/vilt) adopted ViT in its architecture because it was easier to get the image embeddings this way. The image embeddings are jointly processed with the text embeddings. From there, ViLT is pretrained by image text matching, masked language modeling, and whole word masking.\n-\n-[CLIP](model_doc/clip) takes a different approach and makes a pair prediction of (`image`, `text`) . An image encoder (ViT) and a text encoder (Transformer) are jointly trained on a 400 million (`image`, `text`) pair dataset to maximize the similarity between the image and text embeddings of the (`image`, `text`) pairs. After pretraining, you can use natural language to instruct CLIP to predict the text given an image or vice versa. [OWL-ViT](model_doc/owlvit) builds on top of CLIP by using it as its backbone for zero-shot object detection. After pretraining, an object detection head is added to make a set prediction over the (`class`, `bounding box`) pairs.\n-\n-### Encoder-decoder[[mm-encoder-decoder]]\n-\n-Optical character recognition (OCR) is a long-standing text recognition task that typically involves several components to understand the image and generate the text. [TrOCR](model_doc/trocr) simplifies the process using an end-to-end Transformer. The encoder is a ViT-style model for image understanding and processes the image as fixed-size patches. The decoder accepts the encoder's hidden states and autoregressively generates text. [Donut](model_doc/donut) is a more general visual document understanding model that doesn't rely on OCR-based approaches. It uses a Swin Transformer as the encoder and multilingual BART as the decoder. Donut is pretrained to read text by predicting the next word based on the image and text annotations. The decoder generates a token sequence given a prompt. The prompt is represented by a special token for each downstream task. For example, document parsing has a special `parsing` token that is combined with the encoder hidden states to parse the document into a structured output format (JSON).\n-\n-## Reinforcement learning\n-\n-<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FiB3Y6RvWYki7ZuKO6tNgZq%2Freinforcement-learning%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1\" allowfullscreen></iframe>\n-\n-### Decoder[[rl-decoder]]\n-\n-The Decision and Trajectory Transformer casts the state, action, and reward as a sequence modeling problem. The [Decision Transformer](model_doc/decision_transformer) generates a series of actions that lead to a future desired return based on returns-to-go, past states, and actions. For the last *K* timesteps, each of the three modalities are converted into token embeddings and processed by a GPT-like model to predict a future action token. [Trajectory Transformer](model_doc/trajectory_transformer) also tokenizes the states, actions, and rewards and processes them with a GPT architecture. Unlike the Decision Transformer, which is focused on reward conditioning, the Trajectory Transformer generates future actions with beam search."
        },
        {
            "sha": "e06081a93d049d5620eb9102b99364b928e83892",
            "filename": "docs/source/en/task_summary.md",
            "status": "removed",
            "additions": 0,
            "deletions": 338,
            "changes": 338,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b73799500a69242d0afa73c1256bcaf7a3fff37/docs%2Fsource%2Fen%2Ftask_summary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b73799500a69242d0afa73c1256bcaf7a3fff37/docs%2Fsource%2Fen%2Ftask_summary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftask_summary.md?ref=8b73799500a69242d0afa73c1256bcaf7a3fff37",
            "patch": "@@ -1,338 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# What ðŸ¤— Transformers can do\n-\n-ðŸ¤— Transformers is a library of pretrained state-of-the-art models for natural language processing (NLP), computer vision, and audio and speech processing tasks. Not only does the library contain Transformer models, but it also has non-Transformer models like modern convolutional networks for computer vision tasks. If you look at some of the most popular consumer products today, like smartphones, apps, and televisions, odds are that some kind of deep learning technology is behind it. Want to remove a background object from a picture taken by your smartphone? This is an example of a panoptic segmentation task (don't worry if you don't know what this means yet, we'll describe it in the following sections!). \n-\n-This page provides an overview of the different speech and audio, computer vision, and NLP tasks that can be solved with the ðŸ¤— Transformers library in just three lines of code!\n-\n-## Audio\n-\n-Audio and speech processing tasks are a little different from the other modalities mainly because audio as an input is a continuous signal. Unlike text, a raw audio waveform can't be neatly split into discrete chunks the way a sentence can be divided into words. To get around this, the raw audio signal is typically sampled at regular intervals. If you take more samples within an interval, the sampling rate is higher, and the audio more closely resembles the original audio source.\n-\n-Previous approaches preprocessed the audio to extract useful features from it. It is now more common to start audio and speech processing tasks by directly feeding the raw audio waveform to a feature encoder to extract an audio representation. This simplifies the preprocessing step and allows the model to learn the most essential features.\n-\n-### Audio classification\n-\n-Audio classification is a task that labels audio data from a predefined set of classes. It is a broad category with many specific applications, some of which include:\n-\n-* acoustic scene classification: label audio with a scene label (\"office\", \"beach\", \"stadium\")\n-* acoustic event detection: label audio with a sound event label (\"car horn\", \"whale calling\", \"glass breaking\")\n-* tagging: label audio containing multiple sounds (birdsongs, speaker identification in a meeting)\n-* music classification: label music with a genre label (\"metal\", \"hip-hop\", \"country\")\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> classifier = pipeline(task=\"audio-classification\", model=\"superb/hubert-base-superb-er\")\n->>> preds = classifier(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n->>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n->>> preds\n-[{'score': 0.4532, 'label': 'hap'},\n- {'score': 0.3622, 'label': 'sad'},\n- {'score': 0.0943, 'label': 'neu'},\n- {'score': 0.0903, 'label': 'ang'}]\n-```\n-\n-### Automatic speech recognition\n-\n-Automatic speech recognition (ASR) transcribes speech into text. It is one of the most common audio tasks due partly to speech being such a natural form of human communication. Today, ASR systems are embedded in \"smart\" technology products like speakers, phones, and cars. We can ask our virtual assistants to play music, set reminders, and tell us the weather. \n-\n-But one of the key challenges Transformer architectures have helped with is in low-resource languages. By pretraining on large amounts of speech data, finetuning the model on only one hour of labeled speech data in a low-resource language can still produce high-quality results compared to previous ASR systems trained on 100x more labeled data.\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n->>> transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n-{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n-```\n-\n-## Computer vision\n-\n-One of the first and earliest successful computer vision tasks was recognizing images of zip code numbers using a [convolutional neural network (CNN)](glossary#convolution). An image is composed of pixels, and each pixel has a numerical value. This makes it easy to represent an image as a matrix of pixel values. Each particular combination of pixel values describes the colors of an image. \n-\n-Two general ways computer vision tasks can be solved are:\n-\n-1. Use convolutions to learn the hierarchical features of an image from low-level features to high-level abstract things.\n-2. Split an image into patches and use a Transformer to gradually learn how each image patch is related to each other to form an image. Unlike the bottom-up approach favored by a CNN, this is kind of like starting out with a blurry image and then gradually bringing it into focus.\n-\n-### Image classification\n-\n-Image classification labels an entire image from a predefined set of classes. Like most classification tasks, there are many practical use cases for image classification, some of which include:\n-\n-* healthcare: label medical images to detect disease or monitor patient health\n-* environment: label satellite images to monitor deforestation, inform wildland management or detect wildfires\n-* agriculture: label images of crops to monitor plant health or satellite images for land use monitoring \n-* ecology: label images of animal or plant species to monitor wildlife populations or track endangered species\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> classifier = pipeline(task=\"image-classification\")\n->>> preds = classifier(\n-...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n-... )\n->>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n->>> print(*preds, sep=\"\\n\")\n-{'score': 0.4335, 'label': 'lynx, catamount'}\n-{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}\n-{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}\n-{'score': 0.0239, 'label': 'Egyptian cat'}\n-{'score': 0.0229, 'label': 'tiger cat'}\n-```\n-\n-### Object detection\n-\n-Unlike image classification, object detection identifies multiple objects within an image and the objects' positions in an image (defined by the bounding box). Some example applications of object detection include:\n-\n-* self-driving vehicles: detect everyday traffic objects such as other vehicles, pedestrians, and traffic lights\n-* remote sensing: disaster monitoring, urban planning, and weather forecasting\n-* defect detection: detect cracks or structural damage in buildings, and manufacturing defects\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> detector = pipeline(task=\"object-detection\")\n->>> preds = detector(\n-...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n-... )\n->>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n->>> preds\n-[{'score': 0.9865,\n-  'label': 'cat',\n-  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]\n-```\n-\n-### Image segmentation\n-\n-Image segmentation is a pixel-level task that assigns every pixel in an image to a class. It differs from object detection, which uses bounding boxes to label and predict objects in an image because segmentation is more granular. Segmentation can detect objects at a pixel-level. There are several types of image segmentation:\n-\n-* instance segmentation: in addition to labeling the class of an object, it also labels each distinct instance of an object (\"dog-1\", \"dog-2\")\n-* panoptic segmentation: a combination of semantic and instance segmentation; it labels each pixel with a semantic class **and** each distinct instance of an object\n-\n-Segmentation tasks are helpful in self-driving vehicles to create a pixel-level map of the world around them so they can navigate safely around pedestrians and other vehicles. It is also useful for medical imaging, where the task's finer granularity can help identify abnormal cells or organ features. Image segmentation can also be used in ecommerce to virtually try on clothes or create augmented reality experiences by overlaying objects in the real world through your camera.\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> segmenter = pipeline(task=\"image-segmentation\")\n->>> preds = segmenter(\n-...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n-... )\n->>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n->>> print(*preds, sep=\"\\n\")\n-{'score': 0.9879, 'label': 'LABEL_184'}\n-{'score': 0.9973, 'label': 'snow'}\n-{'score': 0.9972, 'label': 'cat'}\n-```\n-\n-### Depth estimation\n-\n-Depth estimation predicts the distance of each pixel in an image from the camera. This computer vision task is especially important for scene understanding and reconstruction. For example, in self-driving cars, vehicles need to understand how far objects like pedestrians, traffic signs, and other vehicles are to avoid obstacles and collisions. Depth information is also helpful for constructing 3D representations from 2D images and can be used to create high-quality 3D representations of biological structures or buildings.\n-\n-There are two approaches to depth estimation:\n-\n-* stereo: depths are estimated by comparing two images of the same image from slightly different angles\n-* monocular: depths are estimated from a single image\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> depth_estimator = pipeline(task=\"depth-estimation\")\n->>> preds = depth_estimator(\n-...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n-... )\n-```\n-\n-## Natural language processing\n-\n-NLP tasks are among the most common types of tasks because text is such a natural way for us to communicate. To get text into a format recognized by a model, it needs to be tokenized. This means dividing a sequence of text into separate words or subwords (tokens) and then converting these tokens into numbers. As a result, you can represent a sequence of text as a sequence of numbers, and once you have a sequence of numbers, it can be input into a model to solve all sorts of NLP tasks!\n-\n-### Text classification\n-\n-Like classification tasks in any modality, text classification labels a sequence of text (it can be sentence-level, a paragraph, or a document) from a predefined set of classes. There are many practical applications for text classification, some of which include:\n-\n-* sentiment analysis: label text according to some polarity like `positive` or `negative` which can inform and support decision-making in fields like politics, finance, and marketing\n-* content classification: label text according to some topic to help organize and filter information in news and social media feeds (`weather`, `sports`, `finance`, etc.)\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> classifier = pipeline(task=\"sentiment-analysis\")\n->>> preds = classifier(\"Hugging Face is the best thing since sliced bread!\")\n->>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n->>> preds\n-[{'score': 0.9991, 'label': 'POSITIVE'}]\n-```\n-\n-### Token classification\n-\n-In any NLP task, text is preprocessed by separating the sequence of text into individual words or subwords. These are known as [tokens](glossary#token). Token classification assigns each token a label from a predefined set of classes. \n-\n-Two common types of token classification are:\n-\n-* named entity recognition (NER): label a token according to an entity category like organization, person, location or date. NER is especially popular in biomedical settings, where it can label genes, proteins, and drug names.\n-* part-of-speech tagging (POS): label a token according to its part-of-speech like noun, verb, or adjective. POS is useful for helping translation systems understand how two identical words are grammatically different (bank as a noun versus bank as a verb).\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> classifier = pipeline(task=\"ner\")\n->>> preds = classifier(\"Hugging Face is a French company based in New York City.\")\n->>> preds = [\n-...     {\n-...         \"entity\": pred[\"entity\"],\n-...         \"score\": round(pred[\"score\"], 4),\n-...         \"index\": pred[\"index\"],\n-...         \"word\": pred[\"word\"],\n-...         \"start\": pred[\"start\"],\n-...         \"end\": pred[\"end\"],\n-...     }\n-...     for pred in preds\n-... ]\n->>> print(*preds, sep=\"\\n\")\n-{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}\n-{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}\n-{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n-{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}\n-{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}\n-{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}\n-{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}\n-```\n-\n-### Question answering\n-\n-Question answering is another token-level task that returns an answer to a question, sometimes with context (open-domain) and other times without context (closed-domain). This task happens whenever we ask a virtual assistant something like whether a restaurant is open. It can also provide customer or technical support and help search engines retrieve the relevant information you're asking for. \n-\n-There are two common types of question answering:\n-\n-* extractive: given a question and some context, the answer is a span of text from the context the model must extract\n-* abstractive: given a question and some context, the answer is generated from the context; this approach is handled by the [`Text2TextGenerationPipeline`] instead of the [`QuestionAnsweringPipeline`] shown below\n-\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> question_answerer = pipeline(task=\"question-answering\")\n->>> preds = question_answerer(\n-...     question=\"What is the name of the repository?\",\n-...     context=\"The name of the repository is huggingface/transformers\",\n-... )\n->>> print(\n-...     f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n-... )\n-score: 0.9327, start: 30, end: 54, answer: huggingface/transformers\n-```\n-\n-### Summarization\n-\n-Summarization creates a shorter version of a text from a longer one while trying to preserve most of the meaning of the original document. Summarization is a sequence-to-sequence task; it outputs a shorter text sequence than the input. There are a lot of long-form documents that can be summarized to help readers quickly understand the main points. Legislative bills, legal and financial documents, patents, and scientific papers are a few examples of documents that could be summarized to save readers time and serve as a reading aid.\n-\n-Like question answering, there are two types of summarization:\n-\n-* extractive: identify and extract the most important sentences from the original text\n-* abstractive: generate the target summary (which may include new words not in the input document) from the original text; the [`SummarizationPipeline`] uses the abstractive approach\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> summarizer = pipeline(task=\"summarization\")\n->>> summarizer(\n-...     \"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\"\n-... )\n-[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]\n-```\n-\n-### Translation\n-\n-Translation converts a sequence of text in one language to another. It is important in helping people from different backgrounds communicate with each other, help translate content to reach wider audiences, and even be a learning tool to help people learn a new language. Along with summarization, translation is a sequence-to-sequence task, meaning the model receives an input sequence and returns a target output sequence. \n-\n-In the early days, translation models were mostly monolingual, but recently, there has been increasing interest in multilingual models that can translate between many pairs of languages.\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> text = \"translate English to French: Hugging Face is a community-based open-source platform for machine learning.\"\n->>> translator = pipeline(task=\"translation\", model=\"google-t5/t5-small\")\n->>> translator(text)\n-[{'translation_text': \"Hugging Face est une tribune communautaire de l'apprentissage des machines.\"}]\n-```\n-\n-### Language modeling\n-\n-Language modeling is a task that predicts a word in a sequence of text. It has become a very popular NLP task because a pretrained language model can be finetuned for many other downstream tasks. Lately, there has been a lot of interest in large language models (LLMs) which demonstrate zero- or few-shot learning. This means the model can solve tasks it wasn't explicitly trained to do! Language models can be used to generate fluent and convincing text, though you need to be careful since the text may not always be accurate.\n-\n-There are two types of language modeling:\n-\n-* causal: the model's objective is to predict the next token in a sequence, and future tokens are masked\n-\n-    ```py\n-    >>> from transformers import pipeline\n-\n-    >>> prompt = \"Hugging Face is a community-based open-source platform for machine learning.\"\n-    >>> generator = pipeline(task=\"text-generation\")\n-    >>> generator(prompt)  # doctest: +SKIP\n-    ```\n-\n-* masked: the model's objective is to predict a masked token in a sequence with full access to the tokens in the sequence\n-    \n-    ```py\n-    >>> text = \"Hugging Face is a community-based open-source <mask> for machine learning.\"\n-    >>> fill_mask = pipeline(task=\"fill-mask\")\n-    >>> preds = fill_mask(text, top_k=1)\n-    >>> preds = [\n-    ...     {\n-    ...         \"score\": round(pred[\"score\"], 4),\n-    ...         \"token\": pred[\"token\"],\n-    ...         \"token_str\": pred[\"token_str\"],\n-    ...         \"sequence\": pred[\"sequence\"],\n-    ...     }\n-    ...     for pred in preds\n-    ... ]\n-    >>> preds\n-    [{'score': 0.224, 'token': 3944, 'token_str': ' tool', 'sequence': 'Hugging Face is a community-based open-source tool for machine learning.'}]\n-    ```\n-\n-## Multimodal\n-\n-Multimodal tasks require a model to process multiple data modalities (text, image, audio, video) to solve a particular problem. Image captioning is an example of a multimodal task where the model takes an image as input and outputs a sequence of text describing the image or some properties of the image. \n-\n-Although multimodal models work with different data types or modalities, internally, the preprocessing steps help the model convert all the data types into embeddings (vectors or list of numbers that holds meaningful information about the data). For a task like image captioning, the model learns relationships between image embeddings and text embeddings.\n-\n-### Document question answering\n-\n-Document question answering is a task that answers natural language questions from a document. Unlike a token-level question answering task which takes text as input, document question answering takes an image of a document as input along with a question about the document and returns an answer. Document question answering can be used to parse structured documents and extract key information from it. In the example below, the total amount and change due can be extracted from a receipt.\n-\n-```py\n->>> from transformers import pipeline\n->>> from PIL import Image\n->>> import requests\n-\n->>> url = \"https://huggingface.co/datasets/hf-internal-testing/example-documents/resolve/main/jpeg_images/2.jpg\"\n->>> image = Image.open(requests.get(url, stream=True).raw)\n-\n->>> doc_question_answerer = pipeline(\"document-question-answering\", model=\"magorshunov/layoutlm-invoices\")\n->>> preds = doc_question_answerer(\n-...     question=\"What is the total amount?\",\n-...     image=image,\n-... )\n->>> preds\n-[{'score': 0.8531, 'answer': '17,000', 'start': 4, 'end': 4}]\n-```\n-\n-Hopefully, this page has given you some more background information about all the types of tasks in each modality and the practical importance of each one. In the next [section](tasks_explained), you'll learn **how** ðŸ¤— Transformers work to solve these tasks.\n\\ No newline at end of file"
        },
        {
            "sha": "afdfb869899def17f0d115df5d9adf49bd34bbe5",
            "filename": "docs/source/en/tasks_explained.md",
            "status": "removed",
            "additions": 0,
            "deletions": 295,
            "changes": 295,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b73799500a69242d0afa73c1256bcaf7a3fff37/docs%2Fsource%2Fen%2Ftasks_explained.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b73799500a69242d0afa73c1256bcaf7a3fff37/docs%2Fsource%2Fen%2Ftasks_explained.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks_explained.md?ref=8b73799500a69242d0afa73c1256bcaf7a3fff37",
            "patch": "@@ -1,295 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# How ðŸ¤— Transformers solve tasks\n-\n-In [What ðŸ¤— Transformers can do](task_summary), you learned about natural language processing (NLP), speech and audio, computer vision tasks, and some important applications of them. This page will look closely at how models solve these tasks and explain what's happening under the hood. There are many ways to solve a given task, some models may implement certain techniques or even approach the task from a new angle, but for Transformer models, the general idea is the same. Owing to its flexible architecture, most models are a variant of an encoder, a decoder, or an encoder-decoder structure. In addition to Transformer models, our library also has several convolutional neural networks (CNNs), which are still used today for computer vision tasks. We'll also explain how a modern CNN works.\n-\n-To explain how tasks are solved, we'll walk through what goes on inside the model to output useful predictions.\n-\n-- [Wav2Vec2](model_doc/wav2vec2) for audio classification and automatic speech recognition (ASR)\n-- [Vision Transformer (ViT)](model_doc/vit) and [ConvNeXT](model_doc/convnext) for image classification\n-- [DETR](model_doc/detr) for object detection\n-- [Mask2Former](model_doc/mask2former) for image segmentation\n-- [GLPN](model_doc/glpn) for depth estimation\n-- [BERT](model_doc/bert) for NLP tasks like text classification, token classification and question answering that use an encoder\n-- [GPT2](model_doc/gpt2) for NLP tasks like text generation that use a decoder\n-- [BART](model_doc/bart) for NLP tasks like summarization and translation that use an encoder-decoder\n-\n-<Tip>\n-\n-Before you go further, it is good to have some basic knowledge of the original Transformer architecture. Knowing how encoders, decoders, and attention work will aid you in understanding how different Transformer models work. If you're just getting started or need a refresher, check out our [course](https://huggingface.co/course/chapter1/4?fw=pt) for more information! \n-\n-</Tip>\n-\n-## Speech and audio\n-\n-[Wav2Vec2](model_doc/wav2vec2) is a self-supervised model pretrained on unlabeled speech data and finetuned on labeled data for audio classification and automatic speech recognition. \n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png\"/>\n-</div>\n-\n-This model has four main components:\n-\n-1. A *feature encoder* takes the raw audio waveform, normalizes it to zero mean and unit variance, and converts it into a sequence of feature vectors that are each 20ms long.\n-\n-2. Waveforms are continuous by nature, so they can't be divided into separate units like a sequence of text can be split into words. That's why the feature vectors are passed to a *quantization module*, which aims to learn discrete speech units. The speech unit is chosen from a collection of codewords, known as a *codebook* (you can think of this as the vocabulary). From the codebook, the vector or speech unit, that best represents the continuous audio input is chosen and forwarded through the model.\n-\n-3. About half of the feature vectors are randomly masked, and the masked feature vector is fed to a *context network*, which is a Transformer encoder that also adds relative positional embeddings.\n-\n-4. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).\n-\n-Now that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech recognition!\n-\n-### Audio classification\n-\n-To use the pretrained model for audio classification, add a sequence classification head on top of the base Wav2Vec2 model. The classification head is a linear layer that accepts the encoder's hidden states. The hidden states represent the learned features from each audio frame which can have varying lengths. To create one vector of fixed-length, the hidden states are pooled first and then transformed into logits over the class labels. The cross-entropy loss is calculated between the logits and target to find the most likely class.\n-\n-Ready to try your hand at audio classification? Check out our complete [audio classification guide](tasks/audio_classification) to learn how to finetune Wav2Vec2 and use it for inference!\n-\n-### Automatic speech recognition\n-\n-To use the pretrained model for automatic speech recognition, add a language modeling head on top of the base Wav2Vec2 model for [connectionist temporal classification (CTC)](glossary#connectionist-temporal-classification-ctc). The language modeling head is a linear layer that accepts the encoder's hidden states and transforms them into logits. Each logit represents a token class (the number of tokens comes from the task vocabulary). The CTC loss is calculated between the logits and targets to find the most likely sequence of tokens, which are then decoded into a transcription.\n-\n-Ready to try your hand at automatic speech recognition? Check out our complete [automatic speech recognition guide](tasks/asr) to learn how to finetune Wav2Vec2 and use it for inference!\n-\n-## Computer vision\n-\n-There are two ways to approach computer vision tasks:\n-\n-1. Split an image into a sequence of patches and process them in parallel with a Transformer.\n-2. Use a modern CNN, like [ConvNeXT](model_doc/convnext), which relies on convolutional layers but adopts modern network designs.\n-\n-<Tip>\n-\n-A third approach mixes Transformers with convolutions (for example, [Convolutional Vision Transformer](model_doc/cvt) or [LeViT](model_doc/levit)). We won't discuss those because they just combine the two approaches we examine here.\n-\n-</Tip>\n-\n-ViT and ConvNeXT are commonly used for image classification, but for other vision tasks like object detection, segmentation, and depth estimation, we'll look at DETR, Mask2Former and GLPN, respectively; these models are better suited for those tasks.\n-\n-### Image classification\n-\n-ViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions.\n-\n-#### Transformer\n-\n-[ViT](model_doc/vit) replaces convolutions entirely with a pure Transformer architecture. If you're familiar with the original Transformer, then you're already most of the way toward understanding ViT.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg\"/>\n-</div>\n-\n-The main change ViT introduced was in how images are fed to a Transformer:\n-\n-1. An image is split into square non-overlapping patches, each of which gets turned into a vector or *patch embedding*. The patch embeddings are generated from a convolutional 2D layer which creates the proper input dimensions (which for a base Transformer is 768 values for each patch embedding). If you had a 224x224 pixel image, you could split it into 196 16x16 image patches. Just like how text is tokenized into words, an image is \"tokenized\" into a sequence of patches.\n-\n-2. A *learnable embedding* - a special `[CLS]` token - is added to the beginning of the patch embeddings just like BERT. The final hidden state of the `[CLS]` token is used as the input to the attached classification head; other outputs are ignored. This token helps the model learn how to encode a representation of the image.\n-\n-3. The last thing to add to the patch and learnable embeddings are the *position embeddings* because the model doesn't know how the image patches are ordered. The position embeddings are also learnable and have the same size as the patch embeddings. Finally, all of the embeddings are passed to the Transformer encoder.\n-\n-4. The output, specifically only the output with the `[CLS]` token, is passed to a multilayer perceptron head (MLP). ViT's pretraining objective is simply classification. Like other classification heads, the MLP head converts the output into logits over the class labels and calculates the cross-entropy loss to find the most likely class.\n-\n-Ready to try your hand at image classification? Check out our complete [image classification guide](tasks/image_classification) to learn how to finetune ViT and use it for inference!\n-\n-#### CNN\n-\n-<Tip>\n-\n-This section briefly explains convolutions, but it'd be helpful to have a prior understanding of how they change an image's shape and size. If you're unfamiliar with convolutions, check out the [Convolution Neural Networks chapter](https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb) from the fastai book!\n-\n-</Tip>\n-\n-[ConvNeXT](model_doc/convnext) is a CNN architecture that adopts new and modern network designs to improve performance. However, convolutions are still at the core of the model. From a high-level perspective, a [convolution](glossary#convolution) is an operation where a smaller matrix (*kernel*) is multiplied by a small window of the image pixels. It computes some features from it, such as a particular texture or curvature of a line. Then it slides over to the next window of pixels; the distance the convolution travels is known as the *stride*. \n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif\"/>\n-</div>\n-\n-<small>A basic convolution without padding or stride, taken from <a href=\"https://huggingface.co/papers/1603.07285\">A guide to convolution arithmetic for deep learning.</a></small>\n-\n-You can feed this output to another convolutional layer, and with each successive layer, the network learns more complex and abstract things like hotdogs or rockets. Between convolutional layers, it is common to add a pooling layer to reduce dimensionality and make the model more robust to variations of a feature's position.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.png\"/>\n-</div>\n-\n-ConvNeXT modernizes a CNN in five ways:\n-\n-1. Change the number of blocks in each stage and \"patchify\" an image with a larger stride and corresponding kernel size. The non-overlapping sliding window makes this patchifying strategy similar to how ViT splits an image into patches.\n-\n-2. A *bottleneck* layer shrinks the number of channels and then restores it because it is faster to do a 1x1 convolution, and you can increase the depth. An inverted bottleneck does the opposite by expanding the number of channels and shrinking them, which is more memory efficient.\n-\n-3. Replace the typical 3x3 convolutional layer in the bottleneck layer with *depthwise convolution*, which applies a convolution to each input channel separately and then stacks them back together at the end. This widens the network width for improved performance.\n-\n-4. ViT has a global receptive field which means it can see more of an image at once thanks to its attention mechanism. ConvNeXT attempts to replicate this effect by increasing the kernel size to 7x7.\n-\n-5. ConvNeXT also makes several layer design changes that imitate Transformer models. There are fewer activation and normalization layers,  the activation function is switched to GELU instead of ReLU, and it uses LayerNorm instead of BatchNorm.\n-\n-The output from the convolution blocks is passed to a classification head which converts the outputs into logits and calculates the cross-entropy loss to find the most likely label.\n-\n-### Object detection\n-\n-[DETR](model_doc/detr), *DEtection TRansformer*, is an end-to-end object detection model that combines a CNN with a Transformer encoder-decoder.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/detr_architecture.png\"/>\n-</div>\n-\n-1. A pretrained CNN *backbone* takes an image, represented by its pixel values, and creates a low-resolution feature map of it. A 1x1 convolution is applied to the feature map to reduce dimensionality and it creates a new feature map with a high-level image representation. Since the Transformer is a sequential model, the feature map is flattened into a sequence of feature vectors that are combined with positional embeddings.\n-\n-2. The feature vectors are passed to the encoder, which learns the image representations using its attention layers. Next, the encoder hidden states are combined with *object queries* in the decoder. Object queries are learned embeddings that focus on the different regions of an image, and they're updated as they progress through each attention layer. The decoder hidden states are passed to a feedforward network that predicts the bounding box coordinates and class label for each object query, or `no object` if there isn't one.\n-\n-    DETR decodes each object query in parallel to output *N* final predictions, where *N* is the number of queries. Unlike a typical autoregressive model that predicts one element at a time, object detection is a set prediction task (`bounding box`, `class label`) that makes *N* predictions in a single pass.\n-\n-3. DETR uses a *bipartite matching loss* during training to compare a fixed number of predictions with a fixed set of ground truth labels. If there are fewer ground truth labels in the set of *N* labels, then they're padded with a `no object` class. This loss function encourages DETR to find a one-to-one assignment between the predictions and ground truth labels. If either the bounding boxes or class labels aren't correct, a loss is incurred. Likewise, if DETR predicts an object that doesn't exist, it is penalized. This encourages DETR to find other objects in an image instead of focusing on one really prominent object.\n-\n-An object detection head is added on top of DETR to find the class label and the coordinates of the bounding box. There are two components to the object detection head: a linear layer to transform the decoder hidden states into logits over the class labels, and a MLP to predict the bounding box.\n-\n-Ready to try your hand at object detection? Check out our complete [object detection guide](tasks/object_detection) to learn how to finetune DETR and use it for inference!\n-\n-### Image segmentation\n-\n-[Mask2Former](model_doc/mask2former) is a universal architecture for solving all types of image segmentation tasks. Traditional segmentation models are typically tailored towards a particular subtask of image segmentation, like instance, semantic or panoptic segmentation. Mask2Former frames each of those tasks as a *mask classification* problem. Mask classification groups pixels into *N* segments, and predicts *N* masks and their corresponding class label for a given image. We'll explain how Mask2Former works in this section, and then you can try finetuning SegFormer at the end.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/mask2former_architecture.png\"/>\n-</div>\n-\n-There are three main components to Mask2Former:\n-\n-1. A [Swin](model_doc/swin) backbone accepts an image and creates a low-resolution image feature map from 3 consecutive 3x3 convolutions.\n-\n-2. The feature map is passed to a *pixel decoder* which gradually upsamples the low-resolution features into high-resolution per-pixel embeddings. The pixel decoder actually generates multi-scale features (contains both low- and high-resolution features) with resolutions 1/32, 1/16, and 1/8th of the original image.\n-\n-3. Each of these feature maps of differing scales is fed successively to one Transformer decoder layer at a time in order to capture small objects from the high-resolution features. The key to Mask2Former is the *masked attention* mechanism in the decoder. Unlike cross-attention which can attend to the entire image, masked attention only focuses on a certain area of the image. This is faster and leads to better performance because the local features of an image are enough for the model to learn from.\n-\n-4. Like [DETR](tasks_explained#object-detection), Mask2Former also uses learned object queries and combines them with the image features from the pixel decoder to make a set prediction (`class label`, `mask prediction`). The decoder hidden states are passed into a linear layer and transformed into logits over the class labels. The cross-entropy loss is calculated between the logits and class label to find the most likely one.\n-\n-    The mask predictions are generated by combining the pixel-embeddings with the final decoder hidden states. The sigmoid cross-entropy and dice loss is calculated between the logits and the ground truth mask to find the most likely mask.\n-\n-Ready to try your hand at image segmentation? Check out our complete [image segmentation guide](tasks/semantic_segmentation) to learn how to finetune SegFormer and use it for inference!\n-\n-### Depth estimation\n-\n-[GLPN](model_doc/glpn), *Global-Local Path Network*, is a Transformer for depth estimation that combines a [SegFormer](model_doc/segformer) encoder with a lightweight decoder.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/glpn_architecture.jpg\"/>\n-</div>\n-\n-1. Like ViT, an image is split into a sequence of patches, except these image patches are smaller. This is better for dense prediction tasks like segmentation or depth estimation. The image patches are transformed into patch embeddings (see the [image classification](#image-classification) section for more details about how patch embeddings are created), which are fed to the encoder.\n-\n-2. The encoder accepts the patch embeddings, and passes them through several encoder blocks. Each block consists of attention and Mix-FFN layers. The purpose of the latter is to provide positional information. At the end of each encoder block is a *patch merging* layer for creating hierarchical representations. The features of each group of neighboring patches are concatenated, and a linear layer is applied to the concatenated features to reduce the number of patches to a resolution of 1/4. This becomes the input to the next encoder block, where this whole process is repeated until you have image features with resolutions of 1/8, 1/16, and 1/32.\n-\n-3. A lightweight decoder takes the last feature map (1/32 scale) from the encoder and upsamples it to 1/16 scale. From here, the feature is passed into a *Selective Feature Fusion (SFF)* module, which selects and combines local and global features from an attention map for each feature and then upsamples it to 1/8th. This process is repeated until the decoded features are the same size as the original image. The output is passed through two convolution layers and then a sigmoid activation is applied to predict the depth of each pixel.\n-\n-## Natural language processing\n-\n-The Transformer was initially designed for machine translation, and since then, it has practically become the default architecture for solving all NLP tasks. Some tasks lend themselves to the Transformer's encoder structure, while others are better suited for the decoder. Still, other tasks make use of both the Transformer's encoder-decoder structure.\n-\n-### Text classification\n-\n-[BERT](model_doc/bert) is an encoder-only model and is the first model to effectively implement deep bidirectionality to learn richer representations of the text by attending to words on both sides.\n-\n-1. BERT uses [WordPiece](tokenizer_summary#wordpiece) tokenization to generate a token embedding of the text. To tell the difference between a single sentence and a pair of sentences, a special `[SEP]` token is added to differentiate them. A special `[CLS]` token is added to the beginning of every sequence of text. The final output with the `[CLS]` token is used as the input to the classification head for classification tasks. BERT also adds a segment embedding to denote whether a token belongs to the first or second sentence in a pair of sentences.\n-\n-2. BERT is pretrained with two objectives: masked language modeling and next-sentence prediction. In masked language modeling, some percentage of the input tokens are randomly masked, and the model needs to predict these. This solves the issue of bidirectionality, where the model could cheat and see all the words and \"predict\" the next word. The final hidden states of the predicted mask tokens are passed to a feedforward network with a softmax over the vocabulary to predict the masked word.\n-\n-    The second pretraining object is next-sentence prediction. The model must predict whether sentence B follows sentence A. Half of the time sentence B is the next sentence, and the other half of the time, sentence B is a random sentence. The prediction, whether it is the next sentence or not, is passed to a feedforward network with a softmax over the two classes (`IsNext` and `NotNext`).\n-\n-3. The input embeddings are passed through multiple encoder layers to output some final hidden states.\n-\n-To use the pretrained model for text classification, add a sequence classification head on top of the base BERT model. The sequence classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and target to find the most likely label.\n-\n-Ready to try your hand at text classification? Check out our complete [text classification guide](tasks/sequence_classification) to learn how to finetune DistilBERT and use it for inference!\n-\n-### Token classification\n-\n-To use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label.\n-\n-Ready to try your hand at token classification? Check out our complete [token classification guide](tasks/token_classification) to learn how to finetune DistilBERT and use it for inference!\n-\n-### Question answering\n-\n-To use BERT for question answering, add a span classification head on top of the base BERT model. This linear layer accepts the final hidden states and performs a linear transformation to compute the `span` start and end logits corresponding to the answer. The cross-entropy loss is calculated between the logits and the label position to find the most likely span of text corresponding to the answer.\n-\n-Ready to try your hand at question answering? Check out our complete [question answering guide](tasks/question_answering) to learn how to finetune DistilBERT and use it for inference!\n-\n-<Tip>\n-\n-ðŸ’¡ Notice how easy it is to use BERT for different tasks once it's been pretrained. You only need to add a specific head to the pretrained model to manipulate the hidden states into your desired output!\n-\n-</Tip>\n-\n-### Text generation\n-\n-[GPT-2](model_doc/gpt2) is a decoder-only model pretrained on a large amount of text. It can generate convincing (though not always true!) text given a prompt and complete other NLP tasks like question answering despite not being explicitly trained to.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png\"/>\n-</div>\n-\n-1. GPT-2 uses [byte pair encoding (BPE)](tokenizer_summary#bytepair-encoding-bpe) to tokenize words and generate a token embedding. Positional encodings are added to the token embeddings to indicate the position of each token in the sequence. The input embeddings are passed through multiple decoder blocks to output some final hidden state. Within each decoder block, GPT-2 uses a *masked self-attention* layer which means GPT-2 can't attend to future tokens. It is only allowed to attend to tokens on the left. This is different from BERT's [`mask`] token because, in masked self-attention, an attention mask is used to set the score to `0` for future tokens.\n-\n-2. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The label is the next token in the sequence, which are created by shifting the logits to the right by one. The cross-entropy loss is calculated between the shifted logits and the labels to output the next most likely token.\n-\n-GPT-2's pretraining objective is based entirely on [causal language modeling](glossary#causal-language-modeling), predicting the next word in a sequence. This makes GPT-2 especially good at tasks that involve generating text.\n-\n-Ready to try your hand at text generation? Check out our complete [causal language modeling guide](tasks/language_modeling#causal-language-modeling) to learn how to finetune DistilGPT-2 and use it for inference!\n-\n-<Tip>\n-\n-For more information about text generation, check out the [text generation strategies](generation_strategies) guide!\n-\n-</Tip>\n-\n-### Summarization\n-\n-Encoder-decoder models like [BART](model_doc/bart) and [T5](model_doc/t5) are designed for the sequence-to-sequence pattern of a summarization task. We'll explain how BART works in this section, and then you can try finetuning T5 at the end.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png\"/>\n-</div>\n-\n-1. BART's encoder architecture is very similar to BERT and accepts a token and positional embedding of the text. BART is pretrained by corrupting the input and then reconstructing it with the decoder. Unlike other encoders with specific corruption strategies, BART can apply any type of corruption. The *text infilling* corruption strategy works the best though. In text infilling, a number of text spans are replaced with a **single** [`mask`] token. This is important because the model has to predict the masked tokens, and it teaches the model to predict the number of missing tokens. The input embeddings and masked spans are passed through the encoder to output some final hidden states, but unlike BERT, BART doesn't add a final feedforward network at the end to predict a word.\n-\n-2. The encoder's output is passed to the decoder, which must predict the masked tokens and any uncorrupted tokens from the encoder's output. This gives additional context to help the decoder restore the original text. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The cross-entropy loss is calculated between the logits and the label, which is just the token shifted to the right.\n-\n-Ready to try your hand at summarization? Check out our complete [summarization guide](tasks/summarization) to learn how to finetune T5 and use it for inference!\n-\n-<Tip>\n-\n-For more information about text generation, check out the [text generation strategies](generation_strategies) guide!\n-\n-</Tip>\n-\n-### Translation\n-\n-Translation is another example of a sequence-to-sequence task, which means you can use an encoder-decoder model like [BART](model_doc/bart) or [T5](model_doc/t5) to do it. We'll explain how BART works in this section, and then you can try finetuning T5 at the end.\n-\n-BART adapts to translation by adding a separate randomly initialized encoder to map a source language to an input that can be decoded into the target language. This new encoder's embeddings are passed to the pretrained encoder instead of the original word embeddings. The source encoder is trained by updating the source encoder, positional embeddings, and input embeddings with the cross-entropy loss from the model output. The model parameters are frozen in this first step, and all the model parameters are trained together in the second step.\n-\n-BART has since been followed up by a multilingual version, mBART, intended for translation and pretrained on many different languages.\n-\n-Ready to try your hand at translation? Check out our complete [translation guide](tasks/translation) to learn how to finetune T5 and use it for inference!\n-\n-<Tip>\n-\n-For more information about text generation, check out the [text generation strategies](generation_strategies) guide!\n-\n-</Tip>"
        },
        {
            "sha": "f65f2b7b14629c0c6be0a120d2cae53702817f1a",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/fdb5da59ddf0e4f79623adc2941dd34bea59225b/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/fdb5da59ddf0e4f79623adc2941dd34bea59225b/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=fdb5da59ddf0e4f79623adc2941dd34bea59225b",
            "patch": "@@ -2,7 +2,6 @@ docs/source/en/_config.py\n docs/source/en/accelerate.md\n docs/source/en/add_new_model.md\n docs/source/en/add_new_pipeline.md\n-docs/source/en/attention.md\n docs/source/en/community.md\n docs/source/en/contributing.md\n docs/source/en/custom_models.md\n@@ -267,7 +266,6 @@ docs/source/en/model_doc/yolos.md\n docs/source/en/model_doc/yoso.md\n docs/source/en/model_memory_anatomy.md\n docs/source/en/model_sharing.md\n-docs/source/en/model_summary.md\n docs/source/en/notebooks.md\n docs/source/en/pad_truncation.md\n docs/source/en/peft.md\n@@ -309,7 +307,6 @@ docs/source/en/tasks/video_classification.md\n docs/source/en/tasks/visual_question_answering.md\n docs/source/en/tasks/zero_shot_image_classification.md\n docs/source/en/tasks/zero_shot_object_detection.md\n-docs/source/en/tasks_explained.md\n docs/source/en/tf_xla.md\n docs/source/en/tflite.md\n docs/source/en/tokenizer_summary.md"
        },
        {
            "sha": "03ef646f04bc7262bec01891ca7763c61992dc2e",
            "filename": "utils/slow_documentation_tests.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/fdb5da59ddf0e4f79623adc2941dd34bea59225b/utils%2Fslow_documentation_tests.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/fdb5da59ddf0e4f79623adc2941dd34bea59225b/utils%2Fslow_documentation_tests.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fslow_documentation_tests.txt?ref=fdb5da59ddf0e4f79623adc2941dd34bea59225b",
            "patch": "@@ -4,7 +4,6 @@ docs/source/en/model_doc/ctrl.md\n docs/source/en/model_doc/kosmos-2.md\n docs/source/en/model_doc/seamless_m4t.md\n docs/source/en/model_doc/seamless_m4t_v2.md\n-docs/source/en/task_summary.md\n docs/source/en/tasks/prompting.md\n docs/source/ja/model_doc/code_llama.md\n src/transformers/models/blip_2/modeling_blip_2.py"
        }
    ],
    "stats": {
        "total": 816,
        "additions": 3,
        "deletions": 813
    }
}