{
    "author": "Cyrilvallez",
    "message": "Remove bad test skips (#41109)\n\n* remove bad skips\n\n* remove more\n\n* fix inits",
    "sha": "99b0995138c17ef953959c70f35cb2bdc41111a2",
    "files": [
        {
            "sha": "8e4eabfdb86cf179c9bb9640c176ef5c2b6d2a14",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/99b0995138c17ef953959c70f35cb2bdc41111a2/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99b0995138c17ef953959c70f35cb2bdc41111a2/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=99b0995138c17ef953959c70f35cb2bdc41111a2",
            "patch": "@@ -459,6 +459,12 @@ def _init_weights(self, module):\n                     nn.init.constant_(layer.layers[-1].weight, 0)\n                     nn.init.constant_(layer.layers[-1].bias, 0)\n \n+            if hasattr(module, \"reg_scale\"):\n+                module.reg_scale.fill_(self.config.reg_scale)\n+\n+            if hasattr(module, \"up\"):\n+                module.up.fill_(self.config.up)\n+\n         if isinstance(module, DFineMultiscaleDeformableAttention):\n             nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n             default_dtype = torch.get_default_dtype()\n@@ -496,6 +502,10 @@ def _init_weights(self, module):\n             init.constant_(module.reg_conf.layers[-1].bias, 0)\n             init.constant_(module.reg_conf.layers[-1].weight, 0)\n \n+        if isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n         if hasattr(module, \"weight_embedding\") and self.config.learn_initial_query:\n             nn.init.xavier_uniform_(module.weight_embedding.weight)\n         if hasattr(module, \"denoising_class_embed\") and self.config.num_denoising > 0:"
        },
        {
            "sha": "a2e044be7b63cd46ec3b3b96634b4395f1f34e70",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/99b0995138c17ef953959c70f35cb2bdc41111a2/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99b0995138c17ef953959c70f35cb2bdc41111a2/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=99b0995138c17ef953959c70f35cb2bdc41111a2",
            "patch": "@@ -635,6 +635,12 @@ def _init_weights(self, module):\n                     nn.init.constant_(layer.layers[-1].weight, 0)\n                     nn.init.constant_(layer.layers[-1].bias, 0)\n \n+            if hasattr(module, \"reg_scale\"):\n+                module.reg_scale.fill_(self.config.reg_scale)\n+\n+            if hasattr(module, \"up\"):\n+                module.up.fill_(self.config.up)\n+\n         if isinstance(module, DFineMultiscaleDeformableAttention):\n             nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n             default_dtype = torch.get_default_dtype()\n@@ -672,6 +678,10 @@ def _init_weights(self, module):\n             init.constant_(module.reg_conf.layers[-1].bias, 0)\n             init.constant_(module.reg_conf.layers[-1].weight, 0)\n \n+        if isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n         if hasattr(module, \"weight_embedding\") and self.config.learn_initial_query:\n             nn.init.xavier_uniform_(module.weight_embedding.weight)\n         if hasattr(module, \"denoising_class_embed\") and self.config.num_denoising > 0:"
        },
        {
            "sha": "4e1d376a3d088f1851f693ffd57bd89c7b2c8357",
            "filename": "src/transformers/models/xcodec/modeling_xcodec.py",
            "status": "modified",
            "additions": 20,
            "deletions": 1,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/99b0995138c17ef953959c70f35cb2bdc41111a2/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99b0995138c17ef953959c70f35cb2bdc41111a2/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py?ref=99b0995138c17ef953959c70f35cb2bdc41111a2",
            "patch": "@@ -332,7 +332,6 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n@@ -341,6 +340,23 @@ def _init_weights(self, module):\n             if module.bias is not None:\n                 k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n                 nn.init.uniform_(module.bias, a=-k, b=k)\n+        elif module.__class__.__name__ == \"Snake1d\":\n+            module.alpha.data.fill_(1.0)\n+        elif isinstance(module, nn.ConvTranspose1d):\n+            module.reset_parameters()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=0.02)\n+        elif isinstance(module, XcodecModel):\n+            # The conv1d are not handled correctly, as `self.acoustic_encoder/decoder` are initialized from a PreTrainedModel,\n+            # but then only the submodules are used (which are not PreTrainedModels...) -> here we reinit them as in DacModel\n+            for submodule in module.acoustic_encoder.modules():\n+                if isinstance(submodule, nn.Conv1d):\n+                    nn.init.trunc_normal_(submodule.weight, std=0.02)\n+                    nn.init.constant_(submodule.bias, 0)\n+            for submodule in module.acoustic_decoder.modules():\n+                if isinstance(submodule, nn.Conv1d):\n+                    nn.init.trunc_normal_(submodule.weight, std=0.02)\n+                    nn.init.constant_(submodule.bias, 0)\n \n     def apply_weight_norm(self):\n         \"\"\"Apply weight norm in the acoustic encoder and decoder because the original checkpoint has weight norm applied.\"\"\"\n@@ -396,6 +412,9 @@ def __init__(self, config):\n         self.fc2 = nn.Linear(config.hidden_size, config.acoustic_model_config.hidden_size)\n         self.quantizer = XcodecResidualVectorQuantization(config)\n \n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n     @staticmethod\n     def _adjust_dac_decoder(decoder: nn.Module):\n         r\"\"\""
        },
        {
            "sha": "6ff4fc061b1b352de450f1fc5430accf9b66c796",
            "filename": "tests/models/d_fine/test_modeling_d_fine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/99b0995138c17ef953959c70f35cb2bdc41111a2/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99b0995138c17ef953959c70f35cb2bdc41111a2/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py?ref=99b0995138c17ef953959c70f35cb2bdc41111a2",
            "patch": "@@ -361,10 +361,6 @@ def test_model_common_attributes(self):\n     def test_resize_tokens_embeddings(self):\n         pass\n \n-    @unittest.skip(reason=\"Not relevant for the model\")\n-    def test_can_init_all_missing_weights(self):\n-        pass\n-\n     @unittest.skip(reason=\"Feed forward chunking is not implemented\")\n     def test_feed_forward_chunking(self):\n         pass"
        },
        {
            "sha": "a5430f9f666cee487c23ebc62c014fb1be51a070",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/99b0995138c17ef953959c70f35cb2bdc41111a2/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99b0995138c17ef953959c70f35cb2bdc41111a2/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=99b0995138c17ef953959c70f35cb2bdc41111a2",
            "patch": "@@ -148,7 +148,6 @@ class Gemma3nAudioModelTest(ModelTesterMixin, unittest.TestCase):\n     _is_stateful = True\n     main_input_name = \"audio_mel\"\n     test_initialization = False\n-    test_can_init_all_missing_weights = False\n \n     def setUp(self):\n         self.model_tester = Gemma3nAudioModelTester(self)"
        },
        {
            "sha": "403eb5a5c71fa739846a473b01ff3b05b0c50054",
            "filename": "tests/models/hgnet_v2/test_modeling_hgnet_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/99b0995138c17ef953959c70f35cb2bdc41111a2/tests%2Fmodels%2Fhgnet_v2%2Ftest_modeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99b0995138c17ef953959c70f35cb2bdc41111a2/tests%2Fmodels%2Fhgnet_v2%2Ftest_modeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhgnet_v2%2Ftest_modeling_hgnet_v2.py?ref=99b0995138c17ef953959c70f35cb2bdc41111a2",
            "patch": "@@ -189,10 +189,6 @@ class HGNetV2ForImageClassificationTest(ModelTesterMixin, PipelineTesterMixin, u\n     def setUp(self):\n         self.model_tester = HGNetV2ModelTester(self)\n \n-    @unittest.skip(reason=\"Does not work on the tiny model.\")\n-    def test_model_parallelism(self):\n-        super().test_model_parallelism()\n-\n     @unittest.skip(reason=\"HGNetV2 does not output attentions\")\n     def test_attention_outputs(self):\n         pass\n@@ -209,14 +205,6 @@ def test_inputs_embeds(self):\n     def test_model_common_attributes(self):\n         pass\n \n-    @unittest.skip(reason=\"HGNetV2 does not have a model\")\n-    def test_model(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not relevant for the model\")\n-    def test_can_init_all_missing_weights(self):\n-        pass\n-\n     def test_backbone(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_backbone(*config_and_inputs)"
        },
        {
            "sha": "f1769415f1bcf77ac27814372473d0b848061608",
            "filename": "tests/models/xcodec/test_modeling_xcodec.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/99b0995138c17ef953959c70f35cb2bdc41111a2/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99b0995138c17ef953959c70f35cb2bdc41111a2/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py?ref=99b0995138c17ef953959c70f35cb2bdc41111a2",
            "patch": "@@ -114,7 +114,6 @@ class XcodecModelTest(ModelTesterMixin, unittest.TestCase):\n     test_headmasking = False\n     test_resize_embeddings = False\n     test_torchscript = False\n-    test_can_init_all_missing_weights = False\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         # model does not support returning hidden states"
        }
    ],
    "stats": {
        "total": 59,
        "additions": 40,
        "deletions": 19
    }
}