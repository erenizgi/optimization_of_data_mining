{
    "author": "MinJu-Ha",
    "message": "Add Fast Image Processor for mobileViT (#37143)\n\n* Add image_processing_mobilevit_fast.py\n\n* Fix copies\n\n* update _preprocess for channel_flip\n\n* Update for batched image processing\n\n* Resolve merge conflicts with main\n\n* Fix import order and remove trailing whitespace (ruff clean-up)\n\n* Fix copy inconsistencies\n\n* Add NotImplementedError for post_process_semantic_segmentation to satisfy repo checks\n\n* Add auto_docstring\n\n* Adjust style\n\n* Update docs/source/en/model_doc/mobilevit.md\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* Update src/transformers/models/mobilevit/image_processing_mobilevit_fast.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* Update src/transformers/models/mobilevit/image_processing_mobilevit_fast.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* Delete not used function\n\n* test: add missing tests for  and\n\n* Add post_process_semantic_segmentation to mobilevit_fast.py\n\n* Add preprocess function to image_processing_mobilebit_fast.py\n\n* ruff check for formatting\n\n* fix: modify preprocess method to handle BatchFeature correctly\n\n* Remove logic for default value assignment\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* Remove normalization adn RGB conversion logic not used in slow processor\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* Simplify return_tensors logic using one-liner conditional expression\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* Remove unused normalization and format parameters\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* add **kwargs and remove default values in _preprocess\n\n* add slow_fast equivalence tests for segmentation\n\n* style: autoformat code with ruff\n\n* Fix slow_fast equivalence test\n\n* merge + remove skipped test\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "49d9fd49bd3d58853d461295bc2fd4f2c808de87",
    "files": [
        {
            "sha": "0ce9f8d21fd9549165d0ab5708d463673a475076",
            "filename": "docs/source/en/model_doc/mobilevit.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/49d9fd49bd3d58853d461295bc2fd4f2c808de87/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/49d9fd49bd3d58853d461295bc2fd4f2c808de87/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md?ref=49d9fd49bd3d58853d461295bc2fd4f2c808de87",
            "patch": "@@ -95,6 +95,12 @@ If you're interested in submitting a resource to be included here, please feel f\n     - preprocess\n     - post_process_semantic_segmentation\n \n+## MobileViTImageProcessorFast\n+\n+[[autodoc]] MobileViTImageProcessorFast\n+    - preprocess\n+    - post_process_semantic_segmentation\n+\n <frameworkcontent>\n <pt>\n "
        },
        {
            "sha": "b8ce8c7280ddc5bea54ac8ee41218c83ed1746c4",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/49d9fd49bd3d58853d461295bc2fd4f2c808de87/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49d9fd49bd3d58853d461295bc2fd4f2c808de87/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=49d9fd49bd3d58853d461295bc2fd4f2c808de87",
            "patch": "@@ -123,8 +123,8 @@\n             (\"mllama\", (\"MllamaImageProcessor\",)),\n             (\"mobilenet_v1\", (\"MobileNetV1ImageProcessor\", \"MobileNetV1ImageProcessorFast\")),\n             (\"mobilenet_v2\", (\"MobileNetV2ImageProcessor\", \"MobileNetV2ImageProcessorFast\")),\n-            (\"mobilevit\", (\"MobileViTImageProcessor\",)),\n-            (\"mobilevitv2\", (\"MobileViTImageProcessor\",)),\n+            (\"mobilevit\", (\"MobileViTImageProcessor\", \"MobileViTImageProcessorFast\")),\n+            (\"mobilevitv2\", (\"MobileViTImageProcessor\", \"MobileViTImageProcessorFast\")),\n             (\"nat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"nougat\", (\"NougatImageProcessor\", \"NougatImageProcessorFast\")),\n             (\"oneformer\", (\"OneFormerImageProcessor\",)),"
        },
        {
            "sha": "6750449a3eae900890eeaffca5c766ba9cba3339",
            "filename": "src/transformers/models/mobilevit/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/49d9fd49bd3d58853d461295bc2fd4f2c808de87/src%2Ftransformers%2Fmodels%2Fmobilevit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49d9fd49bd3d58853d461295bc2fd4f2c808de87/src%2Ftransformers%2Fmodels%2Fmobilevit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2F__init__.py?ref=49d9fd49bd3d58853d461295bc2fd4f2c808de87",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_mobilevit import *\n     from .feature_extraction_mobilevit import *\n     from .image_processing_mobilevit import *\n+    from .image_processing_mobilevit_fast import *\n     from .modeling_mobilevit import *\n     from .modeling_tf_mobilevit import *\n else:"
        },
        {
            "sha": "251666c801211b7641a6f2af061e6116fdfc141b",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit_fast.py",
            "status": "added",
            "additions": 237,
            "deletions": 0,
            "changes": 237,
            "blob_url": "https://github.com/huggingface/transformers/blob/49d9fd49bd3d58853d461295bc2fd4f2c808de87/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49d9fd49bd3d58853d461295bc2fd4f2c808de87/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py?ref=49d9fd49bd3d58853d461295bc2fd4f2c808de87",
            "patch": "@@ -0,0 +1,237 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for MobileViT.\"\"\"\n+\n+from typing import Optional\n+\n+import torch\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    ChannelDimension,\n+    PILImageResampling,\n+    is_torch_tensor,\n+    make_list_of_images,\n+    pil_torch_interpolation_mapping,\n+    validate_kwargs,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring\n+\n+\n+class MobileVitFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    do_flip_channel_order (`bool`, *optional*, defaults to `self.do_flip_channel_order`):\n+        Whether to flip the color channels from RGB to BGR or vice versa.\n+    \"\"\"\n+\n+    do_flip_channel_order: Optional[bool]\n+\n+\n+@auto_docstring\n+class MobileViTImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    size = {\"shortest_edge\": 224}\n+    default_to_square = False\n+    crop_size = {\"height\": 256, \"width\": 256}\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    do_normalize = None\n+    do_convert_rgb = None\n+    do_flip_channel_order = True\n+    valid_kwargs = MobileVitFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[MobileVitFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images,\n+        do_resize: bool,\n+        size: Optional[dict],\n+        interpolation: Optional[str],\n+        do_rescale: bool,\n+        rescale_factor: Optional[float],\n+        do_center_crop: bool,\n+        crop_size: Optional[dict],\n+        do_flip_channel_order: bool,\n+        disable_grouping: bool,\n+        return_tensors: Optional[str],\n+        **kwargs,\n+    ):\n+        processed_images = []\n+\n+        # Group images by shape for more efficient batch processing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+\n+        # Process each group of images with the same shape\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+\n+        # Reorder images to original sequence\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group again after resizing (in case resize produced different sizes)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(image=stacked_images, size=crop_size)\n+            if do_rescale:\n+                stacked_images = self.rescale(image=stacked_images, scale=rescale_factor)\n+            if do_flip_channel_order:\n+                # For batched images, we need to handle them all at once\n+                if stacked_images.ndim > 3 and stacked_images.shape[1] >= 3:\n+                    # Flip RGB â†’ BGR for batched images\n+                    flipped = stacked_images.clone()\n+                    flipped[:, 0:3] = stacked_images[:, [2, 1, 0], ...]\n+                    stacked_images = flipped\n+\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # Stack all processed images if return_tensors is specified\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return processed_images\n+\n+    def _preprocess_segmentation_maps(\n+        self,\n+        segmentation_maps,\n+        **kwargs,\n+    ):\n+        \"\"\"Preprocesses segmentation maps.\"\"\"\n+        processed_segmentation_maps = []\n+        for segmentation_map in segmentation_maps:\n+            segmentation_map = self._process_image(\n+                segmentation_map, do_convert_rgb=False, input_data_format=ChannelDimension.FIRST\n+            )\n+\n+            if segmentation_map.ndim == 2:\n+                segmentation_map = segmentation_map[None, ...]\n+\n+            processed_segmentation_maps.append(segmentation_map)\n+\n+        kwargs[\"do_rescale\"] = False\n+        kwargs[\"do_flip_channel_order\"] = False\n+        kwargs[\"interpolation\"] = pil_torch_interpolation_mapping[PILImageResampling.NEAREST]\n+        processed_segmentation_maps = self._preprocess(images=processed_segmentation_maps, **kwargs)\n+\n+        processed_segmentation_maps = processed_segmentation_maps.squeeze(1)\n+\n+        processed_segmentation_maps = processed_segmentation_maps.to(torch.int64)\n+        return processed_segmentation_maps\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images,\n+        segmentation_maps=None,\n+        **kwargs: Unpack[MobileVitFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess.\n+        \"\"\"\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n+        # Set default kwargs from self. This ensures that if a kwarg is not provided\n+        # by the user, it gets its default value from the instance, or is set to None.\n+        for kwarg_name in self.valid_kwargs.__annotations__:\n+            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n+\n+        # Extract parameters that are only used for preparing the input images\n+        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n+        input_data_format = kwargs.pop(\"input_data_format\")\n+        device = kwargs.pop(\"device\")\n+        # Prepare input images\n+        images = self._prepare_input_images(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+\n+        # Prepare segmentation maps\n+        if segmentation_maps is not None:\n+            segmentation_maps = make_list_of_images(images=segmentation_maps, expected_ndims=2)\n+\n+        # Update kwargs that need further processing before being validated\n+        kwargs = self._further_process_kwargs(**kwargs)\n+\n+        # Validate kwargs\n+        self._validate_preprocess_kwargs(**kwargs)\n+\n+        # torch resize uses interpolation instead of resample\n+        resample = kwargs.pop(\"resample\")\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n+        )\n+\n+        # Pop kwargs that are not needed in _preprocess\n+        kwargs.pop(\"default_to_square\")\n+        kwargs.pop(\"data_format\")\n+\n+        images = self._preprocess(\n+            images=images,\n+            **kwargs,\n+        )\n+\n+        if segmentation_maps is not None:\n+            segmentation_maps = self._preprocess_segmentation_maps(\n+                segmentation_maps=segmentation_maps,\n+                **kwargs,\n+            )\n+            return BatchFeature(data={\"pixel_values\": images, \"labels\": segmentation_maps})\n+\n+        return BatchFeature(data={\"pixel_values\": images})\n+\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+        logits = outputs.logits\n+\n+        # Resize logits and compute semantic segmentation maps\n+        if target_sizes is not None:\n+            if len(logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+            if is_torch_tensor(target_sizes):\n+                target_sizes = target_sizes.numpy()\n+\n+            semantic_segmentation = []\n+\n+            for idx in range(len(logits)):\n+                resized_logits = torch.nn.functional.interpolate(\n+                    logits[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n+                )\n+                semantic_map = resized_logits[0].argmax(dim=0)\n+                semantic_segmentation.append(semantic_map)\n+        else:\n+            semantic_segmentation = logits.argmax(dim=1)\n+            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n+\n+        return semantic_segmentation\n+\n+\n+__all__ = [\"MobileViTImageProcessorFast\"]"
        },
        {
            "sha": "df5caa6b7fb91110bb38d3245b22c02dfd895b69",
            "filename": "tests/models/mobilevit/test_image_processing_mobilevit.py",
            "status": "modified",
            "additions": 151,
            "deletions": 113,
            "changes": 264,
            "blob_url": "https://github.com/huggingface/transformers/blob/49d9fd49bd3d58853d461295bc2fd4f2c808de87/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49d9fd49bd3d58853d461295bc2fd4f2c808de87/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py?ref=49d9fd49bd3d58853d461295bc2fd4f2c808de87",
            "patch": "@@ -15,10 +15,11 @@\n \n import unittest\n \n+import requests\n from datasets import load_dataset\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -27,8 +28,13 @@\n     import torch\n \n if is_vision_available():\n+    from PIL import Image\n+\n     from transformers import MobileViTImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import MobileViTImageProcessorFast\n+\n \n class MobileViTImageProcessingTester:\n     def __init__(\n@@ -98,6 +104,7 @@ def prepare_semantic_batch_inputs():\n @require_vision\n class MobileViTImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = MobileViTImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = MobileViTImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -108,124 +115,155 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"do_flip_channel_order\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"do_flip_channel_order\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 20})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 20})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n+            image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n \n     def test_call_segmentation_maps(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-        maps = []\n-        for image in image_inputs:\n-            self.assertIsInstance(image, torch.Tensor)\n-            maps.append(torch.zeros(image.shape[-2:]).long())\n-\n-        # Test not batched input\n-        encoding = image_processing(image_inputs[0], maps[0], return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n-\n-        # Test batched\n-        encoding = image_processing(image_inputs, maps, return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = self.image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+            maps = []\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+                maps.append(torch.zeros(image.shape[-2:]).long())\n+\n+            # Test not batched input\n+            encoding = image_processing(image_inputs[0], maps[0], return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched\n+            encoding = image_processing(image_inputs, maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test not batched input (PIL images)\n+            image, segmentation_map = prepare_semantic_single_inputs()\n+\n+            encoding = image_processing(image, segmentation_map, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched input (PIL images)\n+            images, segmentation_maps = prepare_semantic_batch_inputs()\n+\n+            encoding = image_processing(images, segmentation_maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.crop_size[\"height\"],\n+                    self.image_processor_tester.crop_size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        # Test with single image\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n         )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n \n-        # Test not batched input (PIL images)\n+        # Test with single image and segmentation map\n         image, segmentation_map = prepare_semantic_single_inputs()\n \n-        encoding = image_processing(image, segmentation_map, return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n-\n-        # Test batched input (PIL images)\n-        images, segmentation_maps = prepare_semantic_batch_inputs()\n-\n-        encoding = image_processing(images, segmentation_maps, return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                2,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                2,\n-                self.image_processor_tester.crop_size[\"height\"],\n-                self.image_processor_tester.crop_size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+        encoding_slow = image_processor_slow(image, segmentation_map, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(image, segmentation_map, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        torch.testing.assert_close(encoding_slow.labels, encoding_fast.labels, atol=1e-1, rtol=1e-3)"
        }
    ],
    "stats": {
        "total": 512,
        "additions": 397,
        "deletions": 115
    }
}