{
    "author": "skwh54",
    "message": "ğŸŒ [i18n-KO] Translated `jamba.md` to Korean (#39890)\n\n* docs: ko: jamba.md\n\n* feat: nmt draft\n\n* fix: manual edits\n\n* fix: resolve suggestion\n\nCo-authored-by: Minseo Kim <75977640+luckyvickyricky@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Minseo Kim <75977640+luckyvickyricky@users.noreply.github.com>",
    "sha": "9e21e502411d947f41391da72d8d6b636c6ea065",
    "files": [
        {
            "sha": "5ba2746e7c896e4e2755b7654747816775936dc3",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e21e502411d947f41391da72d8d6b636c6ea065/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e21e502411d947f41391da72d8d6b636c6ea065/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=9e21e502411d947f41391da72d8d6b636c6ea065",
            "patch": "@@ -597,7 +597,7 @@\n         title: HGNet-V2\n       - local: in_translation\n         title: I-BERT\n-      - local: in_translation\n+      - local: model_doc/jamba\n         title: Jamba\n       - local: in_translation\n         title: JetMoe\n@@ -1241,4 +1241,4 @@\n     - local: in_translation\n       title: (ë²ˆì—­ì¤‘)Environment Variables\n     title: Reference\n-  title: API\n+  title: API\n\\ No newline at end of file"
        },
        {
            "sha": "5eeb4abb712a0e57a982645569748797a6ff96c9",
            "filename": "docs/source/ko/model_doc/jamba.md",
            "status": "added",
            "additions": 158,
            "deletions": 0,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e21e502411d947f41391da72d8d6b636c6ea065/docs%2Fsource%2Fko%2Fmodel_doc%2Fjamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e21e502411d947f41391da72d8d6b636c6ea065/docs%2Fsource%2Fko%2Fmodel_doc%2Fjamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fjamba.md?ref=9e21e502411d947f41391da72d8d6b636c6ea065",
            "patch": "@@ -0,0 +1,158 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+  <div class=\"flex flex-wrap space-x-1\">\n+    <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+    <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+  </div>\n+</div>\n+\n+# Jamba[[jamba]]\n+\n+[Jamba](https://huggingface.co/papers/2403.19887)ëŠ” Transformerì™€ Mamba ê¸°ë°˜ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì „ë¬¸ê°€ í˜¼í•©(MoE) ì–¸ì–´ ëª¨ë¸ë¡œ, ì´ ë§¤ê°œë³€ìˆ˜ ìˆ˜ëŠ” 52Bì—ì„œ 398Bê¹Œì§€ ë‹¤ì–‘í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ Transformer ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ Mambaì™€ ê°™ì€ ìƒíƒœ ê³µê°„ ëª¨ë¸ì˜ íš¨ìœ¨ì„± ë° ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ëŠ¥ë ¥(256K í† í°)ì„ ëª¨ë‘ í™œìš©í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\n+\n+Jambaì˜ ì•„í‚¤í…ì²˜ëŠ” ë¸”ë¡ê³¼ ë ˆì´ì–´ ê¸°ë°˜ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ Transformerì™€ Mamba ì•„í‚¤í…ì²˜ë¥¼ í†µí•©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê° Jamba ë¸”ë¡ì€ ì–´í…ì…˜ ë ˆì´ì–´ ë˜ëŠ” Mamba ë ˆì´ì–´ ì¤‘ í•˜ë‚˜ì™€ ê·¸ ë’¤ë¥¼ ì‡ëŠ” ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP)ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. Transformer ë ˆì´ì–´ëŠ” 8ê°œì˜ ë ˆì´ì–´ ì¤‘ í•˜ë‚˜ì˜ ë¹„ìœ¨ë¡œ ì£¼ê¸°ì ìœ¼ë¡œ ë°°ì¹˜ë©ë‹ˆë‹¤. ë˜í•œ ëª¨ë¸ ìš©ëŸ‰ì„ í™•ì¥í•˜ê¸° ìœ„í•´ MoE ë ˆì´ì–´ê°€ í˜¼í•©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n+\n+ëª¨ë“  ì›ë³¸ Jamba ì²´í¬í¬ì¸íŠ¸ëŠ” [AI21](https://huggingface.co/ai21labs) ì¡°ì§ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+> [!TIP]\n+> ì˜¤ë¥¸ìª½ ì‚¬ì´ë“œë°”ì— ìˆëŠ” Jamba ëª¨ë¸ì„ ëˆ„ë¥´ë©´ ë‹¤ì–‘í•œ ì–¸ì–´ ì‘ì—…ì— Jambaë¥¼ ì ìš©í•˜ëŠ” ì˜ˆì œë¥¼ ë” í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì•„ë˜ ì˜ˆì œëŠ” [`Pipeline`]ê³¼ [`AutoModel`], ê·¸ë¦¬ê³  ì»¤ë§¨ë“œë¼ì¸ì„ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+# ìµœì í™”ëœ Mamba êµ¬í˜„ ì„¤ì¹˜\n+# !pip install mamba-ssm causal-conv1d>=1.2.0\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"text-generation\",\n+    model=\"ai21labs/AI21-Jamba-Mini-1.6\",\n+    torch_dtype=torch.float16,\n+    device=0\n+)\n+pipeline(\"Plants create energy through a process known as\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"ai21labs/AI21-Jamba-Large-1.6\",\n+)\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"ai21labs/AI21-Jamba-Large-1.6\",\n+    torch_dtype=torch.float16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(\"cuda\")\n+\n+output = model.generate(**input_ids, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+echo -e \"Plants create energy through a process known as\" | transformers run --task text-generation --model ai21labs/AI21-Jamba-Mini-1.6 --device 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+ì–‘ìí™”ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë” ë‚®ì€ ì •ë°€ë„ë¡œ í‘œí˜„í•˜ì—¬ ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ë¶€ë‹´ì„ ì¤„ì—¬ì¤ë‹ˆë‹¤. ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì–‘ìí™” ë°±ì—”ë“œì— ëŒ€í•´ì„œëŠ” [Quantization](../quantization/overview)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n+\n+ì•„ë˜ ì˜ˆì œëŠ” [bitsandbytes](../quantization/bitsandbytes)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë§Œ 8ë¹„íŠ¸ë¡œ ì–‘ìí™”í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n+\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n+\n+quantization_config = BitsAndBytesConfig(load_in_8bit=True,\n+                                         llm_int8_skip_modules=[\"mamba\"])\n+\n+# ëª¨ë¸ì„ 8ê°œì˜ GPUì— ê³ ë¥´ê²Œ ë¶„ì‚°ì‹œí‚¤ê¸° ìœ„í•œ ë””ë°”ì´ìŠ¤ ë§µ\n+device_map = {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 4, 'model.layers.37': 4, 'model.layers.38': 4, 'model.layers.39': 4, 'model.layers.40': 4, 'model.layers.41': 4, 'model.layers.42': 4, 'model.layers.43': 4, 'model.layers.44': 4, 'model.layers.45': 5, 'model.layers.46': 5, 'model.layers.47': 5, 'model.layers.48': 5, 'model.layers.49': 5, 'model.layers.50': 5, 'model.layers.51': 5, 'model.layers.52': 5, 'model.layers.53': 5, 'model.layers.54': 6, 'model.layers.55': 6, 'model.layers.56': 6, 'model.layers.57': 6, 'model.layers.58': 6, 'model.layers.59': 6, 'model.layers.60': 6, 'model.layers.61': 6, 'model.layers.62': 6, 'model.layers.63': 7, 'model.layers.64': 7, 'model.layers.65': 7, 'model.layers.66': 7, 'model.layers.67': 7, 'model.layers.68': 7, 'model.layers.69': 7, 'model.layers.70': 7, 'model.layers.71': 7, 'model.final_layernorm': 7, 'lm_head': 7}\n+model = AutoModelForCausalLM.from_pretrained(\"ai21labs/AI21-Jamba-Large-1.6\",\n+                                             torch_dtype=torch.bfloat16,\n+                    attn_implementation=\"flash_attention_2\",\n+                                             quantization_config=quantization_config,\n+                                             device_map=device_map)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"ai21labs/AI21-Jamba-Large-1.6\")\n+\n+messages = [\n+   {\"role\": \"system\", \"content\": \"You are an ancient oracle who speaks in cryptic but wise phrases, always hinting at deeper meanings.\"},\n+   {\"role\": \"user\", \"content\": \"Hello!\"},\n+]\n+\n+input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt').to(model.device)\n+\n+outputs = model.generate(input_ids, max_new_tokens=216)\n+\n+# ì¶œë ¥ ë””ì½”ë”©\n+conversation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n+\n+# ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µë§Œ ì¶”ì¶œ\n+assistant_response = conversation.split(messages[-1]['content'])[1].strip()\n+print(assistant_response)\n+# ì¶œë ¥: Seek and you shall find. The path is winding, but the journey is enlightening. What wisdom do you seek from the ancient echoes?\n+```\n+\n+## ì°¸ê³ [[notes]]\n+\n+- ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ Mamba ë¸”ë¡ì€ ì–‘ìí™”í•˜ì§€ ë§ˆì„¸ìš”.\n+- ìµœì í™”ëœ Mamba ì»¤ë„ ì—†ì´ Mambaë¥¼ ì‚¬ìš©í•˜ë©´ ì§€ì—° ì‹œê°„ì´ í¬ê²Œ ì¦ê°€í•˜ë¯€ë¡œ ê¶Œì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ë˜ë„ ì»¤ë„ ì—†ì´ Mambaë¥¼ ì‚¬ìš©í•˜ê³ ì í•œë‹¤ë©´ [`~AutoModel.from_pretrained`]ì—ì„œ `use_mamba_kernels=False`ë¡œ ì„¤ì •í•˜ì„¸ìš”.\n+\n+  ```py\n+  import torch\n+  from transformers import AutoModelForCausalLM\n+  model = AutoModelForCausalLM.from_pretrained(\"ai21labs/AI21-Jamba-1.5-Large\",\n+                                               use_mamba_kernels=False)\n+  ```\n+\n+## JambaConfig[[transformers.JambaConfig]]\n+\n+[[autodoc]] JambaConfig\n+\n+\n+## JambaModel[[transformers.JambaModel]]\n+\n+[[autodoc]] JambaModel\n+    - forward\n+\n+\n+## JambaForCausalLM[[transformers.JambaForCausalLM]]\n+\n+[[autodoc]] JambaForCausalLM\n+    - forward\n+\n+\n+## JambaForSequenceClassification[[transformers.JambaForSequenceClassification]]\n+\n+[[autodoc]] transformers.JambaForSequenceClassification\n+    - forward"
        }
    ],
    "stats": {
        "total": 162,
        "additions": 160,
        "deletions": 2
    }
}