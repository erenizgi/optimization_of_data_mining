{
    "author": "garg-amit",
    "message": "[Phi-3] Bug on stale kv cache  (#33129)\n\n* fix long seq bug\r\n\r\n* fixed format\r\n\r\n* fixed fn copy inconsistency\r\n\r\n* fix long seq bug\r\n\r\n* fixed format\r\n\r\n* fixed fn copy inconsistency\r\n\r\n* Addressed comments\r\n\r\n* added a unit test\r\n\r\n* fixed cache position\r\n\r\n* Added a warning msg to the forward fn\r\n\r\n* fixed test case",
    "sha": "dfd31158eefab01952e729588a37c9fcc81f0813",
    "files": [
        {
            "sha": "273b6a8f505e799b3e5053b7da2d5a4048756d8e",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 21,
            "deletions": 2,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfd31158eefab01952e729588a37c9fcc81f0813/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfd31158eefab01952e729588a37c9fcc81f0813/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=dfd31158eefab01952e729588a37c9fcc81f0813",
            "patch": "@@ -257,7 +257,7 @@ def __init__(self, dim, config, device=None):\n \n     @torch.no_grad()\n     def forward(self, x, position_ids, seq_len=None):\n-        seq_len = torch.max(position_ids) + 1\n+        seq_len = seq_len or torch.max(position_ids) + 1\n         if seq_len > self.original_max_position_embeddings:\n             ext_factors = torch.tensor(self.long_factor, dtype=torch.float32, device=x.device)\n         else:\n@@ -1239,6 +1239,15 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         'This is an example script .\\n Certainly! Below is a sample script that demonstrates a simple task, such as calculating the sum'\n         ```\"\"\"\n+        if (\n+            use_cache\n+            and self.config.rope_scaling\n+            and cache_position is not None\n+            and cache_position[0] == self.config.original_max_position_embeddings\n+        ):\n+            logger.warning(\n+                f\"If you are not using the generate method, you may encounter nonsensical outputs after the {self.config.original_max_position_embeddings}th token, as the KV cache needs to be recomputed.\"\n+            )\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1295,7 +1304,6 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.prepare_inputs_for_generation\n     def prepare_inputs_for_generation(\n         self,\n         input_ids,\n@@ -1308,6 +1316,17 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n+        # When the first time input length reached long and short factor switching point, enforce re-compute cache\n+        # It will cause downside of slower at this single token position, however, better than current failure.\n+        if (\n+            past_key_values\n+            and self.config.rope_scaling\n+            and input_ids.shape[1] >= self.config.original_max_position_embeddings + 1\n+        ):\n+            past_length = cache_position[0]\n+            if past_length <= self.config.original_max_position_embeddings:\n+                past_key_values = None\n+\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here"
        },
        {
            "sha": "ce0a71878877b5aeb38b87702aca29cb407dfbb4",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/dfd31158eefab01952e729588a37c9fcc81f0813/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dfd31158eefab01952e729588a37c9fcc81f0813/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=dfd31158eefab01952e729588a37c9fcc81f0813",
            "patch": "@@ -442,6 +442,47 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n         self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n         self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n \n+    @parameterized.expand([(\"longrope\",)])\n+    def test_model_rope_scaling_short_long_factor(self, scaling_type):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        n_factors = config.hidden_size // config.num_key_value_heads // 2\n+        config.rope_scaling = {\n+            \"type\": scaling_type,\n+            \"short_factor\": [3.0 for _ in range(n_factors)],\n+            \"long_factor\": [5.0 for _ in range(n_factors)],\n+        }\n+        input_tensor = ids_tensor([1, 4090], config.vocab_size)\n+        model = Phi3ForCausalLM(config)\n+        model.to(torch_device)\n+        model.eval()\n+        generation_args_short = {\n+            \"max_length\": config.original_max_position_embeddings,\n+            \"temperature\": 0.0,\n+            \"use_cache\": True,\n+            \"do_sample\": False,\n+            \"return_dict_in_generate\": True,\n+        }\n+        output_with_short_factor = model.generate(input_tensor, **generation_args_short)\n+        keys_with_short_factor = output_with_short_factor.past_key_values[0][0]\n+        generation_args_long = {\n+            \"max_length\": config.original_max_position_embeddings + 5,\n+            \"temperature\": 0.0,\n+            \"use_cache\": True,\n+            \"do_sample\": False,\n+            \"return_dict_in_generate\": True,\n+            \"output_logits\": True,\n+        }\n+        output_with_long_factor = model.generate(input_tensor, **generation_args_long)\n+        keys_with_long_factor = output_with_long_factor.past_key_values[0][0]\n+        last_token_logits = output_with_long_factor.logits[-1][-1]\n+        regenerated_last_token_logits = model(output_with_long_factor.sequences[:, :-1]).logits[0][-1]\n+        keys_with_long_factor = keys_with_long_factor[:, :, : config.original_max_position_embeddings - 1, :]\n+\n+        # KV cache is re-computed after reaching the (`config.original_max_position_embeddings`+1)th token position\n+        self.assertFalse(torch.allclose(keys_with_short_factor, keys_with_long_factor, atol=1e-2, rtol=1e-2))\n+        # Last token generated using long factor\n+        self.assertTrue(torch.allclose(last_token_logits, regenerated_last_token_logits, atol=1e-2, rtol=1e-2))\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 62,
        "deletions": 2
    }
}