{
    "author": "efeecllk",
    "message": "Fix typos: Remove duplicate duplicate words words (#43040)\n\nFix typos and duplicate words across codebase\n\n- Fixed spelling: \"wether\" â†’ \"whether\", \"paramaters\" â†’ \"parameters\"\n- Fixed grammar: \"will allows\" â†’ \"will allow\"\n- Removed duplicate words: \"a a\", \"of of\", \"to to\", \"is is\", \"for for\", \"with with\"\n\nðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-authored-by: Claude Opus 4.5 <noreply@anthropic.com>",
    "sha": "3aa21543ddda64d24314f1a17d2e80ad8747a9af",
    "files": [
        {
            "sha": "bc8e408d0847edc93afa242069335c2943242cf9",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3aa21543ddda64d24314f1a17d2e80ad8747a9af/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3aa21543ddda64d24314f1a17d2e80ad8747a9af/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=3aa21543ddda64d24314f1a17d2e80ad8747a9af",
            "patch": "@@ -197,7 +197,7 @@ This tokenizer will behave as a Llama-like tokenizer, with an updated vocabulary\n \n **Simplified file loading:** Support is added for passing`vocab` and `merges` as file paths directly to tokenizer initialization. The tokenizer will automatically detect the format (SentencePiece `.model`, Tekken `tekken.json`, or plain vocab/merges files) for loading. For BPE tokenizers, if a vocab is provided but no merges, merges will be automatically generated (excluding special tokens).\n \n-Note: Loading from file paths with `vocab=\"<path_to_a_file>\"`'s primary goal is to allow you to do some quick testing, but for `BPE` models for example we don't check wether you properly passed the merges or not. \n+Note: Loading from file paths with `vocab=\"<path_to_a_file>\"`'s primary goal is to allow you to do some quick testing, but for `BPE` models for example we don't check whether you properly passed the merges or not. \n \n #### 2. Simplified decoding API\n \n@@ -524,7 +524,7 @@ Linked PRs:\n - Old, deprecated output type aliases were removed (e.g. `GreedySearchEncoderDecoderOutput`). We now only have 4 output classes built from the following matrix: decoder-only vs encoder-decoder, uses beams vs doesn't use beams (https://github.com/huggingface/transformers/pull/40998)\n - Removed deprecated classes regarding decoding methods that were moved to the Hub due to low usage (constraints and beam scores) (https://github.com/huggingface/transformers/pull/41223)\n - If `generate` doesn't receive any KV Cache argument, the default cache class used is now defined by the model (as opposed to always being `DynamicCache`) (https://github.com/huggingface/transformers/pull/41505)\n-- Generation parameters are no longer accessible via model's config. If generation paramaters are serialized in `config.json` for any old model, it will be loaded back into model's generation config. Users are expected to access or modify generation parameters only with `model.generation_config.do_sample = True`. \n+- Generation parameters are no longer accessible via model's config. If generation parameters are serialized in `config.json` for any old model, it will be loaded back into model's generation config. Users are expected to access or modify generation parameters only with `model.generation_config.do_sample = True`. \n \n ## Trainer\n "
        },
        {
            "sha": "d7d9b83fa2dadf193ec269d45f5b5149a2ae8388",
            "filename": "src/transformers/models/hunyuan_v1_moe/configuration_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3aa21543ddda64d24314f1a17d2e80ad8747a9af/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3aa21543ddda64d24314f1a17d2e80ad8747a9af/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py?ref=3aa21543ddda64d24314f1a17d2e80ad8747a9af",
            "patch": "@@ -177,7 +177,7 @@ def _rope_parameters_validation(self):\n \n         if not isinstance(self.rope_parameters, dict) or len(self.rope_parameters) != 2:\n             raise ValueError(\n-                \"`rope_parameters` must be a dictionary with with two fields, `type` and `factor` or `type` and `alpha`, \"\n+                \"`rope_parameters` must be a dictionary with two fields, `type` and `factor` or `type` and `alpha`,\"\n                 f\"got {self.rope_parameters}\"\n             )\n         rope_parameters_type = self.rope_parameters.get(\"type\", None)"
        },
        {
            "sha": "33790f12ac45bb35a7b507df25922a6b8cd9bf77",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3aa21543ddda64d24314f1a17d2e80ad8747a9af/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3aa21543ddda64d24314f1a17d2e80ad8747a9af/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=3aa21543ddda64d24314f1a17d2e80ad8747a9af",
            "patch": "@@ -477,7 +477,7 @@ def _merge_input_ids_with_audio_features(\n         self, audio_features, num_audio_tokens, inputs_embeds, input_ids, attention_mask, labels\n     ):\n         \"\"\"\n-        Merge input_ids with with audio features into final embeddings\n+        Merge input_ids with audio features into final embeddings\n \n         Args:\n             audio_features (`torch.Tensor` of shape `(num_audios, max_audio_tokens, embed_dim)`):"
        },
        {
            "sha": "d2db44a5ccd832e9ad8de2e6ac7915472bd2c1f2",
            "filename": "src/transformers/models/xcodec/modeling_xcodec.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3aa21543ddda64d24314f1a17d2e80ad8747a9af/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3aa21543ddda64d24314f1a17d2e80ad8747a9af/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py?ref=3aa21543ddda64d24314f1a17d2e80ad8747a9af",
            "patch": "@@ -519,7 +519,7 @@ def encode(\n         e_semantic = self.encoder_semantic(e_semantic_input.transpose(1, 2))\n \n         # orignal codebase infer to get the output length, but we can directly infer it\n-        # from the model and know wether we should pad\n+        # from the model and know whether we should pad\n         if self._get_conv1d_output_lengths(input_values.shape[2], self.acoustic_encoder) != e_semantic.shape[2]:\n             e_acoustic = self.acoustic_encoder(F.pad(input_values, (self.pad, self.pad)))\n         else:"
        },
        {
            "sha": "5fb8e296f7ead995c1a0de2300c2e1dfe2ae3bc7",
            "filename": "src/transformers/pipelines/token_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3aa21543ddda64d24314f1a17d2e80ad8747a9af/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3aa21543ddda64d24314f1a17d2e80ad8747a9af/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py?ref=3aa21543ddda64d24314f1a17d2e80ad8747a9af",
            "patch": "@@ -279,7 +279,7 @@ def preprocess(self, sentence, offset_mapping=None, **preprocess_params):\n                 raise ValueError(\"When `is_split_into_words=True`, `sentence` must be a list of tokens.\")\n             words = sentence\n             sentence = delimiter.join(words)  # Recreate the sentence string for later display and slicing\n-            # This map will allows to convert back word => char indices\n+            # This map will allow to convert back word => char indices\n             word_to_chars_map = []\n             delimiter_len = len(delimiter)\n             char_offset = 0"
        },
        {
            "sha": "7d6027d966d5fa38c91f35567c1d312f48183892",
            "filename": "tests/utils/test_video_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3aa21543ddda64d24314f1a17d2e80ad8747a9af/tests%2Futils%2Ftest_video_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3aa21543ddda64d24314f1a17d2e80ad8747a9af/tests%2Futils%2Ftest_video_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_video_utils.py?ref=3aa21543ddda64d24314f1a17d2e80ad8747a9af",
            "patch": "@@ -98,7 +98,7 @@ def test_make_batched_videos_numpy(self):\n         self.assertEqual(videos_list[0].shape, (1, 16, 32, 3))\n         self.assertTrue(np.array_equal(videos_list[0][0], video))\n \n-        # Test a 4d array of videos is converted to a a list of 1 video\n+        # Test a 4d array of videos is converted to a list of 1 video\n         video = get_random_video(16, 32)\n         videos_list = make_batched_videos(video)\n         self.assertIsInstance(videos_list, list)\n@@ -126,7 +126,7 @@ def test_make_batched_videos_torch(self):\n         self.assertEqual(videos_list[0].shape, (1, 16, 32, 3))\n         self.assertTrue(np.array_equal(videos_list[0][0], video))\n \n-        # Test a 4d array of videos is converted to a a list of 1 video\n+        # Test a 4d array of videos is converted to a list of 1 video\n         video = get_random_video(16, 32)\n         torch_video = torch.from_numpy(video)\n         videos_list = make_batched_videos(torch_video)"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 8,
        "deletions": 8
    }
}