{
    "author": "gante",
    "message": "[tests] fix blip2 edge case (#40699)",
    "sha": "a2a8a3ca1ef23dd3b134199d87c4a1988f062ea9",
    "files": [
        {
            "sha": "e4d73a3d956f93533dbff2188da7d4a83ea33768",
            "filename": "setup.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2a8a3ca1ef23dd3b134199d87c4a1988f062ea9/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2a8a3ca1ef23dd3b134199d87c4a1988f062ea9/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=a2a8a3ca1ef23dd3b134199d87c4a1988f062ea9",
            "patch": "@@ -79,14 +79,12 @@\n stale_egg_info = Path(__file__).parent / \"transformers.egg-info\"\n if stale_egg_info.exists():\n     print(\n-        (\n-            \"Warning: {} exists.\\n\\n\"\n-            \"If you recently updated transformers to 3.0 or later, this is expected,\\n\"\n-            \"but it may prevent transformers from installing in editable mode.\\n\\n\"\n-            \"This directory is automatically generated by Python's packaging tools.\\n\"\n-            \"I will remove it now.\\n\\n\"\n-            \"See https://github.com/pypa/pip/issues/5466 for details.\\n\"\n-        ).format(stale_egg_info)\n+        f\"Warning: {stale_egg_info} exists.\\n\\n\"\n+        \"If you recently updated transformers to 3.0 or later, this is expected,\\n\"\n+        \"but it may prevent transformers from installing in editable mode.\\n\\n\"\n+        \"This directory is automatically generated by Python's packaging tools.\\n\"\n+        \"I will remove it now.\\n\\n\"\n+        \"See https://github.com/pypa/pip/issues/5466 for details.\\n\"\n     )\n     shutil.rmtree(stale_egg_info)\n "
        },
        {
            "sha": "23145ffc543ff39f21a356b0fa5172eecdaa2e90",
            "filename": "src/transformers/models/blip_2/configuration_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2a8a3ca1ef23dd3b134199d87c4a1988f062ea9/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2a8a3ca1ef23dd3b134199d87c4a1988f062ea9/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py?ref=a2a8a3ca1ef23dd3b134199d87c4a1988f062ea9",
            "patch": "@@ -312,6 +312,7 @@ def __init__(\n         self.image_token_index = image_token_index\n         self.qformer_config.encoder_hidden_size = self.vision_config.hidden_size\n         self.use_decoder_only_language_model = self.text_config.model_type in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n+        self.is_encoder_decoder = self.text_config.is_encoder_decoder\n         self.initializer_factor = 1.0\n         self.initializer_range = 0.02\n "
        },
        {
            "sha": "b01733192cd95a6f14e67f42b6ef85684a43719a",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2a8a3ca1ef23dd3b134199d87c4a1988f062ea9/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2a8a3ca1ef23dd3b134199d87c4a1988f062ea9/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=a2a8a3ca1ef23dd3b134199d87c4a1988f062ea9",
            "patch": "@@ -336,7 +336,7 @@ def test_greedy_generate(self):\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._greedy_generate(model=model, inputs_dict=inputs_dict)\n \n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n+            if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n@@ -360,7 +360,7 @@ def test_greedy_generate_dict_outputs(self):\n                 use_cache=False,\n             )\n \n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n+            if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n                 # Retrocompatibility check\n@@ -400,7 +400,7 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n                 use_cache=True,  # Enable cache\n             )\n \n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n+            if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(\n@@ -417,7 +417,7 @@ def test_sample_generate(self):\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._sample_generate(model=model, inputs_dict=inputs_dict, num_return_sequences=1)\n \n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n+            if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n@@ -442,7 +442,7 @@ def test_sample_generate_dict_output(self):\n                 use_cache=False,\n             )\n \n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n+            if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n                 # Retrocompatibility check\n@@ -467,7 +467,7 @@ def test_beam_search_generate(self):\n             beam_kwargs = self._get_beam_kwargs()\n             output_generate = self._beam_search_generate(model=model, inputs_dict=inputs_dict, beam_kwargs=beam_kwargs)\n \n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n+            if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n@@ -492,7 +492,7 @@ def test_beam_search_generate_dict_output(self):\n                 return_dict_in_generate=True,\n                 use_cache=False,\n             )\n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n+            if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                 # Retrocompatibility check\n@@ -541,7 +541,7 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n                 use_cache=True,  # Enable cache\n             )\n \n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n+            if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(\n@@ -594,7 +594,7 @@ def test_beam_sample_generate(self):\n                 beam_kwargs=beam_kwargs,\n             )\n \n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n+            if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n@@ -621,7 +621,7 @@ def test_beam_sample_generate_dict_output(self):\n                 use_cache=False,\n             )\n \n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n+            if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                 # Retrocompatibility check\n@@ -1791,7 +1791,7 @@ def test_generate_compilation_all_outputs(self):\n             else:\n                 self.assertTrue(hasattr(model, \"_compiled_call\"))  # our auto compile should have been called\n \n-            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n+            if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n             else:"
        },
        {
            "sha": "2f6df2aab27ef375f62eb7a130ed634929b9155e",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 6,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2a8a3ca1ef23dd3b134199d87c4a1988f062ea9/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2a8a3ca1ef23dd3b134199d87c4a1988f062ea9/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=a2a8a3ca1ef23dd3b134199d87c4a1988f062ea9",
            "patch": "@@ -21,6 +21,7 @@\n import numpy as np\n import pytest\n import requests\n+from parameterized import parameterized\n \n from transformers import CONFIG_MAPPING, Blip2Config, Blip2QFormerConfig, Blip2VisionConfig\n from transformers.testing_utils import (\n@@ -40,6 +41,7 @@\n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n     ModelTesterMixin,\n     _config_zero_init,\n     floats_tensor,\n@@ -1094,6 +1096,11 @@ def test_initialization(self):\n     def test_internal_model_config_and_subconfig_are_same(self):\n         pass\n \n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @unittest.skip(\"Won't fix: Blip2 + T5 backbone needs custom input preparation for this test\")\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        pass\n+\n \n class Blip2TextModelWithProjectionTester:\n     def __init__(self, parent, vision_kwargs=None, qformer_kwargs=None, is_training=True):\n@@ -1849,7 +1856,10 @@ def test_inference_t5_multi_accelerator(self):\n         # Test output\n         expected_ids_and_text = Expectations(\n             {\n-                (\"cuda\", None): ([0, 2335, 1556, 28, 1782, 30, 8, 2608, 1], \"woman playing with dog on the beach\"),\n+                (\"cuda\", None): (\n+                    [0, 3, 9, 2335, 19, 1556, 28, 160, 1782, 30, 8, 2608, 1],\n+                    \"a woman is playing with her dog on the beach\",\n+                ),\n                 (\"rocm\", (9, 5)): (\n                     [0, 3, 9, 2335, 19, 1556, 28, 160, 1782, 30, 8, 2608, 1],\n                     \"a woman is playing with her dog on the beach\",\n@@ -1869,11 +1879,8 @@ def test_inference_t5_multi_accelerator(self):\n         # Test output\n         expected_ids_and_text = Expectations(\n             {\n-                (\"cuda\", None): ([0, 3, 7, 152, 67, 839, 1], \"san diego\"),\n-                (\"rocm\", (9, 5)): (\n-                    [0, 3, 7, 152, 2515, 11389, 3523, 1],\n-                    \"san francisco\",  # TODO: check if this is ok\n-                ),\n+                (\"cuda\", None): ([0, 3, 7, 152, 2515, 11389, 3523, 1], \"san francisco\"),\n+                (\"rocm\", (9, 5)): ([0, 3, 7, 152, 2515, 11389, 3523, 1], \"san francisco\"),\n             }\n         ).get_expectation()\n         self.assertEqual(predictions[0].tolist(), expected_ids_and_text[0])"
        }
    ],
    "stats": {
        "total": 56,
        "additions": 31,
        "deletions": 25
    }
}