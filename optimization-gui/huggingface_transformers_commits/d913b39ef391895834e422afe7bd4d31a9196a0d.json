{
    "author": "RyanMullins",
    "message": "fix: HWIO to OIHW (#39200)\n\n* fix: HWIO to OIHW\n\n* Bug in attention type\n\n* Conversion script docstring\n\n* style\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: Arthur <arthur.zucker@gmail.com>",
    "sha": "d913b39ef391895834e422afe7bd4d31a9196a0d",
    "files": [
        {
            "sha": "19013e7fdfe882fe7f2fef26bd95b7745d4cb3b2",
            "filename": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d913b39ef391895834e422afe7bd4d31a9196a0d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d913b39ef391895834e422afe7bd4d31a9196a0d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py?ref=d913b39ef391895834e422afe7bd4d31a9196a0d",
            "patch": "@@ -271,7 +271,7 @@ def __init__(\n \n         if layer_types is None:\n             self.layer_types = [\n-                \"full_attention\" if i % 5 == 0 else \"sliding_attention\" for i in range(self.num_hidden_layers)\n+                \"full_attention\" if (i + 1) % 5 == 0 else \"sliding_attention\" for i in range(self.num_hidden_layers)\n             ]\n         else:\n             self.layer_types = layer_types"
        },
        {
            "sha": "7a55eb5520250ce5dbbb0ab1c99743c6c6397ba9",
            "filename": "src/transformers/models/gemma3n/convert_gemma3n_weights.py",
            "status": "modified",
            "additions": 15,
            "deletions": 11,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/d913b39ef391895834e422afe7bd4d31a9196a0d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d913b39ef391895834e422afe7bd4d31a9196a0d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py?ref=d913b39ef391895834e422afe7bd4d31a9196a0d",
            "patch": "@@ -18,9 +18,9 @@\n \n python src/transformers/models/gemma3n/convert_gemma3n_weights.py \\\n     --variant='gemma3n_e4b' \\\n-    --tokenizer_path=\"$HOME/nano3/checkpoints/tokenizer/gemma-3n-tokenizer.model\" \\\n-    --checkpoint_path=\"$HOME/nano3/checkpoints/g251_orbax/\" \\\n-    --output_path=\"$HOME/nano3/checkpoints/g251_vision_encoder/\"\n+    --tokenizer_path=\"$HOME/tokenizers/gemma-3n-tokenizer.model\" \\\n+    --checkpoint_path=\"$HOME/checkpoints/gemma-3n-orbax/\" \\\n+    --output_path=\"$HOME/checkpoints/gemma-3n-safetensors/\"\n \"\"\"\n \n import json\n@@ -552,8 +552,9 @@ def generate_base_path(path: str, block_type: str) -> tuple[str, tuple[int, int]\n             converted_weight = weights\n     elif _MOBILE_NET_CONV in path:\n         if \"Conv_0\" in path:\n-            converted_path = \"conv_stem.conv.weight\"\n-            converted_weight = weights.transpose(3, 2, 1, 0)\n+            converted_path = (\"conv_stem.conv.weight\", \"conv_stem.conv.bias\")\n+            converted_weight = weights.transpose(3, 2, 0, 1)\n+            converted_weight = (converted_weight, np.zeros(converted_weight.shape[0]))\n         elif \"Normalize_0\" in path:\n             converted_path = \"conv_stem.bn.weight\"\n             converted_weight = weights\n@@ -567,7 +568,7 @@ def generate_base_path(path: str, block_type: str) -> tuple[str, tuple[int, int]\n             converted_weight = weights\n         elif \"expand_conv\" in path:\n             converted_path += \".conv_exp.weight\"\n-            converted_weight = weights.transpose(3, 2, 1, 0)\n+            converted_weight = weights.transpose(3, 2, 0, 1)\n         else:\n             converted_path += \".conv_pwl.weight\"\n             converted_weight = weights.transpose()[:, :, None, None]\n@@ -588,7 +589,7 @@ def generate_base_path(path: str, block_type: str) -> tuple[str, tuple[int, int]\n             converted_weight = weights\n         elif \"key_dwconv\" in path:\n             converted_path += \".attn.key.down_conv.weight\"\n-            converted_weight = weights.transpose()\n+            converted_weight = weights.transpose(3, 2, 0, 1)\n         elif \"key_proj\" in path:\n             converted_path += \".attn.key.proj.weight\"\n             converted_weight = weights.transpose()[:, :, None, None]\n@@ -600,7 +601,7 @@ def generate_base_path(path: str, block_type: str) -> tuple[str, tuple[int, int]\n             converted_weight = weights.transpose()[:, :, None, None]\n         elif \"value_dwconv\" in path:\n             converted_path += \".attn.value.down_conv.weight\"\n-            converted_weight = weights.transpose()\n+            converted_weight = weights.transpose(3, 2, 0, 1)\n         elif \"value_proj\" in path:\n             converted_path += \".attn.value.proj.weight\"\n             converted_weight = weights.transpose()[:, :, None, None]\n@@ -630,15 +631,18 @@ def generate_base_path(path: str, block_type: str) -> tuple[str, tuple[int, int]\n             converted_weight = weights.transpose()[:, :, None, None]\n         elif \"middle_dwconv\" in path:\n             converted_path += \".dw_mid.conv.weight\"\n-            converted_weight = weights.transpose(3, 2, 1, 0)\n+            converted_weight = weights.transpose(3, 2, 0, 1)\n         elif \"project\" in path:\n             converted_path += \".pw_proj.conv.weight\"\n             converted_weight = weights.transpose()[:, :, None, None]\n         elif \"start_dwconv\" in path:\n             converted_path += \".dw_start.conv.weight\"\n-            converted_weight = weights.transpose(3, 2, 1, 0)\n+            converted_weight = weights.transpose(3, 2, 0, 1)\n \n-    return [(converted_path, converted_weight)]\n+    if isinstance(converted_path, (tuple, list)):\n+        return zip(converted_path, converted_weight)\n+    else:\n+        return [(converted_path, converted_weight)]\n \n \n def convert(checkpoint_path: str, config: Gemma3nConfig) -> dict[str, torch.Tensor]:"
        },
        {
            "sha": "bd20faacf53c6fab503bbf56379eaca0748a2855",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d913b39ef391895834e422afe7bd4d31a9196a0d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d913b39ef391895834e422afe7bd4d31a9196a0d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=d913b39ef391895834e422afe7bd4d31a9196a0d",
            "patch": "@@ -283,7 +283,7 @@ def __init__(\n \n         if layer_types is None:\n             self.layer_types = [\n-                \"full_attention\" if i % 5 == 0 else \"sliding_attention\" for i in range(self.num_hidden_layers)\n+                \"full_attention\" if (i + 1) % 5 == 0 else \"sliding_attention\" for i in range(self.num_hidden_layers)\n             ]\n         else:\n             self.layer_types = layer_types"
        }
    ],
    "stats": {
        "total": 30,
        "additions": 17,
        "deletions": 13
    }
}