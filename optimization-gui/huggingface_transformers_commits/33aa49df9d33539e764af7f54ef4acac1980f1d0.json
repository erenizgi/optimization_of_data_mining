{
    "author": "gante",
    "message": "[docs] Ko doc fixes after toc update (#39660)\n\n* update docs\n\n* doc builder working\n\n* make fixup",
    "sha": "33aa49df9d33539e764af7f54ef4acac1980f1d0",
    "files": [
        {
            "sha": "c55638ded1497c574639d8f352dbe1ed303a06fb",
            "filename": ".github/workflows/build_documentation.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/33aa49df9d33539e764af7f54ef4acac1980f1d0/.github%2Fworkflows%2Fbuild_documentation.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/33aa49df9d33539e764af7f54ef4acac1980f1d0/.github%2Fworkflows%2Fbuild_documentation.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fbuild_documentation.yml?ref=33aa49df9d33539e764af7f54ef4acac1980f1d0",
            "patch": "@@ -18,10 +18,6 @@ jobs:\n       notebook_folder: transformers_doc\n       languages: ar de en es fr hi it ko pt tr zh ja te\n       custom_container: huggingface/transformers-doc-builder\n-      # Temporary pin to work around datasets exception in the docbuilder.Remove after docker images and main have\n-      # the right dependencies (which **should** be the case by 2025-07-20). See\n-      # https://github.com/huggingface/transformers/actions/runs/16365952006/job/46243081358?pr=38545\n-      pre_command: uv pip install datasets>=2.15.0\n     secrets:\n       token: ${{ secrets.HUGGINGFACE_PUSH }}\n       hf_token: ${{ secrets.HF_DOC_BUILD_PUSH }}"
        },
        {
            "sha": "c2f61c45354c5db74449a18efaf461b38defe432",
            "filename": ".github/workflows/build_pr_documentation.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/33aa49df9d33539e764af7f54ef4acac1980f1d0/.github%2Fworkflows%2Fbuild_pr_documentation.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/33aa49df9d33539e764af7f54ef4acac1980f1d0/.github%2Fworkflows%2Fbuild_pr_documentation.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fbuild_pr_documentation.yml?ref=33aa49df9d33539e764af7f54ef4acac1980f1d0",
            "patch": "@@ -15,7 +15,3 @@ jobs:\n       pr_number: ${{ github.event.number }}\n       package: transformers\n       languages: en\n-      # Temporary pin to work around datasets exception in the docbuilder. Remove after docker images and main have\n-      # the right dependencies (which **should** be the case by 2025-07-20). See\n-      # https://github.com/huggingface/transformers/actions/runs/16365952006/job/46243081358?pr=38545\n-      pre_command: uv pip install datasets>=2.15.0"
        },
        {
            "sha": "5a40a13f401c775a70984bd41c612ea51f2acff0",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 328,
            "deletions": 328,
            "changes": 656,
            "blob_url": "https://github.com/huggingface/transformers/blob/33aa49df9d33539e764af7f54ef4acac1980f1d0/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/33aa49df9d33539e764af7f54ef4acac1980f1d0/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=33aa49df9d33539e764af7f54ef4acac1980f1d0",
            "patch": "@@ -96,7 +96,7 @@\n       title: (번역중) torch.compile\n     - local: perf_infer_gpu_one\n       title: 하나의 GPU를 활용한 추론\n-    - local: in_translation\n+    - local: perf_infer_gpu_multi\n       title: (번역중) Distributed inference\n     - local: perf_infer_cpu\n       title: CPU로 추론하기\n@@ -119,7 +119,7 @@\n       title: Trainer API를 사용한 하이퍼파라미터 탐색\n     title: Trainer API\n   - sections:\n-    - local: in_translation\n+    - local: accelerator_selection\n       title: (번역중) Accelerator selection\n     - local: accelerate\n       title: 🤗 Accelerate로 분산 학습 구성하기\n@@ -346,7 +346,7 @@\n       title: Trainer\n     - local: in_translation\n       title: (번역중) DeepSpeed\n-    - local: main_classes/executorch\n+    - local: in_translation\n       title: ExecuTorch\n     - local: main_classes/feature_extractor\n       title: 피쳐 추출기\n@@ -357,11 +357,11 @@\n     title: 메인 클래스\n   - sections:\n     - sections:\n-      - local: model_doc/albert\n+      - local: in_translation\n         title: ALBERT\n-      - local: model_doc/arcee\n+      - local: in_translation\n         title: Arcee\n-      - local: model_doc/bamba\n+      - local: in_translation\n         title: Bamba\n       - local: model_doc/bart\n         title: BART\n@@ -371,739 +371,739 @@\n         title: BARTpho\n       - local: model_doc/bert\n         title: BERT\n-      - local: model_doc/bert-generation\n+      - local: in_translation\n         title: BertGeneration\n       - local: model_doc/bert-japanese\n         title: BertJapanese\n       - local: model_doc/bertweet\n         title: BERTweet\n-      - local: model_doc/big_bird\n+      - local: in_translation\n         title: BigBird\n-      - local: model_doc/bigbird_pegasus\n+      - local: in_translation\n         title: BigBirdPegasus\n       - local: model_doc/biogpt\n         title: BioGpt\n-      - local: model_doc/bitnet\n+      - local: in_translation\n         title: BitNet\n-      - local: model_doc/blenderbot\n+      - local: in_translation\n         title: Blenderbot\n-      - local: model_doc/blenderbot-small\n+      - local: in_translation\n         title: Blenderbot Small\n-      - local: model_doc/bloom\n+      - local: in_translation\n         title: BLOOM\n-      - local: model_doc/bort\n+      - local: in_translation\n         title: BORT\n-      - local: model_doc/byt5\n+      - local: in_translation\n         title: ByT5\n-      - local: model_doc/camembert\n+      - local: in_translation\n         title: CamemBERT\n-      - local: model_doc/canine\n+      - local: in_translation\n         title: CANINE\n       - local: model_doc/codegen\n         title: CodeGen\n-      - local: model_doc/code_llama\n+      - local: in_translation\n         title: CodeLlama\n       - local: model_doc/cohere\n         title: Cohere\n-      - local: model_doc/cohere2\n+      - local: in_translation\n         title: Cohere2\n       - local: model_doc/convbert\n         title: ConvBERT\n-      - local: model_doc/cpm\n+      - local: in_translation\n         title: CPM\n-      - local: model_doc/cpmant\n+      - local: in_translation\n         title: CPMANT\n-      - local: model_doc/ctrl\n+      - local: in_translation\n         title: CTRL\n       - local: model_doc/dbrx\n         title: DBRX\n       - local: model_doc/deberta\n         title: DeBERTa\n       - local: model_doc/deberta-v2\n         title: DeBERTa-v2\n-      - local: model_doc/deepseek_v3\n+      - local: in_translation\n         title: DeepSeek-V3\n-      - local: model_doc/dialogpt\n+      - local: in_translation\n         title: DialoGPT\n-      - local: model_doc/diffllama\n+      - local: in_translation\n         title: DiffLlama\n-      - local: model_doc/distilbert\n+      - local: in_translation\n         title: DistilBERT\n-      - local: model_doc/doge\n+      - local: in_translation\n         title: Doge\n-      - local: model_doc/dots1\n+      - local: in_translation\n         title: dots1\n-      - local: model_doc/dpr\n+      - local: in_translation\n         title: DPR\n       - local: model_doc/electra\n         title: ELECTRA\n       - local: model_doc/encoder-decoder\n         title: Encoder Decoder Models\n-      - local: model_doc/ernie\n+      - local: in_translation\n         title: ERNIE\n-      - local: model_doc/ernie_m\n+      - local: in_translation\n         title: ErnieM\n       - local: model_doc/esm\n         title: ESM\n       - local: model_doc/exaone4\n         title: EXAONE-4.0\n-      - local: model_doc/falcon\n+      - local: in_translation\n         title: Falcon\n-      - local: model_doc/falcon3\n+      - local: in_translation\n         title: Falcon3\n-      - local: model_doc/falcon_h1\n+      - local: in_translation\n         title: FalconH1\n-      - local: model_doc/falcon_mamba\n+      - local: in_translation\n         title: FalconMamba\n-      - local: model_doc/flan-t5\n+      - local: in_translation\n         title: FLAN-T5\n-      - local: model_doc/flan-ul2\n+      - local: in_translation\n         title: FLAN-UL2\n-      - local: model_doc/flaubert\n+      - local: in_translation\n         title: FlauBERT\n-      - local: model_doc/fnet\n+      - local: in_translation\n         title: FNet\n-      - local: model_doc/fsmt\n+      - local: in_translation\n         title: FSMT\n-      - local: model_doc/funnel\n+      - local: in_translation\n         title: Funnel Transformer\n-      - local: model_doc/fuyu\n+      - local: in_translation\n         title: Fuyu\n       - local: model_doc/gemma\n         title: Gemma\n       - local: model_doc/gemma2\n         title: Gemma2\n-      - local: model_doc/glm\n+      - local: in_translation\n         title: GLM\n-      - local: model_doc/glm4\n+      - local: in_translation\n         title: glm4\n       - local: model_doc/openai-gpt\n         title: GPT\n-      - local: model_doc/gpt_neo\n+      - local: in_translation\n         title: GPT Neo\n-      - local: model_doc/gpt_neox\n+      - local: in_translation\n         title: GPT NeoX\n       - local: model_doc/gpt_neox_japanese\n         title: GPT NeoX Japanese\n-      - local: model_doc/gptj\n+      - local: in_translation\n         title: GPT-J\n-      - local: model_doc/gpt2\n+      - local: in_translation\n         title: GPT2\n-      - local: model_doc/gpt_bigcode\n+      - local: in_translation\n         title: GPTBigCode\n-      - local: model_doc/gptsan-japanese\n+      - local: in_translation\n         title: GPTSAN Japanese\n-      - local: model_doc/gpt-sw3\n+      - local: in_translation\n         title: GPTSw3\n-      - local: model_doc/granite\n+      - local: in_translation\n         title: Granite\n-      - local: model_doc/granitemoe\n+      - local: in_translation\n         title: GraniteMoe\n-      - local: model_doc/granitemoehybrid\n+      - local: in_translation\n         title: GraniteMoeHybrid\n-      - local: model_doc/granitemoeshared\n+      - local: in_translation\n         title: GraniteMoeShared\n-      - local: model_doc/helium\n+      - local: in_translation\n         title: Helium\n-      - local: model_doc/herbert\n+      - local: in_translation\n         title: HerBERT\n-      - local: model_doc/hgnet_v2\n+      - local: in_translation\n         title: HGNet-V2\n-      - local: model_doc/ibert\n+      - local: in_translation\n         title: I-BERT\n-      - local: model_doc/jamba\n+      - local: in_translation\n         title: Jamba\n-      - local: model_doc/jetmoe\n+      - local: in_translation\n         title: JetMoe\n-      - local: model_doc/jukebox\n+      - local: in_translation\n         title: Jukebox\n-      - local: model_doc/led\n+      - local: in_translation\n         title: LED\n-      - local: model_doc/lfm2\n+      - local: in_translation\n         title: LFM2\n       - local: model_doc/llama\n         title: LLaMA\n       - local: model_doc/llama2\n         title: Llama2\n       - local: model_doc/llama3\n         title: Llama3\n-      - local: model_doc/longformer\n+      - local: in_translation\n         title: Longformer\n-      - local: model_doc/longt5\n+      - local: in_translation\n         title: LongT5\n-      - local: model_doc/luke\n+      - local: in_translation\n         title: LUKE\n-      - local: model_doc/m2m_100\n+      - local: in_translation\n         title: M2M100\n-      - local: model_doc/madlad-400\n+      - local: in_translation\n         title: MADLAD-400\n       - local: model_doc/mamba\n         title: Mamba\n       - local: model_doc/mamba2\n         title: Mamba2\n       - local: model_doc/marian\n         title: MarianMT\n-      - local: model_doc/markuplm\n+      - local: in_translation\n         title: MarkupLM\n-      - local: model_doc/mbart\n+      - local: in_translation\n         title: MBart and MBart-50\n-      - local: model_doc/mega\n+      - local: in_translation\n         title: MEGA\n-      - local: model_doc/megatron-bert\n+      - local: in_translation\n         title: MegatronBERT\n-      - local: model_doc/megatron_gpt2\n+      - local: in_translation\n         title: MegatronGPT2\n-      - local: model_doc/minimax\n+      - local: in_translation\n         title: MiniMax\n       - local: model_doc/mistral\n         title: Mistral\n-      - local: model_doc/mixtral\n+      - local: in_translation\n         title: Mixtral\n-      - local: model_doc/mluke\n+      - local: in_translation\n         title: mLUKE\n-      - local: model_doc/mobilebert\n+      - local: in_translation\n         title: MobileBERT\n-      - local: model_doc/modernbert\n+      - local: in_translation\n         title: ModernBert\n-      - local: model_doc/modernbert-decoder\n+      - local: in_translation\n         title: ModernBERTDecoder\n-      - local: model_doc/mpnet\n+      - local: in_translation\n         title: MPNet\n-      - local: model_doc/mpt\n+      - local: in_translation\n         title: MPT\n-      - local: model_doc/mra\n+      - local: in_translation\n         title: MRA\n-      - local: model_doc/mt5\n+      - local: in_translation\n         title: MT5\n-      - local: model_doc/mvp\n+      - local: in_translation\n         title: MVP\n-      - local: model_doc/myt5\n+      - local: in_translation\n         title: myt5\n-      - local: model_doc/nemotron\n+      - local: in_translation\n         title: Nemotron\n-      - local: model_doc/nezha\n+      - local: in_translation\n         title: NEZHA\n-      - local: model_doc/nllb\n+      - local: in_translation\n         title: NLLB\n-      - local: model_doc/nllb-moe\n+      - local: in_translation\n         title: NLLB-MoE\n-      - local: model_doc/nystromformer\n+      - local: in_translation\n         title: Nyströmformer\n-      - local: model_doc/olmo\n+      - local: in_translation\n         title: OLMo\n-      - local: model_doc/olmo2\n+      - local: in_translation\n         title: OLMo2\n-      - local: model_doc/olmoe\n+      - local: in_translation\n         title: OLMoE\n-      - local: model_doc/open-llama\n+      - local: in_translation\n         title: Open-Llama\n-      - local: model_doc/opt\n+      - local: in_translation\n         title: OPT\n-      - local: model_doc/pegasus\n+      - local: in_translation\n         title: Pegasus\n-      - local: model_doc/pegasus_x\n+      - local: in_translation\n         title: PEGASUS-X\n-      - local: model_doc/persimmon\n+      - local: in_translation\n         title: Persimmon\n-      - local: model_doc/phi\n+      - local: in_translation\n         title: Phi\n-      - local: model_doc/phi3\n+      - local: in_translation\n         title: Phi-3\n-      - local: model_doc/phimoe\n+      - local: in_translation\n         title: PhiMoE\n-      - local: model_doc/phobert\n+      - local: in_translation\n         title: PhoBERT\n-      - local: model_doc/plbart\n+      - local: in_translation\n         title: PLBart\n-      - local: model_doc/prophetnet\n+      - local: in_translation\n         title: ProphetNet\n-      - local: model_doc/qdqbert\n+      - local: in_translation\n         title: QDQBert\n-      - local: model_doc/qwen2\n+      - local: in_translation\n         title: Qwen2\n-      - local: model_doc/qwen2_moe\n+      - local: in_translation\n         title: Qwen2MoE\n-      - local: model_doc/qwen3\n+      - local: in_translation\n         title: Qwen3\n-      - local: model_doc/qwen3_moe\n+      - local: in_translation\n         title: Qwen3MoE\n       - local: model_doc/rag\n         title: RAG\n-      - local: model_doc/realm\n+      - local: in_translation\n         title: REALM\n-      - local: model_doc/recurrent_gemma\n+      - local: in_translation\n         title: RecurrentGemma\n-      - local: model_doc/reformer\n+      - local: in_translation\n         title: Reformer\n-      - local: model_doc/rembert\n+      - local: in_translation\n         title: RemBERT\n-      - local: model_doc/retribert\n+      - local: in_translation\n         title: RetriBERT\n       - local: model_doc/roberta\n         title: RoBERTa\n-      - local: model_doc/roberta-prelayernorm\n+      - local: in_translation\n         title: RoBERTa-PreLayerNorm\n-      - local: model_doc/roc_bert\n+      - local: in_translation\n         title: RoCBert\n-      - local: model_doc/roformer\n+      - local: in_translation\n         title: RoFormer\n-      - local: model_doc/rwkv\n+      - local: in_translation\n         title: RWKV\n-      - local: model_doc/splinter\n+      - local: in_translation\n         title: Splinter\n-      - local: model_doc/squeezebert\n+      - local: in_translation\n         title: SqueezeBERT\n-      - local: model_doc/stablelm\n+      - local: in_translation\n         title: StableLm\n-      - local: model_doc/starcoder2\n+      - local: in_translation\n         title: Starcoder2\n-      - local: model_doc/switch_transformers\n+      - local: in_translation\n         title: SwitchTransformers\n-      - local: model_doc/t5\n+      - local: in_translation\n         title: T5\n-      - local: model_doc/t5gemma\n+      - local: in_translation\n         title: T5Gemma\n-      - local: model_doc/t5v1.1\n+      - local: in_translation\n         title: T5v1.1\n-      - local: model_doc/tapex\n+      - local: in_translation\n         title: TAPEX\n-      - local: model_doc/transfo-xl\n+      - local: in_translation\n         title: Transformer XL\n-      - local: model_doc/ul2\n+      - local: in_translation\n         title: UL2\n-      - local: model_doc/umt5\n+      - local: in_translation\n         title: UMT5\n-      - local: model_doc/xmod\n+      - local: in_translation\n         title: X-MOD\n-      - local: model_doc/xglm\n+      - local: in_translation\n         title: XGLM\n-      - local: model_doc/xlm\n+      - local: in_translation\n         title: XLM\n-      - local: model_doc/xlm-prophetnet\n+      - local: in_translation\n         title: XLM-ProphetNet\n-      - local: model_doc/xlm-roberta\n+      - local: in_translation\n         title: XLM-RoBERTa\n-      - local: model_doc/xlm-roberta-xl\n+      - local: in_translation\n         title: XLM-RoBERTa-XL\n-      - local: model_doc/xlm-v\n+      - local: in_translation\n         title: XLM-V\n-      - local: model_doc/xlnet\n+      - local: in_translation\n         title: XLNet\n-      - local: model_doc/yoso\n+      - local: in_translation\n         title: YOSO\n-      - local: model_doc/zamba\n+      - local: in_translation\n         title: Zamba\n-      - local: model_doc/zamba2\n+      - local: in_translation\n         title: Zamba2\n       title: 텍스트 모델\n     - sections:\n-      - local: model_doc/aimv2\n+      - local: in_translation\n         title: Aimv2\n-      - local: model_doc/beit\n+      - local: in_translation\n         title: BEiT\n-      - local: model_doc/bit\n+      - local: in_translation\n         title: BiT\n-      - local: model_doc/conditional_detr\n+      - local: in_translation\n         title: Conditional DETR\n-      - local: model_doc/convnext\n+      - local: in_translation\n         title: ConvNeXT\n-      - local: model_doc/convnextv2\n+      - local: in_translation\n         title: ConvNeXTV2\n-      - local: model_doc/cvt\n+      - local: in_translation\n         title: CvT\n-      - local: model_doc/d_fine\n+      - local: in_translation\n         title: D-FINE\n-      - local: model_doc/dab-detr\n+      - local: in_translation\n         title: DAB-DETR\n-      - local: model_doc/deepseek_v2\n+      - local: in_translation\n         title: DeepSeek-V2\n-      - local: model_doc/deformable_detr\n+      - local: in_translation\n         title: Deformable DETR\n-      - local: model_doc/deit\n+      - local: in_translation\n         title: DeiT\n-      - local: model_doc/depth_anything\n+      - local: in_translation\n         title: Depth Anything\n-      - local: model_doc/depth_anything_v2\n+      - local: in_translation\n         title: Depth Anything V2\n-      - local: model_doc/depth_pro\n+      - local: in_translation\n         title: DepthPro\n-      - local: model_doc/deta\n+      - local: in_translation\n         title: DETA\n-      - local: model_doc/detr\n+      - local: in_translation\n         title: DETR\n-      - local: model_doc/dinat\n+      - local: in_translation\n         title: DiNAT\n-      - local: model_doc/dinov2\n+      - local: in_translation\n         title: DINOV2\n-      - local: model_doc/dinov2_with_registers\n+      - local: in_translation\n         title: DINOv2 with Registers\n-      - local: model_doc/dit\n+      - local: in_translation\n         title: DiT\n-      - local: model_doc/dpt\n+      - local: in_translation\n         title: DPT\n-      - local: model_doc/efficientformer\n+      - local: in_translation\n         title: EfficientFormer\n-      - local: model_doc/efficientnet\n+      - local: in_translation\n         title: EfficientNet\n-      - local: model_doc/eomt\n+      - local: in_translation\n         title: EoMT\n-      - local: model_doc/focalnet\n+      - local: in_translation\n         title: FocalNet\n-      - local: model_doc/glpn\n+      - local: in_translation\n         title: GLPN\n-      - local: model_doc/hiera\n+      - local: in_translation\n         title: Hiera\n-      - local: model_doc/ijepa\n+      - local: in_translation\n         title: I-JEPA\n-      - local: model_doc/imagegpt\n+      - local: in_translation\n         title: ImageGPT\n-      - local: model_doc/levit\n+      - local: in_translation\n         title: LeViT\n-      - local: model_doc/lightglue\n+      - local: in_translation\n         title: LightGlue\n-      - local: model_doc/mask2former\n+      - local: in_translation\n         title: Mask2Former\n-      - local: model_doc/maskformer\n+      - local: in_translation\n         title: MaskFormer\n-      - local: model_doc/mlcd\n+      - local: in_translation\n         title: MLCD\n-      - local: model_doc/mobilenet_v1\n+      - local: in_translation\n         title: MobileNetV1\n-      - local: model_doc/mobilenet_v2\n+      - local: in_translation\n         title: MobileNetV2\n-      - local: model_doc/mobilevit\n+      - local: in_translation\n         title: MobileViT\n-      - local: model_doc/mobilevitv2\n+      - local: in_translation\n         title: MobileViTV2\n-      - local: model_doc/nat\n+      - local: in_translation\n         title: NAT\n-      - local: model_doc/poolformer\n+      - local: in_translation\n         title: PoolFormer\n-      - local: model_doc/prompt_depth_anything\n+      - local: in_translation\n         title: Prompt Depth Anything\n-      - local: model_doc/pvt\n+      - local: in_translation\n         title: Pyramid Vision Transformer (PVT)\n-      - local: model_doc/pvt_v2\n+      - local: in_translation\n         title: Pyramid Vision Transformer v2 (PVTv2)\n-      - local: model_doc/regnet\n+      - local: in_translation\n         title: RegNet\n-      - local: model_doc/resnet\n+      - local: in_translation\n         title: ResNet\n-      - local: model_doc/rt_detr\n+      - local: in_translation\n         title: RT-DETR\n-      - local: model_doc/rt_detr_v2\n+      - local: in_translation\n         title: RT-DETRv2\n-      - local: model_doc/segformer\n+      - local: in_translation\n         title: SegFormer\n-      - local: model_doc/seggpt\n+      - local: in_translation\n         title: SegGpt\n-      - local: model_doc/superglue\n+      - local: in_translation\n         title: SuperGlue\n-      - local: model_doc/superpoint\n+      - local: in_translation\n         title: SuperPoint\n-      - local: model_doc/swiftformer\n+      - local: in_translation\n         title: SwiftFormer\n       - local: model_doc/swin\n         title: Swin Transformer\n       - local: model_doc/swinv2\n         title: Swin Transformer V2\n       - local: model_doc/swin2sr\n         title: Swin2SR\n-      - local: model_doc/table-transformer\n+      - local: in_translation\n         title: Table Transformer\n-      - local: model_doc/textnet\n+      - local: in_translation\n         title: TextNet\n-      - local: model_doc/timm_wrapper\n+      - local: in_translation\n         title: Timm Wrapper\n-      - local: model_doc/upernet\n+      - local: in_translation\n         title: UperNet\n-      - local: model_doc/van\n+      - local: in_translation\n         title: VAN\n       - local: model_doc/vit\n         title: Vision Transformer (ViT)\n-      - local: model_doc/vit_hybrid\n+      - local: in_translation\n         title: ViT Hybrid\n-      - local: model_doc/vitdet\n+      - local: in_translation\n         title: ViTDet\n-      - local: model_doc/vit_mae\n+      - local: in_translation\n         title: ViTMAE\n-      - local: model_doc/vitmatte\n+      - local: in_translation\n         title: ViTMatte\n-      - local: model_doc/vit_msn\n+      - local: in_translation\n         title: ViTMSN\n-      - local: model_doc/vitpose\n+      - local: in_translation\n         title: ViTPose\n-      - local: model_doc/yolos\n+      - local: in_translation\n         title: YOLOS\n-      - local: model_doc/zoedepth\n+      - local: in_translation\n         title: ZoeDepth\n       title: 비전 모델\n     - sections:\n-      - local: model_doc/audio-spectrogram-transformer\n+      - local: in_translation\n         title: Audio Spectrogram Transformer\n-      - local: model_doc/bark\n+      - local: in_translation\n         title: Bark\n-      - local: model_doc/clap\n+      - local: in_translation\n         title: CLAP\n-      - local: model_doc/csm\n+      - local: in_translation\n         title: CSM\n-      - local: model_doc/dac\n+      - local: in_translation\n         title: dac\n-      - local: model_doc/dia\n+      - local: in_translation\n         title: Dia\n-      - local: model_doc/encodec\n+      - local: in_translation\n         title: EnCodec\n-      - local: model_doc/fastspeech2_conformer\n+      - local: in_translation\n         title: FastSpeech2Conformer\n-      - local: model_doc/granite_speech\n+      - local: in_translation\n         title: GraniteSpeech\n-      - local: model_doc/hubert\n+      - local: in_translation\n         title: Hubert\n-      - local: model_doc/kyutai_speech_to_text\n+      - local: in_translation\n         title: Kyutai Speech-To-Text\n-      - local: model_doc/mctct\n+      - local: in_translation\n         title: MCTCT\n-      - local: model_doc/mimi\n+      - local: in_translation\n         title: Mimi\n-      - local: model_doc/mms\n+      - local: in_translation\n         title: MMS\n-      - local: model_doc/moonshine\n+      - local: in_translation\n         title: Moonshine\n-      - local: model_doc/moshi\n+      - local: in_translation\n         title: Moshi\n-      - local: model_doc/musicgen\n+      - local: in_translation\n         title: MusicGen\n-      - local: model_doc/musicgen_melody\n+      - local: in_translation\n         title: MusicGen Melody\n-      - local: model_doc/pop2piano\n+      - local: in_translation\n         title: Pop2Piano\n-      - local: model_doc/seamless_m4t\n+      - local: in_translation\n         title: Seamless-M4T\n-      - local: model_doc/seamless_m4t_v2\n+      - local: in_translation\n         title: SeamlessM4T-v2\n-      - local: model_doc/sew\n+      - local: in_translation\n         title: SEW\n-      - local: model_doc/sew-d\n+      - local: in_translation\n         title: SEW-D\n-      - local: model_doc/speech_to_text\n+      - local: in_translation\n         title: Speech2Text\n-      - local: model_doc/speech_to_text_2\n+      - local: in_translation\n         title: Speech2Text2\n-      - local: model_doc/speecht5\n+      - local: in_translation\n         title: SpeechT5\n-      - local: model_doc/unispeech\n+      - local: in_translation\n         title: UniSpeech\n-      - local: model_doc/unispeech-sat\n+      - local: in_translation\n         title: UniSpeech-SAT\n-      - local: model_doc/univnet\n+      - local: in_translation\n         title: UnivNet\n-      - local: model_doc/vits\n+      - local: in_translation\n         title: VITS\n-      - local: model_doc/wav2vec2\n+      - local: in_translation\n         title: Wav2Vec2\n-      - local: model_doc/wav2vec2-bert\n+      - local: in_translation\n         title: Wav2Vec2-BERT\n-      - local: model_doc/wav2vec2-conformer\n+      - local: in_translation\n         title: Wav2Vec2-Conformer\n-      - local: model_doc/wav2vec2_phoneme\n+      - local: in_translation\n         title: Wav2Vec2Phoneme\n-      - local: model_doc/wavlm\n+      - local: in_translation\n         title: WavLM\n       - local: model_doc/whisper\n         title: Whisper\n-      - local: model_doc/xls_r\n+      - local: in_translation\n         title: XLS-R\n-      - local: model_doc/xlsr_wav2vec2\n+      - local: in_translation\n         title: XLSR-Wav2Vec2\n       title: 오디오 모델\n     - sections:\n       - local: model_doc/timesformer\n         title: TimeSformer\n-      - local: model_doc/vjepa2\n+      - local: in_translation\n         title: V-JEPA 2\n-      - local: model_doc/videomae\n+      - local: in_translation\n         title: VideoMAE\n       - local: model_doc/vivit\n         title: ViViT\n       title: 비디오 모델\n     - sections:\n-      - local: model_doc/align\n+      - local: in_translation\n         title: ALIGN\n       - local: model_doc/altclip\n         title: AltCLIP\n-      - local: model_doc/aria\n+      - local: in_translation\n         title: Aria\n-      - local: model_doc/aya_vision\n+      - local: in_translation\n         title: AyaVision\n       - local: model_doc/blip\n         title: BLIP\n       - local: model_doc/blip-2\n         title: BLIP-2\n-      - local: model_doc/bridgetower\n+      - local: in_translation\n         title: BridgeTower\n-      - local: model_doc/bros\n+      - local: in_translation\n         title: BROS\n       - local: model_doc/chameleon\n         title: Chameleon\n-      - local: model_doc/chinese_clip\n+      - local: in_translation\n         title: Chinese-CLIP\n       - local: model_doc/clip\n         title: CLIP\n-      - local: model_doc/clipseg\n+      - local: in_translation\n         title: CLIPSeg\n-      - local: model_doc/clvp\n+      - local: in_translation\n         title: CLVP\n-      - local: model_doc/colpali\n+      - local: in_translation\n         title: ColPali\n-      - local: model_doc/colqwen2\n+      - local: in_translation\n         title: ColQwen2\n-      - local: model_doc/data2vec\n+      - local: in_translation\n         title: Data2Vec\n-      - local: model_doc/deplot\n+      - local: in_translation\n         title: DePlot\n-      - local: model_doc/donut\n+      - local: in_translation\n         title: Donut\n-      - local: model_doc/emu3\n+      - local: in_translation\n         title: Emu3\n-      - local: model_doc/flava\n+      - local: in_translation\n         title: FLAVA\n-      - local: model_doc/gemma3\n+      - local: in_translation\n         title: Gemma3\n-      - local: model_doc/gemma3n\n+      - local: in_translation\n         title: Gemma3n\n-      - local: model_doc/git\n+      - local: in_translation\n         title: GIT\n-      - local: model_doc/glm4v\n+      - local: in_translation\n         title: glm4v\n-      - local: model_doc/got_ocr2\n+      - local: in_translation\n         title: GOT-OCR2\n-      - local: model_doc/granitevision\n+      - local: in_translation\n         title: GraniteVision\n-      - local: model_doc/grounding-dino\n+      - local: in_translation\n         title: Grounding DINO\n-      - local: model_doc/groupvit\n+      - local: in_translation\n         title: GroupViT\n-      - local: model_doc/idefics\n+      - local: in_translation\n         title: IDEFICS\n-      - local: model_doc/idefics2\n+      - local: in_translation\n         title: Idefics2\n-      - local: model_doc/idefics3\n+      - local: in_translation\n         title: Idefics3\n-      - local: model_doc/instructblip\n+      - local: in_translation\n         title: InstructBLIP\n-      - local: model_doc/instructblipvideo\n+      - local: in_translation\n         title: InstructBlipVideo\n-      - local: model_doc/internvl\n+      - local: in_translation\n         title: InternVL\n-      - local: model_doc/janus\n+      - local: in_translation\n         title: Janus\n-      - local: model_doc/kosmos-2\n+      - local: in_translation\n         title: KOSMOS-2\n-      - local: model_doc/layoutlm\n+      - local: in_translation\n         title: LayoutLM\n-      - local: model_doc/layoutlmv2\n+      - local: in_translation\n         title: LayoutLMV2\n-      - local: model_doc/layoutlmv3\n+      - local: in_translation\n         title: LayoutLMV3\n-      - local: model_doc/layoutxlm\n+      - local: in_translation\n         title: LayoutXLM\n-      - local: model_doc/lilt\n+      - local: in_translation\n         title: LiLT\n-      - local: model_doc/llama4\n+      - local: in_translation\n         title: Llama4\n-      - local: model_doc/llava\n+      - local: in_translation\n         title: Llava\n-      - local: model_doc/llava_next\n+      - local: in_translation\n         title: LLaVA-NeXT\n-      - local: model_doc/llava_next_video\n+      - local: in_translation\n         title: LLaVa-NeXT-Video\n-      - local: model_doc/llava_onevision\n+      - local: in_translation\n         title: LLaVA-Onevision\n-      - local: model_doc/lxmert\n+      - local: in_translation\n         title: LXMERT\n-      - local: model_doc/matcha\n+      - local: in_translation\n         title: MatCha\n-      - local: model_doc/mgp-str\n+      - local: in_translation\n         title: MGP-STR\n-      - local: model_doc/mistral3\n+      - local: in_translation\n         title: Mistral3\n-      - local: model_doc/mllama\n+      - local: in_translation\n         title: mllama\n-      - local: model_doc/nougat\n+      - local: in_translation\n         title: Nougat\n-      - local: model_doc/omdet-turbo\n+      - local: in_translation\n         title: OmDet-Turbo\n-      - local: model_doc/oneformer\n+      - local: in_translation\n         title: OneFormer\n-      - local: model_doc/owlvit\n+      - local: in_translation\n         title: OWL-ViT\n-      - local: model_doc/owlv2\n+      - local: in_translation\n         title: OWLv2\n       - local: model_doc/paligemma\n         title: PaliGemma\n-      - local: model_doc/perceiver\n+      - local: in_translation\n         title: Perceiver\n-      - local: model_doc/perception_lm\n+      - local: in_translation\n         title: PerceptionLM\n-      - local: model_doc/phi4_multimodal\n+      - local: in_translation\n         title: Phi4 Multimodal\n-      - local: model_doc/pix2struct\n+      - local: in_translation\n         title: Pix2Struct\n-      - local: model_doc/pixtral\n+      - local: in_translation\n         title: Pixtral\n-      - local: model_doc/qwen2_5_omni\n+      - local: in_translation\n         title: Qwen2.5-Omni\n-      - local: model_doc/qwen2_5_vl\n+      - local: in_translation\n         title: Qwen2.5-VL\n-      - local: model_doc/qwen2_audio\n+      - local: in_translation\n         title: Qwen2Audio\n       - local: model_doc/qwen2_vl\n         title: Qwen2VL\n-      - local: model_doc/sam\n+      - local: in_translation\n         title: Segment Anything\n-      - local: model_doc/sam_hq\n+      - local: in_translation\n         title: Segment Anything High Quality\n-      - local: model_doc/shieldgemma2\n+      - local: in_translation\n         title: ShieldGemma2\n       - local: model_doc/siglip\n         title: SigLIP\n-      - local: model_doc/siglip2\n+      - local: in_translation\n         title: SigLIP2\n-      - local: model_doc/smollm3\n+      - local: in_translation\n         title: SmolLM3\n-      - local: model_doc/smolvlm\n+      - local: in_translation\n         title: SmolVLM\n-      - local: model_doc/speech-encoder-decoder\n+      - local: in_translation\n         title: Speech Encoder Decoder Models\n-      - local: model_doc/tapas\n+      - local: in_translation\n         title: TAPAS\n-      - local: model_doc/trocr\n+      - local: in_translation\n         title: TrOCR\n-      - local: model_doc/tvlt\n+      - local: in_translation\n         title: TVLT\n-      - local: model_doc/tvp\n+      - local: in_translation\n         title: TVP\n-      - local: model_doc/udop\n+      - local: in_translation\n         title: UDOP\n-      - local: model_doc/video_llava\n+      - local: in_translation\n         title: VideoLlava\n-      - local: model_doc/vilt\n+      - local: in_translation\n         title: ViLT\n-      - local: model_doc/vipllava\n+      - local: in_translation\n         title: VipLlava\n-      - local: model_doc/vision-encoder-decoder\n+      - local: in_translation\n         title: Vision Encoder Decoder Models\n-      - local: model_doc/vision-text-dual-encoder\n+      - local: in_translation\n         title: Vision Text Dual Encoder\n-      - local: model_doc/visual_bert\n+      - local: in_translation\n         title: VisualBERT\n-      - local: model_doc/voxtral\n+      - local: in_translation\n         title: Voxtral\n-      - local: model_doc/xclip\n+      - local: in_translation\n         title: X-CLIP\n       title: 멀티모달 모델\n     - sections:\n-      - local: model_doc/decision_transformer\n+      - local: in_translation\n         title: Decision Transformer\n       - local: model_doc/trajectory_transformer\n         title: Trajectory Transformer\n@@ -1119,7 +1119,7 @@\n         title: PatchTST\n       - local: model_doc/time_series_transformer\n         title: Time Series Transformer\n-      - local: model_doc/timesfm\n+      - local: in_translation\n         title: TimesFM\n       title: 시게열 모델\n     - sections:"
        },
        {
            "sha": "1e913ba34576d8479e34bbce47d5f01a4d5eb01e",
            "filename": "docs/source/ko/accelerator_selection.md",
            "status": "renamed",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/33aa49df9d33539e764af7f54ef4acac1980f1d0/docs%2Fsource%2Fko%2Faccelerator_selection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/33aa49df9d33539e764af7f54ef4acac1980f1d0/docs%2Fsource%2Fko%2Faccelerator_selection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Faccelerator_selection.md?ref=33aa49df9d33539e764af7f54ef4acac1980f1d0",
            "patch": "@@ -14,6 +14,8 @@ rendered properly in your Markdown viewer.\n \n -->\n \n+<!-- TODO: this was copied from the korean version of `gpu_selection.md`, and is not up to date with the english version of `accelerator_selection.md` -->\n+\n # GPU 선택하기 [[gpu-selection]]\n \n 분산 학습 과정에서 사용할 GPU의 개수와 순서를 정할 수 있습니다. 이 방법은 서로 다른 연산 성능을 가진 GPU가 있을 때 더 빠른 GPU를 우선적으로 사용하거나, 사용 가능한 GPU 중 일부만 선택하여 활용하고자 할 때 유용합니다. 이 선택 과정은 [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)과 [DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)에서 모두 작동합니다. Accelerate나 [DeepSpeed 통합](./main_classes/deepspeed)은 필요하지 않습니다.\n@@ -93,4 +95,4 @@ export CUDA_DEVICE_ORDER=FASTEST_FIRST\n \n The `CUDA_DEVICE_ORDER` is especially useful if your training setup consists of an older and newer GPU, where the older GPU appears first, but you cannot physically swap the cards to make the newer GPU appear first. In this case, set `CUDA_DEVICE_ORDER=FASTEST_FIRST` to always use the newer and faster GPU first (`nvidia-smi` or `rocm-smi` still reports the GPUs in their PCIe order). Or you could also set `export CUDA_VISIBLE_DEVICES=1,0`.\n \n-`CUDA_DEVICE_ORDER`는 구형 GPU와 신형 GPU가 혼합된 환경에서 특히 유용합니다. 예를 들어, 구형 GPU가 먼저 표시되지만 물리적으로 교체할 수 없는 경우, `CUDA_DEVICE_ORDER=FASTEST_FIRST`를 설정하면 항상 신형 및 더 빠른 GPU를 우선적으로 사용(nvidia-smi 또는 rocm-smi는 PCIe 순서대로 GPU를 표시함)할 수 있습니다. 또는, `export CUDA_VISIBLE_DEVICES=1,0`을 설정하여 GPU 사용 순서를 직접 지정할 수도 있습니다.\n\\ No newline at end of file\n+`CUDA_DEVICE_ORDER`는 구형 GPU와 신형 GPU가 혼합된 환경에서 특히 유용합니다. 예를 들어, 구형 GPU가 먼저 표시되지만 물리적으로 교체할 수 없는 경우, `CUDA_DEVICE_ORDER=FASTEST_FIRST`를 설정하면 항상 신형 및 더 빠른 GPU를 우선적으로 사용(nvidia-smi 또는 rocm-smi는 PCIe 순서대로 GPU를 표시함)할 수 있습니다. 또는, `export CUDA_VISIBLE_DEVICES=1,0`을 설정하여 GPU 사용 순서를 직접 지정할 수도 있습니다.",
            "previous_filename": "docs/source/ko/gpu_selection.md"
        },
        {
            "sha": "d9a5eeee1b73eada3d0d704b2ff6744f545c6d2b",
            "filename": "docs/source/ko/attention.md",
            "status": "removed",
            "additions": 0,
            "deletions": 54,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fattention.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fattention.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fattention.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,54 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# 어텐션 메커니즘[[attention_mechanisms]]\n-\n-대부분의 트랜스포머 모델은 정방행렬인 전체 어텐션을 사용합니다. \n-하지만 이는 긴 텍스트를 다룰 때는 큰 계산 병목 현상을 유발할 수 있습니다. \n-`Longformer`와 `Reformer`는 훈련 속도를 높이기 위해 어텐션 행렬의 희소 버전을 사용하여 효율을 높이려는 모델입니다.\n-\n-## LSH 어텐션[[lsh_attention]]\n-\n-\n-[Reformer](model_doc/reformer)는 LSH(Locality Sensitive Hashing) 어텐션을 사용합니다. softmax(QK^t)에서는 행렬 QK^t의 (softmax 차원에서) 가장 큰 요소들만 유용한 기여를 할 것입니다. \n-따라서 각각의 쿼리 q에 대해, q와 가까운 키 k만 고려할 수 있습니다. 해시 함수는 q와 k가 가까운지 여부를 결정하는 데 사용됩니다. \n-어텐션 마스크는 현재 토큰을 마스킹하여 변경됩니다. 이 때 첫 번째 위치의 토큰은 제외합니다. 왜냐하면 쿼리와 키가 동일한 값을 갖게 되기 때문입니다(서로 매우 유사함). \n-해시는 약간의 무작위성을 가질 수 있으므로, 실제로는 여러 개의 해시 함수가 사용되고 (`n_rounds` 매개변수에 의해 결정됨) 그 후에 평균값을 취하게 됩니다.\n-\n-## 지역 어텐션[[local_attention]]\n-\n-[Longformer](model_doc/longformer)는 지역 어텐션을 사용합니다. 종종 특정 토큰에 대해 지역 컨텍스트(예: 왼쪽과 오른쪽에 있는 두 개의 토큰은 무엇인가요?)만으로도 작업을 수행하는데 충분합니다. \n-또한 작은 창(window)을 가진 어텐션 레이어를 쌓음으로써 마지막 레이어는 창 내의 토큰뿐만 아니라 더 많은 수의 토큰에 대한 수용 영역(receptive field)을 갖게 되어 전체 문장의 표현을 구축할 수 있습니다.\n-\n-사전에 선택된 일부 입력 토큰들은 전역 어텐션을 받습니다. 이 몇 개의 토큰에 대해서는 어텐션 행렬이 모든 토큰에 접근할 수 있으며, 이 과정은 대칭적으로 이루어집니다. \n-다른 모든 토큰들은 로컬 창 내의 토큰들에 더해 해당 특정 토큰들에도 접근할 수 있습니다. 이는 논문의 Figure 2d에서 나타나며, 아래에 샘플 어텐션 마스크가 제시되어 있습니다:\n-\n-\n-<div class=\"flex justify-center\">\n-    <img scale=\"50 %\" align=\"center\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png\"/>\n-</div>\n-\n-\n-적은 파라미터의 어텐션 행렬을 사용하면 모델이 더 큰 시퀀스 입력 길이를 가질 수 있습니다.\n-\n-## 다른 방법들[[other_tricks]]\n-\n-### 축별 위치 인코딩[[axial_positional_encodings]]\n-\n-[Reformer](model_doc/reformer)는 축별 위치 인코딩(axial positional encodings)을 사용합니다. 기존의 트랜스포머 모델에서는 위치 인코딩 행렬 E는 크기가 \\\\(l \\times d\\\\)인 행렬이며, \n-여기서 \\\\(l\\\\)은 시퀀스 길이(sequence length)이고 \\\\(d\\\\)는 숨겨진 상태(hidden state)의 차원입니다. 매우 긴 텍스트의 경우, 이 행렬은 매우 크며 GPU 상에서 공간을 많이 차지할 수 있습니다. \n-이를 완화하기 위해, 축별 위치 인코딩은 큰 행렬 E를 두 개의 작은 행렬 E1과 E2로 분해합니다. 이때 E1의 크기는 \\\\(l_{1} \\times d_{1}\\\\)이고, E2의 크기는 \\\\(l_{2} \\times d_{2}\\\\)입니다. \n-이때 \\\\(l_{1} \\times l_{2} = l\\\\)이고 \\\\(d_{1} + d_{2} = d\\\\)(길이에 대한 곱셈 연산을 사용하면 훨씬 작아집니다). E의 시간 단계 j에 대한 임베딩은 E1에서 시간 단계 \\\\(j \\% l1\\\\)의 임베딩과 E2에서 시간 단계  \\\\(j // l1\\\\)의 임베딩을 연결하여 얻습니다.\n\\ No newline at end of file"
        },
        {
            "sha": "e41a2acc7b486b44590387fd2aa495b78b530f75",
            "filename": "docs/source/ko/autoclass_tutorial.md",
            "status": "removed",
            "additions": 0,
            "deletions": 144,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fautoclass_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fautoclass_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fautoclass_tutorial.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,144 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# AutoClass로 사전 학습된 인스턴스 로드[[load-pretrained-instances-with-an-autoclass]]\n-\n-트랜스포머 아키텍처가 매우 다양하기 때문에 체크포인트에 맞는 아키텍처를 생성하는 것이 어려울 수 있습니다. 라이브러리를 쉽고 간단하며 유연하게 사용하기 위한 Transformer 핵심 철학의 일환으로, `AutoClass`는 주어진 체크포인트에서 올바른 아키텍처를 자동으로 추론하여 로드합니다. `from_pretrained()` 메서드를 사용하면 모든 아키텍처에 대해 사전 학습된 모델을 빠르게 로드할 수 있으므로 모델을 처음부터 학습하는 데 시간과 리소스를 투입할 필요가 없습니다. \n-체크포인트에 구애받지 않는 코드를 생성한다는 것은 코드가 한 체크포인트에서 작동하면 아키텍처가 다르더라도 다른 체크포인트(유사한 작업에 대해 학습된 경우)에서도 작동한다는 것을 의미합니다.\n-\n-<Tip>\n-\n-아키텍처는 모델의 골격을 의미하며 체크포인트는 주어진 아키텍처에 대한 가중치입니다. 예를 들어, [BERT](https://huggingface.co/google-bert/bert-base-uncased)는 아키텍처이고, `google-bert/bert-base-uncased`는 체크포인트입니다. 모델은 아키텍처 또는 체크포인트를 의미할 수 있는 일반적인 용어입니다.\n-\n-</Tip>\n-\n-이 튜토리얼에서는 다음을 학습합니다:\n-\n-* 사전 학습된 토크나이저 로드하기.\n-* 사전 학습된 이미지 프로세서 로드하기.\n-* 사전 학습된 특징 추출기 로드하기.\n-* 사전 훈련된 프로세서 로드하기.\n-* 사전 학습된 모델 로드하기.\n-\n-## AutoTokenizer[[autotokenizer]]\n-\n-거의 모든 NLP 작업은 토크나이저로 시작됩니다. 토크나이저는 사용자의 입력을 모델에서 처리할 수 있는 형식으로 변환합니다.\n-[`AutoTokenizer.from_pretrained`]로 토크나이저를 로드합니다:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-```\n-\n-그리고 아래와 같이 입력을 토큰화합니다:\n-\n-```py\n->>> sequence = \"In a hole in the ground there lived a hobbit.\"\n->>> print(tokenizer(sequence))\n-{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], \n- 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n- 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n-```\n-\n-## AutoImageProcessor[[autoimageprocessor]]\n-\n-비전 작업의 경우 이미지 프로세서가 이미지를 올바른 입력 형식으로 처리합니다.\n-\n-```py\n->>> from transformers import AutoImageProcessor\n-\n->>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n-```\n-\n-\n-## AutoFeatureExtractor[[autofeatureextractor]]\n-\n-오디오 작업의 경우 특징 추출기가 오디오 신호를 올바른 입력 형식으로 처리합니다.\n-\n-[`AutoFeatureExtractor.from_pretrained`]로 특징 추출기를 로드합니다:\n-\n-```py\n->>> from transformers import AutoFeatureExtractor\n-\n->>> feature_extractor = AutoFeatureExtractor.from_pretrained(\n-...     \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n-... )\n-```\n-\n-## AutoProcessor[[autoprocessor]]\n-\n-멀티모달 작업에는 두 가지 유형의 전처리 도구를 결합한 프로세서가 필요합니다. 예를 들어 LayoutLMV2 모델에는 이미지를 처리하는 이미지 프로세서와 텍스트를 처리하는 토크나이저가 필요하며, 프로세서는 이 두 가지를 결합합니다.\n-\n-[`AutoProcessor.from_pretrained()`]로 프로세서를 로드합니다:\n-\n-```py\n->>> from transformers import AutoProcessor\n-\n->>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n-```\n-\n-## AutoModel[[automodel]]\n-\n-<frameworkcontent>\n-<pt>\n-마지막으로 AutoModelFor클래스를 사용하면 주어진 작업에 대해 미리 학습된 모델을 로드할 수 있습니다 (사용 가능한 작업의 전체 목록은 [여기](model_doc/auto)를 참조하세요). 예를 들어, [`AutoModelForSequenceClassification.from_pretrained`]를 사용하여 시퀀스 분류용 모델을 로드할 수 있습니다:\n-\n-```py\n->>> from transformers import AutoModelForSequenceClassification\n-\n->>> model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-동일한 체크포인트를 쉽게 재사용하여 다른 작업에 아키텍처를 로드할 수 있습니다:\n-\n-```py\n->>> from transformers import AutoModelForTokenClassification\n-\n->>> model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-<Tip warning={true}>\n-\n-PyTorch모델의 경우 `from_pretrained()` 메서드는 내부적으로 피클을 사용하여 안전하지 않은 것으로 알려진 `torch.load()`를 사용합니다. \n-일반적으로 신뢰할 수 없는 소스에서 가져왔거나 변조되었을 수 있는 모델은 로드하지 마세요. 허깅 페이스 허브에서 호스팅되는 공개 모델의 경우 이러한 보안 위험이 부분적으로 완화되며, 각 커밋 시 멀웨어를 [검사합니다](https://huggingface.co/docs/hub/security-malware). GPG를 사용해 서명된 [커밋 검증](https://huggingface.co/docs/hub/security-gpg#signing-commits-with-gpg)과 같은 모범사례는 [문서](https://huggingface.co/docs/hub/security)를 참조하세요.\n-\n-텐서플로우와 Flax 체크포인트는 영향을 받지 않으며, `from_pretrained`메서드에 `from_tf` 와 `from_flax` 키워드 가변 인자를 사용하여 이 문제를 우회할 수 있습니다.\n-\n-</Tip>\n-\n-일반적으로 AutoTokenizer 클래스와 AutoModelFor 클래스를 사용하여 미리 학습된 모델 인스턴스를 로드하는 것이 좋습니다. 이렇게 하면 매번 올바른 아키텍처를 로드할 수 있습니다. 다음 [튜토리얼](preprocessing)에서는 새롭게 로드한 토크나이저, 이미지 프로세서, 특징 추출기를 사용하여 미세 튜닝용 데이터 세트를 전처리하는 방법에 대해 알아봅니다.\n-</pt>\n-<tf>\n-마지막으로 `TFAutoModelFor` 클래스를 사용하면 주어진 작업에 대해 사전 훈련된 모델을 로드할 수 있습니다. (사용 가능한 작업의 전체 목록은 [여기](model_doc/auto)를 참조하세요. 예를 들어, [`TFAutoModelForSequenceClassification.from_pretrained`]로 시퀀스 분류를 위한 모델을 로드합니다:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-쉽게 동일한 체크포인트를 재사용하여 다른 작업에 아키텍처를 로드할 수 있습니다:\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-일반적으로, `AutoTokenizer`클래스와 `TFAutoModelFor` 클래스를 사용하여 미리 학습된 모델 인스턴스를 로드하는 것이 좋습니다. 이렇게 하면 매번 올바른 아키텍처를 로드할 수 있습니다. 다음 [튜토리얼](preprocessing)에서는 새롭게 로드한 토크나이저, 이미지 프로세서, 특징 추출기를 사용하여 미세 튜닝용 데이터 세트를 전처리하는 방법에 대해 알아봅니다.\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "37e66402d6118df6b28cc66b5df3430434187e70",
            "filename": "docs/source/ko/bertology.md",
            "status": "removed",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fbertology.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fbertology.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fbertology.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,41 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# BERTology\n-\n-BERT와 같은 대규모 트랜스포머의 내부 동작을 조사하는 연구 분야가 점점 더 중요해지고 있습니다.\n-혹자는 \"BERTology\"라 칭하기도 합니다. 이 분야의 좋은 예시는 다음과 같습니다:\n-\n-\n-- BERT는 고전적인 NLP 파이프라인의 재발견 - Ian Tenney, Dipanjan Das, Ellie Pavlick:\n-  https://huggingface.co/papers/1905.05950\n-- 16개의 헤드가 정말로 1개보다 나은가? - Paul Michel, Omer Levy, Graham Neubig:\n-  https://huggingface.co/papers/1905.10650\n-- BERT는 무엇을 보는가? BERT의 어텐션 분석 - Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning:\n-  https://huggingface.co/papers/1906.04341\n-- CAT-probing: 프로그래밍 언어에 대해 사전훈련된 모델이 어떻게 코드 구조를 보는지 알아보기 위한 메트릭 기반 접근 방법:\n-  https://huggingface.co/papers/2210.04633\n-\n-우리는 이 새로운 연구 분야의 발전을 돕기 위해, BERT/GPT/GPT-2 모델에 내부 표현을 살펴볼 수 있는 몇 가지 기능을 추가했습니다.\n-이 기능들은 주로 Paul Michel의 훌륭한 작업을 참고하여 개발되었습니다\n-(https://huggingface.co/papers/1905.10650):\n-\n-\n-- BERT/GPT/GPT-2의 모든 은닉 상태에 접근하기,\n-- BERT/GPT/GPT-2의 각 헤드의 모든 어텐션 가중치에 접근하기,\n-- 헤드의 출력 값과 그래디언트를 검색하여 헤드 중요도 점수를 계산하고 https://huggingface.co/papers/1905.10650에서 설명된 대로 헤드를 제거하는 기능을 제공합니다.\n-\n-이러한 기능들을 이해하고 직접 사용해볼 수 있도록 [bertology.py](https://github.com/huggingface/transformers-research-projects/tree/main/bertology/run_bertology.py) 예제 스크립트를 추가했습니다. 이 예제 스크립트에서는 GLUE에 대해 사전훈련된 모델에서 정보를 추출하고 모델을 가지치기(prune)해봅니다."
        },
        {
            "sha": "3180b51117a97b93566775485455e157ee7bd6ff",
            "filename": "docs/source/ko/big_models.md",
            "status": "removed",
            "additions": 0,
            "deletions": 122,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fbig_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fbig_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fbig_models.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,122 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# 큰 모델 인스턴스화 [[instantiating-a-big-model]]\n-\n-매우 큰 사전훈련된 모델을 사용하려면, RAM 사용을 최소화해야 하는 과제가 있습니다. 일반적인 PyTorch 워크플로우는 다음과 같습니다:\n-\n-1. 무작위 가중치로 모델을 생성합니다.\n-2. 사전훈련된 가중치를 불러옵니다.\n-3. 사전훈련된 가중치를 무작위 모델에 적용합니다.\n-\n-1단계와 2단계 모두 모델의 전체 버전을 메모리에 적재해야 하며, 대부분 문제가 없지만 모델이 기가바이트급의 용량을 차지하기 시작하면 복사본 2개가 RAM을 초과하여 메모리 부족 이슈를 야기할 수 있습니다. 더 심각한 문제는 분산 학습을 위해 `torch.distributed`를 사용하는 경우, 프로세스마다 사전훈련된 모델을 로드하고 복사본을 2개씩 RAM에 저장한다는 것입니다.\n-\n-<Tip>\n-\n-무작위로 생성된 모델은 \"비어 있는\" (즉 그때 메모리에 있던 것으로 이뤄진) 텐서로 초기화되며 메모리 공간을 차지합니다. 초기화된 모델/파라미터의 종류에 적합한 분포(예: 정규 분포)에 따른 무작위 초기화는 가능한 한 빠르게 하기 위해 초기화되지 않은 가중치에 대해 3단계 이후에만 수행됩니다!\n-\n-</Tip>\n-\n-이 안내서에서는 Transformers가 이 문제를 해결하기 위해 제공하는 솔루션을 살펴봅니다. 주의할 점은 아직 활발히 개발 중인 분야이므로 여기서 설명하는 API가 앞으로 약간 변경될 수 있다는 것입니다.\n-\n-## 샤딩된 체크포인트 [[sharded-checkpoints]]\n-\n-4.18.0 버전 이후, 10GB 이상의 공간을 차지하는 모델 체크포인트는 자동으로 작은 조각들로 샤딩됩니다. `model.save_pretrained(save_dir)`를 실행할 때 하나의 단일 체크포인트를 가지게 될 대신, 여러 부분 체크포인트(각각의 크기는 10GB 미만)와 매개변수 이름을 해당 파일에 매핑하는 인덱스가 생성됩니다.\n-\n-`max_shard_size` 매개변수로 샤딩 전 최대 크기를 제어할 수 있으므로, 이 예제를 위해 샤드 크기가 작은 일반 크기의 모델을 사용하겠습니다: 전통적인 BERT 모델을 사용해 봅시다.\n-\n-```py\n-from transformers import AutoModel\n-\n-model = AutoModel.from_pretrained(\"google-bert/bert-base-cased\")\n-```\n-\n-[`~PreTrainedModel.save_pretrained`]을 사용하여 모델을 저장하면, 모델의 구성과 가중치가 들어있는 두 개의 파일이 있는 새 폴더가 생성됩니다:\n-\n-```py\n->>> import os\n->>> import tempfile\n-\n->>> with tempfile.TemporaryDirectory() as tmp_dir:\n-...     model.save_pretrained(tmp_dir)\n-...     print(sorted(os.listdir(tmp_dir)))\n-['config.json', 'pytorch_model.bin']\n-```\n-\n-이제 최대 샤드 크기를 200MB로 사용해 봅시다:\n-\n-```py\n->>> with tempfile.TemporaryDirectory() as tmp_dir:\n-...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n-...     print(sorted(os.listdir(tmp_dir)))\n-['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n-```\n-\n-모델의 구성에 더해, 세 개의 다른 가중치 파일과 파라미터 이름과 해당 파일의 매핑이 포함된 `index.json` 파일을 볼 수 있습니다. 이러한 체크포인트는 [`~PreTrainedModel.from_pretrained`] 메서드를 사용하여 완전히 다시 로드할 수 있습니다:\n-\n-```py\n->>> with tempfile.TemporaryDirectory() as tmp_dir:\n-...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n-...     new_model = AutoModel.from_pretrained(tmp_dir)\n-```\n-\n-큰 모델의 경우 이러한 방식으로 처리하는 주된 장점은 위에서 보여준 흐름의 2단계에서, 각 샤드가 이전 샤드 다음에 로드되므로 메모리 사용량이 모델 크기와 가장 큰 샤드의 크기를 초과하지 않는다는 점입니다.\n-\n-이 인덱스 파일은 키가 체크포인트에 있는지, 그리고 해당 가중치가 어디에 저장되어 있는지를 결정하는 데 사용됩니다. 이 인덱스를 json과 같이 로드하고 딕셔너리를 얻을 수 있습니다:\n-\n-```py\n->>> import json\n-\n->>> with tempfile.TemporaryDirectory() as tmp_dir:\n-...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n-...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n-...         index = json.load(f)\n-\n->>> print(index.keys())\n-dict_keys(['metadata', 'weight_map'])\n-```\n-\n-메타데이터는 현재 모델의 총 크기만 포함됩니다. 앞으로 다른 정보를 추가할 계획입니다:\n-\n-```py\n->>> index[\"metadata\"]\n-{'total_size': 433245184}\n-```\n-\n-가중치 맵은 이 인덱스의 주요 부분으로, 각 매개변수 이름(PyTorch 모델 `state_dict`에서 보통 찾을 수 있는)을 해당 파일에 매핑합니다:\n-\n-```py\n->>> index[\"weight_map\"]\n-{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n- 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n- ...\n-```\n-\n-만약 [`~PreTrainedModel.from_pretrained`]를 사용하지 않고 모델 내에서 이러한 샤딩된 체크포인트를 직접 가져오려면 (전체 체크포인트를 위해 `model.load_state_dict()`를 수행하는 것처럼), [`~modeling_utils.load_sharded_checkpoint`]를 사용해야 합니다.\n-\n-```py\n->>> from transformers.modeling_utils import load_sharded_checkpoint\n-\n->>> with tempfile.TemporaryDirectory() as tmp_dir:\n-...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n-...     load_sharded_checkpoint(model, tmp_dir)\n-```\n-\n-## 저(低)메모리 로딩 [[low-memory-loading]]\n-\n-샤딩된 체크포인트는 위에서 언급한 작업 흐름의 2단계에서 메모리 사용량을 줄이지만, 저(低)메모리 설정에서 모델을 사용하기 위해 우리의 Accelerate 라이브러리를 기반으로 한 도구를 활용하는 것이 좋습니다.\n-\n-자세한 사항은 다음 가이드를 참조해주세요: [Accelerate로 대규모 모델 가져오기 (영문)](../en/main_classes/model#large-model-loading)\n\\ No newline at end of file"
        },
        {
            "sha": "b911669bb174b96ac8b43c9c9490bc16b75f3a7e",
            "filename": "docs/source/ko/create_a_model.md",
            "status": "removed",
            "additions": 0,
            "deletions": 388,
            "changes": 388,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fcreate_a_model.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,388 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# 맞춤형 아키텍처 만들기[[create-a-custom-architecture]]\n-\n-[`AutoClass`](model_doc/auto)는 모델 아키텍처를 자동으로 추론하고 미리 학습된 configuration과 가중치를 다운로드합니다. 일반적으로 체크포인트에 구애받지 않는 코드를 생성하려면 `AutoClass`를 사용하는 것이 좋습니다. 하지만 특정 모델 파라미터를 보다 세밀하게 제어하고자 하는 사용자는 몇 가지 기본 클래스만으로 커스텀 🤗 Transformers 모델을 생성할 수 있습니다. 이는 🤗 Transformers 모델을 연구, 교육 또는 실험하는 데 관심이 있는 모든 사용자에게 특히 유용할 수 있습니다. 이 가이드에서는 'AutoClass'를 사용하지 않고 커스텀 모델을 만드는 방법에 대해 알아보겠습니다:\n-\n-- 모델 configuration을 가져오고 사용자 지정합니다.\n-- 모델 아키텍처를 생성합니다.\n-- 텍스트에 사용할 느리거나 빠른 토큰화기를 만듭니다.\n-- 비전 작업을 위한 이미지 프로세서를 생성합니다.\n-- 오디오 작업을 위한 특성 추출기를 생성합니다.\n-- 멀티모달 작업용 프로세서를 생성합니다.\n-\n-## Configuration[[configuration]]\n-\n-[configuration](main_classes/configuration)은 모델의 특정 속성을 나타냅니다. 각 모델 구성에는 서로 다른 속성이 있습니다. 예를 들어, 모든 NLP 모델에는 `hidden_size`, `num_attention_heads`, `num_hidden_layers` 및 `vocab_size` 속성이 공통으로 있습니다. 이러한 속성은 모델을 구성할 attention heads 또는 hidden layers의 수를 지정합니다.\n-\n-[DistilBERT](model_doc/distilbert) 속성을 검사하기 위해 [`DistilBertConfig`]에 접근하여 자세히 살펴봅니다:\n-\n-```py\n->>> from transformers import DistilBertConfig\n-\n->>> config = DistilBertConfig()\n->>> print(config)\n-DistilBertConfig {\n-  \"activation\": \"gelu\",\n-  \"attention_dropout\": 0.1,\n-  \"dim\": 768,\n-  \"dropout\": 0.1,\n-  \"hidden_dim\": 3072,\n-  \"initializer_range\": 0.02,\n-  \"max_position_embeddings\": 512,\n-  \"model_type\": \"distilbert\",\n-  \"n_heads\": 12,\n-  \"n_layers\": 6,\n-  \"pad_token_id\": 0,\n-  \"qa_dropout\": 0.1,\n-  \"seq_classif_dropout\": 0.2,\n-  \"sinusoidal_pos_embds\": false,\n-  \"transformers_version\": \"4.16.2\",\n-  \"vocab_size\": 30522\n-}\n-```\n-\n-[`DistilBertConfig`]는 기본 [`DistilBertModel`]을 빌드하는 데 사용되는 모든 기본 속성을 표시합니다. 모든 속성은 커스터마이징이 가능하므로 실험을 위한 공간을 만들 수 있습니다. 예를 들어 기본 모델을 다음과 같이 커스터마이즈할 수 있습니다:\n-\n-- `activation` 파라미터로 다른 활성화 함수를 사용해 보세요.\n-- `attention_dropout` 파라미터를 사용하여 어텐션 확률에 더 높은 드롭아웃 비율을 사용하세요.\n-\n-```py\n->>> my_config = DistilBertConfig(activation=\"relu\", attention_dropout=0.4)\n->>> print(my_config)\n-DistilBertConfig {\n-  \"activation\": \"relu\",\n-  \"attention_dropout\": 0.4,\n-  \"dim\": 768,\n-  \"dropout\": 0.1,\n-  \"hidden_dim\": 3072,\n-  \"initializer_range\": 0.02,\n-  \"max_position_embeddings\": 512,\n-  \"model_type\": \"distilbert\",\n-  \"n_heads\": 12,\n-  \"n_layers\": 6,\n-  \"pad_token_id\": 0,\n-  \"qa_dropout\": 0.1,\n-  \"seq_classif_dropout\": 0.2,\n-  \"sinusoidal_pos_embds\": false,\n-  \"transformers_version\": \"4.16.2\",\n-  \"vocab_size\": 30522\n-}\n-```\n-\n-사전 학습된 모델 속성은 [`~PretrainedConfig.from_pretrained`] 함수에서 수정할 수 있습니다:\n-\n-```py\n->>> my_config = DistilBertConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", activation=\"relu\", attention_dropout=0.4)\n-```\n-\n-모델 구성이 만족스러우면 [`~PretrainedConfig.save_pretrained`]로 저장할 수 있습니다. 설정 파일은 지정된 작업 경로에 JSON 파일로 저장됩니다:\n-\n-```py\n->>> my_config.save_pretrained(save_directory=\"./your_model_save_path\")\n-```\n-\n-configuration 파일을 재사용하려면 [`~PretrainedConfig.from_pretrained`]를 사용하여 가져오세요:\n-\n-```py\n->>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/config.json\")\n-```\n-\n-<Tip>\n-\n-configuration 파일을 딕셔너리로 저장하거나 사용자 정의 configuration 속성과 기본 configuration 속성의 차이점만 저장할 수도 있습니다! 자세한 내용은 [configuration](main_classes/configuration) 문서를 참조하세요.\n-\n-</Tip>\n-\n-## 모델[[model]]\n-\n-다음 단계는 [모델(model)](main_classes/models)을 만드는 것입니다. 느슨하게 아키텍처라고도 불리는 모델은 각 계층이 수행하는 동작과 발생하는 작업을 정의합니다. configuration의 `num_hidden_layers`와 같은 속성은 아키텍처를 정의하는 데 사용됩니다. 모든 모델은 기본 클래스 [`PreTrainedModel`]과 입력 임베딩 크기 조정 및 셀프 어텐션 헤드 가지 치기와 같은 몇 가지 일반적인 메소드를 공유합니다. 또한 모든 모델은 [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html), [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) 또는 [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)의 서브클래스이기도 합니다. 즉, 모델은 각 프레임워크의 사용법과 호환됩니다.\n-\n-<frameworkcontent>\n-<pt>\n-사용자 지정 configuration 속성을 모델에 가져옵니다:\n-\n-```py\n->>> from transformers import DistilBertModel\n-\n->>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/config.json\")\n->>> model = DistilBertModel(my_config)\n-```\n-\n-이제 사전 학습된 가중치 대신 임의의 값을 가진 모델이 생성됩니다. 이 모델을 훈련하기 전까지는 유용하게 사용할 수 없습니다. 훈련은 비용과 시간이 많이 소요되는 프로세스입니다. 일반적으로 훈련에 필요한 리소스의 일부만 사용하면서 더 나은 결과를 더 빨리 얻으려면 사전 훈련된 모델을 사용하는 것이 좋습니다.\n-\n-사전 학습된 모델을 [`~PreTrainedModel.from_pretrained`]로 생성합니다:\n-\n-```py\n->>> model = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-🤗 Transformers에서 제공한 모델의 사전 학습된 가중치를 사용하는 경우 기본 모델 configuration을 자동으로 불러옵니다. 그러나 원하는 경우 기본 모델 configuration 속성의 일부 또는 전부를 사용자 지정으로 바꿀 수 있습니다:\n-\n-```py\n->>> model = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n-```\n-</pt>\n-<tf>\n-사용자 지정 configuration 속성을 모델에 불러옵니다:\n-\n-```py\n->>> from transformers import TFDistilBertModel\n-\n->>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/my_config.json\")\n->>> tf_model = TFDistilBertModel(my_config)\n-```\n-\n-이제 사전 학습된 가중치 대신 임의의 값을 가진 모델이 생성됩니다. 이 모델을 훈련하기 전까지는 유용하게 사용할 수 없습니다. 훈련은 비용과 시간이 많이 소요되는 프로세스입니다. 일반적으로 훈련에 필요한 리소스의 일부만 사용하면서 더 나은 결과를 더 빨리 얻으려면 사전 훈련된 모델을 사용하는 것이 좋습니다.\n-\n-사전 학습된 모델을 [`~TFPreTrainedModel.from_pretrained`]로 생성합니다:\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-🤗 Transformers에서 제공한 모델의 사전 학습된 가중치를 사용하는 경우 기본 모델 configuration을 자동으로 불러옵니다. 그러나 원하는 경우 기본 모델 configuration 속성의 일부 또는 전부를 사용자 지정으로 바꿀 수 있습니다:\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n-```\n-</tf>\n-</frameworkcontent>\n-\n-### 모델 헤드[[model-heads]]\n-\n-이 시점에서 *은닉 상태(hidden state)*를 출력하는 기본 DistilBERT 모델을 갖게 됩니다. 은닉 상태는 최종 출력을 생성하기 위해 모델 헤드에 입력으로 전달됩니다. 🤗 Transformers는 모델이 해당 작업을 지원하는 한 각 작업마다 다른 모델 헤드를 제공합니다(즉, 번역과 같은 시퀀스 간 작업에는 DistilBERT를 사용할 수 없음).\n-\n-<frameworkcontent>\n-<pt>\n-예를 들어, [`DistilBertForSequenceClassification`]은 시퀀스 분류 헤드가 있는 기본 DistilBERT 모델입니다. 시퀀스 분류 헤드는 풀링된 출력 위에 있는 선형 레이어입니다.\n-\n-```py\n->>> from transformers import DistilBertForSequenceClassification\n-\n->>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-다른 모델 헤드로 전환하여 이 체크포인트를 다른 작업에 쉽게 재사용할 수 있습니다. 질의응답 작업의 경우, [`DistilBertForQuestionAnswering`] 모델 헤드를 사용할 수 있습니다. 질의응답 헤드는 숨겨진 상태 출력 위에 선형 레이어가 있다는 점을 제외하면 시퀀스 분류 헤드와 유사합니다.\n-\n-```py\n->>> from transformers import DistilBertForQuestionAnswering\n-\n->>> model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-</pt>\n-<tf>\n-예를 들어, [`TFDistilBertForSequenceClassification`]은 시퀀스 분류 헤드가 있는 기본 DistilBERT 모델입니다. 시퀀스 분류 헤드는 풀링된 출력 위에 있는 선형 레이어입니다.\n-\n-```py\n->>> from transformers import TFDistilBertForSequenceClassification\n-\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-다른 모델 헤드로 전환하여 이 체크포인트를 다른 작업에 쉽게 재사용할 수 있습니다. 질의응답 작업의 경우, [`TFDistilBertForQuestionAnswering`] 모델 헤드를 사용할 수 있습니다. 질의응답 헤드는 숨겨진 상태 출력 위에 선형 레이어가 있다는 점을 제외하면 시퀀스 분류 헤드와 유사합니다.\n-\n-```py\n->>> from transformers import TFDistilBertForQuestionAnswering\n-\n->>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-</tf>\n-</frameworkcontent>\n-\n-## 토크나이저[[tokenizer]]\n-\n-텍스트 데이터에 모델을 사용하기 전에 마지막으로 필요한 기본 클래스는 원시 텍스트를 텐서로 변환하는 [토크나이저](main_classes/tokenizer)입니다. 🤗 Transformers에 사용할 수 있는 토크나이저는 두 가지 유형이 있습니다:\n-\n-- [`PreTrainedTokenizer`]: 파이썬으로 구현된 토크나이저입니다.\n-- [`PreTrainedTokenizerFast`]: Rust 기반 [🤗 Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/) 라이브러리로 만들어진 토크나이저입니다. 이 토크나이저는 Rust로 구현되어 배치 토큰화에서 특히 빠릅니다. 빠른 토크나이저는 토큰을 원래 단어나 문자에 매핑하는 *오프셋 매핑*과 같은 추가 메소드도 제공합니다.\n-두 토크나이저 모두 인코딩 및 디코딩, 새 토큰 추가, 특수 토큰 관리와 같은 일반적인 방법을 지원합니다.\n-\n-<Tip warning={true}>\n-\n-모든 모델이 빠른 토크나이저를 지원하는 것은 아닙니다. 이 [표](index#supported-frameworks)에서 모델의 빠른 토크나이저 지원 여부를 확인하세요.\n-\n-</Tip>\n-\n-토크나이저를 직접 학습한 경우, *어휘(vocabulary)* 파일에서 토크나이저를 만들 수 있습니다:\n-\n-```py\n->>> from transformers import DistilBertTokenizer\n-\n->>> my_tokenizer = DistilBertTokenizer(vocab_file=\"my_vocab_file.txt\", do_lower_case=False, padding_side=\"left\")\n-```\n-\n-사용자 지정 토크나이저의 어휘는 사전 학습된 모델의 토크나이저에서 생성된 어휘와 다를 수 있다는 점을 기억하는 것이 중요합니다. 사전 학습된 모델을 사용하는 경우 사전 학습된 모델의 어휘를 사용해야 하며, 그렇지 않으면 입력이 의미를 갖지 못합니다. [`DistilBertTokenizer`] 클래스를 사용하여 사전 학습된 모델의 어휘로 토크나이저를 생성합니다:\n-\n-```py\n->>> from transformers import DistilBertTokenizer\n-\n->>> slow_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-[`DistilBertTokenizerFast`] 클래스로 빠른 토크나이저를 생성합니다:\n-\n-```py\n->>> from transformers import DistilBertTokenizerFast\n-\n->>> fast_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-<Tip>\n-\n-[`AutoTokenizer`]는 기본적으로 빠른 토크나이저를 가져오려고 합니다. 이 동작을 비활성화하려면 `from_pretrained`에서 `use_fast=False`를 설정하면 됩니다.\n-\n-</Tip>\n-\n-## 이미지 프로세서[[image-processor]]\n-\n-이미지 프로세서(image processor)는 비전 입력을 처리합니다. 기본 [`~image_processing_utils.ImageProcessingMixin`] 클래스에서 상속합니다.\n-\n-사용하려면 사용 중인 모델과 연결된 이미지 프로세서를 생성합니다. 예를 들어, 이미지 분류에 [ViT](model_doc/vit)를 사용하는 경우 기본 [`ViTImageProcessor`]를 생성합니다:\n-\n-```py\n->>> from transformers import ViTImageProcessor\n-\n->>> vit_extractor = ViTImageProcessor()\n->>> print(vit_extractor)\n-ViTImageProcessor {\n-  \"do_normalize\": true,\n-  \"do_resize\": true,\n-  \"feature_extractor_type\": \"ViTImageProcessor\",\n-  \"image_mean\": [\n-    0.5,\n-    0.5,\n-    0.5\n-  ],\n-  \"image_std\": [\n-    0.5,\n-    0.5,\n-    0.5\n-  ],\n-  \"resample\": 2,\n-  \"size\": 224\n-}\n-```\n-\n-<Tip>\n-\n-사용자 지정을 원하지 않는 경우 `from_pretrained` 메소드를 사용하여 모델의 기본 이미지 프로세서 매개변수를 불러오면 됩니다.\n-\n-</Tip>\n-\n-사용자 지정 이미지 프로세서를 생성하려면 [`ViTImageProcessor`] 파라미터를 수정합니다:\n-\n-```py\n->>> from transformers import ViTImageProcessor\n-\n->>> my_vit_extractor = ViTImageProcessor(resample=\"PIL.Image.BOX\", do_normalize=False, image_mean=[0.3, 0.3, 0.3])\n->>> print(my_vit_extractor)\n-ViTImageProcessor {\n-  \"do_normalize\": false,\n-  \"do_resize\": true,\n-  \"feature_extractor_type\": \"ViTImageProcessor\",\n-  \"image_mean\": [\n-    0.3,\n-    0.3,\n-    0.3\n-  ],\n-  \"image_std\": [\n-    0.5,\n-    0.5,\n-    0.5\n-  ],\n-  \"resample\": \"PIL.Image.BOX\",\n-  \"size\": 224\n-}\n-```\n-\n-## 특성 추출기[[feature-extractor]]\n-\n-특성 추출기(feature extractor)는 오디오 입력을 처리합니다. 기본 [`~feature_extraction_utils.FeatureExtractionMixin`] 클래스에서 상속되며, 오디오 입력을 처리하기 위해 [`SequenceFeatureExtractor`] 클래스에서 상속할 수도 있습니다.\n-\n-사용하려면 사용 중인 모델과 연결된 특성 추출기를 생성합니다. 예를 들어, 오디오 분류에 [Wav2Vec2](model_doc/wav2vec2)를 사용하는 경우 기본 [`Wav2Vec2FeatureExtractor`]를 생성합니다:\n-\n-```py\n->>> from transformers import Wav2Vec2FeatureExtractor\n-\n->>> w2v2_extractor = Wav2Vec2FeatureExtractor()\n->>> print(w2v2_extractor)\n-Wav2Vec2FeatureExtractor {\n-  \"do_normalize\": true,\n-  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n-  \"feature_size\": 1,\n-  \"padding_side\": \"right\",\n-  \"padding_value\": 0.0,\n-  \"return_attention_mask\": false,\n-  \"sampling_rate\": 16000\n-}\n-```\n-\n-<Tip>\n-\n-사용자 지정이 필요하지 않은 경우 `from_pretrained` 메소드를 사용하여 모델의 기본 특성 추출기 ㅁ개변수를 불러 오면 됩니다.\n-\n-</Tip>\n-\n-사용자 지정 특성 추출기를 만들려면 [`Wav2Vec2FeatureExtractor`] 매개변수를 수정합니다:\n-\n-```py\n->>> from transformers import Wav2Vec2FeatureExtractor\n-\n->>> w2v2_extractor = Wav2Vec2FeatureExtractor(sampling_rate=8000, do_normalize=False)\n->>> print(w2v2_extractor)\n-Wav2Vec2FeatureExtractor {\n-  \"do_normalize\": false,\n-  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n-  \"feature_size\": 1,\n-  \"padding_side\": \"right\",\n-  \"padding_value\": 0.0,\n-  \"return_attention_mask\": false,\n-  \"sampling_rate\": 8000\n-}\n-```\n-\n-\n-## 프로세서[[processor]]\n-\n-멀티모달 작업을 지원하는 모델의 경우, 🤗 Transformers는 특성 추출기 및 토크나이저와 같은 처리 클래스를 단일 객체로 편리하게 래핑하는 프로세서 클래스를 제공합니다. 예를 들어, 자동 음성 인식 작업(Automatic Speech Recognition task (ASR))에 [`Wav2Vec2Processor`]를 사용한다고 가정해 보겠습니다. 자동 음성 인식 작업은 오디오를 텍스트로 변환하므로 특성 추출기와 토크나이저가 필요합니다.\n-\n-오디오 입력을 처리할 특성 추출기를 만듭니다:\n-\n-```py\n->>> from transformers import Wav2Vec2FeatureExtractor\n-\n->>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)\n-```\n-\n-텍스트 입력을 처리할 토크나이저를 만듭니다:\n-\n-```py\n->>> from transformers import Wav2Vec2CTCTokenizer\n-\n->>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file=\"my_vocab_file.txt\")\n-```\n-\n-[`Wav2Vec2Processor`]에서 특성 추출기와 토크나이저를 결합합니다:\n-\n-```py\n->>> from transformers import Wav2Vec2Processor\n-\n->>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n-```\n-\n-configuration과 모델이라는 두 가지 기본 클래스와 추가 전처리 클래스(토크나이저, 이미지 프로세서, 특성 추출기 또는 프로세서)를 사용하면 🤗 Transformers에서 지원하는 모든 모델을 만들 수 있습니다. 이러한 각 기본 클래스는 구성이 가능하므로 원하는 특정 속성을 사용할 수 있습니다. 학습을 위해 모델을 쉽게 설정하거나 기존의 사전 학습된 모델을 수정하여 미세 조정할 수 있습니다."
        },
        {
            "sha": "549ec374738a5c71783212335c9a501730ef33f9",
            "filename": "docs/source/ko/model_summary.md",
            "status": "removed",
            "additions": 0,
            "deletions": 107,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fmodel_summary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fmodel_summary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_summary.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,107 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Transformer 모델군[[the-transformer-model-family]]\n-\n-2017년에 소개된 [기본 Transformer](https://huggingface.co/papers/1706.03762) 모델은 자연어 처리(NLP) 작업을 넘어 새롭고 흥미로운 모델들에 영감을 주었습니다. [단백질 접힘 구조 예측](https://huggingface.co/blog/deep-learning-with-proteins), [치타의 달리기 훈련](https://huggingface.co/blog/train-decision-transformers), [시계열 예측](https://huggingface.co/blog/time-series-transformers) 등을 위한 다양한 모델이 생겨났습니다. Transformer의 변형이 너무 많아서, 큰 그림을 놓치기 쉽습니다. 하지만 여기 있는 모든 모델의 공통점은 기본 Trasnformer 아키텍처를 기반으로 한다는 점입니다. 일부 모델은 인코더 또는 디코더만 사용하고, 다른 모델들은 인코더와 디코더를 모두 사용하기도 합니다. 이렇게 Transformer 모델군 내 상위 레벨에서의 차이점을 분류하고 검토하면 유용한 분류 체계를 얻을 수 있으며, 이전에 접해보지 못한 Transformer 모델들 또한 이해하는 데 도움이 될 것입니다. \n-\n-기본 Transformer 모델에 익숙하지 않거나 복습이 필요한 경우, Hugging Face 강의의 [트랜스포머는 어떻게 동작하나요?](https://huggingface.co/course/chapter1/4?fw=pt) 챕터를 확인하세요. \n-\n-<div align=\"center\">\n-    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/H39Z_720T5s\" title=\"YouTube video player\"\n-    frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope;\n-    picture-in-picture\" allowfullscreen></iframe>\n-</div>\n-\n-## 컴퓨터 비전[[computer-vision]]\n-\n-<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1\" allowfullscreen></iframe> \n-\n-### 합성곱 네트워크[[convolutional-network]]\n-\n-[Vision Transformer](https://huggingface.co/papers/2010.11929)가 확장성과 효율성을 입증하기 전까지 오랫동안 합성곱 네트워크(CNN)가 컴퓨터 비전 작업의 지배적인 패러다임이었습니다. 그럼에도 불구하고, 이동 불변성(translation invariance)과 같은 CNN의 우수한 부분이 도드라지기 때문에 몇몇 (특히 특정 과업에서의) Transformer 모델은 아키텍처에 합성곱을 통합하기도 했습니다. [ConvNeXt](model_doc/convnext)는 이런 관례를 뒤집어 CNN을 현대화하기 위해 Transformer의 디자인을 차용합니다. 예를 들면 ConvNeXt는 겹치지 않는 슬라이딩 창(sliding window)을 사용하여 이미지를 패치화하고, 더 큰 커널로 전역 수용 필드(global receptive field)를 확장시킵니다. ConvNeXt는 또한 메모리 효율을 높이고 성능을 향상시키기 위해 여러 레이어 설계를 선택하기 때문에 Transformer와 견줄만합니다!\n-\n-### 인코더[[cv-encoder]]\n-\n-[Vision Transformer(ViT)](model_doc/vit)는 합성곱 없는 컴퓨터 비전 작업의 막을 열었습니다. ViT는 표준 Transformer 인코더를 사용하지만, 가장 큰 혁신은 이미지를 처리하는 방식이었습니다. 문장을 토큰으로 분할하는 것처럼 이미지를 고정된 크기의 패치로 분할하고, 이를 사용하여 임베딩을 생성합니다. ViT는 Transformer의 효율적인 아키텍처를 활용하여 훈련에 더 적은 자원을 사용하면서도 당시 CNN에 비견하는 결과를 입증했습니다. 그리고 ViT를 뒤이어 분할(segmentation)과 같은 고밀도 비전 작업과 탐지 작업도 다룰 수 있는 다른 비전 모델이 등장했습니다.\n-\n-이러한 모델 중 하나가 [Swin](model_doc/swin) Transformer입니다. 이 모델은 작은 크기의 패치에서 계층적 특징 맵(CNN 👀과 같지만 ViT와는 다름)을 만들고 더 깊은 레이어의 인접 패치와 병합합니다. 어텐션(Attention)은 지역 윈도우 내에서만 계산되며, 모델이 더 잘 학습할 수 있도록 어텐션 레이어 간에 윈도우를 이동하며 연결을 생성합니다. Swin Transformer는 계층적 특징 맵을 생성할 수 있으므로, 분할(segmentation)과 탐지와 같은 고밀도 예측 작업에 적합합니다. [SegFormer](model_doc/segformer) 역시 Transformer 인코더를 사용하여 계층적 특징 맵을 구축하지만, 상단에 간단한 다층 퍼셉트론(MLP) 디코더를 추가하여 모든 특징 맵을 결합하고 예측을 수행합니다. \n-\n-BeIT와 ViTMAE와 같은 다른 비전 모델은 BERT의 사전훈련 목표(objective)에서 영감을 얻었습니다. [BeIT](model_doc/beit)는 *마스크드 이미지 모델링(MIM)*으로 사전훈련되며, 이미지 패치는 임의로 마스킹되고 이미지도 시각적 토큰으로 토큰화됩니다. BeIT는 마스킹된 패치에 해당하는 시각적 토큰을 예측하도록 학습됩니다. [ViTMAE](model_doc/vitmae)도 비슷한 사전훈련 목표가 있지만, 시각적 토큰 대신 픽셀을 예측해야 한다는 점이 다릅니다. 특이한 점은 이미지 패치의 75%가 마스킹되어 있다는 것입니다! 디코더는 마스킹된 토큰과 인코딩된 패치에서 픽셀을 재구성합니다. 사전훈련이 끝나면 디코더는 폐기되고 인코더는 다운스트림 작업에 사용할 준비가 됩니다.\n-\n-### 디코더[[cv-decoder]]\n-\n-대부분의 비전 모델은 인코더에 의존하여 이미지 표현을 학습하기 때문에 디코더 전용 비전 모델은 드뭅니다. 하지만 이미지 생성 등의 사례의 경우, GPT-2와 같은 텍스트 생성 모델에서 보았듯이 디코더가 가장 적합합니다. [ImageGPT](model_doc/imagegpt)는 GPT-2와 동일한 아키텍처를 사용하지만, 시퀀스의 다음 토큰을 예측하는 대신 이미지의 다음 픽셀을 예측합니다. ImageGPT는 이미지 생성 뿐만 아니라 이미지 분류를 위해 미세 조정할 수도 있습니다. \n-\n-### 인코더-디코더[[cv-encoder-decoder]]\n-\n-비전 모델은 일반적으로 인코더(백본으로도 알려짐)를 사용하여 중요한 이미지 특징을 추출한 후, 이를 Transformer 디코더로 전달합니다. [DETR](model_doc/detr)에 사전훈련된 백본이 있지만, 객체 탐지를 위해 완전한 Transformer 인코더-디코더 아키텍처도 사용합니다. 인코더는 이미지 표현을 학습하고 이를 디코더에서 객체 쿼리(각 객체 쿼리는 이미지의 영역 또는 객체에 중점을 두고 학습된 임베딩)와 결합합니다. DETR은 각 객체 쿼리에 대한 바운딩 박스 좌표와 클래스 레이블을 예측합니다.\n-\n-## 자연어처리[[natural-language-processing]]\n-\n-<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1\" allowfullscreen></iframe>\n-\n-### 인코더[[nlp-encoder]]\n-\n-[BERT](model_doc/bert)는 인코더 전용 Transformer로, 다른 토큰을 보고 소위 \"부정 행위\"를 저지르는 걸 막기 위해 입력에서 특정 토큰을 임의로 마스킹합니다. 사전훈련의 목표는 컨텍스트를 기반으로 마스킹된 토큰을 예측하는 것입니다. 이를 통해 BERT는 왼쪽과 오른쪽 컨텍스트를 충분히 활용하여 입력에 대해 더 깊고 풍부한 표현을 학습할 수 있습니다. 그러나 BERT의 사전훈련 전략에는 여전히 개선의 여지가 남아 있었습니다. [RoBERTa](model_doc/roberta)는 더 긴 시간 동안 더 큰 배치에 대한 훈련을 포함하고, 전처리 중에 한 번만 마스킹하는 것이 아니라 각 에폭에서 토큰을 임의로 마스킹하고, 다음 문장 예측 목표를 제거하는 새로운 사전훈련 방식을 도입함으로써 이를 개선했습니다. \n-\n-성능 개선을 위한 전략으로 모델 크기를 키우는 것이 지배적입니다. 하지만 큰 모델을 훈련하려면 계산 비용이 많이 듭니다. 계산 비용을 줄이는 한 가지 방법은 [DistilBERT](model_doc/distilbert)와 같이 작은 모델을 사용하는 것입니다. DistilBERT는 압축 기법인 [지식 증류(knowledge distillation)](https://huggingface.co/papers/1503.02531)를 사용하여, 거의 모든 언어 이해 능력을 유지하면서 더 작은 버전의 BERT를 만듭니다. \n-\n-그러나 대부분의 Transformer 모델에 더 많은 매개변수를 사용하는 경향이 이어졌고, 이에 따라 훈련 효율성을 개선하는 것에 중점을 둔 새로운 모델이 등장했습니다. [ALBERT](model_doc/albert)는 두 가지 방법으로 매개변수 수를 줄여 메모리 사용량을 줄였습니다. 바로 큰 어휘를 두 개의 작은 행렬로 분리하는 것과 레이어가 매개변수를 공유하도록 하는 것입니다. [DeBERTa](model_doc/deberta)는 단어와 그 위치를 두 개의 벡터로 개별적으로 인코딩하는 분리된(disentangled) 어텐션 메커니즘을 추가했습니다. 어텐션은 단어와 위치 임베딩을 포함하는 단일 벡터 대신 이 별도의 벡터에서 계산됩니다. [Longformer](model_doc/longformer)는 특히 시퀀스 길이가 긴 문서를 처리할 때, 어텐션을 더 효율적으로 만드는 것에 중점을 두었습니다. 지역(local) 윈도우 어텐션(각 토큰 주변의 고정된 윈도우 크기에서만 계산되는 어텐션)과 전역(global) 어텐션(분류를 위해 `[CLS]`와 같은 특정 작업 토큰에만 해당)의 조합을 사용하여 전체(full) 어텐션 행렬 대신 희소(sparse) 어텐션 행렬을 생성합니다. \n-\n-### 디코더[[nlp-decoder]]\n-\n-[GPT-2](model_doc/gpt2)는 시퀀스에서 다음 단어를 예측하는 디코더 전용 Transformer입니다. 토큰을 오른쪽으로 마스킹하여 모델이 이전 토큰을 보고 \"부정 행위\"를 하지 못하도록 합니다. GPT-2는 방대한 텍스트에 대해 사전훈련하여 텍스트가 일부만 정확하거나 사실인 경우에도 상당히 능숙하게 텍스트를 생성할 수 있게 되었습니다. 하지만 GPT-2는 BERT가 사전훈련에서 갖는 양방향 컨텍스트가 부족하기 때문에 특정 작업에 적합하지 않았습니다. [XLNET](model_doc/xlnet)은 양방향 훈련이 가능한 permutation language modeling objective(PLM)를 사용하여 BERT와 GPT-2의 사전훈련 목표에 대한 장점을 함께 가지고 있습니다.\n-\n-GPT-2 이후, 언어 모델은 더욱 거대해졌고 현재는 *대규모 언어 모델(LLM)*로 알려져 있습니다. 충분히 큰 데이터 세트로 사전훈련된 LLM은 퓨샷(few-shot) 또는 제로샷(zero-shot) 학습을 수행합니다. [GPT-J](model_doc/gptj)는 6B 크기의 매개변수가 있고 400B 크기의 토큰으로 훈련된 LLM입니다. GPT-J에 이어 디코더 전용 모델군인 [OPT](model_doc/opt)가 등장했으며, 이 중 가장 큰 모델은 175B 크기이고 180B 크기의 토큰으로 훈련되었습니다. [BLOOM](model_doc/bloom)은 비슷한 시기에 출시되었으며, 이 중 가장 큰 모델은 176B 크기의 매개변수가 있고 46개의 언어와 13개의 프로그래밍 언어로 된 366B 크기의 토큰으로 훈련되었습니다. \n-\n-### 인코더-디코더[[nlp-encoder-decoder]]\n-\n-[BART](model_doc/bart)는 기본 Transformer 아키텍처를 유지하지만, 일부 텍스트 스팬(span)이 단일 `마스크` 토큰으로 대체되는 *text infilling* 변형으로 사전훈련 목표를 수정합니다. 디코더는 변형되지 않은 토큰(향후 토큰은 마스킹됨)을 예측하고 인코더의 은닉 상태를 사용하여 이 작업을 돕습니다. [Pegasus](model_doc/pegasus)는 BART와 유사하지만, Pegasus는 텍스트 스팬 대신 전체 문장을 마스킹합니다. Pegasus는 마스크드 언어 모델링 외에도 gap sentence generation(GSG)로 사전훈련됩니다. GSG는 문서에 중요한 문장 전체를 마스킹하여 `마스크` 토큰으로 대체하는 것을 목표로 합니다. 디코더는 남은 문장에서 출력을 생성해야 합니다. [T5](model_doc/t5)는 특정 접두사를 사용하여 모든 NLP 작업을 텍스트 투 텍스트 문제로 변환하는 더 특수한 모델입니다. 예를 들어, 접두사 `Summarize:`은 요약 작업을 나타냅니다. T5는 지도(GLUE 및 SuperGLUE) 훈련과 자기지도 훈련(토큰의 15%를 임의로 샘플링하여 제거)으로 사전훈련됩니다.\n-\n-## 오디오[[audio]]\n-\n-<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1\" allowfullscreen></iframe>\n-\n-### 인코더[[audio-encoder]]\n-\n-[Wav2Vec2](model_doc/wav2vec2)는 Transformer 인코더를 사용하여 원본 오디오 파형(raw audio waveform)에서 직접 음성 표현을 학습합니다. 허위 음성 표현 세트에서 실제 음성 표현을 판별하는 대조 작업으로 사전훈련됩니다. [HuBERT](model_doc/hubert)는 Wav2Vec2와 유사하지만 훈련 과정이 다릅니다. 타겟 레이블이 유사한 오디오 세그먼트가 클러스터에 할당되어 은닉 단위(unit)가 되는 군집화(clustering) 단계에서 생성됩니다. 은닉 단위는 예측을 위한 임베딩에 매핑됩니다.\n-\n-### 인코더-디코더[[audio-encoder-decoder]]\n-\n-[Speech2Text](model_doc/speech_to_text)는 자동 음성 인식(ASR) 및 음성 번역을 위해 고안된 음성 모델입니다. 이 모델은 오디오 파형에서 추출한 log mel-filter bank 특징을 채택하고 자기회귀 방식으로 사전훈련하여, 전사본 또는 번역을 만듭니다. [Whisper](model_doc/whisper)은 ASR 모델이지만, 다른 많은 음성 모델과 달리 제로샷 성능을 위해 대량의 ✨ 레이블이 지정된 ✨ 오디오 전사 데이터에 대해 사전훈련됩니다. 데이터 세트의 큰 묶음에는 영어가 아닌 언어도 포함되어 있어서 자원이 적은 언어에도 Whisper를 사용할 수 있습니다. 구조적으로, Whisper는 Speech2Text와 유사합니다. 오디오 신호는 인코더에 의해 인코딩된 log-mel spectrogram으로 변환됩니다. 디코더는 인코더의 은닉 상태와 이전 토큰으로부터 자기회귀 방식으로 전사를 생성합니다.\n-\n-## 멀티모달[[multimodal]]\n-\n-<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1\" allowfullscreen></iframe>\n-\n-### 인코더[[mm-encoder]]\n-\n-[VisualBERT](model_doc/visual_bert)는 BERT 이후에 출시된 비전 언어 작업을 위한 멀티모달 모델입니다. 이 모델은 BERT와 사전훈련된 객체 탐지 시스템을 결합하여 이미지 특징을 시각 임베딩으로 추출하고, 텍스트 임베딩과 함께 BERT로 전달합니다. VisualBERT는 마스킹되지 않은 텍스트와 시각 임베딩을 기반으로 마스킹된 텍스트를 예측하고, 텍스트가 이미지와 일치하는지 예측해야 합니다. ViT가 이미지 임베딩을 구하는 방식이 더 쉬웠기 때문에, ViT가 출시된 후 [ViLT](model_doc/vilt)는 아키텍처에 ViT를 채택했습니다. 이미지 임베딩은 텍스트 임베딩과 함께 처리됩니다. 여기에서, ViLT는 이미지 텍스트 매칭, 마스크드 언어 모델링, 전체 단어 마스킹을 통해 사전훈련됩니다.\n-\n-[CLIP](model_doc/clip)은 다른 접근 방식을 사용하여 (`이미지`, `텍스트`)의 쌍 예측을 수행합니다. (`이미지`, `텍스트`) 쌍에서의 이미지와 텍스트 임베딩 간의 유사도를 최대화하기 위해 4억 개의 (`이미지`, `텍스트`) 쌍 데이터 세트에 대해 이미지 인코더(ViT)와 텍스트 인코더(Transformer)를 함께 훈련합니다. 사전훈련 후, 자연어를 사용하여 이미지가 주어진 텍스트를 예측하거나 그 반대로 예측하도록 CLIP에 지시할 수 있습니다. [OWL-ViT](model_doc/owlvit)는 CLIP을 제로샷 객체 탐지를 위한 백본(backbone)으로 사용하여 CLIP 상에 구축됩니다. 사전훈련 후, 객체 탐지 헤드가 추가되어 (`클래스`, `바운딩 박스`) 쌍에 대한 집합(set) 예측을 수행합니다.\n-\n-### 인코더-디코더[[mm-encoder-decoder]]\n-\n-광학 문자 인식(OCR)은 이미지를 이해하고 텍스트를 생성하기 위해 다양한 구성 요소를 필요로 하는 전통적인 텍스트 인식 작업입니다. [TrOCR](model_doc/trocr)은 종단간(end-to-end) Transformer를 사용하여 이 프로세스를 간소화합니다. 인코더는 이미지 이해를 위한 ViT 방식의 모델이며 이미지를 고정된 크기의 패치로 처리합니다. 디코더는 인코더의 은닉 상태를 받아서 자기회귀 방식으로 텍스트를 생성합니다. [Donut](model_doc/donut)은 OCR 기반 접근 방식에 의존하지 않는 더 일반적인 시각 문서 이해 모델입니다. 이 모델은 Swin Transformer를 인코더로, 다국어 BART를 디코더로 사용합니다. Donut은 이미지와 텍스트 주석을 기반으로 다음 단어를 예측하여 텍스트를 읽도록 사전훈련됩니다. 디코더는 프롬프트가 주어지면 토큰 시퀀스를 생성합니다. 프롬프트는 각 다운스트림 작업에 대한 특수 토큰으로 표현됩니다. 예를 들어, 문서 파싱(parsing)에는 인코더의 은닉 상태와 결합되어 문서를 정형 출력 형식(JSON)으로 파싱하는 특수 `파싱` 토큰이 있습니다.\n-\n-## 강화 학습[[reinforcement-learning]]\n-\n-<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FiB3Y6RvWYki7ZuKO6tNgZq%2Freinforcement-learning%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1\" allowfullscreen></iframe>\n-\n-### 디코더[[rl-decoder]]\n-\n-Decision 및 Trajectory Transformer는 상태(state), 행동(action), 보상(reward)을 시퀀스 모델링 문제로 표현합니다. [Decision Transformer](model_doc/decision_transformer)는 기대 보상(returns-to-go), 과거 상태 및 행동을 기반으로 미래의 원하는 수익(return)으로 이어지는 일련의 행동을 생성합니다. 마지막 *K* 시간 스텝(timestep)에 대해, 세 가지 모달리티는 각각 토큰 임베딩으로 변환되고 GPT와 같은 모델에 의해 처리되어 미래의 액션 토큰을 예측합니다. [Trajectory Transformer](model_doc/trajectory_transformer)도 상태, 행동, 보상을 토큰화하여 GPT 아키텍처로 처리합니다. 보상 조건에 중점을 둔 Decision Transformer와 달리 Trajectory Transformer는 빔 서치(beam search)로 미래 행동을 생성합니다.\n\\ No newline at end of file"
        },
        {
            "sha": "c0eee024358f3e1a42ed08834183e810a271d8f8",
            "filename": "docs/source/ko/multilingual.md",
            "status": "removed",
            "additions": 0,
            "deletions": 192,
            "changes": 192,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fmultilingual.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fmultilingual.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmultilingual.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,192 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# 다국어 모델 추론하기[[multilingual-models-for-inference]]\n-\n-[[open-in-colab]]\n-\n-🤗 Transformers에는 여러 종류의 다국어(multilingual) 모델이 있으며, 단일 언어(monolingual) 모델과 추론 시 사용법이 다릅니다.\n-그렇다고 해서 *모든* 다국어 모델의 사용법이 다른 것은 아닙니다.\n-\n-[google-bert/bert-base-multilingual-uncased](https://huggingface.co/google-bert/bert-base-multilingual-uncased)와 같은 몇몇 모델은 단일 언어 모델처럼 사용할 수 있습니다.\n-이번 가이드에서 다국어 모델의 추론 시 사용 방법을 알아볼 것입니다.\n-\n-## XLM[[xlm]]\n-\n-XLM에는 10가지 체크포인트(checkpoint)가 있는데, 이 중 하나만 단일 언어입니다. \n-나머지 체크포인트 9개는 언어 임베딩을 사용하는 체크포인트와 그렇지 않은 체크포인트의 두 가지 범주로 나눌 수 있습니다.\n-\n-### 언어 임베딩을 사용하는 XLM[[xlm-with-language-embeddings]]\n-\n-다음 XLM 모델은 추론 시에 언어 임베딩을 사용합니다:\n-\n-- `FacebookAI/xlm-mlm-ende-1024` (마스킹된 언어 모델링, 영어-독일어)\n-- `FacebookAI/xlm-mlm-enfr-1024` (마스킹된 언어 모델링, 영어-프랑스어)\n-- `FacebookAI/xlm-mlm-enro-1024` (마스킹된 언어 모델링, 영어-루마니아어)\n-- `FacebookAI/xlm-mlm-xnli15-1024` (마스킹된 언어 모델링, XNLI 데이터 세트에서 제공하는 15개 국어)\n-- `FacebookAI/xlm-mlm-tlm-xnli15-1024` (마스킹된 언어 모델링 + 번역, XNLI 데이터 세트에서 제공하는 15개 국어)\n-- `FacebookAI/xlm-clm-enfr-1024` (Causal language modeling, 영어-프랑스어)\n-- `FacebookAI/xlm-clm-ende-1024` (Causal language modeling, 영어-독일어)\n-\n-언어 임베딩은 모델에 전달된 `input_ids`와 동일한 shape의 텐서로 표현됩니다.\n-이러한 텐서의 값은 사용된 언어에 따라 다르며 토크나이저의 `lang2id` 및 `id2lang` 속성에 의해 식별됩니다.\n-\n-다음 예제에서는 `FacebookAI/xlm-clm-enfr-1024` 체크포인트(코잘 언어 모델링(causal language modeling), 영어-프랑스어)를 가져옵니다:\n-\n-```py\n->>> import torch\n->>> from transformers import XLMTokenizer, XLMWithLMHeadModel\n-\n->>> tokenizer = XLMTokenizer.from_pretrained(\"FacebookAI/xlm-clm-enfr-1024\")\n->>> model = XLMWithLMHeadModel.from_pretrained(\"FacebookAI/xlm-clm-enfr-1024\")\n-```\n-\n-토크나이저의 `lang2id` 속성은 모델의 언어와 해당 ID를 표시합니다:\n-\n-```py\n->>> print(tokenizer.lang2id)\n-{'en': 0, 'fr': 1}\n-```\n-\n-다음으로, 예제 입력을 만듭니다:\n-\n-```py\n->>> input_ids = torch.tensor([tokenizer.encode(\"Wikipedia was used to\")])  # 배치 크기는 1입니다\n-```\n-\n-언어 ID를 `\"en\"`으로 설정해 언어 임베딩을 정의합니다. \n-언어 임베딩은 영어의 언어 ID인 `0`으로 채워진 텐서입니다.\n-이 텐서는 `input_ids`와 같은 크기여야 합니다. \n-\n-```py\n->>> language_id = tokenizer.lang2id[\"en\"]  # 0\n->>> langs = torch.tensor([language_id] * input_ids.shape[1])  # torch.tensor([0, 0, 0, ..., 0])\n-\n->>> # (batch_size, sequence_length) shape의 텐서가 되도록 만듭니다.\n->>> langs = langs.view(1, -1)  # 이제 [1, sequence_length] shape이 되었습니다(배치 크기는 1입니다)\n-```\n-\n-이제 `input_ids`와 언어 임베딩을 모델로 전달합니다:\n-\n-```py\n->>> outputs = model(input_ids, langs=langs)\n-```\n-\n-[run_generation.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation/run_generation.py) 스크립트로 `xlm-clm` 체크포인트를 사용해 텍스트와 언어 임베딩을 생성할 수 있습니다.\n-\n-### 언어 임베딩을 사용하지 않는 XLM[[xlm-without-language-embeddings]]\n-\n-다음 XLM 모델은 추론 시에 언어 임베딩이 필요하지 않습니다:\n-\n-- `FacebookAI/xlm-mlm-17-1280` (마스킹된 언어 모델링, 17개 국어)\n-- `FacebookAI/xlm-mlm-100-1280` (마스킹된 언어 모델링, 100개 국어)\n-\n-이전의 XLM 체크포인트와 달리 이 모델은 일반 문장 표현에 사용됩니다.\n-\n-## BERT[[bert]]\n-\n-다음 BERT 모델은 다국어 태스크에 사용할 수 있습니다:\n-\n-- `google-bert/bert-base-multilingual-uncased` (마스킹된 언어 모델링 + 다음 문장 예측, 102개 국어)\n-- `google-bert/bert-base-multilingual-cased` (마스킹된 언어 모델링 + 다음 문장 예측, 104개 국어)\n-\n-이러한 모델은 추론 시에 언어 임베딩이 필요하지 않습니다. \n-문맥에서 언어를 식별하고, 식별된 언어로 추론합니다.\n-\n-## XLM-RoBERTa[[xlmroberta]]\n-\n-다음 XLM-RoBERTa 또한 다국어 다국어 태스크에 사용할 수 있습니다:\n-\n-- `FacebookAI/xlm-roberta-base` (마스킹된 언어 모델링, 100개 국어)\n-- `FacebookAI/xlm-roberta-large` (마스킹된 언어 모델링, 100개 국어)\n-\n-XLM-RoBERTa는 100개 국어에 대해 새로 생성되고 정제된 2.5TB 규모의 CommonCrawl 데이터로 학습되었습니다.\n-이전에 공개된 mBERT나 XLM과 같은 다국어 모델에 비해 분류, 시퀀스 라벨링, 질의 응답과 같은 다운스트림(downstream) 작업에서 이점이 있습니다.\n-\n-## M2M100[[m2m100]]\n-\n-다음 M2M100 모델 또한 다국어 다국어 태스크에 사용할 수 있습니다:\n-\n-- `facebook/m2m100_418M` (번역)\n-- `facebook/m2m100_1.2B` (번역)\n-\n-이 예제에서는 `facebook/m2m100_418M` 체크포인트를 가져와서 중국어를 영어로 번역합니다. \n-토크나이저에서 번역 대상 언어(source language)를 설정할 수 있습니다:\n-\n-```py\n->>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n-\n->>> en_text = \"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\"\n->>> chinese_text = \"不要插手巫師的事務, 因為他們是微妙的, 很快就會發怒.\"\n-\n->>> tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"zh\")\n->>> model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n-```\n-\n-문장을 토큰화합니다:\n-\n-```py\n->>> encoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\n-```\n-\n-M2M100은 번역을 진행하기 위해 첫 번째로 생성되는 토큰은 번역할 언어(target language) ID로 강제 지정합니다.\n-영어로 번역하기 위해 `generate` 메소드에서 `forced_bos_token_id`를 `en`으로 설정합니다:\n-\n-```py\n->>> generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n->>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n-'Do not interfere with the matters of the witches, because they are delicate and will soon be angry.'\n-```\n-\n-## MBart[[mbart]]\n-\n-다음 MBart 모델 또한 다국어 태스크에 사용할 수 있습니다:\n-\n-- `facebook/mbart-large-50-one-to-many-mmt` (일대다 다국어 번역, 50개 국어)\n-- `facebook/mbart-large-50-many-to-many-mmt` (다대다 다국어 번역, 50개 국어)\n-- `facebook/mbart-large-50-many-to-one-mmt` (다대일 다국어 번역, 50개 국어)\n-- `facebook/mbart-large-50` (다국어 번역, 50개 국어)\n-- `facebook/mbart-large-cc25`\n-\n-이 예제에서는 핀란드어를 영어로 번역하기 위해 `facebook/mbart-large-50-many-to-many-mmt` 체크포인트를 가져옵니다. \n-토크나이저에서 번역 대상 언어(source language)를 설정할 수 있습니다:\n-\n-```py\n->>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n-\n->>> en_text = \"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\"\n->>> fi_text = \"Älä sekaannu velhojen asioihin, sillä ne ovat hienovaraisia ja nopeasti vihaisia.\"\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"fi_FI\")\n->>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n-```\n-\n-문장을 토큰화합니다:\n-\n-```py\n->>> encoded_en = tokenizer(en_text, return_tensors=\"pt\")\n-```\n-\n-MBart는 번역을 진행하기 위해 첫 번째로 생성되는 토큰은 번역할 언어(target language) ID로 강제 지정합니다.\n-영어로 번역하기 위해 `generate` 메소드에서 `forced_bos_token_id`를 `en`으로 설정합니다:\n-\n-```py\n->>> generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id(\"en_XX\"))\n->>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n-\"Don't interfere with the wizard's affairs, because they are subtle, will soon get angry.\"\n-```\n-\n-`facebook/mbart-large-50-many-to-one-mmt` 체크포인트를 사용하고 있다면, 첫 번째로 생성되는 토큰을 번역할 언어(target language) ID로 강제 지정할 필요는 없습니다."
        },
        {
            "sha": "28d4fdafb96ca85278c3190c7c46be29397ac2f8",
            "filename": "docs/source/ko/perf_train_tpu_tf.md",
            "status": "removed",
            "additions": 0,
            "deletions": 162,
            "changes": 162,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fperf_train_tpu_tf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fperf_train_tpu_tf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperf_train_tpu_tf.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,162 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# TensorFlow로 TPU에서 훈련하기[[training-on-tpu-with-tensorflow]]\n-\n-<Tip>\n-\n-자세한 설명이 필요하지 않고 바로 TPU 샘플 코드를 시작하고 싶다면 [우리의 TPU 예제 노트북!](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)을 확인하세요.\n-\n-</Tip>\n-\n-### TPU가 무엇인가요?[[what-is-a-tpu]]\n-\n-TPU는 **텐서 처리 장치**입니다. Google에서 설계한 하드웨어로, GPU처럼 신경망 내에서 텐서 연산을 더욱 빠르게 처리하기 위해 사용됩니다. 네트워크 훈련과 추론 모두에 사용할 수 있습니다. 일반적으로 Google의 클라우드 서비스를 통해 이용할 수 있지만, Google Colab과 Kaggle Kernel을 통해 소규모 TPU를 무료로 직접 이용할 수도 있습니다.\n-\n-[🤗 Transformers의 모든 Tensorflow 모델은 Keras 모델](https://huggingface.co/blog/tensorflow-philosophy)이기 때문에, 이 문서에서 다루는 대부분의 메소드는 대체로 모든 Keras 모델을 위한 TPU 훈련에 적용할 수 있습니다! 하지만 Transformer와 데이터 세트의 HuggingFace 생태계(hug-o-system?)에 특화된 몇 가지 사항이 있으며, 해당 사항에 대해 설명할 때 반드시 언급하도록 하겠습니다.\n-\n-### 어떤 종류의 TPU가 있나요?[[what-kinds-of-tpu-are-available]]\n-\n-신규 사용자는 TPU의 범위와 다양한 이용 방법에 대해 매우 혼란스러워하는 경우가 많습니다. **TPU 노드**와 **TPU VM**의 차이점은 가장 먼저 이해해야 할 핵심적인 구분 사항입니다.\n-\n-**TPU 노드**를 사용한다면, 실제로는 원격 TPU를 간접적으로 이용하는 것입니다. 네트워크와 데이터 파이프라인을 초기화한 다음, 이를 원격 노드로 전달할 별도의 VM이 필요합니다. Google Colab에서 TPU를 사용하는 경우, **TPU 노드** 방식으로 이용하게 됩니다.\n-\n-TPU 노드를 사용하는 것은 이를 사용하지 않는 사용자에게 예기치 않은 현상이 발생하기도 합니다! 특히, TPU는 파이썬 코드를 실행하는 기기(machine)와 물리적으로 다른 시스템에 있기 때문에 로컬 기기에 데이터를 저장할 수 없습니다. 즉, 컴퓨터의 내부 저장소에서 가져오는 데이터 파이프라인은 절대 작동하지 않습니다! 로컬 기기에 데이터를 저장하는 대신에, 데이터 파이프라인이 원격 TPU 노드에서 실행 중일 때에도 데이터 파이프라인이 계속 이용할 수 있는 Google Cloud Storage에 데이터를 저장해야 합니다.\n-\n-<Tip>\n-\n-메모리에 있는 모든 데이터를 `np.ndarray` 또는 `tf.Tensor`로 맞출 수 있다면, Google Cloud Storage에 업로드할 필요 없이, Colab 또는 TPU 노드를 사용해서 해당 데이터에 `fit()` 할 수 있습니다.\n-\n-</Tip>\n-\n-<Tip>\n-\n-**🤗특수한 Hugging Face 팁🤗:** TF 코드 예제에서 볼 수 있는 `Dataset.to_tf_dataset()` 메소드와 그 상위 래퍼(wrapper)인 `model.prepare_tf_dataset()`는 모두 TPU 노드에서 작동하지 않습니다. 그 이유는 `tf.data.Dataset`을 생성하더라도 “순수한” `tf.data` 파이프라인이 아니며 `tf.numpy_function` 또는 `Dataset.from_generator()`를 사용하여 기본 HuggingFace `Dataset`에서 데이터를 전송하기 때문입니다. 이 HuggingFace `Dataset`는 로컬 디스크에 있는 데이터로 지원되며 원격 TPU 노드가 읽을 수 없습니다.\n-\n-</Tip>\n-\n-TPU를 이용하는 두 번째 방법은 **TPU VM**을 사용하는 것입니다. TPU VM을 사용할 때, GPU VM에서 훈련하는 것과 같이 TPU가 장착된 기기에 직접 연결합니다. 특히 데이터 파이프라인과 관련하여, TPU VM은 대체로 작업하기 더 쉽습니다. 위의 모든 경고는 TPU VM에는 해당되지 않습니다!\n-\n-이 문서는 의견이 포함된 문서이며, 저희의 의견이 여기에 있습니다: **가능하면 TPU 노드를 사용하지 마세요.** TPU 노드는 TPU VM보다 더 복잡하고 디버깅하기가 더 어렵습니다. 또한 향후에는 지원되지 않을 가능성이 높습니다. Google의 최신 TPU인 TPUv4는 TPU VM으로만 이용할 수 있으므로, TPU 노드는 점점 더 \"구식\" 이용 방법이 될 것으로 전망됩니다. 그러나 TPU 노드를 사용하는 Colab과 Kaggle Kernel에서만 무료 TPU 이용이 가능한 것으로 확인되어, 필요한 경우 이를 다루는 방법을 설명해 드리겠습니다! 이에 대한 자세한 설명이 담긴 코드 샘플은 [TPU 예제 노트북](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)에서 확인하시기 바랍니다.\n-\n-### 어떤 크기의 TPU를 사용할 수 있나요?[[what-sizes-of-tpu-are-available]]\n-\n-단일 TPU(v2-8/v3-8/v4-8)는 8개의 복제본(replicas)을 실행합니다. TPU는 수백 또는 수천 개의 복제본을 동시에 실행할 수 있는 **pod**로 존재합니다. 단일 TPU를 하나 이상 사용하지만 전체 Pod보다 적게 사용하는 경우(예를 들면, v3-32), TPU 구성을 **pod 슬라이스**라고 합니다.\n-\n-Colab을 통해 무료 TPU에 이용하는 경우, 기본적으로 단일 v2-8 TPU를 제공받습니다.\n-\n-### XLA에 대해 들어본 적이 있습니다. XLA란 무엇이고 TPU와 어떤 관련이 있나요?[[i-keep-hearing-about-this-xla-thing-whats-xla-and-how-does-it-relate-to-tpus]]\n-\n-XLA는 최적화 컴파일러로, TensorFlow와 JAX에서 모두 사용됩니다. JAX에서는 유일한 컴파일러이지만, TensorFlow에서는 선택 사항입니다(하지만 TPU에서는 필수입니다!). Keras 모델을 훈련할 때 이를 활성화하는 가장 쉬운 방법은 `jit_compile=True` 인수를 `model.compile()`에 전달하는 것입니다. 오류가 없고 성능이 양호하다면, TPU로 전환할 준비가 되었다는 좋은 신호입니다!\n-\n-TPU에서 디버깅하는 것은 대개 CPU/GPU보다 조금 더 어렵기 때문에, TPU에서 시도하기 전에 먼저 XLA로 CPU/GPU에서 코드를 실행하는 것을 권장합니다. 물론 오래 학습할 필요는 없습니다. 즉, 모델과 데이터 파이프라인이 예상대로 작동하는지 확인하기 위해 몇 단계만 거치면 됩니다.\n-\n-<Tip>\n-\n-XLA로 컴파일된 코드는 대체로 더 빠릅니다. 따라서 TPU에서 실행할 계획이 없더라도, `jit_compile=True`를 추가하면 성능이 향상될 수 있습니다. 하지만 XLA 호환성에 대한 아래 주의 사항을 반드시 확인하세요!\n-\n-</Tip>\n-\n-<Tip warning={true}>\n-\n-**뼈아픈 경험에서 얻은 팁:** `jit_compile=True`를 사용하면 속도를 높이고 CPU/GPU 코드가 XLA와 호환되는지 검증할 수 있는 좋은 방법이지만, 실제 TPU에서 훈련할 때 그대로 남겨두면 많은 문제를 초래할 수 있습니다. XLA 컴파일은 TPU에서 암시적으로 이뤄지므로, 실제 TPU에서 코드를 실행하기 전에 해당 줄을 제거하는 것을 잊지 마세요!\n-\n-</Tip>\n-\n-### 제 XLA 모델과 호환하려면 어떻게 해야 하나요?[[how-do-i-make-my-model-xla-compatible]]\n-\n-대부분의 경우, 여러분의 코드는 이미 XLA와 호환될 것입니다! 그러나 표준 TensorFlow에서 작동하지만, XLA에서는 작동하지 않는 몇 가지 사항이 있습니다. 이를 아래 세 가지 핵심 규칙으로 간추렸습니다:\n-\n-<Tip>\n-\n-**특수한 HuggingFace 팁🤗:** 저희는 TensorFlow 모델과 손실 함수를 XLA와 호환되도록 재작성하는 데 많은 노력을 기울였습니다. 저희의 모델과 손실 함수는 대개 기본적으로 규칙 #1과 #2를 따르므로 `transformers` 모델을 사용하는 경우, 이를 건너뛸 수 있습니다. 하지만 자체 모델과 손실 함수를 작성할 때는 이러한 규칙을 잊지 마세요!\n-\n-</Tip>\n-\n-#### XLA 규칙 #1: 코드에서 “데이터 종속 조건문”을 사용할 수 없습니다[[xla-rule-1-your-code-cannot-have-datadependent-conditionals]]\n-\n-어떤 `if`문도 `tf.Tensor` 내부의 값에 종속될 수 없다는 것을 의미합니다. 예를 들어, 이 코드 블록은 XLA로 컴파일할 수 없습니다!\n-\n-```python\n-if tf.reduce_sum(tensor) > 10:\n-    tensor = tensor / 2.0\n-```\n-\n-처음에는 매우 제한적으로 보일 수 있지만, 대부분의 신경망 코드에서는 이를 수행할 필요가 없습니다. `tf.cond`를 사용하거나([여기](https://www.tensorflow.org/api_docs/python/tf/cond) 문서를 참조), 다음과 같이 조건문을 제거하고 대신 지표 변수를 사용하는 영리한 수학 트릭을 찾아내어 이 제한을 우회할 수 있습니다:\n-\n-```python\n-sum_over_10 = tf.cast(tf.reduce_sum(tensor) > 10, tf.float32)\n-tensor = tensor / (1.0 + sum_over_10)\n-```\n-\n-이 코드는 위의 코드와 정확히 동일한 효과를 구현하지만, 조건문을 제거하여 문제 없이 XLA로 컴파일되도록 합니다!\n-\n-#### XLA 규칙 #2: 코드에서 \"데이터 종속 크기\"를 가질 수 없습니다[[xla-rule-2-your-code-cannot-have-datadependent-shapes]]\n-\n-코드에서 모든 `tf.Tensor` 객체의 크기가 해당 값에 종속될 수 없다는 것을 의미합니다. 예를 들어, `tf.unique` 함수는 입력에서 각 고유 값의 인스턴스 하나를 포함하는 `tensor`를 반환하기 때문에 XLA로 컴파일할 수 없습니다. 이 출력의 크기는 입력 `Tensor`가 얼마나 반복적인지에 따라 분명히 달라질 것이므로, XLA는 이를 처리하지 못합니다!\n-\n-일반적으로, 대부분의 신경망 코드는 기본값으로 규칙 2를 따릅니다. 그러나 문제가 되는 몇 가지 대표적인 사례가 있습니다. 가장 흔한 사례 중 하나는 **레이블 마스킹**을 사용하여 손실(loss)을 계산할 때, 해당 위치를 무시하도록 나타내기 위해 레이블을 음수 값으로 설정하는 경우입니다. 레이블 마스킹을 지원하는 NumPy나 PyTorch 손실 함수를 보면 [불 인덱싱](https://numpy.org/doc/stable/user/basics.indexing.html#boolean-array-indexing)을 사용하는 다음과 같은 코드를 자주 접할 수 있습니다:\n-\n-```python\n-label_mask = labels >= 0\n-masked_outputs = outputs[label_mask]\n-masked_labels = labels[label_mask]\n-loss = compute_loss(masked_outputs, masked_labels)\n-mean_loss = torch.mean(loss)\n-```\n-\n-이 코드는 NumPy나 PyTorch에서는 문제 없이 작동하지만, XLA에서는 손상됩니다! 왜 그럴까요? 얼마나 많은 위치가 마스킹되는지에 따라 `masked_outputs`와 `masked_labels`의 크기가 달라져서, **데이터 종속 크기**가 되기 때문입니다. 그러나 규칙 #1과 마찬가지로, 이 코드를 다시 작성하면 데이터 종속적 모양 크기가 정확히 동일한 출력을 산출할 수 있습니다.\n-\n-```python\n-label_mask = tf.cast(labels >= 0, tf.float32)\n-loss = compute_loss(outputs, labels)\n-loss = loss * label_mask  # Set negative label positions to 0\n-mean_loss = tf.reduce_sum(loss) / tf.reduce_sum(label_mask)\n-```\n-\n-여기서, 모든 위치에 대한 손실을 계산하지만, 평균을 계산할 때 분자와 분모 모두에서 마스크된 위치를 0으로 처리합니다. 이는 데이터 종속 크기를 방지하고 XLA 호환성을 유지하면서 첫 번째 블록과 정확히 동일한 결과를 산출합니다. 규칙 #1에서와 동일한 트릭을 사용하여 `tf.bool`을 `tf.float32`로 변환하고 이를 지표 변수로 사용합니다. 해당 트릭은 매우 유용하며, 자체 코드를 XLA로 변환해야 할 경우 기억해 두세요!\n-\n-#### XLA 규칙 #3: XLA는 각기 다른 입력 크기가 나타날 때마다 모델을 다시 컴파일해야 합니다[[xla-rule-3-xla-will-need-to-recompile-your-model-for-every-different-input-shape-it-sees]]\n-\n-이것은 가장 큰 문제입니다. 입력 크기가 매우 가변적인 경우, XLA는 모델을 반복해서 다시 컴파일해야 하므로 성능에 큰 문제가 발생할 수 있습니다. 이 문제는 토큰화 후 입력 텍스트의 길이가 가변적인 NLP 모델에서 주로 발생합니다. 다른 모달리티에서는 정적 크기가 더 흔하며, 해당 규칙이 훨씬 덜 문제시 됩니다.\n-\n-규칙 #3을 어떻게 우회할 수 있을까요? 핵심은 **패딩**입니다. 모든 입력을 동일한 길이로 패딩한 다음, `attention_mask`를 사용하면 어떤 XLA 문제도 없이 가변 크기에서 가져온 것과 동일한 결과를 가져올 수 있습니다. 그러나 과도한 패딩은 심각한 속도 저하를 야기할 수도 있습니다. 모든 샘플을 전체 데이터 세트의 최대 길이로 패딩하면, 무한한 패딩 토큰으로 구성된 배치가 생성되어 많은 연산과 메모리가 낭비될 수 있습니다!\n-\n-이 문제에 대한 완벽한 해결책은 없습니다. 하지만, 몇 가지 트릭을 시도해볼 수 있습니다. 한 가지 유용한 트릭은 **샘플 배치를 32 또는 64 토큰과 같은 숫자의 배수까지 패딩하는 것입니다.** 이는 토큰 수가 소폭 증가하지만, 모든 입력 크기가 32 또는 64의 배수여야 하기 때문에 고유한 입력 크기의 수가 대폭 줄어듭니다. 고유한 입력 크기가 적다는 것은 XLA 컴파일 횟수가 적어진다는 것을 의미합니다!\n-\n-<Tip>\n-\n-**🤗특수한 HuggingFace 팁🤗:** 토크나이저와 데이터 콜레이터에 도움이 될 수 있는 메소드가 있습니다. 토크나이저를 불러올 때 `padding=\"max_length\"` 또는 `padding=\"longest\"`를 사용하여 패딩된 데이터를 출력하도록 할 수 있습니다. 토크나이저와 데이터 콜레이터는 나타나는 고유한 입력 크기의 수를 줄이기 위해 사용할 수 있는 `pad_to_multiple_of` 인수도 있습니다!\n-\n-</Tip>\n-\n-### 실제 TPU로 모델을 훈련하려면 어떻게 해야 하나요?[[how-do-i-actually-train-my-model-on-tpu]]\n-\n-훈련이 XLA와 호환되고 (TPU 노드/Colab을 사용하는 경우) 데이터 세트가 적절하게 준비되었다면, TPU에서 실행하는 것은 놀랍도록 쉽습니다! 코드에서 몇 줄만 추가하여, TPU를 초기화하고 모델과 데이터 세트가 `TPUStrategy` 범위 내에 생성되도록 변경하면 됩니다. [우리의 TPU 예제 노트북](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)을 참조하여 실제로 작동하는 모습을 확인해 보세요!\n-\n-### 요약[[summary]]\n-\n-여기에 많은 내용이 포함되어 있으므로, TPU 훈련을 위한 모델을 준비할 때 따를 수 있는 간략한 체크리스트로 요약해 보겠습니다:\n-\n-- 코드가 XLA의 세 가지 규칙을 따르는지 확인합니다.\n-- CPU/GPU에서 `jit_compile=True`로 모델을 컴파일하고 XLA로 훈련할 수 있는지 확인합니다.\n-- 데이터 세트를 메모리에 가져오거나 TPU 호환 데이터 세트를 가져오는 방식을 사용합니다([노트북](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) 참조)\n-- 코드를 Colab(accelerator가 “TPU”로 설정됨) 또는 Google Cloud의 TPU VM으로 마이그레이션합니다.\n-- TPU 초기화 코드를 추가합니다([노트북](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) 참조)\n-- `TPUStrategy`를 생성하고 데이터 세트를 가져오는 것과 모델 생성이 `strategy.scope()` 내에 있는지 확인합니다([노트북](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) 참조)\n-- TPU로 이동할 때 `jit_compile=True`를 다시 설정하는 것을 잊지 마세요!\n-- 🙏🙏🙏🥺🥺🥺\n-- model.fit()을 불러옵니다.\n-- 여러분이 해냈습니다!\n\\ No newline at end of file"
        },
        {
            "sha": "226bd5f249af5da28d4af4c33251f3eb2d588721",
            "filename": "docs/source/ko/performance.md",
            "status": "removed",
            "additions": 0,
            "deletions": 96,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fperformance.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fperformance.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperformance.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,96 +0,0 @@\n-<!---\n-Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# 성능 및 확장성 [[performance-and-scalability]]\n-\n-점점 더 큰 규모의 트랜스포머 모델을 훈련하고 프로덕션에 배포하는 데에는 다양한 어려움이 따릅니다. 훈련 중에는 모델이 사용 가능한 GPU 메모리보다 더 많은 메모리를 필요로 하거나 훈련 속도가 매우 느릴 수 있으며, 추론을 위해 배포할 때는 제품 환경에서 요구되는 처리량으로 인해 과부하가 발생할 수 있습니다. 이 문서는 이러한 문제를 극복하고 사용 사례에 가장 적합한 설정을 찾도록 도움을 주기 위해 설계되었습니다. 훈련과 추론으로 가이드를 분할했는데, 이는 각각 다른 문제와 해결 방법이 있기 때문입니다. 그리고 각 가이드에는 다양한 종류의 하드웨어 설정에 대한 별도의 가이드가 있습니다(예: 훈련을 위한 단일 GPU vs 다중 GPU 또는 추론을 위한 CPU vs GPU).\n-\n-![perf_overview](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/perf_overview.png)\n-\n-이 문서는 사용자의 상황에 유용할 수 있는 방법들에 대한 개요 및 시작점 역할을 합니다.\n-\n-## 훈련 [[training]]\n-\n-효율적인 트랜스포머 모델 훈련에는 GPU나 TPU와 같은 가속기가 필요합니다. 가장 일반적인 경우는 단일 GPU만 사용하는 경우지만, 다중 GPU 및 CPU 훈련에 대한 섹션도 있습니다(곧 더 많은 내용이 추가될 예정).\n-\n-<Tip>\n-\n- 참고: 단일 GPU 섹션에서 소개된 대부분의 전략(예: 혼합 정밀도 훈련 또는 그라디언트 누적)은 일반적인 모델 훈련에도 적용되므로, 다중 GPU나 CPU 훈련과 같은 섹션을 살펴보기 전에 꼭 참고하시길 바랍니다.\n-\n-</Tip>\n-\n-### 단일 GPU [[single-gpu]]\n-\n-단일 GPU에서 대규모 모델을 훈련하는 것은 어려울 수 있지만, 이를 가능하게 하는 여러 가지 도구와 방법이 있습니다. 이 섹션에서는 혼합 정밀도 훈련, 그라디언트 누적 및 체크포인팅, 효율적인 옵티마이저, 최적의 배치 크기를 결정하기 위한 전략 등에 대해 논의합니다.\n-\n-[단일 GPU 훈련 섹션으로 이동](perf_train_gpu_one)\n-\n-### 다중 GPU [[multigpu]]\n-\n-단일 GPU에서 훈련하는 것이 너무 느리거나 대규모 모델에 적합하지 않은 경우도 있습니다. 다중 GPU 설정으로 전환하는 것은 논리적인 단계이지만, 여러 GPU에서 한 번에 훈련하려면 각 GPU마다 모델의 전체 사본을 둘지, 혹은 모델 자체도 여러 GPU에 분산하여 둘지 등 새로운 결정을 내려야 합니다. 이 섹션에서는 데이터, 텐서 및 파이프라인 병렬화에 대해 살펴봅니다.\n-\n-[다중 GPU 훈련 섹션으로 이동](perf_train_gpu_many)\n-\n-### CPU [[cpu]]\n-\n-\n-[CPU 훈련 섹션으로 이동](perf_train_cpu)\n-\n-\n-### TPU [[tpu]]\n-\n-[_곧 제공될 예정_](perf_train_tpu)\n-\n-### 특수한 하드웨어 [[specialized-hardware]]\n-\n-[_곧 제공될 예정_](perf_train_special)\n-\n-## 추론 [[inference]]\n-\n-제품 및 서비스 환경에서 대규모 모델을 효율적으로 추론하는 것은 모델을 훈련하는 것만큼 어려울 수 있습니다. 이어지는 섹션에서는 CPU 및 단일/다중 GPU 설정에서 추론을 진행하는 단계를 살펴봅니다.\n-\n-### CPU [[cpu]]\n-\n-[CPU 추론 섹션으로 이동](perf_infer_cpu)\n-\n-### 단일 GPU [[single-gpu]]\n-\n-[단일 GPU 추론 섹션으로 이동](perf_infer_gpu_one)\n-\n-### 다중 GPU [[multigpu]]\n-\n-[다중 GPU 추론 섹션으로 이동](perf_infer_gpu_many)\n-\n-### 특수한 하드웨어 [[specialized-hardware]]\n-\n-[_곧 제공될 예정_](perf_infer_special)\n-\n-## 하드웨어 [[hardware]]\n-\n-하드웨어 섹션에서는 자신만의 딥러닝 장비를 구축할 때 유용한 팁과 요령을 살펴볼 수 있습니다.\n-\n-[하드웨어 섹션으로 이동](perf_hardware)\n-\n-\n-## 기여하기 [[contribute]]\n-\n-이 문서는 완성되지 않은 상태이며, 추가해야 할 내용이나 수정 사항이 많이 있습니다. 따라서 추가하거나 수정할 내용이 있으면 주저하지 말고 PR을 열어 주시거나, 자세한 내용을 논의하기 위해 Issue를 시작해 주시기 바랍니다.\n-\n-A가 B보다 좋다고 하는 기여를 할 때는, 재현 가능한 벤치마크와/또는 해당 정보의 출처 링크를 포함해주세요(당신으로부터의 직접적인 정보가 아닌 경우).\n\\ No newline at end of file"
        },
        {
            "sha": "4466b63585723d49a5165135c40ec287d25ff907",
            "filename": "docs/source/ko/preprocessing.md",
            "status": "removed",
            "additions": 0,
            "deletions": 539,
            "changes": 539,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fpreprocessing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fpreprocessing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fpreprocessing.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,539 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# 전처리[[preprocess]]\n-\n-[[open-in-colab]]\n-\n-모델을 훈련하려면 데이터 세트를 모델에 맞는 입력 형식으로 전처리해야 합니다. 텍스트, 이미지 또는 오디오인지 관계없이 데이터를 텐서 배치로 변환하고 조립할 필요가 있습니다. 🤗 Transformers는 모델에 대한 데이터를 준비하는 데 도움이 되는 일련의 전처리 클래스를 제공합니다. 이 튜토리얼에서는 다음 내용을 배울 수 있습니다:\n-\n-* 텍스트는 [Tokenizer](./main_classes/tokenizer)를 사용하여 토큰 시퀀스로 변환하고 토큰의 숫자 표현을 만든 후 텐서로 조립합니다.\n-* 음성 및 오디오는 [Feature extractor](./main_classes/feature_extractor)를 사용하여 오디오 파형에서 시퀀스 특성을 파악하여 텐서로 변환합니다.\n-* 이미지 입력은 [ImageProcessor](./main_classes/image)을 사용하여 이미지를 텐서로 변환합니다.\n-* 멀티모달 입력은 [Processor](./main_classes/processors)을 사용하여 토크나이저와 특성 추출기 또는 이미지 프로세서를 결합합니다.\n-\n-<Tip>\n-\n-`AutoProcessor`는 **언제나** 작동하여 토크나이저, 이미지 프로세서, 특성 추출기 또는 프로세서 등 사용 중인 모델에 맞는 클래스를 자동으로 선택합니다.\n-\n-</Tip>\n-\n-시작하기 전에 🤗 Datasets를 설치하여 실험에 사용할 데이터를 불러올 수 있습니다:\n-\n-```bash\n-pip install datasets\n-```\n-\n-## 자연어처리[[natural-language-processing]]\n-\n-<Youtube id=\"Yffk5aydLzg\"/>\n-\n-텍스트 데이터를 전처리하기 위한 기본 도구는 [tokenizer](main_classes/tokenizer)입니다. 토크나이저는 일련의 규칙에 따라 텍스트를 *토큰*으로 나눕니다. 토큰은 숫자로 변환되고 텐서는 모델 입력이 됩니다. 모델에 필요한 추가 입력은 토크나이저에 의해 추가됩니다.\n-\n-<Tip>\n-\n-사전훈련된 모델을 사용할 계획이라면 모델과 함께 사전훈련된 토크나이저를 사용하는 것이 중요합니다. 이렇게 하면 텍스트가 사전훈련 말뭉치와 동일한 방식으로 분할되고 사전훈련 중에 동일한 해당 토큰-인덱스 쌍(일반적으로 *vocab*이라고 함)을 사용합니다.\n-\n-</Tip>\n-\n-시작하려면 [`AutoTokenizer.from_pretrained`] 메소드를 사용하여 사전훈련된 토크나이저를 불러오세요. 모델과 함께 사전훈련된 *vocab*을 다운로드합니다:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-```\n-\n-그 다음으로 텍스트를 토크나이저에 넣어주세요:\n-\n-```py\n->>> encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n->>> print(encoded_input)\n-{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102],\n- 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n- 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n-```\n-\n-토크나이저는 세 가지 중요한 항목을 포함한 딕셔너리를 반환합니다:\n-\n-* [input_ids](glossary#input-ids)는 문장의 각 토큰에 해당하는 인덱스입니다.\n-* [attention_mask](glossary#attention-mask)는 토큰을 처리해야 하는지 여부를 나타냅니다.\n-* [token_type_ids](glossary#token-type-ids)는 두 개 이상의 시퀀스가 있을 때 토큰이 속한 시퀀스를 식별합니다.\n-\n-`input_ids`를 디코딩하여 입력을 반환합니다:\n-\n-```py\n->>> tokenizer.decode(encoded_input[\"input_ids\"])\n-'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'\n-```\n-\n-토크나이저가 두 개의 특수한 토큰(분류 토큰 `CLS`와 분할 토큰 `SEP`)을 문장에 추가했습니다.\n-모든 모델에 특수한 토큰이 필요한 것은 아니지만, 필요하다면 토크나이저가 자동으로 추가합니다.\n-\n-전처리할 문장이 여러 개 있는 경우에는 리스트로 토크나이저에 전달합니다:\n-\n-```py\n->>> batch_sentences = [\n-...     \"But what about second breakfast?\",\n-...     \"Don't think he knows about second breakfast, Pip.\",\n-...     \"What about elevensies?\",\n-... ]\n->>> encoded_inputs = tokenizer(batch_sentences)\n->>> print(encoded_inputs)\n-{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102],\n-               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n-               [101, 1327, 1164, 5450, 23434, 136, 102]],\n- 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],\n-                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-                    [0, 0, 0, 0, 0, 0, 0]],\n- 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],\n-                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n-                    [1, 1, 1, 1, 1, 1, 1]]}\n-```\n-\n-### 패딩[[pad]]\n-\n-모델 입력인 텐서는 모양이 균일해야 하지만, 문장의 길이가 항상 같지는 않기 때문에 문제가 될 수 있습니다. 패딩은 짧은 문장에 특수한 *패딩 토큰*을 추가하여 텐서를 직사각형 모양이 되도록 하는 전략입니다.\n-\n-`padding` 매개변수를 `True`로 설정하여 배치 내의 짧은 시퀀스를 가장 긴 시퀀스에 맞춰 패딩합니다.\n-\n-```py\n->>> batch_sentences = [\n-...     \"But what about second breakfast?\",\n-...     \"Don't think he knows about second breakfast, Pip.\",\n-...     \"What about elevensies?\",\n-... ]\n->>> encoded_input = tokenizer(batch_sentences, padding=True)\n->>> print(encoded_input)\n-{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n-               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n-               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n- 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n- 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n-                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n-                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}\n-```\n-\n-길이가 짧은 첫 문장과 세 번째 문장이 이제 `0`으로 채워졌습니다.\n-\n-### 잘라내기[[truncation]]\n-\n-한편, 때로는 시퀀스가 모델에서 처리하기에 너무 길 수도 있습니다. 이 경우, 시퀀스를 더 짧게 줄일 필요가 있습니다.\n-\n-모델에서 허용하는 최대 길이로 시퀀스를 자르려면 `truncation` 매개변수를 `True`로 설정하세요:\n-\n-```py\n->>> batch_sentences = [\n-...     \"But what about second breakfast?\",\n-...     \"Don't think he knows about second breakfast, Pip.\",\n-...     \"What about elevensies?\",\n-... ]\n->>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)\n->>> print(encoded_input)\n-{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n-               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n-               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n- 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n- 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n-                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n-                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}\n-```\n-\n-<Tip>\n-\n-다양한 패딩과 잘라내기 인수에 대해 더 알아보려면 [패딩과 잘라내기](./pad_truncation) 개념 가이드를 확인해보세요.\n-\n-</Tip>\n-\n-### 텐서 만들기[[build-tensors]]\n-\n-마지막으로, 토크나이저가 모델에 공급되는 실제 텐서를 반환하도록 합니다.\n-\n-`return_tensors` 매개변수를 PyTorch의 경우 `pt`, TensorFlow의 경우 `tf`로 설정하세요:\n-\n-<frameworkcontent>\n-<pt>\n-\n-```py\n->>> batch_sentences = [\n-...     \"But what about second breakfast?\",\n-...     \"Don't think he knows about second breakfast, Pip.\",\n-...     \"What about elevensies?\",\n-... ]\n->>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n->>> print(encoded_input)\n-{'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n-                      [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n-                      [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]),\n- 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n- 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n-                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n-                           [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n-```\n-</pt>\n-<tf>\n-```py\n->>> batch_sentences = [\n-...     \"But what about second breakfast?\",\n-...     \"Don't think he knows about second breakfast, Pip.\",\n-...     \"What about elevensies?\",\n-... ]\n->>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n->>> print(encoded_input)\n-{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n-       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n-       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n-      dtype=int32)>,\n- 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n- 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n-       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n-       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n-```\n-</tf>\n-</frameworkcontent>\n-\n-## 오디오[[audio]]\n-\n-오디오 작업은 모델에 맞는 데이터 세트를 준비하기 위해 [특성 추출기](main_classes/feature_extractor)가 필요합니다. 특성 추출기는 원시 오디오 데이터에서 특성를 추출하고 이를 텐서로 변환하는 것이 목적입니다.\n-\n-오디오 데이터 세트에 특성 추출기를 사용하는 방법을 보기 위해 [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) 데이터 세트를 가져오세요. (데이터 세트를 가져오는 방법은 🤗 [데이터 세트 튜토리얼](https://huggingface.co/docs/datasets/load_hub)에서 자세히 설명하고 있습니다.)\n-\n-```py\n->>> from datasets import load_dataset, Audio\n-\n->>> dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n-```\n-\n-`audio` 열의 첫 번째 요소에 접근하여 입력을 살펴보세요. `audio` 열을 호출하면 오디오 파일을 자동으로 가져오고 리샘플링합니다.\n-\n-```py\n->>> dataset[0][\"audio\"]\n-{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\n-         0.        ,  0.        ], dtype=float32),\n- 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n- 'sampling_rate': 8000}\n-```\n-\n-이렇게 하면 세 가지 항목이 반환됩니다:\n-\n-* `array`는 1D 배열로 가져와서 (필요한 경우) 리샘플링된 음성 신호입니다.\n-* `path`는 오디오 파일의 위치를 가리킵니다.\n-* `sampling_rate`는 음성 신호에서 초당 측정되는 데이터 포인트 수를 나타냅니다.\n-\n-이 튜토리얼에서는 [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) 모델을 사용합니다. 모델 카드를 보면 Wav2Vec2가 16kHz 샘플링된 음성 오디오를 기반으로 사전훈련된 것을 알 수 있습니다.\n-모델을 사전훈련하는 데 사용된 데이터 세트의 샘플링 레이트와 오디오 데이터의 샘플링 레이트가 일치해야 합니다. 데이터의 샘플링 레이트가 다르면 데이터를 리샘플링해야 합니다.\n-\n-1. 🤗 Datasets의 [`~datasets.Dataset.cast_column`] 메소드를 사용하여 샘플링 레이트를 16kHz로 업샘플링하세요:\n-\n-```py\n->>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n-```\n-\n-2. 오디오 파일을 리샘플링하기 위해 `audio` 열을 다시 호출합니다:\n-\n-```py\n->>> dataset[0][\"audio\"]\n-{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,\n-         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),\n- 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n- 'sampling_rate': 16000}\n-```\n-\n-다음으로, 입력을 정규화하고 패딩할 특성 추출기를 가져오세요. 텍스트 데이터의 경우, 더 짧은 시퀀스에 대해 `0`이 추가됩니다. 오디오 데이터에도 같은 개념이 적용됩니다.\n-특성 추출기는 배열에 `0`(묵음으로 해석)을 추가합니다.\n-\n-[`AutoFeatureExtractor.from_pretrained`]를 사용하여 특성 추출기를 가져오세요:\n-\n-```py\n->>> from transformers import AutoFeatureExtractor\n-\n->>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n-```\n-\n-오디오 `array`를 특성 추출기에 전달하세요. 또한, 발생할 수 있는 조용한 오류(silent errors)를 더 잘 디버깅할 수 있도록 특성 추출기에 `sampling_rate` 인수를 추가하는 것을 권장합니다.\n-\n-```py\n->>> audio_input = [dataset[0][\"audio\"][\"array\"]]\n->>> feature_extractor(audio_input, sampling_rate=16000)\n-{'input_values': [array([ 3.8106556e-04,  2.7506407e-03,  2.8015103e-03, ...,\n-        5.6335266e-04,  4.6588284e-06, -1.7142107e-04], dtype=float32)]}\n-```\n-\n-토크나이저와 마찬가지로 배치 내에서 가변적인 시퀀스를 처리하기 위해 패딩 또는 잘라내기를 적용할 수 있습니다. 이 두 개의 오디오 샘플의 시퀀스 길이를 확인해보세요:\n-\n-```py\n->>> dataset[0][\"audio\"][\"array\"].shape\n-(173398,)\n-\n->>> dataset[1][\"audio\"][\"array\"].shape\n-(106496,)\n-```\n-\n-오디오 샘플의 길이가 동일하도록 데이터 세트를 전처리하는 함수를 만드세요. 최대 샘플 길이를 지정하면 특성 추출기가 해당 길이에 맞춰 시퀀스를 패딩하거나 잘라냅니다:\n-\n-```py\n->>> def preprocess_function(examples):\n-...     audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n-...     inputs = feature_extractor(\n-...         audio_arrays,\n-...         sampling_rate=16000,\n-...         padding=True,\n-...         max_length=100000,\n-...         truncation=True,\n-...     )\n-...     return inputs\n-```\n-\n-`preprocess_function`을 데이터 세트의 처음 예시 몇 개에 적용해보세요:\n-\n-```py\n->>> processed_dataset = preprocess_function(dataset[:5])\n-```\n-\n-이제 샘플 길이가 모두 같고 지정된 최대 길이에 맞게 되었습니다. 드디어 전처리된 데이터 세트를 모델에 전달할 수 있습니다!\n-\n-```py\n->>> processed_dataset[\"input_values\"][0].shape\n-(100000,)\n-\n->>> processed_dataset[\"input_values\"][1].shape\n-(100000,)\n-```\n-\n-## 컴퓨터 비전[[computer-vision]]\n-\n-컴퓨터 비전 작업의 경우, 모델에 대한 데이터 세트를 준비하기 위해 [이미지 프로세서](main_classes/image_processor)가 필요합니다.\n-이미지 전처리는 이미지를 모델이 예상하는 입력으로 변환하는 여러 단계로 이루어집니다.\n-이러한 단계에는 크기 조정, 정규화, 색상 채널 보정, 이미지의 텐서 변환 등이 포함됩니다.\n-\n-<Tip>\n-\n-이미지 전처리는 이미지 증강 기법을 몇 가지 적용한 뒤에 할 수도 있습니다.\n-이미지 전처리 및 이미지 증강은 모두 이미지 데이터를 변형하지만, 서로 다른 목적을 가지고 있습니다:\n-\n-* 이미지 증강은 과적합(over-fitting)을 방지하고 모델의 견고함(resiliency)을 높이는 데 도움이 되는 방식으로 이미지를 수정합니다.\n-밝기와 색상 조정, 자르기, 회전, 크기 조정, 확대/축소 등 다양한 방법으로 데이터를 증강할 수 있습니다.\n-그러나 증강으로 이미지의 의미가 바뀌지 않도록 주의해야 합니다.\n-* 이미지 전처리는 이미지가 모델이 예상하는 입력 형식과 일치하도록 보장합니다.\n-컴퓨터 비전 모델을 미세 조정할 때 이미지는 모델이 초기에 훈련될 때와 정확히 같은 방식으로 전처리되어야 합니다.\n-\n-이미지 증강에는 원하는 라이브러리를 무엇이든 사용할 수 있습니다. 이미지 전처리에는 모델과 연결된 `ImageProcessor`를 사용합니다.\n-\n-</Tip>\n-\n-[food101](https://huggingface.co/datasets/food101) 데이터 세트를 가져와서 컴퓨터 비전 데이터 세트에서 이미지 프로세서를 어떻게 사용하는지 알아보세요.\n-데이터 세트를 불러오는 방법은 🤗 [데이터 세트 튜토리얼](https://huggingface.co/docs/datasets/load_hub)을 참고하세요.\n-\n-<Tip>\n-\n-데이터 세트가 상당히 크기 때문에 🤗 Datasets의 `split` 매개변수를 사용하여 훈련 세트에서 작은 샘플만 가져오세요!\n-\n-</Tip>\n-\n-```py\n->>> from datasets import load_dataset\n-\n->>> dataset = load_dataset(\"food101\", split=\"train[:100]\")\n-```\n-\n-다음으로, 🤗 Datasets의 [`image`](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=image#datasets.Image)로 이미지를 확인해보세요:\n-\n-```py\n->>> dataset[0][\"image\"]\n-```\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png\"/>\n-</div>\n-\n-[`AutoImageProcessor.from_pretrained`]로 이미지 프로세서를 가져오세요:\n-\n-```py\n->>> from transformers import AutoImageProcessor\n-\n->>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n-```\n-\n-먼저 이미지 증강 단계를 추가해 봅시다. 아무 라이브러리나 사용해도 괜찮지만, 이번 튜토리얼에서는 torchvision의 [`transforms`](https://pytorch.org/vision/stable/transforms.html) 모듈을 사용하겠습니다.\n-다른 데이터 증강 라이브러리를 사용해보고 싶다면, [Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb) 또는 [Kornia notebooks](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)에서 어떻게 사용하는지 배울 수 있습니다.\n-\n-1. [`Compose`](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)로  [`RandomResizedCrop`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html)와 [`ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html) 등 변환을 몇 가지 연결하세요.\n-참고로 크기 조정에 필요한 이미지의 크기 요구사항은 `image_processor`에서 가져올 수 있습니다.\n-일부 모델은 정확한 높이와 너비를 요구하지만, 제일 짧은 변의 길이(`shortest_edge`)만 정의된 모델도 있습니다.\n-\n-```py\n->>> from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose\n-\n->>> size = (\n-...     image_processor.size[\"shortest_edge\"]\n-...     if \"shortest_edge\" in image_processor.size\n-...     else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n-... )\n-\n->>> _transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])\n-```\n-\n-2. 모델은 입력으로 [`pixel_values`](model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values)를 받습니다.\n-`ImageProcessor`는 이미지 정규화 및 적절한 텐서 생성을 처리할 수 있습니다.\n-배치 이미지에 대한 이미지 증강 및 이미지 전처리를 결합하고 `pixel_values`를 생성하는 함수를 만듭니다:\n-\n-```py\n->>> def transforms(examples):\n-...     images = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n-...     examples[\"pixel_values\"] = image_processor(images, do_resize=False, return_tensors=\"pt\")[\"pixel_values\"]\n-...     return examples\n-```\n-\n-<Tip>\n-\n-위의 예에서는 이미지 증강 중에 이미지 크기를 조정했기 때문에 `do_resize=False`로 설정하고, 해당 `image_processor`에서 `size` 속성을 활용했습니다.\n-이미지 증강 중에 이미지 크기를 조정하지 않은 경우 이 매개변수를 생략하세요.\n-기본적으로는 `ImageProcessor`가 크기 조정을 처리합니다.\n-\n-증강 변환 과정에서 이미지를 정규화하려면 `image_processor.image_mean` 및 `image_processor.image_std` 값을 사용하세요.\n-\n-</Tip>\n-\n-3. 🤗 Datasets의 [`set_transform`](https://huggingface.co/docs/datasets/process#format-transform)를 사용하여 실시간으로 변환을 적용합니다:\n-\n-```py\n->>> dataset.set_transform(transforms)\n-```\n-\n-4. 이제 이미지에 접근하면 이미지 프로세서가 `pixel_values`를 추가한 것을 알 수 있습니다.\n-드디어 처리된 데이터 세트를 모델에 전달할 수 있습니다!\n-\n-```py\n->>> dataset[0].keys()\n-```\n-\n-다음은 변형이 적용된 후의 이미지입니다. 이미지가 무작위로 잘려나갔고 색상 속성이 다릅니다.\n-\n-```py\n->>> import numpy as np\n->>> import matplotlib.pyplot as plt\n-\n->>> img = dataset[0][\"pixel_values\"]\n->>> plt.imshow(img.permute(1, 2, 0))\n-```\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png\"/>\n-</div>\n-\n-<Tip>\n-\n-`ImageProcessor`는 객체 감지, 시맨틱 세그멘테이션(semantic segmentation), 인스턴스 세그멘테이션(instance segmentation), 파놉틱 세그멘테이션(panoptic segmentation)과 같은 작업에 대한 후처리 방법을 제공합니다.\n-이러한 방법은 모델의 원시 출력을 경계 상자나 세그멘테이션 맵과 같은 의미 있는 예측으로 변환해줍니다.\n-\n-</Tip>\n-\n-### 패딩[[pad]]\n-\n-예를 들어, [DETR](./model_doc/detr)와 같은 경우에는 모델이 훈련할 때 크기 조정 증강을 적용합니다.\n-이로 인해 배치 내 이미지 크기가 달라질 수 있습니다.\n-[`DetrImageProcessor`]의 [`DetrImageProcessor.pad`]를 사용하고 사용자 정의 `collate_fn`을 정의해서 배치 이미지를 처리할 수 있습니다.\n-\n-```py\n->>> def collate_fn(batch):\n-...     pixel_values = [item[\"pixel_values\"] for item in batch]\n-...     encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n-...     labels = [item[\"labels\"] for item in batch]\n-...     batch = {}\n-...     batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n-...     batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n-...     batch[\"labels\"] = labels\n-...     return batch\n-```\n-\n-## 멀티모달[[multimodal]]\n-\n-멀티모달 입력이 필요한 작업의 경우, 모델에 데이터 세트를 준비하기 위한 [프로세서](main_classes/processors)가 필요합니다.\n-프로세서는 토크나이저와 특성 추출기와 같은 두 가지 처리 객체를 결합합니다.\n-\n-[LJ Speech](https://huggingface.co/datasets/lj_speech) 데이터 세트를 가져와서 자동 음성 인식(ASR)을 위한 프로세서를 사용하는 방법을 확인하세요.\n-(데이터 세트를 가져오는 방법에 대한 자세한 내용은 🤗 [데이터 세트 튜토리얼](https://huggingface.co/docs/datasets/load_hub)에서 볼 수 있습니다.)\n-\n-```py\n->>> from datasets import load_dataset\n-\n->>> lj_speech = load_dataset(\"lj_speech\", split=\"train\")\n-```\n-\n-자동 음성 인식(ASR)에서는 `audio`와 `text`에만 집중하면 되므로, 다른 열들은 제거할 수 있습니다:\n-\n-```py\n->>> lj_speech = lj_speech.map(remove_columns=[\"file\", \"id\", \"normalized_text\"])\n-```\n-\n-이제 `audio`와 `text`열을 살펴보세요:\n-\n-```py\n->>> lj_speech[0][\"audio\"]\n-{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,\n-         7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),\n- 'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav',\n- 'sampling_rate': 22050}\n-\n->>> lj_speech[0][\"text\"]\n-'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'\n-```\n-\n-기존에 사전훈련된 모델에서 사용된 데이터 세트와 새로운 오디오 데이터 세트의 샘플링 레이트를 일치시키기 위해 오디오 데이터 세트의 샘플링 레이트를 [리샘플링](preprocessing#audio)해야 합니다!\n-\n-```py\n->>> lj_speech = lj_speech.cast_column(\"audio\", Audio(sampling_rate=16_000))\n-```\n-\n-[`AutoProcessor.from_pretrained`]로 프로세서를 가져오세요:\n-\n-```py\n->>> from transformers import AutoProcessor\n-\n->>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n-```\n-\n-1. `array`에 들어 있는 오디오 데이터를 `input_values`로 변환하고 `text`를 토큰화하여 `labels`로 변환하는 함수를 만듭니다.\n-모델의 입력은 다음과 같습니다:\n-\n-```py\n->>> def prepare_dataset(example):\n-...     audio = example[\"audio\"]\n-\n-...     example.update(processor(audio=audio[\"array\"], text=example[\"text\"], sampling_rate=16000))\n-\n-...     return example\n-```\n-\n-2. 샘플을 `prepare_dataset` 함수에 적용하세요:\n-\n-```py\n->>> prepare_dataset(lj_speech[0])\n-```\n-\n-이제 프로세서가 `input_values`와 `labels`를 추가하고, 샘플링 레이트도 올바르게 16kHz로 다운샘플링했습니다.\n-드디어 처리된 데이터 세트를 모델에 전달할 수 있습니다!"
        },
        {
            "sha": "18aafc28a1612d3c59cc39d9aeefc628c1ef697d",
            "filename": "docs/source/ko/sagemaker.md",
            "status": "removed",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fsagemaker.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Fsagemaker.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fsagemaker.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,28 +0,0 @@\n-<!---\n-Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Amazon SageMaker에서 학습 실행하기[[run-training-on-amazon-sagemaker]]\n-\n-문서가 [hf.co/docs/sagemaker](https://huggingface.co/docs/sagemaker)로 이동되었습니다. 이 페이지는 `transformers` 5.0 에서 삭제될 예정입니다. \n-\n-### 목차[[table-of-content]]\n-\n-- [Train Hugging Face models on Amazon SageMaker with the SageMaker Python SDK](https://huggingface.co/docs/sagemaker/train)\n-- [Deploy Hugging Face models to Amazon SageMaker with the SageMaker Python SDK](https://huggingface.co/docs/sagemaker/inference)"
        },
        {
            "sha": "a0e60c60924b99989eb14bcc322297c8d3bcc5d6",
            "filename": "docs/source/ko/task_summary.md",
            "status": "removed",
            "additions": 0,
            "deletions": 341,
            "changes": 341,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Ftask_summary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Ftask_summary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftask_summary.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,341 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# 🤗 Transformers로 할 수 있는 것[[what__transformers_can_do]]\n-\n-🤗 Transformers는 자연어처리(NLP), 컴퓨터 비전, 오디오 및 음성 처리 작업에 대한 사전훈련된 최첨단 모델 라이브러리입니다. \n-이 라이브러리는 트랜스포머 모델뿐만 아니라 컴퓨터 비전 작업을 위한 현대적인 합성곱 신경망과 같은 트랜스포머가 아닌 모델도 포함하고 있습니다. \n-\n-스마트폰, 앱, 텔레비전과 같은 오늘날 가장 인기 있는 소비자 제품을 살펴보면, 딥러닝 기술이 그 뒤에 사용되고 있을 확률이 높습니다. \n-스마트폰으로 촬영한 사진에서 배경 객체를 제거하고 싶다면 어떻게 할까요? 이는 파놉틱 세그멘테이션 작업의 예입니다(아직 이게 무엇인지 모른다면, 다음 섹션에서 설명하겠습니다!).\n-\n-이 페이지는 다양한 음성 및 오디오, 컴퓨터 비전, NLP 작업을 🤗 Transformers 라이브러리를 활용하여 다루는 간단한 예제를 3줄의 코드로 제공합니다. \n-\n-## 오디오[[audio]]\n-\n-\n-음성 및 오디오 처리 작업은 다른 모달리티와 약간 다릅니다. 이는 주로 오디오가 연속적인 신호로 입력되기 때문입니다. \n-텍스트와 달리 원본 오디오 파형(waveform)은 문장이 단어로 나눠지는 것처럼 깔끔하게 이산적인 묶음으로 나눌 수 없습니다. \n-이를 극복하기 위해 원본 오디오 신호는 일정한 간격으로 샘플링됩니다. 해당 간격 내에서 더 많은 샘플을 취할 경우 샘플링률이 높아지며, 오디오는 원본 오디오 소스에 더 가까워집니다.\n-\n-과거의 접근 방식은 오디오에서 유용한 특징을 추출하기 위해 오디오를 전처리하는 것이었습니다. \n-하지만 현재는 원본 오디오 파형을 특성 인코더에 직접 넣어서 오디오 표현(representation)을 추출하는 것이 더 일반적입니다. \n-이렇게 하면 전처리 단계가 단순해지고 모델이 가장 중요한 특징을 학습할 수 있습니다.\n-\n-### 오디오 분류[[audio_classification]]\n-\n-\n-오디오 분류는 오디오 데이터에 미리 정의된 클래스 집합의 레이블을 지정하는 작업입니다. 이는 많은 구체적인 응용 프로그램을 포함한 넓은 범주입니다.\n-\n-일부 예시는 다음과 같습니다:\n-\n-* 음향 장면 분류: 오디오에 장면 레이블(\"사무실\", \"해변\", \"경기장\")을 지정합니다.\n-* 음향 이벤트 감지: 오디오에 소리 이벤트 레이블(\"차 경적\", \"고래 울음소리\", \"유리 파손\")을 지정합니다.\n-* 태깅: 여러 가지 소리(새 지저귐, 회의에서의 화자 식별)가 포함된 오디오에 레이블을 지정합니다.\n-* 음악 분류: 음악에 장르 레이블(\"메탈\", \"힙합\", \"컨트리\")을 지정합니다.\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> classifier = pipeline(task=\"audio-classification\", model=\"superb/hubert-base-superb-er\")\n->>> preds = classifier(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n->>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n->>> preds\n-[{'score': 0.4532, 'label': 'hap'},\n- {'score': 0.3622, 'label': 'sad'},\n- {'score': 0.0943, 'label': 'neu'},\n- {'score': 0.0903, 'label': 'ang'}]\n-```\n-\n-### 자동 음성 인식[[automatic_speech_recognition]]\n-\n-\n-자동 음성 인식(ASR)은 음성을 텍스트로 변환하는 작업입니다. \n-음성은 인간의 자연스러운 의사소통 형태이기 때문에 ASR은 가장 일반적인 오디오 작업 중 하나입니다. \n-오늘날 ASR 시스템은 스피커, 전화 및 자동차와 같은 \"스마트\" 기술 제품에 내장되어 있습니다. \n-우리는 가상 비서에게 음악 재생, 알림 설정 및 날씨 정보를 요청할 수 있습니다.\n-\n-하지만 트랜스포머 아키텍처가 해결하는 데 도움을 준 핵심 도전 과제 중 하나는 양이 데이터 양이 적은 언어(low-resource language)에 대한 것입니다. 대량의 음성 데이터로 사전 훈련한 후 데이터 양이 적은 언어에서 레이블이 지정된 음성 데이터 1시간만으로 모델을 미세 조정하면 이전의 100배 많은 레이블이 지정된 데이터로 훈련된 ASR 시스템보다 훨씬 더 높은 품질의 결과를 얻을 수 있습니다. \n-```py\n->>> from transformers import pipeline\n-\n->>> transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n->>> transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n-{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n-```\n-\n-## 컴퓨터 비전[[computer_vision]]\n-\n-컴퓨터 비전 작업 중 가장 초기의 성공적인 작업 중 하나는 [합성곱 신경망(CNN)](glossary#convolution)을 사용하여 우편번호 숫자 이미지를 인식하는 것이었습니다. 이미지는 픽셀로 구성되어 있으며 각 픽셀은 숫자 값으로 표현됩니다. 이로써 이미지를 픽셀 값의 행렬로 나타내는 것이 쉬워집니다. 특정한 픽셀 값의 조합은 이미지의 색상을 의미합니다.\n-\n-컴퓨터 비전 작업은 일반적으로 다음 두 가지 방법으로 접근 가능합니다:\n-\n-1. 합성곱을 사용하여 이미지의 낮은 수준 특징에서 높은 수준의 추상적인 요소까지 계층적으로 학습합니다.\n-\n-2. 이미지를 패치로 나누고 트랜스포머를 사용하여 점진적으로 각 이미지 패치가 서로 어떠한 방식으로 연관되어 이미지를 형성하는지 학습합니다. `CNN`에서 선호하는 상향식 접근법과는 달리, 이 방식은 흐릿한 이미지로 초안을 그리고 점진적으로 선명한 이미지로 만들어가는 것과 유사합니다.\n-\n-### 이미지 분류[[image_classification]]\n-\n-\n-이미지 분류는 한 개의 전체 이미지에 미리 정의된 클래스 집합의 레이블을 지정하는 작업입니다. \n-\n-대부분의 분류 작업과 마찬가지로, 이미지 분류에는 다양한 실용적인 용도가 있으며, 일부 예시는 다음과 같습니다:\n-\n-\n-* 의료: 질병을 감지하거나 환자 건강을 모니터링하기 위해 의료 이미지에 레이블을 지정합니다.\n-* 환경: 위성 이미지를 분류하여 산림 벌채를 감시하고 야생 지역 관리를 위한 정보를 제공하거나 산불을 감지합니다. \n-* 농업: 작물 이미지를 분류하여 식물 건강을 확인하거나 위성 이미지를 분류하여 토지 이용 관찰에 사용합니다.\n-* 생태학: 동물이나 식물 종 이미지를 분류하여 야생 동물 개체군을 조사하거나 멸종 위기에 처한 종을 추적합니다.\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> classifier = pipeline(task=\"image-classification\")\n->>> preds = classifier(\n-...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n-... )\n->>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n->>> print(*preds, sep=\"\\n\")\n-{'score': 0.4335, 'label': 'lynx, catamount'}\n-{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}\n-{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}\n-{'score': 0.0239, 'label': 'Egyptian cat'}\n-{'score': 0.0229, 'label': 'tiger cat'}\n-```\n-\n-### 객체 탐지[[object_detection]]\n-\n-\n-이미지 분류와 달리 객체 탐지는 이미지 내에서 여러 객체를 식별하고 바운딩 박스로 정의된 객체의 위치를 파악합니다. \n-\n-객체 탐지의 몇 가지 응용 예시는 다음과 같습니다:\n-\n-* 자율 주행 차량: 다른 차량, 보행자 및 신호등과 같은 일상적인 교통 객체를 감지합니다.\n-* 원격 감지: 재난 모니터링, 도시 계획 및 기상 예측 등을 수행합니다.\n-* 결함 탐지: 건물의 균열이나 구조적 손상, 제조 결함 등을 탐지합니다.\n-\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> detector = pipeline(task=\"object-detection\")\n->>> preds = detector(\n-...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n-... )\n->>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n->>> preds\n-[{'score': 0.9865,\n-  'label': 'cat',\n-  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]\n-```\n-\n-### 이미지 분할[[image_segmentation]]\n-\n-\n-이미지 분할은 픽셀 차원의 작업으로, 이미지 내의 모든 픽셀을 클래스에 할당합니다. 이는 객체 탐지와 다릅니다. 객체 탐지는 바운딩 박스를 사용하여 이미지 내의 객체를 레이블링하고 예측하는 반면, 분할은 더 세분화된 작업입니다. 분할은 픽셀 수준에서 객체를 감지할 수 있습니다. \n-\n-이미지 분할에는 여러 유형이 있습니다:\n-\n-* 인스턴스 분할: 개체의 클래스를 레이블링하는 것 외에도, 개체의 각 구분된 인스턴스에도 레이블을 지정합니다 (\"개-1\", \"개-2\" 등).\n-* 파놉틱 분할: 의미적 분할과 인스턴스 분할의 조합입니다. 각 픽셀을 의미적 클래스로 레이블링하는 **동시에** 개체의 각각 구분된 인스턴스로도 레이블을 지정합니다.\n-\n-분할 작업은 자율 주행 차량에서 유용하며, 주변 환경의 픽셀 수준 지도를 생성하여 보행자와 다른 차량 주변에서 안전하게 탐색할 수 있습니다. 또한 의료 영상에서도 유용합니다. 분할 작업이 픽셀 수준에서 객체를 감지할 수 있기 때문에 비정상적인 세포나 장기의 특징을 식별하는 데 도움이 될 수 있습니다. 이미지 분할은 의류 가상 시착이나 카메라를 통해 실제 세계에 가상 개체를 덧씌워 증강 현실 경험을 만드는 등 전자 상거래 분야에서도 사용될 수 있습니다.\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> segmenter = pipeline(task=\"image-segmentation\")\n->>> preds = segmenter(\n-...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n-... )\n->>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n->>> print(*preds, sep=\"\\n\")\n-{'score': 0.9879, 'label': 'LABEL_184'}\n-{'score': 0.9973, 'label': 'snow'}\n-{'score': 0.9972, 'label': 'cat'}\n-```\n-\n-### 깊이 추정[[depth_estimation]]\n-\n-깊이 추정은 카메라로부터 이미지 내부의 각 픽셀의 거리를 예측합니다. 이 컴퓨터 비전 작업은 특히 장면 이해와 재구성에 중요합니다. 예를 들어, 자율 주행 차량은 보행자, 교통 표지판 및 다른 차량과 같은 객체와의 거리를 이해하여 장애물과 충돌을 피해야 합니다. 깊이 정보는 또한 2D 이미지에서 3D 표현을 구성하는 데 도움이 되며 생물학적 구조나 건물의 고품질 3D 표현을 생성하는 데 사용될 수 있습니다.\n-\n-깊이 추정에는 두 가지 접근 방식이 있습니다:\n-\n-* 스테레오: 약간 다른 각도에서 촬영된 동일한 이미지 두 장을 비교하여 깊이를 추정합니다.\n-* 단안: 단일 이미지에서 깊이를 추정합니다.\n-\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> depth_estimator = pipeline(task=\"depth-estimation\")\n->>> preds = depth_estimator(\n-...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n-... )\n-```\n-\n-## 자연어처리[[natural_language_processing]]\n-\n-텍스트는 인간이 의사 소통하는 자연스러운 방식 중 하나이기 때문에 자연어처리 역시 가장 일반적인 작업 유형 중 하나입니다. 모델이 인식하는 형식으로 텍스트를 변환하려면 토큰화해야 합니다. 이는 텍스트 시퀀스를 개별 단어 또는 하위 단어(토큰)로 분할한 다음 이러한 토큰을 숫자로 변환하는 것을 의미합니다. 결과적으로 텍스트 시퀀스를 숫자 시퀀스로 표현할 수 있으며, 숫자 시퀀스를 다양한 자연어처리 작업을 해결하기 위한 모델에 입력할 수 있습니다!\n-\n-### 텍스트 분류[[text_classification]]\n-\n-다른 모달리티에서의 분류 작업과 마찬가지로 텍스트 분류는 미리 정의된 클래스 집합에서 텍스트 시퀀스(문장 수준, 단락 또는 문서 등)에 레이블을 지정합니다. 텍스트 분류에는 다양한 실용적인 응용 사례가 있으며, 일부 예시는 다음과 같습니다:\n-\n-* 감성 분석: 텍스트를 `긍정` 또는 `부정`과 같은 어떤 극성에 따라 레이블링하여 정치, 금융, 마케팅과 같은 분야에서 의사 결정에 정보를 제공하고 지원할 수 있습니다.\n-* 콘텐츠 분류: 텍스트를 주제에 따라 레이블링(날씨, 스포츠, 금융 등)하여 뉴스 및 소셜 미디어 피드에서 정보를 구성하고 필터링하는 데 도움이 될 수 있습니다.\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> classifier = pipeline(task=\"sentiment-analysis\")\n->>> preds = classifier(\"Hugging Face is the best thing since sliced bread!\")\n->>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n->>> preds\n-[{'score': 0.9991, 'label': 'POSITIVE'}]\n-```\n-\n-### 토큰 분류[[token_classification]]\n-\n-모든 자연어처리 작업에서는 텍스트가 개별 단어나 하위 단어로 분리되어 전처리됩니다. 분리된 단어를 [토큰](/glossary#token)이라고 합니다. 토큰 분류는 각 토큰에 미리 정의된 클래스 집합의 레이블을 할당합니다.\n-\n-토큰 분류의 두 가지 일반적인 유형은 다음과 같습니다:\n-\n-* 개체명 인식 (NER): 토큰을 조직, 인물, 위치 또는 날짜와 같은 개체 범주에 따라 레이블링합니다. NER은 특히 유전체학적인 환경에서 유전자, 단백질 및 약물 이름에 레이블을 지정하는 데 널리 사용됩니다.\n-* 품사 태깅 (POS): 명사, 동사, 형용사와 같은 품사에 따라 토큰에 레이블을 할당합니다. POS는 번역 시스템이 동일한 단어가 문법적으로 어떻게 다른지 이해하는 데 도움이 됩니다 (명사로 사용되는 \"bank(은행)\"과 동사로 사용되는 \"bank(예금을 예치하다)\"과 같은 경우).\n-\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> classifier = pipeline(task=\"ner\")\n->>> preds = classifier(\"Hugging Face is a French company based in New York City.\")\n->>> preds = [\n-...     {\n-...         \"entity\": pred[\"entity\"],\n-...         \"score\": round(pred[\"score\"], 4),\n-...         \"index\": pred[\"index\"],\n-...         \"word\": pred[\"word\"],\n-...         \"start\": pred[\"start\"],\n-...         \"end\": pred[\"end\"],\n-...     }\n-...     for pred in preds\n-... ]\n->>> print(*preds, sep=\"\\n\")\n-{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}\n-{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}\n-{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n-{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}\n-{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}\n-{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}\n-{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}\n-```\n-\n-### 질의응답[[question_answering]]\n-\n-질의응답은 또 하나의 토큰 차원의 작업으로, 문맥이 있을 때(개방형 도메인)와 문맥이 없을 때(폐쇄형 도메인) 질문에 대한 답변을 반환합니다. 이 작업은 가상 비서에게 식당이 영업 중인지와 같은 질문을 할 때마다 발생할 수 있습니다. 고객 지원 또는 기술 지원을 제공하거나 검색 엔진이 요청한 정보를 검색하는 데 도움을 줄 수 있습니다.\n-\n-질문 답변에는 일반적으로 두 가지 유형이 있습니다:\n-\n-* 추출형: 질문과 문맥이 주어졌을 때, 모델이 주어진 문맥의 일부에서 가져온 텍스트의 범위를 답변으로 합니다.\n-* 생성형: 질문과 문맥이 주어졌을 때, 주어진 문맥을 통해 답변을 생성합니다. 이 접근 방식은 [`QuestionAnsweringPipeline`] 대신 [`Text2TextGenerationPipeline`]을 통해 처리됩니다.\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> question_answerer = pipeline(task=\"question-answering\")\n->>> preds = question_answerer(\n-...     question=\"What is the name of the repository?\",\n-...     context=\"The name of the repository is huggingface/transformers\",\n-... )\n->>> print(\n-...     f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n-... )\n-score: 0.9327, start: 30, end: 54, answer: huggingface/transformers\n-```\n-\n-### 요약[[summarization]]\n-\n-요약은 원본 문서의 의미를 최대한 보존하면서 긴 문서를 짧은 문서로 만드는 작업입니다. 요약은 `sequence-to-sequence` 작업입니다. 입력보다 짧은 텍스트 시퀀스를 출력합니다. 요약 작업은 독자가 장문 문서들의 주요 포인트를 빠르게 이해하는 데 도움을 줄 수 있습니다. 입법안, 법률 및 금융 문서, 특허 및 과학 논문은 요약 작업이 독자의 시간을 절약하고 독서 보조 도구로 사용될 수 있는 몇 가지 예시입니다.\n-\n-질문 답변과 마찬가지로 요약에는 두 가지 유형이 있습니다:\n-\n-* 추출형: 원본 텍스트에서 가장 중요한 문장을 식별하고 추출합니다.\n-* 생성형: 원본 텍스트에서 목표 요약을 생성합니다. 입력 문서에 없는 새로운 단어를 포함할 수도 있습니다. [`SummarizationPipeline`]은 생성형 접근 방식을 사용합니다.\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> summarizer = pipeline(task=\"summarization\")\n->>> summarizer(\n-...     \"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\"\n-... )\n-[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]\n-```\n-\n-### 번역[[translation]]\n-\n-번역은 한 언어로 된 텍스트 시퀀스를 다른 언어로 변환하는 작업입니다. 이는 서로 다른 배경을 가진 사람들이 서로 소통하는 데 도움을 주는 중요한 역할을 합니다. 더 넓은 대중에게 콘텐츠를 번역하여 전달하거나, 새로운 언어를 배우는 데 도움이 되는 학습 도구가 될 수도 있습니다. 요약과 마찬가지로, 번역은 `sequence-to-sequence` 작업입니다. 즉, 모델은 입력 시퀀스를 받아서 출력이 되는 목표 시퀀스를 반환합니다.\n-\n-초기의 번역 모델은 대부분 단일 언어로 이루어져 있었지만, 최근에는 많은 언어 쌍 간에 번역을 수행할 수 있는 다중 언어 모델에 대한 관심이 높아지고 있습니다.\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> text = \"translate English to French: Hugging Face is a community-based open-source platform for machine learning.\"\n->>> translator = pipeline(task=\"translation\", model=\"google-t5/t5-small\")\n->>> translator(text)\n-[{'translation_text': \"Hugging Face est une tribune communautaire de l'apprentissage des machines.\"}]\n-```\n-\n-### 언어 모델링[[language_modeling]]\n-\n-언어 모델링은 텍스트 시퀀스에서 단어를 예측하는 작업입니다. 사전 훈련된 언어 모델은 많은 다른 하위 작업에 따라 미세 조정될 수 있기 때문에 매우 인기 있는 자연어처리 작업이 되었습니다. 최근에는 제로 샷(zero-shot) 또는 퓨 샷(few-shot) 학습이 가능한 대규모 언어 모델(Large Language Models, LLM)에 대한 많은 관심이 발생하고 있습니다. 이는 모델이 명시적으로 훈련되지 않은 작업도 해결할 수 있다는 것을 의미합니다! 언어 모델은 유창하고 설득력 있는 텍스트를 생성하는 데 사용될 수 있지만, 텍스트가 항상 정확하지는 않을 수 있으므로 주의가 필요합니다.\n-\n-언어 모델링에는 두 가지 유형이 있습니다:\n-\n-* 인과적 언어 모델링: 이 모델의 목적은 시퀀스에서 다음 토큰을 예측하는 것이며, 미래 토큰이 마스킹 됩니다.\n-    ```py\n-    >>> from transformers import pipeline\n-\n-    >>> prompt = \"Hugging Face is a community-based open-source platform for machine learning.\"\n-    >>> generator = pipeline(task=\"text-generation\")\n-    >>> generator(prompt)  # doctest: +SKIP\n-    ```\n-\n-* 마스킹된 언어 모델링: 이 모델의 목적은 시퀀스 내의 마스킹된 토큰을 예측하는 것이며, 시퀀스 내의 모든 토큰에 대한 접근이 제공됩니다.\n-    \n-    ```py\n-    >>> text = \"Hugging Face is a community-based open-source <mask> for machine learning.\"\n-    >>> fill_mask = pipeline(task=\"fill-mask\")\n-    >>> preds = fill_mask(text, top_k=1)\n-    >>> preds = [\n-    ...     {\n-    ...         \"score\": round(pred[\"score\"], 4),\n-    ...         \"token\": pred[\"token\"],\n-    ...         \"token_str\": pred[\"token_str\"],\n-    ...         \"sequence\": pred[\"sequence\"],\n-    ...     }\n-    ...     for pred in preds\n-    ... ]\n-    >>> preds\n-    [{'score': 0.2236,\n-      'token': 1761,\n-      'token_str': ' platform',\n-      'sequence': 'Hugging Face is a community-based open-source platform for machine learning.'}]\n-    ```\n-\n-이 페이지를 통해 각 모달리티의 다양한 작업 유형과 각 작업의 실용적 중요성에 대해 추가적인 배경 정보를 얻으셨기를 바랍니다. 다음 [섹션](tasks_explained)에서는 🤗 Transformer가 이러한 작업을 해결하는 **방법**에 대해 알아보실 수 있습니다.\n\\ No newline at end of file"
        },
        {
            "sha": "57ccd57a797e6cfc343276e81dd9481cb1ec5a09",
            "filename": "docs/source/ko/tasks_explained.md",
            "status": "removed",
            "additions": 0,
            "deletions": 295,
            "changes": 295,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Ftasks_explained.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Ftasks_explained.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks_explained.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,295 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# 🤗 Transformers로 작업을 해결하는 방법[[how-transformers-solve-tasks]]\n-\n-[🤗 Transformers로 할 수 있는 작업](task_summary)에서 자연어 처리(NLP), 음성 및 오디오, 컴퓨터 비전 작업 등의 중요한 응용을 배웠습니다. 이 페이지에서는 모델이 이러한 작업을 어떻게 해결하는지 자세히 살펴보고 내부에서 어떤 일이 일어나는지 설명합니다. 주어진 작업을 해결하는 많은 방법이 있으며, 일부 모델은 특정 기술을 구현하거나 심지어 새로운 방식으로 작업에 접근할 수도 있지만, Transformer 모델의 경우 일반적인 아이디어는 동일합니다. 유연한 아키텍처 덕분에 대부분의 모델은 인코더, 디코더 또는 인코더-디코더 구조의 변형입니다. Transformer 모델뿐만 아니라 우리의 라이브러리에는 오늘날 컴퓨터 비전 작업에 사용되는 몇 가지 합성곱 신경망(CNNs)도 있습니다. 또한, 우리는 현대 CNN의 작동 방식에 대해 설명할 것입니다.\n-\n-작업이 어떻게 해결되는지 설명하기 위해, 유용한 예측을 출력하고자 모델 내부에서 어떤 일이 일어나는지 살펴봅니다.\n-\n-- 오디오 분류 및 자동 음성 인식(ASR)을 위한 [Wav2Vec2](model_doc/wav2vec2)\n-- 이미지 분류를 위한 [Vision Transformer (ViT)](model_doc/vit) 및 [ConvNeXT](model_doc/convnext)\n-- 객체 탐지를 위한 [DETR](model_doc/detr)\n-- 이미지 분할을 위한 [Mask2Former](model_doc/mask2former)\n-- 깊이 추정을 위한 [GLPN](model_doc/glpn)\n-- 인코더를 사용하는 텍스트 분류, 토큰 분류 및 질의응답과 같은 NLP 작업을 위한 [BERT](model_doc/bert)\n-- 디코더를 사용하는 텍스트 생성과 같은 NLP 작업을 위한 [GPT2](model_doc/gpt2)\n-- 인코더-디코더를 사용하는 요약 및 번역과 같은 NLP 작업을 위한 [BART](model_doc/bart)\n-\n-<Tip>\n-\n-더 나아가기 전에, 기존 Transformer 아키텍처에 대한 기본적인 지식을 숙지하는 것이 좋습니다. 인코더, 디코더 및 어텐션의 작동 방식을 알면 다양한 Transformer 모델이 어떻게 작동하는지 이해하는 데 도움이 됩니다. 시작 단계거나 복습이 필요한 경우, 더 많은 정보를 위해 [코스](https://huggingface.co/course/chapter1/4?fw=pt)를 확인하세요!\n-\n-</Tip>\n-\n-## 음성 및 오디오[[speech-and-audio]]\n-\n-[Wav2Vec2](model_doc/wav2vec2)는 레이블이 지정되지 않은 음성 데이터에 대해 사전훈련된 모델로, 오디오 분류 및 자동 음성 인식을 위해 레이블이 지정된 데이터로 미세 조정합니다.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png\"/>\n-</div>\n-\n-이 모델에는 4가지 주요 구성 요소가 있습니다:\n-\n-1. *특징 인코더(feature encoder)*는 원시 오디오 파형(raw audio waveform)을 가져와서 제로 평균 및 단위 분산으로 표준화하고, 각각 20ms 길이의 특징 벡터의 시퀀스로 변환합니다.\n-\n-2. 오디오 파형은 본질적으로 연속적이기 때문에, 텍스트 시퀀스를 단어로 나누는 것과 같이 분할할 수 없습니다. 그래서 *양자화 모듈(quantization module)*로 전달되는 특징 벡터는 이산형 음성 단위를 학습하기 위한 것입니다. 음성 단위는 *코드북(codebook)*(어휘집이라고 생각할 수 있습니다)이라는 코드단어(codewords) 콜렉션에서 선택됩니다. 코드북에서 연속적인 오디오 입력을 가장 잘 나타내는 벡터 또는 음성 단위가 선택되어 모델을 통과합니다.\n-\n-3. 특징 벡터의 절반은 무작위로 마스크가 적용되며, 마스크된 특징 벡터는 *상대적 위치 임베딩*을 추가하는 Transformer 인코더인 *문맥 네트워크(context network)*로 전달됩니다.\n-\n-4. 문맥 네트워크의 사전훈련 목표는 *대조적 작업(contrastive task)*입니다. 모델은 잘못된 예측 시퀀스에서 마스크된 예측의 실제 양자화된 음성 표현을 예측하며, 모델이 가장 유사한 컨텍스트 벡터와 양자화된 음성 단위(타겟 레이블)를 찾도록 권장합니다.\n-\n-이제 wav2vec2가 사전훈련되었으므로, 오디오 분류 또는 자동 음성 인식을 위해 데이터에 맞춰 미세 조정할 수 있습니다!\n-\n-### 오디오 분류[[audio-classification]]\n-\n-사전훈련된 모델을 오디오 분류에 사용하려면, 기본 Wav2Vec2 모델 상단에 시퀀스 분류 헤드를 추가하면 됩니다. 분류 헤드는 인코더의 은닉 상태(hidden states)를 받는 선형 레이어입니다. 은닉 상태는 각각 길이가 다른 오디오 프레임에서 학습된 특징을 나타냅니다. 고정 길이의 벡터 하나를 만들기 위해, 은닉 상태는 먼저 풀링되고, 클래스 레이블에 대한 로짓으로 변환됩니다. 가장 가능성이 높은 클래스를 찾기 위해 로짓과 타겟 사이의 교차 엔트로피 손실이 계산됩니다.\n-\n-오디오 분류에 직접 도전할 준비가 되셨나요? 완전한 [오디오 분류 가이드](tasks/audio_classification)를 확인하여 Wav2Vec2를 미세 조정하고 추론에 사용하는 방법을 학습하세요!\n-\n-### 자동 음성 인식[[automatic-speech-recognition]]\n-\n-사전훈련된 모델을 자동 음성 인식에 사용하려면, [연결주의적 시간 분류(CTC, Connectionist Temporal Classification)](glossary#connectionist-temporal-classification-ctc)를 위해 기본 Wav2Vec2 모델 상단에 언어 모델링 헤드를 추가합니다. 언어 모델링 헤드는 인코더의 은닉 상태를 받아서 로짓으로 변환합니다. 각 로짓은 토큰 클래스(토큰 수는 작업의 어휘에서 나타납니다)를 나타냅니다. CTC 손실은 텍스트로 디코딩된 토큰에서 가장 가능성이 높은 토큰 시퀀스를 찾기 위해 로짓과 타겟 사이에서 계산됩니다. \n-\n-자동 음성 인식에 직접 도전할 준비가 되셨나요? 완전한 [자동 음성 인식 가이드](tasks/asr)를 확인하여 Wav2Vec2를 미세 조정하고 추론에 사용하는 방법을 학습하세요!\n-\n-## 컴퓨터 비전[[computer-vision]]\n-\n-컴퓨터 비전 작업에 접근하는 2가지 방법이 있습니다:\n-\n-1. 이미지를 패치 시퀀스로 분리하고 Transformer로 병렬 처리합니다.\n-2. [ConvNeXT](model_doc/convnext)와 같은 현대 CNN을 사용합니다. 이는 합성곱 레이어를 기반으로 하지만 현대 네트워크 설계를 적용합니다.\n-\n-<Tip>\n-\n-세 번째 방법은 Transformer와 합성곱(예를 들어, [Convolutional Vision Transformer](model_doc/cvt) 또는 [LeViT](model_doc/levit))을 결합하는 것입니다. 우리는 살펴볼 두 가지 방법만 결합하기 때문에 여기서 이 방법을 다루지 않습니다.\n-\n-</Tip>\n-\n-ViT와 ConvNeXT는 일반적으로 이미지 분류에서 사용되지만, 물체 감지, 분할, 깊이 추정과 같은 다른 비전 작업에는 각각 DETR, Mask2Former, GLPN이 더 적합하므로 이러한 모델을 살펴보겠습니다.\n-\n-### 이미지 분류[[image-classification]]\n-\n-ViT와 ConvNeXT 모두 이미지 분류에 사용될 수 있지만, ViT는 어텐션 메커니즘을, ConvNeXT는 합성곱을 사용하는 것이 주된 차이입니다.\n-\n-#### Transformer[[transformer]]\n-\n-[ViT](model_doc/vit)은 합성곱을 전적으로 순수 Transformer 아키텍처로 대체합니다. 기존 Transformer에 익숙하다면, ViT를 이해하는 방법의 대부분을 이미 파악했다고 볼 수 있습니다.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg\"/>\n-</div>\n-\n-ViT가 도입한 주요 변경 사항은 이미지가 Transformer로 어떻게 전달되는지에 있습니다:\n-\n-1. 이미지는 서로 중첩되지 않는 정사각형 패치로 분할되고, 각 패치는 벡터 또는 *패치 임베딩(patch embedding)*으로 변환됩니다. 패치 임베딩은 적절한 입력 차원을 만드는 2D 합성곱 계층에서 생성됩니다(기본 Transformer의 경우 각 패치의 임베딩마다 768개의 값이 필요합니다). 224x224 픽셀 이미지가 있다면, 16x16 이미지 패치 196개로 분할할 수 있습니다. 텍스트가 단어로 토큰화되는 것처럼, 이미지도 패치 시퀀스로 \"토큰화\"됩니다.\n-\n-2. *학습 가능한 임베딩(learnable embedding)*(특수한 `[CLS]` 토큰)이 BERT와 같이 패치 임베딩의 시작 부분에 추가됩니다. `[CLS]` 토큰의 마지막 은닉 상태는 부착된 분류 헤드의 입력으로 사용되고, 다른 출력은 무시됩니다. 이 토큰은 모델이 이미지의 표현을 인코딩하는 방법을 학습하는 데 도움이 됩니다.\n-\n-3. 패치와 학습 가능한 임베딩에 마지막으로 추가할 것은 *위치 임베딩*입니다. 왜냐하면 모델은 이미지 패치의 순서를 모르기 때문입니다. 위치 임베딩도 학습 가능하며, 패치 임베딩과 동일한 크기를 가집니다. 최종적으로, 모든 임베딩이 Transformer 인코더에 전달됩니다.\n-\n-4. `[CLS]` 토큰을 포함한 출력은 다층 퍼셉트론 헤드(MLP)에 전달됩니다. ViT의 사전훈련 목표는 단순히 분류입니다. 다른 분류 헤드와 같이, MLP 헤드는 출력을 클래스 레이블에 대해 로짓으로 변환하고 교차 엔트로피 손실을 계산하여 가장 가능성이 높은 클래스를 찾습니다.\n-\n-이미지 분류에 직접 도전할 준비가 되셨나요? 완전한 [이미지 분류 가이드](tasks/image_classification)를 확인하여 ViT를 미세 조정하고 추론에 사용하는 방법을 학습하세요!\n-\n-#### CNN[[cnn]]\n-\n-<Tip>\n-\n-이 섹션에서는 합성곱에 대해 간략하게 설명합니다. 그러나 이미지의 모양과 크기가 어떻게 변화하는지에 대한 사전 이해가 있다면 도움이 될 것입니다. 합성곱에 익숙하지 않은 경우, fastai book의 [합성곱 신경망 챕터](https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb)를 확인하세요!\n-\n-</Tip>\n-\n-[ConvNeXT](model_doc/convnext)는 성능을 높이기 위해 새로운 현대 네트워크 설계를 적용한 CNN 구조입니다. 그러나 합성곱은 여전히 모델의 핵심입니다. 높은 수준의 관점에서 볼 때, [합성곱](glossary#convolution)은 작은 행렬(*커널*)에 이미지 픽셀의 작은 윈도우를 곱하는 연산입니다. 이는 특정 텍스쳐(texture)이나 선의 곡률과 같은 일부 특징을 계산합니다. 그러고 다음 픽셀 윈도우로 넘어가는데, 여기서 합성곱이 이동하는 거리를 *보폭(stride)*이라고 합니다.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif\"/>\n-</div>\n-\n-<small>패딩이나 보폭이 없는 기본 합성곱, <a href=\"https://huggingface.co/papers/1603.07285\">딥러닝을 위한 합성곱 연산 가이드</a></small>\n-\n-이 출력을 다른 합성곱 레이어에 전달할 수 있으며, 각 연속적인 레이어를 통해 네트워크는 핫도그나 로켓과 같이 더 복잡하고 추상적인 것을 학습합니다. 합성곱 레이어 사이에 풀링 레이어를 추가하여 차원을 줄이고 특징의 위치 변화에 대해 모델을 더 견고하게 만드는 것이 일반적입니다.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.png\"/>\n-</div>\n-\n-ConvNeXT는 CNN을 5가지 방식으로 현대화합니다:\n-\n-1. 각 단계의 블록 수를 변경하고 더 큰 보폭과 그에 대응하는 커널 크기로 이미지를 \"패치화(patchify)\"합니다. 겹치지 않는 슬라이딩 윈도우는 ViT가 이미지를 패치로 분할하는 방법과 유사하게 이 패치화 전략을 만듭니다.\n-\n-2. *병목(bottleneck)* 레이어는 채널 수를 줄였다가 다시 복원합니다. 왜냐하면 1x1 합성곱을 수행하는 것이 더 빠르고, 깊이를 늘릴 수 있기 때문입니다. 역 병목(inverted bottlenect)은 채널 수를 확장하고 축소함으로써 그 반대로 수행하므로, 메모리 효율이 더 높습니다.\n-\n-3. 병목 레이어의 일반적인 3x3 합성곱 레이어를 각 입력 채널에 개별적으로 합성곱을 적용한 다음 마지막에 쌓는 *깊이별 합성곱(depthwise convolution)*으로 대체합니다. 이는 네트워크 폭이 넓혀 성능이 향상됩니다.\n-\n-4. ViT는 어텐션 메커니즘 덕분에 한 번에 더 많은 이미지를 볼 수 있는 전역 수신 필드를 가지고 있습니다. ConvNeXT는 커널 크기를 7x7로 늘려 이 효과를 재현하려고 시도합니다.\n-\n-5. 또한 ConvNeXT는 Transformer 모델을 모방하는 몇 가지 레이어 설계를 변경합니다. 활성화 및 정규화 레이어가 더 적고, 활성화 함수가 ReLU 대신 GELU로 전환되고, BatchNorm 대신 LayerNorm을 사용합니다.\n-\n-합성곱 블록의 출력은 분류 헤드로 전달되며, 분류 헤드는 출력을 로짓으로 변환하고 교차 엔트로피 손실을 계산하여 가장 가능성이 높은 레이블을 찾습니다.\n-\n-### 객체 탐지[[object-detection]]\n-\n-[DETR](model_doc/detr), *DEtection TRansformer*는 CNN과 Transformer 인코더-디코더를 결합한 종단간(end-to-end) 객체 탐지 모델입니다.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/detr_architecture.png\"/>\n-</div>\n-\n-1. 사전훈련된 CNN *백본(backbone)*은 픽셀 값으로 나타낸 이미지를 가져와 저해상도 특징 맵을 만듭니다. 특징 맵에 대해 1x1 합성곱을 적용하여 차원을 줄이고, 고수준 이미지 표현을 가진 새로운 특징 맵을 생성합니다. Transformer는 시퀀스 모델이기 때문에 특징 맵을 위치 임베딩과 결합된 특징 벡터의 시퀀스로 평탄화합니다.\n-\n-2. 특징 벡터는 어텐션 레이어를 사용하여 이미지 표현을 학습하는 인코더에 전달됩니다. 다음으로, 인코더의 은닉 상태는 디코더에서 *객체 쿼리*와 결합됩니다. 객체 쿼리는 이미지의 다른 영역에 초점을 맞춘 학습된 임베딩으로 학습되고, 각 어텐션 레이어를 진행하면서 갱신됩니다. 디코더의 은닉 상태는 각 객체 쿼리에 대한 바운딩 박스 좌표와 클래스 레이블을 예측하는 순방향 네트워크에 전달되며, 객체가 없는 경우 `no object`가 출력됩니다.\n-\n-    DETR은 각 객체 쿼리를 병렬로 디코딩하여 *N* 개의 최종 예측을 출력합니다. 여기서 *N*은 쿼리 수입니다. 한 번에 하나의 요소를 예측하는 일반적인 자기회귀 모델과 달리, 객체 탐지는 한 번에 *N* 개의 예측을 수행하는 집합 예측 작업(`바운딩 박스`, `클래스 레이블`)입니다.\n-\n-3. DETR은 훈련 중 *이분 매칭 손실(bipartite matching loss)*을 사용하여 고정된 수의 예측과 고정된 실제 정답 레이블(ground truth labels) 세트를 비교합니다. *N*개의 레이블 세트에 실제 정답 레이블보다 적은 경우, `no object` 클래스로 패딩됩니다. 이 손실 함수는 DETR이 예측과 실제 정답 레이블 간 1:1 대응을 찾도록 권장합니다. 바운딩 박스 또는 클래스 레이블 중 하나라도 잘못된 경우, 손실이 발생합니다. 마찬가지로, 존재하지 않는 객체를 예측하는 경우, 패널티를 받습니다. 이로 인해 DETR은 이미지에서 눈에 잘 띄는 물체 하나에 집중하는 대신, 다른 객체를 찾도록 권장됩니다.\n-\n-객체 탐지 헤드가 DETR 상단에 추가되어 클래스 레이블과 바운딩 박스의 좌표를 찾습니다. 객체 탐지 헤드에는 두 가지 구성 요소가 있습니다: 디코더 은닉 상태를 클래스 레이블의 로짓으로 변환하는 선형 레이어 및 바운딩 박스를 예측하는 MLP\n-\n-객체 탐지에 직접 도전할 준비가 되셨나요? 완전한 [객체 탐지 가이드](tasks/object_detection)를 확인하여 DETR을 미세 조정하고 추론에 사용하는 방법을 학습하세요!\n-\n-### 이미지 분할[[image-segmentation]]\n-\n-[Mask2Former](model_doc/mask2former)는 모든 유형의 이미지 분할 작업을 해결하는 범용 아키텍처입니다. 전통적인 분할 모델은 일반적으로 시멘틱(semantic) 또는 파놉틱(panoptic) 분할과 같은 이미지 분할의 특정 하위 작업에 맞춰 조정됩니다. Mask2Former는 모든 작업을 *마스크 분류* 문제로 구성합니다. 마스크 분류는 픽셀을 *N*개 세그먼트로 그룹화하고, 주어진 이미지에 대해 *N*개의 마스크와 그에 대응하는 클래스 레이블을 예측합니다. 이 섹션에서 Mask2Former의 작동 방법을 설명한 다음, 마지막에 SegFormer를 미세 조정해볼 수 있습니다.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/mask2former_architecture.png\"/>\n-</div>\n-\n-Mask2Former에는 3가지 주요 구성 요소가 있습니다:\n-\n-1. [Swin](model_doc/swin) 백본이 이미지를 받아 3개의 연속된 3x3 합성곱에서 저해상도 이미지 특징 맵을 생성합니다.\n-\n-2. 특징 맵은 *픽셀 디코더*에 전달됩니다. 이 디코더는 저해상도 특징을 고해상도 픽셀 임베딩으로 점진적으로 업샘플링합니다. 픽셀 디코더는 실제로 원본 이미지의 1/32, 1/16, 1/8 해상도의 다중 스케일 특징(저해상도 및 고해상도 특징 모두 포함)을 생성합니다.\n-\n-3. 이러한 서로 다른 크기의 특징 맵은 고해상도 특징에서 작은 객체를 포착하기 위해 한 번에 하나의 Transformer 디코더 레이어에 연속적으로 공급됩니다. Mask2Former의 핵심은 디코더의 *마스크 어텐션* 메커니즘입니다. 전체 이미지를 참조할 수 있는 크로스 어텐션(cross-attention)과 달리, 마스크 어텐션은 이미지의 특정 영역에만 집중합니다. 이는 이미지의 지역적 특징만으로 모델이 충분히 학습할 수 있기 때문에 더 빠르고 성능이 우수합니다.\n-\n-4. [DETR](tasks_explained#object-detection)과 같이, Mask2Former는 학습된 객체 쿼리를 사용하고 이를 픽셀 디코더에서의 이미지 특징과 결합하여 예측 집합(`클래스 레이블`, `마스크 예측`)을 생성합니다. 디코더의 은닉 상태는 선형 레이어로 전달되어 클래스 레이블에 대한 로짓으로 변환됩니다. 로짓과 클래스 레이블 사이의 교차 엔트로피 손실을 계산하여 가장 가능성이 높은 것을 찾습니다.\n-\n-    마스크 예측은 픽셀 임베딩과 최종 디코더 은닉 상태를 결합하여 생성됩니다. 시그모이드 교차 엔트로피 및 Dice 손실은 로짓과 실제 정답 마스크(ground truth mask) 사이에서 계산되어 가장 가능성이 높은 마스크를 찾습니다.\n-\n-이미지 분할에 직접 도전할 준비가 되셨나요? 완전한 [이미지 분할 가이드](tasks/semantic_segmentation)를 확인하여 SegFormer를 미세 조정하고 추론에 사용하는 방법을 학습하세요!\n-\n-### 깊이 추정[[depth-estimation]]\n-\n-[GLPN](model_doc/glpn), *Global-Local Path Network*는 [SegFormer](model_doc/segformer) 인코더와 경량 디코더를 결합한 깊이 추정을 위한 Transformer입니다.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/glpn_architecture.jpg\"/>\n-</div>\n-\n-1. ViT와 같이, 이미지는 패치 시퀀스로 분할되지만, 이미지 패치가 더 작다는 점이 다릅니다. 이는 세그멘테이션이나 깊이 추정과 같은 밀도 예측 작업에 더 적합합니다. 이미지 패치는 패치 임베딩으로 변환되어(패치 임베딩이 생성되는 방법은 [이미지 분류](#image-classification) 섹션을 참조하세요), 인코더로 전달됩니다.\n-\n-2. 인코더는 패치 임베딩을 받아, 여러 인코더 블록에 전달합니다. 각 블록은 어텐션 및 Mix-FFN 레이어로 구성됩니다. 후자의 목적은 위치 정보를 제공하는 것입니다. 각 인코더 블록의 끝에는 계층적 표현을 생성하기 위한 *패치 병합(patch merging)* 레이어가 있습니다. 각 인접한 패치 그룹의 특징은 연결되고, 연결된 특징에 선형 레이어가 적용되어 패치 수를 1/4의 해상도로 줄입니다. 이는 다음 인코더 블록의 입력이 되며, 이러한 전체 프로세스는 1/8, 1/16, 1/32 해상도의 이미지 특징을 가질 때까지 반복됩니다.\n-\n-3. 경량 디코더는 인코더에서 마지막 특징 맵(1/32 크기)을 가져와 1/16 크기로 업샘플링합니다. 여기서, 특징은 *선택적 특징 융합(SFF, Selective Feature Fusion)* 모듈로 전달됩니다. 이 모듈은 각 특징에 대해 어텐션 맵에서 로컬 및 전역 특징을 선택하고 결합한 다음, 1/8로 업샘플링합니다. 이 프로세스는 디코딩된 특성이 원본 이미지와 동일한 크기가 될 때까지 반복됩니다. 출력은 두 개의 합성곱 레이어를 거친 다음, 시그모이드 활성화가 적용되어 각 픽셀의 깊이를 예측합니다.\n-\n-## 자연어처리[[natural-language-processing]]\n-\n-Transformer는 초기에 기계 번역을 위해 설계되었고, 그 이후로는 사실상 모든 NLP 작업을 해결하기 위한 기본 아키텍처가 되었습니다. 어떤 작업은 Transformer의 인코더 구조에 적합하며, 다른 작업은 디코더에 더 적합합니다. 또 다른 작업은 Transformer의 인코더-디코더 구조를 모두 활용합니다.\n-\n-### 텍스트 분류[[text-classification]]\n-\n-[BERT](model_doc/bert)는 인코더 전용 모델이며, 텍스트의 풍부한 표현을 학습하기 위해 양방향의 단어에 주목함으로써 심층 양방향성(deep bidirectionality)을 효과적으로 구현한 최초의 모델입니다.\n-\n-1. BERT는 [WordPiece](tokenizer_summary#wordpiece) 토큰화를 사용하여 문장의 토큰 임베딩을 생성합니다. 단일 문장과 한 쌍의 문장을 구분하기 위해 특수한 `[SEP]` 토큰이 추가됩니다. 모든 텍스트 시퀀스의 시작 부분에는 특수한 `[CLS]` 토큰이 추가됩니다. `[CLS]` 토큰이 있는 최종 출력은 분류 작업을 위한 분류 헤드로 입력에 사용됩니다. BERT는 또한 한 쌍의 문장에서 각 토큰이 첫 번째 문장인지 두 번째 문장에 속하는지 나타내는 세그먼트 임베딩(segment embedding)을 추가합니다.\n-\n-2. BERT는 마스크드 언어 모델링과 다음 문장 예측, 두 가지 목적으로 사전훈련됩니다. 마스크드 언어 모델링에서는 입력 토큰의 일부가 무작위로 마스킹되고, 모델은 이를 예측해야 합니다. 이는 모델이 모든 단어를 보고 다음 단어를 \"예측\"할 수 있는 양방향성 문제를 해결합니다. 예측된 마스크 토큰의 최종 은닉 상태는 어휘에 대한 소프트맥스가 있는 순방향 네트워크로 전달되어 마스크된 단어를 예측합니다.\n-\n-    두 번째 사전훈련 대상은 다음 문장 예측입니다. 모델은 문장 B가 문장 A 다음에 오는지 예측해야 합니다. 문장 B가 다음 문장인 경우와 무작위 문장인 경우 각각 50%의 확률로 발생합니다. 다음 문장인지 아닌지에 대한 예측은 두 개의 클래스(`IsNext` 및 `NotNext`)에 대한 소프트맥스가 있는 순방향 네트워크로 전달됩니다.\n-\n-3. 입력 임베딩은 여러 인코더 레이어를 거쳐서 최종 은닉 상태를 출력합니다.\n-\n-사전훈련된 모델을 텍스트 분류에 사용하려면, 기본 BERT 모델 상단에 시퀀스 분류 헤드를 추가합니다. 시퀀스 분류 헤드는 최종 은닉 상태를 받는 선형 레이어이며, 로짓으로 변환하기 위해 선형 변환을 수행합니다. 교차 엔트로피 손실은 로짓과 타겟 간에 계산되어 가장 가능성이 높은 레이블을 찾습니다. \n-\n-텍스트 분류에 직접 도전할 준비가 되셨나요? 완전한 [텍스트 분류 가이드](tasks/sequence_classification)를 확인하여 DistilBERT를 미세 조정하고 추론에 사용하는 방법을 학습하세요!\n-\n-### 토큰 분류[[token-classification]]\n-\n-개체명 인식(Named Entity Recognition, NER)과 같은 토큰 분류 작업에 BERT를 사용하려면, 기본 BERT 모델 상단에 토큰 분류 헤드를 추가합니다. 토큰 분류 헤드는 최종 은닉 상태를 받는 선형 레이어이며, 로짓으로 변환하기 위해 선형 변환을 수행합니다. 교차 엔트로피 손실은 로짓과 각 토큰 간에 계산되어 가장 가능성이 높은 레이블을 찾습니다. \n-\n-토큰 분류에 직접 도전할 준비가 되셨나요? 완전한 [토큰 분류 가이드](tasks/token_classification)를 확인하여 DistilBERT를 미세 조정하고 추론에 사용하는 방법을 학습하세요!\n-\n-### 질의응답[[question-answering]]\n-\n-질의응답에 BERT를 사용하려면, 기본 BERT 모델 위에 스팬(span) 분류 헤드를 추가합니다. 이 선형 레이어는 최종 은닉 상태를 받고, 답변에 대응하는 `스팬`의 시작과 끝 로그를 계산하기 위해 선형 변환을 수행합니다. 교차 엔트로피 손실은 로짓과 각 레이블 위치 간에 계산되어 답변에 대응하는 가장 가능성이 높은 텍스트의 스팬을 찾습니다. \n-\n-질의응답에 직접 도전할 준비가 되셨나요? 완전한 [질의응답 가이드](tasks/question_answering)를 확인하여 DistilBERT를 미세 조정하고 추론에 사용하는 방법을 학습하세요!\n-\n-<Tip>\n-\n-💡 사전훈련된 BERT를 다양한 작업에 사용하는 것이 얼마나 쉬운지 주목하세요. 사전훈련된 모델에 특정 헤드를 추가하기만 하면 은닉 상태를 원하는 출력으로 조작할 수 있습니다!\n-\n-</Tip>\n-\n-### 텍스트 생성[[text-generation]]\n-\n-[GPT-2](model_doc/gpt2)는 대량의 텍스트에 대해 사전훈련된 디코딩 전용 모델입니다. 프롬프트를 주어지면 설득력 있는 (항상 사실은 아니지만!) 텍스트를 생성하고 명시적으로 훈련되지 않았음에도 불구하고 질의응답과 같은 다른 NLP 작업을 완수할 수 있습니다.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png\"/>\n-</div>\n-\n-1. GPT-2는 단어를 토큰화하고 토큰 임베딩을 생성하기 위해 [바이트 페어 인코딩(BPE, byte pair encoding)](tokenizer_summary#bytepair-encoding-bpe)을 사용합니다. 위치 인코딩은 시퀀스에서 각 토큰의 위치를 나타내기 위해 토큰 임베딩에 추가됩니다. 입력 임베딩은 여러 디코더 블록을 거쳐 일부 최종 은닉 상태를 출력합니다. 각 디코더 블록 내에서 GPT-2는 *마스크드 셀프 어텐션(masked self-attention)* 레이어를 사용합니다. 이는 GPT-2가 이후 토큰(future tokens)에 주의를 기울일 수 없도록 합니다. 왼쪽에 있는 토큰에만 주의를 기울일 수 있습니다. 마스크드 셀프 어텐션에서는 어텐션 마스크를 사용하여 이후 토큰에 대한 점수(score)를 `0`으로 설정하기 때문에 BERT의 [`mask`] 토큰과 다릅니다.\n-\n-2. 디코더의 출력은 언어 모델링 헤드에 전달되며, 언어 모델링 헤드는 은닉 상태를 로짓으로 선형 변환을 수행합니다. 레이블은 시퀀스의 다음 토큰으로, 로짓을 오른쪽으로 하나씩 이동하여 생성됩니다. 교차 엔트로피 손실은 이동된 로짓과 레이블 간에 계산되어 가장 가능성이 높은 다음 토큰을 출력합니다.\n-\n-GPT-2의 사전훈련 목적은 전적으로 [인과적 언어 모델링](glossary#causal-language-modeling)에 기반하여, 시퀀스에서 다음 단어를 예측하는 것입니다. 이는 GPT-2가 텍스트 생성에 관련된 작업에 특히 우수하도록 합니다.\n-\n-텍스트 생성에 직접 도전할 준비가 되셨나요? 완전한 [인과적 언어 모델링 가이드](tasks/language_modeling#causal-language-modeling)를 확인하여 DistilGPT-2를 미세 조정하고 추론에 사용하는 방법을 학습하세요!\n-\n-<Tip>\n-\n-텍스트 생성에 대한 자세한 내용은 [텍스트 생성 전략](generation_strategies) 가이드를 확인하세요!\n-\n-</Tip>\n-\n-### 요약[[summarization]]\n-\n-[BART](model_doc/bart) 및 [T5](model_doc/t5)와 같은 인코더-디코더 모델은 요약 작업의 시퀀스-투-시퀀스 패턴을 위해 설계되었습니다. 이 섹션에서 BART의 작동 방법을 설명한 다음, 마지막에 T5를 미세 조정해볼 수 있습니다. \n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png\"/>\n-</div>\n-\n-1. BART의 인코더 아키텍처는 BERT와 매우 유사하며 텍스트의 토큰 및 위치 임베딩을 받습니다. BART는 입력을 변형시키고 디코더로 재구성하여 사전훈련됩니다. 특정 변형 기법이 있는 다른 인코더와는 달리, BART는 모든 유형의 변형을 적용할 수 있습니다. 그러나 *text infilling* 변형 기법이 가장 잘 작동합니다. Text Infiling에서는 여러 텍스트 스팬을 **단일** [`mask`] 토큰으로 대체합니다. 이는 모델이 마스크된 토큰을 예측해야 하고, 모델에 누락된 토큰의 수를 예측하도록 가르치기 때문에 중요합니다. 입력 임베딩과 마스크된 스팬이 인코더를 거쳐 최종 은닉 상태를 출력하지만, BERT와 달리 BART는 마지막에 단어를 예측하는 순방향 네트워크를 추가하지 않습니다.\n-\n-2. 인코더의 출력은 디코더로 전달되며, 디코더는 인코더의 출력에서 마스크 토큰과 변형되지 않은 토큰을 예측해야 합니다. 이는 디코더가 원본 텍스트를 복원하는 데 도움이 되는 추가적인 문맥을 얻도록 합니다. 디코더의 출력은 언어 모델링 헤드에 전달되며, 언어 모델링 헤드는 은닉 상태를 로짓으로 선형 변환을 수행합니다. 교차 엔트로피 손실은 로짓과 토큰이 오른쪽으로 이동된 레이블 간에 계산됩니다.\n-\n-요약에 직접 도전할 준비가 되셨나요? 완전한 [요약 가이드](tasks/summarization)를 확인하여 T5를 미세 조정하고 추론에 사용하는 방법을 학습하세요!\n-\n-<Tip>\n-\n-텍스트 생성에 대한 자세한 내용은 [텍스트 생성 전략](generation_strategies) 가이드를 확인하세요!\n-\n-</Tip>\n-\n-### 번역[[translation]]\n-\n-번역은 시퀀스-투-시퀀스 작업의 또 다른 예로, [BART](model_doc/bart) 또는 [T5](model_doc/t5)와 같은 인코더-디코더 모델을 사용할 수 있습니다. 이 섹션에서 BART의 작동 방법을 설명한 다음, 마지막에 T5를 미세 조정해볼 수 있습니다. \n-\n-BART는 원천 언어를 타겟 언어로 디코딩할 수 있는 입력에 매핑하기 위해 무작위로 초기화된 별도의 인코더를 추가하여 번역에 적용합니다. 이 새로운 인코더의 임베딩은 원본 단어 임베딩 대신 사전훈련된 인코더로 전달됩니다. 원천 인코더는 모델 출력의 교차 엔트로피 손실로부터 원천 인코더, 위치 임베딩, 입력 임베딩을 갱신하여 훈련됩니다. 첫 번째 단계에서는 모델 파라미터가 고정되고, 두 번째 단계에서는 모든 모델 파라미터가 함께 훈련됩니다.\n-\n-BART는 이후 번역을 위해 다양한 언어로 사전훈련된 다국어 버전의 mBART로 확장되었습니다.\n-\n-번역에 직접 도전할 준비가 되셨나요? 완전한 [번역 가이드](tasks/summarization)를 확인하여 T5를 미세 조정하고 추론에 사용하는 방법을 학습하세요!\n-\n-<Tip>\n-\n-텍스트 생성에 대한 자세한 내용은 [텍스트 생성 전략](generation_strategies) 가이드를 확인하세요!\n-\n-</Tip>\n\\ No newline at end of file"
        },
        {
            "sha": "0b47d6fbad89d61dd4708285ab7960165e2124ca",
            "filename": "docs/source/ko/tf_xla.md",
            "status": "removed",
            "additions": 0,
            "deletions": 174,
            "changes": 174,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Ftf_xla.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e20698985887215f7e91a02621265f047af2d7/docs%2Fsource%2Fko%2Ftf_xla.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftf_xla.md?ref=c4e20698985887215f7e91a02621265f047af2d7",
            "patch": "@@ -1,174 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# TensorFlow 모델을 위한 XLA 통합 [[xla-integration-for-tensorflow-models]]\n-\n-[[open-in-colab]]\n-\n-XLA(Accelerated Linear Algebra)는 TensorFlow 모델의 실행 시간을 가속화하기 위한 컴파일러입니다. [공식 문서](https://www.tensorflow.org/xla)에 따르면 다음과 같습니다:\n-\n-XLA(Accelerated Linear Algebra)는 선형 대수를 위한 도메인 특화 컴파일러로, TensorFlow 모델을 소스 코드 변경 없이 가속화할 수 있습니다.\n-\n-TensorFlow에서 XLA를 사용하는 것은 간단합니다. XLA는 `tensorflow` 라이브러리 내에 패키지로 제공되며, [`tf.function`](https://www.tensorflow.org/guide/intro_to_graphs)과 같은 그래프 생성 함수에서 `jit_compile` 인수를 사용하여 활성화할 수 있습니다. `fit()` 및 `predict()`와 같은 Keras 메소드를 사용하는 경우, `jit_compile` 인수를 `model.compile()`에 전달하여 XLA를 간단하게 활성화할 수 있습니다. 그러나 XLA는 이러한 메소드에 국한되지 않고 임의의 `tf.function`을 가속화하는 데에도 사용할 수 있습니다.\n-\n-🤗 Transformers에서는 [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2), [T5](https://huggingface.co/docs/transformers/model_doc/t5), [OPT](https://huggingface.co/docs/transformers/model_doc/opt)와 같은 모델의 텍스트 생성, 그리고 [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)와 같은 모델의 음성 처리를 포함하여 여러 TensorFlow 메소드가 XLA와 호환되도록 다시 작성되었습니다.\n-\n-정확한 속도 향상은 모델에 따라 다르지만, 🤗 Transformers 내의 TensorFlow 텍스트 생성 모델의 경우 최대 100배의 속도 향상을 확인했습니다. 이 문서에서는 이러한 모델에 대해 XLA를 사용하여 최대 성능을 얻는 방법을 설명합니다. 또한 XLA 통합의 벤치마크 및 디자인 철학에 대한 추가 자료 링크도 제공할 것입니다.\n-\n-## XLA를 사용하여 TF 함수 실행하기 [[running-tf-functions-with-xla]]\n-\n-TensorFlow에서 다음과 같은 모델을 고려해 봅시다:\n-\n-```py\n-import tensorflow as tf\n-\n-model = tf.keras.Sequential(\n-    [tf.keras.layers.Dense(10, input_shape=(10,), activation=\"relu\"), tf.keras.layers.Dense(5, activation=\"softmax\")]\n-)\n-```\n-\n-위 모델은 차원이 `(10, )`인 입력을 받습니다. 다음과 같이 모델을 사용하여 순전파를 실행할 수 있습니다:\n-\n-```py\n-# 모델에 대한 임의의 입력을 생성합니다.\n-batch_size = 16\n-input_vector_dim = 10\n-random_inputs = tf.random.normal((batch_size, input_vector_dim))\n-\n-# 순전파를 실행합니다.\n-_ = model(random_inputs)\n-```\n-\n-XLA로 컴파일된 함수로 순전파를 실행하려면 다음과 같이 해야 합니다:\n-\n-```py\n-xla_fn = tf.function(model, jit_compile=True)\n-_ = xla_fn(random_inputs)\n-```\n-\n-`model`의 기본 `call()` 함수는 XLA 그래프를 컴파일하는 데 사용됩니다. 그러나 다른 모델 함수를 XLA로 컴파일하려면 다음과 같이 할 수도 있습니다:\n-\n-```py\n-my_xla_fn = tf.function(model.my_xla_fn, jit_compile=True)\n-```\n-\n-## 🤗 Transformers에서 XLA를 사용하여 TF 텍스트 생성 모델 실행하기 [[running-a-tf-text-generation-model-with-xla-from-transformers]]\n-\n-🤗 Transformers에서 XLA로 가속화된 생성을 활성화하려면 최신 버전의 `transformers`가 설치되어 있어야 합니다. 다음과 같이 설치할 수 있습니다:\n-\n-```bash\n-pip install transformers --upgrade\n-```\n-\n-그리고 다음 코드를 실행할 수 있습니다:\n-\n-```py\n-import tensorflow as tf\n-from transformers import AutoTokenizer, TFAutoModelForCausalLM\n-\n-# 최소 버전의 Transformers가 설치되어 있지 않다면 오류가 발생합니다.\n-from transformers.utils import check_min_version\n-\n-check_min_version(\"4.21.0\")\n-\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\n-model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-input_string = [\"TensorFlow is\"]\n-\n-# XLA 생성 함수를 만들기 위한 한 줄\n-xla_generate = tf.function(model.generate, jit_compile=True)\n-\n-tokenized_input = tokenizer(input_string, return_tensors=\"tf\")\n-generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n-\n-decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n-print(f\"Generated -- {decoded_text}\")\n-# Generated -- TensorFlow is an open-source, open-source, distributed-source application # framework for the\n-```\n-\n-알 수 있듯이, `generate()`에서 XLA를 활성화하는 것은 단 한 줄의 코드입니다. 코드의 나머지 부분은 변경되지 않습니다. 그러나 위 코드 스니펫에서는 XLA에 특정한 몇 가지 주의할 점이 있습니다. XLA가 가져다줄 속도 향상을 실현하기 위해서는 이를 알고 있어야 합니다. 다음 섹션에서 이에 대해 논의합니다.\n-\n-## 주의할 점 [[gotchas-to-be-aware-of]]\n-\n-XLA 활성화 함수(`xla_generate()`와 같은)를 처음 실행할 때 내부적으로 계산 그래프를 추론하려고 하며, 이는 시간이 소요됩니다. 이 과정은 [“추적(tracing)”](https://www.tensorflow.org/guide/intro_to_graphs#when_is_a_function_tracing)이라고 알려져 있습니다.\n-\n-생성 시간이 빠르지 않다는 것을 알 수 있을 것입니다. `xla_generate()`(또는 다른 XLA 활성화 함수)의 연속 호출은 함수에 전달된 입력이 초기에 구축된 계산 그래프와 동일한 형태를 따른다면, 계산 그래프를 추론할 필요가 없습니다. 이는 입력 형태가 고정된 모달리티(예: 이미지)에는 문제가 되지 않지만, 가변 입력 형태 모달리티(예: 텍스트)를 사용할 때 주의해야 합니다.\n-\n-`xla_generate()`가 항상 동일한 입력 형태로 동작하도록 하려면, 토크나이저를 호출할 때 `padding` 인수를 지정할 수 있습니다.\n-\n-```py\n-import tensorflow as tf\n-from transformers import AutoTokenizer, TFAutoModelForCausalLM\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\n-model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-input_string = [\"TensorFlow is\"]\n-\n-xla_generate = tf.function(model.generate, jit_compile=True)\n-\n-# 여기서, padding 옵션이 있는 토크나이저를 호출합니다.\n-tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n-\n-generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n-decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n-print(f\"Generated -- {decoded_text}\")\n-```\n-\n-이렇게 하면 `xla_generate()`에 대한 입력이 항상 추적된 형태로 전달되어 생성 시간이 가속화됩니다. 다음 코드로 이를 확인할 수 있습니다:\n-\n-```py\n-import time\n-import tensorflow as tf\n-from transformers import AutoTokenizer, TFAutoModelForCausalLM\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\n-model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-\n-xla_generate = tf.function(model.generate, jit_compile=True)\n-\n-for input_string in [\"TensorFlow is\", \"TensorFlow is a\", \"TFLite is a\"]:\n-    tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n-    start = time.time_ns()\n-    generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n-    end = time.time_ns()\n-    print(f\"Execution time -- {(end - start) / 1e6:.1f} ms\\n\")\n-```\n-\n-Tesla T4 GPU에서는 다음과 같은 출력을 예상할 수 있습니다:\n-\n-```bash\n-Execution time -- 30819.6 ms\n-\n-Execution time -- 79.0 ms\n-\n-Execution time -- 78.9 ms\n-```\n-`xla_generate()`의 첫 번째 호출은 추적 때문에 시간이 오래 걸리지만, 연속 호출은 몇 배나 빠릅니다. 생성 옵션에 대한 어떤 변경이든 다시 추적을 유발하므로 생성 시간이 느려질 수 있음을 명심하세요.\n-\n-이 문서에서는 🤗 Transformers에서 제공하는 모든 텍스트 생성 옵션을 다루지 않았습니다. 고급 사용 사례에 대해 문서를 참조하시기 바랍니다.\n-\n-## 추가 자료 [[additional-resources]]\n-\n-여기에 🤗 Transformers와 XLA에 대해 더 자세히 알고 싶은 경우 도움이 될 수 있는 몇 가지 추가 자료를 제공합니다. \n- \n-* [이 Colab 노트북](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/91_tf_xla_generate.ipynb)은 XLA와 호환되는 인코더-디코더([T5](https://huggingface.co/docs/transformers/model_doc/t5)와 같은) 및 디코더 전용([GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)와 같은) 텍스트 생성 모델을 실험해 볼 수 있는 대화형 데모를 제공합니다.\n-* [이 블로그 글](https://huggingface.co/blog/tf-xla-generate)은 TensorFlow에서 XLA에 대한 친절한 소개와 함께 XLA와 호환되는 모델의 비교 벤치마크에 대한 개요를 제공합니다.\n-* [이 블로그 글](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html)은 🤗 Transformers의 TensorFlow 모델에 XLA 지원을 추가하는 것에 대한 디자인 철학을 논의합니다.\n-* XLA와 TensorFlow 그래프에 대해 더 자세히 알고 싶은 경우 추천하는 글:\n-    * [XLA: 기계 학습을 위한 최적화 컴파일러](https://www.tensorflow.org/xla)\n-    * [그래프 및 tf.function 소개](https://www.tensorflow.org/guide/intro_to_graphs)\n-    * [tf.function으로 성능 향상하기](https://www.tensorflow.org/guide/function) \n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 3351,
        "additions": 331,
        "deletions": 3020
    }
}