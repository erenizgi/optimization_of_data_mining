{
    "author": "Cyrilvallez",
    "message": "Fix chunked attention mask with left-padding (#40324)\n\n* add fix\n\n* add test\n\n* raise proper warning for older versions\n\n* fix\n\n* fix and add 2nd test\n\n* fix for flex and torch 2.5",
    "sha": "c2e3cc24e09a3113e650d381c9913a3ccfa23fa3",
    "files": [
        {
            "sha": "931c58870d6202d93bdb36fb357103f22819f531",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 43,
            "deletions": 8,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/c2e3cc24e09a3113e650d381c9913a3ccfa23fa3/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c2e3cc24e09a3113e650d381c9913a3ccfa23fa3/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=c2e3cc24e09a3113e650d381c9913a3ccfa23fa3",
            "patch": "@@ -20,7 +20,7 @@\n \n from .cache_utils import Cache\n from .configuration_utils import PretrainedConfig\n-from .utils import is_torch_xpu_available\n+from .utils import is_torch_xpu_available, logging\n from .utils.generic import GeneralInterface\n from .utils.import_utils import is_torch_flex_attn_available, is_torch_greater_or_equal, is_torchdynamo_compiling\n \n@@ -40,6 +40,9 @@\n     from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex\n \n \n+logger = logging.get_logger(__name__)\n+\n+\n def and_masks(*mask_functions: list[Callable]) -> Callable:\n     \"\"\"Returns a mask function that is the intersection of provided mask functions\"\"\"\n     if not all(callable(arg) for arg in mask_functions):\n@@ -87,12 +90,24 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n     return inner_mask\n \n \n-def chunked_overlay(chunk_size: int) -> Callable:\n+def chunked_overlay(chunk_size: int, left_padding: torch.Tensor) -> Callable:\n     \"\"\"\n     This is an overlay depicting a chunked attention pattern. Add it on top of a causal mask for a proper chunked\n     attention mask.\n     \"\"\"\n \n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        return (kv_idx - left_padding[batch_idx]) // chunk_size == (q_idx - left_padding[batch_idx]) // chunk_size\n+\n+    return inner_mask\n+\n+\n+def _legacy_chunked_overlay(chunk_size: int) -> Callable:\n+    \"\"\"\n+    Same as the above function, but do not correctly account for left padding tokens.\n+    Only kept for compatibility with older torch versions (< 2.6).\n+    \"\"\"\n+\n     def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n         return kv_idx // chunk_size == q_idx // chunk_size\n \n@@ -106,11 +121,13 @@ def sliding_window_causal_mask_function(sliding_window: int) -> Callable:\n     return and_masks(sliding_window_overlay(sliding_window), causal_mask_function)\n \n \n-def chunked_causal_mask_function(chunk_size: int) -> Callable:\n+def chunked_causal_mask_function(chunk_size: int, left_padding: torch.Tensor) -> Callable:\n     \"\"\"\n     This return the mask_function function to create a chunked attention mask.\n     \"\"\"\n-    return and_masks(chunked_overlay(chunk_size), causal_mask_function)\n+    if not _is_torch_greater_or_equal_than_2_6:\n+        return and_masks(_legacy_chunked_overlay(chunk_size), causal_mask_function)\n+    return and_masks(chunked_overlay(chunk_size, left_padding), causal_mask_function)\n \n \n def padding_mask_function(padding_mask: torch.Tensor) -> Callable:\n@@ -298,7 +315,7 @@ def sdpa_mask_recent_torch(\n     You can do\n \n     ```python\n-    >>> create_4d_causal_mask(batch_size=1, cache_position=torch.arange(5), kv_length=5)\n+    >>> sdpa_mask(batch_size=1, cache_position=torch.arange(5), kv_length=5)\n     >>> tensor([[[[ True, False, False, False, False],\n                   [ True,  True, False, False, False],\n                   [ True,  True,  True, False, False],\n@@ -319,7 +336,7 @@ def sdpa_mask_recent_torch(\n     You can do\n \n     ```python\n-    >>> create_4d_causal_mask(batch_size=1, cache_position=torch.arange(5), kv_length=5, mask_function=sliding_window_causal_mask_function(3))\n+    >>> sdpa_mask(batch_size=1, cache_position=torch.arange(5), kv_length=5, mask_function=sliding_window_causal_mask_function(3))\n     >>> tensor([[[[ True, False, False, False, False],\n                   [ True,  True, False, False, False],\n                   [ True,  True,  True, False, False],\n@@ -340,7 +357,7 @@ def sdpa_mask_recent_torch(\n     You can do\n \n     ```python\n-    >>> create_4d_causal_mask(batch_size=1, cache_position=torch.arange(5), kv_length=5, mask_function=chunked_causal_mask_function(3))\n+    >>> sdpa_mask(batch_size=1, cache_position=torch.arange(5), kv_length=5, mask_function=chunked_causal_mask_function(3, torch.zeros(1, dtype=int)))\n     >>> tensor([[[[ True, False, False, False, False],\n                 [ True,  True, False, False, False],\n                 [ True,  True,  True, False, False],\n@@ -973,7 +990,25 @@ def create_chunked_causal_mask(\n         )\n \n     batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n-    mask_factory_function = chunked_causal_mask_function(chunk_size)\n+    # For chunked attention and batched inputs, we need to take the number of left padding tokens into account\n+    # to start the chunk from the actual start of the sequence for the padded sequence\n+    if attention_mask is not None:\n+        # Only count the left padding tokens, not all of them\n+        left_padding_tokens = (attention_mask.cumsum(dim=-1) == torch.zeros_like(attention_mask)).sum(dim=-1)\n+    else:\n+        left_padding_tokens = torch.zeros(batch_size, device=cache_position.device, dtype=int)\n+    # Raise a warning for older versions if the problematic left-padding situation arises\n+    if (\n+        not _is_torch_greater_or_equal_than_2_6\n+        and kv_length + kv_offset > chunk_size\n+        and (left_padding_tokens > 0).any()\n+    ):\n+        logger.warning_once(\n+            \"Due to limitations of your current torch version, we cannot correctly account for the left-padding \"\n+            \"when computing the chunked attention pattern. This will lead to a wrong attention mask for the padded \"\n+            \"sequences. Behavior will be undefined. Please upgrade to `torch>=2.6` to solve this issue.\"\n+        )\n+    mask_factory_function = chunked_causal_mask_function(chunk_size, left_padding_tokens)\n     mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n \n     # Do not allow skip if we are compiling (this is to match BC)"
        },
        {
            "sha": "98ce87189fab5f6207f610b47aeb3209f0e74c81",
            "filename": "tests/utils/test_masking_utils.py",
            "status": "modified",
            "additions": 111,
            "deletions": 2,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/c2e3cc24e09a3113e650d381c9913a3ccfa23fa3/tests%2Futils%2Ftest_masking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c2e3cc24e09a3113e650d381c9913a3ccfa23fa3/tests%2Futils%2Ftest_masking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_masking_utils.py?ref=c2e3cc24e09a3113e650d381c9913a3ccfa23fa3",
            "patch": "@@ -21,8 +21,9 @@\n     import torch\n     from torch.nn.attention.flex_attention import create_block_mask\n \n-    from transformers import LlamaConfig\n-    from transformers.masking_utils import create_causal_mask, find_packed_sequence_indices\n+    from transformers import DynamicCache, LlamaConfig\n+    from transformers.cache_utils import DynamicSlidingWindowLayer\n+    from transformers.masking_utils import create_causal_mask, create_chunked_causal_mask, find_packed_sequence_indices\n \n \n # fmt: off\n@@ -135,3 +136,111 @@ def test_find_packed_sequence_indices(self):\n         position_ids = torch.tensor([[0, 1, 2, 3, 0, 1, 0, 1, 2, 3], [0, 1, 2, 3, 4, 5, 0, 1, 2, 3]])\n         EXPECTED_SEQUENCE_INDICES = torch.tensor([[0, 0, 0, 0, 1, 1, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]])\n         self.assertTrue((find_packed_sequence_indices(position_ids) == EXPECTED_SEQUENCE_INDICES).all())\n+\n+    def test_chunked_mask_with_left_padding_and_large_prefill(self):\n+        # Make sur we have an attention_chunk_size in the config\n+        config = LlamaConfig(attention_chunk_size=3, attn_implementation=\"sdpa\")\n+\n+        batch_size = 2\n+        sequence_length = 8\n+        pad_tokens = 4\n+\n+        input_ids = torch.randint(100, 200, (batch_size, sequence_length))\n+        attention_mask = torch.tensor(\n+            [[0 if i < pad_tokens else 1 for i in range(sequence_length)], [1] * sequence_length]\n+        )\n+        inputs_embeds = torch.empty_like(input_ids, dtype=torch.float16)\n+        cache_position = torch.arange(sequence_length)\n+        position_ids = torch.empty(batch_size, sequence_length, dtype=cache_position.dtype)\n+        position_ids[0, :pad_tokens] = 1\n+        position_ids[0, pad_tokens:] = torch.arange(sequence_length - pad_tokens)\n+        position_ids[1, :] = cache_position\n+\n+        chunked_attention_mask = create_chunked_causal_mask(\n+            config=config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=None,\n+            position_ids=position_ids,\n+        )\n+\n+        # fmt: off\n+        EXPECTED_CHUNKED_MASK = torch.tensor(\n+            # Here, for the padded sequence, the chunk size should start correctly at index 4 (otherwise, with 4 padding\n+            # tokens are chunk_size=3, the first chunk is from indices 0-2, then 3-6 if we don't account for the padding correctly)\n+            [[[[False, False, False, False, False, False, False, False],\n+                [False, False, False, False, False, False, False, False],\n+                [False, False, False, False, False, False, False, False],\n+                [False, False, False, False, False, False, False, False],\n+                [False, False, False, False,  True, False, False, False],\n+                [False, False, False, False,  True,  True, False, False],\n+                [False, False, False, False,  True,  True,  True, False],\n+                [False, False, False, False, False, False, False,  True]]],\n+\n+\n+            [[[ True, False, False, False, False, False, False, False],\n+                [ True,  True, False, False, False, False, False, False],\n+                [ True,  True,  True, False, False, False, False, False],\n+                [False, False, False,  True, False, False, False, False],\n+                [False, False, False,  True,  True, False, False, False],\n+                [False, False, False,  True,  True,  True, False, False],\n+                [False, False, False, False, False, False,  True, False],\n+                [False, False, False, False, False, False,  True,  True]]]],\n+            dtype=torch.bool)\n+        # fmt: on\n+\n+        self.assertTrue((chunked_attention_mask == EXPECTED_CHUNKED_MASK).all())\n+\n+    def test_chunked_mask_with_left_padding_decoding(self):\n+        # Make sur we have an attention_chunk_size in the config\n+        config = LlamaConfig(attention_chunk_size=4, attn_implementation=\"sdpa\", num_hidden_layers=1)\n+\n+        cache = DynamicCache(config=config)\n+        # Sanity check\n+        self.assertEqual(len(cache), 1)\n+        self.assertTrue(isinstance(cache.layers[0], DynamicSlidingWindowLayer))\n+\n+        # Fill-in the Cache (sequence length is bigger than chunk size here)\n+        batch_size = 2\n+        prefill_size = 8\n+        pad_tokens = 7\n+        fake_kv = torch.rand(batch_size, 32, prefill_size, 32)\n+        cache.update(fake_kv, fake_kv, 0, torch.arange(prefill_size))\n+\n+        # Create a new input after the prefill\n+        input_ids = torch.randint(100, 200, (batch_size, 1))\n+        attention_mask = torch.tensor(\n+            [[0 if i < pad_tokens else 1 for i in range(prefill_size + 1)], [1] * (prefill_size + 1)]\n+        )\n+        inputs_embeds = torch.empty_like(input_ids, dtype=torch.float16)\n+        cache_position = torch.tensor([prefill_size], dtype=int)\n+        position_ids = torch.tensor([[prefill_size - pad_tokens], [prefill_size]])\n+\n+        chunked_attention_mask = create_chunked_causal_mask(\n+            config=config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=cache,\n+            position_ids=position_ids,\n+        )\n+\n+        # To understand a bit more the following expected mask, here is the full 2d mask, where the \"|\" characters are the chunk\n+        # separators (where the tokens should stop seeing each other)\n+        # [0, 0, 0, 0, 0, 0, 0, | 1, 1],    -> due to left padding, the first chunk only starts after the padding tokens\n+        # [| 1, 1, 1, 1, | 1, 1, 1, 1, | 1]])  -> easy case, each 4 tokens is a new chunk\n+\n+        # fmt: off\n+        EXPECTED_CHUNKED_MASK = torch.tensor(\n+            # Here, for the padded sequence, the chunk size should start correctly at index 7 (the first unpadded\n+            # index), and so only indices 7 and 8 should be True\n+            [[[[False, False,  True,  True]]],\n+\n+            # Here, for the unpadded sequence, the chunks start at index 0. Since we have 9 tokens in total, the last\n+            # token (index 8) will only see itself (we have 2 full chunks before)\n+            [[[False, False, False,  True]]]],\n+            dtype=torch.bool)\n+        # fmt: on\n+\n+        self.assertTrue((chunked_attention_mask == EXPECTED_CHUNKED_MASK).all())"
        }
    ],
    "stats": {
        "total": 164,
        "additions": 154,
        "deletions": 10
    }
}