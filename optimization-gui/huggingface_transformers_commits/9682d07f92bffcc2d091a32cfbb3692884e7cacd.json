{
    "author": "paulpak58",
    "message": "LFM2 (#39340)\n\n* [modeling][lfm2] LFM2 model on 4.53.0 interface\n\n* [configuration] hook in LFM2 keys\n\n* [modeling][lfm2] update modeling interface for 4.53.1\n\n* [modeling][lfm2] apply mask to hidden conv states\n\n* [misc] ruff format/lint\n\n* [modeling][lfm2] minor: NotImplemented legacy cache conversion\n\n* Create lfm2.md\n\n* create nice modular\n\n* style\n\n* Update modeling_auto.py\n\n* clean and start adding tests\n\n* style\n\n* Update test_modeling_lfm2.py\n\n* Update __init__.py\n\n* small test model size\n\n* config\n\n* small fix\n\n* fix\n\n* remove useless config attrs -> block_dim and conv_dim are hiden_size\n\n* fix prepare inputs\n\n* fix config\n\n* test\n\n* typo\n\n* skip tests accordingly\n\n* config docstrings\n\n* add doc to .md\n\n* skip config docstring check\n\n---------\n\nCo-authored-by: Maxime Labonne <81252890+mlabonne@users.noreply.github.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "9682d07f92bffcc2d091a32cfbb3692884e7cacd",
    "files": [
        {
            "sha": "12a46e02c40a38f314086434e28e52b7eadde972",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -517,6 +517,8 @@\n         title: Jukebox\n       - local: model_doc/led\n         title: LED\n+      - local: model_doc/lfm2\n+        title: LFM2\n       - local: model_doc/llama\n         title: LLaMA\n       - local: model_doc/llama2"
        },
        {
            "sha": "c94e421d76911d9b18c6a357d9108e1d2e30295f",
            "filename": "docs/source/en/model_doc/lfm2.md",
            "status": "added",
            "additions": 84,
            "deletions": 0,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2.md?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -0,0 +1,84 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+# LFM2\n+\n+## Overview\n+\n+[LFM2](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models) represents a new generation of Liquid Foundation Models developed by [Liquid AI](https://liquid.ai/), specifically designed for edge AI and on-device deployment. \n+\n+The models are available in three sizes (350M, 700M, and 1.2B parameters) and are engineered to run efficiently on CPU, GPU, and NPU hardware, making them particularly well-suited for applications requiring low latency, offline operation, and privacy.\n+\n+## Architecture\n+\n+The architecture consists of 16 blocks total: 10 double-gated short-range convolution blocks and 6 blocks of grouped query attention. This design stems from the concept of dynamical systems, where linear operations are modulated by input-dependent gates, allowing for \"liquid\" dynamics that can adapt in real-time. The short convolutions are particularly optimized for embedded SoC CPUs, making them ideal for devices that require fast, local inference without relying on cloud connectivity.\n+\n+The key architectural innovation of LFM2 lies in its systematic approach to balancing quality, latency, and memory efficiency through our STAR neural architecture search engine. Using STAR, Liquid AI optimized the models for real-world performance on embedded hardware, measuring actual peak memory usage and inference speed on Qualcomm Snapdragon processors. This results in models that achieve 2x faster decode and prefill performance compared to similar-sized models, while maintaining superior benchmark performance across knowledge, mathematics, instruction following, and multilingual tasks.\n+\n+## Example\n+\n+The following example shows how to generate an answer using the `AutoModelForCausalLM` class.\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+# Load model and tokenizer\n+model_id = \"LiquidAI/LFM2-1.2B\"\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    torch_dtype=\"bfloat16\",\n+)\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+# Generate answer\n+prompt = \"What is C. elegans?\"\n+input_ids = tokenizer.apply_chat_template(\n+    [{\"role\": \"user\", \"content\": prompt}],\n+    add_generation_prompt=True,\n+    return_tensors=\"pt\",\n+    tokenize=True,\n+)\n+\n+output = model.generate(\n+    input_ids,\n+    do_sample=True,\n+    temperature=0.3,\n+    min_p=0.15,\n+    repetition_penalty=1.05,\n+    max_new_tokens=512,\n+)\n+\n+print(tokenizer.decode(output[0], skip_special_tokens=False))\n+```\n+\n+## Lfm2Config\n+\n+[[autodoc]] Lfm2Config\n+\n+## Lfm2Model\n+\n+[[autodoc]] Lfm2Model\n+    - forward\n+\n+## Lfm2ForCausalLM\n+\n+[[autodoc]] Lfm2ForCausalLM\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "1daab346d94d3ef84f9dd92da78433a6df213d26",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -1989,6 +1989,7 @@ def _supports_default_dynamic_cache(self) -> bool:\n             and \"zamba\" not in self.__class__.__name__.lower()\n             and \"bamba\" not in self.__class__.__name__.lower()\n             and \"minimax\" not in self.__class__.__name__.lower()\n+            and \"lfm2\" not in self.__class__.__name__.lower()\n         )\n \n     def _prepare_cache_for_generation("
        },
        {
            "sha": "d2c59c0e8ffa34662320b74c37d6fba432fcf25a",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -168,6 +168,7 @@\n     from .layoutxlm import *\n     from .led import *\n     from .levit import *\n+    from .lfm2 import *\n     from .lightglue import *\n     from .lilt import *\n     from .llama import *"
        },
        {
            "sha": "9eaa3fc669eacf105a46f4a17c53539e8051f03c",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -201,6 +201,7 @@\n         (\"layoutlmv3\", \"LayoutLMv3Config\"),\n         (\"led\", \"LEDConfig\"),\n         (\"levit\", \"LevitConfig\"),\n+        (\"lfm2\", \"Lfm2Config\"),\n         (\"lightglue\", \"LightGlueConfig\"),\n         (\"lilt\", \"LiltConfig\"),\n         (\"llama\", \"LlamaConfig\"),\n@@ -591,6 +592,7 @@\n         (\"layoutxlm\", \"LayoutXLM\"),\n         (\"led\", \"LED\"),\n         (\"levit\", \"LeViT\"),\n+        (\"lfm2\", \"Lfm2\"),\n         (\"lightglue\", \"LightGlue\"),\n         (\"lilt\", \"LiLT\"),\n         (\"llama\", \"LLaMA\"),"
        },
        {
            "sha": "7b27324511f1a7fc861c3cf9ebaea83f89795ed3",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -190,6 +190,7 @@\n         (\"layoutlmv3\", \"LayoutLMv3Model\"),\n         (\"led\", \"LEDModel\"),\n         (\"levit\", \"LevitModel\"),\n+        (\"lfm2\", \"Lfm2Model\"),\n         (\"lightglue\", \"LightGlueForKeypointMatching\"),\n         (\"lilt\", \"LiltModel\"),\n         (\"llama\", \"LlamaModel\"),\n@@ -614,6 +615,7 @@\n         (\"helium\", \"HeliumForCausalLM\"),\n         (\"jamba\", \"JambaForCausalLM\"),\n         (\"jetmoe\", \"JetMoeForCausalLM\"),\n+        (\"lfm2\", \"Lfm2ForCausalLM\"),\n         (\"llama\", \"LlamaForCausalLM\"),\n         (\"llama4\", \"Llama4ForCausalLM\"),\n         (\"llama4_text\", \"Llama4ForCausalLM\"),"
        },
        {
            "sha": "1210de68df09a6a382c54d05d64c13efbf1cce4f",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -32,7 +32,7 @@\n import transformers.models.jamba.modeling_jamba as modeling_jamba\n from transformers.activations import ACT2FN\n \n-from ...cache_utils import Cache  # we need __iter__ and __len__ of pkv\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter"
        },
        {
            "sha": "20067056ba5443913e293cd053101bf69f90b22d",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -32,10 +32,7 @@\n \n from transformers.activations import ACT2FN\n \n-from ...cache_utils import (\n-    Cache,\n-    DynamicCache,  # we need __iter__ and __len__ of pkv\n-)\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter"
        },
        {
            "sha": "ce2f13eb0eaedc5929a4bbc9560b9c694b461e08",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -28,7 +28,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache  # we need __iter__ and __len__ of pkv\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available"
        },
        {
            "sha": "239ab879832c57465eb85f910cadd0f373abe46a",
            "filename": "src/transformers/models/lfm2/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Flfm2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Flfm2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2F__init__.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_lfm2 import *\n+    from .modeling_lfm2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "ce331a311a7ebd8c3b4719b35a4a7992b4f60a7f",
            "filename": "src/transformers/models/lfm2/configuration_lfm2.py",
            "status": "added",
            "additions": 165,
            "deletions": 0,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -0,0 +1,165 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional\n+\n+from ...configuration_utils import PretrainedConfig\n+\n+\n+class Lfm2Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Lfm2Model`]. It is used to instantiate a LFM2\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the LFM2-1.2B model.\n+    e.g. [LiquidAI/LFM2-1.2B](https://huggingface.co/LiquidAI/LFM2-1.2B)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 65536):\n+            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Lfm2Model`]\n+        hidden_size (`int`, *optional*, defaults to 2560):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 12288):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 8):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        max_position_embeddings (`int`, *optional*, defaults to 128000):\n+            The maximum sequence length that this model might ever be used with. Lfm2 1 supports up to 2048 tokens,\n+            Lfm2 2 up to 4096, CodeLfm2 up to 16384.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 1000000.0):\n+            The base period of the RoPE embeddings.\n+        conv_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use bias in the conv layers.\n+        conv_L_cache (`int`, *optional*, defaults to 3):\n+            L_cache dim in the conv layers.\n+        block_multiple_of (`int`, *optional*, defaults to 256):\n+            Multiple for the `intermediate_size`.\n+        block_ffn_dim_multiplier (`float`, *optional*, defaults to 1.0):\n+            Multiplier for the `intermediate_size`.\n+        block_auto_adjust_ff_dim (`bool`, *optional*, defaults to `True`):\n+            Whether to adjust the dim of the `intermediate_size`.\n+        full_attn_idxs (`Optional`, *optional*):\n+            Index of the layers which use attention.\n+        layer_types (`Optional`, *optional*):\n+            Type of each layers.\n+\n+    ```python\n+    >>> from transformers import Lfm2Model, Lfm2Config\n+\n+    >>> # Initializing a LFM2 model\n+    >>> configuration = Lfm2Config()\n+\n+    >>> # Initializing a model from the LFM2-1.2B style configuration\n+    >>> model = Lfm2Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"lfm2\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size: int = 65536,\n+        hidden_size: int = 2560,\n+        intermediate_size: int = 12288,\n+        num_hidden_layers: int = 32,\n+        num_attention_heads: int = 32,\n+        num_key_value_heads: int = 8,\n+        max_position_embeddings: int = 128_000,\n+        initializer_range: float = 0.02,\n+        norm_eps: float = 0.00001,\n+        use_cache: bool = True,\n+        pad_token_id: int = 0,\n+        bos_token_id: int = 1,\n+        eos_token_id: int = 2,\n+        tie_word_embeddings: bool = True,\n+        rope_theta: float = 1000000.0,\n+        conv_bias: bool = False,\n+        conv_L_cache: int = 3,\n+        block_multiple_of: int = 256,\n+        block_ffn_dim_multiplier: float = 1.0,\n+        block_auto_adjust_ff_dim: bool = True,\n+        full_attn_idxs: Optional[list[int]] = None,\n+        layer_types: Optional[list[str]] = None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.rope_theta = kwargs.get(\"theta\", rope_theta)  # to fit original config keys\n+        self.max_position_embeddings = max_position_embeddings\n+        self.use_cache = use_cache\n+        self.norm_eps = norm_eps\n+        self.initializer_range = initializer_range\n+\n+        # attn operator config\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+\n+        # custom operator config\n+        self.conv_bias = conv_bias\n+        self.conv_L_cache = conv_L_cache\n+\n+        # MLP config\n+        self.intermediate_size = kwargs.get(\"block_ff_dim\", intermediate_size)  # to fit original config keys\n+        self.block_multiple_of = block_multiple_of\n+        self.block_ffn_dim_multiplier = block_ffn_dim_multiplier\n+        self.block_auto_adjust_ff_dim = block_auto_adjust_ff_dim\n+\n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            full_attn_idxs = full_attn_idxs if full_attn_idxs is not None else list(range(num_hidden_layers))\n+            self.layer_types = [\"full_attention\" if i in full_attn_idxs else \"conv\" for i in range(num_hidden_layers)]\n+\n+        tie_word_embeddings = kwargs.get(\"tie_embedding\", tie_word_embeddings)  # to fit original config keys\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"Lfm2Config\"]"
        },
        {
            "sha": "049d5073f32344ef43dd3b73e9fa7f6a1caf15d4",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "added",
            "additions": 763,
            "deletions": 0,
            "changes": 763,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -0,0 +1,763 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/lfm2/modular_lfm2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_lfm2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import check_model_inputs\n+from ...utils.import_utils import is_causal_conv1d_available\n+from .configuration_lfm2 import Lfm2Config\n+\n+\n+if is_causal_conv1d_available():\n+    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+else:\n+    causal_conv1d_fn, causal_conv1d_update = None, None\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Lfm2RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Lfm2RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Lfm2RotaryEmbedding(nn.Module):\n+    def __init__(self, config: Lfm2Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class Lfm2MLP(nn.Module):\n+    def __init__(self, config: Lfm2Config):\n+        super().__init__()\n+        intermediate_size = config.intermediate_size\n+        if config.block_auto_adjust_ff_dim:\n+            intermediate_size = int(2 * intermediate_size / 3)\n+            # custom dim factor multiplier\n+            if config.block_ffn_dim_multiplier is not None:\n+                intermediate_size = int(config.block_ffn_dim_multiplier * intermediate_size)\n+                intermediate_size = config.block_multiple_of * (\n+                    (intermediate_size + config.block_multiple_of - 1) // config.block_multiple_of\n+                )\n+        self.w1 = nn.Linear(config.hidden_size, intermediate_size, bias=False)\n+        self.w3 = nn.Linear(config.hidden_size, intermediate_size, bias=False)\n+        self.w2 = nn.Linear(intermediate_size, config.hidden_size, bias=False)\n+\n+    def forward(self, x):\n+        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n+\n+\n+class Lfm2HybridConvCache(DynamicCache):\n+    \"\"\"\n+    Attention and conv cache for Lfm2.\n+\n+    It stores the Key and Value states as a list of tensors, one for each layer.\n+    Attention layer cache shape: `[batch_size, num_heads, seq_len, head_dim]`.\n+    Conv layer cache shape: `[batch_size, hidden_size, L_cache-1]`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: Lfm2Config,\n+        max_batch_size: int,\n+        dtype: torch.dtype = torch.float32,\n+        device: Union[torch.device, str, None] = None,\n+    ):\n+        super().__init__()  # initialize key and value cache\n+        self.max_batch_size = max_batch_size\n+        self.layer_types = config.layer_types\n+        self.first_attention_layer = self.layer_types.index(\"full_attention\")\n+        self.conv_L_cache = config.conv_L_cache\n+        self._dtype = dtype\n+\n+        self.conv_cache: list[torch.Tensor] = []\n+        device = torch.device(device) if device is not None else None\n+\n+        for _ in range(config.num_hidden_layers):\n+            conv_state = torch.zeros(\n+                self.max_batch_size,\n+                config.hidden_size,\n+                self.conv_L_cache,\n+                dtype=self._dtype,\n+                device=device,\n+            )\n+            torch._dynamo.mark_static_address(conv_state)\n+            self.conv_cache.append(conv_state)\n+\n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        layer_idx: int,\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n+\n+        Parameters:\n+            key_states (`torch.Tensor`):\n+                The new key states to cache.\n+            value_states (`torch.Tensor`):\n+                The new value states to cache.\n+            layer_idx (`int`):\n+                The index of the layer to cache the states for.\n+            cache_kwargs (`Dict[str, Any]`, `optional`):\n+                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n+\n+        Return:\n+            A tuple containing the updated key and value states.\n+        \"\"\"\n+        # Update the number of seen tokens\n+        if layer_idx == self.first_attention_layer:\n+            self._seen_tokens += key_states.shape[-2]\n+\n+        # Update the cache\n+        if key_states is not None:\n+            if len(self.key_cache) <= layer_idx:\n+                # There may be skipped layers, fill them with empty lists\n+                for _ in range(len(self.key_cache), layer_idx):\n+                    self.key_cache.append(torch.tensor([]))\n+                    self.value_cache.append(torch.tensor([]))\n+                self.key_cache.append(key_states)\n+                self.value_cache.append(value_states)\n+            elif (\n+                not self.key_cache[layer_idx].numel()  # prefers not t.numel() to len(t) == 0 to export the model\n+            ):  # fills previously skipped layers; checking for tensor causes errors\n+                self.key_cache[layer_idx] = key_states\n+                self.value_cache[layer_idx] = value_states\n+            else:\n+                self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n+                self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n+\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def reorder_cache(self, beam_idx: torch.LongTensor):\n+        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n+        for layer_idx in range(len(self.key_cache)):\n+            device = self.key_cache[layer_idx].device\n+            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+            device = self.value_cache[layer_idx].device\n+            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+            device = self.conv_cache[layer_idx].device\n+            self.conv_cache[layer_idx] = self.conv_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n+        # take any layer that contains cache and not empty tensor\n+        layer_idx = self.first_attention_layer if self.layer_types[layer_idx] != \"full_attention\" else layer_idx\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].numel() == 0:\n+            return 0\n+        return self.key_cache[layer_idx].shape[-2]\n+\n+    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n+        raise NotImplementedError(\"Lfm2HybridConvCache does not have a legacy cache equivalent.\")\n+\n+    @classmethod\n+    def from_legacy_cache(cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n+        raise NotImplementedError(\"Lfm2HybridConvCache does not have a legacy cache equivalent.\")\n+\n+    def reset(self):\n+        for layer_idx in range(len(self.conv_cache)):\n+            # In-place ops prevent breaking the static address\n+            self.conv_cache[layer_idx].zero_()\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Lfm2Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Lfm2Config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.is_causal = True\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.out_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.q_layernorm = Lfm2RMSNorm(self.head_dim, eps=config.norm_eps)\n+        self.k_layernorm = Lfm2RMSNorm(self.head_dim, eps=config.norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_layernorm(self.q_proj(hidden_states).view(*hidden_shape)).transpose(1, 2)\n+        key_states = self.k_layernorm(self.k_proj(hidden_states).view(*hidden_shape)).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        output = self.out_proj(attn_output)\n+        return output, attn_weights\n+\n+\n+def apply_mask_to_padding_states(hidden_states, attention_mask):\n+    \"\"\"\n+    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n+        dtype = hidden_states.dtype\n+        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n+\n+    return hidden_states\n+\n+\n+kernel_modules = (causal_conv1d_fn, causal_conv1d_update)\n+is_fast_path_available = all(kernel_modules)\n+\n+\n+class Lfm2ShortConv(nn.Module):\n+    def __init__(\n+        self,\n+        config: Lfm2Config,\n+        layer_idx: int,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.L_cache = config.conv_L_cache\n+        self.bias = config.conv_bias\n+\n+        self.conv = nn.Conv1d(\n+            in_channels=config.hidden_size,\n+            out_channels=config.hidden_size,\n+            kernel_size=self.L_cache,\n+            groups=config.hidden_size,\n+            bias=self.bias,\n+            padding=self.L_cache - 1,\n+        )\n+        self.in_proj = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=self.bias)\n+        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=self.bias)\n+\n+    def cuda_kernels_forward(\n+        self,\n+        x: torch.Tensor,\n+        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        x = apply_mask_to_padding_states(x, attention_mask)\n+        BCx = self.in_proj(x).transpose(-1, -2)\n+        B, C, x = BCx.chunk(3, dim=-2)\n+\n+        Bx = B * x\n+\n+        conv_weights = self.conv.weight.view(self.conv.weight.size(0), self.conv.weight.size(2))\n+        if past_key_value is not None and cache_position[0] > 0:\n+            conv_out = causal_conv1d_update(\n+                Bx.squeeze(-1),\n+                past_key_value.conv_cache[self.layer_idx],\n+                conv_weights,\n+                self.conv.bias,\n+                None,\n+            )\n+            conv_out = conv_out.unsqueeze(-1)\n+        else:\n+            if past_key_value is not None:\n+                conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n+                past_key_value.conv_cache[self.layer_idx].copy_(conv_state)\n+\n+            conv_out = causal_conv1d_fn(Bx, conv_weights, self.conv.bias, activation=None)\n+\n+        y = C * conv_out\n+        y = self.out_proj(y.transpose(-1, -2).contiguous())\n+        return y\n+\n+    def slow_forward(\n+        self,\n+        x: torch.Tensor,\n+        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        seqlen = x.shape[1]\n+\n+        x = apply_mask_to_padding_states(x, attention_mask)\n+        BCx = self.in_proj(x).transpose(-1, -2)\n+        B, C, x = BCx.chunk(3, dim=-2)\n+\n+        Bx = B * x\n+\n+        if past_key_value is not None and cache_position[0] > 0:\n+            conv_state = past_key_value.conv_cache[self.layer_idx]\n+            cache_position = cache_position.clamp(0, self.L_cache - 1)\n+            conv_state = conv_state.roll(shifts=-1, dims=-1)\n+            conv_state[:, :, cache_position] = Bx.to(device=conv_state.device, dtype=conv_state.dtype)\n+            past_key_value.conv_cache[self.layer_idx].copy_(conv_state)\n+            conv_out = torch.sum(conv_state.to(Bx.device) * self.conv.weight[:, 0, :], dim=-1)\n+            if self.bias:\n+                conv_out += self.conv.bias\n+\n+            conv_out = conv_out.unsqueeze(-1)\n+        else:\n+            if past_key_value is not None:\n+                conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n+                past_key_value.conv_cache[self.layer_idx].copy_(conv_state)\n+\n+            conv_out = self.conv(Bx)[..., :seqlen]\n+\n+        y = C * conv_out\n+        y = y.transpose(-1, -2).contiguous()\n+        y = self.out_proj(y)\n+        return y\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        if is_fast_path_available and \"cuda\" in hidden_states.device.type and not torch._dynamo.is_compiling():\n+            return self.cuda_kernels_forward(hidden_states, past_key_value, cache_position, attention_mask)\n+        return self.slow_forward(hidden_states, past_key_value, cache_position, attention_mask)\n+\n+\n+class Lfm2DecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Lfm2Config, layer_idx: int):\n+        super().__init__()\n+        self.is_attention_layer = config.layer_types[layer_idx] == \"full_attention\"\n+\n+        if self.is_attention_layer:\n+            self.self_attn = Lfm2Attention(config, layer_idx)\n+        else:\n+            self.conv = Lfm2ShortConv(config, layer_idx)\n+        self.feed_forward = Lfm2MLP(config)\n+        self.operator_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.ffn_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        if self.is_attention_layer:\n+            hidden_states, _ = self.self_attn(\n+                hidden_states=self.operator_norm(hidden_states),\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+        else:\n+            hidden_states = self.conv(\n+                hidden_states=self.operator_norm(hidden_states),\n+                past_key_value=past_key_value,\n+                cache_position=cache_position,\n+                attention_mask=attention_mask,\n+            )\n+        hidden_states = hidden_states + residual\n+        hidden_states = hidden_states + self.feed_forward(self.ffn_norm(hidden_states))\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Lfm2PreTrainedModel(PreTrainedModel):\n+    config_class = Lfm2Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Lfm2DecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = False\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Lfm2DecoderLayer,\n+        \"attentions\": Lfm2Attention,\n+    }\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, (nn.Linear, nn.Conv1d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Lfm2RMSNorm):\n+            module.weight.data.fill_(1.0)\n+\n+\n+@auto_docstring\n+class Lfm2Model(Lfm2PreTrainedModel):\n+    def __init__(self, config: Lfm2Config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Lfm2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.rotary_emb = Lfm2RotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+        self.pos_emb = Lfm2RotaryEmbedding(config)\n+        self.embedding_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Lfm2HybridConvCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            batch_size = inputs_embeds.shape[0]\n+            past_key_values = Lfm2HybridConvCache(\n+                config=self.config, max_batch_size=batch_size, dtype=self.dtype, device=self.device\n+            )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.pos_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.embedding_norm(hidden_states)\n+\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class Lfm2ForCausalLM(Lfm2PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Lfm2Model(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Lfm2ForCausalLM\n+\n+        >>> model = Lfm2ForCausalLM.from_pretrained(\"meta-lfm2/Lfm2-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-lfm2/Lfm2-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"Lfm2ForCausalLM\", \"Lfm2Model\", \"Lfm2PreTrainedModel\"]"
        },
        {
            "sha": "e0d617daf60a05b6f302c9eb44aa957bf439e766",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "added",
            "additions": 500,
            "deletions": 0,
            "changes": 500,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -0,0 +1,500 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...cache_utils import DynamicCache\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, logging\n+from ...utils.import_utils import is_causal_conv1d_available\n+from ..bamba.modeling_bamba import apply_mask_to_padding_states\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaForCausalLM,\n+    LlamaModel,\n+    LlamaPreTrainedModel,\n+    LlamaRMSNorm,\n+    LlamaRotaryEmbedding,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+from .configuration_lfm2 import Lfm2Config\n+\n+\n+if is_causal_conv1d_available():\n+    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+else:\n+    causal_conv1d_fn, causal_conv1d_update = None, None\n+\n+\n+kernel_modules = (causal_conv1d_fn, causal_conv1d_update)\n+is_fast_path_available = all(kernel_modules)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Lfm2RMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class Lfm2RotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class Lfm2MLP(nn.Module):\n+    def __init__(self, config: Lfm2Config):\n+        super().__init__()\n+        intermediate_size = config.intermediate_size\n+        if config.block_auto_adjust_ff_dim:\n+            intermediate_size = int(2 * intermediate_size / 3)\n+            # custom dim factor multiplier\n+            if config.block_ffn_dim_multiplier is not None:\n+                intermediate_size = int(config.block_ffn_dim_multiplier * intermediate_size)\n+                intermediate_size = config.block_multiple_of * (\n+                    (intermediate_size + config.block_multiple_of - 1) // config.block_multiple_of\n+                )\n+        self.w1 = nn.Linear(config.hidden_size, intermediate_size, bias=False)\n+        self.w3 = nn.Linear(config.hidden_size, intermediate_size, bias=False)\n+        self.w2 = nn.Linear(intermediate_size, config.hidden_size, bias=False)\n+\n+    def forward(self, x):\n+        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n+\n+\n+class Lfm2HybridConvCache(DynamicCache):\n+    \"\"\"\n+    Attention and conv cache for Lfm2.\n+\n+    It stores the Key and Value states as a list of tensors, one for each layer.\n+    Attention layer cache shape: `[batch_size, num_heads, seq_len, head_dim]`.\n+    Conv layer cache shape: `[batch_size, hidden_size, L_cache-1]`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: Lfm2Config,\n+        max_batch_size: int,\n+        dtype: torch.dtype = torch.float32,\n+        device: Union[torch.device, str, None] = None,\n+    ):\n+        super().__init__()  # initialize key and value cache\n+        self.max_batch_size = max_batch_size\n+        self.layer_types = config.layer_types\n+        self.first_attention_layer = self.layer_types.index(\"full_attention\")\n+        self.conv_L_cache = config.conv_L_cache\n+        self._dtype = dtype\n+\n+        self.conv_cache: list[torch.Tensor] = []\n+        device = torch.device(device) if device is not None else None\n+\n+        for _ in range(config.num_hidden_layers):\n+            conv_state = torch.zeros(\n+                self.max_batch_size,\n+                config.hidden_size,\n+                self.conv_L_cache,\n+                dtype=self._dtype,\n+                device=device,\n+            )\n+            torch._dynamo.mark_static_address(conv_state)\n+            self.conv_cache.append(conv_state)\n+\n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        layer_idx: int,\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n+\n+        Parameters:\n+            key_states (`torch.Tensor`):\n+                The new key states to cache.\n+            value_states (`torch.Tensor`):\n+                The new value states to cache.\n+            layer_idx (`int`):\n+                The index of the layer to cache the states for.\n+            cache_kwargs (`Dict[str, Any]`, `optional`):\n+                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n+\n+        Return:\n+            A tuple containing the updated key and value states.\n+        \"\"\"\n+        # Update the number of seen tokens\n+        if layer_idx == self.first_attention_layer:\n+            self._seen_tokens += key_states.shape[-2]\n+\n+        # Update the cache\n+        if key_states is not None:\n+            if len(self.key_cache) <= layer_idx:\n+                # There may be skipped layers, fill them with empty lists\n+                for _ in range(len(self.key_cache), layer_idx):\n+                    self.key_cache.append(torch.tensor([]))\n+                    self.value_cache.append(torch.tensor([]))\n+                self.key_cache.append(key_states)\n+                self.value_cache.append(value_states)\n+            elif (\n+                not self.key_cache[layer_idx].numel()  # prefers not t.numel() to len(t) == 0 to export the model\n+            ):  # fills previously skipped layers; checking for tensor causes errors\n+                self.key_cache[layer_idx] = key_states\n+                self.value_cache[layer_idx] = value_states\n+            else:\n+                self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n+                self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n+\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def reorder_cache(self, beam_idx: torch.LongTensor):\n+        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n+        for layer_idx in range(len(self.key_cache)):\n+            device = self.key_cache[layer_idx].device\n+            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+            device = self.value_cache[layer_idx].device\n+            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+            device = self.conv_cache[layer_idx].device\n+            self.conv_cache[layer_idx] = self.conv_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n+        # take any layer that contains cache and not empty tensor\n+        layer_idx = self.first_attention_layer if self.layer_types[layer_idx] != \"full_attention\" else layer_idx\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].numel() == 0:\n+            return 0\n+        return self.key_cache[layer_idx].shape[-2]\n+\n+    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n+        raise NotImplementedError(\"Lfm2HybridConvCache does not have a legacy cache equivalent.\")\n+\n+    @classmethod\n+    def from_legacy_cache(cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n+        raise NotImplementedError(\"Lfm2HybridConvCache does not have a legacy cache equivalent.\")\n+\n+    def reset(self):\n+        for layer_idx in range(len(self.conv_cache)):\n+            # In-place ops prevent breaking the static address\n+            self.conv_cache[layer_idx].zero_()\n+\n+\n+class Lfm2Attention(LlamaAttention):\n+    def __init__(self, config: Lfm2Config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.out_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.q_layernorm = Lfm2RMSNorm(self.head_dim, eps=config.norm_eps)\n+        self.k_layernorm = Lfm2RMSNorm(self.head_dim, eps=config.norm_eps)\n+        del self.o_proj\n+        del self.attention_dropout\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_layernorm(self.q_proj(hidden_states).view(*hidden_shape)).transpose(1, 2)\n+        key_states = self.k_layernorm(self.k_proj(hidden_states).view(*hidden_shape)).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        output = self.out_proj(attn_output)\n+        return output, attn_weights\n+\n+\n+class Lfm2ShortConv(nn.Module):\n+    def __init__(\n+        self,\n+        config: Lfm2Config,\n+        layer_idx: int,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.L_cache = config.conv_L_cache\n+        self.bias = config.conv_bias\n+\n+        self.conv = nn.Conv1d(\n+            in_channels=config.hidden_size,\n+            out_channels=config.hidden_size,\n+            kernel_size=self.L_cache,\n+            groups=config.hidden_size,\n+            bias=self.bias,\n+            padding=self.L_cache - 1,\n+        )\n+        self.in_proj = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=self.bias)\n+        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=self.bias)\n+\n+    def cuda_kernels_forward(\n+        self,\n+        x: torch.Tensor,\n+        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        x = apply_mask_to_padding_states(x, attention_mask)\n+        BCx = self.in_proj(x).transpose(-1, -2)\n+        B, C, x = BCx.chunk(3, dim=-2)\n+\n+        Bx = B * x\n+\n+        conv_weights = self.conv.weight.view(self.conv.weight.size(0), self.conv.weight.size(2))\n+        if past_key_value is not None and cache_position[0] > 0:\n+            conv_out = causal_conv1d_update(\n+                Bx.squeeze(-1),\n+                past_key_value.conv_cache[self.layer_idx],\n+                conv_weights,\n+                self.conv.bias,\n+                None,\n+            )\n+            conv_out = conv_out.unsqueeze(-1)\n+        else:\n+            if past_key_value is not None:\n+                conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n+                past_key_value.conv_cache[self.layer_idx].copy_(conv_state)\n+\n+            conv_out = causal_conv1d_fn(Bx, conv_weights, self.conv.bias, activation=None)\n+\n+        y = C * conv_out\n+        y = self.out_proj(y.transpose(-1, -2).contiguous())\n+        return y\n+\n+    def slow_forward(\n+        self,\n+        x: torch.Tensor,\n+        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        seqlen = x.shape[1]\n+\n+        x = apply_mask_to_padding_states(x, attention_mask)\n+        BCx = self.in_proj(x).transpose(-1, -2)\n+        B, C, x = BCx.chunk(3, dim=-2)\n+\n+        Bx = B * x\n+\n+        if past_key_value is not None and cache_position[0] > 0:\n+            conv_state = past_key_value.conv_cache[self.layer_idx]\n+            cache_position = cache_position.clamp(0, self.L_cache - 1)\n+            conv_state = conv_state.roll(shifts=-1, dims=-1)\n+            conv_state[:, :, cache_position] = Bx.to(device=conv_state.device, dtype=conv_state.dtype)\n+            past_key_value.conv_cache[self.layer_idx].copy_(conv_state)\n+            conv_out = torch.sum(conv_state.to(Bx.device) * self.conv.weight[:, 0, :], dim=-1)\n+            if self.bias:\n+                conv_out += self.conv.bias\n+\n+            conv_out = conv_out.unsqueeze(-1)\n+        else:\n+            if past_key_value is not None:\n+                conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n+                past_key_value.conv_cache[self.layer_idx].copy_(conv_state)\n+\n+            conv_out = self.conv(Bx)[..., :seqlen]\n+\n+        y = C * conv_out\n+        y = y.transpose(-1, -2).contiguous()\n+        y = self.out_proj(y)\n+        return y\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        past_key_value: Optional[Lfm2HybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        if is_fast_path_available and \"cuda\" in hidden_states.device.type and not torch._dynamo.is_compiling():\n+            return self.cuda_kernels_forward(hidden_states, past_key_value, cache_position, attention_mask)\n+        return self.slow_forward(hidden_states, past_key_value, cache_position, attention_mask)\n+\n+\n+class Lfm2DecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Lfm2Config, layer_idx: int):\n+        super().__init__()\n+        self.is_attention_layer = config.layer_types[layer_idx] == \"full_attention\"\n+\n+        if self.is_attention_layer:\n+            self.self_attn = Lfm2Attention(config, layer_idx)\n+        else:\n+            self.conv = Lfm2ShortConv(config, layer_idx)\n+        self.feed_forward = Lfm2MLP(config)\n+        self.operator_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.ffn_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        if self.is_attention_layer:\n+            hidden_states, _ = self.self_attn(\n+                hidden_states=self.operator_norm(hidden_states),\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+        else:\n+            hidden_states = self.conv(\n+                hidden_states=self.operator_norm(hidden_states),\n+                past_key_value=past_key_value,\n+                cache_position=cache_position,\n+                attention_mask=attention_mask,\n+            )\n+        hidden_states = hidden_states + residual\n+        hidden_states = hidden_states + self.feed_forward(self.ffn_norm(hidden_states))\n+\n+        return hidden_states\n+\n+\n+class Lfm2PreTrainedModel(LlamaPreTrainedModel):\n+    _supports_static_cache = False\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, (nn.Linear, nn.Conv1d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Lfm2RMSNorm):\n+            module.weight.data.fill_(1.0)\n+\n+\n+class Lfm2Model(LlamaModel):\n+    def __init__(self, config: Lfm2Config):\n+        super().__init__(config)\n+        self.pos_emb = Lfm2RotaryEmbedding(config)\n+        self.embedding_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n+        del self.norm\n+        del self.rotary_emv\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Lfm2HybridConvCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            batch_size = inputs_embeds.shape[0]\n+            past_key_values = Lfm2HybridConvCache(\n+                config=self.config, max_batch_size=batch_size, dtype=self.dtype, device=self.device\n+            )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.pos_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.embedding_norm(hidden_states)\n+\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+class Lfm2ForCausalLM(LlamaForCausalLM):\n+    pass\n+\n+\n+__all__ = [\"Lfm2ForCausalLM\", \"Lfm2Model\", \"Lfm2PreTrainedModel\"]"
        },
        {
            "sha": "6a82319cf9056acb9da8e526dd77fa9193b1c1dc",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -2503,6 +2503,7 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n             \"xlnet\",\n             \"zamba\",\n             \"zamba2\",\n+            \"lfm2\",\n         )\n         has_standard_cache = not any(\n             model_name in config.__class__.__name__.lower() for model_name in models_without_standard_cache"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/lfm2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/tests%2Fmodels%2Flfm2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/tests%2Fmodels%2Flfm2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2%2F__init__.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd"
        },
        {
            "sha": "173e5e101dbb0124e4f6cd5671ed4e1bc171c13c",
            "filename": "tests/models/lfm2/test_modeling_lfm2.py",
            "status": "added",
            "additions": 93,
            "deletions": 0,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -0,0 +1,93 @@\n+# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch LLaMA model.\"\"\"\n+\n+import unittest\n+\n+from transformers import is_torch_available\n+from transformers.testing_utils import (\n+    require_read_token,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+)\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+if is_torch_available():\n+    from transformers import Lfm2Config, Lfm2ForCausalLM, Lfm2Model\n+\n+\n+class Lfm2ModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = Lfm2Config\n+        base_model_class = Lfm2Model\n+        causal_lm_class = Lfm2ForCausalLM\n+\n+    def __init__(\n+        self,\n+        parent,\n+        layer_types=[\"full_attention\", \"conv\"],\n+    ):\n+        super().__init__(parent)\n+        self.layer_types = layer_types\n+\n+\n+@require_torch\n+class Lfm2ModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (Lfm2Model, Lfm2ForCausalLM) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Lfm2Model,\n+            \"text-generation\": Lfm2ForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False\n+    model_tester_class = Lfm2ModelTester\n+    # used in `test_torch_compile_for_training`\n+    _torch_compile_train_cls = Lfm2ForCausalLM if is_torch_available() else None\n+\n+    @unittest.skip(\n+        \"Lfm2 alternates between attention and conv layers, so attention are only returned for attention layers\"\n+    )\n+    def test_attention_outputs(self):\n+        pass\n+\n+    @unittest.skip(\"Lfm2 has a special cache format as it alternates between attention and conv layers\")\n+    def test_past_key_values_format(self):\n+        pass\n+\n+    @unittest.skip(\"Lfm2 has a special cache format which is not compatible with contrastive search\")\n+    def test_contrastive_generate(self):\n+        pass\n+\n+    @unittest.skip(\"Lfm2 has a special cache format which is not compatible with contrastive search\")\n+    def test_contrastive_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(\"Lfm2 has a special cache format which is not compatible with contrastive search\")\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n+\n+@require_torch_accelerator\n+@require_read_token\n+@slow\n+class Lfm2IntegrationTest(unittest.TestCase):\n+    pass"
        },
        {
            "sha": "3795270baf7248d678f594cc76b208baed73eea5",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9682d07f92bffcc2d091a32cfbb3692884e7cacd/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9682d07f92bffcc2d091a32cfbb3692884e7cacd/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=9682d07f92bffcc2d091a32cfbb3692884e7cacd",
            "patch": "@@ -32,6 +32,7 @@\n CONFIG_MAPPING = transformers.models.auto.configuration_auto.CONFIG_MAPPING\n \n SPECIAL_CASES_TO_ALLOW = {\n+    \"Lfm2Config\": [\"full_attn_idxs\", \"tie_word_embeddings\"],\n     # used internally during generation to provide the custom logit processors with their necessary information\n     \"DiaConfig\": [\n         \"delay_pattern\","
        }
    ],
    "stats": {
        "total": 1651,
        "additions": 1645,
        "deletions": 6
    }
}