{
    "author": "remi-or",
    "message": "Add prefix sharing to continuous batching (#42094)\n\n* Fix a bug in the CB memory calcuation\n\n* Nit in example\n\n* Replace _free_blocks with a proper object BlockManager\n\n* Removed dead code\n\n* Added hasing mechanism (wip)\n\n* Added de-duplication\n\n* Add de-initialization mechnaism\n\n* Add prefix detection\n\n* Ensure we always keep 1 token for decode start\n\n* Removed some todos and small fix\n\n* Update src/transformers/generation/continuous_batching/cache.py\n\nCo-authored-by: Luc Georges <McPatate@users.noreply.github.com>\n\n* Update src/transformers/generation/continuous_batching/continuous_api.py\n\nCo-authored-by: Luc Georges <McPatate@users.noreply.github.com>\n\n* DOCSSSS\n\n* Review comments\n\n* Style\n\n* Added a flag to allow prefix sharing\n\n* [IMPORTANT] bug fix for prefix length memoization\n\n* Added a test for Cb prefix sharing\n\n* Example, start of refactor\n\n* End of refactor for example script\n\n* Added a do sample arg\n\n* Added reporting on prefix sharing\n\n* Added a context managr option for CB manager\n\n* Nit and style\n\n* Review comment from ArthurZucker\n\n---------\n\nCo-authored-by: Luc Georges <McPatate@users.noreply.github.com>",
    "sha": "47227f477532e8ff6dd3290dc6b9e0e38aef303b",
    "files": [
        {
            "sha": "1d584cbee1919de1c529058f47e5099a980fa43c",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 95,
            "deletions": 81,
            "changes": 176,
            "blob_url": "https://github.com/huggingface/transformers/blob/47227f477532e8ff6dd3290dc6b9e0e38aef303b/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47227f477532e8ff6dd3290dc6b9e0e38aef303b/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=47227f477532e8ff6dd3290dc6b9e0e38aef303b",
            "patch": "@@ -17,6 +17,7 @@\n import json\n import os\n import time\n+from itertools import cycle\n from typing import Optional\n \n import datasets\n@@ -29,42 +30,32 @@\n from transformers.generation.continuous_batching.requests import logger\n \n \n-# MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n-SLIDING_WINDOW = 0\n-MODEL_ID = \"google/gemma-2-2b-it\" if SLIDING_WINDOW > 0 else \"meta-llama/Meta-Llama-3-8B\"\n-FORCE_MAX_LENGTH = False  # should be False unless you are debugging sliding window features\n-SKIP_SPECIAL_TOKENS = False\n-\n-\n-def generate_simple(\n-    attn_impl: str, simple_batch_inputs: list[int], generation_config: GenerationConfig\n+def generate_without_cb(\n+    model_id: str, sliding_window: int, attn_impl: str, batched_inputs: list[int], generation_config: GenerationConfig\n ) -> dict[str, str]:\n-    attn_impl = {\n-        \"sdpa\": \"sdpa\",\n-        \"eager\": \"eager\",\n-        \"paged_attention\": \"eager\",  # TODO: this does not work on AMD docker\n-        \"flash_paged\": \"flash_attention_2\",  # TODO: this does not work on AMD docker\n-        \"kernels-community/flash-attn\": \"eager\",\n-    }[attn_impl]\n-\n-    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype=torch.bfloat16, attn_implementation=attn_impl)\n+    # Setup model and tokenizer\n+    model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, attn_implementation=attn_impl)\n     model = model.cuda().eval()\n-    if getattr(model.config, \"sliding_window\", None) is not None:\n-        model.config.sliding_window = SLIDING_WINDOW\n-\n+    if sliding_window > 0 and getattr(model.config, \"sliding_window\", None) is not None:\n+        model.config.sliding_window = sliding_window\n+    tokenizer = AutoTokenizer.from_pretrained(model_id)\n+    # Generate one by one\n     decoded_outputs = {}\n-    for input_ids in tqdm(simple_batch_inputs, desc=\"Generating outputs without CB\"):\n+    for input_ids in tqdm(batched_inputs, desc=\"Generating outputs without CB\"):\n         key = \" \".join(map(str, input_ids))  # This will be used to identify the output after batched generation\n         input_ids = torch.tensor([input_ids]).to(\"cuda\")\n-        # attention_mask = torch.ones_like(input_ids)\n-        outputs = model.generate(input_ids, generation_config=generation_config, use_model_defaults=False)\n+        attention_mask = torch.ones_like(input_ids)\n+        outputs = model.generate(\n+            input_ids, attention_mask=attention_mask, generation_config=generation_config, use_model_defaults=False\n+        )\n         generated_tokens = outputs[0][input_ids.shape[1] :]\n-        decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=SKIP_SPECIAL_TOKENS)\n-        decoded_outputs[key] = decoded_output\n+        decoded_outputs[key] = tokenizer.decode(generated_tokens, skip_special_tokens=False)\n     return decoded_outputs\n \n \n-def setup_metrics():\n+def maybe_setup_metrics(use_metrics: bool) -> None:\n+    if not use_metrics:\n+        return\n     try:\n         from opentelemetry import metrics, trace\n         from opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter\n@@ -119,16 +110,14 @@ def batch_generate(\n     token_count = 0\n     data = []\n     for i, request in enumerate(batch_outputs):\n-        input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=SKIP_SPECIAL_TOKENS)\n+        input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=False)\n         # The key is used to tie back to the output of unbatched generation\n         key = \" \".join(map(str, batch_outputs[request].prompt_ids))\n         data.append({\"input\": input_text, \"key\": key})\n \n         # Try to decode the output\n         try:\n-            output_text = tokenizer.decode(\n-                batch_outputs[request].generated_tokens, skip_special_tokens=SKIP_SPECIAL_TOKENS\n-            )\n+            output_text = tokenizer.decode(batch_outputs[request].generated_tokens, skip_special_tokens=False)\n             token_count += len(batch_outputs[request].generated_tokens[1:])\n             data[-1][\"cb_outputs\"] = output_text\n         except Exception as e:\n@@ -138,14 +127,7 @@ def batch_generate(\n \n         # Display sample if asked\n         if i < displayed_samples:\n-            if len(output_text) > 0:\n-                print(\"-\" * 20)\n-                print(f\"{request} Input:  {input_text}\")\n-                print(f\"{request} Output: {output_text}\")\n-            else:\n-                print(f\"{request} Input:  {input_text}\")\n-                print(\"[WARN]\")\n-                print(f\"{request} Output was empty!\")\n+            print(\"-\" * 20, f\"{request} Input:  {input_text}\", f\"{request} Output: {output_text}\", sep=\"\\n\")\n \n         # Compare with classic generate if asked\n         if expected_outputs is not None:\n@@ -182,83 +164,115 @@ def batch_generate(\n \n \n if __name__ == \"__main__\":\n-    # Parse args\n     parser = argparse.ArgumentParser()\n+\n+    # Continuous batching parameters\n     parser.add_argument(\"--num-blocks\", \"-n\", type=int, default=None)\n     parser.add_argument(\"--max-batch-tokens\", \"-b\", type=int, default=None)\n \n+    # Model parameters\n+    parser.add_argument(\"--sliding-window\", type=int, default=0)\n     parser.add_argument(\"--attn\", type=str, default=\"kernels-community/flash-attn\", help=\"Attention implementation\")\n+\n+    # Performance parameters\n     parser.add_argument(\"--matmul-precision\", \"-mp\", type=str, default=\"high\")  # set to \"none\" to disable\n     parser.add_argument(\"--cuda-graph\", \"-cg\", help=\"Use cuda graphs\", type=str, default=None)\n     parser.add_argument(\"--compile\", action=\"store_true\", help=\"Compile the model using torch.compile\")\n+    parser.add_argument(\"--do-sample\", action=\"store_true\", help=\"Activate sampling\")\n \n+    # Benchmark parameters\n     parser.add_argument(\"--samples\", type=int, default=500, help=\"Number of samples to generate\")\n+    parser.add_argument(\"--add-prefix\", action=\"store_true\", help=\"Add a prefix to the samples\")\n+    parser.add_argument(\"--compare\", action=\"store_true\", help=\"Compare CB generation with classic generate\")\n+    parser.add_argument(\"--profile\", type=str, default=None)\n+    parser.add_argument(\"--metrics\", action=\"store_true\")\n+    parser.add_argument(\"--force-max-length\", action=\"store_true\", help=\"Force generation to stop at max length\")\n+\n+    # Display parameters\n     parser.add_argument(\"--displayed\", type=int, default=0, help=\"Number of samples to display\")\n     parser.add_argument(\"--log-level\", type=str, default=\"INFO\")\n     parser.add_argument(\"--output-file\", type=str, default=None)\n-    parser.add_argument(\"--compare\", action=\"store_true\")\n-    parser.add_argument(\"--metrics\", action=\"store_true\")\n-    parser.add_argument(\"--profile\", type=str, default=None)\n+\n     args = parser.parse_args()\n \n-    # Set log level\n-    logger.setLevel(args.log_level.upper())\n+    # Create model\n+    model_id = \"google/gemma-2-2b-it\" if args.sliding_window > 0 else \"meta-llama/Llama-3.1-8B-Instruct\"\n+    has_system_role = args.sliding_window == 0\n+\n+    model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=args.attn, dtype=torch.bfloat16)\n+    model = model.cuda().eval()\n \n-    # If turned on, we setup metrics\n-    if args.metrics:\n-        setup_metrics()\n+    if args.sliding_window > 0 and getattr(model.config, \"sliding_window\", None) is not None:\n+        print(f\"Setting sliding window from {model.config.sliding_window} to {args.sliding_window}\")\n+        model.config.sliding_window = args.sliding_window\n \n-    # Set matmul precision if not none\n+    # Set up diagnostics\n+    logger.setLevel(args.log_level.upper())\n+    maybe_setup_metrics(args.metrics)\n+\n+    # Set up performance\n     if args.matmul_precision != \"none\":\n         torch.set_float32_matmul_precision(args.matmul_precision)\n-    # Parse cuda graph argument\n-    if args.cuda_graph is not None:\n-        use_cuda_graph = {\n-            \"none\": None,\n-            \"yes\": True, \"y\": True, \"true\": True, \"t\": True, \"1\": True,\n-            \"no\": False, \"n\": False, \"false\": False, \"f\": False, \"0\": False,\n-        }[args.cuda_graph.lower()]  # fmt: skip\n-    else:\n-        use_cuda_graph = None\n \n-    # Prepare model\n-    model = AutoModelForCausalLM.from_pretrained(\n-        MODEL_ID,\n-        attn_implementation=args.attn,\n-        dtype=torch.bfloat16,\n-    )\n-    model = model.cuda().eval()\n-    if getattr(model.config, \"sliding_window\", None) is not None:\n-        print(f\"Setting sliding window from {model.config.sliding_window} to {SLIDING_WINDOW}\")\n-        model.config.sliding_window = SLIDING_WINDOW\n+    cuda_graph_arg = args.cuda_graph.lower() if args.cuda_graph is not None else None\n+    use_cuda_graph = {\n+        \"none\": None, None: None,\n+        \"yes\": True, \"y\": True, \"true\": True, \"t\": True, \"1\": True,\n+        \"no\": False, \"n\": False, \"false\": False, \"f\": False, \"0\": False,\n+    }[cuda_graph_arg]  # fmt: skip\n \n-    # If turned on, we compile the model\n     if args.compile:\n         model.forward = torch.compile(model.forward, mode=\"max-autotune-no-cudagraphs\")\n \n     # Prepare tokenizer and dataset\n-    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n+    tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n \n     dataset = datasets.load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")\n     dataset = dataset.select(range(args.samples))\n \n-    simple_batch_inputs = [tokenizer(item[\"question\"])[\"input_ids\"] for item in dataset]\n+    if args.add_prefix:\n+        possible_prefixes = [\n+            None,\n+            \"You are a bot that solves math problems.\",\n+            \"You are a bot who solves math problems. Try to make your answer clear and understandable, and include your stages of reasoning.\",\n+            \"You are a bot with the aim to solves math problems. Try to make your answer clear and understandable, and include your stages of reasoning. No loud words or emojis, all responses must be readable by a child. Here is now the problem:\",\n+        ]  # fmt: skip\n+    else:\n+        possible_prefixes = [None]\n+\n+    batched_inputs = []\n+    for item, prefix in zip(dataset, cycle(possible_prefixes)):\n+        messages = []\n+        question = item[\"question\"]\n+        if prefix is not None:\n+            if has_system_role:\n+                messages.append({\"role\": \"system\", \"content\": prefix})\n+            else:\n+                question = prefix + \"\\n\\n\" + question\n+        messages.append({\"role\": \"user\", \"content\": question})\n+        inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n+        batched_inputs.append(inputs[\"input_ids\"])\n \n     # Prepare generation config\n-    generation_config = GenerationConfig(\n+    generation_cfg = GenerationConfig(\n         max_new_tokens=512,\n         use_cuda_graph=use_cuda_graph,\n-        eos_token_id=tokenizer.pad_token_id if FORCE_MAX_LENGTH else tokenizer.eos_token_id,\n+        eos_token_id=tokenizer.pad_token_id if args.force_max_length else tokenizer.eos_token_id,\n         pad_token_id=tokenizer.pad_token_id,\n-        do_sample=not args.compare,\n+        do_sample=args.do_sample,\n         temperature=0.8,\n         top_p=0.9,\n         num_blocks=args.num_blocks,\n         max_batch_tokens=args.max_batch_tokens,\n     )\n \n     # If we need to compare, we need to generate the reference outputs\n-    expected_outputs = generate_simple(args.attn, simple_batch_inputs, generation_config) if args.compare else None\n+    if args.compare:\n+        expected_outputs = generate_without_cb(\n+            model_id, args.sliding_window, args.attn, batched_inputs, generation_cfg\n+        )\n+    else:\n+        expected_outputs = None\n \n     # If no output file is provided, we pick a name based on the args\n     if args.output_file is None:\n@@ -271,8 +285,8 @@ def batch_generate(\n     # Run warmup batch generation # TODO: understand why warmup incurs a large overhead during cache creation\n     batch_generate(\n         model,\n-        simple_batch_inputs[: min(5, args.samples)],\n-        generation_config,\n+        batched_inputs[: min(5, args.samples)],\n+        generation_cfg,\n         tokenizer,\n         displayed_samples=-1,\n     )\n@@ -285,8 +299,8 @@ def batch_generate(\n         # Run batch generation\n         gen_time, tok_per_sec = batch_generate(\n             model,\n-            simple_batch_inputs,\n-            generation_config,\n+            batched_inputs,\n+            generation_cfg,\n             tokenizer,\n             displayed_samples=args.displayed,\n             output_file=args.output_file,\n@@ -297,5 +311,5 @@ def batch_generate(\n         prof.export_chrome_trace(filename)\n \n # Example usage:\n-# python examples/pytorch/continuous_batching.py --attn sdpa_paged -mp none --samples 3 --compare\n-# python examples/pytorch/continuous_batching.py --num-blocks 369 --max-batch-tokens 23 --attn sdpa_paged -mp none --samples 1 --displayed 0 --output-file sliced.json\n+# python examples/pytorch/continuous_batching.py --attn sdpa --add-prefix --samples 10 --compare\n+# python examples/pytorch/continuous_batching.py --attn flash_attention_2 -mp none --add-prefix --samples 500"
        },
        {
            "sha": "424cb860890d7ac55967b976b9a6897fd3e57b76",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 72,
            "deletions": 19,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/47227f477532e8ff6dd3290dc6b9e0e38aef303b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47227f477532e8ff6dd3290dc6b9e0e38aef303b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=47227f477532e8ff6dd3290dc6b9e0e38aef303b",
            "patch": "@@ -12,7 +12,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from collections import deque\n from math import floor, gcd, sqrt\n from typing import Optional\n \n@@ -21,8 +20,8 @@\n from ...configuration_utils import PreTrainedConfig\n from ...generation.configuration_utils import GenerationConfig\n from ...utils.metrics import attach_tracer, traced\n-from .cache_manager import CacheAllocator, FullAttentionCacheAllocator, SlidingAttentionCacheAllocator\n-from .requests import get_device_and_memory_breakdown, logger\n+from .cache_manager import BlockManager, CacheAllocator, FullAttentionCacheAllocator, SlidingAttentionCacheAllocator\n+from .requests import RequestState, get_device_and_memory_breakdown, logger\n \n \n def group_layers_by_attn_type(config: PreTrainedConfig) -> tuple[list[list[int]], list[str]]:\n@@ -32,7 +31,7 @@ def group_layers_by_attn_type(config: PreTrainedConfig) -> tuple[list[list[int]]\n         - All groups have the same number of layers\n \n     For a model with the following layer types: [\"sliding\", \"full\", \"full\", \"sliding\", \"full\", \"full\", \"full\", \"full\"]\n-    We would get two groups: [0, 3] and [1, 2], [4,5], [6,7].\n+    We would get four groups: [0, 3], [1, 2], [4,5] and [6,7].\n     \"\"\"\n     # If the config has no layer_type attribute, it means all layers are the same attention type\n     layer_types = getattr(config, \"layer_types\", None)\n@@ -116,23 +115,25 @@ class PagedAttentionCache:\n     for the sliding-attention group, although it is not needed.\n     \"\"\"\n \n-    # TODO: this init is quite long, maybe a refactor is in order\n     def __init__(\n         self,\n         config: PreTrainedConfig,\n         generation_config: GenerationConfig,\n         device: torch.device,\n         dtype: torch.dtype = torch.float16,\n         tp_size: Optional[int] = None,\n+        allow_prefix_sharing: bool = True,\n     ) -> None:\n-        \"\"\"Initialize a paged attention cache for efficient memory usage.\n+        \"\"\"Initialize a paged attention cache for efficient memory usage. Also turns in prefix sharing if the model has\n+        only full attention layers.\n \n         Args:\n             config: Model configuration\n             generation_config: Generation configuration containing cache parameters\n             device: Device for the cache tensors\n             dtype: Data type of the cache\n             tp_size: Tensor parallelism size\n+            allow_prefix_sharing: A flag to allow prefix sharing if the model has only full attention layers.\n         \"\"\"\n         self.config = config\n         self.dtype = dtype\n@@ -173,10 +174,12 @@ def __init__(\n         page_size = self.head_dim * self.num_key_value_heads\n \n         if \"flash\" in self.config._attn_implementation:\n-            num_attention_masks = 1  # only used to compute the default meme args\n-        else:\n+            num_attention_masks = 0  # only used to compute the default memory footprint args\n+        elif \"sliding_attention\" in group_types:\n             # TODO: when we generalize to allow for block-attn, we can use `num_attention_masks=sum(set(group_types))`\n-            num_attention_masks = 2 if \"sliding_attention\" in group_types else 1\n+            num_attention_masks = 2\n+        else:\n+            num_attention_masks = 1\n \n         memory_handler = PagedAttentionMemoryHandler(\n             block_size=self.block_size,\n@@ -218,7 +221,6 @@ def __init__(\n         logger.info(f\"{self.cache_shape = } {self.key_cache[0].shape = } {self.key_cache[0].numel() = }\")\n \n         # Block management data structures\n-        self._free_blocks = deque(range(num_blocks))\n         self.group_cache_managers: list[CacheAllocator] = []\n         for i, group_type in enumerate(group_types):\n             if group_type == \"full_attention\":\n@@ -229,13 +231,19 @@ def __init__(\n                 raise ValueError(f\"Invalid group type: {group_type}\")\n             self.group_cache_managers.append(cm)\n \n+        # We only use prefix sharing if the whole model has only full attention layers\n+        self.use_prefix_sharing = allow_prefix_sharing and group_types == [\"full_attention\"]\n+        self._block_manager = BlockManager(num_blocks, self.block_size, self.use_prefix_sharing)\n+        self.blocks_to_complete: dict[str, int] = {}\n+        self._total_prefix_length: int = 0  # a counter to measure the impact of prefix sharing, also used in tests\n+\n     @traced\n-    def allocate_blocks(self, n_blocks: int, request_id: str) -> int:\n+    def allocate_blocks(self, n_blocks: int, state: RequestState) -> int:\n         \"\"\"Allocate cache blocks across all layer groups for a given request. Actual allocation is done by the cache\n         managers, and this method only returns the maximum number of blocks actually allocated across all managers.\"\"\"\n         max_allocated = 0\n         for cm in self.group_cache_managers:\n-            allocated = cm.allocate_blocks(n_blocks, request_id, self._free_blocks)\n+            allocated = cm.allocate_blocks(n_blocks, state.request_id, self._block_manager)\n             if allocated is None:\n                 return None\n             max_allocated = max(max_allocated, allocated)\n@@ -246,11 +254,11 @@ def free_blocks(self, request_id: str) -> None:\n         \"\"\"Free all allocated cache blocks for a given request across all layer groups. Actual deallocation is done\n         by the cache managers.\"\"\"\n         for cm in self.group_cache_managers:\n-            cm.free_blocks(request_id, self._free_blocks)\n+            cm.free_blocks(request_id, self._block_manager)\n \n     def get_num_free_blocks(self) -> int:\n         \"\"\"Get the current number of unallocated blocks available for new requests.\"\"\"\n-        return len(self._free_blocks)\n+        return self._block_manager.num_free_blocks\n \n     @traced\n     def extend_read_indices(\n@@ -337,6 +345,44 @@ def update(\n         # Return the new KV values\n         return key_states_with_cache, value_states_with_cache\n \n+    def search_prefix_match(self, request_id: str, prompt_ids: list[int]) -> int:\n+        \"\"\"Searches for a prefix match in the cache for the given (prompts_ids). If one is found, we reference the\n+        matching blocks in the (request_id), increase the reference count of the blocks and return the number of blocks\n+        that match. If no prefix match is found, we return 0.\"\"\"\n+        current_hash = None\n+        allocated_blocks = []\n+        for b in range(len(prompt_ids) // self.block_size):\n+            tokens = prompt_ids[b * self.block_size : (b + 1) * self.block_size]\n+            current_hash = self._block_manager.compute_hash(current_hash, tokens)\n+            block_id = self._block_manager._hash_to_id.get(current_hash)\n+            if block_id is not None:\n+                allocated_blocks.append(block_id)\n+                self._block_manager.increase_ref_count(block_id)\n+            else:\n+                break\n+        # If we found a matching prefix, we reference the blocks in the request\n+        if allocated_blocks:\n+            logger.debug(f\"Found prefix match for request {request_id} with {len(allocated_blocks)} blocks\")\n+            cm = self.group_cache_managers[0]\n+            cm.block_table[request_id] = allocated_blocks\n+\n+        prefix_length = len(allocated_blocks) * self.block_size\n+        self._total_prefix_length += prefix_length\n+        return prefix_length\n+\n+    def mark_blocks_as_complete(self, state: RequestState) -> None:\n+        \"\"\"Marks the blocks that have been computed in the forward pass as complete. If prefix sharing is off, this is\n+        a no-op.\"\"\"\n+        num_complete_blocks = 0 if not self.use_prefix_sharing else self.blocks_to_complete.pop(state.request_id)\n+        if num_complete_blocks == 0:\n+            return None\n+        cm = self.group_cache_managers[0]  # if prefix sharing is on, there is only one group\n+        self._block_manager.mark_blocks_as_complete(\n+            num_complete_blocks=num_complete_blocks,\n+            allocated_blocks=cm.block_table[state.request_id],\n+            prompt_ids=(state.full_prompt_ids + state.static_outputs),\n+        )\n+\n \n # TODO: rework computation with the groups and their sizes\n class PagedAttentionMemoryHandler:\n@@ -471,6 +517,8 @@ def compute_num_blocks_and_max_batch_tokens(\n             2N * (layer_group_size * page_size * cache_dtype + 2 * num_group),\n             m * N * (peak_activation_per_token * activation_dtype + 28 + 4 * num_group),\n         ])\n+\n+        If num_attention_masks is 0, the equation simplifies to a 1st degree polynomial.\n         \"\"\"\n         cache_memory = self.get_available_memory(max_memory_percent)\n         logger.info(f\"Cache memory: {cache_memory}\")\n@@ -482,11 +530,16 @@ def compute_num_blocks_and_max_batch_tokens(\n         c = -cache_memory\n         logger.debug(f\"Coefficients of 2nd degree polynomial: {a = }, {b = }, {c = }\")\n \n-        # Compute discriminant and greatest solution\n-        discriminant = b**2 - 4 * a * c\n-        if discriminant < 0:\n-            raise ValueError(f\"Discriminant is negative: {discriminant = }\")\n-        greatest_solution = (-b + sqrt(discriminant)) / (2 * a)\n+        # If num_attention_masks is 0, the equation simplifies to a 1st degree polynomial\n+        if self.num_attention_masks == 0:\n+            greatest_solution = -c / b\n+        # Otherwise, we solve the quadratic equation\n+        else:\n+            discriminant = b**2 - 4 * a * c\n+            if discriminant < 0:\n+                raise ValueError(f\"Discriminant is negative: {discriminant = }\")\n+            greatest_solution = (-b + sqrt(discriminant)) / (2 * a)\n+\n         if greatest_solution < 0:\n             raise ValueError(f\"Greatest solution is negative: {greatest_solution = }\")\n "
        },
        {
            "sha": "35c66671163339de380ddb33bb693068b2592025",
            "filename": "src/transformers/generation/continuous_batching/cache_manager.py",
            "status": "modified",
            "additions": 224,
            "deletions": 53,
            "changes": 277,
            "blob_url": "https://github.com/huggingface/transformers/blob/47227f477532e8ff6dd3290dc6b9e0e38aef303b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47227f477532e8ff6dd3290dc6b9e0e38aef303b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py?ref=47227f477532e8ff6dd3290dc6b9e0e38aef303b",
            "patch": "@@ -14,29 +14,211 @@\n # limitations under the License.\n from abc import ABC, abstractmethod\n from collections import deque\n+from collections.abc import Iterator\n from math import ceil\n-from typing import Optional\n+from typing import Optional, TypeVar\n \n from .requests import logger\n \n \n+T = TypeVar(\"T\")\n+\n+\n+def reverse_enumerate(xs: list[T]) -> Iterator[tuple[int, T]]:\n+    index = len(xs) - 1\n+    for x in xs[::-1]:\n+        yield index, x\n+        index -= 1\n+\n+\n+class Block:\n+    \"\"\"A class to represent a block managed by the block manager. We say that a block is complete when the physical KV\n+    cache it points to is fully computed. A block can have a parent, which is the block that came before in the\n+    sequence. Once a block is complete, it is given a hash, which takes into account the tokens ids of the block and\n+    its parent's hash (if there is a parent).\"\"\"\n+\n+    def __init__(self, id_: int, parent_id: int | None) -> None:\n+        self.id: int = id_\n+        self.parent_id: int | None = parent_id\n+        self.hash: int | None = None\n+        self.ref_count: int = 1\n+\n+    def __repr__(self) -> str:\n+        return f\"Block(id={self.id}, parent_id={self.parent_id}, hash={self.hash}, ref_count={self.ref_count})\"\n+\n+    @property\n+    def is_complete(self) -> bool:\n+        return self.hash is not None\n+\n+\n+class BlockManager:\n+    \"\"\"A class to manage the number of free blocks and block re-use. If prefix sharing is off, the block manager is a\n+    simple FIFO structure where blocks are either free or in use. If prefix sharing is on, blocks can have 3 states:\n+      - in use: one or more requests references this block, thus it cannot be written over. The number of requests\n+        referencing this block is stored as ref_count in the Block object.\n+      - un-initialized: the block points to a space in the KV cache tensor that contains no data yet. Those blocks can\n+        be given as free blocks to new requests without any overhead.\n+      - initialized: the block is complete and was used by one or more request that are finished. It contains KV cache\n+        data and its hash is stored in the hash table. If a new request needs a block with the same hash, we increase\n+        the ref_count of the block and remove it from the list of initialized blocks, because it is now in use.\n+        Still, the block can be freed if no un-initialized blocks are left. In that case, we remove its hash from the\n+        hash table.\n+    There is no structure to keep track of the blocks in use: if a block is neither un-initialized nor initialized,\n+    it is in use.\n+    \"\"\"\n+\n+    def __init__(self, num_blocks: int, block_size: int, use_prefix_sharing: bool) -> None:\n+        \"\"\"Initializes the block manager with a given number of blocks (num_blocks) of size (block_size). Prefix sharing\n+        can be turned on with the (use_prefix_sharing) flag, which only happens if the model has only full attention\n+        layers.\"\"\"\n+        self.num_blocks = num_blocks\n+        self.block_size = block_size\n+        self._uninit_block_ids = deque(range(num_blocks))\n+        self._init_block_ids: dict[int, None] = {}  # effectively act as an ordered set\n+        self._use_prefix_sharing = use_prefix_sharing\n+        self._hash_to_id: dict[int, int] = {}\n+        self._id_to_block: dict[int, Block] = {}\n+\n+    @property\n+    def num_free_blocks(self) -> int:\n+        \"\"\"Returns the number of free blocks left. Both initialized and uninitialized blocks are considered free.\"\"\"\n+        return len(self._uninit_block_ids) + len(self._init_block_ids)\n+\n+    def has_enough_free_blocks(self, n_blocks: int) -> bool:\n+        \"\"\"Checks if there are enough free blocks to allocate the requested number of blocks (n_blocks). If there are\n+        not enough uninitialized blocks, we uninitialize the required number of initialized blocks.\"\"\"\n+        # Exit early if there are enough uninitialized blocks\n+        if len(self._uninit_block_ids) >= n_blocks:\n+            return True\n+        # Exit early if even after uninitializing all initialized blocks, there are not enough free blocks\n+        block_to_unintialize = n_blocks - len(self._uninit_block_ids)\n+        if len(self._init_block_ids) < block_to_unintialize:\n+            return False\n+        # Uninitialize the required amount of blocks\n+        for _ in range(block_to_unintialize):\n+            id_to_unintialize = self._init_block_ids.popitem()[0]\n+            block = self._id_to_block[id_to_unintialize]\n+            self._hash_to_id.pop(block.hash)\n+            self._uninit_block_ids.append(id_to_unintialize)\n+        return True\n+\n+    def get_free_blocks(self, n_blocks: int, last_block_id: int | None) -> list[int] | None:\n+        \"\"\"Returns a list of (n_blocks) free block and mark them as no longuer free in the internal data structures. One\n+        can also pass a (last_block_id) to indicate the last block id in the sequence, which is used to keep track of\n+        the parent block. If the manager cannot find enough free blocks, it returns None.\"\"\"\n+        if not self.has_enough_free_blocks(n_blocks):\n+            return None\n+        allocated_block_ids = [self._uninit_block_ids.popleft() for _ in range(n_blocks)]\n+        # If we use prefix caching, we keep track of the allocated blocks as partial blocks\n+        if self._use_prefix_sharing:\n+            for block_id in allocated_block_ids:\n+                block = Block(block_id, last_block_id)\n+                self._id_to_block[block_id] = block\n+                last_block_id = block_id\n+        # In both cases, we return the allocated block ids\n+        return allocated_block_ids\n+\n+    def increase_ref_count(self, block_id: int) -> None:\n+        \"\"\"Increases the reference count of a given (block_id).\"\"\"\n+        block = self._id_to_block[block_id]\n+        block.ref_count += 1\n+        if block.ref_count == 1:\n+            self._init_block_ids.pop(block_id)\n+\n+    def decrease_ref_count(self, block_id: int) -> None:\n+        \"\"\"Decreases the reference count of a given (block_id). If the reference count reaches 0, the block is no longer\n+        in use, and becomes initialized (if it was complete) or uninitialized (if it was incomplete).\"\"\"\n+        block = self._id_to_block[block_id]\n+        block.ref_count -= 1\n+        if block.ref_count == 0:\n+            if block.is_complete:\n+                self._init_block_ids[block_id] = None\n+            else:\n+                self._id_to_block.pop(block_id)\n+                self._uninit_block_ids.append(block_id)\n+\n+    def free_blocks(self, blocks: list[int]) -> None:\n+        \"\"\"Marks a list of (blocks) as free. If there is no prefix sharing, we simply add them to the uninitialized\n+        blocks queue. Otherwise, their new state depends on whether they are complete.\"\"\"\n+        if self._use_prefix_sharing:\n+            for block_id in blocks:\n+                self.decrease_ref_count(block_id)\n+        else:\n+            self._uninit_block_ids.extend(blocks)\n+\n+    def mark_blocks_as_complete(\n+        self, num_complete_blocks: int, allocated_blocks: list[int], prompt_ids: list[int]\n+    ) -> None:\n+        \"\"\"Among the list of (allocated_blocks), mark (num_complete_blocks) incomplete blocks as now complete. The list\n+        of (prompt_ids) is used to compute the hash of the new block.\"\"\"\n+        # Look for the first complete block, starting from the last block in the sequence\n+        parent_hash = None\n+        incomplete_blocks: list[Block] = []\n+        for i, block_id in reverse_enumerate(allocated_blocks):\n+            block = self._id_to_block[block_id]\n+            if block.is_complete:\n+                parent_hash = block.hash\n+                break\n+            incomplete_blocks.append((i, block))\n+\n+        # Now go through the incomplete blocks and updated them\n+        new_parent_id = None\n+        while incomplete_blocks:\n+            i, block = incomplete_blocks.pop()\n+\n+            # If the parent id has been updated, we apply the change\n+            if new_parent_id is not None:\n+                block.parent_id = new_parent_id\n+                new_parent_id = None\n+\n+            # If we have set the hash for all complete blocks, we can stop\n+            if num_complete_blocks == 0:\n+                break\n+\n+            # Otherwise, we compute the hash\n+            num_complete_blocks -= 1\n+            tokens = prompt_ids[i * self.block_size : (i + 1) * self.block_size]\n+            block.hash = self.compute_hash(parent_hash, tokens)\n+\n+            existing_block_id = self._hash_to_id.get(block.hash)\n+            # If the block hash is already in the hash to id mapping, we reference the existing block instead\n+            if existing_block_id is not None:\n+                logger.debug(f\"Found existing block {existing_block_id} for block {block.id}\")\n+                allocated_blocks[i] = existing_block_id\n+                self._id_to_block[existing_block_id].ref_count += 1\n+                new_parent_id = existing_block_id\n+                self.free_blocks([block.id])\n+\n+            # Otherwise, we add the completed block to the hash table\n+            else:\n+                self._hash_to_id[block.hash] = block.id\n+\n+            # Update loop variables\n+            parent_hash = block.hash\n+\n+    def compute_hash(self, parent_hash: int | None, tokens: list[int]) -> int:\n+        \"\"\"Computes the hash of a block containing the given (tokens) with a given (parent_hash). If the block has no\n+        parent, the parent hash is None.\"\"\"\n+        return hash((parent_hash, tuple(tokens)))\n+\n+\n class CacheAllocator(ABC):\n     \"\"\"Abstract base class for cache managers. Cache managers keep track of per-request cache allocations, determine\n     when a new physical block needs to be allocated and compute physical indices for reading or writing to the cache.\"\"\"\n \n     _index: int\n-    _block_table: dict[str, list[int]]  # request_id -> list of block_ids allocated to the request\n+    block_table: dict[str, list[int]]  # request_id -> list of block_ids allocated to the request\n \n     @abstractmethod\n-    def allocate_blocks(self, n_blocks: int, request_id: str, free_blocks: deque[int]) -> Optional[int]:\n-        \"\"\"Allocates n_blocks for a given request_id. Returns the num of blocks allocated if successful and None\n-        otherwise.\"\"\"\n-\n-    def free_blocks(self, request_id: str, free_blocks: deque[int]) -> None:\n-        \"\"\"Frees all blocks associated with a request_id.\"\"\"\n-        if request_id in self._block_table:\n-            blocks_to_free = self._block_table.pop(request_id)\n-            free_blocks.extend(blocks_to_free)\n+    def allocate_blocks(self, n_blocks: int, request_id: str, block_manager: BlockManager) -> Optional[int]:\n+        \"\"\"Allocates (n_blocks) for a given (request_id) using the (block_manager). Returns the num of blocks allocated\n+        if successful and None otherwise.\"\"\"\n+\n+    def free_blocks(self, request_id: str, block_manager: BlockManager) -> None:\n+        \"\"\"Frees all blocks associated with a (request_id) using the (block_manager).\"\"\"\n+        if request_id in self.block_table:\n+            blocks_to_free = self.block_table.pop(request_id)\n+            block_manager.free_blocks(blocks_to_free)\n         else:\n             logger.warning(\n                 f\"CacheAllocator {self._index} attempted to free blocks for non-existent request_id: {request_id}\"\n@@ -66,23 +248,30 @@ def __init__(self, index: int, block_size: int) -> None:\n         \"\"\"\n         self._index = index\n         self.block_size = block_size\n-        self._block_table = {}\n+        self.block_table = {}\n \n-    def allocate_blocks(self, n_blocks: int, request_id: str, free_blocks: deque[int]) -> Optional[int]:\n-        \"\"\"Allocate blocks for a given request_id. Returns the number of blocks allocated if successful and None\n-        otherwise. For group of full attention layers, we always allocate the number of requested blocks.\"\"\"\n-        if len(free_blocks) < n_blocks:\n+    def allocate_blocks(self, n_blocks: int, request_id: str, block_manager: BlockManager) -> Optional[int]:\n+        \"\"\"Allocate (n_blocks) for a given (request_id) using the (block_manager). Returns the number of blocks\n+        allocated if successful and None otherwise. For group of full attention layers, we always allocate the number of\n+        requested blocks.\"\"\"\n+        # Make sure the request_id is in the block table and get the first block id\n+        if request_id not in self.block_table:\n+            self.block_table[request_id] = []  # TODO: check the impact of making this a deque\n+            last_block_id = None\n+        else:\n+            last_block_id = self.block_table[request_id][-1]\n+        # Actual allocation, return early if failed\n+        allocated_blocks = block_manager.get_free_blocks(n_blocks, last_block_id)\n+        if allocated_blocks is None:\n             return None\n-        if request_id not in self._block_table:\n-            self._block_table[request_id] = []\n-        self._block_table[request_id].extend(free_blocks.popleft() for _ in range(n_blocks))\n+        self.block_table[request_id].extend(allocated_blocks)\n         return n_blocks\n \n     def get_read_indices(self, request_id: str, past_length: int, query_length: int) -> list[int]:\n         \"\"\"Returns the physical indices of where to read request_id's cache. For a group of full attention layers, we\n         first write the new cache to the cache tensor and then read the entire cache from the beginning to the end.\"\"\"\n         # Retrieve the block table for the request and raise an error if it doesn't exist\n-        block_table = self._block_table.get(request_id)\n+        block_table = self.block_table.get(request_id)\n         if block_table is None:\n             raise ValueError(f\"No block table found for request {request_id}\")\n         # Compute the physical indices\n@@ -97,7 +286,7 @@ def get_read_indices(self, request_id: str, past_length: int, query_length: int)\n     def get_write_indices(self, request_id: str, past_length: int, query_length: int) -> list[int]:\n         \"\"\"Returns the physical indices for writing to the cache. For a group of full attention layers, we write the new\n         cache as a continuation of the existing cache for the same request.\"\"\"\n-        block_table = self._block_table.get(request_id)\n+        block_table = self.block_table.get(request_id)\n         if block_table is None:\n             raise ValueError(f\"No block table found for request {request_id}\")\n         # Compute the physical indices\n@@ -129,25 +318,26 @@ def __init__(self, index: int, block_size: int, sliding_window: int) -> None:\n         self.block_size = block_size\n         self.sliding_window = sliding_window\n         self._max_blocks_per_request = ceil(self.sliding_window / self.block_size)\n-        self._block_table = {}\n-\n-    def allocate_blocks(self, n_blocks: int, request_id: str, free_blocks: deque[int]) -> Optional[int]:\n-        \"\"\"Allocate blocks for a given request_id. Returns the number of blocks allocated if successful and None\n-        otherwise. For group of sliding window attention layers, we only allocate up to the point where we can fit an\n-        entire sliding window in the cache tensor.\"\"\"\n-        if request_id not in self._block_table:\n-            self._block_table[request_id] = []\n+        self.block_table = {}\n+\n+    def allocate_blocks(self, n_blocks: int, request_id: str, block_manager: BlockManager) -> Optional[int]:\n+        \"\"\"Allocate (n_blocks) for a given (request_id) using the (block_manager). Returns the number of blocks\n+        allocated otherwise. For group of sliding window attention layers, we only allocate up to the point where we can\n+        fit an entire sliding window in the cache tensor.\"\"\"\n+        if request_id not in self.block_table:\n+            self.block_table[request_id] = []\n         # Early return if we are already at the max number of blocks per request\n-        already_allocated = len(self._block_table[request_id])\n+        already_allocated = len(self.block_table[request_id])\n         if already_allocated == self._max_blocks_per_request:\n             return 0\n         # Compute actual number of blocks to allocate\n         after_allocation = min(already_allocated + n_blocks, self._max_blocks_per_request)\n         actual_n_blocks = after_allocation - already_allocated\n         # Classic allocation\n-        if len(free_blocks) < actual_n_blocks:\n+        allocated_blocks = block_manager.get_free_blocks(actual_n_blocks, None)  # no prefix caching w/ sliding window\n+        if allocated_blocks is None:\n             return None\n-        self._block_table[request_id].extend(free_blocks.popleft() for _ in range(actual_n_blocks))\n+        self.block_table[request_id].extend(allocated_blocks)\n         return actual_n_blocks\n \n     def get_read_indices(self, request_id: str, past_length: int, query_length: int) -> list[int]:\n@@ -157,7 +347,7 @@ def get_read_indices(self, request_id: str, past_length: int, query_length: int)\n         sliding_window - 1 cache page and then manually add the new key / values states after. Hence the -1 indices\n         which indicate where to store the new key or values indices.\"\"\"\n         # Retrieve the block table for the request and raise an error if it doesn't exist\n-        block_table = self._block_table.get(request_id)\n+        block_table = self.block_table.get(request_id)\n         if block_table is None:\n             raise ValueError(f\"No block table found for request {request_id}\")\n         # Apply sliding window\n@@ -178,7 +368,7 @@ def get_write_indices(self, request_id: str, past_length: int, query_length: int\n         sliding window attention layers, we write the new cache in rolling-buffer kind of way: if we reach the end of\n         the allocated physical cache, we start writing from the beginning of the physical cache again.\"\"\"\n         # Retrieve the block table for the request and raise an error if it doesn't exist\n-        block_table = self._block_table.get(request_id)\n+        block_table = self.block_table.get(request_id)\n         if block_table is None:\n             raise ValueError(f\"No block table found for request {request_id}\")\n         # Apply sliding window\n@@ -201,22 +391,3 @@ def get_seqlens_k(self, request_id: str, past_length: int, query_length: int) ->\n         \"\"\"Returns the attention type of the cache allocator and the key sequence length for the given request_id.\"\"\"\n         seqlens_k = query_length + min(past_length, self.sliding_window - 1)\n         return \"sliding_attention\", seqlens_k\n-\n-\n-# TODO: test the impact of this\n-# def get_read_indices(self, request_id: str, past_length: int) -> list[int]:\n-#     # Retrieve the block table for the request and raise an error if it doesn't exist\n-#     block_table = self._block_table.get(request_id)\n-#     if block_table is None:\n-#         raise ValueError(f\"No block table found for request {request_id}\")\n-#     # Compute the physical indices\n-#     physical_indices = []\n-#     n_left = past_length\n-#     for block_idx in block_table:\n-#         block_physical_index = block_idx * self.block_size\n-#         pages_used = min(self.block_size, n_left)\n-#         physical_indices.extend(block_physical_index + i for i in range(pages_used))\n-#         n_left -= pages_used\n-#         if n_left == 0:\n-#             return physical_indices\n-#     raise ValueError(f\"Request {request_id} required too many indices: {past_length = } and {len(block_table) = }\")"
        },
        {
            "sha": "f8d154f4d664ffe2b0572abec3c3fc2f2f564323",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 44,
            "deletions": 29,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/47227f477532e8ff6dd3290dc6b9e0e38aef303b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47227f477532e8ff6dd3290dc6b9e0e38aef303b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=47227f477532e8ff6dd3290dc6b9e0e38aef303b",
            "patch": "@@ -16,12 +16,13 @@\n import queue\n import threading\n from collections.abc import Generator\n+from contextlib import contextmanager\n from dataclasses import dataclass\n from functools import partial\n from itertools import count\n from math import ceil\n from time import perf_counter\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torch import nn\n@@ -446,10 +447,7 @@ def prepare_next_batch(self) -> bool:\n         cumulative_seqlens_q = [0]\n         logits_indices = []\n \n-        if isinstance(self.cumulative_seqlens_k, dict):\n-            cumulative_seqlens_k = {layer_type: [0] for layer_type in self.cumulative_seqlens_k}\n-        else:\n-            cumulative_seqlens_k = [0]\n+        cumulative_seqlens_k = {layer_type: [0] for layer_type in self.cumulative_seqlens_k}\n \n         read_index = [[] for _ in range(self.cache.num_groups)]\n         write_index = [[] for _ in range(self.cache.num_groups)]\n@@ -498,10 +496,7 @@ def prepare_next_batch(self) -> bool:\n         self.metrics.record_kv_cache_memory_metrics(self.cache)\n \n         if logger.isEnabledFor(logging.DEBUG):\n-            if isinstance(self.cumulative_seqlens_k, dict):\n-                ck = max(cumulative_seqlens_k[layer_type][-1] for layer_type in self.cumulative_seqlens_k)\n-            else:\n-                ck = cumulative_seqlens_k[-1]\n+            ck = max(cumulative_seqlens_k[layer_type][-1] for layer_type in self.cumulative_seqlens_k)\n             logger.debug(\n                 f\"Scheduled: {len(self.requests_in_batch)}, Waiting: {len(self.scheduler.waiting_requests)}, \"\n                 f\"Active: {len(self.scheduler.active_requests)}. cum Q: {cumulative_seqlens_q[-1]}. \"\n@@ -517,7 +512,7 @@ def _build_tensors(\n         read_index: list[list[int]],\n         write_index: list[list[int]],\n         cumulative_seqlens_q: list[int],\n-        cumulative_seqlens_k: Union[list[int], dict[str, list[int]]],\n+        cumulative_seqlens_k: dict[str, list[int]],\n         logits_indices: list[int],\n     ) -> None:\n         \"\"\"Builds the actual tensors for the current batch, by modifying the already allocated tensors in place.\"\"\"\n@@ -561,27 +556,35 @@ def _sync(self) -> list[int]:\n     @traced\n     def _maybe_send_output(self, state: RequestState) -> None:\n         \"\"\"Send output to the queue based on streaming mode and request state.\"\"\"\n-        if state.streaming:\n-            self.output_queue.put(state.to_generation_output())\n-        elif state.status == RequestStatus.FINISHED:\n+        if state.streaming or state.status == RequestStatus.FINISHED:\n             self.output_queue.put(state.to_generation_output())\n \n     @traced\n     def update_batch(self) -> None:\n         \"\"\"Update request states based on generated tokens.\"\"\"\n         out_tokens = self._sync()\n         for i, state in enumerate(self.requests_in_batch):\n+            # If the request has no remaining prompt ids, it means prefill has already ended or just finished\n             if len(state.remaining_prompt_ids) == 0:\n                 self.metrics.record_ttft_metric(state.created_time, state.request_id)\n                 state.status = RequestStatus.DECODING\n                 token = out_tokens[self.logits_indices[i]]\n                 state.prompt_ids = [token]\n-                if state.update_with_token(token):\n+                # Update the request and stop if it is complete\n+                is_finished = state.update_and_check_completion(token)\n+                # We mark the completed blocks as such\n+                self.cache.mark_blocks_as_complete(state)\n+                if is_finished:\n                     self.metrics.record_request_completion(state.created_time, state.request_id)\n                     self.scheduler.finish_request(state.request_id, evict_from_cache=(not self.manual_eviction))\n                 self._maybe_send_output(state)\n+            #  Otherwise, the request is still prefilling, but the prefill has been split\n             elif state.status == RequestStatus.PREFILLING_SPLIT:\n+                self.cache.mark_blocks_as_complete(state)\n                 state.status = RequestStatus.SPLIT_PENDING_REMAINDER\n+            else:\n+                raise ValueError(f\"Request {state.request_id} is in an unexpected state: {state.status}\")\n+\n         if self.cache.get_num_free_blocks() == 0:\n             raise ValueError(\"No more free blocks\")\n \n@@ -726,6 +729,7 @@ def __init__(\n         max_queue_size: int = 0,\n         num_q_cuda_graphs: int = 0,\n         num_kv_cuda_graphs: int = 0,\n+        allow_prefix_sharing: bool = True,\n     ) -> None:\n         \"\"\"Initialize the continuous batching manager.\n \n@@ -735,6 +739,7 @@ def __init__(\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n             num_q_cuda_graphs: (optional) Number of CUDA graphs to use for the query dimension\n             num_kv_cuda_graphs: (optional) Number of CUDA graphs to use for the keys/values dimension\n+            allow_prefix_sharing: (optional) Whether to allow prefix sharing if the model has only full attention layers\n         \"\"\"\n         if \"paged|\" not in model.config._attn_implementation:\n             attn_implementation = f\"paged|{model.config._attn_implementation}\"\n@@ -767,6 +772,8 @@ def __init__(\n         self.manual_eviction = manual_eviction\n         self.batch_processor: Optional[ContinuousBatchProcessor] = None\n \n+        self._allow_prefix_sharing = allow_prefix_sharing\n+\n         # If a number of cuda graphs was specified for either Q or KV, we activate cuda graphs\n         if num_q_cuda_graphs > 0 or num_kv_cuda_graphs > 0:\n             self.use_cuda_graph = True\n@@ -799,7 +806,6 @@ def start(self) -> None:\n             logger.warning(\"Manager thread is already running.\")\n             return\n \n-        self._result_queue = queue.Queue()\n         self._generation_thread = threading.Thread(target=self._run_generation_loop)\n         self._generation_thread.start()\n \n@@ -814,6 +820,16 @@ def stop(self, block: bool = True, timeout: Optional[float] = None) -> None:\n             block: Whether to wait for the thread to stop\n             timeout: Maximum time to wait for the thread to stop\n         \"\"\"\n+        if self.batch_processor is None:\n+            logger.warning(\"\\nBatch processor was not initialized.\")\n+        else:\n+            if self.batch_processor.cache.use_prefix_sharing:\n+                logger.warning(\n+                    f\"\\nPrefix sharing was on. Total prefix length: {self.batch_processor.cache._total_prefix_length}\"\n+                )\n+            else:\n+                logger.warning(\"\\nPrefix sharing was off.\")\n+\n         if self._generation_thread is None:\n             logger.warning(\"Manager not started.\")\n             return\n@@ -939,20 +955,6 @@ def request_id_iter(self, request_id: str) -> Generator[GenerationOutput]:\n                 request_cancelled = self.batch_processor.scheduler.request_is_cancelled(request_id)\n \n     @traced\n-    def warmup(self, batch_processor: ContinuousBatchProcessor) -> None:\n-        stream = torch.cuda.Stream(device=self.model.device)\n-        stream.wait_stream(torch.cuda.current_stream())\n-        with torch.cuda.stream(stream):\n-            # Warmup the model with a dummy forward pass\n-            self._generation_step(batch_processor)\n-        torch.cuda.current_stream().wait_stream(stream)\n-\n-        self.graph = torch.cuda.CUDAGraph()\n-        with torch.cuda.graph(self.graph, stream=stream):\n-            self._generation_step(batch_processor)\n-\n-    @traced\n-    # @torch.compile\n     def _generation_step(self) -> None:\n         \"\"\"Perform a single generation step. This is cuda graphed\"\"\"\n         self.batch_processor._generation_step(self.model, self.logit_processor, self.do_sample)\n@@ -968,6 +970,7 @@ def _run_generation_loop(self) -> None:\n                 self.model.device,\n                 self.model.dtype,\n                 tp_size=getattr(self.model, \"_tp_size\", None),  # Use model's actual TP setting\n+                allow_prefix_sharing=self._allow_prefix_sharing,\n             )\n             logger.debug(f\"PagedAttentionCache created in {perf_counter() - t0} seconds\")\n \n@@ -1059,13 +1062,23 @@ def evict_request_from_cache(self, request_id: str) -> None:\n class ContinuousMixin:\n     \"\"\"Mixin class for models to add continuous batching capabilities.\"\"\"\n \n+    @contextmanager\n+    def continuous_batching_context_manager(self, **kwargs) -> Generator[ContinuousBatchingManager]:\n+        manager = self.init_continuous_batching(**kwargs)\n+        manager.start()\n+        try:\n+            yield manager\n+        finally:\n+            manager.stop(block=True)\n+\n     def init_continuous_batching(\n         self,\n         generation_config: Optional[GenerationConfig] = None,\n         manual_eviction: bool = False,\n         max_queue_size: int = 0,\n         num_q_cuda_graphs: int = 0,\n         num_kv_cuda_graphs: int = 0,\n+        allow_prefix_sharing: bool = True,\n     ) -> ContinuousBatchingManager:\n         \"\"\"Initialize a manager for continuous batching inference.\n \n@@ -1098,6 +1111,7 @@ def init_continuous_batching(\n             max_queue_size=max_queue_size,\n             num_q_cuda_graphs=num_q_cuda_graphs,\n             num_kv_cuda_graphs=num_kv_cuda_graphs,\n+            allow_prefix_sharing=allow_prefix_sharing,\n         )\n \n     # TODO: support streaming\n@@ -1169,5 +1183,6 @@ def generate_batch(\n         except Exception as e:\n             logger.error(f\"Error during batch generation: {e}\", exc_info=True)\n         finally:\n+            logger.debug(\"Generate batch is finished.\")  # a dummy log needed for the logs of stop to show. Won't show.\n             manager.stop(block=True, timeout=5.0)\n         return results"
        },
        {
            "sha": "1084170b11edff6ebd27f5dd7dd63b3ae8acca21",
            "filename": "src/transformers/generation/continuous_batching/requests.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/47227f477532e8ff6dd3290dc6b9e0e38aef303b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47227f477532e8ff6dd3290dc6b9e0e38aef303b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py?ref=47227f477532e8ff6dd3290dc6b9e0e38aef303b",
            "patch": "@@ -116,10 +116,10 @@ class RequestState:\n         error (Optional[str]): Any error message associated with the request. When None, has had no error yet.\n     \"\"\"\n \n-    # Required fields\n+    # Required fields # TODO: come up with better names / not sure prompt_ids and such are not redundant\n     request_id: str\n     full_prompt_ids: Optional[list[int]] = None  # Full initial prompt\n-    prompt_ids: Optional[list[int]] = None  # Tokens IDs currently being processed (initial + generated)\n+    prompt_ids: Optional[list[int]] = None  # Tokens IDs currently being processed\n     remaining_prompt_ids: list[int] = field(default_factory=list)  # For split requests, prefill left to process\n     static_outputs: list[int] = field(default_factory=list)  # Generated tokens\n     allocated_blocks: int = 0  # Number of blocks allocated to the request\n@@ -164,7 +164,7 @@ def generated_len(self) -> int:\n \n     # TODO: this logic seems one token off, check it out\n     @traced\n-    def update_with_token(self, token_id: int) -> bool:\n+    def update_and_check_completion(self, token_id: int) -> bool:\n         \"\"\"Update the request with a newly generated token and check for completion.\n \n         Args:"
        },
        {
            "sha": "cbac247e5741ee2b700bf6d1cfa5e3852714d172",
            "filename": "src/transformers/generation/continuous_batching/scheduler.py",
            "status": "modified",
            "additions": 68,
            "deletions": 45,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/47227f477532e8ff6dd3290dc6b9e0e38aef303b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47227f477532e8ff6dd3290dc6b9e0e38aef303b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py?ref=47227f477532e8ff6dd3290dc6b9e0e38aef303b",
            "patch": "@@ -104,7 +104,7 @@ def request_is_cancelled(self, request_id: str) -> bool:\n         )\n \n     @traced\n-    def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int) -> bool:\n+    def _allocate_blocks_if_needed(self, state: RequestState) -> bool:\n         \"\"\"Allocate additional cache blocks for a request if the currently allocated blocks are insufficient to\n         accommodate the next tokens. It calculates how many blocks are needed based on the request's current\n         cache occupancy and the number of tokens to be processed. The allocation itself is done by the CacheAllocator\n@@ -113,10 +113,11 @@ def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int)\n         # 1. we check that the occupancy is less than the requested length\n         # 2. we allocate enough blocks to cover the requested length\n         current_len = state.current_len()\n+        len_next_tokens = len(state.prompt_ids)\n         occupancy = state.allocated_blocks * self.cache.block_size - current_len\n         if occupancy < len_next_tokens or state.allocated_blocks == 0:\n             blocks_needed = ((len_next_tokens - occupancy + 1) // self.cache.block_size) + 1\n-            allocated = self.cache.allocate_blocks(blocks_needed, state.request_id)\n+            allocated = self.cache.allocate_blocks(blocks_needed, state)\n             if allocated is None:\n                 return False\n             state.allocated_blocks += allocated\n@@ -125,11 +126,29 @@ def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int)\n     @traced(span_name=\"prepare_request\")\n     def _prepare_request_for_processing(\n         self, state: RequestState, token_budget: int, request_ids_to_remove_from_waiting: set[str]\n-    ):\n-        \"\"\"Prepares a request for processing in the current batch.\"\"\"\n-        request_tokens = (\n-            state.remaining_prompt_ids if state.status == RequestStatus.SPLIT_PENDING_REMAINDER else state.prompt_ids\n-        )\n+    ) -> None:\n+        \"\"\"Prepares a request for processing in the current batch. If prefix sharing is enabled, and the request was\n+        pending, this is where we look for a prefix match and split the request if found.\"\"\"\n+        # If prefix sharing is enabled, we look for a prefix match and split the request if found\n+        if self.cache.use_prefix_sharing and state.status == RequestStatus.PENDING:\n+            prefill_length = self.cache.search_prefix_match(state.request_id, state.prompt_ids)\n+            if prefill_length > 0:\n+                self.active_requests[state.request_id] = state\n+                request_ids_to_remove_from_waiting.add(state.request_id)\n+                state.status = RequestStatus.SPLIT_PENDING_REMAINDER\n+                # Even if we match the whole request, we keep at least 1 token to start decoding\n+                prefill_length = min(prefill_length, len(state.prompt_ids) - 1)\n+                state.remaining_prompt_ids = state.prompt_ids[prefill_length:]\n+                state.prompt_ids = state.prompt_ids[prefill_length:]\n+                state.position_offset += prefill_length\n+\n+        # If the request has a split prefill, the tokens to process are the remaining prompt ids\n+        if state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+            request_tokens = state.remaining_prompt_ids\n+        # Otherwise, the tokens to process are the prompt ids, which are the full prompt or the last predicted tokens\n+        else:\n+            request_tokens = state.prompt_ids\n+\n         if len(request_tokens) < token_budget:\n             # Can process the entire prompt/remainder\n             if state.status == RequestStatus.PENDING:\n@@ -152,6 +171,7 @@ def _prepare_request_for_processing(\n             state.prompt_ids = request_tokens[:token_budget]\n \n \n+# TODO: further common-ize the two classes\n @attach_tracer()\n class FIFOScheduler(Scheduler):\n     \"\"\"This scheduler processes requests in the order they arrive, meaning decoding requests has priority over\n@@ -195,30 +215,31 @@ def schedule_batch(self, token_budget: int) -> list[RequestState]:\n \n             self._prepare_request_for_processing(state, token_budget, request_ids_to_remove_from_waiting)\n             request_len = len(state.prompt_ids)\n-            if not self._allocate_blocks_if_needed(\n-                state, len(state.prompt_ids)\n-            ):  # don't schedule if we can't allocate blocks\n-                if len(self.cache._free_blocks) == 0:\n+            # If we can't allocate blocks, do not schedule the request and break if the cache is full\n+            if not self._allocate_blocks_if_needed(state):\n+                if self.cache.get_num_free_blocks() == 0:\n                     break\n                 continue\n \n-            @traced\n-            def _add_to_scheduled_requests(state: RequestState):\n-                scheduled_requests.append(state)\n-\n-            _add_to_scheduled_requests(state)\n+            # Add the request to the scheduled requests\n+            scheduled_requests.append(state)\n \n+            # Update the token budget\n             token_budget -= request_len\n-\n-            @traced\n-            def _remove_from_waiting_requests(state: RequestState):\n-                req_id = state.request_id\n-                if req_id in self.waiting_requests:\n-                    del self.waiting_requests[req_id]\n-                    request_ids_to_remove_from_waiting.add(req_id)\n-\n-            _remove_from_waiting_requests(state)\n-\n+            # If using prefix sharing, we make note of the blocks that will be computed in the forward pass\n+            if self.cache.use_prefix_sharing:\n+                tokens_in_current_block = state.current_len() % self.cache.block_size\n+                tokens_after_forward = tokens_in_current_block + request_len\n+                complete_blocks = tokens_after_forward // self.cache.block_size\n+                self.cache.blocks_to_complete[state.request_id] = complete_blocks\n+\n+            # Remove the request from the waiting queue and mark it as removed\n+            req_id = state.request_id\n+            was_waiting = self.waiting_requests.pop(req_id, None) is not None\n+            if was_waiting:\n+                request_ids_to_remove_from_waiting.add(req_id)\n+\n+            # Early exit of the loop if we have no token budget left\n             if token_budget == 0:\n                 break\n \n@@ -249,6 +270,7 @@ def schedule_batch(self, token_budget: int) -> list[RequestState]:\n             elif state.status == RequestStatus.DECODING:\n                 second_priority_states.append(state)\n \n+        # Add waiting requests to second priority\n         for req_id in self.waiting_requests_order:\n             second_priority_states.append(self.waiting_requests[req_id])\n \n@@ -259,30 +281,31 @@ def schedule_batch(self, token_budget: int) -> list[RequestState]:\n         for state in candidates:\n             self._prepare_request_for_processing(state, token_budget, request_ids_to_remove_from_waiting)\n             request_len = len(state.prompt_ids)\n-            if not self._allocate_blocks_if_needed(\n-                state, len(state.prompt_ids)\n-            ):  # don't schedule if we can't allocate blocks\n-                if len(self.cache._free_blocks) == 0:\n+            # If we can't allocate blocks, do not schedule the request and break if the cache is full\n+            if not self._allocate_blocks_if_needed(state):\n+                if self.cache.get_num_free_blocks() == 0:\n                     break\n                 continue\n \n-            @traced\n-            def _add_to_scheduled_requests(state: RequestState):\n-                scheduled_requests.append(state)\n-\n-            _add_to_scheduled_requests(state)\n+            # Add the request to the scheduled requests\n+            scheduled_requests.append(state)\n \n+            # Update the token budget\n             token_budget -= request_len\n-\n-            @traced\n-            def _remove_from_waiting_requests(state: RequestState):\n-                req_id = state.request_id\n-                if req_id in self.waiting_requests:\n-                    del self.waiting_requests[req_id]\n-                    request_ids_to_remove_from_waiting.add(req_id)\n-\n-            _remove_from_waiting_requests(state)\n-\n+            # If using prefix sharing, we make note of the blocks that will be computed in the forward pass\n+            if self.cache.use_prefix_sharing:\n+                tokens_in_current_block = state.current_len() % self.cache.block_size\n+                tokens_after_forward = tokens_in_current_block + request_len\n+                complete_blocks = tokens_after_forward // self.cache.block_size\n+                self.cache.blocks_to_complete[state.request_id] = complete_blocks\n+\n+            # Remove the request from the waiting queue and mark it as removed\n+            req_id = state.request_id\n+            if req_id in self.waiting_requests:\n+                del self.waiting_requests[req_id]\n+                request_ids_to_remove_from_waiting.add(req_id)\n+\n+            # Early exit of the loop if we have no token budget left\n             if token_budget == 0:\n                 break\n "
        },
        {
            "sha": "8ecc6ba42af2f07f30a0901b3e31ea87ca0b36c1",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 64,
            "deletions": 1,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/47227f477532e8ff6dd3290dc6b9e0e38aef303b/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47227f477532e8ff6dd3290dc6b9e0e38aef303b/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=47227f477532e8ff6dd3290dc6b9e0e38aef303b",
            "patch": "@@ -17,7 +17,7 @@\n import torch\n from parameterized import parameterized\n \n-from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList\n+from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, LogitsProcessorList\n from transformers.generation.continuous_batching.cache import group_layers_by_attn_type\n from transformers.generation.continuous_batching.continuous_api import build_attention_mask\n from transformers.testing_utils import (\n@@ -456,6 +456,69 @@ def test_streaming_and_non_streaming_requests_can_alternate(self) -> None:\n \n         manager.stop(block=True)\n \n+    @require_torch_accelerator\n+    def test_prefix_sharing(self) -> None:\n+        model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n+        max_new_tokens = 32\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n+\n+        generation_config = GenerationConfig(do_sample=False, block_size=32)\n+        with model.continuous_batching_context_manager(generation_config=generation_config) as manager:\n+            manager.logit_processor = LogitsProcessorList()\n+\n+            # Create a request with at least 32 tokens but less than 64 so prefill only generates one complete block\n+            messages = [{\"content\": \"What is the Transformers library known for?\", \"role\": \"user\"}]\n+\n+            inputs = tokenizer.apply_chat_template(\n+                messages, return_tensors=\"pt\", add_generation_prompt=True, return_dict=False\n+            )\n+            inputs = inputs.to(model.device)[0].tolist()\n+            self.assertGreaterEqual(len(inputs), 32, f\"Input length is {len(inputs)} instead of at least 32\")\n+            self.assertLess(len(inputs), 64, f\"Input length is {len(inputs)} instead of less than 64\")\n+\n+            # First request, which populates the cache with a complete block\n+            request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens)\n+            chunk_no_reuse = next(manager.request_id_iter(request_id))\n+\n+            hash_table = manager.batch_processor.cache._block_manager._hash_to_id\n+            self.assertEqual(\n+                len(hash_table),\n+                2,\n+                f\"There should be 2 blocks, one for the prefill and one for the decode, but {len(hash_table) = }\",\n+            )\n+            total_prefix_length = manager.batch_processor.cache._total_prefix_length\n+            self.assertEqual(\n+                total_prefix_length, 0, f\"Expected total prefix length to be 0, got {total_prefix_length}\"\n+            )\n+\n+            # Second request, which should reuse the same block\n+            request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens)\n+            chunk_with_reuse = next(manager.request_id_iter(request_id))\n+\n+            # There should only still be two blocks in the hash table because of block reuse\n+            self.assertEqual(\n+                len(hash_table),\n+                2,\n+                f\"Because of block reuse, there should still be two blocks in the hash table, but {len(hash_table) = }\",\n+            )\n+\n+            # Check that the whole prefill was matched\n+            total_prefix_length = manager.batch_processor.cache._total_prefix_length\n+            self.assertEqual(\n+                total_prefix_length, 32, f\"Expected total prefix length to be 32, got {total_prefix_length}\"\n+            )\n+\n+        # Check the outputs were the same\n+        self.assertEqual(chunk_no_reuse.generated_tokens, chunk_with_reuse.generated_tokens)\n+\n+        # As an additional sanity check, we also compare to the generated tokens when prefix sharing is disabled\n+        expected_generated_tokens = Expectations({\n+            (\"rocm\", (9, 4)): [785, 80532, 6733, 374, 3881, 369, 1181, 5726, 311, 1855, 323, 36635, 3460, 12934, 4128, 4119, 11, 2670, 1846, 429, 646, 6923, 1467, 11, 14683, 1467, 11, 323, 2736, 1008, 4128, 13904],\n+        }).get_expectation()  # fmt: skip\n+        self.assertEqual(chunk_no_reuse.generated_tokens, expected_generated_tokens)\n+\n \n # FIXME: the gemma test seem broken, there is a message about cuda graphs and the sdpa and flash expecteations are\n # inverted on CUDA. On AMD they do fine."
        }
    ],
    "stats": {
        "total": 801,
        "additions": 570,
        "deletions": 231
    }
}