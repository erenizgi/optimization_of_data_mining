{
    "author": "MilkClouds",
    "message": "Fix AutoImageProcessor.register and documentation in auto processing modules (#41864)",
    "sha": "32e49f2884cdc23c172513d1a2cafe0f03255591",
    "files": [
        {
            "sha": "22bc20728aad84632d1811811ed2c45c88054a00",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e49f2884cdc23c172513d1a2cafe0f03255591/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e49f2884cdc23c172513d1a2cafe0f03255591/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=32e49f2884cdc23c172513d1a2cafe0f03255591",
            "patch": "@@ -93,7 +93,7 @@ def feature_extractor_class_from_name(class_name: str):\n         if getattr(extractor, \"__name__\", None) == class_name:\n             return extractor\n \n-    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n+    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # init and we return the proper dummy to get an appropriate error message.\n     main_module = importlib.import_module(\"transformers\")\n     if hasattr(main_module, class_name):\n@@ -113,7 +113,7 @@ def get_feature_extractor_config(\n     **kwargs,\n ):\n     \"\"\"\n-    Loads the tokenizer configuration from a pretrained model tokenizer configuration.\n+    Loads the feature extractor configuration from a pretrained model feature extractor configuration.\n \n     Args:\n         pretrained_model_name_or_path (`str` or `os.PathLike`):\n@@ -122,7 +122,7 @@ def get_feature_extractor_config(\n             - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n               huggingface.co.\n             - a path to a *directory* containing a configuration file saved using the\n-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n+              [`~FeatureExtractionMixin.save_pretrained`] method, e.g., `./my_model_directory/`.\n \n         cache_dir (`str` or `os.PathLike`, *optional*):\n             Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n@@ -141,7 +141,7 @@ def get_feature_extractor_config(\n             git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n             identifier allowed by git.\n         local_files_only (`bool`, *optional*, defaults to `False`):\n-            If `True`, will only try to load the tokenizer configuration from local files.\n+            If `True`, will only try to load the feature extractor configuration from local files.\n \n     <Tip>\n \n@@ -150,22 +150,22 @@ def get_feature_extractor_config(\n     </Tip>\n \n     Returns:\n-        `Dict`: The configuration of the tokenizer.\n+        `Dict`: The configuration of the feature extractor.\n \n     Examples:\n \n     ```python\n     # Download configuration from huggingface.co and cache.\n-    tokenizer_config = get_tokenizer_config(\"google-bert/bert-base-uncased\")\n-    # This model does not have a tokenizer config so the result will be an empty dict.\n-    tokenizer_config = get_tokenizer_config(\"FacebookAI/xlm-roberta-base\")\n+    feature_extractor_config = get_feature_extractor_config(\"facebook/wav2vec2-base-960h\")\n+    # This model does not have a feature extractor config so the result will be an empty dict.\n+    feature_extractor_config = get_feature_extractor_config(\"FacebookAI/xlm-roberta-base\")\n \n-    # Save a pretrained tokenizer locally and you can reload its config\n-    from transformers import AutoTokenizer\n+    # Save a pretrained feature extractor locally and you can reload its config\n+    from transformers import AutoFeatureExtractor\n \n-    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-    tokenizer.save_pretrained(\"tokenizer-test\")\n-    tokenizer_config = get_tokenizer_config(\"tokenizer-test\")\n+    feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n+    feature_extractor.save_pretrained(\"feature-extractor-test\")\n+    feature_extractor_config = get_feature_extractor_config(\"feature-extractor-test\")\n     ```\"\"\"\n     resolved_config_file = cached_file(\n         pretrained_model_name_or_path,"
        },
        {
            "sha": "0bd8cc850e2c5478c4c82e8a86ee0d188cd0690e",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e49f2884cdc23c172513d1a2cafe0f03255591/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e49f2884cdc23c172513d1a2cafe0f03255591/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=32e49f2884cdc23c172513d1a2cafe0f03255591",
            "patch": "@@ -260,7 +260,7 @@ def get_image_processor_config(\n             - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n               huggingface.co.\n             - a path to a *directory* containing a configuration file saved using the\n-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n+              [`~ProcessorMixin.save_pretrained`] method, e.g., `./my_model_directory/`.\n \n         cache_dir (`str` or `os.PathLike`, *optional*):\n             Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n@@ -299,7 +299,7 @@ def get_image_processor_config(\n     image_processor_config = get_image_processor_config(\"FacebookAI/xlm-roberta-base\")\n \n     # Save a pretrained image processor locally and you can reload its config\n-    from transformers import AutoTokenizer\n+    from transformers import AutoImageProcessor\n \n     image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n     image_processor.save_pretrained(\"image-processor-test\")\n@@ -629,19 +629,6 @@ def register(\n         ):\n             raise ValueError(\"The `fast_image_processor_class` should inherit from `BaseImageProcessorFast`.\")\n \n-        if (\n-            slow_image_processor_class is not None\n-            and fast_image_processor_class is not None\n-            and issubclass(fast_image_processor_class, BaseImageProcessorFast)\n-            and fast_image_processor_class.slow_image_processor_class != slow_image_processor_class\n-        ):\n-            raise ValueError(\n-                \"The fast processor class you are passing has a `slow_image_processor_class` attribute that is not \"\n-                \"consistent with the slow processor class you passed (fast tokenizer has \"\n-                f\"{fast_image_processor_class.slow_image_processor_class} and you passed {slow_image_processor_class}. Fix one of those \"\n-                \"so they match!\"\n-            )\n-\n         # Avoid resetting a set slow/fast image processor if we are passing just the other ones.\n         if config_class in IMAGE_PROCESSOR_MAPPING._extra_content:\n             existing_slow, existing_fast = IMAGE_PROCESSOR_MAPPING[config_class]"
        },
        {
            "sha": "137828d3f6cc790b1e4c40b515fe03f22eee9fff",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e49f2884cdc23c172513d1a2cafe0f03255591/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e49f2884cdc23c172513d1a2cafe0f03255591/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=32e49f2884cdc23c172513d1a2cafe0f03255591",
            "patch": "@@ -175,7 +175,7 @@ def processor_class_from_name(class_name: str):\n         if getattr(processor, \"__name__\", None) == class_name:\n             return processor\n \n-    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n+    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # init and we return the proper dummy to get an appropriate error message.\n     main_module = importlib.import_module(\"transformers\")\n     if hasattr(main_module, class_name):"
        },
        {
            "sha": "9b85aeb7913557730ef7c5faf7f44a0c9247f067",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e49f2884cdc23c172513d1a2cafe0f03255591/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e49f2884cdc23c172513d1a2cafe0f03255591/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=32e49f2884cdc23c172513d1a2cafe0f03255591",
            "patch": "@@ -815,7 +815,7 @@ def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n             if getattr(tokenizer, \"__name__\", None) == class_name:\n                 return tokenizer\n \n-    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n+    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # init and we return the proper dummy to get an appropriate error message.\n     main_module = importlib.import_module(\"transformers\")\n     if hasattr(main_module, class_name):"
        },
        {
            "sha": "bcac454b2d65f9d6637d741bcf181fc302c69f8d",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e49f2884cdc23c172513d1a2cafe0f03255591/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e49f2884cdc23c172513d1a2cafe0f03255591/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=32e49f2884cdc23c172513d1a2cafe0f03255591",
            "patch": "@@ -122,7 +122,7 @@ def get_video_processor_config(\n             - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n               huggingface.co.\n             - a path to a *directory* containing a configuration file saved using the\n-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n+              [`~BaseVideoProcessor.save_pretrained`] method, e.g., `./my_model_directory/`.\n \n         cache_dir (`str` or `os.PathLike`, *optional*):\n             Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n@@ -313,9 +313,14 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n \n         has_remote_code = video_processor_auto_map is not None\n         has_local_code = video_processor_class is not None or type(config) in VIDEO_PROCESSOR_MAPPING\n-        trust_remote_code = resolve_trust_remote_code(\n-            trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n-        )\n+        if has_remote_code:\n+            if \"--\" in video_processor_auto_map:\n+                upstream_repo = video_processor_auto_map.split(\"--\")[0]\n+            else:\n+                upstream_repo = None\n+            trust_remote_code = resolve_trust_remote_code(\n+                trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n+            )\n \n         if has_remote_code and trust_remote_code:\n             class_ref = video_processor_auto_map"
        }
    ],
    "stats": {
        "total": 60,
        "additions": 26,
        "deletions": 34
    }
}