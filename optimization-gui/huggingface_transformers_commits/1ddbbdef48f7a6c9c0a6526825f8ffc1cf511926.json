{
    "author": "SunMarc",
    "message": "[Trainer] deprecate ray scope (#41403)",
    "sha": "1ddbbdef48f7a6c9c0a6526825f8ffc1cf511926",
    "files": [
        {
            "sha": "e0b2cd3363725a21f6b3fc85385972f2e40ef049",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ddbbdef48f7a6c9c0a6526825f8ffc1cf511926/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ddbbdef48f7a6c9c0a6526825f8ffc1cf511926/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=1ddbbdef48f7a6c9c0a6526825f8ffc1cf511926",
            "patch": "@@ -290,6 +290,14 @@ def _objective(trial: optuna.Trial, checkpoint_dir=None):\n \n \n def run_hp_search_ray(trainer, n_trials: int, direction: str, **kwargs) -> BestRun:\n+    \"\"\"\n+    Environment:\n+        - **RAY_SCOPE** (`str`, *optional*, defaults to `\"last\"`):\n+            The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray\n+            will then use the last checkpoint of all trials, compare those, and select the best one. However,\n+            other options are also available. See the Ray documentation (https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial)\n+            for more options\n+    \"\"\"\n     import ray\n     import ray.train\n \n@@ -416,7 +424,8 @@ def dynamic_modules_import_trainable(*args, **kwargs):\n         num_samples=n_trials,\n         **kwargs,\n     )\n-    best_trial = analysis.get_best_trial(metric=\"objective\", mode=direction[:3], scope=trainer.args.ray_scope)\n+    ray_scope = os.getenv(\"RAY_SCOPE\", \"last\")\n+    best_trial = analysis.get_best_trial(metric=\"objective\", mode=direction[:3], scope=ray_scope)\n     best_run = BestRun(best_trial.trial_id, best_trial.last_result[\"objective\"], best_trial.config, analysis)\n     if _tb_writer is not None:\n         trainer.add_callback(_tb_writer)"
        },
        {
            "sha": "a09f56caed7fc17e3a4f1bb429525a64a9da333a",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ddbbdef48f7a6c9c0a6526825f8ffc1cf511926/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ddbbdef48f7a6c9c0a6526825f8ffc1cf511926/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=1ddbbdef48f7a6c9c0a6526825f8ffc1cf511926",
            "patch": "@@ -690,12 +690,6 @@ class TrainingArguments:\n         full_determinism (`bool`, *optional*, defaults to `False`)\n             If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in\n             distributed training. Important: this will negatively impact the performance, so only use it for debugging.\n-        ray_scope (`str`, *optional*, defaults to `\"last\"`):\n-            The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray will\n-            then use the last checkpoint of all trials, compare those, and select the best one. However, other options\n-            are also available. See the [Ray documentation](\n-            https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for\n-            more options.\n         ddp_timeout (`int`, *optional*, defaults to 1800):\n             The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts when\n             performing slow operations in distributed runnings. Please refer the [PyTorch documentation]\n@@ -1311,9 +1305,10 @@ class TrainingArguments:\n         },\n     )\n     ray_scope: str = field(\n-        default=\"last\",\n+        default=None,\n         metadata={\n             \"help\": (\n+                \"This argument is deprecated and will be removed in v5.2. Set env var RAY_SCOPE instead.\"\n                 'The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray'\n                 \" will then use the last checkpoint of all trials, compare those, and select the best one. However,\"\n                 \" other options are also available. See the Ray documentation\""
        }
    ],
    "stats": {
        "total": 20,
        "additions": 12,
        "deletions": 8
    }
}