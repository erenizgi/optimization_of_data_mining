{
    "author": "ariG23498",
    "message": "[Docs] Adding documentation of MXFP4 Quantization (#40885)\n\n* adding mxfp4 quantization docs\n\n* review suggestions\n\n* Apply suggestions from code review\n\nCo-authored-by: vb <vaibhavs10@gmail.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: vb <vaibhavs10@gmail.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "6e50a8afb2540ac1acaa4b62cf1dd5f1170f6511",
    "files": [
        {
            "sha": "4ddfa2ed167dc4cb9ef7a2c12b95d1a0345524d8",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e50a8afb2540ac1acaa4b62cf1dd5f1170f6511/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e50a8afb2540ac1acaa4b62cf1dd5f1170f6511/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=6e50a8afb2540ac1acaa4b62cf1dd5f1170f6511",
            "patch": "@@ -199,6 +199,8 @@\n     title: HIGGS\n   - local: quantization/hqq\n     title: HQQ\n+  - local: quantization/mxfp4\n+    title: MXFP4\n   - local: quantization/optimum\n     title: Optimum\n   - local: quantization/quanto"
        },
        {
            "sha": "a2b9f7634c8d6f4a65dd99f868d2be1c79144161",
            "filename": "docs/source/en/quantization/mxfp4.md",
            "status": "added",
            "additions": 80,
            "deletions": 0,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e50a8afb2540ac1acaa4b62cf1dd5f1170f6511/docs%2Fsource%2Fen%2Fquantization%2Fmxfp4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e50a8afb2540ac1acaa4b62cf1dd5f1170f6511/docs%2Fsource%2Fen%2Fquantization%2Fmxfp4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fmxfp4.md?ref=6e50a8afb2540ac1acaa4b62cf1dd5f1170f6511",
            "patch": "@@ -0,0 +1,80 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# MXFP4\n+\n+Note: MXFP4 quantisation currently only works for OpenAI GPT-OSS 120b and 20b. \n+\n+MXFP4 is a 4-bit floating point format that dramatically reduces the memory requirements of large models. Large models (GPT-OSS-120B) can fit on a single 80GB GPU and smaller models (GPT-OSS-20B) only require 16GB of memory. It uses blockwise scaling to preserve it's range and accuracy, which typically becomes degraded at lower precisions.\n+\n+To use MXPF4, make sure your hardware meets the following requirements.\n+\n+- Install Accelerate, kernels, and Triton ≥ 3.4. Only manually install Triton ≥ 3.4 if you're using PyTorch 2.7 because it is already supported in PyTorch 2.8.\n+- NVIDIA GPU Compute Capability ≥ 7.5 which includes Tesla GPUs and newer. Use [get_device_capability](https://docs.pytorch.org/docs/stable/generated/torch.cuda.get_device_capability.html) to check Compute Capability.\n+\n+\n+```python\n+from torch import cuda\n+cuda.get_device_capability()\n+\n+# (7, 5)\n+```\n+\n+Check a model's quantization config as shown below to see if it supports MXFP4. If `'quant_method': 'mxfp4'`, then the model automatically uses MXFP4.\n+\n+```py\n+from transformers import GptOssConfig\n+\n+model_id = \"openai/gpt-oss-120b\"\n+cfg = GptOssConfig.from_pretrained(model_id)\n+print(cfg.quantization_config)\n+\n+# Example output:\n+# {\n+#   'modules_to_not_convert': [\n+#     'model.layers.*.self_attn',\n+#     'model.layers.*.mlp.router',\n+#     'model.embed_tokens',\n+#     'lm_head'\n+#   ],\n+#   'quant_method': 'mxfp4'\n+# }\n+```\n+\n+\n+## MXFP4 kernels\n+\n+Transformers automatically pulls the MXFP4-aware Triton kernels from the community repository when you load a model that needs them. The kernels are stored in your local cache and used during the forward pass.\n+\n+MXFP4 kernels are used by default, if available and supported, and does not require any code changes.\n+\n+You can use [hf cache scan](https://huggingface.co/docs/huggingface_hub/en/guides/manage-cache#scan-your-cache) to verify the kernels are downloaded.\n+\n+```shell\n+hf cache scan\n+```\n+\n+\n+```shell\n+REPO ID                          REPO TYPE SIZE ON DISK\n+-------------------------------- --------- ------------\n+kernels-community/triton_kernels model           536.2K\n+openai/gpt-oss-20b               model            13.8G\n+```\n+\n+## Resources\n+\n+Learn more about MXFP4 quantization and how blockwise scaling works in this [blog post](https://huggingface.co/blog/faster-transformers#mxfp4-quantization)."
        }
    ],
    "stats": {
        "total": 82,
        "additions": 82,
        "deletions": 0
    }
}