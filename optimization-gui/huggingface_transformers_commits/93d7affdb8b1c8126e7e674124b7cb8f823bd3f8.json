{
    "author": "zucchini-nlp",
    "message": "Generation config boolean defaults (#43000)\n\n* push\n\n* fix a few tests\n\n* why no tests are failing, suspicious\n\n* one tiny fix\n\n* maybe?\n\n* typo\n\n* clvp has incorrect config types\n\n* nit\n\n* docstring and revert unrelated trainer\n\n* forgot to revert this one\n\n* Update src/transformers/generation/configuration_utils.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* delete the prev fix and add small comment\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "93d7affdb8b1c8126e7e674124b7cb8f823bd3f8",
    "files": [
        {
            "sha": "be58b78fd4a897af46048be1bfe0188f0a9782a5",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=93d7affdb8b1c8126e7e674124b7cb8f823bd3f8",
            "patch": "@@ -1144,11 +1144,15 @@ def register_for_auto_class(cls, auto_class=\"AutoConfig\"):\n \n     def _get_generation_parameters(self) -> dict[str, Any]:\n         \"\"\"\n-        Gets the non-default generation parameters on the PreTrainedConfig instance\n+        Checks if there are generation parameters in `PreTrainedConfig` instance. Note that\n+        we should not save generation params in PreTrainedConfig, and we will raise error\n+        if there are any.\n         \"\"\"\n         generation_params = {}\n         default_config = self.__class__().to_dict() if not self.has_no_defaults_at_init else {}\n         for key in GenerationConfig._get_default_generation_params().keys():\n+            if key == \"use_cache\":\n+                continue  # common key for most models\n             if hasattr(self, key) and getattr(self, key) is not None and key not in default_config:\n                 generation_params[key] = getattr(self, key)\n "
        },
        {
            "sha": "8d9d80e9fc996851e4e6ce5860fa6374fd417d2a",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 46,
            "deletions": 34,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=93d7affdb8b1c8126e7e674124b7cb8f823bd3f8",
            "patch": "@@ -100,6 +100,10 @@ class GenerationConfig(PushToHubMixin):\n \n     </Tip>\n \n+    Note: the configuration field that are still `None` will be overriden by `GenerationConfig._get_default_generation_params()`\n+    during the generation loop. If you want to use different values for these fields, make sure to explicitly set them in the\n+    generation config.\n+\n     Arg:\n         > Parameters that control the length of the output\n \n@@ -128,14 +132,14 @@ class GenerationConfig(PushToHubMixin):\n \n         > Parameters that control the generation strategy used\n \n-        do_sample (`bool`, defaults to `False`):\n+        do_sample (`bool`):\n             Whether or not to use sampling ; use greedy decoding otherwise.\n         num_beams (`int`, *optional*):\n             Number of beams for beam search. 1 means no beam search.\n \n         > Parameters that control the cache\n \n-        use_cache (`bool`, defaults to `True`):\n+        use_cache (`bool`):\n             Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n             speed up decoding.\n         cache_implementation (`str`, *optional*):\n@@ -205,7 +209,7 @@ class GenerationConfig(PushToHubMixin):\n         bad_words_ids (`list[list[int]]`, *optional*):\n             List of list of token ids that are not allowed to be generated. Check\n             [`~generation.NoBadWordsLogitsProcessor`] for further documentation and examples.\n-        renormalize_logits (`bool`, defaults to `False`):\n+        renormalize_logits (`bool`):\n             Whether to renormalize the logits after applying all the logits processors (including the custom\n             ones). It's highly recommended to set this flag to `True` as the search algorithms suppose the score logits\n             are normalized but some logit processors break the normalization.\n@@ -216,7 +220,7 @@ class GenerationConfig(PushToHubMixin):\n         forced_eos_token_id (`int` or list[int]`, *optional*, defaults to `model.config.forced_eos_token_id`):\n             The id of the token to force as the last generated token when `max_length` is reached. Optionally, use a\n             list to set multiple *end-of-sequence* tokens.\n-        remove_invalid_values (`bool`, defaults to `model.config.remove_invalid_values`):\n+        remove_invalid_values (`bool`):\n             Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to crash.\n             Note that using `remove_invalid_values` can slow down generation.\n         exponential_decay_length_penalty (`tuple(int, float)`, *optional*):\n@@ -233,7 +237,7 @@ class GenerationConfig(PushToHubMixin):\n             Dictionary that maps a sequence of tokens to its bias term. Positive biases increase the odds of the\n             sequence being selected, while negative biases do the opposite. Check\n             [`~generation.SequenceBiasLogitsProcessor`] for further documentation and examples.\n-        token_healing (`bool`, defaults to `False`):\n+        token_healing (`bool`):\n             Heal tail tokens of prompts by replacing them with their appropriate extensions.\n             This enhances the quality of completions for prompts affected by greedy tokenization bias.\n         guidance_scale (`float`, *optional*):\n@@ -247,20 +251,20 @@ class GenerationConfig(PushToHubMixin):\n \n         > Parameters that define the output variables of generate\n \n-        num_return_sequences (`int`, *optional*, defaults to 1):\n+        num_return_sequences (`int`, *optional*):\n             The number of independently computed returned sequences for each element in the batch.\n-        output_attentions (`bool`, defaults to `False`):\n+        output_attentions (`bool`):\n             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n             tensors for more details.\n-        output_hidden_states (`bool`, defaults to `False`):\n+        output_hidden_states (`bool`):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more details.\n-        output_scores (`bool`, defaults to `False`):\n+        output_scores (`bool`):\n             Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n-        output_logits (`bool`, defaults to `False`):\n+        output_logits (`bool`):\n             Whether or not to return the unprocessed prediction logit scores. See `logits` under returned tensors for\n             more details.\n-        return_dict_in_generate (`bool`, defaults to `False`):\n+        return_dict_in_generate (`bool`):\n             Whether or not to return a [`~utils.ModelOutput`], as opposed to returning exclusively the generated\n             sequence. This flag must be set to `True` to return the generation cache (when `use_cache` is `True`)\n             or optional outputs (see flags starting with `output_`)\n@@ -285,7 +289,7 @@ class GenerationConfig(PushToHubMixin):\n             (e.g. multilingual models with different target languages in one batch)\n \n         > Generation parameters exclusive to assistant generation\n-        is_assistant (`bool`, defaults to `False`):\n+        is_assistant (`bool`):\n             Whether the model is an assistant (draft) model.\n         num_assistant_tokens (`int`, *optional*):\n             Defines the number of _speculative tokens_ that shall be generated by the assistant model before being\n@@ -326,7 +330,7 @@ class GenerationConfig(PushToHubMixin):\n         compile_config (CompileConfig, *optional*):\n             If using a compilable cache, this controls how `generate` will `compile` the forward pass for faster\n             inference.\n-        disable_compile (`bool`, defaults to `False`):\n+        disable_compile (`bool`):\n             Whether to disable the automatic compilation of the forward pass. Automatic compilation happens when\n             specific criteria are met, including using a compilable cache. Please open an issue if you find the\n             need to use this flag.\n@@ -345,11 +349,11 @@ def __init__(self, **kwargs):\n         self.stop_strings = kwargs.pop(\"stop_strings\", None)\n \n         # Parameters that control the generation strategy used\n-        self.do_sample = kwargs.pop(\"do_sample\", False)\n+        self.do_sample = kwargs.pop(\"do_sample\", None)\n         self.num_beams = kwargs.pop(\"num_beams\", None)\n \n         # Parameters that control the cache\n-        self.use_cache = kwargs.pop(\"use_cache\", True)\n+        self.use_cache = kwargs.pop(\"use_cache\", None)\n         self.cache_implementation = kwargs.pop(\"cache_implementation\", None)\n         self.cache_config = kwargs.pop(\"cache_config\", None)\n \n@@ -367,28 +371,28 @@ def __init__(self, **kwargs):\n         self.length_penalty = kwargs.pop(\"length_penalty\", None)\n         self.no_repeat_ngram_size = kwargs.pop(\"no_repeat_ngram_size\", None)\n         self.bad_words_ids = kwargs.pop(\"bad_words_ids\", None)\n-        self.renormalize_logits = kwargs.pop(\"renormalize_logits\", False)\n+        self.renormalize_logits = kwargs.pop(\"renormalize_logits\", None)\n         self.forced_bos_token_id = kwargs.pop(\"forced_bos_token_id\", None)\n         self.forced_eos_token_id = kwargs.pop(\"forced_eos_token_id\", None)\n-        self.remove_invalid_values = kwargs.pop(\"remove_invalid_values\", False)\n+        self.remove_invalid_values = kwargs.pop(\"remove_invalid_values\", None)\n         self.exponential_decay_length_penalty = kwargs.pop(\"exponential_decay_length_penalty\", None)\n         self.suppress_tokens = kwargs.pop(\"suppress_tokens\", None)\n         self.begin_suppress_tokens = kwargs.pop(\"begin_suppress_tokens\", None)\n         self.sequence_bias = kwargs.pop(\"sequence_bias\", None)\n-        self.token_healing = kwargs.pop(\"token_healing\", False)\n+        self.token_healing = kwargs.pop(\"token_healing\", None)\n         self.guidance_scale = kwargs.pop(\"guidance_scale\", None)\n \n         self.watermarking_config = kwargs.pop(\"watermarking_config\", None)\n         if isinstance(self.watermarking_config, dict):\n             self.watermarking_config = WatermarkingConfig.from_dict(self.watermarking_config)\n \n         # Parameters that define the output variables of `generate`\n-        self.num_return_sequences = kwargs.pop(\"num_return_sequences\", 1)\n-        self.output_attentions = kwargs.pop(\"output_attentions\", False)\n-        self.output_hidden_states = kwargs.pop(\"output_hidden_states\", False)\n-        self.output_scores = kwargs.pop(\"output_scores\", False)\n-        self.output_logits = kwargs.pop(\"output_logits\", False)\n-        self.return_dict_in_generate = kwargs.pop(\"return_dict_in_generate\", False)\n+        self.num_return_sequences = kwargs.pop(\"num_return_sequences\", None)\n+        self.output_attentions = kwargs.pop(\"output_attentions\", None)\n+        self.output_hidden_states = kwargs.pop(\"output_hidden_states\", None)\n+        self.output_scores = kwargs.pop(\"output_scores\", None)\n+        self.output_logits = kwargs.pop(\"output_logits\", None)\n+        self.return_dict_in_generate = kwargs.pop(\"return_dict_in_generate\", None)\n \n         # Special tokens that can be used at generation time\n         self.pad_token_id = kwargs.pop(\"pad_token_id\", None)\n@@ -400,7 +404,7 @@ def __init__(self, **kwargs):\n         self.decoder_start_token_id = kwargs.pop(\"decoder_start_token_id\", None)\n \n         # Assistant generation\n-        self.is_assistant = kwargs.pop(\"is_assistant\", False)\n+        self.is_assistant = kwargs.pop(\"is_assistant\", None)\n         self.num_assistant_tokens = kwargs.pop(\"num_assistant_tokens\", None)\n         self.num_assistant_tokens_schedule = kwargs.pop(\"num_assistant_tokens_schedule\", None)\n         self.assistant_confidence_threshold = kwargs.pop(\"assistant_confidence_threshold\", None)\n@@ -412,7 +416,7 @@ def __init__(self, **kwargs):\n \n         # Performance\n         self.compile_config = kwargs.pop(\"compile_config\", None)\n-        self.disable_compile = kwargs.pop(\"disable_compile\", False)\n+        self.disable_compile = kwargs.pop(\"disable_compile\", None)\n \n         # Deprecated (moved to the Hub). TODO remove for v5\n         self.low_memory = kwargs.pop(\"low_memory\", None)\n@@ -482,7 +486,7 @@ def get_generation_mode(self, assistant_model: Optional[\"PreTrainedModel\"] = Non\n         if self.constraints is not None or self.force_words_ids is not None:\n             generation_mode = GenerationMode.CONSTRAINED_BEAM_SEARCH\n         elif self.num_beams is None or self.num_beams == 1:\n-            if not self.do_sample:\n+            if self.do_sample is not True:\n                 if (\n                     self.top_k is not None\n                     and self.top_k > 1\n@@ -497,7 +501,7 @@ def get_generation_mode(self, assistant_model: Optional[\"PreTrainedModel\"] = Non\n         else:\n             if self.num_beam_groups is not None and self.num_beam_groups > 1:\n                 generation_mode = GenerationMode.GROUP_BEAM_SEARCH\n-            elif self.do_sample:\n+            elif self.do_sample is True:\n                 generation_mode = GenerationMode.BEAM_SAMPLE\n             else:\n                 generation_mode = GenerationMode.BEAM_SEARCH\n@@ -536,6 +540,7 @@ def _get_default_generation_params() -> dict[str, Any]:\n             \"max_length\": 20,\n             \"min_length\": 0,\n             \"do_sample\": False,\n+            \"use_cache\": True,\n             \"early_stopping\": False,\n             \"num_beams\": 1,\n             \"temperature\": 1.0,\n@@ -615,7 +620,10 @@ def validate(self, strict=False):\n \n         # 2. Validation of attribute combinations\n         # 2.1. detect sampling-only parameterization when not in sampling mode\n-        if not self.do_sample:\n+\n+        # Note that we check `is not True` in purpose. Boolean fields can also be `None` so we\n+        # have to be explicit. Value of `None` is same as having `False`, i.e. the default value\n+        if self.do_sample is not True:\n             greedy_wrong_parameter_msg = (\n                 \"`do_sample` is set not to set `True`. However, `{flag_name}` is set to `{flag_value}` -- this flag is only \"\n                 \"used in sample-based generation modes. You should set `do_sample=True` or unset `{flag_name}`.\"\n@@ -661,21 +669,25 @@ def validate(self, strict=False):\n                 )\n \n         # 2.4. check `num_return_sequences`\n-        if self.num_return_sequences > 1:\n+        if self.num_return_sequences is not None and self.num_return_sequences > 1:\n             if self.num_beams is None or self.num_beams == 1:\n                 if not self.do_sample:\n                     raise ValueError(\n                         \"Greedy methods (do_sample != True) without beam search do not support \"\n                         f\"`num_return_sequences` different than 1 (got {self.num_return_sequences}).\"\n                     )\n-            elif self.num_beams is not None and self.num_return_sequences > self.num_beams:\n+            elif (\n+                self.num_beams is not None\n+                and self.num_return_sequences is not None\n+                and self.num_return_sequences > self.num_beams\n+            ):\n                 raise ValueError(\n                     f\"`num_return_sequences` ({self.num_return_sequences}) has to be smaller or equal to `num_beams` \"\n                     f\"({self.num_beams}).\"\n                 )\n \n         # 2.5. check cache-related arguments\n-        if not self.use_cache:\n+        if self.use_cache is not True:\n             # In this case, all cache-related arguments should be unset. However, since `use_cache=False` is often used\n             # passed to `generate` directly to hot-fix cache issues, let's raise a warning instead of an error\n             # (otherwise a user might need to overwrite several parameters).\n@@ -690,9 +702,9 @@ def validate(self, strict=False):\n                     )\n \n         # 2.6. other incorrect combinations\n-        if not self.return_dict_in_generate:\n+        if self.return_dict_in_generate is not True:\n             for extra_output_flag in self.extra_output_flags:\n-                if getattr(self, extra_output_flag):\n+                if getattr(self, extra_output_flag) is True:\n                     minor_issues[extra_output_flag] = (\n                         f\"`return_dict_in_generate` is NOT set to `True`, but `{extra_output_flag}` is. When \"\n                         f\"`return_dict_in_generate` is not `True`, `{extra_output_flag}` is ignored.\""
        },
        {
            "sha": "70da77a19ed623d17da9b0ecae7bc32e5f033df1",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=93d7affdb8b1c8126e7e674124b7cb8f823bd3f8",
            "patch": "@@ -1797,20 +1797,6 @@ def _prepare_generation_config(\n         generation_config.update(**self.generation_config.to_dict(), defaults_only=True)\n         generation_config.update(**global_defaults, defaults_only=True)\n \n-        # Due to some values being boolean and not `None`, we need additional logic to overwrite\n-        # them explicitly (`defaults_only=False`) on the condition that it's only a previous\n-        # default value\n-        default_generation_config = GenerationConfig()\n-        generation_config.update(\n-            **{\n-                k: v\n-                for k, v in self.generation_config.to_dict().items()\n-                if isinstance(v, bool)\n-                and hasattr(default_generation_config, k)\n-                and getattr(generation_config, k, None) == getattr(default_generation_config, k)\n-            }\n-        )\n-\n         # Finally, if there are any kwargs, update config with it -> highest priority at the end\n         model_kwargs = generation_config.update(**kwargs)\n "
        },
        {
            "sha": "ac637c92f163b9d69cb99ee83b13874e9255a878",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=93d7affdb8b1c8126e7e674124b7cb8f823bd3f8",
            "patch": "@@ -836,6 +836,8 @@ class ClvpEncoder(ClvpPreTrainedModel):\n         config: ClvpConfig\n     \"\"\"\n \n+    config: ClvpEncoderConfig\n+\n     def __init__(self, config: ClvpConfig):\n         super().__init__(config)\n \n@@ -989,6 +991,8 @@ class ClvpDecoder(ClvpPreTrainedModel):\n     Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`ClvpDecoderLayer`]\n     \"\"\"\n \n+    config: ClvpDecoderConfig\n+\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -1151,6 +1155,8 @@ def forward(\n \n @auto_docstring\n class ClvpModel(ClvpPreTrainedModel):\n+    config: ClvpDecoderConfig\n+\n     def __init__(self, config: ClvpDecoderConfig):\n         super().__init__(config)\n         self.config = config\n@@ -1221,6 +1227,8 @@ def forward(\n     \"\"\"\n )\n class ClvpForCausalLM(ClvpPreTrainedModel, GenerationMixin):\n+    config: ClvpDecoderConfig\n+\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -1416,8 +1424,6 @@ def forward(\n     \"\"\"\n )\n class ClvpModelForConditionalGeneration(ClvpPreTrainedModel, GenerationMixin):\n-    config: ClvpConfig\n-\n     def __init__(self, config: ClvpConfig):\n         super().__init__(config)\n "
        },
        {
            "sha": "3ca904db0c57d735e19531bd1f2aba454358e4a0",
            "filename": "tests/generation/test_configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_configuration_utils.py?ref=93d7affdb8b1c8126e7e674124b7cb8f823bd3f8",
            "patch": "@@ -124,7 +124,7 @@ def test_kwarg_init(self):\n         \"\"\"Tests that we can overwrite attributes at `from_pretrained` time.\"\"\"\n         default_config = GenerationConfig()\n         self.assertEqual(default_config.temperature, None)\n-        self.assertEqual(default_config.do_sample, False)\n+        self.assertEqual(default_config.do_sample, None)\n         self.assertEqual(default_config.num_beams, None)\n \n         config = GenerationConfig("
        },
        {
            "sha": "f645fc1e6d3ebd740a5b380abf124f97a6fb0c52",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=93d7affdb8b1c8126e7e674124b7cb8f823bd3f8",
            "patch": "@@ -1207,7 +1207,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n \n             input_ids = inputs_dict.pop(\"input_ids\")\n \n-            model.config.use_cache = True\n+            model.generation_config.use_cache = True\n             model.config.is_decoder = True\n             batch_size = input_ids.shape[0]\n             max_new_tokens = 10\n@@ -1676,10 +1676,10 @@ def test_generate_methods_with_logits_to_keep(self):\n                 self.skipTest(reason=\"This model does not support `logits_to_keep` argument.\")\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            config.use_cache = True\n             config.is_decoder = True\n \n             model = model_class(config).to(torch_device).eval()\n+            model.generation_config.use_cache = True\n             # All generation methods (except assisted decoding) rely on always extracting the last token logits of the\n             # full logits matrix, so testing out only greedy search and assisted decoding is enough (if it works,\n             # other methods will work as well)"
        },
        {
            "sha": "813f6d9c2d306986dbf0b4b4b20558bfe1b20c0b",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/93d7affdb8b1c8126e7e674124b7cb8f823bd3f8/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=93d7affdb8b1c8126e7e674124b7cb8f823bd3f8",
            "patch": "@@ -987,7 +987,7 @@ def test_xsum_summarization_same_as_fairseq(self):\n \n     def test_xsum_config_generation_params(self):\n         model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-xsum\")\n-        expected_params = {\"num_beams\": 6, \"do_sample\": False, \"early_stopping\": True, \"length_penalty\": None}\n+        expected_params = {\"num_beams\": 6, \"do_sample\": None, \"early_stopping\": True, \"length_penalty\": None}\n         config_params = {k: getattr(model.generation_config, k, \"MISSING\") for k, v in expected_params.items()}\n         self.assertDictEqual(expected_params, config_params)\n "
        }
    ],
    "stats": {
        "total": 118,
        "additions": 63,
        "deletions": 55
    }
}