{
    "author": "yonigozlan",
    "message": "Optim deformable detr (#33600)\n\n* optimize deformable detr\r\n\r\n* fix copies\r\n\r\n* remove deformable_detr_basline\r\n\r\n* fix hardcoded float16 and .float()\r\n\r\n* [run slow] deformable-detr,grounding-dino,mask2former,oneformer,rt-detr\r\n\r\n* [run slow] deformable_detr,grounding_dino,mask2former,oneformer,rt_detr",
    "sha": "ee71c9853a2d19b1b35b9f8fc8896feac5b2659c",
    "files": [
        {
            "sha": "f380c3c3b4813929cea5528ef1f56d31dbe47ac2",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 39,
            "deletions": 15,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee71c9853a2d19b1b35b9f8fc8896feac5b2659c/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee71c9853a2d19b1b35b9f8fc8896feac5b2659c/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=ee71c9853a2d19b1b35b9f8fc8896feac5b2659c",
            "patch": "@@ -523,14 +523,14 @@ def __init__(self, embedding_dim=64, temperature=10000, normalize=False, scale=N\n     def forward(self, pixel_values, pixel_mask):\n         if pixel_mask is None:\n             raise ValueError(\"No pixel mask provided\")\n-        y_embed = pixel_mask.cumsum(1, dtype=torch.float32)\n-        x_embed = pixel_mask.cumsum(2, dtype=torch.float32)\n+        y_embed = pixel_mask.cumsum(1, dtype=pixel_values.dtype)\n+        x_embed = pixel_mask.cumsum(2, dtype=pixel_values.dtype)\n         if self.normalize:\n             eps = 1e-6\n             y_embed = (y_embed - 0.5) / (y_embed[:, -1:, :] + eps) * self.scale\n             x_embed = (x_embed - 0.5) / (x_embed[:, :, -1:] + eps) * self.scale\n \n-        dim_t = torch.arange(self.embedding_dim, dtype=torch.int64, device=pixel_values.device).float()\n+        dim_t = torch.arange(self.embedding_dim, dtype=pixel_values.dtype, device=pixel_values.device)\n         dim_t = self.temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / self.embedding_dim)\n \n         pos_x = x_embed[:, :, :, None] / dim_t\n@@ -580,11 +580,14 @@ def build_position_encoding(config):\n \n \n def multi_scale_deformable_attention(\n-    value: Tensor, value_spatial_shapes: Tensor, sampling_locations: Tensor, attention_weights: Tensor\n+    value: Tensor,\n+    value_spatial_shapes: Union[Tensor, List[Tuple]],\n+    sampling_locations: Tensor,\n+    attention_weights: Tensor,\n ) -> Tensor:\n     batch_size, _, num_heads, hidden_dim = value.shape\n     _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-    value_list = value.split([height.item() * width.item() for height, width in value_spatial_shapes], dim=1)\n+    value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n     sampling_grids = 2 * sampling_locations - 1\n     sampling_value_list = []\n     for level_id, (height, width) in enumerate(value_spatial_shapes):\n@@ -672,6 +675,7 @@ def forward(\n         position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         output_attentions: bool = False,\n     ):\n@@ -681,7 +685,8 @@ def forward(\n \n         batch_size, num_queries, _ = hidden_states.shape\n         batch_size, sequence_length, _ = encoder_hidden_states.shape\n-        if (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() != sequence_length:\n+        total_elements = sum(height * width for height, width in spatial_shapes_list)\n+        if total_elements != sequence_length:\n             raise ValueError(\n                 \"Make sure to align the spatial shapes with the sequence length of the encoder hidden states\"\n             )\n@@ -716,9 +721,11 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        if self.disable_custom_kernels:\n+        if self.disable_custom_kernels or MultiScaleDeformableAttention is None:\n             # PyTorch implementation\n-            output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n+            output = multi_scale_deformable_attention(\n+                value, spatial_shapes_list, sampling_locations, attention_weights\n+            )\n         else:\n             try:\n                 # custom kernel\n@@ -732,7 +739,9 @@ def forward(\n                 )\n             except Exception:\n                 # PyTorch implementation\n-                output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n+                output = multi_scale_deformable_attention(\n+                    value, spatial_shapes_list, sampling_locations, attention_weights\n+                )\n         output = self.output_proj(output)\n \n         return output, attention_weights\n@@ -877,6 +886,7 @@ def forward(\n         position_embeddings: torch.Tensor = None,\n         reference_points=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         output_attentions: bool = False,\n     ):\n@@ -909,6 +919,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             reference_points=reference_points,\n             spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n             level_start_index=level_start_index,\n             output_attentions=output_attentions,\n         )\n@@ -974,6 +985,7 @@ def forward(\n         position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -1025,6 +1037,7 @@ def forward(\n             position_embeddings=position_embeddings,\n             reference_points=reference_points,\n             spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n             level_start_index=level_start_index,\n             output_attentions=output_attentions,\n         )\n@@ -1216,6 +1229,7 @@ def forward(\n         attention_mask=None,\n         position_embeddings=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         valid_ratios=None,\n         output_attentions=None,\n@@ -1257,7 +1271,8 @@ def forward(\n         hidden_states = inputs_embeds\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        reference_points = self.get_reference_points(spatial_shapes, valid_ratios, device=inputs_embeds.device)\n+        spatial_shapes_tuple = tuple(spatial_shapes_list)\n+        reference_points = self.get_reference_points(spatial_shapes_tuple, valid_ratios, device=inputs_embeds.device)\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -1272,6 +1287,7 @@ def forward(\n                     position_embeddings,\n                     reference_points,\n                     spatial_shapes,\n+                    spatial_shapes_list,\n                     level_start_index,\n                     output_attentions,\n                 )\n@@ -1282,6 +1298,7 @@ def forward(\n                     position_embeddings=position_embeddings,\n                     reference_points=reference_points,\n                     spatial_shapes=spatial_shapes,\n+                    spatial_shapes_list=spatial_shapes_list,\n                     level_start_index=level_start_index,\n                     output_attentions=output_attentions,\n                 )\n@@ -1338,6 +1355,7 @@ def forward(\n         position_embeddings=None,\n         reference_points=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         valid_ratios=None,\n         output_attentions=None,\n@@ -1413,6 +1431,7 @@ def forward(\n                     position_embeddings,\n                     reference_points_input,\n                     spatial_shapes,\n+                    spatial_shapes_list,\n                     level_start_index,\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n@@ -1425,6 +1444,7 @@ def forward(\n                     encoder_hidden_states=encoder_hidden_states,\n                     reference_points=reference_points_input,\n                     spatial_shapes=spatial_shapes,\n+                    spatial_shapes_list=spatial_shapes_list,\n                     level_start_index=level_start_index,\n                     encoder_attention_mask=encoder_attention_mask,\n                     output_attentions=output_attentions,\n@@ -1586,7 +1606,7 @@ def get_proposal_pos_embed(self, proposals):\n         temperature = 10000\n         scale = 2 * math.pi\n \n-        dim_t = torch.arange(num_pos_feats, dtype=torch.int64, device=proposals.device).float()\n+        dim_t = torch.arange(num_pos_feats, dtype=proposals.dtype, device=proposals.device)\n         dim_t = temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / num_pos_feats)\n         # batch_size, num_queries, 4\n         proposals = proposals.sigmoid() * scale\n@@ -1717,7 +1737,9 @@ def forward(\n                     source = self.input_proj[level](features[-1][0])\n                 else:\n                     source = self.input_proj[level](sources[-1])\n-                mask = nn.functional.interpolate(pixel_mask[None].float(), size=source.shape[-2:]).to(torch.bool)[0]\n+                mask = nn.functional.interpolate(pixel_mask[None].to(pixel_values.dtype), size=source.shape[-2:]).to(\n+                    torch.bool\n+                )[0]\n                 pos_l = self.backbone.position_embedding(source, mask).to(source.dtype)\n                 sources.append(source)\n                 masks.append(mask)\n@@ -1732,11 +1754,11 @@ def forward(\n         source_flatten = []\n         mask_flatten = []\n         lvl_pos_embed_flatten = []\n-        spatial_shapes = []\n+        spatial_shapes_list = []\n         for level, (source, mask, pos_embed) in enumerate(zip(sources, masks, position_embeddings_list)):\n             batch_size, num_channels, height, width = source.shape\n             spatial_shape = (height, width)\n-            spatial_shapes.append(spatial_shape)\n+            spatial_shapes_list.append(spatial_shape)\n             source = source.flatten(2).transpose(1, 2)\n             mask = mask.flatten(1)\n             pos_embed = pos_embed.flatten(2).transpose(1, 2)\n@@ -1747,7 +1769,7 @@ def forward(\n         source_flatten = torch.cat(source_flatten, 1)\n         mask_flatten = torch.cat(mask_flatten, 1)\n         lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)\n-        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=source_flatten.device)\n+        spatial_shapes = torch.as_tensor(spatial_shapes_list, dtype=torch.long, device=source_flatten.device)\n         level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))\n         valid_ratios = torch.stack([self.get_valid_ratio(m, dtype=source_flatten.dtype) for m in masks], 1)\n \n@@ -1759,6 +1781,7 @@ def forward(\n                 attention_mask=mask_flatten,\n                 position_embeddings=lvl_pos_embed_flatten,\n                 spatial_shapes=spatial_shapes,\n+                spatial_shapes_list=spatial_shapes_list,\n                 level_start_index=level_start_index,\n                 valid_ratios=valid_ratios,\n                 output_attentions=output_attentions,\n@@ -1816,6 +1839,7 @@ def forward(\n             encoder_attention_mask=mask_flatten,\n             reference_points=reference_points,\n             spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n             level_start_index=level_start_index,\n             valid_ratios=valid_ratios,\n             output_attentions=output_attentions,"
        },
        {
            "sha": "aaac7488f430f54150877d8f571f43012a3a748b",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee71c9853a2d19b1b35b9f8fc8896feac5b2659c/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee71c9853a2d19b1b35b9f8fc8896feac5b2659c/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=ee71c9853a2d19b1b35b9f8fc8896feac5b2659c",
            "patch": "@@ -583,11 +583,14 @@ def build_position_encoding(config):\n \n # Copied from transformers.models.deformable_detr.modeling_deformable_detr.multi_scale_deformable_attention\n def multi_scale_deformable_attention(\n-    value: Tensor, value_spatial_shapes: Tensor, sampling_locations: Tensor, attention_weights: Tensor\n+    value: Tensor,\n+    value_spatial_shapes: Union[Tensor, List[Tuple]],\n+    sampling_locations: Tensor,\n+    attention_weights: Tensor,\n ) -> Tensor:\n     batch_size, _, num_heads, hidden_dim = value.shape\n     _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-    value_list = value.split([height.item() * width.item() for height, width in value_spatial_shapes], dim=1)\n+    value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n     sampling_grids = 2 * sampling_locations - 1\n     sampling_value_list = []\n     for level_id, (height, width) in enumerate(value_spatial_shapes):\n@@ -676,6 +679,7 @@ def forward(\n         position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         output_attentions: bool = False,\n     ):\n@@ -685,6 +689,7 @@ def forward(\n \n         batch_size, num_queries, _ = hidden_states.shape\n         batch_size, sequence_length, _ = encoder_hidden_states.shape\n+        # Ignore copy\n         if (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() != sequence_length:\n             raise ValueError(\n                 \"Make sure to align the spatial shapes with the sequence length of the encoder hidden states\"\n@@ -720,7 +725,7 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        if self.disable_custom_kernels:\n+        if self.disable_custom_kernels or MultiScaleDeformableAttention is None:\n             # PyTorch implementation\n             output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n         else:"
        },
        {
            "sha": "6b94caf355d994b4cb59d0963fcd50b836a206d2",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee71c9853a2d19b1b35b9f8fc8896feac5b2659c/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee71c9853a2d19b1b35b9f8fc8896feac5b2659c/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=ee71c9853a2d19b1b35b9f8fc8896feac5b2659c",
            "patch": "@@ -17,7 +17,7 @@\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import Dict, List, Optional, Tuple\n+from typing import Dict, List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -800,11 +800,14 @@ def get_num_masks(self, class_labels: torch.Tensor, device: torch.device) -> tor\n \n # Copied from transformers.models.deformable_detr.modeling_deformable_detr.multi_scale_deformable_attention\n def multi_scale_deformable_attention(\n-    value: Tensor, value_spatial_shapes: Tensor, sampling_locations: Tensor, attention_weights: Tensor\n+    value: Tensor,\n+    value_spatial_shapes: Union[Tensor, List[Tuple]],\n+    sampling_locations: Tensor,\n+    attention_weights: Tensor,\n ) -> Tensor:\n     batch_size, _, num_heads, hidden_dim = value.shape\n     _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-    value_list = value.split([height.item() * width.item() for height, width in value_spatial_shapes], dim=1)\n+    value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n     sampling_grids = 2 * sampling_locations - 1\n     sampling_value_list = []\n     for level_id, (height, width) in enumerate(value_spatial_shapes):"
        },
        {
            "sha": "aeeccb68a92fb784b5c03ac5f1a25202b4d1b0fe",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee71c9853a2d19b1b35b9f8fc8896feac5b2659c/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee71c9853a2d19b1b35b9f8fc8896feac5b2659c/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=ee71c9853a2d19b1b35b9f8fc8896feac5b2659c",
            "patch": "@@ -18,7 +18,7 @@\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import Dict, List, Optional, Tuple\n+from typing import Dict, List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -63,11 +63,14 @@ def _get_clones(module, N):\n \n # Copied from transformers.models.deformable_detr.modeling_deformable_detr.multi_scale_deformable_attention\n def multi_scale_deformable_attention(\n-    value: Tensor, value_spatial_shapes: Tensor, sampling_locations: Tensor, attention_weights: Tensor\n+    value: Tensor,\n+    value_spatial_shapes: Union[Tensor, List[Tuple]],\n+    sampling_locations: Tensor,\n+    attention_weights: Tensor,\n ) -> Tensor:\n     batch_size, _, num_heads, hidden_dim = value.shape\n     _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-    value_list = value.split([height.item() * width.item() for height, width in value_spatial_shapes], dim=1)\n+    value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n     sampling_grids = 2 * sampling_locations - 1\n     sampling_value_list = []\n     for level_id, (height, width) in enumerate(value_spatial_shapes):"
        },
        {
            "sha": "c4daba6d2747ea7fc728eeff6654d177ee6a4d22",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee71c9853a2d19b1b35b9f8fc8896feac5b2659c/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee71c9853a2d19b1b35b9f8fc8896feac5b2659c/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=ee71c9853a2d19b1b35b9f8fc8896feac5b2659c",
            "patch": "@@ -733,13 +733,14 @@ def forward(self, hidden_state):\n \n # Copied from transformers.models.deformable_detr.modeling_deformable_detr.multi_scale_deformable_attention\n def multi_scale_deformable_attention(\n-    value: Tensor, value_spatial_shapes: Tensor, sampling_locations: Tensor, attention_weights: Tensor\n+    value: Tensor,\n+    value_spatial_shapes: Union[Tensor, List[Tuple]],\n+    sampling_locations: Tensor,\n+    attention_weights: Tensor,\n ) -> Tensor:\n     batch_size, _, num_heads, hidden_dim = value.shape\n     _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-    # Ignore copy\n     value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n-\n     sampling_grids = 2 * sampling_locations - 1\n     sampling_value_list = []\n     for level_id, (height, width) in enumerate(value_spatial_shapes):\n@@ -838,9 +839,7 @@ def forward(\n \n         batch_size, num_queries, _ = hidden_states.shape\n         batch_size, sequence_length, _ = encoder_hidden_states.shape\n-\n-        # Ignore copy\n-        total_elements = sum(shape[0] * shape[1] for shape in spatial_shapes_list)\n+        total_elements = sum(height * width for height, width in spatial_shapes_list)\n         if total_elements != sequence_length:\n             raise ValueError(\n                 \"Make sure to align the spatial shapes with the sequence length of the encoder hidden states\"\n@@ -876,7 +875,6 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        # Ignore copy\n         if self.disable_custom_kernels or MultiScaleDeformableAttention is None:\n             # PyTorch implementation\n             output = multi_scale_deformable_attention("
        }
    ],
    "stats": {
        "total": 95,
        "additions": 64,
        "deletions": 31
    }
}