{
    "author": "cyyever",
    "message": "Fix pylint warnings (#41222)\n\n* Remove unused variables\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Remove reimported packages\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix code\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix pylint warnings\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Simplify\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "1c8f206ecc31c3a9aa7e515f668c262d6939867d",
    "files": [
        {
            "sha": "e2895de96e6e342fe4df289bfa1326fc621cd38d",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=1c8f206ecc31c3a9aa7e515f668c262d6939867d",
            "patch": "@@ -770,7 +770,6 @@\n     from .utils import is_torch_npu_available as is_torch_npu_available\n     from .utils import is_torch_xla_available as is_torch_xla_available\n     from .utils import is_torch_xpu_available as is_torch_xpu_available\n-    from .utils import logging as logging\n \n     # bitsandbytes config\n     from .utils.quantization_config import AqlmConfig as AqlmConfig"
        },
        {
            "sha": "3dd33165041291c2c567f562fa0fbc491bc82c2b",
            "filename": "src/transformers/commands/chat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fcommands%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fcommands%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fchat.py?ref=1c8f206ecc31c3a9aa7e515f668c262d6939867d",
            "patch": "@@ -59,9 +59,7 @@\n \n     from transformers import (\n         AutoModelForCausalLM,\n-        AutoTokenizer,\n         BitsAndBytesConfig,\n-        GenerationConfig,\n     )\n \n ALLOWED_KEY_CHARS = set(string.ascii_letters + string.whitespace)\n@@ -533,7 +531,7 @@ def parse_eos_tokens(\n     # -----------------------------------------------------------------------------------------------------------------\n     # Model loading and performance automation methods\n     @staticmethod\n-    def get_quantization_config(model_args: ChatArguments) -> Optional[\"BitsAndBytesConfig\"]:\n+    def get_quantization_config(model_args: ChatArguments) -> Optional[BitsAndBytesConfig]:\n         if model_args.load_in_4bit:\n             quantization_config = BitsAndBytesConfig(\n                 load_in_4bit=True,"
        },
        {
            "sha": "0ffc025b65a0451523004df12f5a4ae5e9d17b9a",
            "filename": "src/transformers/data/metrics/squad_metrics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fdata%2Fmetrics%2Fsquad_metrics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fdata%2Fmetrics%2Fsquad_metrics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fmetrics%2Fsquad_metrics.py?ref=1c8f206ecc31c3a9aa7e515f668c262d6939867d",
            "patch": "@@ -148,7 +148,7 @@ def find_best_thresh_v2(preds, scores, na_probs, qid_to_has_ans):\n     best_score = cur_score\n     best_thresh = 0.0\n     qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n-    for i, qid in enumerate(qid_list):\n+    for qid in qid_list:\n         if qid not in scores:\n             continue\n         if qid_to_has_ans[qid]:"
        },
        {
            "sha": "7d81501a783d1d90b734f1cd7612a65c8cbb76ef",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=1c8f206ecc31c3a9aa7e515f668c262d6939867d",
            "patch": "@@ -369,7 +369,6 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n \n         if scores.dim() == 3:\n             if self.logits_indices is not None and self.cu_seq_lens_q is not None:\n-                batch_size, seq_len, vocab_size = scores.shape\n                 last_positions = self.logits_indices\n                 last_scores = scores[0, last_positions, :]\n "
        },
        {
            "sha": "df8a6ef7d483af1c5134d3e35f91dd8531b319f7",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=1c8f206ecc31c3a9aa7e515f668c262d6939867d",
            "patch": "@@ -24,14 +24,9 @@\n from torch.nn import BCELoss\n \n from ..modeling_utils import PreTrainedModel\n-from ..utils import ModelOutput, is_torch_available, logging\n+from ..utils import ModelOutput, logging\n from .configuration_utils import PretrainedConfig, WatermarkingConfig\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-    from .logits_process import SynthIDTextWatermarkLogitsProcessor, WatermarkLogitsProcessor\n+from .logits_process import SynthIDTextWatermarkLogitsProcessor, WatermarkLogitsProcessor\n \n \n logger = logging.get_logger(__name__)"
        },
        {
            "sha": "e746ed60a7e40242b0c2f62c885687af3908a04f",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=1c8f206ecc31c3a9aa7e515f668c262d6939867d",
            "patch": "@@ -1009,7 +1009,7 @@ def add_tensor_parallel_hooks_to_module(\n \n \n def shard_and_distribute_module(\n-    model, param, empty_param, parameter_name, param_casting_dtype, is_contiguous, rank, device_mesh, set_param=True\n+    model, param, empty_param, parameter_name, param_casting_dtype, is_contiguous, rank, device_mesh\n ):  # TODO: rename to shard_and_distribute_param\n     r\"\"\"\n     This function is called in `from_pretrained` when loading a model's checkpoints."
        },
        {
            "sha": "71fa6a769b08a671435f8d6cb29cbd948320ab7f",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1c8f206ecc31c3a9aa7e515f668c262d6939867d",
            "patch": "@@ -2065,8 +2065,6 @@ def tp_plan(self, plan: dict[str, str]):\n             if hasattr(self, \"named_parameters\"):\n                 model_param_names = [name for name, _ in self.named_parameters()]\n                 if model_param_names:  # Only validate if model has parameters\n-                    import re\n-\n                     for layer_pattern in plan.keys():\n                         # Convert pattern to regex (replace * with .*)\n                         regex_pattern = layer_pattern.replace(\"*\", r\"\\d+\")"
        },
        {
            "sha": "a2ab8d1726ce737f370c0005764bdfc1064725e3",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=1c8f206ecc31c3a9aa7e515f668c262d6939867d",
            "patch": "@@ -814,7 +814,6 @@ def pipeline(\n \n     # Retrieve the task\n     if task in custom_tasks:\n-        normalized_task = task\n         targeted_task, task_options = clean_custom_task(custom_tasks[task])\n         if pipeline_class is None:\n             if not trust_remote_code:"
        },
        {
            "sha": "345900693a5a392e3e441b028bcbec731856ef96",
            "filename": "src/transformers/pipelines/question_answering.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py?ref=1c8f206ecc31c3a9aa7e515f668c262d6939867d",
            "patch": "@@ -658,7 +658,7 @@ def span_to_answer(self, text: str, start: int, end: int) -> dict[str, Union[str\n         words = []\n         token_idx = char_start_idx = char_end_idx = chars_idx = 0\n \n-        for i, word in enumerate(text.split(\" \")):\n+        for word in text.split(\" \"):\n             token = self.tokenizer.tokenize(word)\n \n             # Append words if they are in the span"
        },
        {
            "sha": "c1275734819dc53ae6e5136af303a149338eaabc",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c8f206ecc31c3a9aa7e515f668c262d6939867d/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=1c8f206ecc31c3a9aa7e515f668c262d6939867d",
            "patch": "@@ -2138,7 +2138,7 @@ def _from_pretrained(\n             if template_file is None:\n                 continue  # I think this should never happen, but just in case\n             template_name = extra_chat_template.removeprefix(\"chat_template_\")\n-            with open(template_file) as chat_template_handle:\n+            with open(template_file, encoding=\"utf8\") as chat_template_handle:\n                 chat_templates[template_name] = chat_template_handle.read()\n         if len(chat_templates) == 1 and \"default\" in chat_templates:\n             init_kwargs[\"chat_template\"] = chat_templates[\"default\"]"
        }
    ],
    "stats": {
        "total": 26,
        "additions": 7,
        "deletions": 19
    }
}