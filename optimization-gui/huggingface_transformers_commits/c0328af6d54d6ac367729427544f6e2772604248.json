{
    "author": "YangKai0616",
    "message": "Fix the FA2 logic in the longcat_flash model (#42549)\n\n* Matching FA2 naming under kernels\n\n* make style\n\n* convert model\n\n* Follow the comments",
    "sha": "c0328af6d54d6ac367729427544f6e2772604248",
    "files": [
        {
            "sha": "e7e541ba70ca939b7bb9d19a70aaad54c21e2608",
            "filename": "src/transformers/models/longcat_flash/modeling_longcat_flash.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0328af6d54d6ac367729427544f6e2772604248/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0328af6d54d6ac367729427544f6e2772604248/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py?ref=c0328af6d54d6ac367729427544f6e2772604248",
            "patch": "@@ -431,7 +431,7 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n+        if \"flash\" in self.config._attn_implementation and self.qk_head_dim != self.v_head_dim:\n             value_states = F.pad(value_states, [0, self.qk_head_dim - self.v_head_dim])\n \n         attention_interface: Callable = eager_attention_forward\n@@ -449,7 +449,7 @@ def forward(\n             **kwargs,\n         )\n \n-        if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n+        if \"flash\" in self.config._attn_implementation and self.qk_head_dim != self.v_head_dim:\n             attn_output = attn_output[:, :, :, : self.v_head_dim]\n \n         attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()"
        },
        {
            "sha": "f07f8b174de9e7e06799f46f8574d2137ec1fcd1",
            "filename": "src/transformers/models/longcat_flash/modular_longcat_flash.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0328af6d54d6ac367729427544f6e2772604248/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0328af6d54d6ac367729427544f6e2772604248/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py?ref=c0328af6d54d6ac367729427544f6e2772604248",
            "patch": "@@ -215,7 +215,7 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n+        if \"flash\" in self.config._attn_implementation and self.qk_head_dim != self.v_head_dim:\n             value_states = F.pad(value_states, [0, self.qk_head_dim - self.v_head_dim])\n \n         attention_interface: Callable = eager_attention_forward\n@@ -233,7 +233,7 @@ def forward(\n             **kwargs,\n         )\n \n-        if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n+        if \"flash\" in self.config._attn_implementation and self.qk_head_dim != self.v_head_dim:\n             attn_output = attn_output[:, :, :, : self.v_head_dim]\n \n         attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}