{
    "author": "tvukovic-amd",
    "message": "guard torch distributed check (#39057)\n\n* guard torch distributed check\n\n* Update src/transformers/pipelines/base.py\n\n---------\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>",
    "sha": "c8764ab9353f7cd822f1184a0e9848cef5c04a6f",
    "files": [
        {
            "sha": "e871942ce92b6805119b4aaf1b88b7b1edda7dce",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8764ab9353f7cd822f1184a0e9848cef5c04a6f/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8764ab9353f7cd822f1184a0e9848cef5c04a6f/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=c8764ab9353f7cd822f1184a0e9848cef5c04a6f",
            "patch": "@@ -1033,7 +1033,7 @@ def __init__(\n         else:\n             self.device = device if device is not None else -1\n \n-        if is_torch_available() and torch.distributed.is_initialized():\n+        if is_torch_available() and torch.distributed.is_available() and torch.distributed.is_initialized():\n             self.device = self.model.device\n         logger.warning(f\"Device set to use {self.device}\")\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}