{
    "author": "MekkCyber",
    "message": "AutoAWQ tests  (#41295)\n\n* initial commit\n\n* fix\n\n* fix multi gpu\n\n* fix expected output\n\n* fix\n\n* latest\n\n* add comment\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "217ff1e4ef6b006d601d0fa834c2bd68c79b66cd",
    "files": [
        {
            "sha": "4181220b44b0039a36b50d1c6c83df24dcc89fa0",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/217ff1e4ef6b006d601d0fa834c2bd68c79b66cd/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/217ff1e4ef6b006d601d0fa834c2bd68c79b66cd/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=217ff1e4ef6b006d601d0fa834c2bd68c79b66cd",
            "patch": "@@ -89,7 +89,7 @@ def pytest_configure(config):\n     config.addinivalue_line(\"markers\", \"torch_compile_test: mark test which tests torch compile functionality\")\n     config.addinivalue_line(\"markers\", \"torch_export_test: mark test which tests torch export functionality\")\n \n-    os.environ['DISABLE_SAFETENSORS_CONVERSION'] = 'true'\n+    os.environ[\"DISABLE_SAFETENSORS_CONVERSION\"] = \"true\"\n \n \n def pytest_collection_modifyitems(items):"
        },
        {
            "sha": "08a30b3dd88c5354cd0e7a49b2cfaf048589c31f",
            "filename": "src/transformers/activations.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/217ff1e4ef6b006d601d0fa834c2bd68c79b66cd/src%2Ftransformers%2Factivations.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/217ff1e4ef6b006d601d0fa834c2bd68c79b66cd/src%2Ftransformers%2Factivations.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Factivations.py?ref=217ff1e4ef6b006d601d0fa834c2bd68c79b66cd",
            "patch": "@@ -51,6 +51,10 @@ def forward(self, input: Tensor) -> Tensor:\n         return self.act(input)\n \n \n+# Added for compatibility with autoawq which is archived now and imports PytorchGELUTanh from activations.py\n+PytorchGELUTanh = GELUTanh\n+\n+\n @use_kernel_forward_from_hub(\"NewGELU\")\n class NewGELUActivation(nn.Module):\n     \"\"\""
        },
        {
            "sha": "2dfa0658549785da603c11e6b7578f6af64f1a78",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 20,
            "deletions": 8,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/217ff1e4ef6b006d601d0fa834c2bd68c79b66cd/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/217ff1e4ef6b006d601d0fa834c2bd68c79b66cd/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=217ff1e4ef6b006d601d0fa834c2bd68c79b66cd",
            "patch": "@@ -109,8 +109,20 @@ class AwqTest(unittest.TestCase):\n \n     input_text = \"Hello my name is\"\n \n-    EXPECTED_OUTPUT = \"Hello my name is Katie and I am a 20 year old student at the University of North Carolina at Chapel Hill. I am a junior and I am majoring in Journalism and minoring in Spanish\"\n-    EXPECTED_OUTPUT_BF16 = \"Hello my name is Katie and I am a 20 year old student at the University of North Carolina at Chapel Hill. I am a junior and I am majoring in Journalism and minoring in Spanish\"\n+    EXPECTED_OUTPUT = set()\n+    EXPECTED_OUTPUT.add(\n+        \"Hello my name is Katie and I am a 20 year old student at the University of North Carolina at Chapel Hill. I am a junior and I am majoring in Journalism and minoring in Spanish\"\n+    )\n+    EXPECTED_OUTPUT.add(\n+        \"Hello my name is Katie and I am a 20 year old student at the University of North Carolina at Chapel Hill. I am a junior and I am majoring in Journalism and minoring in Spanish. I am\"\n+    )\n+    EXPECTED_OUTPUT.add(\n+        \"Hello my name is Katie and I am a 20 year old student at the University of North Carolina at Chapel Hill. I am a junior and I am majoring in Exercise and Sport Science with a\"\n+    )\n+\n+    EXPECTED_OUTPUT_BF16 = [\n+        \"Hello my name is Katie and I am a 20 year old student at the University of North Carolina at Chapel Hill. I am a junior and I am majoring in Journalism and minoring in Spanish\"\n+    ]\n \n     EXPECTED_OUTPUT_EXLLAMA = [\n         \"Hello my name is Katie and I am a 20 year old student from the UK. I am currently studying for a degree in English Literature and History at the University of York. I am a very out\",\n@@ -181,7 +193,7 @@ def test_quantized_model(self):\n         input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n \n         output = self.quantized_model.generate(**input_ids, max_new_tokens=40)\n-        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+        self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n     def test_raise_if_non_quantized(self):\n         model_id = \"facebook/opt-125m\"\n@@ -199,7 +211,7 @@ def test_quantized_model_bf16(self):\n         quantized_model = AutoModelForCausalLM.from_pretrained(self.model_name, dtype=torch.bfloat16).to(torch_device)\n \n         output = quantized_model.generate(**input_ids, max_new_tokens=40)\n-        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT_BF16)\n+        self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT_BF16)\n \n     @require_torch_gpu\n     def test_quantized_model_exllama(self):\n@@ -225,7 +237,7 @@ def test_quantized_model_no_device_map(self):\n         quantized_model = AutoModelForCausalLM.from_pretrained(self.model_name).to(torch_device)\n         output = quantized_model.generate(**input_ids, max_new_tokens=40)\n \n-        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+        self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n     def test_save_pretrained(self):\n         \"\"\"\n@@ -238,7 +250,7 @@ def test_save_pretrained(self):\n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n \n             output = model.generate(**input_ids, max_new_tokens=40)\n-            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+            self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n     @require_torch_multi_accelerator\n     def test_quantized_model_multi_accelerator(self):\n@@ -249,11 +261,11 @@ def test_quantized_model_multi_accelerator(self):\n \n         quantized_model = AutoModelForCausalLM.from_pretrained(self.model_name, device_map=\"auto\")\n \n-        self.assertTrue(set(quantized_model.hf_device_map.values()) == {0, 1})\n+        self.assertTrue(len(set(quantized_model.hf_device_map.values())) >= 2)\n \n         output = quantized_model.generate(**input_ids, max_new_tokens=40)\n \n-        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+        self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n     def test_quantized_model_no_k_proj_quantized(self):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 34,
        "additions": 25,
        "deletions": 9
    }
}