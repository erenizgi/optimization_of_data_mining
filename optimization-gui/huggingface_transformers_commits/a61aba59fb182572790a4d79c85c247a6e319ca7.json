{
    "author": "yonigozlan",
    "message": "Improve BatchFeature: stack list and lists of torch tensors (#42750)\n\n* stack lists of tensors in BatchFeature, improve error messages, add tests\n\n* remove unnecessary stack in fast image processors and video processors\n\n* make style\n\n* fix tests",
    "sha": "a61aba59fb182572790a4d79c85c247a6e319ca7",
    "files": [
        {
            "sha": "f359fd9e8d006a71ce24928097c9f3e68f189831",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 38,
            "deletions": 9,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -67,11 +67,18 @@ class BatchFeature(UserDict):\n         tensor_type (`Union[None, str, TensorType]`, *optional*):\n             You can give a tensor_type here to convert the lists of integers in PyTorch/Numpy Tensors at\n             initialization.\n+        skip_tensor_conversion (`list[str]` or `set[str]`, *optional*):\n+            List or set of keys that should NOT be converted to tensors, even when `tensor_type` is specified.\n     \"\"\"\n \n-    def __init__(self, data: Optional[dict[str, Any]] = None, tensor_type: Union[None, str, TensorType] = None):\n+    def __init__(\n+        self,\n+        data: Optional[dict[str, Any]] = None,\n+        tensor_type: Union[None, str, TensorType] = None,\n+        skip_tensor_conversion: Optional[Union[list[str], set[str]]] = None,\n+    ):\n         super().__init__(data)\n-        self.convert_to_tensors(tensor_type=tensor_type)\n+        self.convert_to_tensors(tensor_type=tensor_type, skip_tensor_conversion=skip_tensor_conversion)\n \n     def __getitem__(self, item: str) -> Any:\n         \"\"\"\n@@ -110,6 +117,14 @@ def _get_is_as_tensor_fns(self, tensor_type: Optional[Union[str, TensorType]] =\n             import torch\n \n             def as_tensor(value):\n+                if torch.is_tensor(value):\n+                    return value\n+\n+                # stack list of tensors if tensor_type is PyTorch (# torch.tensor() does not support list of tensors)\n+                if isinstance(value, (list, tuple)) and len(value) > 0 and torch.is_tensor(value[0]):\n+                    return torch.stack(value)\n+\n+                # convert list of numpy arrays to numpy array (stack) if tensor_type is Numpy\n                 if isinstance(value, (list, tuple)) and len(value) > 0:\n                     if isinstance(value[0], np.ndarray):\n                         value = np.array(value)\n@@ -138,14 +153,20 @@ def as_tensor(value, dtype=None):\n             is_tensor = is_numpy_array\n         return is_tensor, as_tensor\n \n-    def convert_to_tensors(self, tensor_type: Optional[Union[str, TensorType]] = None):\n+    def convert_to_tensors(\n+        self,\n+        tensor_type: Optional[Union[str, TensorType]] = None,\n+        skip_tensor_conversion: Optional[Union[list[str], set[str]]] = None,\n+    ):\n         \"\"\"\n         Convert the inner content to tensors.\n \n         Args:\n             tensor_type (`str` or [`~utils.TensorType`], *optional*):\n                 The type of tensors to use. If `str`, should be one of the values of the enum [`~utils.TensorType`]. If\n                 `None`, no modification is done.\n+            skip_tensor_conversion (`list[str]` or `set[str]`, *optional*):\n+                List or set of keys that should NOT be converted to tensors, even when `tensor_type` is specified.\n         \"\"\"\n         if tensor_type is None:\n             return self\n@@ -154,18 +175,26 @@ def convert_to_tensors(self, tensor_type: Optional[Union[str, TensorType]] = Non\n \n         # Do the tensor conversion in batch\n         for key, value in self.items():\n+            # Skip keys explicitly marked for no conversion\n+            if skip_tensor_conversion and key in skip_tensor_conversion:\n+                continue\n+\n             try:\n                 if not is_tensor(value):\n                     tensor = as_tensor(value)\n-\n                     self[key] = tensor\n-            except:  # noqa E722\n+            except Exception as e:\n                 if key == \"overflowing_values\":\n-                    raise ValueError(\"Unable to create tensor returning overflowing values of different lengths. \")\n+                    raise ValueError(\n+                        f\"Unable to create tensor for '{key}' with overflowing values of different lengths. \"\n+                        f\"Original error: {str(e)}\"\n+                    ) from e\n                 raise ValueError(\n-                    \"Unable to create tensor, you should probably activate padding \"\n-                    \"with 'padding=True' to have batched tensors with the same length.\"\n-                )\n+                    f\"Unable to convert output '{key}' (type: {type(value).__name__}) to tensor: {str(e)}\\n\"\n+                    f\"You can try:\\n\"\n+                    f\"  1. Use padding=True to ensure all outputs have the same shape\\n\"\n+                    f\"  2. Set return_tensors=None to return Python objects instead of tensors\"\n+                ) from e\n \n         return self\n "
        },
        {
            "sha": "e1e6d935aa10de4f2dcd760eedf17fe6b315cf03",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -932,7 +932,6 @@ def _preprocess(\n         if do_pad:\n             processed_images = self.pad(processed_images, pad_size=pad_size, disable_grouping=disable_grouping)\n \n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n     def to_dict(self):"
        },
        {
            "sha": "b739a1fab5799b2172ca6c98a2f2115d86eeb3b3",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -163,7 +163,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "b4ca1caf0d3de2bd0c2e53cd8511cfd7dcf49624",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -251,10 +251,8 @@ def _preprocess(\n             processed_images, processed_masks = self.pad(\n                 processed_images, return_mask=True, disable_grouping=disable_grouping\n             )\n-            processed_masks = torch.stack(processed_masks, dim=0) if return_tensors else processed_masks\n             data[\"pixel_mask\"] = processed_masks\n \n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n         data[\"pixel_values\"] = processed_images\n \n         return BatchFeature(data=data, tensor_type=return_tensors)"
        },
        {
            "sha": "d15809968cf3d87c2f2f8d34247795bc16b8f169",
            "filename": "src/transformers/models/cohere2_vision/image_processing_cohere2_vision_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -263,7 +263,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(\n             data={\"pixel_values\": processed_images, \"num_patches\": num_patches}, tensor_type=return_tensors"
        },
        {
            "sha": "fb122b41f2ca1e3e2b5c5285db3d8a2019e59b65",
            "filename": "src/transformers/models/convnext/image_processing_convnext_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -162,7 +162,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "aab51fdf967937325b4a54685f8425d82a230a1e",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -171,7 +171,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "ef2ee384b73604ca65ffd8ff1984d0faa19e5e04",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -207,9 +207,6 @@ def _preprocess(\n             )\n             high_res_processed_images_grouped[shape] = stacked_high_res_images\n         high_res_processed_images = reorder_images(high_res_processed_images_grouped, grouped_high_res_images_index)\n-        high_res_processed_images = (\n-            torch.stack(high_res_processed_images, dim=0) if return_tensors else high_res_processed_images\n-        )\n \n         resized_images_grouped = {}\n         for shape, stacked_high_res_padded_images in high_res_padded_images.items():\n@@ -233,7 +230,6 @@ def _preprocess(\n             )\n             processed_images_grouped[shape] = stacked_images\n         processed_images = reorder_images(processed_images_grouped, grouped_resized_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(\n             data={\"pixel_values\": processed_images, \"high_res_pixel_values\": high_res_processed_images},"
        },
        {
            "sha": "6224583289773411a9a434a057983da584ea243f",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -888,9 +888,6 @@ def _preprocess(\n             )\n             high_res_processed_images_grouped[shape] = stacked_high_res_images\n         high_res_processed_images = reorder_images(high_res_processed_images_grouped, grouped_high_res_images_index)\n-        high_res_processed_images = (\n-            torch.stack(high_res_processed_images, dim=0) if return_tensors else high_res_processed_images\n-        )\n \n         resized_images_grouped = {}\n         for shape, stacked_high_res_padded_images in high_res_padded_images.items():\n@@ -914,7 +911,6 @@ def _preprocess(\n             )\n             processed_images_grouped[shape] = stacked_images\n         processed_images = reorder_images(processed_images_grouped, grouped_resized_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(\n             data={\"pixel_values\": processed_images, \"high_res_pixel_values\": high_res_processed_images},"
        },
        {
            "sha": "93cf889d43ee5263694264ee4c102a9c872be2c8",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -94,7 +94,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "3f3a2334ab5cb0f2f555d903053de65c23844a6c",
            "filename": "src/transformers/models/dinov3_vit/image_processing_dinov3_vit_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -88,7 +88,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "f27c9491cb59d4e05233c466b753661b980e2f46",
            "filename": "src/transformers/models/donut/image_processing_donut_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -231,7 +231,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "06fd884afaf7385c9fb981d25ea86427b3f22fd1",
            "filename": "src/transformers/models/dpt/image_processing_dpt_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -225,8 +225,7 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n-        return BatchFeature(data={\"pixel_values\": processed_images})\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n     def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n         \"\"\""
        },
        {
            "sha": "ea7a789a536dc2cc4db8208b37f7080aea98f1e3",
            "filename": "src/transformers/models/dpt/modular_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -228,8 +228,7 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n-        return BatchFeature(data={\"pixel_values\": processed_images})\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n     def post_process_depth_estimation(\n         self,"
        },
        {
            "sha": "1584e2a782ad3ec8f24563bd3b8b8376417b65dc",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -153,9 +153,8 @@ def _preprocess(\n         stacked_pairs = [torch.stack(pair, dim=0) for pair in image_pairs]\n \n         # Return in same format as slow processor\n-        image_pairs = torch.stack(stacked_pairs, dim=0) if return_tensors else stacked_pairs\n \n-        return BatchFeature(data={\"pixel_values\": image_pairs})\n+        return BatchFeature(data={\"pixel_values\": stacked_pairs}, tensor_type=return_tensors)\n \n     def post_process_keypoint_matching(\n         self,"
        },
        {
            "sha": "93e1237f061ca402e585fd0a6796c0b4f154c4cc",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -178,7 +178,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "ec85d98f7d39252c0530fbadc5b0f38d5893f877",
            "filename": "src/transformers/models/eomt/image_processing_eomt_fast.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -162,8 +162,7 @@ def _preprocess_image_like_inputs(\n         )\n         ignore_index = kwargs.pop(\"ignore_index\", None)\n         images_kwargs = kwargs.copy()\n-        processed_images, patch_offsets = self._preprocess(images, **images_kwargs)\n-        outputs = BatchFeature({\"pixel_values\": processed_images})\n+        outputs = self._preprocess(images, **images_kwargs)\n \n         if segmentation_maps is not None:\n             processed_segmentation_maps = self._prepare_image_like_inputs(\n@@ -183,9 +182,9 @@ def _preprocess_image_like_inputs(\n                 }\n             )\n \n-            processed_segmentation_maps, _ = self._preprocess(\n+            processed_segmentation_maps = self._preprocess(\n                 images=processed_segmentation_maps, **segmentation_maps_kwargs\n-            )\n+            ).pixel_values\n             processed_segmentation_maps = processed_segmentation_maps.squeeze(1).to(torch.int64)\n             # Convert to list of binary masks and labels\n             mask_labels, class_labels = [], []\n@@ -208,8 +207,8 @@ def _preprocess_image_like_inputs(\n             outputs[\"mask_labels\"] = mask_labels\n             outputs[\"class_labels\"] = class_labels\n \n-        if patch_offsets:\n-            outputs[\"patch_offsets\"] = [torch.tensor(offsets) for offsets in patch_offsets]\n+        if outputs.patch_offsets:\n+            outputs[\"patch_offsets\"] = [torch.tensor(offsets) for offsets in outputs.patch_offsets]\n \n         return outputs\n \n@@ -274,11 +273,13 @@ def _preprocess(\n                 stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n             )\n             processed_images_grouped[shape] = stacked_images\n-        images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n \n-        processed_images = torch.stack(images, dim=0) if return_tensors else images\n-\n-        return processed_images, patch_offsets\n+        return BatchFeature(\n+            data={\"pixel_values\": processed_images, \"patch_offsets\": patch_offsets},\n+            tensor_type=return_tensors,\n+            skip_tensor_conversion=[\"patch_offsets\"],\n+        )\n \n     def merge_image_patches(\n         self,"
        },
        {
            "sha": "8959bf3c2bb856bc7c86bf1f201ea07e293ad299",
            "filename": "src/transformers/models/flava/image_processing_flava_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -306,7 +306,6 @@ def _preprocess_image(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return processed_images\n \n@@ -397,7 +396,6 @@ def _preprocess(\n                 mask_group_max_aspect_ratio=mask_group_max_aspect_ratio,\n             )\n             masks = [mask_generator() for _ in range(len(images))]\n-            masks = torch.stack(masks, dim=0) if return_tensors else masks\n             data[\"bool_masked_pos\"] = masks\n \n         return BatchFeature(data=data, tensor_type=return_tensors)"
        },
        {
            "sha": "c1a2d492c5df144c26181e9abb99caa7b5ed6661",
            "filename": "src/transformers/models/fuyu/image_processing_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -94,7 +94,7 @@ class FuyuBatchFeature(BatchFeature):\n     The outputs dictionary from the processors contains a mix of tensors and lists of tensors.\n     \"\"\"\n \n-    def convert_to_tensors(self, tensor_type: Optional[Union[str, TensorType]] = None):\n+    def convert_to_tensors(self, tensor_type: Optional[Union[str, TensorType]] = None, **kwargs):\n         \"\"\"\n         Convert the inner content to tensors.\n "
        },
        {
            "sha": "a6ad4d9c67a2fa900997df282655555353656dc1",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -231,7 +231,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n         return BatchFeature(\n             data={\"pixel_values\": processed_images, \"num_crops\": num_crops}, tensor_type=return_tensors\n         )"
        },
        {
            "sha": "07f737a37f9662220fac6fc177aa592f3cccf810",
            "filename": "src/transformers/models/glpn/image_processing_glpn_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -107,7 +107,6 @@ def _preprocess(\n             processed_groups[shape] = stacked_images\n \n         processed_images = reorder_images(processed_groups, grouped_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n     def post_process_depth_estimation(self, outputs, target_sizes=None):"
        },
        {
            "sha": "be183ae79415235f1fc7d830aa435653422157b8",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -189,7 +189,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(\n             data={\"pixel_values\": processed_images, \"num_patches\": num_patches}, tensor_type=return_tensors"
        },
        {
            "sha": "44aff7c91245118f090b5bb4805d6b09248b4dec",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -164,12 +164,8 @@ def _preprocess(\n \n             input_ids = reorder_images(input_ids_grouped, grouped_images_index)\n \n-            return BatchFeature(\n-                data={\"input_ids\": torch.stack(input_ids, dim=0) if return_tensors else input_ids},\n-                tensor_type=return_tensors,\n-            )\n+            return BatchFeature(data={\"input_ids\": input_ids}, tensor_type=return_tensors)\n \n-        pixel_values = torch.stack(pixel_values, dim=0) if return_tensors else pixel_values\n         return BatchFeature(data={\"pixel_values\": pixel_values}, tensor_type=return_tensors)\n \n     def to_dict(self):"
        },
        {
            "sha": "ba5933769af97c08a21b003e414c41e3b8fbe8a1",
            "filename": "src/transformers/models/instructblipvideo/video_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -84,7 +84,6 @@ def _preprocess(\n             processed_videos_grouped[shape] = stacked_videos\n \n         processed_videos = reorder_videos(processed_videos_grouped, grouped_videos_index)\n-        processed_videos = torch.stack(processed_videos, dim=0) if return_tensors else processed_videos\n \n         return BatchFeature(data={\"pixel_values\": processed_videos}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "de82e04602dcb9a869faf5344cdb6deaec3a9e5f",
            "filename": "src/transformers/models/internvl/video_processing_internvl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -140,7 +140,6 @@ def _preprocess(\n             processed_videos_grouped[shape] = stacked_videos\n \n         processed_videos = reorder_videos(processed_videos_grouped, grouped_videos_index)\n-        processed_videos = torch.stack(processed_videos, dim=0) if return_tensors else processed_videos\n \n         return BatchFeature(data={\"pixel_values_videos\": processed_videos}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "0a176ca9818e568f25cd46cea554477d263e8932",
            "filename": "src/transformers/models/janus/image_processing_janus_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -180,7 +180,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "3e7b61c1c8187136fab1ca0a99a3c0439fb6053b",
            "filename": "src/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -264,8 +264,8 @@ def _preprocess(\n \n         encoded_outputs = BatchFeature(\n             data={\n-                \"flattened_patches\": torch.stack(flattened_patches, dim=0) if return_tensors else flattened_patches,\n-                \"attention_mask\": torch.stack(attention_masks, dim=0) if return_tensors else attention_masks,\n+                \"flattened_patches\": flattened_patches,\n+                \"attention_mask\": attention_masks,\n                 \"width\": width,\n                 \"height\": height,\n                 \"rows\": rows,"
        },
        {
            "sha": "a74b3f02c1186db83be08d653d9403764f8ad56a",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -101,7 +101,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         data = BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "0553b2b3a1a6a4cfee4fc9c7d884b048def84e77",
            "filename": "src/transformers/models/layoutlmv3/image_processing_layoutlmv3_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -115,7 +115,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         data = BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "2785ac652730f2f9381454db2e77cf4875dfe616",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -174,9 +174,8 @@ def _preprocess(\n         stacked_pairs = [torch.stack(pair, dim=0) for pair in image_pairs]\n \n         # Return in same format as slow processor\n-        image_pairs = torch.stack(stacked_pairs, dim=0) if return_tensors else stacked_pairs\n \n-        return BatchFeature(data={\"pixel_values\": image_pairs})\n+        return BatchFeature(data={\"pixel_values\": stacked_pairs}, tensor_type=return_tensors)\n \n     def post_process_keypoint_matching(\n         self,"
        },
        {
            "sha": "00c58cede8ff986c51b290436a5af1290ff793bd",
            "filename": "src/transformers/models/llama4/image_processing_llama4_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -419,10 +419,9 @@ def _preprocess(\n                 )\n                 grouped_processed_images[shape] = torch.cat([processed_images, global_tiles.unsqueeze(1)], dim=1)\n         processed_images = reorder_images(grouped_processed_images, grouped_images_index)\n-        aspect_ratios_list = reorder_images(grouped_aspect_ratios, grouped_images_index)\n+        aspect_ratios = reorder_images(grouped_aspect_ratios, grouped_images_index)\n \n         processed_images = torch.cat(processed_images, dim=0) if return_tensors else processed_images\n-        aspect_ratios = torch.stack(aspect_ratios_list, dim=0) if return_tensors else aspect_ratios_list\n         return BatchFeature(\n             data={\"pixel_values\": processed_images, \"aspect_ratios\": aspect_ratios}, tensor_type=return_tensors\n         )"
        },
        {
            "sha": "e2f941d7ac49954bfe8b518536cf85c16333bab9",
            "filename": "src/transformers/models/llava/image_processing_llava_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -149,7 +149,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "936d88fba086a3f594e1c38762e4043e8a08d204",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -260,7 +260,6 @@ def _preprocess(\n \n         if do_pad:\n             processed_images = self._pad_for_batching(processed_images)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n         return BatchFeature(\n             data={\"pixel_values\": processed_images, \"image_sizes\": image_sizes}, tensor_type=return_tensors\n         )"
        },
        {
            "sha": "02f108105d002b5bc2c14e795f5f52c25824e79f",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -279,7 +279,6 @@ def _preprocess(\n \n         if do_pad:\n             processed_images = self._pad_for_batching(processed_images)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n         return BatchFeature(\n             data={\"pixel_values\": processed_images, \"image_sizes\": image_sizes, \"batch_num_images\": batch_num_images},\n             tensor_type=return_tensors,"
        },
        {
            "sha": "cb837861f9a1cb2c050b34ade7b77c4098de5291",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -211,7 +211,6 @@ def _preprocess(\n \n         if do_pad:\n             processed_images = self._pad_for_batching(processed_images)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n         return BatchFeature(\n             data={\"pixel_values\": processed_images, \"image_sizes\": image_sizes, \"batch_num_images\": batch_num_images},\n             tensor_type=return_tensors,"
        },
        {
            "sha": "dfd91ffca490b68dec2f3fac26ac884c48f8e499",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -387,10 +387,7 @@ def _preprocess(\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n         processed_pixel_masks = reorder_images(processed_pixel_masks_grouped, grouped_images_index)\n         encoded_inputs = BatchFeature(\n-            data={\n-                \"pixel_values\": torch.stack(processed_images, dim=0) if return_tensors else processed_images,\n-                \"pixel_mask\": torch.stack(processed_pixel_masks, dim=0) if return_tensors else processed_pixel_masks,\n-            },\n+            data={\"pixel_values\": processed_images, \"pixel_mask\": processed_pixel_masks},\n             tensor_type=return_tensors,\n         )\n         if segmentation_maps is not None:"
        },
        {
            "sha": "bbe1dd39857ffa63500845fa345b525713a23e8f",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -391,10 +391,7 @@ def _preprocess(\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n         processed_pixel_masks = reorder_images(processed_pixel_masks_grouped, grouped_images_index)\n         encoded_inputs = BatchFeature(\n-            data={\n-                \"pixel_values\": torch.stack(processed_images, dim=0) if return_tensors else processed_images,\n-                \"pixel_mask\": torch.stack(processed_pixel_masks, dim=0) if return_tensors else processed_pixel_masks,\n-            },\n+            data={\"pixel_values\": processed_images, \"pixel_mask\": processed_pixel_masks},\n             tensor_type=return_tensors,\n         )\n         if segmentation_maps is not None:"
        },
        {
            "sha": "f29035e4442284de7cc188164fa84cd9d248829e",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -180,7 +180,6 @@ def _preprocess(\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n \n         # Stack all processed images if return_tensors is specified\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "5edb9a6dd0158853c72fe9a5e849ff8c15d54f03",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -182,7 +182,6 @@ def _preprocess(\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n \n         # Stack all processed images if return_tensors is specified\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "e5b60b5ffe8e1a732d6fae0675d2e6f093df48e7",
            "filename": "src/transformers/models/nougat/image_processing_nougat_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -290,7 +290,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "a8ec773ddb59d0a4c38f7c4cfee6baac1e005b13",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -213,7 +213,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n         return BatchFeature(data={\"pixel_values\": processed_images, \"grids\": grids}, tensor_type=return_tensors)\n \n "
        },
        {
            "sha": "2fda6f16cbf932ef945534bcfff1d1025ffe0731",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -336,8 +336,6 @@ def _preprocess(\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n \n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n-\n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n "
        },
        {
            "sha": "3d2012e71a6f4cc1339d6fdbf205abe6659d4600",
            "filename": "src/transformers/models/owlv2/modular_owlv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -205,8 +205,6 @@ def _preprocess(\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n \n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n-\n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n "
        },
        {
            "sha": "5f103bf03233f91ec3b88f8c9991957898a749a0",
            "filename": "src/transformers/models/perceiver/image_processing_perceiver_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -113,7 +113,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "8c169a0b6804e3599f7bef703d12ea89f612d2d1",
            "filename": "src/transformers/models/perception_lm/image_processing_perception_lm_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -307,7 +307,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n         processed_images = [p[None] if p.ndim == 3 else p for p in processed_images]  # add tiles dimension if needed\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n "
        },
        {
            "sha": "31c73fadb628ac720c7b6f8645473fc1e060b2c1",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -231,7 +231,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "efdee9e232e696928294cd6c887b26a6616e16f2",
            "filename": "src/transformers/models/sam/image_processing_sam_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -267,7 +267,6 @@ def _preprocess(\n         if do_pad:\n             processed_images = self.pad(processed_images, pad_size=pad_size, disable_grouping=disable_grouping)\n \n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n         return BatchFeature(\n             data={\"pixel_values\": processed_images, \"reshaped_input_sizes\": reshaped_input_sizes},\n             tensor_type=return_tensors,"
        },
        {
            "sha": "ec9a070f23b916c93ec6857b8d11943724445117",
            "filename": "src/transformers/models/segformer/image_processing_segformer_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -168,7 +168,6 @@ def _preprocess(\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n \n         # Stack images into a single tensor if return_tensors is set\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "1fcb7d840f715c4b25110865515224dea599f8f2",
            "filename": "src/transformers/models/segformer/modular_segformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -140,7 +140,6 @@ def _preprocess(\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n \n         # Stack images into a single tensor if return_tensors is set\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "9e08c6a85c41b6849e9394e106f645e32434c9ab",
            "filename": "src/transformers/models/smolvlm/video_processing_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -331,7 +331,6 @@ def _preprocess(\n             processed_videos = reorder_videos(processed_videos_grouped, grouped_videos_index)\n             pixel_attention_mask = reorder_videos(processed_padded_mask_grouped, grouped_videos_index)\n \n-        processed_videos = torch.stack(processed_videos, dim=0) if return_tensors else processed_videos\n         data = {\"pixel_values\": processed_videos}\n \n         if do_pad:"
        },
        {
            "sha": "f751928471cad822010dc276dfec9eaf7c36cf00",
            "filename": "src/transformers/models/superglue/image_processing_superglue_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -161,9 +161,8 @@ def _preprocess(\n         stacked_pairs = [torch.stack(pair, dim=0) for pair in image_pairs]\n \n         # Return in same format as slow processor\n-        image_pairs = torch.stack(stacked_pairs, dim=0) if return_tensors else stacked_pairs\n \n-        return BatchFeature(data={\"pixel_values\": image_pairs})\n+        return BatchFeature(data={\"pixel_values\": stacked_pairs}, tensor_type=return_tensors)\n \n     def post_process_keypoint_matching(\n         self,"
        },
        {
            "sha": "24638c1892ea8358874e914fd76d452722376c79",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -110,8 +110,7 @@ def _preprocess(\n                 stacked_images = self.rescale(stacked_images, rescale_factor)\n             processed_images_grouped[shape] = stacked_images\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n-        return BatchFeature(data={\"pixel_values\": processed_images})\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n     def post_process_keypoint_detection(\n         self, outputs: \"SuperPointKeypointDescriptionOutput\", target_sizes: Union[TensorType, list[tuple]]"
        },
        {
            "sha": "82fe6b71ee389e18869a4806de187164c9f5d739",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -97,7 +97,6 @@ def _preprocess(\n                 stacked_images = self.pad(stacked_images, size_divisor=size_divisor)\n             processed_image_grouped[shape] = stacked_images\n         processed_images = reorder_images(processed_image_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "574aafba4d3ebdaefd80fabce46b94e38af32282",
            "filename": "src/transformers/models/textnet/image_processing_textnet_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -137,7 +137,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "b8719d9e358d9106b4d0c7aea83a429f0556069d",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -152,7 +152,6 @@ def _preprocess(\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "c9dcb959431ae99ffcd1cdb2856a89739d9e66e8",
            "filename": "src/transformers/models/vitpose/image_processing_vitpose_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -156,7 +156,6 @@ def _preprocess(\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n \n         # Stack into batch tensor\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "facc7d744d398ef07a78a0b74ca1f5a0a9364748",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -171,9 +171,7 @@ def _preprocess(\n             if do_normalize:\n                 stacked_images = self.normalize(stacked_images, image_mean, image_std)\n             resized_images_grouped[shape] = stacked_images\n-        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n-\n-        processed_images = torch.stack(resized_images, dim=0) if return_tensors else resized_images\n+        processed_images = reorder_images(resized_images_grouped, grouped_images_index)\n \n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "094e526d5d8bd0eff3be05054f744d55bb121db5",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -442,7 +442,6 @@ def _preprocess(\n             processed_videos_grouped[shape] = stacked_videos\n \n         processed_videos = reorder_videos(processed_videos_grouped, grouped_videos_index)\n-        processed_videos = torch.stack(processed_videos, dim=0) if return_tensors else processed_videos\n \n         return BatchFeature(data={\"pixel_values_videos\": processed_videos}, tensor_type=return_tensors)\n "
        },
        {
            "sha": "d71cb03708954ea5f3691e738c3e0c68315aa0c8",
            "filename": "tests/models/dac/test_feature_extraction_dac.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/tests%2Fmodels%2Fdac%2Ftest_feature_extraction_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/tests%2Fmodels%2Fdac%2Ftest_feature_extraction_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdac%2Ftest_feature_extraction_dac.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -207,7 +207,7 @@ def test_truncation_and_padding(self):\n         # force no pad\n         with self.assertRaisesRegex(\n             ValueError,\n-            \"^Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.$\",\n+            r\"Unable to convert output[\\s\\S]*padding=True\",\n         ):\n             truncated_outputs = feature_extractor(input_audio, padding=False, return_tensors=\"pt\").input_values\n "
        },
        {
            "sha": "990e6ead9c59b557670376c8a462d72c175682f5",
            "filename": "tests/models/dia/test_feature_extraction_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/tests%2Fmodels%2Fdia%2Ftest_feature_extraction_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/tests%2Fmodels%2Fdia%2Ftest_feature_extraction_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_feature_extraction_dia.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -223,7 +223,7 @@ def test_truncation_and_padding(self):\n         # force no pad\n         with self.assertRaisesRegex(\n             ValueError,\n-            \"^Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.$\",\n+            r\"Unable to convert output[\\s\\S]*padding=True\",\n         ):\n             truncated_outputs = feature_extractor(input_audio, padding=False, return_tensors=\"pt\").input_values\n "
        },
        {
            "sha": "c3850dd7135896a02897298d53ddaa4ad7281ac8",
            "filename": "tests/models/encodec/test_feature_extraction_encodec.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/tests%2Fmodels%2Fencodec%2Ftest_feature_extraction_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/tests%2Fmodels%2Fencodec%2Ftest_feature_extraction_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_feature_extraction_encodec.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -221,7 +221,7 @@ def test_truncation_and_padding(self):\n         # force no pad\n         with self.assertRaisesRegex(\n             ValueError,\n-            \"^Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.$\",\n+            r\"Unable to convert output[\\s\\S]*padding=True\",\n         ):\n             truncated_outputs = feature_extractor(input_audio, padding=False, return_tensors=\"pt\").input_values\n \n@@ -232,7 +232,7 @@ def test_truncation_and_padding(self):\n         feature_extractor.chunk_length_s = None\n         with self.assertRaisesRegex(\n             ValueError,\n-            \"^Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.$\",\n+            r\"Unable to convert output[\\s\\S]*padding=True\",\n         ):\n             truncated_outputs = feature_extractor(input_audio, padding=False, return_tensors=\"pt\").input_values\n \n@@ -244,7 +244,7 @@ def test_truncation_and_padding(self):\n         feature_extractor.overlap = None\n         with self.assertRaisesRegex(\n             ValueError,\n-            \"^Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.$\",\n+            r\"Unable to convert output[\\s\\S]*padding=True\",\n         ):\n             truncated_outputs = feature_extractor(input_audio, padding=False, return_tensors=\"pt\").input_values\n "
        },
        {
            "sha": "01e511bc289a397ce8b0c6754b4076e7143aca9e",
            "filename": "tests/utils/test_feature_extraction_utils.py",
            "status": "modified",
            "additions": 138,
            "deletions": 1,
            "changes": 139,
            "blob_url": "https://github.com/huggingface/transformers/blob/a61aba59fb182572790a4d79c85c247a6e319ca7/tests%2Futils%2Ftest_feature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a61aba59fb182572790a4d79c85c247a6e319ca7/tests%2Futils%2Ftest_feature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_feature_extraction_utils.py?ref=a61aba59fb182572790a4d79c85c247a6e319ca7",
            "patch": "@@ -20,19 +20,156 @@\n from pathlib import Path\n \n import httpx\n+import numpy as np\n \n from transformers import AutoFeatureExtractor, Wav2Vec2FeatureExtractor\n-from transformers.testing_utils import TOKEN, TemporaryHubRepo, get_tests_dir, is_staging_test\n+from transformers.feature_extraction_utils import BatchFeature\n+from transformers.testing_utils import TOKEN, TemporaryHubRepo, get_tests_dir, is_staging_test, require_torch\n+from transformers.utils import is_torch_available\n \n \n sys.path.append(str(Path(__file__).parent.parent.parent / \"utils\"))\n \n from test_module.custom_feature_extraction import CustomFeatureExtractor  # noqa E402\n \n \n+if is_torch_available():\n+    import torch\n+\n+\n SAMPLE_FEATURE_EXTRACTION_CONFIG_DIR = get_tests_dir(\"fixtures\")\n \n \n+class BatchFeatureTester(unittest.TestCase):\n+    \"\"\"Tests for the BatchFeature class and tensor conversion.\"\"\"\n+\n+    def test_batch_feature_basic_access_and_no_conversion(self):\n+        \"\"\"Test basic dict/attribute access and no conversion when tensor_type=None.\"\"\"\n+        data = {\"input_values\": [[1, 2, 3], [4, 5, 6]], \"labels\": [0, 1]}\n+        batch = BatchFeature(data)\n+\n+        # Dict-style and attribute-style access\n+        self.assertEqual(batch[\"input_values\"], [[1, 2, 3], [4, 5, 6]])\n+        self.assertEqual(batch.labels, [0, 1])\n+\n+        # No conversion without tensor_type\n+        self.assertIsInstance(batch[\"input_values\"], list)\n+\n+    @require_torch\n+    def test_batch_feature_numpy_conversion(self):\n+        \"\"\"Test conversion to numpy arrays from lists and existing numpy arrays.\"\"\"\n+        # From lists\n+        batch = BatchFeature({\"input_values\": [[1, 2, 3], [4, 5, 6]]}, tensor_type=\"np\")\n+        self.assertIsInstance(batch[\"input_values\"], np.ndarray)\n+        self.assertEqual(batch[\"input_values\"].shape, (2, 3))\n+\n+        # From numpy arrays (should remain numpy)\n+        numpy_data = np.array([[1, 2, 3], [4, 5, 6]])\n+        batch_arrays = BatchFeature({\"input_values\": numpy_data}, tensor_type=\"np\")\n+        np.testing.assert_array_equal(batch_arrays[\"input_values\"], numpy_data)\n+\n+        # From list of numpy arrays with same shape should stack\n+        numpy_data = [np.array([[1, 2, 3], [4, 5, 6]]), np.array([[7, 8, 9], [10, 11, 12]])]\n+        batch_stacked = BatchFeature({\"input_values\": numpy_data}, tensor_type=\"np\")\n+        np.testing.assert_array_equal(\n+            batch_stacked[\"input_values\"], np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n+        )\n+\n+        # from tensor\n+        tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n+        batch_tensor = BatchFeature({\"input_values\": tensor}, tensor_type=\"np\")\n+        np.testing.assert_array_equal(batch_tensor[\"input_values\"], tensor.numpy())\n+\n+        # from list of tensors with same shape should stack\n+        tensors = [torch.tensor([[1, 2, 3], [4, 5, 6]]), torch.tensor([[7, 8, 9], [10, 11, 12]])]\n+        batch_stacked = BatchFeature({\"input_values\": tensors}, tensor_type=\"np\")\n+        self.assertIsInstance(batch_stacked[\"input_values\"], np.ndarray)\n+        np.testing.assert_array_equal(\n+            batch_stacked[\"input_values\"], np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n+        )\n+\n+    @require_torch\n+    def test_batch_feature_pytorch_conversion(self):\n+        \"\"\"Test conversion to PyTorch tensors from various input types.\"\"\"\n+        # From lists\n+        batch = BatchFeature({\"input_values\": [[1, 2, 3], [4, 5, 6]]}, tensor_type=\"pt\")\n+        self.assertIsInstance(batch[\"input_values\"], torch.Tensor)\n+        self.assertEqual(batch[\"input_values\"].shape, (2, 3))\n+\n+        # from tensor (should be returned as-is)\n+        tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n+        batch_tensor = BatchFeature({\"input_values\": tensor}, tensor_type=\"pt\")\n+        torch.testing.assert_close(batch_tensor[\"input_values\"], tensor)\n+\n+        # From numpy arrays\n+        batch_numpy = BatchFeature({\"input_values\": np.array([[1, 2]])}, tensor_type=\"pt\")\n+        self.assertIsInstance(batch_numpy[\"input_values\"], torch.Tensor)\n+\n+        # List of same-shape tensors should stack\n+        tensors = [torch.randn(3, 10, 10) for _ in range(3)]\n+        batch_stacked = BatchFeature({\"pixel_values\": tensors}, tensor_type=\"pt\")\n+        self.assertEqual(batch_stacked[\"pixel_values\"].shape, (3, 3, 10, 10))\n+\n+        # List of same-shape numpy arrays should stack\n+        numpy_arrays = [np.random.randn(3, 10, 10) for _ in range(3)]\n+        batch_stacked = BatchFeature({\"pixel_values\": numpy_arrays}, tensor_type=\"pt\")\n+        self.assertIsInstance(batch_stacked[\"pixel_values\"], torch.Tensor)\n+        self.assertEqual(batch_stacked[\"pixel_values\"].shape, (3, 3, 10, 10))\n+\n+    @require_torch\n+    def test_batch_feature_error_handling(self):\n+        \"\"\"Test clear error messages for common conversion failures.\"\"\"\n+        # Ragged tensors (different shapes)\n+        data_ragged = {\"values\": [torch.randn(3, 224, 224), torch.randn(3, 448, 448)]}\n+        with self.assertRaises(ValueError) as context:\n+            BatchFeature(data_ragged, tensor_type=\"pt\")\n+        error_msg = str(context.exception)\n+        self.assertIn(\"stack expects each tensor to be equal size\", error_msg.lower())\n+        self.assertIn(\"return_tensors=None\", error_msg)\n+\n+        # Ragged numpy arrays (different shapes)\n+        data_ragged = {\"values\": [np.random.randn(3, 224, 224), np.random.randn(3, 448, 448)]}\n+        with self.assertRaises(ValueError) as context:\n+            BatchFeature(data_ragged, tensor_type=\"np\")\n+        error_msg = str(context.exception)\n+        self.assertIn(\"inhomogeneous\", error_msg.lower())\n+        self.assertIn(\"return_tensors=None\", error_msg)\n+\n+        # Unconvertible type (dict)\n+        data_dict = {\"values\": [[1, 2]], \"metadata\": {\"key\": \"val\"}}\n+        with self.assertRaises(ValueError) as context:\n+            BatchFeature(data_dict, tensor_type=\"pt\")\n+        self.assertIn(\"metadata\", str(context.exception))\n+\n+    @require_torch\n+    def test_batch_feature_skip_tensor_conversion(self):\n+        \"\"\"Test skip_tensor_conversion parameter for metadata fields.\"\"\"\n+        import torch\n+\n+        data = {\"pixel_values\": [[1, 2, 3]], \"num_crops\": [1, 2], \"sizes\": [(224, 224)]}\n+        batch = BatchFeature(data, tensor_type=\"pt\", skip_tensor_conversion=[\"num_crops\", \"sizes\"])\n+\n+        # pixel_values should be converted\n+        self.assertIsInstance(batch[\"pixel_values\"], torch.Tensor)\n+        # num_crops and sizes should remain as lists\n+        self.assertIsInstance(batch[\"num_crops\"], list)\n+        self.assertIsInstance(batch[\"sizes\"], list)\n+\n+    @require_torch\n+    def test_batch_feature_convert_to_tensors_method(self):\n+        \"\"\"Test convert_to_tensors method can be called after initialization.\"\"\"\n+        import torch\n+\n+        data = {\"input_values\": [[1, 2, 3]], \"metadata\": [1, 2]}\n+        batch = BatchFeature(data)  # No conversion initially\n+        self.assertIsInstance(batch[\"input_values\"], list)\n+\n+        # Convert with skip parameter\n+        batch.convert_to_tensors(tensor_type=\"pt\", skip_tensor_conversion=[\"metadata\"])\n+        self.assertIsInstance(batch[\"input_values\"], torch.Tensor)\n+        self.assertIsInstance(batch[\"metadata\"], list)\n+\n+\n class FeatureExtractorUtilTester(unittest.TestCase):\n     def test_cached_files_are_used_when_internet_is_down(self):\n         # A mock response for an HTTP head request to emulate server down"
        }
    ],
    "stats": {
        "total": 317,
        "additions": 206,
        "deletions": 111
    }
}