{
    "author": "Sai-Suraj-27",
    "message": "Fixed failing `BioGPT` batch generation test (#43128)\n\n* Fixed failing BioGPT batch generation test\n\n* Fixed failing BioGPT batch generation test\n\n* Comment.",
    "sha": "6d7b10d57fa273e83d2500cc8c3c2f79636cb452",
    "files": [
        {
            "sha": "da5706353c8a6d12156b895c8c0e4825d023256a",
            "filename": "tests/models/biogpt/test_modeling_biogpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d7b10d57fa273e83d2500cc8c3c2f79636cb452/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d7b10d57fa273e83d2500cc8c3c2f79636cb452/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py?ref=6d7b10d57fa273e83d2500cc8c3c2f79636cb452",
            "patch": "@@ -335,9 +335,8 @@ def test_batch_generation(self):\n \n         num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_padded = model.generate(\n-            input_ids=inputs_padded, max_length=model.generation_config.max_length - num_paddings\n-        )\n+        # 20 is the default max_length in the generation config\n+        output_padded = model.generate(input_ids=inputs_padded, max_length=20 - num_paddings)\n \n         batch_out_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n         non_padded_sentence = tokenizer.decode(output_non_padded[0], skip_special_tokens=True)"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 2,
        "deletions": 3
    }
}