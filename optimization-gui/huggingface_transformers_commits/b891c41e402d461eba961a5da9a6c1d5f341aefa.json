{
    "author": "remi-or",
    "message": "[CB] Minor fix in kwargs (#43147)\n\n* [CB] Minor fix in kwargs\n\n* Comment",
    "sha": "b891c41e402d461eba961a5da9a6c1d5f341aefa",
    "files": [
        {
            "sha": "ba3ea62729209c9302ad02e8cc954aa0e9e730d4",
            "filename": "src/transformers/generation/continuous_batching/requests.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b891c41e402d461eba961a5da9a6c1d5f341aefa/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b891c41e402d461eba961a5da9a6c1d5f341aefa/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py?ref=b891c41e402d461eba961a5da9a6c1d5f341aefa",
            "patch": "@@ -242,13 +242,13 @@ def fork(self, new_request_id: str) -> \"RequestState\":\n             generated_tokens=self.generated_tokens[:],\n             allocated_blocks=self.allocated_blocks,\n             position_offset=self.position_offset,\n-            status=self.status,\n+            _status=self.status,\n             max_new_tokens=self.max_new_tokens,\n             eos_token_id=self.eos_token_id,\n             streaming=self.streaming,\n             created_time=t,\n             lifespan=(t, -1),\n-            timestamps=None if self.timestamps is None else self.timestamps[:],\n+            _timestamps=None if self.timestamps is None else self.timestamps[:],\n             error=self.error,\n             record_timestamps=self.record_timestamps,\n         )"
        },
        {
            "sha": "9ef42a28092522bc290060b1deba6f730b17c2d0",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b891c41e402d461eba961a5da9a6c1d5f341aefa/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b891c41e402d461eba961a5da9a6c1d5f341aefa/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=b891c41e402d461eba961a5da9a6c1d5f341aefa",
            "patch": "@@ -504,6 +504,8 @@ def test_block_sharing_with_hybrid_model(self) -> None:\n \n         return self._test_block_sharing(model_id, num_layer_groups, input_msg, expected_generated_tokens)\n \n+    # The test always passes on H100 with torch 2.9, but only passed case 0 on A100 with torch 2.6 and fails on A100\n+    # with torch 2.9. This might be due to a GPU diff, so test might be flaky on the CI which runs on A10.\n     @parameterized.expand([True, False])\n     @require_torch_accelerator\n     def test_num_return_sequences(self, allow_block_sharing: bool) -> None:"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}