{
    "author": "farrosalferro",
    "message": "Add Fast Image Processor for Chameleon (#37140)\n\n* Add Fast Image Processor for Chameleon\n\n* add warning to resize and move blend_rgba to convert_to_rgb\n\n* Remove unrelated files\n\n* Update image_processing_chameleon_fast to use auto_docstring\n\n* fix equivalence test\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "dd7dc4a4a2281c4a3eda1247fc05e34149a55786",
    "files": [
        {
            "sha": "b0265b1b72767bbe114bcb0bd22c88c4e86fae77",
            "filename": "docs/source/en/model_doc/chameleon.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd7dc4a4a2281c4a3eda1247fc05e34149a55786/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd7dc4a4a2281c4a3eda1247fc05e34149a55786/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md?ref=dd7dc4a4a2281c4a3eda1247fc05e34149a55786",
            "patch": "@@ -191,6 +191,11 @@ model = ChameleonForConditionalGeneration.from_pretrained(\n [[autodoc]] ChameleonImageProcessor\n     - preprocess\n \n+## ChameleonImageProcessorFast\n+\n+[[autodoc]] ChameleonImageProcessorFast\n+    - preprocess\n+\n ## ChameleonVQVAE\n \n [[autodoc]] ChameleonVQVAE"
        },
        {
            "sha": "64666456075e6be6cc7ab22d6f5bc5d2311958a6",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd7dc4a4a2281c4a3eda1247fc05e34149a55786/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd7dc4a4a2281c4a3eda1247fc05e34149a55786/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=dd7dc4a4a2281c4a3eda1247fc05e34149a55786",
            "patch": "@@ -63,7 +63,7 @@\n             (\"blip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n             (\"blip-2\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n             (\"bridgetower\", (\"BridgeTowerImageProcessor\", \"BridgeTowerImageProcessorFast\")),\n-            (\"chameleon\", (\"ChameleonImageProcessor\",)),\n+            (\"chameleon\", (\"ChameleonImageProcessor\", \"ChameleonImageProcessorFast\")),\n             (\"chinese_clip\", (\"ChineseCLIPImageProcessor\", \"ChineseCLIPImageProcessorFast\")),\n             (\"clip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"clipseg\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),"
        },
        {
            "sha": "6ad11a90a24bc4e8c9fd744bca6297e5388fd52e",
            "filename": "src/transformers/models/chameleon/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd7dc4a4a2281c4a3eda1247fc05e34149a55786/src%2Ftransformers%2Fmodels%2Fchameleon%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd7dc4a4a2281c4a3eda1247fc05e34149a55786/src%2Ftransformers%2Fmodels%2Fchameleon%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2F__init__.py?ref=dd7dc4a4a2281c4a3eda1247fc05e34149a55786",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_chameleon import *\n     from .image_processing_chameleon import *\n+    from .image_processing_chameleon_fast import *\n     from .modeling_chameleon import *\n     from .processing_chameleon import *\n else:"
        },
        {
            "sha": "dea89a0d16972272122b4f89dcef45ea61dc631e",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon_fast.py",
            "status": "added",
            "additions": 124,
            "deletions": 0,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd7dc4a4a2281c4a3eda1247fc05e34149a55786/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd7dc4a4a2281c4a3eda1247fc05e34149a55786/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon_fast.py?ref=dd7dc4a4a2281c4a3eda1247fc05e34149a55786",
            "patch": "@@ -0,0 +1,124 @@\n+# coding=utf-8\n+# Copyright 2025 Meta Inc. and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Chameleon.\"\"\"\n+\n+import numpy as np\n+\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n+from ...image_utils import ImageInput, PILImageResampling, SizeDict\n+from ...utils import (\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    is_vision_available,\n+    logging,\n+)\n+\n+\n+if is_vision_available():\n+    import PIL\n+if is_torch_available():\n+    import torch\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@auto_docstring\n+class ChameleonImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.LANCZOS\n+    image_mean = [1.0, 1.0, 1.0]\n+    image_std = [1.0, 1.0, 1.0]\n+    size = {\"shortest_edge\": 512}\n+    default_to_square = False\n+    crop_size = {\"height\": 512, \"width\": 512}\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    rescale_factor = 0.0078\n+    do_normalize = True\n+    do_convert_rgb = True\n+\n+    def convert_to_rgb(self, image: ImageInput) -> ImageInput:\n+        \"\"\"\n+        Convert image to RGB by blending the transparency layer if it's in RGBA format.\n+        If image is not `PIL.Image`, it si simply returned without modifications.\n+\n+        Args:\n+            image (`ImageInput`):\n+                Image to convert.\n+        \"\"\"\n+\n+        if not isinstance(image, PIL.Image.Image):\n+            return image\n+        elif image.mode == \"RGB\":\n+            return image\n+\n+        img_rgba = np.array(image.convert(\"RGBA\"))\n+\n+        # If there is no transparency layer, simple convert and return.\n+        if not (img_rgba[:, :, 3] < 255).any():\n+            return image.convert(\"RGB\")\n+\n+        # There is a transparency layer, blend it with a white background.\n+        # Calculate the alpha proportion for blending.\n+        alpha = img_rgba[:, :, 3] / 255.0\n+        img_rgb = (1 - alpha[:, :, np.newaxis]) * 255 + alpha[:, :, np.newaxis] * img_rgba[:, :, :3]\n+        return PIL.Image.fromarray(img_rgb.astype(\"uint8\"), \"RGB\")\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image to `(size[\"height\"], size[\"width\"])`.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            resample (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+\n+        Returns:\n+            `torch.Tensor`: The resized image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        if interpolation == F.InterpolationMode.LANCZOS:\n+            logger.warning_once(\n+                \"You have used fast image processor with LANCZOS resample which not yet supported for torch.Tensor. \"\n+                \"BICUBIC resample will be used as an alternative. Please fall back to slow image processor if you \"\n+                \"want full consistency with the original model.\"\n+            )\n+            interpolation = F.InterpolationMode.BICUBIC\n+\n+        return super().resize(\n+            image=image,\n+            size=size,\n+            interpolation=interpolation,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"ChameleonImageProcessorFast\"]"
        },
        {
            "sha": "78576725f78ae7d8dd9f62bf5fa40e8954b58025",
            "filename": "tests/models/chameleon/test_image_processing_chameleon.py",
            "status": "modified",
            "additions": 92,
            "deletions": 78,
            "changes": 170,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd7dc4a4a2281c4a3eda1247fc05e34149a55786/tests%2Fmodels%2Fchameleon%2Ftest_image_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd7dc4a4a2281c4a3eda1247fc05e34149a55786/tests%2Fmodels%2Fchameleon%2Ftest_image_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_image_processing_chameleon.py?ref=dd7dc4a4a2281c4a3eda1247fc05e34149a55786",
            "patch": "@@ -16,8 +16,9 @@\n \n import numpy as np\n \n+from transformers.image_utils import PILImageResampling\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -30,6 +31,9 @@\n \n     from transformers import ChameleonImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import ChameleonImageProcessorFast\n+\n \n class ChameleonImageProcessingTester:\n     def __init__(\n@@ -48,6 +52,7 @@ def __init__(\n         image_mean=[1.0, 1.0, 1.0],\n         image_std=[1.0, 1.0, 1.0],\n         do_convert_rgb=True,\n+        resample=PILImageResampling.BILINEAR,\n     ):\n         size = size if size is not None else {\"shortest_edge\": 18}\n         crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n@@ -65,6 +70,7 @@ def __init__(\n         self.image_mean = image_mean\n         self.image_std = image_std\n         self.do_convert_rgb = do_convert_rgb\n+        self.resample = resample\n \n     def prepare_image_processor_dict(self):\n         return {\n@@ -76,6 +82,7 @@ def prepare_image_processor_dict(self):\n             \"image_mean\": self.image_mean,\n             \"image_std\": self.image_std,\n             \"do_convert_rgb\": self.do_convert_rgb,\n+            \"resample\": self.resample,\n         }\n \n     # Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTester.expected_output_image_shape\n@@ -99,6 +106,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class ChameleonImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = ChameleonImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = ChameleonImageProcessorFast if is_torchvision_available() else None\n \n     # Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTest.setUp with CLIP->Chameleon\n     def setUp(self):\n@@ -111,94 +119,100 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 18})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 18})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n \n     def test_call_pil(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, Image.Image)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (1, 3, 18, 18)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-\n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (7, 3, 18, 18)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (1, 3, 18, 18)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 3, 18, 18)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n     def test_call_numpy(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random numpy tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, np.ndarray)\n-\n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (1, 3, 18, 18)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-\n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (7, 3, 18, 18)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (1, 3, 18, 18)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 3, 18, 18)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n     def test_call_pytorch(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n \n-        for image in image_inputs:\n-            self.assertIsInstance(image, torch.Tensor)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n \n-        # Test not batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (1, 3, 18, 18)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (1, 3, 18, 18)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (7, 3, 18, 18)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 3, 18, 18)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n     def test_nested_input(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n-\n-        # Test batched as a list of images\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (7, 3, 18, 18)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-\n-        # Test batched as a nested list of images, where each sublist is one batch\n-        image_inputs_nested = [image_inputs[:3], image_inputs[3:]]\n-        encoded_images_nested = image_processing(image_inputs_nested, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (7, 3, 18, 18)\n-        self.assertEqual(tuple(encoded_images_nested.shape), expected_output_image_shape)\n-\n-        # Image processor should return same pixel values, independently of input format\n-        self.assertTrue((encoded_images_nested == encoded_images).all())\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+\n+            # Test batched as a list of images\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 3, 18, 18)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched as a nested list of images, where each sublist is one batch\n+            image_inputs_nested = [image_inputs[:3], image_inputs[3:]]\n+            encoded_images_nested = image_processing(image_inputs_nested, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 3, 18, 18)\n+            self.assertEqual(tuple(encoded_images_nested.shape), expected_output_image_shape)\n+\n+            # Image processor should return same pixel values, independently of input format\n+            self.assertTrue((encoded_images_nested == encoded_images).all())"
        }
    ],
    "stats": {
        "total": 302,
        "additions": 223,
        "deletions": 79
    }
}