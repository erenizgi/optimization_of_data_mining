{
    "author": "alex-jw-brooks",
    "message": "Granite Vision Support (#35579)\n\n* Add multimodal granite support\n\nSigned-off-by: Alex-Brooks <Alex.Brooks@ibm.com>\n\nSupport multiple image feature layres\n\nSigned-off-by: Alex-Brooks <Alex.Brooks@ibm.com>\n\n* Remove failing validation for visual encoders with no cls\n\nSigned-off-by: Alex-Brooks <Alex.Brooks@ibm.com>\n\n* Update llava based models / configs to support list of feature layers\n\nSigned-off-by: Alex-Brooks <Alex.Brooks@ibm.com>\n\n* Add tests for multiple feature layers\n\nSigned-off-by: Alex-Brooks <Alex.Brooks@ibm.com>\n\n* Use conditional instead of except for misaligned feature shapes\n\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\n\n* crop cls from each hidden state\n\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\n\n* Fix formatting\n\nSigned-off-by: Alex-Brooks <Alex.Brooks@ibm.com>\n\n* Support single vision feature int in vipllava\n\nSigned-off-by: Alex-Brooks <Alex.Brooks@ibm.com>\n\n* Fix typo in vision feature selection strategy validation\n\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\n\n* Add tentative integration test for granite vision models\n\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\n\n* Add granite vision docs\n\nReplace multimodal granite refs with granite vision\n\nAdd granite vision / llava next alias\n\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\n\n* Use image url in granitevision example\n\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\n\n---------\n\nSigned-off-by: Alex-Brooks <Alex.Brooks@ibm.com>\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>",
    "sha": "71cc8161b2470f9a3ec433dff345e224eb3277d5",
    "files": [
        {
            "sha": "34aacd0796a3adb9c25d275807d2f0376f410ef4",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -448,6 +448,8 @@\n         title: Granite\n       - local: model_doc/granitemoe\n         title: GraniteMoe\n+      - local: model_doc/granitevision\n+        title: GraniteVision\n       - local: model_doc/helium\n         title: Helium\n       - local: model_doc/herbert"
        },
        {
            "sha": "42f9df2ee31cd2f54364ae0fd9b816e247631140",
            "filename": "docs/source/en/model_doc/granitevision.md",
            "status": "added",
            "additions": 90,
            "deletions": 0,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -0,0 +1,90 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Granite Vision\n+\n+## Overview\n+\n+The Granite Vision model is a variant of [LLaVA-NeXT](llava_next), leveraging a [Granite](granite) language model alongside a [SigLIP](SigLIP) visual encoder. It utilizes multiple concatenated vision hidden states as its image features, similar to [VipLlava](vipllava). It also uses a larger set of image grid pinpoints than the original LlaVa-NeXT models to support additional aspect ratios.\n+\n+Tips:\n+- This model is loaded into Transformers as an instance of LlaVA-Next. The usage and tips from [LLaVA-NeXT](llava_next) apply to this model as well.\n+\n+- You can apply the chat template on the tokenizer / processor in the same way as well. Example chat format:\n+```bash\n+\"<|user|>\\nWhat’s shown in this image?\\n<|assistant|>\\nThis image shows a red stop sign.<|end_of_text|><|user|>\\nDescribe the image in more details.\\n<|assistant|>\\n\"\n+```\n+\n+Sample inference:\n+```python\n+from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n+from PIL import Image\n+import requests\n+\n+# Note: These docs were written prior to the public model release,\n+# and this path is subject to change.\n+# Please see https://huggingface.co/ibm-granite for the current model list.\n+model_path = \"ibm-granite/granite-3.1-2b-instruct-vision\"\n+processor = LlavaNextProcessor.from_pretrained(model_path)\n+\n+model = LlavaNextForConditionalGeneration.from_pretrained(model_path).to(\"cuda\")\n+\n+# prepare image and text prompt, using the appropriate prompt template\n+url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n+\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"url\": url},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ],\n+    },\n+]\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(\"cuda\")\n+\n+\n+# autoregressively complete prompt\n+output = model.generate(**inputs, max_new_tokens=100)\n+\n+print(processor.decode(output[0], skip_special_tokens=True))\n+```\n+\n+This model was contributed by [Alexander Brooks](https://huggingface.co/abrooks9944).\n+\n+## LlavaNextConfig\n+\n+[[autodoc]] LlavaNextConfig\n+\n+## LlavaNextImageProcessor\n+\n+[[autodoc]] LlavaNextImageProcessor\n+    - preprocess\n+\n+## LlavaNextProcessor\n+\n+[[autodoc]] LlavaNextProcessor\n+\n+## LlavaNextForConditionalGeneration\n+\n+[[autodoc]] LlavaNextForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "f4590c81c7d524f155d97cf3a3e09c3dd3be5d3d",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -134,6 +134,7 @@\n         (\"gptsan-japanese\", \"GPTSanJapaneseConfig\"),\n         (\"granite\", \"GraniteConfig\"),\n         (\"granitemoe\", \"GraniteMoeConfig\"),\n+        (\"granitevision\", \"LlavaNextConfig\"),\n         (\"graphormer\", \"GraphormerConfig\"),\n         (\"grounding-dino\", \"GroundingDinoConfig\"),\n         (\"groupvit\", \"GroupViTConfig\"),\n@@ -458,6 +459,7 @@\n         (\"gptsan-japanese\", \"GPTSAN-japanese\"),\n         (\"granite\", \"Granite\"),\n         (\"granitemoe\", \"GraniteMoeMoe\"),\n+        (\"granitevision\", \"LLaVA-NeXT\"),\n         (\"graphormer\", \"Graphormer\"),\n         (\"grounding-dino\", \"Grounding DINO\"),\n         (\"groupvit\", \"GroupViT\"),\n@@ -729,6 +731,7 @@\n         (\"siglip_vision_model\", \"siglip\"),\n         (\"chinese_clip_vision_model\", \"chinese_clip\"),\n         (\"rt_detr_resnet\", \"rt_detr\"),\n+        (\"granitevision\", \"llava_next\"),\n     ]\n )\n "
        },
        {
            "sha": "d2a3e9747b66fa85fbdf50bec9d930a5b1dcdb34",
            "filename": "src/transformers/models/llava/configuration_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -46,8 +46,10 @@ class LlavaConfig(PretrainedConfig):\n         vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Can be one of `\"default\"` or `\"full\"`.\n-        vision_feature_layer (`int`, *optional*, defaults to -2):\n-            The index of the layer to select the vision feature.\n+        vision_feature_layer (`Union[int, List[int]]`, *optional*, defaults to -2):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n         image_seq_length (`int`, *optional*, defaults to 576):\n             Sequence length of one image embedding.\n         multimodal_projector_bias (`bool`, *optional*, defaults to `True`):"
        },
        {
            "sha": "7b405507228242c7443e2862851c6d72b21c980f",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 35,
            "deletions": 14,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -87,8 +87,12 @@ class LlavaCausalLMOutputWithPast(ModelOutput):\n class LlavaMultiModalProjector(nn.Module):\n     def __init__(self, config: LlavaConfig):\n         super().__init__()\n+        # We have hidden_size * the number of vision feature layers\n+        num_feature_layers = 1 if isinstance(config.vision_feature_layer, int) else len(config.vision_feature_layer)\n         self.linear_1 = nn.Linear(\n-            config.vision_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+            config.vision_config.hidden_size * num_feature_layers,\n+            config.text_config.hidden_size,\n+            bias=config.multimodal_projector_bias,\n         )\n         self.act = ACT2FN[config.projector_hidden_act]\n         self.linear_2 = nn.Linear(\n@@ -208,8 +212,10 @@ def _init_weights(self, module):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n-        vision_feature_layer (`int`, *optional*, defaults to -2):\n-            The index of the layer to select the vision feature.\n+        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n         vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Can be one of `\"default\"` or `\"full\"`.\n@@ -270,31 +276,46 @@ def get_decoder(self):\n         return self.language_model.get_decoder()\n \n     def get_image_features(\n-        self, pixel_values: torch.FloatTensor, vision_feature_layer: int, vision_feature_select_strategy: str\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Union[int, List[int]],\n+        vision_feature_select_strategy: str,\n     ):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n                The tensors corresponding to the input images.\n-            vision_feature_layer (`int`):\n-                The index of the layer to select the vision feature.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n             vision_feature_select_strategy (`str`):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n         Returns:\n             image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n         \"\"\"\n+        if vision_feature_select_strategy not in [\"default\", \"full\"]:\n+            raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n+\n+        # this is not memory efficient at all (output_hidden_states=True) will save all the hidden states.\n         image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n-        # this is not memory efficient at all (output_hidden_states=True) will save all the hidden stated.\n-        selected_image_feature = image_outputs.hidden_states[vision_feature_layer]\n-        if vision_feature_select_strategy == \"default\":\n-            selected_image_feature = selected_image_feature[:, 1:]\n-        elif vision_feature_select_strategy == \"full\":\n-            selected_image_feature = selected_image_feature\n+\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_image_feature = image_outputs.hidden_states[vision_feature_layer]\n+            if vision_feature_select_strategy == \"default\":\n+                selected_image_feature = selected_image_feature[:, 1:]\n         else:\n-            raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n+            hs_pool = [image_outputs.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            # For default; crop CLS from each hidden state in the hidden state pool\n+            if vision_feature_select_strategy == \"default\":\n+                hs_pool = [hs[:, 1:] for hs in hs_pool]\n+            selected_image_feature = torch.cat(hs_pool, dim=-1)\n+\n         image_features = self.multi_modal_projector(selected_image_feature)\n         return image_features\n \n@@ -392,7 +413,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[int] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "2610275cedfdb2d15b4c1ac5356cec4a3efb9afe",
            "filename": "src/transformers/models/llava_next/configuration_llava_next.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -46,8 +46,10 @@ class LlavaNextConfig(PretrainedConfig):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n             If `\"full\"`, the full vision features are used.\n-        vision_feature_layer (`int`, *optional*, defaults to -2):\n-            The index of the layer to select the vision feature.\n+        vision_feature_layer (`Union[int, List[int]]`, *optional*, defaults to -2):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n         image_grid_pinpoints (`List`, *optional*, defaults to `[[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]`):\n             A list of possible resolutions to use for processing high resolution images. Each item in the list should be a tuple or list\n             of the form `(height, width)`."
        },
        {
            "sha": "b9387eaab0c03f30db498a5807240b5ac1480680",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 35,
            "deletions": 15,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -195,8 +195,12 @@ class LlavaNextCausalLMOutputWithPast(ModelOutput):\n class LlavaNextMultiModalProjector(nn.Module):\n     def __init__(self, config: LlavaNextConfig):\n         super().__init__()\n+        # We have hidden_size * the number of vision feature layers\n+        num_feature_layers = 1 if isinstance(config.vision_feature_layer, int) else len(config.vision_feature_layer)\n         self.linear_1 = nn.Linear(\n-            config.vision_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+            config.vision_config.hidden_size * num_feature_layers,\n+            config.text_config.hidden_size,\n+            bias=config.multimodal_projector_bias,\n         )\n         self.act = ACT2FN[config.projector_hidden_act]\n         self.linear_2 = nn.Linear(\n@@ -319,8 +323,10 @@ def _init_weights(self, module):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n-        vision_feature_layer (`int`, *optional*, defaults to -2):\n-            The index of the layer to select the vision feature.\n+        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n         vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n@@ -664,18 +670,22 @@ def pack_image_features(self, image_features, image_sizes, vision_feature_select\n                 image_feature = image_feature[1:]\n                 height = width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n \n-                if vision_feature_select_strategy == \"default\":\n-                    expected_num_patches = height * width\n-                elif vision_feature_select_strategy == \"full\":\n-                    expected_num_patches = height * width + 1\n-                if expected_num_patches != base_image_feature.shape[0]:\n-                    raise ValueError(\"The number of patches is not consistent with the image size.\")\n-\n                 num_patch_height, num_patch_width = get_anyres_image_grid_shape(\n                     image_sizes[image_idx],\n                     self.config.image_grid_pinpoints,\n                     self.config.vision_config.image_size,\n                 )\n+\n+                if (\n+                    np.prod(image_feature.shape) % (num_patch_height * num_patch_width * height * width) != 0\n+                    and vision_feature_select_strategy == \"default\"\n+                ):\n+                    logger.warning_once(\n+                        \"Image feature shape does not line up with the provided patch size. \"\n+                        \"You may be using the `default` vision_feature_select_strategy with a\"\n+                        \" visual encoder that does not have CLS.\"\n+                    )\n+\n                 image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1)\n                 image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                 image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n@@ -706,7 +716,7 @@ def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         image_sizes: torch.Tensor,\n-        vision_feature_layer: int,\n+        vision_feature_layer: Union[int, List[int]],\n         vision_feature_select_strategy: str,\n     ):\n         \"\"\"\n@@ -717,8 +727,10 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n                 Actual image size of each images (H, W).\n-            vision_feature_layer (`int`):\n-                The index of the layer to select the vision feature.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n             vision_feature_select_strategy (`str`):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n@@ -744,11 +756,19 @@ def get_image_features(\n             raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n \n         image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-        selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+        else:\n+            hs_pool = [image_features.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            selected_image_feature = torch.cat(hs_pool, dim=-1)\n+\n         if vision_feature_select_strategy == \"default\":\n             selected_image_feature = selected_image_feature[:, 1:]\n         elif vision_feature_select_strategy == \"full\":\n             selected_image_feature = selected_image_feature\n+\n         image_features = self.multi_modal_projector(selected_image_feature)\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n         return image_features\n@@ -765,7 +785,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[int] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "6b85ebb4455e107a5efd8b5055e0007bef8bb496",
            "filename": "src/transformers/models/llava_next_video/configuration_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -50,8 +50,10 @@ class LlavaNextVideoConfig(PretrainedConfig):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n             If `\"full\"`, the full vision features are used.\n-        vision_feature_layer (`int`, *optional*, defaults to -2):\n-            The index of the layer to select the vision feature.\n+        vision_feature_layer (`Union[int, List[int]]`, *optional*, defaults to -2):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n         image_grid_pinpoints (`List`, *optional*, defaults to `[[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]`):\n             A list of possible resolutions to use for processing high resolution images. Each item in the list should be a tuple or list\n             of the form `(height, width)`."
        },
        {
            "sha": "3e288520edba547017ff0559bb8e69927e18b226",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 51,
            "deletions": 19,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -180,8 +180,12 @@ def _init_weights(self, module):\n class LlavaNextVideoMultiModalProjector(nn.Module):\n     def __init__(self, config: LlavaNextVideoConfig):\n         super().__init__()\n+        # We have hidden_size * the number of vision feature layers\n+        num_feature_layers = 1 if isinstance(config.vision_feature_layer, int) else len(config.vision_feature_layer)\n         self.linear_1 = nn.Linear(\n-            config.vision_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+            config.vision_config.hidden_size * num_feature_layers,\n+            config.text_config.hidden_size,\n+            bias=config.multimodal_projector_bias,\n         )\n         self.act = ACT2FN[config.projector_hidden_act]\n         self.linear_2 = nn.Linear(\n@@ -356,8 +360,10 @@ def unpad_image(tensor, original_size):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n-        vision_feature_layer (`int`, *optional*, defaults to -2):\n-            The index of the layer to select the vision feature.\n+        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n         vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n@@ -699,18 +705,22 @@ def pack_image_features(self, image_features, image_sizes, vision_feature_select\n                 image_feature = image_feature[1:]\n                 height = width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n \n-                if vision_feature_select_strategy == \"default\":\n-                    expected_num_patches = height * width\n-                elif vision_feature_select_strategy == \"full\":\n-                    expected_num_patches = height * width + 1\n-                if expected_num_patches != base_image_feature.shape[0]:\n-                    raise ValueError(\"The number of patches is not consistent with the image size.\")\n-\n                 num_patch_height, num_patch_width = get_anyres_image_grid_shape(\n                     image_sizes[image_idx],\n                     self.config.image_grid_pinpoints,\n                     self.config.vision_config.image_size,\n                 )\n+\n+                if (\n+                    np.prod(image_feature.shape) % (num_patch_height * num_patch_width * height * width) != 0\n+                    and vision_feature_select_strategy == \"default\"\n+                ):\n+                    logger.warning_once(\n+                        \"Image feature shape does not line up with the provided patch size. \"\n+                        \"You may be using the `default` vision_feature_select_strategy with a\"\n+                        \" visual encoder that does not have CLS.\"\n+                    )\n+\n                 image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1)\n                 image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                 image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n@@ -741,7 +751,7 @@ def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         image_sizes: torch.Tensor,\n-        vision_feature_layer: int,\n+        vision_feature_layer: Union[int, List[int]],\n         vision_feature_select_strategy: str,\n     ):\n         \"\"\"\n@@ -752,8 +762,10 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n                 Actual image size of each images (H, W).\n-            vision_feature_layer (`int`):\n-                The index of the layer to select the vision feature.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n             vision_feature_select_strategy (`str`):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n@@ -779,7 +791,14 @@ def get_image_features(\n             raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n \n         image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-        selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+        else:\n+            hs_pool = [image_features.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            selected_image_feature = torch.cat(hs_pool, dim=-1)\n+\n         if vision_feature_select_strategy == \"default\":\n             selected_image_feature = selected_image_feature[:, 1:]\n         elif vision_feature_select_strategy == \"full\":\n@@ -801,7 +820,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[int] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1043,16 +1062,21 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n     def get_video_features(\n-        self, pixel_values: torch.FloatTensor, vision_feature_layer: int, vision_feature_select_strategy: str\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Union[int, List[int]],\n+        vision_feature_select_strategy: str,\n     ):\n         \"\"\"\n         Obtains video last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n                The tensors corresponding to the input video.\n-            vision_feature_layer (`int`):\n-                The index of the layer to select the vision feature.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n             vision_feature_select_strategy (`str`):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n@@ -1063,7 +1087,15 @@ def get_video_features(\n         batch_size, frames, channels, height, width = pixel_values.shape\n         pixel_values = pixel_values.reshape(batch_size * frames, channels, height, width)\n         video_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-        selected_video_features = video_features.hidden_states[vision_feature_layer]\n+\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_video_features = video_features.hidden_states[vision_feature_layer]\n+        else:\n+            hs_pool = [video_features.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            selected_video_features = torch.cat(hs_pool, dim=-1)\n+\n         if vision_feature_select_strategy == \"default\":\n             selected_video_features = selected_video_features[:, 1:]\n         elif vision_feature_select_strategy == \"full\":"
        },
        {
            "sha": "77e0f08c7e3ff377297f829af4ba5283c0160147",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 35,
            "deletions": 11,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -64,8 +64,10 @@ class LlavaNextVideoConfig(PretrainedConfig):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n             If `\"full\"`, the full vision features are used.\n-        vision_feature_layer (`int`, *optional*, defaults to -2):\n-            The index of the layer to select the vision feature.\n+        vision_feature_layer (`Union[int, List[int]]`, *optional*, defaults to -2):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n         image_grid_pinpoints (`List`, *optional*, defaults to `[[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]`):\n             A list of possible resolutions to use for processing high resolution images. Each item in the list should be a tuple or list\n             of the form `(height, width)`.\n@@ -237,7 +239,7 @@ def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         image_sizes: torch.Tensor,\n-        vision_feature_layer: int,\n+        vision_feature_layer: Union[int, List[int]],\n         vision_feature_select_strategy: str,\n     ):\n         \"\"\"\n@@ -248,8 +250,10 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n                 Actual image size of each images (H, W).\n-            vision_feature_layer (`int`):\n-                The index of the layer to select the vision feature.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n             vision_feature_select_strategy (`str`):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n@@ -275,7 +279,14 @@ def get_image_features(\n             raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n \n         image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-        selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+        else:\n+            hs_pool = [image_features.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            selected_image_feature = torch.cat(hs_pool, dim=-1)\n+\n         if vision_feature_select_strategy == \"default\":\n             selected_image_feature = selected_image_feature[:, 1:]\n         elif vision_feature_select_strategy == \"full\":\n@@ -285,16 +296,21 @@ def get_image_features(\n         return image_features\n \n     def get_video_features(\n-        self, pixel_values: torch.FloatTensor, vision_feature_layer: int, vision_feature_select_strategy: str\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Union[int, List[int]],\n+        vision_feature_select_strategy: str,\n     ):\n         \"\"\"\n         Obtains video last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n                The tensors corresponding to the input video.\n-            vision_feature_layer (`int`):\n-                The index of the layer to select the vision feature.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n             vision_feature_select_strategy (`str`):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n@@ -305,7 +321,15 @@ def get_video_features(\n         batch_size, frames, channels, height, width = pixel_values.shape\n         pixel_values = pixel_values.reshape(batch_size * frames, channels, height, width)\n         video_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-        selected_video_features = video_features.hidden_states[vision_feature_layer]\n+\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_video_features = video_features.hidden_states[vision_feature_layer]\n+        else:\n+            hs_pool = [video_features.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            selected_video_features = torch.cat(hs_pool, dim=-1)\n+\n         if vision_feature_select_strategy == \"default\":\n             selected_video_features = selected_video_features[:, 1:]\n         elif vision_feature_select_strategy == \"full\":\n@@ -327,7 +351,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[int] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "c3d43d69d76fdb3eb2a8629d630a7ebf640a025e",
            "filename": "src/transformers/models/llava_onevision/configuration_llava_onevision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -49,8 +49,10 @@ class LlavaOnevisionConfig(PretrainedConfig):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n             If `\"full\"`, the full vision features are used.\n-        vision_feature_layer (`int`, *optional*, defaults to -1):\n-            The index of the layer to select the vision feature.\n+        vision_feature_layer (`Union[int, List[int]]`, *optional*, defaults to -1):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n         vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n             Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n         image_grid_pinpoints (`List`, *optional*):"
        },
        {
            "sha": "b75ef9ab0bb3ed5fd39b84e98eeb21cf74ed4240",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 39,
            "deletions": 12,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -202,8 +202,12 @@ class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n class LlavaOnevisionMultiModalProjector(nn.Module):\n     def __init__(self, config: LlavaOnevisionConfig):\n         super().__init__()\n+        # We have hidden_size * the number of vision feature layers\n+        num_feature_layers = 1 if isinstance(config.vision_feature_layer, int) else len(config.vision_feature_layer)\n         self.linear_1 = nn.Linear(\n-            config.vision_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+            config.vision_config.hidden_size * num_feature_layers,\n+            config.text_config.hidden_size,\n+            bias=config.multimodal_projector_bias,\n         )\n         self.act = ACT2FN[config.projector_hidden_act]\n         self.linear_2 = nn.Linear(\n@@ -334,8 +338,10 @@ def _init_weights(self, module):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n-        vision_feature_layer (`int`, *optional*, defaults to -2):\n-            The index of the layer to select the vision feature.\n+        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n         vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n@@ -488,7 +494,7 @@ def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         image_sizes: torch.Tensor,\n-        vision_feature_layer: int,\n+        vision_feature_layer: Union[int, List[int]],\n         vision_feature_select_strategy: str,\n     ):\n         \"\"\"\n@@ -499,8 +505,10 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n                 Actual image size of each images (H, W).\n-            vision_feature_layer (`int`):\n-                The index of the layer to select the vision feature.\n+            vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n             vision_feature_select_strategy (`str`):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n@@ -526,7 +534,14 @@ def get_image_features(\n             raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n \n         image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-        selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+        else:\n+            hs_pool = [image_features.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            selected_image_feature = torch.cat(hs_pool, dim=-1)\n+\n         if vision_feature_select_strategy == \"default\":\n             selected_image_feature = selected_image_feature[:, 1:]\n         elif vision_feature_select_strategy == \"full\":\n@@ -536,16 +551,21 @@ def get_image_features(\n         return image_features\n \n     def get_video_features(\n-        self, pixel_values: torch.FloatTensor, vision_feature_layer: int, vision_feature_select_strategy: str\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Union[int, List[int]],\n+        vision_feature_select_strategy: str,\n     ):\n         \"\"\"\n         Obtains video last hidden states from the vision tower, apply multimodal projection and pooling.\n \n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n                The tensors corresponding to the input video.\n-            vision_feature_layer (`int`):\n-                The index of the layer to select the vision feature.\n+            vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n             vision_feature_select_strategy (`str`):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n@@ -556,7 +576,14 @@ def get_video_features(\n         batch_size, frames, channels, height, width = pixel_values.shape\n         pixel_values = pixel_values.view(batch_size * frames, channels, height, width)\n         video_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-        selected_video_feature = video_features.hidden_states[vision_feature_layer]\n+\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_video_feature = video_features.hidden_states[vision_feature_layer]\n+        else:\n+            hs_pool = [video_features.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            selected_video_feature = torch.cat(hs_pool, dim=-1)\n \n         if vision_feature_select_strategy == \"default\":\n             selected_video_feature = selected_video_feature[:, 1:]\n@@ -582,7 +609,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[int] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         vision_aspect_ratio: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "becd2004033247658cf457a5c8e0c34daa33815e",
            "filename": "src/transformers/models/video_llava/configuration_video_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -49,8 +49,10 @@ class VideoLlavaConfig(PretrainedConfig):\n         vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n             The feature selection strategy used to select the vision feature from the CLIP backbone.\n             Can be either \"full\" to select all features or \"default\" to select features without `CLS`.\n-        vision_feature_layer (`int`, *optional*, defaults to -2):\n-            The index of the layer to select the vision feature.\n+        vision_feature_layer (`Union[int, List[int]]`, *optional*, defaults to -2):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n         image_seq_length (`int`, *optional*, defaults to 256):\n             Sequence length of one image embedding.\n         video_seq_length (`int`, *optional*, defaults to 2056):"
        },
        {
            "sha": "c7dd0a1f933818f16404a767d7fcc58c39b6eed9",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 49,
            "deletions": 17,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -89,8 +89,12 @@ class VideoLlavaCausalLMOutputWithPast(ModelOutput):\n class VideoLlavaMultiModalProjector(nn.Module):\n     def __init__(self, config: VideoLlavaConfig):\n         super().__init__()\n+        # We have hidden_size * the number of vision feature layers\n+        num_feature_layers = 1 if isinstance(config.vision_feature_layer, int) else len(config.vision_feature_layer)\n         self.linear_1 = nn.Linear(\n-            config.vision_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+            config.vision_config.hidden_size * num_feature_layers,\n+            config.text_config.hidden_size,\n+            bias=config.multimodal_projector_bias,\n         )\n         self.act = ACT2FN[config.projector_hidden_act]\n         self.linear_2 = nn.Linear(\n@@ -210,8 +214,10 @@ def _init_weights(self, module):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n-        vision_feature_layer (`int`, *optional*, defaults to -2):\n-            The index of the layer to select the vision feature.\n+        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n         vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Can be one of `\"default\"` or `\"full\"`\n@@ -357,46 +363,64 @@ def _merge_input_ids_with_visual_features(\n         return final_embedding, final_attention_mask, final_labels, position_ids, final_input_ids\n \n     def get_image_features(\n-        self, pixel_values_images: torch.FloatTensor, vision_feature_layer: int, vision_feature_select_strategy: str\n+        self,\n+        pixel_values_images: torch.FloatTensor,\n+        vision_feature_layer: Union[int, List[int]],\n+        vision_feature_select_strategy: str,\n     ):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n             pixel_values_images (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n                The tensors corresponding to the input images.\n-            vision_feature_layer (`int`):\n-                The index of the layer to select the vision feature.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n             vision_feature_select_strategy (`str`):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n         Returns:\n             image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n         \"\"\"\n+        if vision_feature_select_strategy not in [\"default\", \"full\"]:\n+            raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n \n         image_outputs = self.image_tower(pixel_values_images, output_hidden_states=True)\n-        image_outputs = image_outputs.hidden_states[vision_feature_layer].squeeze(1)\n \n-        if vision_feature_select_strategy == \"default\":\n-            image_outputs = image_outputs[:, 1:]\n-        elif vision_feature_select_strategy == \"full\":\n-            image_outputs = image_outputs\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            image_outputs = image_outputs.hidden_states[vision_feature_layer]\n+            if vision_feature_select_strategy == \"default\":\n+                image_outputs = image_outputs[:, 1:]\n         else:\n-            raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n+            hs_pool = [image_outputs.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            # For default; crop CLS from each hidden state in the hidden state pool\n+            if vision_feature_select_strategy == \"default\":\n+                hs_pool = [hs[:, 1:] for hs in hs_pool]\n+            image_outputs = torch.cat(hs_pool, dim=-1)\n \n         image_features = self.multi_modal_projector(image_outputs)\n \n         return image_features\n \n-    def get_video_features(self, pixel_values_videos: torch.FloatTensor, vision_feature_layer: int):\n+    def get_video_features(\n+        self,\n+        pixel_values_videos: torch.FloatTensor,\n+        vision_feature_layer: Union[int, List[int]],\n+    ):\n         \"\"\"\n         Obtains video last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n             pixel_values_videos (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n                The tensors corresponding to the input videos.\n-            vision_feature_layer (`int`):\n-                The index of the layer to select the vision feature.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n         Returns:\n             video_features (`torch.Tensor`): Video feature tensor of shape `(num_videos * num_frames, image_length, embed_dim)`).\n             frames (`int`): Number of frames the videos have.\n@@ -405,7 +429,15 @@ def get_video_features(self, pixel_values_videos: torch.FloatTensor, vision_feat\n \n         pixel_values = pixel_values_videos.reshape(batch_size_vid * num_frames, channels, height, width)\n         video_outputs = self.video_tower(pixel_values, output_hidden_states=True)\n-        video_features = video_outputs.hidden_states[vision_feature_layer].squeeze(1)\n+\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            video_features = video_outputs.hidden_states[vision_feature_layer]\n+        else:\n+            hs_pool = [video_outputs.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            video_features = torch.cat(hs_pool, dim=-1)\n+\n         video_features = self.multi_modal_projector(video_features)\n \n         return video_features, num_frames\n@@ -422,7 +454,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[int] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "94d890c4b84efe24f7037de91a7617369b07c358",
            "filename": "src/transformers/models/vipllava/configuration_vipllava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -45,8 +45,8 @@ class VipLlavaConfig(PretrainedConfig):\n             The activation function used by the multimodal projector.\n         projector_layernorm_eps (`float`, *optional*, defaults to 1e-05):\n             The layer norm epsilon of the projector layernorm\n-        vision_feature_layers (`List[int]`, *optional*, defaults to `[-2, -5, -8, -11, 6]`):\n-            The list of layers to select the vision features from.\n+        vision_feature_layers (`Union[int, List[int]]`, *optional*, defaults to `[-2, -5, -8, -11, 6]`):\n+            The vision feature layer, or list of layers to select the vision features from.\n         image_seq_length (`int`, *optional*, defaults to 576):\n             Sequence length of one image embedding.\n "
        },
        {
            "sha": "0fbef3308675dd5934a43fbb00728eee6668bb1c",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -85,12 +85,13 @@ class VipLlavaCausalLMOutputWithPast(ModelOutput):\n class VipLlavaMultiModalProjector(nn.Module):\n     def __init__(self, config: VipLlavaConfig):\n         super().__init__()\n+        num_feature_layers = 1 if isinstance(config.vision_feature_layers, int) else len(config.vision_feature_layers)\n         self.projector_layernorm = nn.LayerNorm(\n-            len(config.vision_feature_layers) * config.vision_config.hidden_size, eps=config.projector_layernorm_eps\n+            num_feature_layers * config.vision_config.hidden_size, eps=config.projector_layernorm_eps\n         )\n \n         self.linear_1 = nn.Linear(\n-            len(config.vision_feature_layers) * config.vision_config.hidden_size,\n+            num_feature_layers * config.vision_config.hidden_size,\n             config.text_config.hidden_size,\n             bias=True,\n         )\n@@ -270,24 +271,29 @@ def get_decoder(self):\n         return self.language_model.get_decoder()\n \n     # Ignore copy\n-    def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_layers: List[int]):\n+    def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_layers: Union[int, List[int]]):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n                The tensors corresponding to the input images.\n-            vision_feature_layers (`List[int]`):\n-                The list og indexes of the layers to select the vision feature.\n+            vision_feature_layers (`Union[int, List[int]]`):\n+                The vision feature layer, or the list of indexes of the layers to select\n+                the vision feature.\n         Returns:\n             image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n         \"\"\"\n         image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n \n-        # For VIP-llava, the image features are computed this way\n-        # We select the features from index 1: for the layers -2, -5, -8, -11 and 6\n-        image_features = [image_outputs.hidden_states[index][:, 1:] for index in vision_feature_layers]\n-        image_features = torch.cat(image_features, dim=-1)\n+        # If multiple feature layers are provided (which is usually the case)\n+        # then the image features are concatenated after the CLS is removed.\n+        if isinstance(vision_feature_layers, int):\n+            image_features = image_outputs.hidden_states[vision_feature_layers][:, 1:]\n+        else:\n+            # Usually, we select the features from index 1: the layers -2, -5, -8, -11 and 6\n+            image_features = [image_outputs.hidden_states[index][:, 1:] for index in vision_feature_layers]\n+            image_features = torch.cat(image_features, dim=-1)\n         image_features = self.multi_modal_projector(image_features)\n         return image_features\n \n@@ -386,7 +392,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layers: Optional[List[int]] = None,\n+        vision_feature_layers: Optional[Union[int, List[int]]] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "0b8ebb9a17a35b4c25e36a3f7bf9aebc19ae7c85",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -17,6 +17,7 @@\n import unittest\n \n import requests\n+from parameterized import parameterized\n \n from transformers import (\n     AutoProcessor,\n@@ -272,6 +273,32 @@ def test_mismatching_num_image_tokens(self):\n             pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n             _ = model(input_ids=input_ids, pixel_values=pixel_values)\n \n+    @parameterized.expand(\n+        [\n+            (-1,),\n+            ([-1],),\n+            ([-1, -2],),\n+        ],\n+    )\n+    def test_vision_feature_layers(self, vision_feature_layer):\n+        \"\"\"\n+        Test that we can use either one vision feature layer, or a list of\n+        vision feature layers.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.vision_feature_layer = vision_feature_layer\n+\n+        num_feature_layers = 1 if isinstance(vision_feature_layer, int) else len(vision_feature_layer)\n+        hidden_size = config.vision_config.hidden_size\n+        expected_features = hidden_size * num_feature_layers\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            # We should have the right number of input features,\n+            # and should be able to run a forward pass without exploding\n+            assert model.multi_modal_projector.linear_1.in_features == expected_features\n+            model(**input_dict)\n+\n     @unittest.skip(\n         reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )"
        },
        {
            "sha": "c797a2b0c4afd291ffd19de5452e8c64e2c5d806",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -18,6 +18,7 @@\n \n import requests\n from huggingface_hub import hf_hub_download\n+from parameterized import parameterized\n \n from transformers import (\n     AutoProcessor,\n@@ -321,6 +322,32 @@ def test_mismatching_num_image_tokens(self):\n             image_sizes = torch.cat([image_sizes, image_sizes], dim=0)\n             _ = model(input_ids=input_ids, pixel_values=pixel_values, image_sizes=image_sizes)\n \n+    @parameterized.expand(\n+        [\n+            (-1,),\n+            ([-1],),\n+            ([-1, -2],),\n+        ],\n+    )\n+    def test_vision_feature_layers(self, vision_feature_layer):\n+        \"\"\"\n+        Test that we can use either one vision feature layer, or a list of\n+        vision feature layers.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.vision_feature_layer = vision_feature_layer\n+\n+        num_feature_layers = 1 if isinstance(vision_feature_layer, int) else len(vision_feature_layer)\n+        hidden_size = config.vision_config.hidden_size\n+        expected_features = hidden_size * num_feature_layers\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            # We should have the right number of input features,\n+            # and should be able to run a forward pass without exploding\n+            assert model.multi_modal_projector.linear_1.in_features == expected_features\n+            model(**input_dict)\n+\n     @unittest.skip(\n         reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n@@ -558,3 +585,25 @@ def test_small_model_integration_test_full_vision_state_selection(self):\n             self.processor.decode(output[0], skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,\n         )\n+\n+    @unittest.skip(reason=\"Granite multimodal [vision] models are not yet released\")\n+    @slow\n+    def test_granite_vision(self):\n+        \"\"\"\n+        Check the expected output of a granite vision model, which leverages\n+        multiple vision feature layers and a visual encoder with no CLS (siglip).\n+        \"\"\"\n+        # TODO @alex-jw-brooks - update the path and enable this test once the 2b model is released\n+        granite_model_path = \"llava-granite-2b\"\n+        model = LlavaNextForConditionalGeneration.from_pretrained(granite_model_path)\n+        self.processor = AutoProcessor.from_pretrained(granite_model_path)\n+        prompt = \"<|user|>\\n<image>\\nWhat is shown in this image?\\n<|assistant|>\\n\"\n+        inputs = self.processor(prompt, self.image, return_tensors=\"pt\").to(model.device)\n+\n+        # verify generation\n+        output = model.generate(**inputs, max_new_tokens=30)\n+        EXPECTED_DECODED_TEXT = \"<|user|>\\n\\nWhat is shown in this image?\\n<|assistant|>\\nThe image depicts a diagram.\"\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )"
        },
        {
            "sha": "576329fcfae838e794f34207ff88eabdeac9527f",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -18,6 +18,7 @@\n \n import numpy as np\n from huggingface_hub import hf_hub_download\n+from parameterized import parameterized\n \n from transformers import (\n     AutoProcessor,\n@@ -338,6 +339,32 @@ def test_mismatching_num_image_tokens(self):\n             image_sizes = torch.cat([image_sizes, image_sizes], dim=0)\n             _ = model(input_ids=input_ids, pixel_values=pixel_values, image_sizes=image_sizes)\n \n+    @parameterized.expand(\n+        [\n+            (-1,),\n+            ([-1],),\n+            ([-1, -2],),\n+        ],\n+    )\n+    def test_vision_feature_layers(self, vision_feature_layer):\n+        \"\"\"\n+        Test that we can use either one vision feature layer, or a list of\n+        vision feature layers.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.vision_feature_layer = vision_feature_layer\n+\n+        num_feature_layers = 1 if isinstance(vision_feature_layer, int) else len(vision_feature_layer)\n+        hidden_size = config.vision_config.hidden_size\n+        expected_features = hidden_size * num_feature_layers\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            # We should have the right number of input features,\n+            # and should be able to run a forward pass without exploding\n+            assert model.multi_modal_projector.linear_1.in_features == expected_features\n+            model(**input_dict)\n+\n     @unittest.skip(\n         reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )"
        },
        {
            "sha": "45cb433d867f9053006f454bf43c7b7310cc0157",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -19,6 +19,7 @@\n import numpy as np\n import requests\n from huggingface_hub import hf_hub_download\n+from parameterized import parameterized\n \n from transformers import (\n     AutoProcessor,\n@@ -292,6 +293,32 @@ def test_inputs_embeds_matches_input_ids(self):\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n             self.assertTrue(torch.allclose(out_embeds, out_ids))\n \n+    @parameterized.expand(\n+        [\n+            (-1,),\n+            ([-1],),\n+            ([-1, -2],),\n+        ],\n+    )\n+    def test_vision_feature_layers(self, vision_feature_layer):\n+        \"\"\"\n+        Test that we can use either one vision feature layer, or a list of\n+        vision feature layers.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.vision_feature_layer = vision_feature_layer\n+\n+        num_feature_layers = 1 if isinstance(vision_feature_layer, int) else len(vision_feature_layer)\n+        hidden_size = config.vision_config.hidden_size\n+        expected_features = hidden_size * num_feature_layers\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            # We should have the right number of input features,\n+            # and should be able to run a forward pass without exploding\n+            assert model.multi_modal_projector.linear_1.in_features == expected_features\n+            model(**input_dict)\n+\n     @unittest.skip(\n         reason=\"This architecure seem to not compute gradients properly when using GC, SiglipVisionModel does not support standalone training\"\n     )"
        },
        {
            "sha": "fa7800bdc47ec47c5445e365718915742841e143",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -19,6 +19,7 @@\n import numpy as np\n import requests\n from huggingface_hub import hf_hub_download\n+from parameterized import parameterized\n \n from transformers import (\n     VideoLlavaConfig,\n@@ -419,6 +420,32 @@ def test_mismatching_num_image_tokens(self):\n             pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n             _ = model(input_ids=input_ids, pixel_values_images=pixel_values)\n \n+    @parameterized.expand(\n+        [\n+            (-1,),\n+            ([-1],),\n+            ([-1, -2],),\n+        ],\n+    )\n+    def test_vision_feature_layers(self, vision_feature_layer):\n+        \"\"\"\n+        Test that we can use either one vision feature layer, or a list of\n+        vision feature layers.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.vision_feature_layer = vision_feature_layer\n+\n+        num_feature_layers = 1 if isinstance(vision_feature_layer, int) else len(vision_feature_layer)\n+        hidden_size = config.vision_config.hidden_size\n+        expected_features = hidden_size * num_feature_layers\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            # We should have the right number of input features,\n+            # and should be able to run a forward pass without exploding\n+            assert model.multi_modal_projector.linear_1.in_features == expected_features\n+            model(**input_dict)\n+\n \n @require_torch\n class VideoLlavaForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "3cbac0ddefa8c575d5820c63cde04506d2c311dc",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/71cc8161b2470f9a3ec433dff345e224eb3277d5/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71cc8161b2470f9a3ec433dff345e224eb3277d5/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=71cc8161b2470f9a3ec433dff345e224eb3277d5",
            "patch": "@@ -17,6 +17,7 @@\n import unittest\n \n import requests\n+from parameterized import parameterized\n \n from transformers import (\n     AutoProcessor,\n@@ -257,6 +258,37 @@ def test_mismatching_num_image_tokens(self):\n             pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n             _ = model(input_ids=input_ids, pixel_values=pixel_values)\n \n+    @parameterized.expand(\n+        [\n+            (-1,),\n+            ([-1],),\n+            ([-1, -2],),\n+        ],\n+    )\n+    def test_vision_feature_layers(self, vision_feature_layers):\n+        \"\"\"\n+        Test that we can use either one vision feature layer, or a list of\n+        vision feature layers.\n+        \"\"\"\n+        # NOTE: vipllava uses vision_feature_layers instead of vision_feature_layer as the\n+        # config key. The reason is that other llava classes supported one vision feature layer\n+        # and added support for a list of layers with granite vision support, while vipllava\n+        # originally supported multiple feature layers, and added support for a single layer for\n+        # for compatibility reasons.\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.vision_feature_layers = vision_feature_layers\n+\n+        num_feature_layers = 1 if isinstance(vision_feature_layers, int) else len(vision_feature_layers)\n+        hidden_size = config.vision_config.hidden_size\n+        expected_features = hidden_size * num_feature_layers\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            # We should have the right number of input features,\n+            # and should be able to run a forward pass without exploding\n+            assert model.multi_modal_projector.linear_1.in_features == expected_features\n+            model(**input_dict)\n+\n     @unittest.skip(\n         reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )"
        }
    ],
    "stats": {
        "total": 676,
        "additions": 566,
        "deletions": 110
    }
}