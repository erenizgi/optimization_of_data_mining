{
    "author": "andyvu923",
    "message": "Updated model card for OLMo2 (#38394)\n\n* Updated OLMo2 model card\n\n* added command line\n\n* Add suggestions\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Added suggestions\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Indented code block as per suggestions\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "3b3ebcec4077f124f2cd0ec3cd5d028dc352a3e5",
    "files": [
        {
            "sha": "1ed21b660f1b4ab555ce4b79c689b586b18aff46",
            "filename": "docs/source/en/model_doc/olmo2.md",
            "status": "modified",
            "additions": 106,
            "deletions": 14,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3ebcec4077f124f2cd0ec3cd5d028dc352a3e5/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3ebcec4077f124f2cd0ec3cd5d028dc352a3e5/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md?ref=3b3ebcec4077f124f2cd0ec3cd5d028dc352a3e5",
            "patch": "@@ -14,27 +14,119 @@ rendered properly in your Markdown viewer.\n \n -->\n \n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n # OLMo2\n+[OLMo2](https://huggingface.co/papers/2501.00656) improves on [OLMo](./olmo) by changing the architecture and training recipes of the original models. This includes excluding all biases to improve training stability, non-parametric layer norm, SwiGLU activation function, rotary positional embeddings, and a modified BPE-based tokenizer that masks personal identifiable information. It is pretrained on [Dolma](https://huggingface.co/datasets/allenai/dolma), a dataset of 3T tokens.\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n+You can find all the original OLMo2 checkpoints under the [OLMo2](https://huggingface.co/collections/allenai/olmo-2-674117b93ab84e98afc72edc) collection.\n+\n+> [!TIP]\n+> Click on the OLMo2 models in the right sidebar for more examples of how to apply OLMo2 to different language tasks.\n+\n+The example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`] and from the command line.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipe = pipeline(\n+    task=\"text-generation\",\n+    model=\"allenai/OLMo-2-0425-1B\",\n+    torch_dtype=torch.float16,\n+    device=0,\n+)\n+    \n+result = pipe(\"Plants create energy through a process known as\")\n+print(result)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"allenai/OLMo-2-0425-1B\"\n+)\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"allenai/OLMo-2-0425-1B\",\n+    torch_dtype=torch.float16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n+\n+output = model.generate(**input_ids, max_length=50, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+echo -e \"Plants create energy through a process known as\" | transformers-cli run --task text-generation --model allenai/OLMo-2-0425-1B --device 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [torchao](../quantization/torchao) to only quantize the weights to 4-bits.\n+```py\n+\n+#pip install torchao\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n+\n+torchao_config = TorchAoConfig(\n+    \"int4_weight_only\",\n+    group_size=128\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"allenai/OLMo-2-0425-1B\"\n+)\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"allenai/OLMo-2-0425-1B\",\n+    quantization_config=torchao_config,\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n+\n+output = model.generate(**input_ids, max_length=50, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n-## Overview\n+```\n \n-The OLMo2 model is the successor of the OLMo model, which was proposed in\n-[OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838).\n \n- The architectural changes from the original OLMo model to this model are:\n+## Notes\n \n-- RMSNorm is used instead of standard layer norm.\n-- Norm is applied to attention queries and keys.\n-- Norm is applied after attention/feedforward layers rather than before.\n+- OLMo2 uses RMSNorm instead of standard layer norm. The RMSNorm is applied to attention queries and keys, and it is applied after the attention and feedforward layers rather than before.\n+- OLMo2 requires Transformers v4.48 or higher.\n+- Load specific intermediate checkpoints by adding the `revision` parameter to [`~PreTrainedModel.from_pretrained`]. \n \n-This model was contributed by [shanearora](https://huggingface.co/shanearora).\n-The original code can be found [here](https://github.com/allenai/OLMo/tree/main/olmo).\n+    ```py\n+    from transformers import AutoModelForCausalLM\n+    \n+    model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-2-0425-1B\", revision=\"stage1-step140000-tokens294B\")\n+    ```\n \n \n ## Olmo2Config"
        }
    ],
    "stats": {
        "total": 120,
        "additions": 106,
        "deletions": 14
    }
}