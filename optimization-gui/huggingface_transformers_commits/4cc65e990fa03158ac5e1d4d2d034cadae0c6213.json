{
    "author": "premmurugan229",
    "message": "Replace default split function with jnp.split() in flax models (#37001)\n\nReplace split with jnp's split function for flax models (#36854)",
    "sha": "4cc65e990fa03158ac5e1d4d2d034cadae0c6213",
    "files": [
        {
            "sha": "df2ebddc7e6275a35524dd72381c3518e34a1b95",
            "filename": "src/transformers/models/albert/modeling_flax_albert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cc65e990fa03158ac5e1d4d2d034cadae0c6213/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cc65e990fa03158ac5e1d4d2d034cadae0c6213/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py?ref=4cc65e990fa03158ac5e1d4d2d034cadae0c6213",
            "patch": "@@ -1087,7 +1087,7 @@ def __call__(\n         hidden_states = outputs[0]\n \n         logits = self.qa_outputs(hidden_states)\n-        start_logits, end_logits = logits.split(self.config.num_labels, axis=-1)\n+        start_logits, end_logits = jnp.split(logits, self.config.num_labels, axis=-1)\n         start_logits = start_logits.squeeze(-1)\n         end_logits = end_logits.squeeze(-1)\n "
        },
        {
            "sha": "7f43a4c5abb8d3eaa6869d39c318d57209e5448c",
            "filename": "src/transformers/models/big_bird/modeling_flax_big_bird.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cc65e990fa03158ac5e1d4d2d034cadae0c6213/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cc65e990fa03158ac5e1d4d2d034cadae0c6213/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py?ref=4cc65e990fa03158ac5e1d4d2d034cadae0c6213",
            "patch": "@@ -2407,7 +2407,7 @@ def __call__(\n             # removing question tokens from the competition\n             logits = logits - logits_mask * 1e6\n \n-        start_logits, end_logits = logits.split(self.config.num_labels, axis=-1)\n+        start_logits, end_logits = jnp.split(logits, self.config.num_labels, axis=-1)\n         start_logits = start_logits.squeeze(-1)\n         end_logits = end_logits.squeeze(-1)\n "
        },
        {
            "sha": "1f2b6ac96ab63590dba11b9535042d05edb58400",
            "filename": "src/transformers/models/distilbert/modeling_flax_distilbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cc65e990fa03158ac5e1d4d2d034cadae0c6213/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cc65e990fa03158ac5e1d4d2d034cadae0c6213/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py?ref=4cc65e990fa03158ac5e1d4d2d034cadae0c6213",
            "patch": "@@ -861,7 +861,7 @@ def __call__(\n \n         hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n         logits = self.qa_outputs(hidden_states)\n-        start_logits, end_logits = logits.split(self.config.num_labels, axis=-1)\n+        start_logits, end_logits = jnp.split(logits, self.config.num_labels, axis=-1)\n         start_logits = start_logits.squeeze(-1)\n         end_logits = end_logits.squeeze(-1)\n "
        },
        {
            "sha": "77a445e6ccaa7ce3294655894be874bc83300a38",
            "filename": "src/transformers/models/electra/modeling_flax_electra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cc65e990fa03158ac5e1d4d2d034cadae0c6213/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cc65e990fa03158ac5e1d4d2d034cadae0c6213/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py?ref=4cc65e990fa03158ac5e1d4d2d034cadae0c6213",
            "patch": "@@ -1364,7 +1364,7 @@ def __call__(\n         )\n         hidden_states = outputs[0]\n         logits = self.qa_outputs(hidden_states)\n-        start_logits, end_logits = logits.split(self.config.num_labels, axis=-1)\n+        start_logits, end_logits = jnp.split(logits, self.config.num_labels, axis=-1)\n         start_logits = start_logits.squeeze(-1)\n         end_logits = end_logits.squeeze(-1)\n "
        },
        {
            "sha": "f47146f16bbd7bbbf466dc6f59e09f93247a85ef",
            "filename": "src/transformers/models/roformer/modeling_flax_roformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cc65e990fa03158ac5e1d4d2d034cadae0c6213/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_flax_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cc65e990fa03158ac5e1d4d2d034cadae0c6213/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_flax_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_flax_roformer.py?ref=4cc65e990fa03158ac5e1d4d2d034cadae0c6213",
            "patch": "@@ -262,7 +262,7 @@ def __call__(\n \n     @staticmethod\n     def apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer, value_layer=None):\n-        sin, cos = sinusoidal_pos.split(2, axis=-1)\n+        sin, cos = jnp.split(sinusoidal_pos, 2, axis=-1)\n         sin_pos = jnp.stack([sin, sin], axis=-1).reshape(sinusoidal_pos.shape)\n         cos_pos = jnp.stack([cos, cos], axis=-1).reshape(sinusoidal_pos.shape)\n \n@@ -1046,7 +1046,7 @@ def __call__(\n         hidden_states = outputs[0]\n \n         logits = self.qa_outputs(hidden_states)\n-        start_logits, end_logits = logits.split(self.config.num_labels, axis=-1)\n+        start_logits, end_logits = jnp.split(logits, self.config.num_labels, axis=-1)\n         start_logits = start_logits.squeeze(-1)\n         end_logits = end_logits.squeeze(-1)\n "
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}