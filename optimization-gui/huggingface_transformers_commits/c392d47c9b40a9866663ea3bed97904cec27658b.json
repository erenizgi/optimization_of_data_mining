{
    "author": "zucchini-nlp",
    "message": "[attention] fix test for packed padfree masking (#39582)\n\n* fix most tests\n\n* skip a few more tests\n\n* address comments\n\n* fix chameleon tests\n\n* forgot to uncomment\n\n* qwen has its own tests with images, rename it as well",
    "sha": "c392d47c9b40a9866663ea3bed97904cec27658b",
    "files": [
        {
            "sha": "7aa2bb6e20eec21f4a1b6a34ab10baaf3b158c9d",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 25,
            "deletions": 63,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -408,69 +408,31 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.input_embeds_layer = new_embeddings\n \n-    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, cache_position=None, **kwargs):\n-        # Overwritten -- bark has a model-specific hack\n-        input_embeds = kwargs.get(\"input_embeds\", None)\n-\n-        attention_mask = kwargs.get(\"attention_mask\", None)\n-        position_ids = kwargs.get(\"position_ids\", None)\n-\n-        if cache_position[0] != 0:\n-            # Omit tokens covered by past_key_values\n-            seq_len = input_ids.shape[1]\n-            past_length = past_key_values.get_seq_length()\n-\n-            # Some generation methods already pass only the last input ID\n-            if input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = input_ids.shape[1] - 1\n-\n-            input_ids = input_ids[:, remove_prefix_length:]\n-\n-            # input_embeds have already been used and is not required anymore\n-            input_embeds = None\n-        else:\n-            if input_embeds is not None and kwargs.get(\"use_cache\"):\n-                seq_len = input_embeds.shape[1]\n-            else:\n-                seq_len = input_ids.shape[1]\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        attention_mask=None,\n+        input_embeds=None,\n+        past_key_values=None,\n+        position_ids=None,\n+        use_cache=None,\n+        cache_position=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- bark uses `input_embeds` not `inputS_embeds`\n \n-        # ensure that attention_mask and position_ids shapes are aligned with the weird Bark hack of reducing\n-        # sequence length on the first forward pass\n-        if attention_mask is not None:\n-            attention_mask = attention_mask[:, :seq_len]\n-        if position_ids is not None:\n-            position_ids = position_ids[:, :seq_len]\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-        else:\n-            position_ids = None\n-\n-        if input_embeds is not None and kwargs.get(\"use_cache\"):\n-            return {\n-                \"input_ids\": None,\n-                \"input_embeds\": input_embeds,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": kwargs.get(\"use_cache\"),\n-                \"position_ids\": position_ids,\n-                \"attention_mask\": attention_mask,\n-                \"cache_position\": cache_position,\n-            }\n-        return {\n-            \"input_ids\": input_ids,\n-            \"past_key_values\": past_key_values,\n-            \"use_cache\": kwargs.get(\"use_cache\"),\n-            \"position_ids\": position_ids,\n-            \"attention_mask\": attention_mask,\n-            \"cache_position\": cache_position,\n-        }\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            inputs_embeds=input_embeds,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        model_inputs[\"input_embeds\"] = model_inputs.pop(\"inputs_embeds\", None)\n+        return model_inputs\n \n     @auto_docstring\n     def forward(\n@@ -546,7 +508,7 @@ def forward(\n             return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n-        past_length = past_key_values.get_seq_length() if past_key_values is not None else past_key_values\n+        past_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n \n         if position_ids is None:\n             position_ids = torch.arange(past_length, seq_length + past_length, dtype=torch.long, device=device)"
        },
        {
            "sha": "b003f9e4e8f5261da079d192fcd9c8634945abd7",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 10,
            "deletions": 144,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -25,7 +25,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -35,19 +35,12 @@\n     TransformersKwargs,\n     auto_docstring,\n     can_return_tuple,\n-    is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n )\n from .configuration_chameleon import ChameleonConfig, ChameleonVQVAEConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -353,15 +346,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -942,7 +928,7 @@ def forward(\n             else:\n                 special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n \n-            n_image_tokens_in_text = (special_image_mask).sum()\n+            n_image_tokens_in_text = special_image_mask.sum()\n             special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n \n             image_embeds = self.get_image_features(pixel_values)\n@@ -966,8 +952,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         # embed positions\n@@ -1015,131 +1006,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "35de309661033f2ece9e50507eb2fd801f50394d",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -270,7 +270,7 @@ def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n         input_ids[input_ids == self.image_token_id] = self.pad_token_id\n         input_ids[:, : self.image_seq_length] = self.image_token_id\n-        attention_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n+        attention_mask = input_ids.ne(self.pad_token_id).to(torch_device)\n         pixel_values = floats_tensor([self.batch_size, 3, self.image_size, self.image_size])\n \n         config = self.get_config()\n@@ -325,6 +325,14 @@ def test_disk_offload_safetensors(self):\n     def test_model_is_small(self):\n         pass\n \n+    @unittest.skip(\"Chameleon applies key/query norm which doesn't work with packing\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Chameleon applies key/query norm which doesn't work with packing\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong"
        },
        {
            "sha": "6c4f718590b692dfcd7be612b23bcf28b7213d6e",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -89,7 +89,7 @@ def __init__(\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-        attention_mask = input_ids.ne(1).to(torch_device)\n+        attention_mask = input_ids.ne(self.pad_token_id).to(torch_device)\n \n         config = self.get_config()\n \n@@ -234,9 +234,9 @@ def prepare_config_and_inputs(self):\n         config = self.get_config()\n \n         input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size)\n-        attention_mask = input_ids.ne(1).to(torch_device)\n         input_ids[input_ids == self.image_token_id] = self.pad_token_id\n         input_ids[:, : self.image_seq_length] = self.image_token_id\n+        attention_mask = input_ids.ne(self.pad_token_id).to(torch_device)\n \n         pixel_values = floats_tensor(\n             ["
        },
        {
            "sha": "6ca7b23af6c5c0eaeb6f75f120becedb0fc861f0",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -214,6 +214,14 @@ def test_model_parallelism(self):\n     def test_generate_continue_from_inputs_embeds():\n         pass\n \n+    @unittest.skip(\"Persimmon backbone applies key/query norm which doesn't work with packing\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Persimmon backbone applies key/query norm which doesn't work with packing\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "65fcf547b42ecc1c65018b7394678afda21893ee",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -143,6 +143,14 @@ def test_eager_matches_fa2_generate(self):\n     def test_multi_gpu_data_parallel_forward(self):\n         pass\n \n+    @unittest.skip(\"Gemma3 applies key/query norm which doesn't work with packing\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma3 applies key/query norm which doesn't work with packing\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n \n class Gemma3Vision2TextModelTester:\n     def __init__("
        },
        {
            "sha": "6bb86406cedfa685bd83454127968d7aeba36f78",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -465,6 +465,14 @@ def test_eager_matches_sdpa_inference(\n     ):\n         pass\n \n+    @unittest.skip(\"KOSMOS-2 doesn't support padding\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"KOSMOS-2 doesn't support padding\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n         # Overwrite because Kosmos-2 need to padd pixel values and pad image-attn-mask"
        },
        {
            "sha": "f8d61fb62bb20fa237bea5263423037903da60b4",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -152,9 +152,10 @@ def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         config, pixel_values = config_and_inputs\n         input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n-        attention_mask = input_ids.ne(1).to(torch_device)\n         input_ids[input_ids == config.image_token_index] = self.pad_token_id\n         input_ids[:, : self.num_image_tokens] = config.image_token_index\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+\n         inputs_dict = {\n             \"pixel_values\": pixel_values,\n             \"input_ids\": input_ids,"
        },
        {
            "sha": "475f9a2d07441e2493fec8d93de9626b0381ab12",
            "filename": "tests/models/minimax/test_modeling_minimax.py",
            "status": "modified",
            "additions": 14,
            "deletions": 6,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -214,34 +214,42 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n             batch_size, seq_length = inputs[\"input_ids\"].shape\n             self._check_past_key_values_for_generate(batch_size, past_kv, seq_length, config)\n \n-    @unittest.skip(reason=\"MiniMaxCache doesnot support `crop()` method\")\n+    @unittest.skip(reason=\"MiniMaxCache does not support `crop()` method\")\n     def test_prompt_lookup_decoding_matches_greedy_search(self):\n         pass\n \n-    @unittest.skip(reason=\"MiniMaxCache doesnot support `crop()` method\")\n+    @unittest.skip(reason=\"MiniMaxCache does not support `crop()` method\")\n     def test_contrastive_generate_low_memory(self):\n         pass\n \n-    @unittest.skip(reason=\"MiniMaxCache doesnot support `crop()` method\")\n+    @unittest.skip(reason=\"MiniMaxCache does not support `crop()` method\")\n     def test_assisted_decoding_sample(self):\n         pass\n \n-    @unittest.skip(reason=\"MiniMaxCache doesnot support `crop()` method\")\n+    @unittest.skip(reason=\"MiniMaxCache does not support `crop()` method\")\n     def test_assisted_decoding_matches_greedy_search_0_random(self):\n         pass\n \n-    @unittest.skip(reason=\"MiniMaxCache doesnot support `crop()` method\")\n+    @unittest.skip(reason=\"MiniMaxCache does not support `crop()` method\")\n     def test_assisted_decoding_matches_greedy_search_1_same(self):\n         pass\n \n-    @unittest.skip(reason=\"MiniMaxCache doesnot support `crop()` method\")\n+    @unittest.skip(reason=\"MiniMaxCache does not support `crop()` method\")\n     def test_contrastive_generate_dict_outputs_use_cache(self):\n         pass\n \n     @unittest.skip(\"Model needs refactor\")\n     def test_attention_outputs(self):\n         pass\n \n+    @unittest.skip(\"MiniMax is special\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"MiniMax is special\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n \n @require_torch\n @require_torch_accelerator"
        },
        {
            "sha": "2267e6dc9d46ef5b956e8c4c241ec77ad83ffd1a",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -290,6 +290,14 @@ def test_feed_forward_chunking(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n+    @unittest.skip(\"Paligemma position ids are 1 indexed\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Paloigemma position ids are 1 indexed\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n     def test_attention_mask_with_token_types(self):\n         \"\"\"Test that attention masking works correctly both with and without token type IDs.\"\"\"\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "7523f83fd96eba901043e003de8147a3d97698bb",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -321,3 +321,11 @@ def test_generate_with_static_cache(self):\n     @is_flaky\n     def test_generate_compile_model_forward(self):\n         super().test_generate_compile_model_forward()\n+\n+    @unittest.skip(\"Paligemma position ids are 1 indexed\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Paligemma position ids are 1 indexed\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass"
        },
        {
            "sha": "e683170e057df2edd7fa5f1a1192e8bab70dffe1",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -76,6 +76,14 @@ class PersimmonModelTest(CausalLMModelTest, unittest.TestCase):\n     test_headmasking = False\n     test_pruning = False\n \n+    @unittest.skip(\"Persimmon applies key/query norm which doesn't work with packing\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Persimmon applies key/query norm which doesn't work with packing\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n \n @require_torch\n class PersimmonIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "0863d8def4e3886fdb91277ad662d0a7a1b0e50e",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -332,7 +332,7 @@ def test_sdpa_can_dispatch_composite_models(self):\n                     if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n                         raise ValueError(\"The eager model should not have SDPA attention layers\")\n \n-    def flash_attention_padding_matches_padding_free_with_position_ids(\n+    def attention_mask_padding_matches_padding_free_with_position_ids(\n         self, attn_implementation: str, fa_kwargs: bool = False\n     ):\n         max_new_tokens = 30"
        },
        {
            "sha": "385a714f131ccce539514e90ccc7fa0adee8d0ae",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -325,7 +325,7 @@ def test_video_forward(self):\n             )\n             self.assertIsNotNone(outputs)\n \n-    def flash_attention_padding_matches_padding_free_with_position_ids(\n+    def attention_mask_padding_matches_padding_free_with_position_ids(\n         self, attn_implementation: str, fa_kwargs: bool = False\n     ):\n         max_new_tokens = 30"
        },
        {
            "sha": "ea0a8992b434869ee3ed8527523513598c2e28b9",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -283,7 +283,7 @@ def test_forward_with_rope_deltas_cached(self):\n                 generation_output.logits[0], forward_output.logits[:, -1, :], rtol=1e-4, atol=1e-4\n             )\n \n-    def flash_attention_padding_matches_padding_free_with_position_ids(\n+    def attention_mask_padding_matches_padding_free_with_position_ids(\n         self, attn_implementation: str, fa_kwargs: bool = False\n     ):\n         max_new_tokens = 30"
        },
        {
            "sha": "5ad7552c01f1ab963fccffe94bfd074e6d5bbb8d",
            "filename": "tests/models/voxtral/test_modeling_voxtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -60,6 +60,7 @@ def __init__(\n             \"use_mrope\": False,\n             \"vocab_size\": 99,\n             \"head_dim\": 8,\n+            \"pad_token_id\": 0,\n         },\n         is_training=True,\n         audio_config={"
        },
        {
            "sha": "57722d226179656ef77eb24ed4dca286556e6a7e",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 39,
            "deletions": 30,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c392d47c9b40a9866663ea3bed97904cec27658b/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=c392d47c9b40a9866663ea3bed97904cec27658b",
            "patch": "@@ -4125,9 +4125,13 @@ def test_flash_attn_2_can_compile_with_attention_mask_None_without_graph_break(s\n \n         assert not loss.isnan().any()\n \n-    def flash_attention_padding_matches_padding_free_with_position_ids(\n+    def attention_mask_padding_matches_padding_free_with_position_ids(\n         self, attn_implementation: str, fa_kwargs: bool = False\n     ):\n+        \"\"\"\n+        Tests that the given attention implementation can work with packed sequences and infers the mask\n+        from position ids. This test requires the model to use new attention mask API which handles packing.\n+        \"\"\"\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n@@ -4142,17 +4146,27 @@ def flash_attention_padding_matches_padding_free_with_position_ids(\n             if attn_implementation != \"eager\" and not getattr(model_class, support_flag[attn_implementation]):\n                 self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n \n+            # can't infer if new attn mask API is supported by assume that only model with attention backend support it\n+            if not model_class._supports_attention_backend:\n+                self.skipTest(f\"{model_class.__name__} does not support new attention mask API\")\n+\n+            if model_class._is_stateful:  # non-transformer models most probably have no packing support\n+                self.skipTest(f\"{model_class.__name__} doesn't support packing!\")\n+\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            if config.is_encoder_decoder:\n+                self.skipTest(\"Model is an encoder-decoder\")\n+\n             if 0 not in inputs_dict.get(\"attention_mask\", []) or \"attention_mask\" not in inputs_dict:\n                 self.skipTest(\"Model dummy inputs should contain padding in their attention mask\")\n \n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.float16]:\n-                dummy_input = dummy_input.to(torch.bfloat16)\n+            if \"input_ids\" not in inputs_dict or inputs_dict[\"input_ids\"].ndim != 2:\n+                self.skipTest(\"Model dummy inputs should contain text input ids\")\n \n             # make sure that all models have enough positions for generation\n+            dummy_input_ids = inputs_dict[\"input_ids\"]\n             if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n+                config.max_position_embeddings = max_new_tokens + dummy_input_ids.shape[1] + 1\n \n             model = model_class(config)\n             if \"position_ids\" not in inspect.signature(model.forward).parameters:\n@@ -4164,11 +4178,14 @@ def flash_attention_padding_matches_padding_free_with_position_ids(\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n \n-                # ensure left padding, to adapt for some models\n+                # Drop all keys except for the minimal set. Hard to manipulate with multimodals/head_mask/etc\n+                inputs_dict = {k: v for k, v in inputs_dict.items() if k in [\"input_ids\", \"attention_mask\"]}\n+\n+                # Ensure left padding, to adapt for some models\n                 if 0 in inputs_dict[\"attention_mask\"][:, -1]:\n                     inputs_dict[\"attention_mask\"] = inputs_dict[\"attention_mask\"].flip(1)\n                 dummy_attention_mask = inputs_dict[\"attention_mask\"]\n-                inputs_dict[\"input_ids\"][~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n+                dummy_input_ids[~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n \n                 model = (\n                     model_class.from_pretrained(\n@@ -4183,8 +4200,7 @@ def flash_attention_padding_matches_padding_free_with_position_ids(\n                 if fa_kwargs:\n                     # flatten\n                     features = [\n-                        {\"input_ids\": i[a.bool()].tolist()}\n-                        for i, a in zip(inputs_dict[\"input_ids\"], inputs_dict[\"attention_mask\"])\n+                        {\"input_ids\": i[a.bool()].tolist()} for i, a in zip(dummy_input_ids, dummy_attention_mask)\n                     ]\n \n                     # add position_ids + fa_kwargs\n@@ -4194,55 +4210,48 @@ def flash_attention_padding_matches_padding_free_with_position_ids(\n                         k: t.to(torch_device) if torch.is_tensor(t) else t for k, t in batch.items()\n                     }\n                 else:\n-                    # flatten\n-                    padfree_inputs_dict = {\n-                        k: v[dummy_attention_mask.bool()].unsqueeze(0)\n-                        for k, v in inputs_dict.items()\n-                        if not k == \"attention_mask\"\n-                    }\n-                    # add position_ids\n-                    padfree_inputs_dict[\"position_ids\"] = (\n+                    # create packed position_ids\n+                    position_ids = (\n                         torch.cat([torch.arange(length) for length in dummy_attention_mask.sum(1).tolist()])\n                         .long()\n                         .unsqueeze(0)\n                         .to(torch_device)\n                     )\n+                    padfree_inputs_dict = {\n+                        \"input_ids\": dummy_input_ids[dummy_attention_mask.bool()].unsqueeze(0),\n+                        \"position_ids\": position_ids,\n+                    }\n \n-                # We need to do simple forward without cache in roder to trigger packed SDPA/FLEX/EAGER path\n+                # We need to do simple forward without cache in order to trigger packed SDPA/flex/eager attention path\n                 res_padded = model(**inputs_dict, use_cache=False)\n                 res_padfree = model(**padfree_inputs_dict, use_cache=False)\n \n-                logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n+                logits_padded = res_padded.logits[dummy_attention_mask.bool()]\n                 logits_padfree = res_padfree.logits[0]\n \n-                torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), rtol=0, atol=0)\n                 # acceptable numerical instability\n                 tol = torch.finfo(torch.bfloat16).eps\n                 torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n \n-    # Mark slow for now as it is failing for all multimodals/non-transformer arch models and a few LLMs\n-    # FIXME @raushan\n-    @slow\n     def test_eager_padding_matches_padding_free_with_position_ids(self):\n-        self.flash_attention_padding_matches_padding_free_with_position_ids(attn_implementation=\"eager\")\n+        self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"eager\")\n \n-    @slow\n     def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n-        self.flash_attention_padding_matches_padding_free_with_position_ids(attn_implementation=\"sdpa\")\n+        self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"sdpa\")\n \n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n-        self.flash_attention_padding_matches_padding_free_with_position_ids(attn_implementation=\"flash_attention_2\")\n+        self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"flash_attention_2\")\n \n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n-        self.flash_attention_padding_matches_padding_free_with_position_ids(\n+        self.attention_mask_padding_matches_padding_free_with_position_ids(\n             attn_implementation=\"flash_attention_2\", fa_kwargs=True\n         )\n \n@@ -4251,14 +4260,14 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa\n     @mark.flash_attn_3_test\n     @slow\n     def test_flash_attention_3_padding_matches_padding_free_with_position_ids(self):\n-        self.flash_attention_padding_matches_padding_free_with_position_ids(attn_implementation=\"flash_attention_3\")\n+        self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"flash_attention_3\")\n \n     @require_flash_attn_3\n     @require_torch_gpu\n     @mark.flash_attn_3_test\n     @slow\n     def test_flash_attention_3_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n-        self.flash_attention_padding_matches_padding_free_with_position_ids(\n+        self.attention_mask_padding_matches_padding_free_with_position_ids(\n             attn_implementation=\"flash_attention_3\", fa_kwargs=True\n         )\n "
        }
    ],
    "stats": {
        "total": 403,
        "additions": 153,
        "deletions": 250
    }
}