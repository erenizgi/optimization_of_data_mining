{
    "author": "techkang",
    "message": "use a tinymodel to test generation config which aviod timeout (#34482)\n\n* use a tinymodel to test generation config which aviod timeout\r\n\r\n* remove tailing whitespace",
    "sha": "655bec2da7120a8681acc2ce951f8d58c6f0e6ef",
    "files": [
        {
            "sha": "0452a10d5d57e69bc3abf9e2b9497f9d8fe43997",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/655bec2da7120a8681acc2ce951f8d58c6f0e6ef/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/655bec2da7120a8681acc2ce951f8d58c6f0e6ef/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=655bec2da7120a8681acc2ce951f8d58c6f0e6ef",
            "patch": "@@ -1544,15 +1544,16 @@ def test_pretrained_low_mem_new_config(self):\n             self.assertEqual(model.__class__.__name__, model_ref.__class__.__name__)\n \n     def test_generation_config_is_loaded_with_model(self):\n-        # Note: `TinyLlama/TinyLlama-1.1B-Chat-v1.0` has a `generation_config.json` containing `max_length: 2048`\n+        # Note: `hf-internal-testing/tiny-random-MistralForCausalLM` has a `generation_config.json`\n+        # containing `bos_token_id: 1`\n \n         # 1. Load without further parameters\n-        model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n-        self.assertEqual(model.generation_config.max_length, 2048)\n+        model = AutoModelForCausalLM.from_pretrained(TINY_MISTRAL)\n+        self.assertEqual(model.generation_config.bos_token_id, 1)\n \n         # 2. Load with `device_map`\n-        model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device_map=\"auto\")\n-        self.assertEqual(model.generation_config.max_length, 2048)\n+        model = AutoModelForCausalLM.from_pretrained(TINY_MISTRAL, device_map=\"auto\")\n+        self.assertEqual(model.generation_config.bos_token_id, 1)\n \n     @require_safetensors\n     def test_safetensors_torch_from_torch(self):"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 6,
        "deletions": 5
    }
}