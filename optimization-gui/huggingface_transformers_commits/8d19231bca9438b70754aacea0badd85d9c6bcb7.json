{
    "author": "gante",
    "message": "[serve] allow array `content` inputs for LLMs (#39829)\n\nfix bug; add tests",
    "sha": "8d19231bca9438b70754aacea0badd85d9c6bcb7",
    "files": [
        {
            "sha": "53b333a8e2b1205de84ff70db76249772ebd156a",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 14,
            "deletions": 5,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d19231bca9438b70754aacea0badd85d9c6bcb7/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d19231bca9438b70754aacea0badd85d9c6bcb7/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=8d19231bca9438b70754aacea0badd85d9c6bcb7",
            "patch": "@@ -829,13 +829,22 @@ def get_processor_inputs_from_inbound_messages(messages, modality: Modality):\n             parsed_message = {\"role\": message[\"role\"], \"content\": []}\n \n             if modality == Modality.LLM:\n-                # If we're working with LLMs, then \"content\" is a single string.\n-                content = message[\"content\"] if isinstance(message[\"content\"], str) else message[\"content\"][\"text\"]\n-                parsed_message[\"content\"] = content\n+                # Input: `content` is a string or a list of dictionaries with a \"text\" key.\n+                # Output: `content` is a string.\n+                if isinstance(message[\"content\"], str):\n+                    parsed_content = message[\"content\"]\n+                elif isinstance(message[\"content\"], list):\n+                    parsed_content = []\n+                    for content in message[\"content\"]:\n+                        if content[\"type\"] == \"text\":\n+                            parsed_content.append(content[\"text\"])\n+                    parsed_content = \" \".join(parsed_content)\n+                parsed_message[\"content\"] = parsed_content\n \n             elif modality == Modality.VLM:\n-                # If we're working with VLMs, then \"content\" is a dictionary, containing a \"type\" key indicating\n-                # which other key will be present and the type of the value of said key.\n+                # Input: `content` is a string or a list of dictionaries with a \"type\" key (possible types: \"text\",\n+                # \"image_url\").\n+                # Output: `content` is a list of dictionaries with a \"type\" key\n                 if isinstance(message[\"content\"], str):\n                     parsed_message[\"content\"].append({\"type\": \"text\", \"text\": message[\"content\"]})\n                 else:"
        },
        {
            "sha": "2683bc7ac6620b4c226ee2d026f2d9a0545c1f9f",
            "filename": "tests/commands/test_serving.py",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d19231bca9438b70754aacea0badd85d9c6bcb7/tests%2Fcommands%2Ftest_serving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d19231bca9438b70754aacea0badd85d9c6bcb7/tests%2Fcommands%2Ftest_serving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcommands%2Ftest_serving.py?ref=8d19231bca9438b70754aacea0badd85d9c6bcb7",
            "patch": "@@ -282,6 +282,37 @@ def test_processor_inputs_from_inbound_messages_llm(self):\n         outputs = ServeCommand.get_processor_inputs_from_inbound_messages(messages, modality)\n         self.assertListEqual(expected_outputs, outputs)\n \n+        messages_with_type = [\n+            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"How are you doing?\"}]},\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"I'm doing great, thank you for asking! How can I assist you today?\"}\n+                ],\n+            },\n+            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Can you help me write tests?\"}]},\n+        ]\n+        outputs = ServeCommand.get_processor_inputs_from_inbound_messages(messages_with_type, modality)\n+        self.assertListEqual(expected_outputs, outputs)\n+\n+        messages_multiple_text = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"How are you doing?\"},\n+                    {\"type\": \"text\", \"text\": \"I'm doing great, thank you for asking! How can I assist you today?\"},\n+                ],\n+            },\n+        ]\n+        expected_outputs_multiple_text = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": \"How are you doing? I'm doing great, thank you for asking! How can I assist you today?\",\n+            },\n+        ]\n+        outputs = ServeCommand.get_processor_inputs_from_inbound_messages(messages_multiple_text, modality)\n+        self.assertListEqual(expected_outputs_multiple_text, outputs)\n+\n     def test_processor_inputs_from_inbound_messages_vlm_text_only(self):\n         modality = Modality.VLM\n         messages = ["
        }
    ],
    "stats": {
        "total": 50,
        "additions": 45,
        "deletions": 5
    }
}