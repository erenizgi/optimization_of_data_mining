{
    "author": "gante",
    "message": "[CI] remove redundant checks in `test_eager_matches_sdpa_inference` (#36740)",
    "sha": "cff4caa0c1436e5fbce1ad4f96446c6dd21004c1",
    "files": [
        {
            "sha": "817c5208d071d721b71e74b8f6426fd443c07548",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/cff4caa0c1436e5fbce1ad4f96446c6dd21004c1/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cff4caa0c1436e5fbce1ad4f96446c6dd21004c1/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=cff4caa0c1436e5fbce1ad4f96446c6dd21004c1",
            "patch": "@@ -138,16 +138,16 @@\n     (\n         # test name for the test runner\n         f\"{dtype}_pad_{padding_side}{'' if use_attention_mask else '_no_attn_mask'}\"\n-        f\"{'_output_attn' if output_attentions else ''}{'_sdpa_kernels' if enable_kernels else ''}\",\n+        f\"{'_sdpa_kernels' if enable_kernels else ''}\",\n         # parameterization\n-        *(dtype, padding_side, use_attention_mask, output_attentions, enable_kernels),\n+        *(dtype, padding_side, use_attention_mask, False, enable_kernels),\n     )\n     for dtype in (\"fp16\", \"fp32\", \"bf16\")\n     for padding_side in (\"left\", \"right\")\n     for use_attention_mask in (True, False)\n-    for output_attentions in (True, False)\n     for enable_kernels in (True, False)\n-]\n+    # Extra test case: `output_attentions=True` has special attention mask handling and sdpa reverts to eager\n+] + [(\"fp32_pad_left_output_attentions\", \"fp32\", \"left\", True, True, False)]\n \n \n def _config_zero_init(config):\n@@ -3618,7 +3618,7 @@ def test_eager_matches_sdpa_inference(\n             (\"cuda\", False, torch.bfloat16): 1e-2,\n             (\"cuda\", False, torch.float16): 5e-3,\n             (\"cuda\", True, torch.float32): 1e-4,\n-            (\"cuda\", True, torch.bfloat16): 3e-2,\n+            (\"cuda\", True, torch.bfloat16): 3e-2,  # (different from others)\n             (\"cuda\", True, torch.float16): 5e-3,\n         }\n "
        }
    ],
    "stats": {
        "total": 10,
        "additions": 5,
        "deletions": 5
    }
}