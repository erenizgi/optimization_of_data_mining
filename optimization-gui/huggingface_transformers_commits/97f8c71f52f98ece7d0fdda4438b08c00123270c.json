{
    "author": "garrett361",
    "message": "Add padding-free to Granite hybrid moe models  (#39677)\n\n* start fixing kwarg handling\n\n* fmt\n\n* updates padding free tests\n\n* docs\n\n* add missing kwargs modeling_granitemoe.py\n\n* run modular util\n\n* rm unrelated changes from modular util",
    "sha": "97f8c71f52f98ece7d0fdda4438b08c00123270c",
    "files": [
        {
            "sha": "92d6e3b70ac48bac2037197feba3a96ee71824b9",
            "filename": "docs/source/en/model_doc/granitemoehybrid.md",
            "status": "modified",
            "additions": 27,
            "deletions": 1,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/97f8c71f52f98ece7d0fdda4438b08c00123270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoehybrid.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/97f8c71f52f98ece7d0fdda4438b08c00123270c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoehybrid.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoehybrid.md?ref=97f8c71f52f98ece7d0fdda4438b08c00123270c",
            "patch": "@@ -48,6 +48,32 @@ for i in output:\n \n This HF implementation is contributed by [Sukriti Sharma](https://huggingface.co/SukritiSharma) and [Alexander Brooks](https://huggingface.co/abrooks9944).\n \n+## Notes\n+\n+- `GraniteMoeHybridForCausalLM` supports padding-free training which concatenates distinct training examples while still processing inputs as separate batches. It can significantly accelerate inference by [~2x](https://github.com/huggingface/transformers/pull/35861#issue-2807873129) (depending on model and data distribution) and reduce memory-usage if there are examples of varying lengths by avoiding unnecessary compute and memory overhead from padding tokens.\n+\n+  Padding-free training requires the `flash-attn`, `mamba-ssm`, and `causal-conv1d` packages and the following arguments must be passed to the model in addition to `input_ids` and `labels`.\n+\n+  - `position_ids: torch.LongTensor`: the position index of each token in each sequence.\n+  - `seq_idx: torch.IntTensor`: the index of each sequence in the batch.\n+  - Each of the [`FlashAttentionKwargs`]\n+    - `cu_seq_lens_q: torch.LongTensor`: the cumulative sequence lengths of all queries.\n+    - `cu_seq_lens_k: torch.LongTensor`: the cumulative sequence lengths of all keys.\n+    - `max_length_q: int`: the longest query length in the batch.\n+    - `max_length_k: int`: the longest key length in the batch.\n+\n+  The `attention_mask` inputs should not be provided. The [`DataCollatorWithFlattening`] programmatically generates the set of additional arguments above using `return_seq_idx=True` and `return_flash_attn_kwargs=True`. See the [Improving Hugging Face Training Efficiency Through Packing with Flash Attention](https://huggingface.co/blog/packing-with-FA2) blog post for additional information.\n+\n+  ```python\n+  from transformers import DataCollatorWithFlattening\n+\n+  # Example of using padding-free training\n+  data_collator = DataCollatorWithFlattening(\n+      tokenizer=tokenizer,\n+      return_seq_idx=True,\n+      return_flash_attn_kwargs=True\n+  )\n+  ```\n \n ## GraniteMoeHybridConfig\n \n@@ -61,4 +87,4 @@ This HF implementation is contributed by [Sukriti Sharma](https://huggingface.co\n ## GraniteMoeHybridForCausalLM\n \n [[autodoc]] GraniteMoeHybridForCausalLM\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "fffe51d794bdfc79ffac8748254b742367aa5dce",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/97f8c71f52f98ece7d0fdda4438b08c00123270c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97f8c71f52f98ece7d0fdda4438b08c00123270c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=97f8c71f52f98ece7d0fdda4438b08c00123270c",
            "patch": "@@ -641,6 +641,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -947,6 +948,7 @@ def forward(\n             output_router_logits=output_router_logits,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         # Only compute necessary logits"
        },
        {
            "sha": "408cb861a142bf866dd1237df0d26cc49cec7fd0",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 34,
            "deletions": 4,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/97f8c71f52f98ece7d0fdda4438b08c00123270c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97f8c71f52f98ece7d0fdda4438b08c00123270c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=97f8c71f52f98ece7d0fdda4438b08c00123270c",
            "patch": "@@ -19,7 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Any, Callable, Optional, Union\n+from typing import Any, Callable, Optional, TypedDict, Union\n \n import torch\n import torch.nn.functional as F\n@@ -34,6 +34,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_granitemoehybrid import GraniteMoeHybridConfig\n@@ -918,6 +919,31 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n+class GraniteFlashAttentionKwargs(TypedDict, total=False):\n+    \"\"\"\n+    Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n+    Use cases include padding-free training and fewer `torch.compile` graph breaks.\n+\n+    Attributes:\n+        cu_seq_lens_q (`torch.LongTensor`)\n+            Gets cumulative sequence length for query state.\n+        cu_seq_lens_k (`torch.LongTensor`)\n+            Gets cumulative sequence length for key state.\n+        max_length_q (`int`):\n+            Maximum sequence length for query state.\n+        max_length_k (`int`):\n+            Maximum sequence length for key state.\n+        seq_idx (`torch.IntTensor):\n+            Index of each packed sequence.\n+    \"\"\"\n+\n+    cu_seq_lens_q: torch.LongTensor\n+    cu_seq_lens_k: torch.LongTensor\n+    max_length_q: int\n+    max_length_k: int\n+    seq_idx: torch.IntTensor\n+\n+\n class GraniteMoeHybridRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -1125,7 +1151,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         output_router_logits: Optional[bool] = False,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        **kwargs,\n+        **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -1149,8 +1175,8 @@ def forward(\n                 Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n                 with `head_dim` being the embedding dimension of each attention head.\n             kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n+                Arbitrary kwargs.Can be used to provide `GraniteFlashAttentionKwargs` for\n+                padding-free training and/or improve torch.compile performance.\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n@@ -1161,6 +1187,7 @@ def forward(\n                 cache_position=cache_position,\n                 cache_params=past_key_value,\n                 attention_mask=attention_mask,\n+                **kwargs,\n             )\n             # No attention weights for state space layers\n             self_attn_weights = None\n@@ -1303,6 +1330,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1374,6 +1402,7 @@ def forward(\n                 cache_position=cache_position,\n                 output_router_logits=output_router_logits,\n                 position_embeddings=position_embeddings,\n+                **kwargs,\n             )\n \n             hidden_states = layer_outputs[0]\n@@ -1706,6 +1735,7 @@ def forward(\n             output_router_logits=output_router_logits,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         # Only compute necessary logits"
        },
        {
            "sha": "242c95b90767e3048c2d505da73f0db20c6f36ad",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/97f8c71f52f98ece7d0fdda4438b08c00123270c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97f8c71f52f98ece7d0fdda4438b08c00123270c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=97f8c71f52f98ece7d0fdda4438b08c00123270c",
            "patch": "@@ -20,10 +20,12 @@\n \n from ...cache_utils import Cache\n from ...modeling_outputs import BaseModelOutputWithPast, MoeModelOutputWithPast\n+from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, logging\n from ..bamba.configuration_bamba import BambaConfig\n from ..bamba.modeling_bamba import BambaMixer, BambaRMSNormGated, HybridMambaAttentionDynamicCache\n from ..granitemoeshared.modeling_granitemoeshared import (\n+    GraniteFlashAttentionKwargs,\n     GraniteMoeSharedAttention,\n     GraniteMoeSharedDecoderLayer,\n     GraniteMoeSharedForCausalLM,\n@@ -84,7 +86,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         output_router_logits: Optional[bool] = False,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        **kwargs,\n+        **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -108,8 +110,8 @@ def forward(\n                 Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n                 with `head_dim` being the embedding dimension of each attention head.\n             kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n+                Arbitrary kwargs.Can be used to provide `GraniteFlashAttentionKwargs` for\n+                padding-free training and/or improve torch.compile performance.\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n@@ -120,6 +122,7 @@ def forward(\n                 cache_position=cache_position,\n                 cache_params=past_key_value,\n                 attention_mask=attention_mask,\n+                **kwargs,\n             )\n             # No attention weights for state space layers\n             self_attn_weights = None\n@@ -198,6 +201,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -269,6 +273,7 @@ def forward(\n                 cache_position=cache_position,\n                 output_router_logits=output_router_logits,\n                 position_embeddings=position_embeddings,\n+                **kwargs,\n             )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "21e9d13f719582d27695e171fdb7dfc5c7612687",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 32,
            "deletions": 4,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/97f8c71f52f98ece7d0fdda4438b08c00123270c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97f8c71f52f98ece7d0fdda4438b08c00123270c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=97f8c71f52f98ece7d0fdda4438b08c00123270c",
            "patch": "@@ -19,7 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Callable, Optional, Union\n+from typing import Callable, Optional, TypedDict, Union\n \n import torch\n import torch.nn.functional as F\n@@ -33,6 +33,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n from .configuration_granitemoeshared import GraniteMoeSharedConfig\n \n@@ -46,6 +47,31 @@\n logger = logging.get_logger(__name__)\n \n \n+class GraniteFlashAttentionKwargs(TypedDict, total=False):\n+    \"\"\"\n+    Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n+    Use cases include padding-free training and fewer `torch.compile` graph breaks.\n+\n+    Attributes:\n+        cu_seq_lens_q (`torch.LongTensor`)\n+            Gets cumulative sequence length for query state.\n+        cu_seq_lens_k (`torch.LongTensor`)\n+            Gets cumulative sequence length for key state.\n+        max_length_q (`int`):\n+            Maximum sequence length for query state.\n+        max_length_k (`int`):\n+            Maximum sequence length for key state.\n+        seq_idx (`torch.IntTensor):\n+            Index of each packed sequence.\n+    \"\"\"\n+\n+    cu_seq_lens_q: torch.LongTensor\n+    cu_seq_lens_k: torch.LongTensor\n+    max_length_q: int\n+    max_length_k: int\n+    seq_idx: torch.IntTensor\n+\n+\n class GraniteMoeSharedMLP(nn.Module):\n     \"\"\"\n     MLP layer for shared experts\n@@ -431,7 +457,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         output_router_logits: Optional[bool] = False,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        **kwargs,\n+        **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -455,8 +481,8 @@ def forward(\n                 Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n                 with `head_dim` being the embedding dimension of each attention head.\n             kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n+                Arbitrary kwargs. Can be used to provide `GraniteFlashAttentionKwargs` for\n+                padding-free training and/or improve torch.compile performance.\n         \"\"\"\n         residual = hidden_states\n \n@@ -593,6 +619,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -979,6 +1006,7 @@ def forward(\n             output_router_logits=output_router_logits,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         # Only compute necessary logits"
        },
        {
            "sha": "630e5aa18439d93cbc7adac7e6e293ccc7adeb37",
            "filename": "src/transformers/models/granitemoeshared/modular_granitemoeshared.py",
            "status": "modified",
            "additions": 30,
            "deletions": 4,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/97f8c71f52f98ece7d0fdda4438b08c00123270c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97f8c71f52f98ece7d0fdda4438b08c00123270c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py?ref=97f8c71f52f98ece7d0fdda4438b08c00123270c",
            "patch": "@@ -13,13 +13,14 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional\n+from typing import Optional, TypedDict\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache\n+from ...processing_utils import Unpack\n from ...utils import logging\n from ..granitemoe.modeling_granitemoe import (\n     GraniteMoeDecoderLayer,\n@@ -33,6 +34,31 @@\n logger = logging.get_logger(__name__)\n \n \n+class GraniteFlashAttentionKwargs(TypedDict, total=False):\n+    \"\"\"\n+    Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n+    Use cases include padding-free training and fewer `torch.compile` graph breaks.\n+\n+    Attributes:\n+        cu_seq_lens_q (`torch.LongTensor`)\n+            Gets cumulative sequence length for query state.\n+        cu_seq_lens_k (`torch.LongTensor`)\n+            Gets cumulative sequence length for key state.\n+        max_length_q (`int`):\n+            Maximum sequence length for query state.\n+        max_length_k (`int`):\n+            Maximum sequence length for key state.\n+        seq_idx (`torch.IntTensor):\n+            Index of each packed sequence.\n+    \"\"\"\n+\n+    cu_seq_lens_q: torch.LongTensor\n+    cu_seq_lens_k: torch.LongTensor\n+    max_length_q: int\n+    max_length_k: int\n+    seq_idx: torch.IntTensor\n+\n+\n class GraniteMoeSharedMLP(nn.Module):\n     \"\"\"\n     MLP layer for shared experts\n@@ -75,7 +101,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         output_router_logits: Optional[bool] = False,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        **kwargs,\n+        **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -99,8 +125,8 @@ def forward(\n                 Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n                 with `head_dim` being the embedding dimension of each attention head.\n             kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n+                Arbitrary kwargs. Can be used to provide `GraniteFlashAttentionKwargs` for\n+                padding-free training and/or improve torch.compile performance.\n         \"\"\"\n         residual = hidden_states\n "
        },
        {
            "sha": "cf660b7fa09d37a2c9cc835bf793ff829e6b6e79",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/97f8c71f52f98ece7d0fdda4438b08c00123270c/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97f8c71f52f98ece7d0fdda4438b08c00123270c/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=97f8c71f52f98ece7d0fdda4438b08c00123270c",
            "patch": "@@ -551,6 +551,15 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids_seq_id\n                     inputs_dict[\"attention_mask\"] = inputs_dict[\"attention_mask\"].flip(1)\n                 dummy_attention_mask = inputs_dict[\"attention_mask\"]\n                 inputs_dict[\"input_ids\"][~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n+                # Ensure inputs_dict also has labels in it, as their presence/absence can induce\n+                # dtype conversions. This also lets us compare losses.\n+                labels = inputs_dict[\"input_ids\"].clone()\n+                # Mask padding tokens\n+                labels[~dummy_attention_mask.bool()] = -100\n+                # Also need to mask the first non-trivial token to match the padding-free batch.\n+                first_nonneg_idx = (labels >= 0).int().argmax(dim=1)\n+                labels[torch.arange(labels.size(0), device=labels.device), first_nonneg_idx] = -100\n+                inputs_dict[\"labels\"] = labels\n \n                 model = (\n                     model_class.from_pretrained(\n@@ -586,6 +595,10 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids_seq_id\n                 tol = torch.finfo(torch.float16).eps\n                 torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n \n+                loss_padded = res_padded.loss\n+                loss_padfree = res_padfree.loss\n+                torch.testing.assert_close(loss_padded, loss_padfree)\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 162,
        "additions": 146,
        "deletions": 16
    }
}