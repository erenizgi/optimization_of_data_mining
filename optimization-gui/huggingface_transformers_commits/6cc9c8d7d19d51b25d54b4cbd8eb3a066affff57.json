{
    "author": "cyyever",
    "message": "Remove deprecated batch_size parameter (#37007)",
    "sha": "6cc9c8d7d19d51b25d54b4cbd8eb3a066affff57",
    "files": [
        {
            "sha": "f8cced3bb33aca1ca4e579f3a78a3c229e931b66",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 46,
            "deletions": 88,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cc9c8d7d19d51b25d54b4cbd8eb3a066affff57/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cc9c8d7d19d51b25d54b4cbd8eb3a066affff57/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=6cc9c8d7d19d51b25d54b4cbd8eb3a066affff57",
            "patch": "@@ -1065,6 +1065,8 @@ def update(\n         \"\"\"\n         # Optional kwargs for `SinkCache` -- needed on models using RoPE. `partial_rotation_size` is used on models\n         # with partially rotated position embeddings, like Phi or Persimmon.\n+        if cache_kwargs is None:\n+            cache_kwargs = {}\n         sin = cache_kwargs.get(\"sin\")\n         cos = cache_kwargs.get(\"cos\")\n         partial_rotation_size = cache_kwargs.get(\"partial_rotation_size\")\n@@ -1140,20 +1142,20 @@ class StaticCache(Cache):\n     Parameters:\n         config (`PretrainedConfig`):\n             The configuration file defining the shape-related attributes required to initialize the static cache.\n-        batch_size (`int`):\n-            The batch size with which the model will be used. Note that a new instance must be instantiated if a\n+        max_batch_size (`int`):\n+            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a\n             smaller batch size is used. If you are manually setting the batch size, make sure to take into account the\n             number of beams if you are running beam search\n-        max_cache_len (`int`):\n+        max_cache_len (`int`, *optional*):\n             The maximum sequence length with which the model will be used.\n-        device (`torch.device` or `str`):\n+        device (`torch.device` or `str`, *optional*):\n             The device on which the cache should be initialized. If you're using more than 1 computation device, you\n             should pass the `layer_device_map` argument instead.\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n-        layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n+        layer_device_map (`Optional[Dict[int, Union[str, torch.device, int]]]]`, *optional*):\n             Mapping between the layers and its device. This is required when you are manually initializing the cache\n-            and the model is splitted between differents gpus. You can know which layers mapped to which device by\n+            and the model is split between different gpus. You can know which layers mapped to which device by\n             checking the associated device_map: `model.hf_device_map`.\n \n \n@@ -1170,7 +1172,7 @@ class StaticCache(Cache):\n         >>> # Prepare a cache class and pass it to model's forward\n         >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n         >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = StaticCache(config=model.config, batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n+        >>> past_key_values = StaticCache(config=model.config, max_batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n         >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n         >>> outputs.past_key_values # access cache filled with key/values from generation\n         StaticCache()\n@@ -1179,25 +1181,17 @@ class StaticCache(Cache):\n \n     is_compileable = True\n \n-    # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n     def __init__(\n         self,\n         config: PretrainedConfig,\n-        batch_size: Optional[int] = None,\n+        max_batch_size: int,\n         max_cache_len: Optional[int] = None,\n-        device: torch.device = None,\n+        device: Union[torch.device, str, None] = None,\n         dtype: torch.dtype = torch.float32,\n-        max_batch_size: Optional[int] = None,\n         layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         super().__init__()\n-        if batch_size is not None:\n-            logger.warning_once(\n-                f\"The 'batch_size' argument of {self.__class__.__name__} is deprecated and will be removed in \"\n-                \"v4.49. Use the more precisely named 'max_batch_size' argument instead.\"\n-            )\n-\n-        self.max_batch_size = batch_size or max_batch_size\n+        self.max_batch_size = max_batch_size\n         self.max_cache_len = config.max_position_embeddings if max_cache_len is None else max_cache_len\n \n         # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n@@ -1256,6 +1250,8 @@ def update(\n         Return:\n             A tuple containing the updated key and value states.\n         \"\"\"\n+        if cache_kwargs is None:\n+            cache_kwargs = {}\n         cache_position = cache_kwargs.get(\"cache_position\")\n         k_out = self.key_cache[layer_idx]\n         v_out = self.value_cache[layer_idx]\n@@ -1296,14 +1292,6 @@ def reset(self):\n             self.key_cache[layer_idx].zero_()\n             self.value_cache[layer_idx].zero_()\n \n-    @property\n-    def batch_size(self):\n-        logger.warning_once(\n-            f\"The 'batch_size' attribute of {self.__class__.__name__} is deprecated and will be removed in \"\n-            \"v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\"\n-        )\n-        return self.max_batch_size\n-\n \n class SlidingWindowCache(StaticCache):\n     \"\"\"\n@@ -1325,19 +1313,19 @@ class SlidingWindowCache(StaticCache):\n     Parameters:\n         config (`PretrainedConfig`):\n             The configuration file defining the shape-related attributes required to initialize the static cache.\n-        batch_size (`int`):\n-            The batch size with which the model will be used. Note that a new instance must be instantiated if a\n+        max_batch_size (`int`):\n+            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a\n             smaller batch size is used.\n-        max_cache_len (`int`):\n+        max_cache_len (`int`, *optional*):\n             The maximum sequence length with which the model will be used.\n-        device (`torch.device` or `str`):\n+        device (`torch.device` or `str`, *optional*):\n             The device on which the cache should be initialized. If you're using more than 1 computation device, you\n             should pass the `layer_device_map` argument instead.\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n-        layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n+        layer_device_map (`Optional[Dict[int, Union[str, torch.device, int]]]]`, *optional*):\n             Mapping between the layers and its device. This is required when you are manually initializing the cache\n-            and the model is splitted between differents gpus. You can know which layers mapped to which device by\n+            and the model is split between different gpus. You can know which layers mapped to which device by\n             checking the associated device_map: `model.hf_device_map`.\n \n     Example:\n@@ -1353,7 +1341,7 @@ class SlidingWindowCache(StaticCache):\n         >>> # Prepare a cache class and pass it to model's forward\n         >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n         >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = SlidingWindowCache(config=model.config, batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n+        >>> past_key_values = SlidingWindowCache(config=model.config, max_batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n         >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n         >>> outputs.past_key_values # access cache filled with key/values from generation\n         SlidingWindowCache()\n@@ -1363,15 +1351,13 @@ class SlidingWindowCache(StaticCache):\n     is_sliding = True\n     is_compileable = True\n \n-    # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n     def __init__(\n         self,\n         config: PretrainedConfig,\n-        batch_size: Optional[int] = None,\n+        max_batch_size: int,\n         max_cache_len: Optional[int] = None,\n-        device: torch.device = None,\n+        device: Union[torch.device, str, None] = None,\n         dtype: torch.dtype = torch.float32,\n-        max_batch_size: Optional[int] = None,\n         layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         if not hasattr(config, \"sliding_window\") or config.sliding_window is None:\n@@ -1383,11 +1369,10 @@ def __init__(\n         max_cache_len = min(config.sliding_window, max_cache_len)\n         super().__init__(\n             config=config,\n-            batch_size=batch_size,\n+            max_batch_size=max_batch_size,\n             max_cache_len=max_cache_len,\n             device=device,\n             dtype=dtype,\n-            max_batch_size=max_batch_size,\n             layer_device_map=layer_device_map,\n         )\n \n@@ -1397,7 +1382,9 @@ def update(\n         value_states: torch.Tensor,\n         layer_idx: int,\n         cache_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        if cache_kwargs is None:\n+            cache_kwargs = {}\n         cache_position = cache_kwargs.get(\"cache_position\")\n         k_out = self.key_cache[layer_idx]\n         v_out = self.value_cache[layer_idx]\n@@ -1631,19 +1618,19 @@ class HybridCache(Cache):\n     Parameters:\n         config (`PretrainedConfig):\n             The configuration file defining the shape-related attributes required to initialize the static cache.\n-        batch_size (`int`):\n-            The batch size with which the model will be used. Note that a new instance must be instantiated if a\n+        max_batch_size (`int`):\n+            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a\n             smaller batch size is used.\n-        max_cache_len (`int`):\n+        max_cache_len (`int`, *optional*):\n             The maximum sequence length with which the model will be used.\n         device (`torch.device` or `str`, *optional*):\n             The device on which the cache should be initialized. If you're using more than 1 computation device, you\n             should pass the `layer_device_map` argument instead.\n         dtype (torch.dtype, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n-        layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n+        layer_device_map (`Optional[Dict[int, Union[str, torch.device, int]]]]`, *optional*):\n             Mapping between the layers and its device. This is required when you are manually initializing the cache\n-            and the model is splitted between differents gpus. You can know which layers mapped to which device by\n+            and the model is split between different gpus. You can know which layers mapped to which device by\n             checking the associated device_map: `model.hf_device_map`.\n \n     Example:\n@@ -1659,7 +1646,7 @@ class HybridCache(Cache):\n         >>> # Prepare a cache class and pass it to model's forward\n         >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n         >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = HybridCache(config=model.config, batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n+        >>> past_key_values = HybridCache(config=model.config, max_batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n         >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n         >>> outputs.past_key_values # access cache filled with key/values from generation\n         HybridCache()\n@@ -1670,31 +1657,24 @@ class HybridCache(Cache):\n     # ALL changes from the PR that commented the line below when reactivating it.\n     # is_compileable = True\n \n-    # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n     def __init__(\n         self,\n         config: PretrainedConfig,\n-        batch_size: Optional[int] = None,\n+        max_batch_size: int,\n         max_cache_len: Optional[int] = None,\n-        device: Union[torch.device, str] = None,\n+        device: Union[torch.device, str, None] = None,\n         dtype: torch.dtype = torch.float32,\n-        max_batch_size: Optional[int] = None,\n         layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         super().__init__()\n-        if batch_size is not None:\n-            logger.warning_once(\n-                f\"The 'batch_size' argument of {self.__class__.__name__} is deprecated and will be removed in \"\n-                \"v4.49. Use the more precisely named 'max_batch_size' argument instead.\"\n-            )\n         if not hasattr(config, \"sliding_window\") or config.sliding_window is None:\n             raise ValueError(\n                 \"Setting `cache_implementation` to 'sliding_window' requires the model config supporting \"\n                 \"sliding window attention, please check if there is a `sliding_window` field in the model \"\n                 \"config and it's not set to None.\"\n             )\n         self.max_cache_len = max_cache_len\n-        self.max_batch_size = batch_size or max_batch_size\n+        self.max_batch_size = max_batch_size\n         # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n         self.head_dim = (\n             config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n@@ -1718,7 +1698,7 @@ def __init__(\n             min(config.sliding_window, max_cache_len),\n             self.head_dim,\n         )\n-        device = torch.device(device) if device is not None else None\n+        device = torch.device(device) if device is not None and isinstance(device, str) else None\n         for i in range(config.num_hidden_layers):\n             if layer_device_map is not None:\n                 layer_device = layer_device_map[i]\n@@ -1776,7 +1756,9 @@ def update(\n         value_states: torch.Tensor,\n         layer_idx: int,\n         cache_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        if cache_kwargs is None:\n+            cache_kwargs = {}\n         cache_position = cache_kwargs.get(\"cache_position\")\n         sliding_window = cache_kwargs.get(\"sliding_window\")\n \n@@ -1828,14 +1810,6 @@ def reset(self):\n             self.key_cache[layer_idx].zero_()\n             self.value_cache[layer_idx].zero_()\n \n-    @property\n-    def batch_size(self):\n-        logger.warning_once(\n-            f\"The 'batch_size' attribute of {self.__class__.__name__} is deprecated and will be removed in \"\n-            \"v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\"\n-        )\n-        return self.max_batch_size\n-\n \n class MambaCache:\n     \"\"\"\n@@ -1844,9 +1818,8 @@ class MambaCache:\n     Arguments:\n         config (`PretrainedConfig):\n             The configuration file defining the shape-related attributes required to initialize the static cache.\n-        batch_size (`int`):\n-            The batch size with which the model will be used. Note that a new instance must be instantiated if a\n-            smaller batch size is used.\n+        max_batch_size (`int`):\n+            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a smaller batch size is used.\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float16`):\n             The default `dtype` to use when initializing the layer.\n         device (`torch.device` or `str`, *optional*):\n@@ -1863,7 +1836,7 @@ class MambaCache:\n         >>> inputs = tokenizer(text=\"My name is Mamba\", return_tensors=\"pt\")\n \n         >>> # Prepare a cache class and pass it to model's forward\n-        >>> past_key_values = MambaCache(config=model.config, batch_size=1, device=model.device, dtype=model.dtype)\n+        >>> past_key_values = MambaCache(config=model.config, max_batch_size=1, device=model.device, dtype=model.dtype)\n         >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n         >>> outputs.past_key_values\n         MambaCache()\n@@ -1872,23 +1845,16 @@ class MambaCache:\n \n     is_compileable = True\n \n-    # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n     # TODO (joao): add layer_device_map arg and update code in `generate` accordingly\n     def __init__(\n         self,\n         config: PretrainedConfig,\n-        batch_size: Optional[int] = None,\n+        max_batch_size: int,\n         dtype: torch.dtype = torch.float16,\n-        device: Optional[Union[torch.device, str]] = None,\n-        max_batch_size: Optional[int] = None,\n+        device: Union[torch.device, str, None] = None,\n     ):\n-        if batch_size is not None:\n-            logger.warning_once(\n-                f\"The 'batch_size' argument of {self.__class__.__name__} is deprecated and will be removed in \"\n-                \"v4.49. Use the more precisely named 'max_batch_size' argument instead.\"\n-            )\n         self.dtype = dtype\n-        self.max_batch_size = batch_size or max_batch_size\n+        self.max_batch_size = max_batch_size\n         self.intermediate_size = config.intermediate_size\n         self.ssm_state_size = config.state_size\n         self.conv_kernel_size = config.conv_kernel\n@@ -1944,14 +1910,6 @@ def reset(self):\n             self.conv_states[layer_idx].zero_()\n             self.ssm_states[layer_idx].zero_()\n \n-    @property\n-    def batch_size(self):\n-        logger.warning_once(\n-            f\"The 'batch_size' attribute of {self.__class__.__name__} is deprecated and will be removed in \"\n-            \"v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\"\n-        )\n-        return self.max_batch_size\n-\n \n class OffloadedStaticCache(StaticCache):\n     \"\"\""
        },
        {
            "sha": "63540575b206e0758fbd68d5df75134a5a72ce9c",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cc9c8d7d19d51b25d54b4cbd8eb3a066affff57/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cc9c8d7d19d51b25d54b4cbd8eb3a066affff57/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=6cc9c8d7d19d51b25d54b4cbd8eb3a066affff57",
            "patch": "@@ -422,7 +422,7 @@ def test_dtype_mismatch_handled_in_cache(self):\n         model.eval()\n \n         # Create cache with float32 dtype\n-        cache_params = MambaCache(config, batch_size=input_ids.size(0), dtype=torch.float32, device=torch_device)\n+        cache_params = MambaCache(config, max_batch_size=input_ids.size(0), dtype=torch.float32, device=torch_device)\n \n         # If code is correct, no error occurs and test passes\n         outputs = model("
        },
        {
            "sha": "15a3e44ef8d476c432a6d2999cd8e7b325be2a5d",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cc9c8d7d19d51b25d54b4cbd8eb3a066affff57/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cc9c8d7d19d51b25d54b4cbd8eb3a066affff57/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=6cc9c8d7d19d51b25d54b4cbd8eb3a066affff57",
            "patch": "@@ -151,23 +151,23 @@ def _random_kvs(config):\n             return random_keys, random_values\n \n         mha_config = LlamaConfig(num_attention_heads=32)\n-        mha_static_cache = StaticCache(config=mha_config, batch_size=1, max_cache_len=10, device=torch_device)\n+        mha_static_cache = StaticCache(config=mha_config, max_batch_size=1, max_cache_len=10, device=torch_device)\n         cached_keys, cached_values = mha_static_cache.update(\n             *_random_kvs(mha_config), 0, cache_kwargs={\"cache_position\": torch.arange(1).to(torch_device)}\n         )\n         self.assertTrue(cached_keys.shape == (1, 32, 10, 128))\n         self.assertTrue(cached_values.shape == (1, 32, 10, 128))\n \n         gqa_config = LlamaConfig(num_attention_heads=32, num_key_value_heads=4)\n-        gqa_static_cache = StaticCache(config=gqa_config, batch_size=1, max_cache_len=10, device=torch_device)\n+        gqa_static_cache = StaticCache(config=gqa_config, max_batch_size=1, max_cache_len=10, device=torch_device)\n         cached_keys, cached_values = gqa_static_cache.update(\n             *_random_kvs(gqa_config), 0, cache_kwargs={\"cache_position\": torch.arange(1).to(torch_device)}\n         )\n         self.assertTrue(cached_keys.shape == (1, 4, 10, 128))\n         self.assertTrue(cached_values.shape == (1, 4, 10, 128))\n \n         mqa_config = LlamaConfig(num_attention_heads=32, num_key_value_heads=1)\n-        mqa_static_cache = StaticCache(config=mqa_config, batch_size=1, max_cache_len=10, device=torch_device)\n+        mqa_static_cache = StaticCache(config=mqa_config, max_batch_size=1, max_cache_len=10, device=torch_device)\n         cached_keys, cached_values = mqa_static_cache.update(\n             *_random_kvs(mqa_config), 0, cache_kwargs={\"cache_position\": torch.arange(1).to(torch_device)}\n         )"
        },
        {
            "sha": "bdcec87c2ba55313ac0d8818a74ca539b4ad16ee",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cc9c8d7d19d51b25d54b4cbd8eb3a066affff57/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cc9c8d7d19d51b25d54b4cbd8eb3a066affff57/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=6cc9c8d7d19d51b25d54b4cbd8eb3a066affff57",
            "patch": "@@ -74,11 +74,6 @@\n     \"TFSequenceSummary\",\n     \"TFBertTokenizer\",\n     \"TFGPT2Tokenizer\",\n-    # Going through an argument deprecation cycle, remove after v4.46\n-    \"HybridCache\",\n-    \"MambaCache\",\n-    \"SlidingWindowCache\",\n-    \"StaticCache\",\n     # Missing arguments in the docstring\n     \"ASTFeatureExtractor\",\n     \"AlbertModel\","
        }
    ],
    "stats": {
        "total": 147,
        "additions": 50,
        "deletions": 97
    }
}