{
    "author": "manueldeprada",
    "message": "ðŸ”´ Move variable output controls to `_prepare_generation_config ` (#40715)\n\n* move checks to validate steps where possible\n\n* fix csm and other models that override _sample\n\n* ops dia you again\n\n* opsie\n\n* joao review\n\n* Move variable output controls to `prepare_inputs_for_generation`\n\n* fix a bunch of models\n\n* back to basics\n\n* final touches",
    "sha": "0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
    "files": [
        {
            "sha": "e5235d24406812f9a36f9cbf094359711077fecd",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 14,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -46,6 +46,7 @@\n from ..tokenization_utils import ExtensionsTrie\n from ..utils import (\n     ModelOutput,\n+    TransformersKwargs,\n     is_accelerate_available,\n     is_hqq_available,\n     is_optimum_quanto_available,\n@@ -559,8 +560,9 @@ def prepare_inputs_for_generation(\n         **kwargs,\n     ):\n         \"\"\"\n-        Prepare the model inputs for generation. It includes operations like computing the 4D attention mask or\n-        slicing inputs given the existing cache.\n+        Prepare the model inputs for generation. Notable steps include selecting the correct input key and cloning when appropriate,\n+        creating position_ids from the attention_mask when missing, slicing inputs and converting 2D attention masks to 4D for\n+        compilable caches, and finally forwarding all additional keyword arguments unchanged to the model's forward pass.\n \n         See the forward pass in the model documentation for expected arguments (different models might have different\n         requirements for e.g. `past_key_values`). This function should work as is for most LLMs.\n@@ -1592,8 +1594,9 @@ def _validate_model_kwargs(self, model_kwargs: dict[str, Any]):\n                 decoder_model_args = set(inspect.signature(decoder.forward).parameters)\n                 model_args |= {f\"decoder_{x}\" for x in decoder_model_args}\n \n+        # TransformersKwargs are model-agnostic attention and generation arguments such as 'output_attentions'\n         for key, value in model_kwargs.items():\n-            if value is not None and key not in model_args:\n+            if value is not None and key not in model_args and key not in TransformersKwargs.__optional_keys__:\n                 unused_model_args.append(key)\n \n         if unused_model_args:\n@@ -1798,6 +1801,11 @@ def _prepare_generation_config(\n \n         # Finally, apply any passed kwargs\n         model_kwargs = generation_config.update(**kwargs)\n+        # And keep in model_kwargs variable output controls\n+        output_attentions = generation_config.output_attentions\n+        output_hidden_states = generation_config.output_hidden_states\n+        model_kwargs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n+        model_kwargs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n \n         return generation_config, model_kwargs\n \n@@ -2761,10 +2769,6 @@ def _sample(\n             # prepare model inputs\n             model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n \n-            # prepare variable output controls (note: some models won't accept all output controls)\n-            model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n-            model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n-\n             if is_prefill:\n                 outputs = self(**model_inputs, return_dict=True)\n                 is_prefill = False\n@@ -3247,10 +3251,6 @@ def _beam_search(\n             flat_running_sequences = self._flatten_beam_dim(running_sequences[:, :, :cur_len])\n             model_inputs = self.prepare_inputs_for_generation(flat_running_sequences, **model_kwargs)\n \n-            # prepare variable output controls (note: some models won't accept all output controls)\n-            model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n-            model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n-\n             model_outputs = self(**model_inputs, return_dict=True)\n \n             # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n@@ -3575,9 +3575,6 @@ def _assisted_decoding(\n                 model_inputs[\"logits_to_keep\"] = candidate_length + 1\n \n             # 2.2. Run a forward pass on the candidate sequence\n-            # prepare variable output controls (note: some models won't accept all output controls)\n-            model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n-            model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n \n             outputs = self(**model_inputs)\n "
        },
        {
            "sha": "09f00845524de5cc7d793aa52c2b534a76a86693",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1492,6 +1492,12 @@ def prepare_inputs_for_generation(\n                 \"cache_position\": cache_position,\n             }\n         )\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n "
        },
        {
            "sha": "52814930a17213b48757a5ec30a9c24c0c41e585",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1211,6 +1211,12 @@ def prepare_inputs_for_generation(\n                 \"cache_position\": cache_position,\n             }\n         )\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n "
        },
        {
            "sha": "605ae4f59b63d3ecc799227437dd4fa8c80aa577",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -818,6 +818,12 @@ def prepare_inputs_for_generation(\n                 \"attention_mask\": attention_mask,\n             }\n         )\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n     @auto_docstring"
        },
        {
            "sha": "506bed039b17d4dadfce2755366207f3a2e718f3",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -570,7 +570,17 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, use_cac\n \n             input_ids = input_ids[:, remove_prefix_length:]\n \n-        return {\"input_ids\": input_ids, \"past_key_values\": past_key_values, \"use_cache\": use_cache}\n+        model_inputs = {\"input_ids\": input_ids, \"past_key_values\": past_key_values, \"use_cache\": use_cache}\n+\n+        # token_type_ids are computed on CTRLModel.forward()\n+        kwargs.pop(\"token_type_ids\", None)\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                print(f\"Warning: {key} is not a recognized input.\")\n+                model_inputs[key] = value\n+\n+        return model_inputs\n \n \n @auto_docstring("
        },
        {
            "sha": "5f08309b2085099ae67337a3e38fa6e2986d7396",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1607,6 +1607,12 @@ def prepare_inputs_for_generation(\n                 \"cache_position\": cache_position,\n             }\n         )\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n "
        },
        {
            "sha": "c81e8967bcf21f13c890567f02ba07fc1379d4a2",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1372,6 +1372,12 @@ def prepare_inputs_for_generation(\n                 \"cache_position\": cache_position,\n             }\n         )\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n "
        },
        {
            "sha": "3cdf6da7bda376a34dd00545d96c44a99fc0e660",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -862,6 +862,12 @@ def prepare_inputs_for_generation(\n                 \"attention_mask\": attention_mask,\n             }\n         )\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n     @auto_docstring"
        },
        {
            "sha": "0125132718a3aad50b3d18aea58dbe80a75cd378",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1442,13 +1442,20 @@ def prepare_inputs_for_generation(\n         if attention_mask is None:\n             attention_mask = input_ids.new_ones(input_shape)\n \n-        return {\n+        model_inputs = {\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n             \"pixel_values\": kwargs.get(\"pixel_values\"),\n             \"past_key_values\": past_key_values,\n             \"use_cache\": use_cache,\n         }\n \n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n+        return model_inputs\n+\n \n __all__ = [\"GitForCausalLM\", \"GitModel\", \"GitPreTrainedModel\", \"GitVisionModel\"]"
        },
        {
            "sha": "7f9883779c435a85bb1e605364541ff50abf2ef1",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1829,6 +1829,12 @@ def prepare_inputs_for_generation(\n                 \"cache_position\": cache_position,\n             }\n         )\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n "
        },
        {
            "sha": "55ad2b43c1bbceb07dc5a6945b6a24d80ded4615",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -383,6 +383,12 @@ def prepare_inputs_for_generation(\n                 \"cache_position\": cache_position,\n             }\n         )\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n "
        },
        {
            "sha": "17246d6f1b2e2a22c3c285ab8d8867aa5ee14c4e",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1448,6 +1448,12 @@ def prepare_inputs_for_generation(\n                 \"cache_position\": cache_position,\n             }\n         )\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n "
        },
        {
            "sha": "1bb70fd5093d54cb675be1ec8e4156db4adf671c",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1642,7 +1642,7 @@ def prepare_inputs_for_generation(\n                 dim=1,\n             )\n \n-        return {\n+        model_inputs = {\n             \"input_ids\": input_ids,\n             \"image_embeds\": image_embeds,\n             \"image_embeds_position_mask\": image_embeds_position_mask,\n@@ -1652,6 +1652,13 @@ def prepare_inputs_for_generation(\n             \"use_cache\": use_cache,\n         }\n \n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in model_kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n+        return model_inputs\n+\n \n @add_start_docstrings(\n     \"\"\""
        },
        {
            "sha": "4a53c47c8b4ad36aea1d306d37026103b3e75b41",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -803,6 +803,12 @@ def prepare_inputs_for_generation(\n                 \"attention_mask\": attention_mask,\n             }\n         )\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n     @auto_docstring"
        },
        {
            "sha": "738c5376c33ec510cfb145356428cde1b73dea04",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -989,6 +989,12 @@ def prepare_inputs_for_generation(\n                 \"attention_mask\": attention_mask,\n             }\n         )\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n     @auto_docstring"
        },
        {
            "sha": "503177f95b4a981456f72cc304d0bd1f56847e82",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -2242,7 +2242,7 @@ def prepare_inputs_for_generation(\n \n         # we want to do it after a first token has been generated\n         if model_inputs[\"input_ids\"] is not None:\n-            last_hidden_state = kwargs.get(\"last_hidden_state\")\n+            last_hidden_state = kwargs.pop(\"last_hidden_state\")\n             # (batch_size, sequence_length, dim) -> (batch_size * sequence_length, 1, dim)\n             last_hidden_state = last_hidden_state.view(-1, 1, last_hidden_state.shape[-1])\n \n@@ -2274,6 +2274,11 @@ def prepare_inputs_for_generation(\n             model_inputs[\"input_ids\"] = None\n             model_inputs[\"inputs_embeds\"] = inputs_embeds\n \n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n     def _update_model_kwargs_for_generation("
        },
        {
            "sha": "44fa05227ff831bbed49d229ad5a22e846977f14",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -602,7 +602,14 @@ def forward(\n \n     def prepare_inputs_for_generation(self, input_ids: torch.LongTensor, **kwargs) -> dict[str, Any]:\n         # Overwritten -- old model with reduced inputs\n-        return {\"input_ids\": input_ids}\n+        model_inputs = {\"input_ids\": input_ids}\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n+        return model_inputs\n \n \n @auto_docstring("
        },
        {
            "sha": "5e80ee4f0faa7d1e01afe085e33cce9216774cef",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -2006,14 +2006,24 @@ def prepare_inputs_for_generation(\n         if past_key_values is not None and past_key_values.get_seq_length() > 0:\n             input_ids = input_ids[:, -1:]\n         # first step, decoder_cached_states are empty\n-        return {\n+        model_inputs = {\n             \"input_ids\": input_ids,  # encoder_outputs is defined. input_ids not needed\n             \"attention_mask\": attention_mask,\n             \"head_mask\": head_mask,\n             \"past_key_values\": past_key_values,\n             \"use_cache\": use_cache,\n         }\n \n+        # Prophetnet does not support cache_position\n+        kwargs.pop(\"cache_position\", None)\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n+        return model_inputs\n+\n \n class ProphetNetDecoderWrapper(ProphetNetPreTrainedModel):\n     \"\"\""
        },
        {
            "sha": "990f21359bc062875f166d0a964c8da975338216",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -2345,14 +2345,22 @@ def prepare_inputs_for_generation(\n         if past_key_values is not None:\n             input_ids = input_ids[:, -1:]\n \n-        inputs_dict = {\n+        model_inputs = {\n             \"input_ids\": input_ids,\n             \"past_buckets_states\": past_key_values,\n             \"use_cache\": use_cache,\n             \"num_hashes\": num_hashes,\n         }\n \n-        return inputs_dict\n+        # Attention mask is computed on ReformerModel.forward()\n+        kwargs.pop(\"attention_mask\", None)\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                print(f\"Warning: {key} is not a recognized input.\")\n+                model_inputs[key] = value\n+\n+        return model_inputs\n \n     def _reorder_cache(self, past_key_values, beam_idx):\n         reord_past_buckets_states = []"
        },
        {
            "sha": "d86d4d0f8707bb88891fdf93a224347a06f8b11a",
            "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -719,6 +719,12 @@ def prepare_inputs_for_generation(self, input_ids, state=None, inputs_embeds=Non\n \n         model_inputs[\"state\"] = state\n         model_inputs[\"use_cache\"] = use_cache\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n     @auto_docstring"
        },
        {
            "sha": "a73b4a51cea43b95574d90c4720e3c55f74879ab",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -994,7 +994,18 @@ def prepare_inputs_for_generation(self, input_ids, **kwargs):\n             langs = torch.full_like(input_ids, lang_id)\n         else:\n             langs = None\n-        return {\"input_ids\": input_ids, \"langs\": langs}\n+        model_inputs = {\"input_ids\": input_ids, \"langs\": langs}\n+\n+        # They are calculated on the fly on XLMModel.forward()\n+        kwargs.pop(\"token_type_ids\", None)\n+        kwargs.pop(\"attention_mask\", None)\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n+        return model_inputs\n \n     @auto_docstring\n     def forward("
        },
        {
            "sha": "99b925015a71302e9754269508cb66898ee4e4d8",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1013,13 +1013,23 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attenti\n             if position_ids is not None:\n                 position_ids = position_ids[:, remove_prefix_length:]\n \n-        return {\n+        model_inputs = {\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n             \"position_ids\": position_ids,\n             \"past_key_values\": past_key_values,\n         }\n \n+        # They are calculated on the fly on XLMRobertaXLModel.forward()\n+        model_kwargs.pop(\"token_type_ids\", None)\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in model_kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n+        return model_inputs\n+\n \n @auto_docstring\n class XLMRobertaXLForMaskedLM(XLMRobertaXLPreTrainedModel):"
        },
        {
            "sha": "0c6b9f76eade59770df2323040abc8b54b92fb51",
            "filename": "src/transformers/models/xlnet/modeling_xlnet.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1472,7 +1472,7 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, use_mem\n         )\n         target_mapping[:, 0, -1] = 1.0\n \n-        inputs = {\n+        model_inputs = {\n             \"input_ids\": input_ids,\n             \"perm_mask\": perm_mask,\n             \"target_mapping\": target_mapping,\n@@ -1481,9 +1481,18 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, use_mem\n \n         # if past is defined in model kwargs then use it for faster decoding\n         if past_key_values:\n-            inputs[\"mems\"] = tuple(layer_past[:-offset, :, :] for layer_past in past_key_values)\n-\n-        return inputs\n+            model_inputs[\"mems\"] = tuple(layer_past[:-offset, :, :] for layer_past in past_key_values)\n+\n+        # Attention mask is computed on the fly on XLNetModel.forward()\n+        kwargs.pop(\"attention_mask\", None)\n+        # TODO: Ignoring use_cache should not happen, fixme.\n+        kwargs.pop(\"use_cache\", None)\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n+        return model_inputs\n \n     @auto_docstring\n     def forward("
        },
        {
            "sha": "7e2fce99768324673b2d6129031c8b1279b67ab1",
            "filename": "src/transformers/models/xlstm/modeling_xlstm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1556,6 +1556,12 @@ def prepare_inputs_for_generation(\n             model_inputs = {\"input_ids\": input_ids}\n \n         model_inputs.update({\"cache_params\": cache_params, \"use_cache\": use_cache})\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n     @can_return_tuple"
        },
        {
            "sha": "2f9edb1e113c0658b6a2102352dd1d519d9e64b5",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1185,6 +1185,12 @@ def prepare_inputs_for_generation(\n                 \"cache_position\": cache_position,\n             }\n         )\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n "
        },
        {
            "sha": "33e7e4b5a351317e21cb95cf435de31d552876d3",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1606,6 +1606,12 @@ def prepare_inputs_for_generation(\n                 \"cache_position\": cache_position,\n             }\n         )\n+\n+        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        for key, value in kwargs.items():\n+            if key not in model_inputs:\n+                model_inputs[key] = value\n+\n         return model_inputs\n \n "
        },
        {
            "sha": "3b828cd8313adea08478437d2d22cfb78d21898c",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -1803,6 +1803,41 @@ def test_inherits_generation_mixin(self):\n         for model_class in self.all_generative_model_classes:\n             self.assertTrue(\"GenerationMixin\" in str(model_class.__bases__))\n \n+    @pytest.mark.generate\n+    def test_prepare_inputs_for_generation_kwargs_forwards(self, **extra_kwargs):\n+        \"\"\"Tests that prepare_inputs_for_generation forwards arbitrary kwargs.\"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            config, _ = self.prepare_config_and_inputs_for_generate()\n+\n+            model = model_class(config).to(torch_device).eval()\n+\n+            input_ids = torch.tensor([[1, 2, 3], [4, 5, 6]]).to(torch_device)\n+\n+            input_args = {\n+                \"input_ids\": input_ids,\n+                \"cache_position\": torch.tensor([9]).to(torch_device),\n+                \"position_ids\": torch.tensor([[0, 1, 2], [0, 1, 2]]).to(torch_device),\n+            }\n+            arbitrary_kwargs = {\n+                \"output_attentions\": True,\n+                \"output_hidden_states\": True,\n+                \"custom_arg\": \"test_value\",\n+                \"numeric_arg\": 42,\n+            }\n+\n+            model_inputs = model.prepare_inputs_for_generation(**input_args, **arbitrary_kwargs, **extra_kwargs)\n+\n+            # Verify that input_ids has proper name\n+            if config.is_encoder_decoder:\n+                self.assertTrue(\"decoder_input_ids\" in model_inputs)\n+            else:\n+                self.assertTrue(\"input_ids\" in model_inputs)\n+\n+            # Verify that arbitrary kwargs are forwarded\n+            for key, value in arbitrary_kwargs.items():\n+                self.assertTrue(key in model_inputs)\n+                self.assertTrue(model_inputs[key] == value)\n+\n     def _test_attention_implementation(self, attn_implementation):\n         \"\"\"\n         Compares the output of generate with the eager attention implementation against other implementations."
        },
        {
            "sha": "5ac321c5a75393ba7694a0bb1504b7f030583db2",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -517,6 +517,10 @@ def test_generate_continue_from_past_key_values(self):\n                         )\n                     )\n \n+    @pytest.mark.generate\n+    def test_prepare_inputs_for_generation_kwargs_forwards(self):\n+        super().test_prepare_inputs_for_generation_kwargs_forwards(encoder_outputs=torch.randn(2, 2, 32))\n+\n     @unittest.skip(reason=\"Indirectly checked in Dia through the generate methods.\")\n     def test_hidden_states_output(self):\n         pass"
        },
        {
            "sha": "21f56e1bc56d5954570946c170c85eb2047ebae1",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=0957999f7f8b69e1e1d4ef2f940586f9d3cbd4f5",
            "patch": "@@ -868,6 +868,14 @@ def test_generate_continue_from_inputs_embeds(self):\n     def test_save_load(self):\n         super().test_save_load()\n \n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"Moshi requires setting `model.generated_audio_codes` in generate() before preparing inputs\")\n+    def test_prepare_inputs_for_generation_kwargs_forwards(self):\n+        # If in the future `model.generated_audio_codes` is not required, this test can be re-enabled\n+        super().test_prepare_inputs_for_generation_kwargs_forwards(\n+            last_hidden_state=torch.randn(2, 3, 32), kwargs_depth_decoder={}\n+        )\n+\n \n def place_dict_on_device(dict_to_place, device):\n     for key in dict_to_place:"
        }
    ],
    "stats": {
        "total": 274,
        "additions": 246,
        "deletions": 28
    }
}