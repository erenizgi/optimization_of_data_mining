{
    "author": "vasqu",
    "message": "[`VaultGemma`] Update expectations in integration tests (#40855)\n\n* fix tests\n\n* style",
    "sha": "3442b2f3008ebfd2d343ba5537928e969c73cff0",
    "files": [
        {
            "sha": "3d40eed91ac97d675f77fb4ef5aff9f1d65370f8",
            "filename": "tests/models/vaultgemma/test_modeling_vaultgemma.py",
            "status": "modified",
            "additions": 10,
            "deletions": 23,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/3442b2f3008ebfd2d343ba5537928e969c73cff0/tests%2Fmodels%2Fvaultgemma%2Ftest_modeling_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3442b2f3008ebfd2d343ba5537928e969c73cff0/tests%2Fmodels%2Fvaultgemma%2Ftest_modeling_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvaultgemma%2Ftest_modeling_vaultgemma.py?ref=3442b2f3008ebfd2d343ba5537928e969c73cff0",
            "patch": "@@ -107,8 +107,8 @@ def tearDown(self):\n     def test_model_bf16(self):\n         model_id = \"google/vaultgemma-1b\"\n         EXPECTED_TEXTS = [\n-            \"<bos>Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many\",\n-            \"<pad><pad><bos>Hi today I'm going to be talking about the history of the United States. The United States of America\",\n+            \"<bos>Hello I am doing a project on a 1990 240sx. I have a 1\",\n+            \"<pad><pad><bos>Hi today I am going to show you how to make a simple 3D model of a 3D\",\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, attn_implementation=\"eager\").to(\n@@ -128,13 +128,11 @@ def test_model_pipeline_bf16(self):\n         model_id = \"google/vaultgemma-1b\"\n         # EXPECTED_TEXTS should match the same non-pipeline test, minus the special tokens\n         EXPECTED_TEXTS = [\n-            \"Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many\",\n-            \"Hi today I'm going to be talking about the history of the United States. The United States of America\",\n+            \"Hello I am doing a project on a 1990 240sx. I have a 1\",\n+            \"Hi today I am going to show you how to make a simple 3D model of a 3D\",\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(\n-            model_id, dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n-        ).to(torch_device)\n+        model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16).to(torch_device)\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n \n@@ -158,18 +156,7 @@ def test_export_static_cache(self):\n         tokenizer = AutoTokenizer.from_pretrained(model_id, pad_token=\"</s>\", padding_side=\"right\")\n         EXPECTED_TEXT_COMPLETIONS = Expectations(\n             {\n-                (\"xpu\", 3): [\n-                    \"Hello I am doing a project for my school and I need to know how to make a program that will take a number\"\n-                ],\n-                (\"cuda\", 7): [\n-                    \"Hello I am doing a project for my school and I need to know how to make a program that will take a number\"\n-                ],\n-                (\"cuda\", 8): [\n-                    \"Hello I am doing a project for my class and I am having trouble with the code. I am trying to make a\"\n-                ],\n-                (\"rocm\", (9, 5)): [\n-                    \"Hello I am doing a project for my school and I need to know how to make a program that will take a number\"\n-                ],\n+                (\"cuda\", 8): [\"Hello I am doing a project on a 1990 240sx. I have a 1\"],\n             }\n         )\n         EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n@@ -239,8 +226,8 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n \n         model_id = \"google/vaultgemma-1b\"\n         EXPECTED_COMPLETIONS = [\n-            \" the people, the food, the culture, the history, the music, the art, the architecture\",\n-            \", green, yellow, orange, purple, pink, brown, black, white, gray, silver\",\n+            \" place pretty place pretty place. place pretty place pretty place. place pretty place pretty place. place pretty\",\n+            \", green, yellow, orange, purple, black, white, and gray.\\n\\nA list of\",\n         ]\n \n         input_text = [\n@@ -285,8 +272,8 @@ def test_generation_beyond_sliding_window_dynamic(self, attn_implementation: str\n \n         model_id = \"google/vaultgemma-1b\"\n         EXPECTED_COMPLETIONS = [\n-            \" the people, the food, the culture, the history, the music, the art, the architecture\",\n-            \", green, yellow, orange, purple, pink, brown, black, white, gray, silver\",\n+            \" place pretty place pretty place. place pretty place pretty place. place pretty place pretty place. place pretty\",\n+            \", green, yellow, orange, purple, black, white, and gray.\\n\\nA list of\",\n         ]\n \n         input_text = ["
        }
    ],
    "stats": {
        "total": 33,
        "additions": 10,
        "deletions": 23
    }
}