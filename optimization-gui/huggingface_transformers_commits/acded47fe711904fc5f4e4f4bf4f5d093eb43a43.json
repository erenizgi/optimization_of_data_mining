{
    "author": "cyr0930",
    "message": "[llava] one pixel is missing from padding when length is odd (#37819)\n\n* [fix] one pixel should be added when length is odd\n\n* [fix] add vision_aspect_ratio args & typo\n\n* [fix] style\n\n* [fix] do not fix fast file directly\n\n* [fix] convert using modular\n\n* remove duplicate codes\n\n* match unpad logic with pad logic\n\n* test odd-sized images for llava & aria\n\n* test unpad odd-sized padding for llava family\n\n* fix style\n\n* add kwarg to onvision modular\n\n* move vision_aspect_ratio from image_processor to processor\n(llava_onevision)",
    "sha": "acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
    "files": [
        {
            "sha": "2d3f45cf14cea3cdb9e24b015e43a4441878fd88",
            "filename": "setup.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -466,7 +466,12 @@ def run(self):\n     package_data={\"\": [\"**/*.cu\", \"**/*.cpp\", \"**/*.cuh\", \"**/*.h\", \"**/*.pyx\", \"py.typed\"]},\n     zip_safe=False,\n     extras_require=extras,\n-    entry_points={\"console_scripts\": [\"transformers=transformers.commands.transformers_cli:main\", \"transformers-cli=transformers.commands.transformers_cli:main_cli\"]},\n+    entry_points={\n+        \"console_scripts\": [\n+            \"transformers=transformers.commands.transformers_cli:main\",\n+            \"transformers-cli=transformers.commands.transformers_cli:main_cli\",\n+        ]\n+    },\n     python_requires=\">=3.9.0\",\n     install_requires=list(install_requires),\n     classifiers=["
        },
        {
            "sha": "44c6d40a4c69b85460765af2f26b73187144a7b2",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 6,
            "deletions": 24,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -18,12 +18,11 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import math\n from typing import Iterable, List, Optional, Tuple, Union\n \n import numpy as np\n \n-from ...image_processing_utils import BaseImageProcessor, BatchFeature, select_best_resolution\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_patch_output_size, select_best_resolution\n from ...image_transforms import PaddingMode, convert_to_rgb, pad, resize, to_channel_dimension_format\n from ...image_utils import (\n     ChannelDimension,\n@@ -71,23 +70,6 @@ def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> Li\n     return patches\n \n \n-def _get_patch_output_size(image, target_resolution, input_data_format):\n-    original_height, original_width = get_image_size(image, channel_dim=input_data_format)\n-    target_height, target_width = target_resolution\n-\n-    scale_w = target_width / original_width\n-    scale_h = target_height / original_height\n-\n-    if scale_w < scale_h:\n-        new_width = target_width\n-        new_height = min(math.ceil(original_height * scale_w), target_height)\n-    else:\n-        new_height = target_height\n-        new_width = min(math.ceil(original_width * scale_h), target_width)\n-\n-    return new_height, new_width\n-\n-\n class AriaImageProcessor(BaseImageProcessor):\n     \"\"\"\n     A vision processor for the Aria model that handles image preprocessing.\n@@ -375,7 +357,7 @@ def _resize_for_patching(\n         Returns:\n             np.array: The resized and padded image.\n         \"\"\"\n-        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n         # Resize the image\n         resized_image = resize(image, (new_height, new_width), resample=resample, input_data_format=input_data_format)\n@@ -389,12 +371,12 @@ def _pad_for_patching(\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n         target_height, target_width = target_resolution\n-        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n-        paste_x = (target_width - new_width) // 2\n-        paste_y = (target_height - new_height) // 2\n+        paste_x, r_x = divmod(target_width - new_width, 2)\n+        paste_y, r_y = divmod(target_height - new_height, 2)\n \n-        padded_image = self.pad(image, padding=((paste_y, paste_y), (paste_x, paste_x)))\n+        padded_image = self.pad(image, padding=((paste_y, paste_y + r_y), (paste_x, paste_x + r_x)))\n \n         return padded_image\n "
        },
        {
            "sha": "e4d063f78277385b9582b62cde306fdc4fe9ae2b",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 6,
            "deletions": 24,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -12,15 +12,14 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import math\n from typing import Dict, Iterable, List, Optional, Tuple, Union\n \n import numpy as np\n \n from ...activations import ACT2FN\n from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationMixin\n-from ...image_processing_utils import BaseImageProcessor, BatchFeature, select_best_resolution\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_patch_output_size, select_best_resolution\n from ...image_transforms import PaddingMode, convert_to_rgb, pad, resize, to_channel_dimension_format\n from ...image_utils import (\n     ChannelDimension,\n@@ -461,23 +460,6 @@ def forward(self, key_value_states: torch.Tensor, attn_mask: Optional[torch.Tens\n         return out\n \n \n-def _get_patch_output_size(image, target_resolution, input_data_format):\n-    original_height, original_width = get_image_size(image, channel_dim=input_data_format)\n-    target_height, target_width = target_resolution\n-\n-    scale_w = target_width / original_width\n-    scale_h = target_height / original_height\n-\n-    if scale_w < scale_h:\n-        new_width = target_width\n-        new_height = min(math.ceil(original_height * scale_w), target_height)\n-    else:\n-        new_height = target_height\n-        new_width = min(math.ceil(original_width * scale_h), target_width)\n-\n-    return new_height, new_width\n-\n-\n class AriaImageProcessor(BaseImageProcessor):\n     \"\"\"\n     A vision processor for the Aria model that handles image preprocessing.\n@@ -765,7 +747,7 @@ def _resize_for_patching(\n         Returns:\n             np.array: The resized and padded image.\n         \"\"\"\n-        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n         # Resize the image\n         resized_image = resize(image, (new_height, new_width), resample=resample, input_data_format=input_data_format)\n@@ -779,12 +761,12 @@ def _pad_for_patching(\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n         target_height, target_width = target_resolution\n-        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n-        paste_x = (target_width - new_width) // 2\n-        paste_y = (target_height - new_height) // 2\n+        paste_x, r_x = divmod(target_width - new_width, 2)\n+        paste_y, r_y = divmod(target_height - new_height, 2)\n \n-        padded_image = self.pad(image, padding=((paste_y, paste_y), (paste_x, paste_x)))\n+        padded_image = self.pad(image, padding=((paste_y, paste_y + r_y), (paste_x, paste_x + r_x)))\n \n         return padded_image\n "
        },
        {
            "sha": "bf8920e955abc3bcc18e81dc7292172504541808",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 12,
            "deletions": 24,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -14,12 +14,17 @@\n # limitations under the License.\n \"\"\"Image processor class for LLaVa-NeXT.\"\"\"\n \n-import math\n from typing import Dict, Iterable, List, Optional, Tuple, Union\n \n import numpy as np\n \n-from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict, select_best_resolution\n+from ...image_processing_utils import (\n+    BaseImageProcessor,\n+    BatchFeature,\n+    get_patch_output_size,\n+    get_size_dict,\n+    select_best_resolution,\n+)\n from ...image_transforms import (\n     PaddingMode,\n     convert_to_rgb,\n@@ -99,23 +104,6 @@ def expand_to_square(image: np.array, background_color, input_data_format) -> np\n         return result\n \n \n-def _get_patch_output_size(image, target_resolution, input_data_format):\n-    original_height, original_width = get_image_size(image, channel_dim=input_data_format)\n-    target_height, target_width = target_resolution\n-\n-    scale_w = target_width / original_width\n-    scale_h = target_height / original_height\n-\n-    if scale_w < scale_h:\n-        new_width = target_width\n-        new_height = min(math.ceil(original_height * scale_w), target_height)\n-    else:\n-        new_height = target_height\n-        new_width = min(math.ceil(original_width * scale_h), target_width)\n-\n-    return new_height, new_width\n-\n-\n class LlavaNextImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a LLaVa-NeXT image processor. Based on [`CLIPImageProcessor`] with incorporation of additional techniques\n@@ -429,7 +417,7 @@ def _resize_for_patching(\n         Returns:\n             np.array: The resized and padded image.\n         \"\"\"\n-        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n         # Resize the image\n         resized_image = resize(image, (new_height, new_width), resample=resample, input_data_format=input_data_format)\n@@ -443,12 +431,12 @@ def _pad_for_patching(\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n         target_height, target_width = target_resolution\n-        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n-        paste_x = (target_width - new_width) // 2\n-        paste_y = (target_height - new_height) // 2\n+        paste_x, r_x = divmod(target_width - new_width, 2)\n+        paste_y, r_y = divmod(target_height - new_height, 2)\n \n-        padded_image = self.pad(image, padding=((paste_y, paste_y), (paste_x, paste_x)))\n+        padded_image = self.pad(image, padding=((paste_y, paste_y + r_y), (paste_x, paste_x + r_x)))\n \n         return padded_image\n "
        },
        {
            "sha": "1118db06e079f773377b580d549a28ed7f115bcf",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -102,8 +102,8 @@ def __init__(self, **kwargs: Unpack[LlavaNextFastImageProcessorKwargs]):\n                 A list of possible resolutions to use for processing high resolution images. Each item in the list should be a tuple or list\n                 of the form `(height, width)`.\n             do_pad (`bool`, *optional*):\n-                    Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n-                    number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n+                Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n+                number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n         \"\"\",\n     )\n     def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaNextFastImageProcessorKwargs]) -> BatchFeature:\n@@ -164,10 +164,10 @@ def _pad_for_patching(\n         target_height, target_width = target_resolution\n         new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n-        paste_x = (target_width - new_width) // 2\n-        paste_y = (target_height - new_height) // 2\n+        paste_x, r_x = divmod(target_width - new_width, 2)\n+        paste_y, r_y = divmod(target_height - new_height, 2)\n \n-        padded_image = F.pad(image, padding=[paste_x, paste_y, paste_x, paste_y])\n+        padded_image = F.pad(image, padding=[paste_x, paste_y, paste_x + r_x, paste_y + r_y])\n \n         return padded_image\n "
        },
        {
            "sha": "fa7c04bdf40ed3abf4c9e950a6525cf02498b08d",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -139,14 +139,14 @@ def unpad_image(tensor, original_size):\n \n     if original_aspect_ratio > current_aspect_ratio:\n         scale_factor = current_width / original_width\n-        new_height = int(round(original_height * scale_factor, 7))\n-        padding = (current_height - new_height) // 2\n-        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n+        new_height = min(math.ceil(original_height * scale_factor), current_height)\n+        padding, r = divmod(current_height - new_height, 2)\n+        unpadded_tensor = tensor[:, padding : current_height - (padding + r), :]\n     else:\n         scale_factor = current_height / original_height\n-        new_width = int(round(original_width * scale_factor, 7))\n-        padding = (current_width - new_width) // 2\n-        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n+        new_width = min(math.ceil(original_width * scale_factor), current_width)\n+        padding, r = divmod(current_width - new_width, 2)\n+        unpadded_tensor = tensor[:, :, padding : current_width - (padding + r)]\n \n     return unpadded_tensor\n "
        },
        {
            "sha": "113441fd2aa700ceab6e4c0aa205cb8319f8c7ed",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -262,14 +262,14 @@ def unpad_image(tensor, original_size):\n \n     if original_aspect_ratio > current_aspect_ratio:\n         scale_factor = current_width / original_width\n-        new_height = int(round(original_height * scale_factor, 7))\n-        padding = (current_height - new_height) // 2\n-        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n+        new_height = min(math.ceil(original_height * scale_factor), current_height)\n+        padding, r = divmod(current_height - new_height, 2)\n+        unpadded_tensor = tensor[:, padding : current_height - (padding + r), :]\n     else:\n         scale_factor = current_height / original_height\n-        new_width = int(round(original_width * scale_factor, 7))\n-        padding = (current_width - new_width) // 2\n-        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n+        new_width = min(math.ceil(original_width * scale_factor), current_width)\n+        padding, r = divmod(current_width - new_width, 2)\n+        unpadded_tensor = tensor[:, :, padding : current_width - (padding + r)]\n \n     return unpadded_tensor\n "
        },
        {
            "sha": "502e9ac74c4407bee061ed8e78dc0ab4a5190de7",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 14,
            "deletions": 27,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -14,12 +14,17 @@\n # limitations under the License.\n \"\"\"Image processor class for LLaVa-Onevision.\"\"\"\n \n-import math\n from typing import Dict, Iterable, List, Optional, Tuple, Union\n \n import numpy as np\n \n-from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict, select_best_resolution\n+from ...image_processing_utils import (\n+    BaseImageProcessor,\n+    BatchFeature,\n+    get_patch_output_size,\n+    get_size_dict,\n+    select_best_resolution,\n+)\n from ...image_transforms import (\n     PaddingMode,\n     convert_to_rgb,\n@@ -99,24 +104,6 @@ def expand_to_square(image: np.array, background_color, input_data_format) -> np\n         return result\n \n \n-# Copied from transformers.models.llava_next.image_processing_llava_next._get_patch_output_size\n-def _get_patch_output_size(image, target_resolution, input_data_format):\n-    original_height, original_width = get_image_size(image, channel_dim=input_data_format)\n-    target_height, target_width = target_resolution\n-\n-    scale_w = target_width / original_width\n-    scale_h = target_height / original_height\n-\n-    if scale_w < scale_h:\n-        new_width = target_width\n-        new_height = min(math.ceil(original_height * scale_w), target_height)\n-    else:\n-        new_height = target_height\n-        new_width = min(math.ceil(original_width * scale_h), target_width)\n-\n-    return new_height, new_width\n-\n-\n class LlavaOnevisionImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a LLaVa-Onevision image processor. Based on [`SiglipImageProcessor`] with incorporation of processing each video frame.\n@@ -151,8 +138,8 @@ class LlavaOnevisionImageProcessor(BaseImageProcessor):\n             number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n         do_pad (`bool`, *optional*, defaults to `True`):\n-                Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n-                number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n+            Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n+            number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n         do_convert_rgb (`bool`, *optional*, defaults to `True`):\n             Whether to convert the image to RGB.\n     \"\"\"\n@@ -321,7 +308,7 @@ def _resize_for_patching(\n         Returns:\n             np.array: The resized and padded image.\n         \"\"\"\n-        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n         # Resize the image\n         resized_image = resize(image, (new_height, new_width), resample=resample, input_data_format=input_data_format)\n@@ -336,12 +323,12 @@ def _pad_for_patching(\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n         target_height, target_width = target_resolution\n-        new_height, new_width = _get_patch_output_size(image, target_resolution, input_data_format)\n+        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n-        paste_x = (target_width - new_width) // 2\n-        paste_y = (target_height - new_height) // 2\n+        paste_x, r_x = divmod(target_width - new_width, 2)\n+        paste_y, r_y = divmod(target_height - new_height, 2)\n \n-        padded_image = self.pad(image, padding=((paste_y, paste_y), (paste_x, paste_x)))\n+        padded_image = self.pad(image, padding=((paste_y, paste_y + r_y), (paste_x, paste_x + r_x)))\n \n         return padded_image\n "
        },
        {
            "sha": "f5e2da2cd9e37c2a7c6a4de03302ee00439ac2ce",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -84,8 +84,8 @@ def __init__(self, **kwargs: Unpack[LlavaOnevisionFastImageProcessorKwargs]):\n                 A list of possible resolutions to use for processing high resolution images. Each item in the list should be a tuple or list\n                 of the form `(height, width)`.\n             do_pad (`bool`, *optional*):\n-                    Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n-                    number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n+                Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n+                number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n         \"\"\",\n     )\n     def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaOnevisionFastImageProcessorKwargs]) -> BatchFeature:\n@@ -146,10 +146,10 @@ def _pad_for_patching(\n         target_height, target_width = target_resolution\n         new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n-        paste_x = (target_width - new_width) // 2\n-        paste_y = (target_height - new_height) // 2\n+        paste_x, r_x = divmod(target_width - new_width, 2)\n+        paste_y, r_y = divmod(target_height - new_height, 2)\n \n-        padded_image = F.pad(image, padding=[paste_x, paste_y, paste_x, paste_y])\n+        padded_image = F.pad(image, padding=[paste_x, paste_y, paste_x + r_x, paste_y + r_y])\n \n         return padded_image\n "
        },
        {
            "sha": "be67df9b3aff97dbd681442a085d53efc704432c",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -140,14 +140,14 @@ def unpad_image(tensor, original_size):\n \n     if original_aspect_ratio > current_aspect_ratio:\n         scale_factor = current_width / original_width\n-        new_height = int(round(original_height * scale_factor, 7))\n-        padding = (current_height - new_height) // 2\n-        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n+        new_height = min(math.ceil(original_height * scale_factor), current_height)\n+        padding, r = divmod(current_height - new_height, 2)\n+        unpadded_tensor = tensor[:, padding : current_height - (padding + r), :]\n     else:\n         scale_factor = current_height / original_height\n-        new_width = int(round(original_width * scale_factor, 7))\n-        padding = (current_width - new_width) // 2\n-        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n+        new_width = min(math.ceil(original_width * scale_factor), current_width)\n+        padding, r = divmod(current_width - new_width, 2)\n+        unpadded_tensor = tensor[:, :, padding : current_width - (padding + r)]\n \n     return unpadded_tensor\n "
        },
        {
            "sha": "a5acd5b320e080772fa29bff39fca4fe731759a1",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -70,6 +70,8 @@ class LlavaOnevisionProcessor(ProcessorMixin):\n             Special token used to denote image location.\n         video_token (`str`, *optional*, defaults to `\"<video>\"`):\n             Special token used to denote video location.\n+        vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n+            Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n@@ -79,6 +81,7 @@ class LlavaOnevisionProcessor(ProcessorMixin):\n         \"vision_feature_select_strategy\",\n         \"image_token\",\n         \"video_token\",\n+        \"vision_aspect_ratio\",\n     ]\n     image_processor_class = \"AutoImageProcessor\"\n     tokenizer_class = \"AutoTokenizer\"\n@@ -94,6 +97,7 @@ def __init__(\n         chat_template=None,\n         image_token=\"<image>\",\n         video_token=\"<video>\",\n+        vision_aspect_ratio=\"anyres_max_9\",\n         **kwargs,\n     ):\n         self.num_image_tokens = num_image_tokens\n@@ -110,6 +114,7 @@ def __init__(\n             if getattr(tokenizer, \"video_token_id\", None)\n             else tokenizer.convert_tokens_to_ids(self.video_token)\n         )\n+        self.vision_aspect_ratio = vision_aspect_ratio\n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n \n     def __call__(\n@@ -264,7 +269,8 @@ def _get_unpadded_features(self, height, width, patches_height, patches_width, s\n         unpadded_features = current_height * current_width\n         newline_features = current_height\n \n-        ratio = math.sqrt(current_height * current_width / (9 * patches_height**2))\n+        max_num_patches = int(self.vision_aspect_ratio.strip(\"anyres_max_\"))\n+        ratio = math.sqrt(current_height * current_width / (max_num_patches * patches_height**2))\n         if ratio > 1.1:\n             unpadded_features = int(current_height // ratio) * int(current_width // ratio)\n             newline_features = int(current_height // ratio)"
        },
        {
            "sha": "f366c6b028c0dcf2717149808998eeede5064604",
            "filename": "tests/models/aria/test_image_processing_aria.py",
            "status": "modified",
            "additions": 39,
            "deletions": 1,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/tests%2Fmodels%2Faria%2Ftest_image_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/tests%2Fmodels%2Faria%2Ftest_image_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_image_processing_aria.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -17,7 +17,7 @@\n \n import numpy as np\n \n-from transformers.image_utils import PILImageResampling\n+from transformers.image_utils import ChannelDimension, PILImageResampling\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n@@ -264,3 +264,41 @@ def test_call_pytorch(self):\n                 tuple(encoded_images.shape),\n                 (self.image_processor_tester.batch_size, *expected_output_image_shape),\n             )\n+\n+    def test_pad_for_patching(self):\n+        for image_processing_class in self.image_processor_list:\n+            if image_processing_class == self.fast_image_processing_class:\n+                numpify = False\n+                torchify = True\n+                input_data_format = image_processing_class.data_format\n+            else:\n+                numpify = True\n+                torchify = False\n+                input_data_format = ChannelDimension.LAST\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # Create odd-sized images\n+            image_input = self.image_processor_tester.prepare_image_inputs(\n+                batch_size=1,\n+                max_resolution=400,\n+                num_images=1,\n+                equal_resolution=True,\n+                numpify=numpify,\n+                torchify=torchify,\n+            )[0][0]\n+            self.assertIn(image_input.shape, [(3, 400, 400), (400, 400, 3)])\n+\n+            # Test odd-width\n+            image_shape = (400, 601)\n+            encoded_images = image_processing._pad_for_patching(image_input, image_shape, input_data_format)\n+            encoded_image_shape = (\n+                encoded_images.shape[:-1] if input_data_format == ChannelDimension.LAST else encoded_images.shape[1:]\n+            )\n+            self.assertEqual(encoded_image_shape, image_shape)\n+\n+            # Test odd-height\n+            image_shape = (503, 400)\n+            encoded_images = image_processing._pad_for_patching(image_input, image_shape, input_data_format)\n+            encoded_image_shape = (\n+                encoded_images.shape[:-1] if input_data_format == ChannelDimension.LAST else encoded_images.shape[1:]\n+            )\n+            self.assertEqual(encoded_image_shape, image_shape)"
        },
        {
            "sha": "f5a6cd17032bb4d79cefeb218dedf4e83b0f1bea",
            "filename": "tests/models/llava_next/test_image_processing_llava_next.py",
            "status": "modified",
            "additions": 36,
            "deletions": 1,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/tests%2Fmodels%2Fllava_next%2Ftest_image_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/tests%2Fmodels%2Fllava_next%2Ftest_image_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_image_processing_llava_next.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -16,7 +16,7 @@\n \n import numpy as np\n \n-from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n+from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, ChannelDimension\n from transformers.models.llava_next.image_processing_llava_next import select_best_resolution\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n@@ -230,3 +230,38 @@ def test_nested_input(self):\n \n             # Image processor should return same pixel values, independently of ipnut format\n             self.assertTrue((encoded_images_nested == encoded_images).all())\n+\n+    def test_pad_for_patching(self):\n+        for image_processing_class in self.image_processor_list:\n+            if image_processing_class == self.fast_image_processing_class:\n+                numpify = False\n+                torchify = True\n+                input_data_format = image_processing_class.data_format\n+            else:\n+                numpify = True\n+                torchify = False\n+                input_data_format = ChannelDimension.LAST\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # Create odd-sized images\n+            image_input = self.image_processor_tester.prepare_image_inputs(\n+                equal_resolution=True,\n+                numpify=numpify,\n+                torchify=torchify,\n+            )[0]\n+            self.assertIn(image_input.shape, [(3, 400, 400), (400, 400, 3)])\n+\n+            # Test odd-width\n+            image_shape = (400, 601)\n+            encoded_images = image_processing._pad_for_patching(image_input, image_shape, input_data_format)\n+            encoded_image_shape = (\n+                encoded_images.shape[:-1] if input_data_format == ChannelDimension.LAST else encoded_images.shape[1:]\n+            )\n+            self.assertEqual(encoded_image_shape, image_shape)\n+\n+            # Test odd-height\n+            image_shape = (503, 400)\n+            encoded_images = image_processing._pad_for_patching(image_input, image_shape, input_data_format)\n+            encoded_image_shape = (\n+                encoded_images.shape[:-1] if input_data_format == ChannelDimension.LAST else encoded_images.shape[1:]\n+            )\n+            self.assertEqual(encoded_image_shape, image_shape)"
        },
        {
            "sha": "95f5eaa083cc923841fc0e2bbaf6965edabd43be",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -48,7 +48,7 @@\n if is_torch_available():\n     import torch\n \n-    from transformers.models.llava_next.modeling_llava_next import image_size_to_num_patches\n+    from transformers.models.llava_next.modeling_llava_next import image_size_to_num_patches, unpad_image\n \n \n if is_vision_available():\n@@ -288,6 +288,19 @@ def test_mismatching_num_image_tokens(self):\n             image_sizes = torch.cat([image_sizes, image_sizes], dim=0)\n             _ = model(input_ids=input_ids, pixel_values=pixel_values, image_sizes=image_sizes)\n \n+    def test_unpad_image(self):\n+        original_size = (400, 400)\n+\n+        # Test case width is padded\n+        pixel_values = floats_tensor([3, 400, 601])\n+        unpadded_tensor = unpad_image(pixel_values, original_size)\n+        self.assertEqual(unpadded_tensor.shape[1:], original_size)\n+\n+        # Test case height is padded\n+        pixel_values = floats_tensor([3, 503, 400])\n+        unpadded_tensor = unpad_image(pixel_values, original_size)\n+        self.assertEqual(unpadded_tensor.shape[1:], original_size)\n+\n     @parameterized.expand(\n         [\n             (-1,),"
        },
        {
            "sha": "628611d689a2de8f6f22e8be18e04ef116d65479",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -47,6 +47,8 @@\n if is_torch_available():\n     import torch\n \n+    from transformers.models.llava_next_video.modeling_llava_next_video import unpad_image\n+\n \n if is_vision_available():\n     from PIL import Image\n@@ -302,6 +304,19 @@ def test_mismatching_num_image_tokens(self):\n             image_sizes = torch.cat([image_sizes, image_sizes], dim=0)\n             _ = model(input_ids=input_ids, pixel_values=pixel_values, image_sizes=image_sizes)\n \n+    def test_unpad_image(self):\n+        original_size = (400, 400)\n+\n+        # Test case width is padded\n+        pixel_values = floats_tensor([3, 400, 601])\n+        unpadded_tensor = unpad_image(pixel_values, original_size)\n+        self.assertEqual(unpadded_tensor.shape[1:], original_size)\n+\n+        # Test case height is padded\n+        pixel_values = floats_tensor([3, 503, 400])\n+        unpadded_tensor = unpad_image(pixel_values, original_size)\n+        self.assertEqual(unpadded_tensor.shape[1:], original_size)\n+\n     @parameterized.expand(\n         [\n             (-1,),"
        },
        {
            "sha": "8a411d99b234a078973bb7f40f275d2d12015d7e",
            "filename": "tests/models/llava_onevision/test_image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 36,
            "deletions": 1,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -16,7 +16,7 @@\n \n import numpy as np\n \n-from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n+from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, ChannelDimension\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n@@ -305,3 +305,38 @@ def test_call_pytorch_video(self):\n     )  # FIXME yoni\n     def test_can_compile_fast_image_processor(self):\n         pass\n+\n+    def test_pad_for_patching(self):\n+        for image_processing_class in self.image_processor_list:\n+            if image_processing_class == self.fast_image_processing_class:\n+                numpify = False\n+                torchify = True\n+                input_data_format = image_processing_class.data_format\n+            else:\n+                numpify = True\n+                torchify = False\n+                input_data_format = ChannelDimension.LAST\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # Create odd-sized images\n+            image_input = self.image_processor_tester.prepare_image_inputs(\n+                equal_resolution=True,\n+                numpify=numpify,\n+                torchify=torchify,\n+            )[0]\n+            self.assertIn(image_input.shape, [(3, 400, 400), (400, 400, 3)])\n+\n+            # Test odd-width\n+            image_shape = (400, 601)\n+            encoded_images = image_processing._pad_for_patching(image_input, image_shape, input_data_format)\n+            encoded_image_shape = (\n+                encoded_images.shape[:-1] if input_data_format == ChannelDimension.LAST else encoded_images.shape[1:]\n+            )\n+            self.assertEqual(encoded_image_shape, image_shape)\n+\n+            # Test odd-height\n+            image_shape = (503, 400)\n+            encoded_images = image_processing._pad_for_patching(image_input, image_shape, input_data_format)\n+            encoded_image_shape = (\n+                encoded_images.shape[:-1] if input_data_format == ChannelDimension.LAST else encoded_images.shape[1:]\n+            )\n+            self.assertEqual(encoded_image_shape, image_shape)"
        },
        {
            "sha": "4ec719d3c54f9a5f12e5fc888f565b1ed0150d5b",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acded47fe711904fc5f4e4f4bf4f5d093eb43a43/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=acded47fe711904fc5f4e4f4bf4f5d093eb43a43",
            "patch": "@@ -48,6 +48,8 @@\n if is_torch_available():\n     import torch\n \n+    from transformers.models.llava_onevision.modeling_llava_onevision import unpad_image\n+\n \n if is_vision_available():\n     from PIL import Image\n@@ -258,6 +260,19 @@ def test_inputs_embeds_matches_input_ids(self):\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n             torch.testing.assert_close(out_embeds, out_ids)\n \n+    def test_unpad_image(self):\n+        original_size = (400, 400)\n+\n+        # Test case width is padded\n+        pixel_values = floats_tensor([3, 400, 601])\n+        unpadded_tensor = unpad_image(pixel_values, original_size)\n+        self.assertEqual(unpadded_tensor.shape[1:], original_size)\n+\n+        # Test case height is padded\n+        pixel_values = floats_tensor([3, 503, 400])\n+        unpadded_tensor = unpad_image(pixel_values, original_size)\n+        self.assertEqual(unpadded_tensor.shape[1:], original_size)\n+\n     @parameterized.expand(\n         [\n             (-1,),"
        }
    ],
    "stats": {
        "total": 367,
        "additions": 234,
        "deletions": 133
    }
}