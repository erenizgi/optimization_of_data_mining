{
    "author": "VladOS95-cyber",
    "message": "Add Qwen2Moe GGUF loading support  (#33264)\n\n* update gguf doc, config and tensor mapping\r\n\r\n* add qwen2moe architecture support, GGUFQwen2MoeConverter and q4 unit tests\r\n\r\n* apply code style fixes\r\n\r\n* reformat files\r\n\r\n* assign GGUFQwen2Converter to qwen2_moe",
    "sha": "5d11de4a2f523c5202610d5a572c164744b7e66b",
    "files": [
        {
            "sha": "748987938e988eb16adb5398c79470fb5bd2e96c",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d11de4a2f523c5202610d5a572c164744b7e66b/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d11de4a2f523c5202610d5a572c164744b7e66b/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=5d11de4a2f523c5202610d5a572c164744b7e66b",
            "patch": "@@ -78,6 +78,7 @@ For now the supported model architectures are the architectures that have been v\n - LLaMa\n - Mistral\n - Qwen2\n+- Qwen2Moe\n \n ## Example usage\n "
        },
        {
            "sha": "447988a983d6cc17445231902d43c2aa7f50a782",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 37,
            "deletions": 1,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d11de4a2f523c5202610d5a572c164744b7e66b/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d11de4a2f523c5202610d5a572c164744b7e66b/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=5d11de4a2f523c5202610d5a572c164744b7e66b",
            "patch": "@@ -79,6 +79,21 @@\n         \"output.weight\": \"lm_head.weight\",\n         \"output_norm\": \"model.norm\",\n     },\n+    \"qwen2moe\": {\n+        \"token_embd\": \"model.embed_tokens\",\n+        \"blk\": \"model.layers\",\n+        \"ffn_up\": \"mlp.up_proj\",\n+        \"ffn_down\": \"mlp.down_proj\",\n+        \"ffn_gate\": \"mlp.gate_proj\",\n+        \"ffn_norm\": \"post_attention_layernorm\",\n+        \"attn_norm\": \"input_layernorm\",\n+        \"attn_q\": \"self_attn.q_proj\",\n+        \"attn_v\": \"self_attn.v_proj\",\n+        \"attn_k\": \"self_attn.k_proj\",\n+        \"attn_output\": \"self_attn.o_proj\",\n+        \"output.weight\": \"lm_head.weight\",\n+        \"output_norm\": \"model.norm\",\n+    },\n }\n \n \n@@ -123,6 +138,18 @@\n         \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n         \"vocab_size\": \"vocab_size\",\n     },\n+    \"qwen2moe\": {\n+        \"context_length\": \"max_position_embeddings\",\n+        \"block_count\": \"num_hidden_layers\",\n+        \"feed_forward_length\": \"intermediate_size\",\n+        \"embedding_length\": \"hidden_size\",\n+        \"rope.dimension_count\": None,\n+        \"rope.freq_base\": \"rope_theta\",\n+        \"attention.head_count\": \"num_attention_heads\",\n+        \"attention.head_count_kv\": \"num_key_value_heads\",\n+        \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n+        \"vocab_size\": \"vocab_size\",\n+    },\n     \"tokenizer\": {\n         \"ggml.bos_token_id\": \"bos_token_id\",\n         \"ggml.eos_token_id\": \"eos_token_id\",\n@@ -244,7 +271,15 @@ def tokenizer(self, proto):\n         bos_token = proto.tokens[proto.bos_token_id] if getattr(proto, \"bos_token_id\", None) is not None else None\n         eos_token = proto.tokens[proto.bos_token_id] if getattr(proto, \"eos_token_id\", None) is not None else None\n \n-        tokenizer = Tokenizer(BPE(bpe_vocab, merges, unk_token=unk_token, fuse_unk=True, byte_fallback=True))\n+        tokenizer = Tokenizer(\n+            BPE(\n+                bpe_vocab,\n+                merges,\n+                unk_token=unk_token,\n+                fuse_unk=True,\n+                byte_fallback=True,\n+            )\n+        )\n \n         special_tokens = []\n \n@@ -358,6 +393,7 @@ def converted(self) -> Tokenizer:\n GGUF_TO_FAST_CONVERTERS = {\n     \"llama\": GGUFLlamaConverter,\n     \"qwen2\": GGUFQwen2Converter,\n+    \"qwen2_moe\": GGUFQwen2Converter,\n }\n \n "
        },
        {
            "sha": "f7677a2db270e8f1d4962d5713caf693a2459e37",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d11de4a2f523c5202610d5a572c164744b7e66b/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d11de4a2f523c5202610d5a572c164744b7e66b/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=5d11de4a2f523c5202610d5a572c164744b7e66b",
            "patch": "@@ -96,6 +96,9 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n     else:\n         updated_architecture = architecture\n \n+    if \"qwen2moe\" in architecture:\n+        updated_architecture = \"qwen2_moe\"\n+\n     if architecture not in GGUF_SUPPORTED_ARCHITECTURES:\n         raise ValueError(f\"Architecture {architecture} not supported\")\n "
        },
        {
            "sha": "284ead6634bd2e83773383248c5adf0574239453",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 35,
            "deletions": 4,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d11de4a2f523c5202610d5a572c164744b7e66b/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d11de4a2f523c5202610d5a572c164744b7e66b/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=5d11de4a2f523c5202610d5a572c164744b7e66b",
            "patch": "@@ -16,7 +16,12 @@\n import unittest\n \n from transformers import AddedToken, AutoModelForCausalLM, AutoTokenizer\n-from transformers.testing_utils import require_gguf, require_torch_gpu, slow, torch_device\n+from transformers.testing_utils import (\n+    require_gguf,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils import is_torch_available\n \n \n@@ -33,6 +38,7 @@ class GgufIntegrationTests(unittest.TestCase):\n     imatrix_model_id = \"duyntnet/TinyLlama-1.1B-Chat-v1.0-imatrix-GGUF\"\n     mistral_model_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n     qwen2_model_id = \"Qwen/Qwen1.5-0.5B-Chat-GGUF\"\n+    qwen2_moe_model_id = \"RichardErkhov/Qwen_-_Qwen1.5-MoE-A2.7B-Chat-gguf\"\n     llama3_model_id = \"NousResearch/Meta-Llama-3-8B-GGUF\"\n     tinyllama_model_id = \"PenutChen/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n \n@@ -59,6 +65,7 @@ class GgufIntegrationTests(unittest.TestCase):\n \n     q4_0_mistral_model_id = \"mistral-7b-instruct-v0.2.Q4_0.gguf\"\n     q4_0_qwen2_model_id = \"qwen1_5-0_5b-chat-q4_0.gguf\"\n+    q4_0_qwen2_moe_model_id = \"Qwen1.5-MoE-A2.7B-Chat.Q4_0.gguf\"\n     q4_llama3_model_id = \"Meta-Llama-3-8B-Q4_K_M.gguf\"\n     f16_tinyllama_model_id = \"TinyLlama-1.1B-Chat-v1.0.FP16.gguf\"\n \n@@ -298,7 +305,10 @@ def test_f16(self):\n     def test_mistral_q4_0(self):\n         tokenizer = AutoTokenizer.from_pretrained(self.mistral_model_id, gguf_file=self.q4_0_mistral_model_id)\n         model = AutoModelForCausalLM.from_pretrained(\n-            self.mistral_model_id, gguf_file=self.q4_0_mistral_model_id, device_map=\"auto\", torch_dtype=torch.float16\n+            self.mistral_model_id,\n+            gguf_file=self.q4_0_mistral_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n         )\n \n         text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n@@ -310,7 +320,10 @@ def test_mistral_q4_0(self):\n     def test_qwen2_q4_0(self):\n         tokenizer = AutoTokenizer.from_pretrained(self.qwen2_model_id, gguf_file=self.q4_0_qwen2_model_id)\n         model = AutoModelForCausalLM.from_pretrained(\n-            self.qwen2_model_id, gguf_file=self.q4_0_qwen2_model_id, device_map=\"auto\", torch_dtype=torch.float16\n+            self.qwen2_model_id,\n+            gguf_file=self.q4_0_qwen2_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n         )\n \n         text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n@@ -319,6 +332,21 @@ def test_qwen2_q4_0(self):\n         EXPECTED_TEXT = \"Hello.jsoup\\n\\nI am a beginner\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n+    def test_qwen2_moe_q4_0(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.qwen2_moe_model_id, gguf_file=self.q4_0_qwen2_moe_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.qwen2_moe_model_id,\n+            gguf_file=self.q4_0_qwen2_moe_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello everyone, I'm a newbie here and would like\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n     def test_llama3_q4_0_tokenizer(self):\n         tokenizer = AutoTokenizer.from_pretrained(self.llama3_model_id, gguf_file=self.q4_llama3_model_id)\n         with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -331,7 +359,10 @@ def test_llama3_q4_0_tokenizer(self):\n     def test_llama3_q4_0(self):\n         tokenizer = AutoTokenizer.from_pretrained(self.llama3_model_id, gguf_file=self.q4_llama3_model_id)\n         model = AutoModelForCausalLM.from_pretrained(\n-            self.llama3_model_id, gguf_file=self.q4_llama3_model_id, device_map=\"auto\", torch_dtype=torch.float16\n+            self.llama3_model_id,\n+            gguf_file=self.q4_llama3_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n         )\n \n         text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)"
        }
    ],
    "stats": {
        "total": 81,
        "additions": 76,
        "deletions": 5
    }
}