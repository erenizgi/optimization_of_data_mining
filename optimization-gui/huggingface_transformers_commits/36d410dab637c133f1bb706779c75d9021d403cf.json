{
    "author": "MekkCyber",
    "message": "FEAT : Adding BitNet quantization method to HFQuantizer (#33410)\n\n* rebasing changes\r\n\r\n* fixing style\r\n\r\n* adding some doc to functions\r\n\r\n* remove bitblas\r\n\r\n* change dtype\r\n\r\n* fixing check_code_quality\r\n\r\n* fixing import order\r\n\r\n* adding doc to tree\r\n\r\n* Small update on BitLinear\r\n\r\n* adding some tests\r\n\r\n* sorting imports\r\n\r\n* small update\r\n\r\n* reformatting\r\n\r\n* reformatting\r\n\r\n* reformatting with ruff\r\n\r\n* adding assert\r\n\r\n* changes after review\r\n\r\n* update disk offloading\r\n\r\n* adapting after review\r\n\r\n* Update after review\r\n\r\n* add is_serializable back\r\n\r\n* fixing style\r\n\r\n* adding serialization test\r\n\r\n* make style\r\n\r\n* small updates after review",
    "sha": "36d410dab637c133f1bb706779c75d9021d403cf",
    "files": [
        {
            "sha": "02595f30db2893df3fe4b7c798bb287b398404e2",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/36d410dab637c133f1bb706779c75d9021d403cf/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/36d410dab637c133f1bb706779c75d9021d403cf/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=36d410dab637c133f1bb706779c75d9021d403cf",
            "patch": "@@ -179,6 +179,8 @@\n     title: Optimum\n   - local: quantization/torchao\n     title: TorchAO\n+  - local: quantization/bitnet\n+    title: BitNet\n   - local: quantization/compressed_tensors\n     title: compressed-tensors\n   - local: quantization/contribute"
        },
        {
            "sha": "3f44569697777b11e6d25424936c4e3167cd4f97",
            "filename": "docs/source/en/main_classes/quantization.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/36d410dab637c133f1bb706779c75d9021d403cf/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/36d410dab637c133f1bb706779c75d9021d403cf/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md?ref=36d410dab637c133f1bb706779c75d9021d403cf",
            "patch": "@@ -68,3 +68,7 @@ Learn how to quantize models in the [Quantization](../quantization) guide.\n ## TorchAoConfig\n \n [[autodoc]] TorchAoConfig\n+\n+## BitNetConfig\n+\n+[[autodoc]] BitNetConfig"
        },
        {
            "sha": "6bd65e8b53a476231187a5ff55e5f517c9a01987",
            "filename": "docs/source/en/quantization/bitnet.md",
            "status": "added",
            "additions": 75,
            "deletions": 0,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/36d410dab637c133f1bb706779c75d9021d403cf/docs%2Fsource%2Fen%2Fquantization%2Fbitnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/36d410dab637c133f1bb706779c75d9021d403cf/docs%2Fsource%2Fen%2Fquantization%2Fbitnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fbitnet.md?ref=36d410dab637c133f1bb706779c75d9021d403cf",
            "patch": "@@ -0,0 +1,75 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# BitNet\n+\n+[BitNet](https://arxiv.org/abs/2402.17764) replaces traditional Linear layers in Multi-Head Attention and Feed-Forward Networks with specialized layers called BitLinear with ternary (or binary in the older version) precision. The BitLinear layers introduced here quantize the weights using ternary precision (with values of -1, 0, and 1) and quantize the activations to 8-bit precision.\n+\n+\n+<figure style=\"text-align: center;\">\n+  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/bitlinear.png\" alt=\"Alt Text\" />\n+  <figcaption>The architecture of BitNet with BitLinear layers</figcaption>\n+</figure>\n+\n+During training, we start by quantizing the weights into ternary values, using symmetric per tensor quantization. First, we compute the average of the absolute values of the weight matrix and use this as a scale. We then divide the weights by the scale, round the values, constrain them between -1 and 1, and finally rescale them to continue in full precision.\n+\n+$$\n+scale_w = \\frac{1}{\\frac{1}{nm} \\sum_{ij} |W_{ij}|}\n+$$\n+\n+$$\n+W_q = \\text{clamp}_{[-1,1]}(\\text{round}(W*scale))\n+$$\n+\n+$$\n+W_{dequantized} = W_q*scale_w\n+$$\n+\n+Activations are then quantized to a specified bit-width (e.g., 8-bit) using [absmax](https://arxiv.org/pdf/2208.07339) quantization (symmetric per channel quantization). This involves scaling the activations into a range [−128,127[. The quantization formula is:\n+\n+$$\n+scale_x = \\frac{127}{|X|_{\\text{max}, \\, \\text{dim}=-1}}\n+$$\n+\n+$$\n+X_q = \\text{clamp}_{[-128,127]}(\\text{round}(X*scale))\n+$$\n+\n+$$\n+X_{dequantized} = X_q * scale_x\n+$$\n+\n+To learn more about how we trained, and fine-tuned bitnet models checkout the blogpost [here](https://huggingface.co/blog/1_58_llm_extreme_quantization)\n+\n+## Load a BitNet Model from the Hub\n+BitNet models can't be quantized on the fly—they need to be pre-trained or fine-tuned with the quantization applied (it's a Quantization aware training technique). Once trained, these models are already quantized and available as packed versions on the hub.\n+\n+A quantized model can be load : \n+\n+```py\n+from transformers import AutoModelForCausalLM\n+path = \"/path/to/model\"\n+model = AutoModelForCausalLM.from_pretrained(path, device_map=\"auto\")\n+```\n+## Pre-training / Fine-tuning a BitNet Model\n+\n+If you're looking to pre-train or fine-tune your own 1.58-bit model using Nanotron, check out this [PR](https://github.com/huggingface/nanotron/pull/180), all you need to get started is there !\n+\n+For fine-tuning, you'll need to convert the model from Hugging Face format to Nanotron format (which has some differences). You can find the conversion steps in this [PR](https://github.com/huggingface/nanotron/pull/174).\n+\n+## Kernels\n+\n+In our initial version, we chose to use `@torch.compile` to unpack the weights and perform the forward pass. It’s very straightforward to implement and delivers significant speed improvements. We plan to integrate additional optimized kernels in future versions.\n\\ No newline at end of file"
        },
        {
            "sha": "ab829c6894c0f93414b99a8bd1a750ce22300d75",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/36d410dab637c133f1bb706779c75d9021d403cf/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36d410dab637c133f1bb706779c75d9021d403cf/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=36d410dab637c133f1bb706779c75d9021d403cf",
            "patch": "@@ -968,6 +968,7 @@\n     \"utils.quantization_config\": [\n         \"AqlmConfig\",\n         \"AwqConfig\",\n+        \"BitNetConfig\",\n         \"BitsAndBytesConfig\",\n         \"CompressedTensorsConfig\",\n         \"EetqConfig\",\n@@ -5869,6 +5870,7 @@\n     from .utils.quantization_config import (\n         AqlmConfig,\n         AwqConfig,\n+        BitNetConfig,\n         BitsAndBytesConfig,\n         CompressedTensorsConfig,\n         EetqConfig,"
        },
        {
            "sha": "03e5b4802ec7d33fad5f5a5a1fa809f290e3ec1b",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/36d410dab637c133f1bb706779c75d9021d403cf/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36d410dab637c133f1bb706779c75d9021d403cf/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=36d410dab637c133f1bb706779c75d9021d403cf",
            "patch": "@@ -25,6 +25,12 @@\n         \"replace_quantization_scales\",\n         \"replace_with_awq_linear\",\n     ],\n+    \"bitnet\": [\n+        \"BitLinear\",\n+        \"pack_weights\",\n+        \"replace_with_bitnet_linear\",\n+        \"unpack_weights\",\n+    ],\n     \"bitsandbytes\": [\n         \"dequantize_and_replace\",\n         \"get_keys_to_not_convert\",\n@@ -120,6 +126,12 @@\n         replace_quantization_scales,\n         replace_with_awq_linear,\n     )\n+    from .bitnet import (\n+        BitLinear,\n+        pack_weights,\n+        replace_with_bitnet_linear,\n+        unpack_weights,\n+    )\n     from .bitsandbytes import (\n         dequantize_and_replace,\n         get_keys_to_not_convert,"
        },
        {
            "sha": "3386bdcb43b27c2d03e9be96216d2de814f84386",
            "filename": "src/transformers/integrations/bitnet.py",
            "status": "added",
            "additions": 286,
            "deletions": 0,
            "changes": 286,
            "blob_url": "https://github.com/huggingface/transformers/blob/36d410dab637c133f1bb706779c75d9021d403cf/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36d410dab637c133f1bb706779c75d9021d403cf/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitnet.py?ref=36d410dab637c133f1bb706779c75d9021d403cf",
            "patch": "@@ -0,0 +1,286 @@\n+from ..utils import is_accelerate_available, is_torch_available, logging\n+\n+\n+if is_accelerate_available():\n+    from accelerate import init_empty_weights\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn as nn\n+    import torch.nn.functional as F\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# the weights are ternary so can be represented with 2 bits, and they are packed in uint8 tensors, hence the number of values per item is 4\n+VALUES_PER_ITEM = 4\n+\n+\n+def pack_weights(quantized_weights: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    Packs a tensor of quantized weights into a compact format using 2 bits per value.\n+\n+    Parameters:\n+    -----------\n+    quantized_weights : torch.Tensor\n+        A tensor containing ternary quantized weights with values in {-1, 0, 1}. These values are adjusted to\n+        {0, 1, 2} before being packed.\n+\n+    Returns:\n+    --------\n+    torch.Tensor\n+        A packed tensor where each element stores 4 quantized values (each using 2 bits) in an 8-bit format.\n+    \"\"\"\n+\n+    original_shape = quantized_weights.shape\n+\n+    row_dim = (original_shape[0] + VALUES_PER_ITEM - 1) // VALUES_PER_ITEM\n+\n+    if len(original_shape) == 1:\n+        packed_tensor_shape = (row_dim,)\n+    else:\n+        packed_tensor_shape = (row_dim, *original_shape[1:])\n+\n+    quantized_weights += 1\n+    packed = torch.zeros(packed_tensor_shape, device=quantized_weights.device, dtype=torch.uint8)\n+    unpacked = quantized_weights.to(torch.uint8)\n+\n+    it = min(VALUES_PER_ITEM, (original_shape[0] // row_dim) + 1)\n+    for i in range(it):\n+        start = i * row_dim\n+        end = min(start + row_dim, original_shape[0])\n+        packed[: (end - start)] |= unpacked[start:end] << 2 * i\n+\n+    return packed\n+\n+\n+@torch.compile\n+def unpack_weights(packed: torch.Tensor, dtype: torch.dtype) -> torch.Tensor:\n+    \"\"\"\n+    Unpacks a tensor of quantized weights that were stored in a packed format using 2 bits per value.\n+\n+    Parameters:\n+    -----------\n+    packed : torch.Tensor\n+        A tensor containing packed weights where each element represents 4 quantized values (using 2 bits per value).\n+    dtype : torch.dtype\n+        The dtype of the returned Tensor\n+    Returns:\n+    --------\n+    torch.Tensor\n+        A tensor of unpacked weights, where each value is converted from its packed 2-bit representation.\n+\n+    Example:\n+    --------\n+    packed = torch.tensor([[0b10100001, 0b00011000],\n+                           [0b10010000, 0b00001010]], dtype=torch.uint8)\n+\n+    # Unpack the values\n+    unpacked = unpack_weights(packed)\n+\n+    # Resulting unpacked tensor\n+    print(unpacked)\n+    # Output: tensor([[ 0, -1],\n+                      [-1,  1],\n+                      [-1,  1],\n+                      [-1,  1],\n+                      [ 1,  0],\n+                      [ 0, -1],\n+                      [ 1, -1],\n+                      [ 1, -1]])\n+\n+    Explanation of the example:\n+    ---------------------------\n+    Let's take the first value for example 0b10100001, we we will only focus on the first column,\n+    because every element is unpacked across the first dimension\n+    - First 2 bits: `01` → 0 at [0][0]\n+    - Second 2 bits: `00` → -1 at [0][2]\n+    - Third 2 bits: `10` → 1 at [0][4]\n+    - Fourth 2 bits: `10` → 1 at [0][6]\n+    the second value of the same row (0b10010000) will give the values for [0][1], [0][3], [0][5], [0][7]\n+\n+    We subtract 1 because during the packing process, it's easier to work with values like 0, 1, and 2. To make this possible,\n+    we add 1 to the original ternary weights (which are typically -1, 0, and 1) when packing them. When unpacking, we reverse\n+    this by subtracting 1 to restore the original ternary values.\n+    \"\"\"\n+    packed_shape = packed.shape\n+\n+    if len(packed_shape) == 1:\n+        original_row_dim = packed_shape[0] * VALUES_PER_ITEM\n+        unpacked_shape = (original_row_dim,)\n+    else:\n+        original_row_dim = packed_shape[0] * VALUES_PER_ITEM\n+        unpacked_shape = (original_row_dim, *packed_shape[1:])\n+\n+    unpacked = torch.zeros(unpacked_shape, device=packed.device, dtype=torch.uint8)\n+\n+    for i in range(VALUES_PER_ITEM):\n+        start = i * packed_shape[0]\n+        end = start + packed_shape[0]\n+        mask = 3 << (2 * i)\n+        unpacked[start:end] = (packed & mask) >> (2 * i)\n+\n+    return unpacked.to(dtype) - 1\n+\n+\n+class BitLinear(nn.Module):\n+    def __init__(self, in_features: int, out_features: int, bias: bool, device=None, dtype=None):\n+        super().__init__()\n+        self.dtype = dtype\n+        self.register_buffer(\n+            \"weight\",\n+            torch.zeros(\n+                (out_features // VALUES_PER_ITEM, in_features),\n+                dtype=torch.uint8,\n+                device=device,\n+            ),\n+        )\n+        self.register_buffer(\n+            \"weight_scale\",\n+            torch.ones(\n+                (1),\n+                dtype=dtype,\n+                device=device,\n+            ),\n+        )\n+        if bias:\n+            self.register_buffer(\"bias\", torch.zeros((out_features), dtype=dtype, device=device))\n+        else:\n+            self.bias = None\n+\n+    @torch.compile\n+    def activation_quant(self, input, num_bits=8):\n+        \"\"\"\n+        Activation function : Performs symmetric, per-token quantization on the input activations.\n+        Parameters:\n+        -----------\n+        x : torch.Tensor\n+            Input activations to be quantized.\n+        num_bits : int, optional (default=8)\n+            Number of bits to use for quantization, determining the quantization range.\n+\n+        Returns:\n+        --------\n+        result : torch.Tensor\n+            Quantized activation tensor, with values mapped to an `int8` range.\n+        scale : torch.Tensor\n+            The per-channel scaling factors used to quantize the tensor.\n+        \"\"\"\n+        Qn = -(2 ** (num_bits - 1))\n+        Qp = 2 ** (num_bits - 1) - 1\n+        scale = Qp / input.abs().max(dim=-1, keepdim=True).values.clamp(min=1e-5)\n+        result = (input * scale).round().clamp(Qn, Qp)\n+        return result.to(torch.int8), scale\n+\n+    @torch.compile\n+    def post_quant_process(self, input, input_scale, weight_scale):\n+        out = input / (input_scale * weight_scale)\n+        return out\n+\n+    def forward(self, input):\n+        w = self.weight\n+        w_quant = unpack_weights(w, dtype=self.dtype)\n+        input_quant, input_scale = self.activation_quant(input)\n+        y = F.linear(input_quant.to(self.dtype), w_quant)\n+        y = self.post_quant_process(y, self.weight_scale, input_scale)\n+        if self.bias is not None:\n+            y += self.bias.view(1, -1).expand_as(y)\n+        return y\n+\n+\n+def _replace_with_bitnet_linear(\n+    model,\n+    modules_to_not_convert=None,\n+    current_key_name=None,\n+    quantization_config=None,\n+    has_been_replaced=False,\n+    pre_quantized=False,\n+):\n+    \"\"\"\n+    Private method that wraps the recursion for module replacement.\n+\n+    Returns the converted model and a boolean that indicates if the conversion has been successfull or not.\n+    \"\"\"\n+\n+    if current_key_name is None:\n+        current_key_name = []\n+\n+    for name, module in model.named_children():\n+        if current_key_name is None:\n+            current_key_name = []\n+        current_key_name.append(name)\n+\n+        # Check if the current key is not in the `modules_to_not_convert`\n+        if not any(key in \".\".join(current_key_name) for key in modules_to_not_convert):\n+            with init_empty_weights():\n+                if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n+                    in_features = module.in_features\n+                    out_features = module.out_features\n+                    model._modules[name] = BitLinear(\n+                        in_features=in_features,\n+                        out_features=out_features,\n+                        bias=module.bias is not None,\n+                        device=module.weight.device,\n+                        dtype=module.weight.dtype,\n+                    )\n+                    has_been_replaced = True\n+                    model._modules[name].requires_grad_(False)\n+\n+        if len(list(module.children())) > 0:\n+            _, has_been_replaced = _replace_with_bitnet_linear(\n+                module,\n+                modules_to_not_convert=modules_to_not_convert,\n+                current_key_name=current_key_name,\n+                quantization_config=quantization_config,\n+                has_been_replaced=has_been_replaced,\n+            )\n+        # Remove the last key for recursion\n+        current_key_name.pop(-1)\n+    return model, has_been_replaced\n+\n+\n+def replace_with_bitnet_linear(\n+    model,\n+    modules_to_not_convert=None,\n+    current_key_name=None,\n+    quantization_config=None,\n+    pre_quantized=False,\n+):\n+    \"\"\"\n+    A helper function to replace all `torch.nn.Linear` modules by `BitLinear158` modules`.\n+\n+    The function will be run recursively and replace all `torch.nn.Linear` modules except for the `lm_head` that should\n+    be kept as a `torch.nn.Linear` module. The replacement is done under `init_empty_weights` context manager so no\n+    CPU/GPU memory is required to run this function. Each weight will be quantized along the channel.\n+\n+    Parameters:\n+        model (`torch.nn.Module`):\n+            Input model or `torch.nn.Module` as the function is run recursively.\n+        modules_to_not_convert (`List[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n+            Names of the modules to not convert in `EetqLinear`. In practice we keep the `lm_head` in full precision\n+            for numerical stability reasons.\n+        current_key_name (`List[`str`]`, *optional*):\n+            An array to track the current key of the recursion. This is used to check whether the current key (part of\n+            it) is not in the list of modules to not convert (for instances modules that are offloaded to `cpu` or\n+            `disk`).\n+    \"\"\"\n+    modules_to_not_convert = [\"lm_head\"] if modules_to_not_convert is None else modules_to_not_convert\n+    if quantization_config and quantization_config.modules_to_not_convert is not None:\n+        modules_to_not_convert.extend(quantization_config.modules_to_not_convert)\n+    modules_to_not_convert = list(set(modules_to_not_convert))\n+    model, has_been_replaced = _replace_with_bitnet_linear(\n+        model,\n+        modules_to_not_convert,\n+        current_key_name,\n+        quantization_config,\n+        pre_quantized=pre_quantized,\n+    )\n+\n+    if not has_been_replaced:\n+        logger.warning(\n+            \"You are loading your model using bitnet but no linear modules were found in your model.\"\n+            \" Please double check your model architecture, or submit an issue on github if you think this is\"\n+            \" a bug.\"\n+        )\n+\n+    return model"
        },
        {
            "sha": "38bebd2d8410e476c7d588dd8e996c4ffb64c66d",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/36d410dab637c133f1bb706779c75d9021d403cf/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36d410dab637c133f1bb706779c75d9021d403cf/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=36d410dab637c133f1bb706779c75d9021d403cf",
            "patch": "@@ -18,6 +18,7 @@\n from ..utils.quantization_config import (\n     AqlmConfig,\n     AwqConfig,\n+    BitNetConfig,\n     BitsAndBytesConfig,\n     CompressedTensorsConfig,\n     EetqConfig,\n@@ -31,6 +32,7 @@\n )\n from .quantizer_aqlm import AqlmHfQuantizer\n from .quantizer_awq import AwqQuantizer\n+from .quantizer_bitnet import BitNetHfQuantizer\n from .quantizer_bnb_4bit import Bnb4BitHfQuantizer\n from .quantizer_bnb_8bit import Bnb8BitHfQuantizer\n from .quantizer_compressed_tensors import CompressedTensorsHfQuantizer\n@@ -54,6 +56,7 @@\n     \"compressed-tensors\": CompressedTensorsHfQuantizer,\n     \"fbgemm_fp8\": FbgemmFp8HfQuantizer,\n     \"torchao\": TorchAoHfQuantizer,\n+    \"bitnet\": BitNetHfQuantizer,\n }\n \n AUTO_QUANTIZATION_CONFIG_MAPPING = {\n@@ -68,6 +71,7 @@\n     \"compressed-tensors\": CompressedTensorsConfig,\n     \"fbgemm_fp8\": FbgemmFp8Config,\n     \"torchao\": TorchAoConfig,\n+    \"bitnet\": BitNetConfig,\n }\n \n "
        },
        {
            "sha": "3607caa00733ccb6ce67d313bf41f52bcad4657a",
            "filename": "src/transformers/quantizers/quantizer_bitnet.py",
            "status": "added",
            "additions": 115,
            "deletions": 0,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/36d410dab637c133f1bb706779c75d9021d403cf/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36d410dab637c133f1bb706779c75d9021d403cf/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py?ref=36d410dab637c133f1bb706779c75d9021d403cf",
            "patch": "@@ -0,0 +1,115 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING, Dict, List, Union\n+\n+from .base import HfQuantizer\n+\n+\n+if TYPE_CHECKING:\n+    from ..modeling_utils import PreTrainedModel\n+\n+from ..utils import is_accelerate_available, is_torch_available, logging\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class BitNetHfQuantizer(HfQuantizer):\n+    \"\"\"\n+    1.58-bit quantization from BitNet quantization method:\n+    Before loading: it converts the linear layers into BitLinear layers during loading.\n+\n+    Checkout the paper introducing this method : https://arxiv.org/pdf/2402.17764\n+    \"\"\"\n+\n+    requires_parameters_quantization = False\n+    requires_calibration = True\n+\n+    required_packages = [\"accelerate\"]\n+\n+    def __init__(self, quantization_config, **kwargs):\n+        super().__init__(quantization_config, **kwargs)\n+        self.quantization_config = quantization_config\n+\n+    def validate_environment(self, *args, **kwargs):\n+        if not is_accelerate_available():\n+            raise ImportError(\"Loading a BitNet quantized model requires accelerate (`pip install accelerate`)\")\n+\n+        if kwargs.get(\"from_tf\", False) or kwargs.get(\"from_flax\", False):\n+            raise ValueError(\n+                \"Loading ternary weights from tf/flax is currently not supported, please make\"\n+                \" sure the weights are in PyTorch format.\"\n+            )\n+\n+        if not torch.cuda.is_available():\n+            logger.warning_once(\n+                \"You don't have a GPU available to load the model, the inference will be slow because of weight unpacking\"\n+            )\n+            return\n+\n+        device_map = kwargs.get(\"device_map\", None)\n+        if device_map is None:\n+            logger.warning_once(\n+                \"You have loaded a BitNet model on CPU and have a CUDA device available, make sure to set \"\n+                \"your model on a GPU device in order to run your model.\"\n+            )\n+        elif device_map is not None:\n+            if isinstance(device_map, dict) and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n+                raise ValueError(\n+                    \"You are attempting to load a BitNet model with a device_map that contains a CPU or disk device.\"\n+                    \"This is not supported. Please remove the CPU or disk device from the device_map.\"\n+                )\n+\n+    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n+        return model\n+\n+    def _process_model_before_weight_loading(\n+        self,\n+        model: \"PreTrainedModel\",\n+        device_map,\n+        keep_in_fp32_modules: List[str] = [],\n+        **kwargs,\n+    ):\n+        from ..integrations import get_keys_to_not_convert, replace_with_bitnet_linear\n+\n+        self.modules_to_not_convert = get_keys_to_not_convert(model)\n+\n+        if self.quantization_config.modules_to_not_convert is not None:\n+            self.modules_to_not_convert.extend(self.quantization_config.modules_to_not_convert)\n+\n+        model = replace_with_bitnet_linear(\n+            model,\n+            modules_to_not_convert=self.modules_to_not_convert,\n+            quantization_config=self.quantization_config,\n+            pre_quantized=self.pre_quantized,\n+        )\n+\n+    def adjust_max_memory(self, max_memory: Dict[str, Union[int, str]]) -> Dict[str, Union[int, str]]:\n+        max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n+        return max_memory\n+\n+    def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n+        target_dtype = torch.int8\n+        return target_dtype\n+\n+    def is_serializable(self, safe_serialization=None):\n+        return True\n+\n+    @property\n+    def is_trainable(self) -> bool:\n+        return False"
        },
        {
            "sha": "026a206679857435f28b41e519bebd360cbe1d91",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 20,
            "deletions": 1,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/36d410dab637c133f1bb706779c75d9021d403cf/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36d410dab637c133f1bb706779c75d9021d403cf/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=36d410dab637c133f1bb706779c75d9021d403cf",
            "patch": "@@ -45,6 +45,7 @@ class QuantizationMethod(str, Enum):\n     COMPRESSED_TENSORS = \"compressed-tensors\"\n     FBGEMM_FP8 = \"fbgemm_fp8\"\n     TORCHAO = \"torchao\"\n+    BITNET = \"bitnet\"\n \n \n class AWQLinearVersion(str, Enum):\n@@ -1308,4 +1309,22 @@ def get_apply_tensor_subclass(self):\n         return _STR_TO_METHOD[self.quant_type](**self.quant_type_kwargs)\n \n     def __repr__(self):\n-        return f\"{self.quant_type}({', '.join(str(k) + '=' + str(v) for k, v in self.quant_type_kwargs.items())})\"\n+        return f\"{self.quant_type}({', '.join(str(k) + '=' + str(v) for k, v in self.kwargs.items())})\"\n+\n+\n+@dataclass\n+class BitNetConfig(QuantizationConfigMixin):\n+    def __init__(\n+        self,\n+        modules_to_not_convert: Optional[List] = None,\n+        **kwargs,\n+    ):\n+        self.quant_method = QuantizationMethod.BITNET\n+        self.modules_to_not_convert = modules_to_not_convert\n+        self.post_init()\n+\n+    def post_init(self):\n+        r\"\"\"\n+        Safety checker that arguments are correct\n+        \"\"\"\n+        pass"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/quantization/bitnet_integration/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/36d410dab637c133f1bb706779c75d9021d403cf/tests%2Fquantization%2Fbitnet_integration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36d410dab637c133f1bb706779c75d9021d403cf/tests%2Fquantization%2Fbitnet_integration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbitnet_integration%2F__init__.py?ref=36d410dab637c133f1bb706779c75d9021d403cf"
        },
        {
            "sha": "ef71cc82dbf57f9f4b1a2600747d6ed42ba3396c",
            "filename": "tests/quantization/bitnet_integration/test_bitnet.py",
            "status": "added",
            "additions": 225,
            "deletions": 0,
            "changes": 225,
            "blob_url": "https://github.com/huggingface/transformers/blob/36d410dab637c133f1bb706779c75d9021d403cf/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36d410dab637c133f1bb706779c75d9021d403cf/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py?ref=36d410dab637c133f1bb706779c75d9021d403cf",
            "patch": "@@ -0,0 +1,225 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import gc\n+import unittest\n+\n+from transformers import (\n+    AutoConfig,\n+    AutoModelForCausalLM,\n+    AutoTokenizer,\n+    BitNetConfig,\n+    OPTForCausalLM,\n+)\n+from transformers.testing_utils import (\n+    require_accelerate,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_accelerate_available, is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_accelerate_available():\n+    from accelerate import init_empty_weights\n+\n+\n+@require_torch_gpu\n+class BitNetConfigTest(unittest.TestCase):\n+    def test_to_dict(self):\n+        \"\"\"\n+        Simple test that checks if one uses a config and converts it to a dict, the dict is the same as the config object\n+        \"\"\"\n+        quantization_config = BitNetConfig()\n+        config_to_dict = quantization_config.to_dict()\n+\n+        for key in config_to_dict:\n+            self.assertEqual(getattr(quantization_config, key), config_to_dict[key])\n+\n+\n+@slow\n+@require_torch_gpu\n+@require_accelerate\n+class BitNetTest(unittest.TestCase):\n+    model_name = \"HF1BitLLM/Llama3-8B-1.58-100B-tokens\"\n+    device = \"cuda\"\n+\n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        \"\"\"\n+        Load the model\n+        \"\"\"\n+        cls.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n+        cls.quantized_model = AutoModelForCausalLM.from_pretrained(cls.model_name, device_map=cls.device)\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+        gc.collect()\n+\n+    def test_replace_with_bitlinear(self):\n+        from transformers.integrations import BitLinear, replace_with_bitnet_linear\n+\n+        model_id = \"facebook/opt-350m\"\n+        config = AutoConfig.from_pretrained(model_id)\n+\n+        with init_empty_weights():\n+            model = OPTForCausalLM(config)\n+\n+        nb_linears = 0\n+        for module in model.modules():\n+            if isinstance(module, torch.nn.Linear):\n+                nb_linears += 1\n+\n+        model = replace_with_bitnet_linear(model)\n+        nb_bitnet_linear = 0\n+        for module in model.modules():\n+            if isinstance(module, BitLinear):\n+                nb_bitnet_linear += 1\n+\n+        self.assertEqual(nb_linears - 1, nb_bitnet_linear)\n+\n+    def test_quantized_model(self, quantized_model, tokenizer):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly\n+        \"\"\"\n+        input_text = \"What are we having for dinner?\"\n+        expected_output = \"What are we having for dinner? What are we going to do for fun this weekend?\"\n+        input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=11, do_sample=False)\n+        self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), expected_output)\n+\n+    def test_packing_unpacking(self):\n+        \"\"\"\n+        Simple test the packing and unpacking logic\n+        \"\"\"\n+\n+        from transformers.integrations import pack_weights, unpack_weights\n+\n+        u = torch.randint(0, 255, (1024, 1024), dtype=torch.uint8)\n+        unpacked_u = unpack_weights(u, dtype=torch.bfloat16)\n+        self.assertEqual(pack_weights(unpacked_u), u)\n+\n+    def test_activation_quant(self):\n+        \"\"\"\n+        test the activation function behaviour\n+        \"\"\"\n+\n+        from transformers.integrations import BitLinear\n+\n+        layer = BitLinear(in_features=4, out_features=2, bias=False, dtype=torch.float32)\n+        layer.to(self.device)\n+\n+        input_tensor = torch.tensor([[1.0, -1.0, -1.0, 1.0], [1.0, -1.0, 1.0, 1.0]], dtype=torch.float32).to(\n+            torch_device\n+        )\n+\n+        # Quantize the input tensor\n+        quantized_tensor, scale = layer.activation_quant(input_tensor)\n+\n+        # Verify the output quantized tensor\n+        self.assertEqual(quantized_tensor, input_tensor)\n+\n+        # Verify the scale tensor\n+        self.assertEqual(scale, 127)\n+\n+    def test_weights_dtype(self):\n+        \"\"\"\n+        test the weights dtype after loading\n+        \"\"\"\n+\n+        self_attn_q = self.quantized_model.model.layers[0].self_attn.q_proj.weight\n+        self_attn_k = self.quantized_model.model.layers[0].self_attn.k_proj.weight\n+        self_attn_v = self.quantized_model.model.layers[0].self_attn.v_proj.weight\n+        self_attn_o = self.quantized_model.model.layers[0].self_attn.o_proj.weight\n+        mlp_gate = self.quantized_model.model.layers[0].mlp.gate_proj.weight\n+        mlp_up = self.quantized_model.model.layers[0].mlp.up_proj.weight\n+        mlp_down = self.quantized_model.model.layers[0].mlp.down_proj.weight\n+\n+        self.assertEqual(self_attn_q.dtype, torch.uint8)\n+        self.assertEqual(self_attn_k.dtype, torch.uint8)\n+        self.assertEqual(self_attn_v.dtype, torch.uint8)\n+        self.assertEqual(self_attn_o.dtype, torch.uint8)\n+        self.assertEqual(mlp_up.dtype, torch.uint8)\n+        self.assertEqual(mlp_gate.dtype, torch.uint8)\n+        self.assertEqual(mlp_down.dtype, torch.uint8)\n+\n+    def test_replace_with_bitlinear_shape(self):\n+        \"\"\"\n+        test that the BitNet layer weight shapes are correct, and the weight_scale is correctly initialized to 1\n+        \"\"\"\n+\n+        from transformers.integrations import replace_with_bitnet_linear\n+\n+        out_features = 1024\n+        in_features = 512\n+\n+        class SimpleLinearModule(torch.nn.Module):\n+            \"\"\"\n+            Simple class to test BitLinear\n+            \"\"\"\n+\n+            def __init__(\n+                self,\n+                in_features: int = in_features,\n+                out_features: int = out_features,\n+                bias: bool = False,\n+            ):\n+                super().__init__()\n+                self.linear = torch.nn.Linear(in_features=in_features, out_features=out_features, bias=bias)\n+\n+            def forward(self, x):\n+                return self.linear(x)\n+\n+        model = SimpleLinearModule()\n+        replace_with_bitnet_linear(model)\n+\n+        self.assertEqual(list(model.linear.weight.shape), [out_features // 4, in_features])\n+        self.assertEqual(model.linear.weight_scale, 1)\n+\n+\n+@slow\n+@require_torch_gpu\n+@require_accelerate\n+class BitNetSerializationTest(unittest.TestCase):\n+    def test_model_serialization(self):\n+        model_name = \"HF1BitLLM/Llama3-8B-1.58-100B-tokens\"\n+        device = \"cuda\"\n+        quantized_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device)\n+        input_tensor = torch.zeros((1, 8), dtype=torch.int32, device=device)\n+\n+        with torch.no_grad():\n+            logits_ref = quantized_model.forward(input_tensor).logits\n+\n+        # Save\n+        saved_model_id = \"quant_model\"\n+        quantized_model.save_pretrained(saved_model_id)\n+\n+        # Remove old model\n+        del quantized_model\n+        torch.cuda.empty_cache()\n+\n+        # Load and check if the logits match\n+        model_loaded = AutoModelForCausalLM.from_pretrained(\"quant_model\", device_map=device)\n+\n+        with torch.no_grad():\n+            logits_loaded = model_loaded.forward(input_tensor).logits\n+\n+        self.assertEqual((logits_loaded - logits_ref).abs().mean().item(), 0)"
        }
    ],
    "stats": {
        "total": 746,
        "additions": 745,
        "deletions": 1
    }
}