{
    "author": "ydshieh",
    "message": "Add `position_ids` in `XLMRobertaXLForCausalLM.prepare_inputs_for_generation` (#35044)\n\n* fix\r\n\r\n* fix\r\n\r\n* cleanup\r\n\r\n* style\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "203e978826618ce6b7b7c9233e455cf6942475fe",
    "files": [
        {
            "sha": "0a3e0812a42a963de3706991a9458273b63f3c01",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/203e978826618ce6b7b7c9233e455cf6942475fe/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/203e978826618ce6b7b7c9233e455cf6942475fe/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=203e978826618ce6b7b7c9233e455cf6942475fe",
            "patch": "@@ -1119,6 +1119,13 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attenti\n         if attention_mask is None:\n             attention_mask = input_ids.new_ones(input_shape)\n \n+        # Create missing `position_ids` on the fly\n+        position_ids = None\n+        if model_kwargs.get(\"position_ids\") is None:\n+            position_ids = create_position_ids_from_input_ids(\n+                input_ids, padding_idx=self.config.pad_token_id\n+            )  # placed in kwargs for further processing (see below)\n+\n         # cut decoder_input_ids if past_key_values is used\n         if past_key_values is not None:\n             past_length = past_key_values[0][0].shape[2]\n@@ -1131,8 +1138,15 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attenti\n                 remove_prefix_length = input_ids.shape[1] - 1\n \n             input_ids = input_ids[:, remove_prefix_length:]\n-\n-        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past_key_values}\n+            if position_ids is not None:\n+                position_ids = position_ids[:, remove_prefix_length:]\n+\n+        return {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"position_ids\": position_ids,\n+            \"past_key_values\": past_key_values,\n+        }\n \n     def _reorder_cache(self, past_key_values, beam_idx):\n         reordered_past = ()"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 16,
        "deletions": 2
    }
}