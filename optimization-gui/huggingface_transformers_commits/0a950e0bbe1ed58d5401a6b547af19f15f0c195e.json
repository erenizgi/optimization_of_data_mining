{
    "author": "jack89roberts",
    "message": "Fix uploading processors/tokenizers to WandB on train end (#35701)\n\n* rename tokenizer to processing_class in WandbCallback.on_train_end\n\n* rename tokenizer to processing_class in ClearMLCallback and DVCLiveCallback",
    "sha": "0a950e0bbe1ed58d5401a6b547af19f15f0c195e",
    "files": [
        {
            "sha": "fb9956905bbda05250d06662060f244443a46cbd",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a950e0bbe1ed58d5401a6b547af19f15f0c195e/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a950e0bbe1ed58d5401a6b547af19f15f0c195e/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=0a950e0bbe1ed58d5401a6b547af19f15f0c195e",
            "patch": "@@ -915,13 +915,13 @@ def on_train_begin(self, args, state, control, model=None, **kwargs):\n         if not self._initialized:\n             self.setup(args, state, model, **kwargs)\n \n-    def on_train_end(self, args, state, control, model=None, tokenizer=None, **kwargs):\n+    def on_train_end(self, args, state, control, model=None, processing_class=None, **kwargs):\n         if self._wandb is None:\n             return\n         if self._log_model.is_enabled and self._initialized and state.is_world_process_zero:\n             from ..trainer import Trainer\n \n-            fake_trainer = Trainer(args=args, model=model, processing_class=tokenizer, eval_dataset=[\"fake\"])\n+            fake_trainer = Trainer(args=args, model=model, processing_class=processing_class, eval_dataset=[\"fake\"])\n             with tempfile.TemporaryDirectory() as temp_dir:\n                 fake_trainer.save_model(temp_dir)\n                 metadata = (\n@@ -1765,7 +1765,7 @@ def __init__(self):\n         self._log_model = False\n         self._checkpoints_saved = []\n \n-    def setup(self, args, state, model, tokenizer, **kwargs):\n+    def setup(self, args, state, model, processing_class, **kwargs):\n         if self._clearml is None:\n             return\n         if self._initialized:\n@@ -1864,25 +1864,25 @@ def setup(self, args, state, model, tokenizer, **kwargs):\n                         description=configuration_object_description,\n                     )\n \n-    def on_train_begin(self, args, state, control, model=None, tokenizer=None, **kwargs):\n+    def on_train_begin(self, args, state, control, model=None, processing_class=None, **kwargs):\n         if self._clearml is None:\n             return\n         self._checkpoints_saved = []\n         if state.is_hyper_param_search:\n             self._initialized = False\n         if not self._initialized:\n-            self.setup(args, state, model, tokenizer, **kwargs)\n+            self.setup(args, state, model, processing_class, **kwargs)\n \n     def on_train_end(self, args, state, control, **kwargs):\n         if ClearMLCallback._should_close_on_train_end:\n             self._clearml_task.close()\n             ClearMLCallback._train_run_counter = 0\n \n-    def on_log(self, args, state, control, model=None, tokenizer=None, logs=None, **kwargs):\n+    def on_log(self, args, state, control, model=None, processing_class=None, logs=None, **kwargs):\n         if self._clearml is None:\n             return\n         if not self._initialized:\n-            self.setup(args, state, model, tokenizer, **kwargs)\n+            self.setup(args, state, model, processing_class, **kwargs)\n         if state.is_world_process_zero:\n             eval_prefix = \"eval_\"\n             eval_prefix_len = len(eval_prefix)\n@@ -2131,7 +2131,7 @@ def on_train_end(self, args, state, control, **kwargs):\n                 fake_trainer = Trainer(\n                     args=args,\n                     model=kwargs.get(\"model\"),\n-                    processing_class=kwargs.get(\"tokenizer\"),\n+                    processing_class=kwargs.get(\"processing_class\"),\n                     eval_dataset=[\"fake\"],\n                 )\n                 name = \"best\" if args.load_best_model_at_end else \"last\""
        }
    ],
    "stats": {
        "total": 16,
        "additions": 8,
        "deletions": 8
    }
}