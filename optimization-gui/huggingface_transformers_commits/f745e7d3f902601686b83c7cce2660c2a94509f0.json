{
    "author": "amyeroberts",
    "message": "Remove repeated prepare_images in processor tests (#33163)\n\n* Remove repeated prepare_images\r\n\r\n* Address comments - update docstring; explanatory comment",
    "sha": "f745e7d3f902601686b83c7cce2660c2a94509f0",
    "files": [
        {
            "sha": "73e0d1df91f36381a2fc3444847b0e9cffcafd47",
            "filename": "tests/models/align/test_processor_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Falign%2Ftest_processor_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Falign%2Ftest_processor_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_processor_align.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -18,7 +18,6 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n from transformers import BertTokenizer, BertTokenizerFast\n@@ -30,8 +29,6 @@\n \n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import AlignProcessor, EfficientNetImageProcessor\n \n \n@@ -86,15 +83,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-        return image_inputs\n-\n     def test_save_load_pretrained_default(self):\n         tokenizer_slow = self.get_tokenizer()\n         tokenizer_fast = self.get_rust_tokenizer()"
        },
        {
            "sha": "7b851c618a773dad0564db5135cb71dead418c6d",
            "filename": "tests/models/blip/test_processor_blip.py",
            "status": "modified",
            "additions": 5,
            "deletions": 15,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fblip%2Ftest_processor_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fblip%2Ftest_processor_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_processor_blip.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -15,21 +15,22 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n from transformers.testing_utils import require_vision\n from transformers.utils import is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n \n-if is_vision_available():\n-    from PIL import Image\n \n+if is_vision_available():\n     from transformers import AutoProcessor, BertTokenizer, BlipImageProcessor, BlipProcessor, PreTrainedTokenizerFast\n \n \n @require_vision\n-class BlipProcessorTest(unittest.TestCase):\n+class BlipProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = BlipProcessor\n+\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n@@ -49,17 +50,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_additional_features(self):\n         processor = BlipProcessor(tokenizer=self.get_tokenizer(), image_processor=self.get_image_processor())\n         processor.save_pretrained(self.tmpdirname)"
        },
        {
            "sha": "8c7ca2ab698f486182e38045acb64e63e2ab584a",
            "filename": "tests/models/blip_2/test_processor_blip_2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 15,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -15,21 +15,22 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n from transformers.testing_utils import require_vision\n from transformers.utils import is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n \n-if is_vision_available():\n-    from PIL import Image\n \n+if is_vision_available():\n     from transformers import AutoProcessor, Blip2Processor, BlipImageProcessor, GPT2Tokenizer, PreTrainedTokenizerFast\n \n \n @require_vision\n-class Blip2ProcessorTest(unittest.TestCase):\n+class Blip2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Blip2Processor\n+\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n@@ -49,17 +50,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_additional_features(self):\n         processor = Blip2Processor(tokenizer=self.get_tokenizer(), image_processor=self.get_image_processor())\n         processor.save_pretrained(self.tmpdirname)"
        },
        {
            "sha": "e433c38f78910459a5e29df2f7dc584e5d9f9c79",
            "filename": "tests/models/chinese_clip/test_processor_chinese_clip.py",
            "status": "modified",
            "additions": 10,
            "deletions": 15,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fchinese_clip%2Ftest_processor_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fchinese_clip%2Ftest_processor_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_processor_chinese_clip.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -18,23 +18,24 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n from transformers import BertTokenizer, BertTokenizerFast\n from transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_vision\n from transformers.utils import FEATURE_EXTRACTOR_NAME, is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n \n-if is_vision_available():\n-    from PIL import Image\n \n+if is_vision_available():\n     from transformers import ChineseCLIPImageProcessor, ChineseCLIPProcessor\n \n \n @require_vision\n-class ChineseCLIPProcessorTest(unittest.TestCase):\n+class ChineseCLIPProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = ChineseCLIPProcessor\n+\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n@@ -76,6 +77,11 @@ def setUp(self):\n         with open(self.image_processor_file, \"w\", encoding=\"utf-8\") as fp:\n             json.dump(image_processor_map, fp)\n \n+        tokenizer = self.get_tokenizer()\n+        image_processor = self.get_image_processor()\n+        processor = ChineseCLIPProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+        processor.save_pretrained(self.tmpdirname)\n+\n     def get_tokenizer(self, **kwargs):\n         return BertTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n \n@@ -88,17 +94,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_default(self):\n         tokenizer_slow = self.get_tokenizer()\n         tokenizer_fast = self.get_rust_tokenizer()"
        },
        {
            "sha": "7d7ea25b70cf0a2c5c7baef492702fd07d3a68f2",
            "filename": "tests/models/clip/test_processor_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fclip%2Ftest_processor_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fclip%2Ftest_processor_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_processor_clip.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -18,7 +18,6 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n from transformers import CLIPTokenizer, CLIPTokenizerFast\n@@ -30,8 +29,6 @@\n \n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import CLIPImageProcessor, CLIPProcessor\n \n \n@@ -79,17 +76,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_default(self):\n         tokenizer_slow = self.get_tokenizer()\n         tokenizer_fast = self.get_rust_tokenizer()"
        },
        {
            "sha": "5147ed39753913c63b088ccc887ffe08f5bc9331",
            "filename": "tests/models/clipseg/test_processor_clipseg.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fclipseg%2Ftest_processor_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fclipseg%2Ftest_processor_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_processor_clipseg.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -18,23 +18,24 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n from transformers import CLIPTokenizer, CLIPTokenizerFast\n from transformers.models.clip.tokenization_clip import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_vision\n from transformers.utils import IMAGE_PROCESSOR_NAME, is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n \n-if is_vision_available():\n-    from PIL import Image\n \n+if is_vision_available():\n     from transformers import CLIPSegProcessor, ViTImageProcessor\n \n \n @require_vision\n-class CLIPSegProcessorTest(unittest.TestCase):\n+class CLIPSegProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = CLIPSegProcessor\n+\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n@@ -75,16 +76,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_default(self):\n         tokenizer_slow = self.get_tokenizer()\n         tokenizer_fast = self.get_rust_tokenizer()"
        },
        {
            "sha": "8489322efd6901d543fd20aa59ecec6b0ca9bfac",
            "filename": "tests/models/flava/test_processor_flava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 15,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fflava%2Ftest_processor_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fflava%2Ftest_processor_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_processor_flava.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -19,18 +19,17 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n from transformers import BertTokenizer, BertTokenizerFast\n from transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_vision\n from transformers.utils import IMAGE_PROCESSOR_NAME, is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n \n-if is_vision_available():\n-    from PIL import Image\n \n+if is_vision_available():\n     from transformers import FlavaImageProcessor, FlavaProcessor\n     from transformers.models.flava.image_processing_flava import (\n         FLAVA_CODEBOOK_MEAN,\n@@ -41,7 +40,9 @@\n \n \n @require_vision\n-class FlavaProcessorTest(unittest.TestCase):\n+class FlavaProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = FlavaProcessor\n+\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n@@ -91,17 +92,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_default(self):\n         tokenizer_slow = self.get_tokenizer()\n         tokenizer_fast = self.get_rust_tokenizer()"
        },
        {
            "sha": "bc62454cef6e25260621f7ecb56b37120ace842d",
            "filename": "tests/models/git/test_processor_git.py",
            "status": "modified",
            "additions": 5,
            "deletions": 15,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fgit%2Ftest_processor_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fgit%2Ftest_processor_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_processor_git.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -15,21 +15,22 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n from transformers.testing_utils import require_vision\n from transformers.utils import is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n \n-if is_vision_available():\n-    from PIL import Image\n \n+if is_vision_available():\n     from transformers import AutoProcessor, BertTokenizer, CLIPImageProcessor, GitProcessor, PreTrainedTokenizerFast\n \n \n @require_vision\n-class GitProcessorTest(unittest.TestCase):\n+class GitProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = GitProcessor\n+\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n@@ -51,17 +52,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_additional_features(self):\n         processor = GitProcessor(tokenizer=self.get_tokenizer(), image_processor=self.get_image_processor())\n         processor.save_pretrained(self.tmpdirname)"
        },
        {
            "sha": "c2d8aee828dd526067f948f83697a3f0c6b3eafa",
            "filename": "tests/models/grounding_dino/test_processor_grounding_dino.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fgrounding_dino%2Ftest_processor_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fgrounding_dino%2Ftest_processor_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_processor_grounding_dino.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -18,7 +18,6 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n from transformers import BertTokenizer, BertTokenizerFast, GroundingDinoProcessor\n@@ -35,8 +34,6 @@\n     from transformers.models.grounding_dino.modeling_grounding_dino import GroundingDinoObjectDetectionOutput\n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import GroundingDinoImageProcessor\n \n \n@@ -96,18 +93,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    # Copied from tests.models.clip.test_processor_clip.CLIPProcessorTest.prepare_image_inputs\n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def get_fake_grounding_dino_output(self):\n         torch.manual_seed(42)\n         return GroundingDinoObjectDetectionOutput("
        },
        {
            "sha": "cc929e3575a6d80d9449236b694fe7f17f52aeb9",
            "filename": "tests/models/instructblip/test_processor_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -15,7 +15,6 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n from transformers.testing_utils import require_torch, require_vision\n@@ -25,8 +24,6 @@\n \n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import (\n         AutoProcessor,\n         BertTokenizerFast,\n@@ -64,17 +61,6 @@ def get_qformer_tokenizer(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_additional_features(self):\n         processor = InstructBlipProcessor(\n             tokenizer=self.get_tokenizer(),"
        },
        {
            "sha": "e07ba5fc106b6c58ed52df5874b789ad540fcd19",
            "filename": "tests/models/kosmos2/test_processor_kosmos2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 12,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fkosmos2%2Ftest_processor_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fkosmos2%2Ftest_processor_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_processor_kosmos2.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -23,6 +23,7 @@\n import pytest\n import requests\n \n+from transformers.models.auto.processing_auto import processor_class_from_name\n from transformers.testing_utils import (\n     get_tests_dir,\n     require_sentencepiece,\n@@ -32,6 +33,8 @@\n )\n from transformers.utils import is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n+\n \n if is_vision_available():\n     from PIL import Image\n@@ -52,7 +55,9 @@\n @require_sentencepiece\n @require_tokenizers\n @require_vision\n-class Kosmos2ProcessorTest(unittest.TestCase):\n+class Kosmos2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Kosmos2Processor\n+\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n@@ -65,6 +70,20 @@ def setUp(self):\n         processor = Kosmos2Processor(image_processor, fast_tokenizer)\n         processor.save_pretrained(self.tmpdirname)\n \n+    # We override this method to take the fast tokenizer or image processor by default\n+    def get_component(self, attribute, **kwargs):\n+        assert attribute in self.processor_class.attributes\n+        component_class_name = getattr(self.processor_class, f\"{attribute}_class\")\n+        if isinstance(component_class_name, tuple):\n+            component_class_name = component_class_name[-1]\n+\n+        component_class = processor_class_from_name(component_class_name)\n+        component = component_class.from_pretrained(self.tmpdirname, **kwargs)  # noqa\n+        if attribute == \"tokenizer\" and not component.pad_token:\n+            component.pad_token = \"[TEST_PAD]\"\n+\n+        return component\n+\n     def get_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n \n@@ -74,17 +93,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_image_procesor_load_save_reload(self):\n         # make sure load from Hub repo. -> save -> reload locally work\n         image_processor = CLIPImageProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")"
        },
        {
            "sha": "a2676195ffd37ca476e628d6266d96dabfd81353",
            "filename": "tests/models/layoutlmv2/test_processor_layoutlmv2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 16,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Flayoutlmv2%2Ftest_processor_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Flayoutlmv2%2Ftest_processor_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_processor_layoutlmv2.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -19,26 +19,27 @@\n import unittest\n from typing import List\n \n-import numpy as np\n-\n from transformers import PreTrainedTokenizer, PreTrainedTokenizerBase, PreTrainedTokenizerFast\n-from transformers.models.layoutlmv2 import LayoutLMv2Tokenizer, LayoutLMv2TokenizerFast\n+from transformers.models.layoutlmv2 import LayoutLMv2Processor, LayoutLMv2Tokenizer, LayoutLMv2TokenizerFast\n from transformers.models.layoutlmv2.tokenization_layoutlmv2 import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_pytesseract, require_tokenizers, require_torch, slow\n from transformers.utils import FEATURE_EXTRACTOR_NAME, cached_property, is_pytesseract_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n+\n \n if is_pytesseract_available():\n     from PIL import Image\n \n-    from transformers import LayoutLMv2ImageProcessor, LayoutLMv2Processor\n+    from transformers import LayoutLMv2ImageProcessor\n \n \n @require_pytesseract\n @require_tokenizers\n-class LayoutLMv2ProcessorTest(unittest.TestCase):\n+class LayoutLMv2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     tokenizer_class = LayoutLMv2Tokenizer\n     rust_tokenizer_class = LayoutLMv2TokenizerFast\n+    processor_class = LayoutLMv2Processor\n \n     def setUp(self):\n         vocab_tokens = [\n@@ -88,17 +89,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_default(self):\n         image_processor = self.get_image_processor()\n         tokenizers = self.get_tokenizers()"
        },
        {
            "sha": "e55b19ea44b0611305ceebb6cc85f8741fc2d978",
            "filename": "tests/models/layoutlmv3/test_processor_layoutlmv3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 16,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Flayoutlmv3%2Ftest_processor_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Flayoutlmv3%2Ftest_processor_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_processor_layoutlmv3.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -19,26 +19,27 @@\n import unittest\n from typing import List\n \n-import numpy as np\n-\n from transformers import PreTrainedTokenizer, PreTrainedTokenizerBase, PreTrainedTokenizerFast\n-from transformers.models.layoutlmv3 import LayoutLMv3Tokenizer, LayoutLMv3TokenizerFast\n+from transformers.models.layoutlmv3 import LayoutLMv3Processor, LayoutLMv3Tokenizer, LayoutLMv3TokenizerFast\n from transformers.models.layoutlmv3.tokenization_layoutlmv3 import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_pytesseract, require_tokenizers, require_torch, slow\n from transformers.utils import FEATURE_EXTRACTOR_NAME, cached_property, is_pytesseract_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n+\n \n if is_pytesseract_available():\n     from PIL import Image\n \n-    from transformers import LayoutLMv3ImageProcessor, LayoutLMv3Processor\n+    from transformers import LayoutLMv3ImageProcessor\n \n \n @require_pytesseract\n @require_tokenizers\n-class LayoutLMv3ProcessorTest(unittest.TestCase):\n+class LayoutLMv3ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     tokenizer_class = LayoutLMv3Tokenizer\n     rust_tokenizer_class = LayoutLMv3TokenizerFast\n+    processor_class = LayoutLMv3Processor\n \n     def setUp(self):\n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n@@ -101,17 +102,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_default(self):\n         image_processor = self.get_image_processor()\n         tokenizers = self.get_tokenizers()"
        },
        {
            "sha": "b970a3e526830a6776306c4f633ceb058e71a6d1",
            "filename": "tests/models/layoutxlm/test_processor_layoutxlm.py",
            "status": "modified",
            "additions": 11,
            "deletions": 16,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Flayoutxlm%2Ftest_processor_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Flayoutxlm%2Ftest_processor_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutxlm%2Ftest_processor_layoutxlm.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -19,10 +19,8 @@\n import unittest\n from typing import List\n \n-import numpy as np\n-\n from transformers import PreTrainedTokenizer, PreTrainedTokenizerBase, PreTrainedTokenizerFast\n-from transformers.models.layoutxlm import LayoutXLMTokenizer, LayoutXLMTokenizerFast\n+from transformers.models.layoutxlm import LayoutXLMProcessor, LayoutXLMTokenizer, LayoutXLMTokenizerFast\n from transformers.testing_utils import (\n     require_pytesseract,\n     require_sentencepiece,\n@@ -32,19 +30,22 @@\n )\n from transformers.utils import FEATURE_EXTRACTOR_NAME, cached_property, is_pytesseract_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n+\n \n if is_pytesseract_available():\n     from PIL import Image\n \n-    from transformers import LayoutLMv2ImageProcessor, LayoutXLMProcessor\n+    from transformers import LayoutLMv2ImageProcessor\n \n \n @require_pytesseract\n @require_sentencepiece\n @require_tokenizers\n-class LayoutXLMProcessorTest(unittest.TestCase):\n+class LayoutXLMProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     tokenizer_class = LayoutXLMTokenizer\n     rust_tokenizer_class = LayoutXLMTokenizerFast\n+    processor_class = LayoutXLMProcessor\n \n     def setUp(self):\n         image_processor_map = {\n@@ -61,6 +62,11 @@ def setUp(self):\n         # taken from `test_tokenization_layoutxlm.LayoutXLMTokenizationTest.test_save_pretrained`\n         self.tokenizer_pretrained_name = \"hf-internal-testing/tiny-random-layoutxlm\"\n \n+        tokenizer = self.get_tokenizer()\n+        image_processor = self.get_image_processor()\n+        processor = LayoutXLMProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+        processor.save_pretrained(self.tmpdirname)\n+\n     def get_tokenizer(self, **kwargs) -> PreTrainedTokenizer:\n         return self.tokenizer_class.from_pretrained(self.tokenizer_pretrained_name, **kwargs)\n \n@@ -76,17 +82,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_default(self):\n         image_processor = self.get_image_processor()\n         tokenizers = self.get_tokenizers()"
        },
        {
            "sha": "783a61ebf144a9bd6451535bc8b3decee81d2bdd",
            "filename": "tests/models/mgp_str/test_processor_mgp_str.py",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fmgp_str%2Ftest_processor_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fmgp_str%2Ftest_processor_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmgp_str%2Ftest_processor_mgp_str.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -70,6 +70,17 @@ def setUp(self):\n         with open(self.image_processor_file, \"w\", encoding=\"utf-8\") as fp:\n             json.dump(image_processor_map, fp)\n \n+    # We copy here rather than use the ProcessorTesterMixin as this processor has a `char_tokenizer` instad of a\n+    # tokenizer attribute, which means all the tests would need to be overridden.\n+    @require_vision\n+    def prepare_image_inputs(self):\n+        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n+        or a list of PyTorch tensors if one specifies torchify=True.\n+        \"\"\"\n+        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n+        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n+        return image_inputs\n+\n     def get_tokenizer(self, **kwargs):\n         return MgpstrTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n \n@@ -79,15 +90,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images.\"\"\"\n-\n-        image_input = np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)\n-\n-        image_input = Image.fromarray(np.moveaxis(image_input, 0, -1))\n-\n-        return image_input\n-\n     def test_save_load_pretrained_default(self):\n         tokenizer = self.get_tokenizer()\n         image_processor = self.get_image_processor()"
        },
        {
            "sha": "f31dbaf9fbccab36b897f9d6cc86015b94822e08",
            "filename": "tests/models/owlvit/test_processor_owlvit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 15,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fowlvit%2Ftest_processor_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fowlvit%2Ftest_processor_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_processor_owlvit.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -18,23 +18,24 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n from transformers import CLIPTokenizer, CLIPTokenizerFast\n from transformers.models.clip.tokenization_clip import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_vision\n from transformers.utils import IMAGE_PROCESSOR_NAME, is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n \n-if is_vision_available():\n-    from PIL import Image\n \n+if is_vision_available():\n     from transformers import OwlViTImageProcessor, OwlViTProcessor\n \n \n @require_vision\n-class OwlViTProcessorTest(unittest.TestCase):\n+class OwlViTProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = OwlViTProcessor\n+\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n@@ -75,17 +76,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_default(self):\n         tokenizer_slow = self.get_tokenizer()\n         tokenizer_fast = self.get_rust_tokenizer()"
        },
        {
            "sha": "17b3298145f823a5e509f847e9fac4d62a30faf8",
            "filename": "tests/models/pix2struct/test_processor_pix2struct.py",
            "status": "modified",
            "additions": 5,
            "deletions": 15,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fpix2struct%2Ftest_processor_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fpix2struct%2Ftest_processor_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_processor_pix2struct.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -15,16 +15,15 @@\n import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n \n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n \n-if is_vision_available():\n-    from PIL import Image\n \n+if is_vision_available():\n     from transformers import (\n         AutoProcessor,\n         Pix2StructImageProcessor,\n@@ -36,7 +35,9 @@\n \n @require_vision\n @require_torch\n-class Pix2StructProcessorTest(unittest.TestCase):\n+class Pix2StructProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Pix2StructProcessor\n+\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n@@ -56,17 +57,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"\n-        This function prepares a list of random PIL images of the same fixed size.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_additional_features(self):\n         processor = Pix2StructProcessor(tokenizer=self.get_tokenizer(), image_processor=self.get_image_processor())\n         processor.save_pretrained(self.tmpdirname)"
        },
        {
            "sha": "22eb88d03d6b0418a96b36da5ec6c7987fa3b848",
            "filename": "tests/models/sam/test_processor_sam.py",
            "status": "modified",
            "additions": 11,
            "deletions": 24,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -26,6 +26,8 @@\n )\n from transformers.utils import is_tf_available, is_torch_available, is_vision_available\n \n+from ...test_processing_common import prepare_image_inputs\n+\n \n if is_vision_available():\n     from PIL import Image\n@@ -54,13 +56,10 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n+    # Processor tester class can't use ProcessorTesterMixin atm because the processor is atypical e.g. only contains an image processor\n     def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-        return image_inputs\n+        \"\"\"This function prepares a list of PIL images.\"\"\"\n+        return prepare_image_inputs()\n \n     def prepare_mask_inputs(self):\n         \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n@@ -166,16 +165,10 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n+    # Processor tester class can't use ProcessorTesterMixin as processor is atypical e.g. only contains an image processor and it assumes torch\n     def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n+        \"\"\"This function prepares a list of PIL images.\"\"\"\n+        return prepare_image_inputs()\n \n     def test_save_load_pretrained_additional_features(self):\n         processor = SamProcessor(image_processor=self.get_image_processor())\n@@ -255,16 +248,10 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n+    # Processor tester class can't use ProcessorTesterMixin atm because the processor is atypical e.g. only contains an image processor\n     def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n+        \"\"\"This function prepares a list of PIL images.\"\"\"\n+        return prepare_image_inputs()\n \n     @is_pt_tf_cross_test\n     def test_post_process_masks_equivalence(self):"
        },
        {
            "sha": "749ec7c3d6df7807fc603a0a6145c3f07a44a4d2",
            "filename": "tests/models/udop/test_processor_udop.py",
            "status": "modified",
            "additions": 11,
            "deletions": 15,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fudop%2Ftest_processor_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fudop%2Ftest_processor_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_processor_udop.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -19,12 +19,11 @@\n import unittest\n from typing import List\n \n-import numpy as np\n-\n from transformers import (\n     PreTrainedTokenizer,\n     PreTrainedTokenizerBase,\n     PreTrainedTokenizerFast,\n+    UdopProcessor,\n     UdopTokenizer,\n     UdopTokenizerFast,\n )\n@@ -37,6 +36,8 @@\n )\n from transformers.utils import FEATURE_EXTRACTOR_NAME, cached_property, is_pytesseract_available, is_torch_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n+\n \n if is_torch_available():\n     import torch\n@@ -45,16 +46,17 @@\n if is_pytesseract_available():\n     from PIL import Image\n \n-    from transformers import LayoutLMv3ImageProcessor, UdopProcessor\n+    from transformers import LayoutLMv3ImageProcessor\n \n \n @require_pytesseract\n @require_sentencepiece\n @require_tokenizers\n-class UdopProcessorTest(unittest.TestCase):\n+class UdopProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     tokenizer_class = UdopTokenizer\n     rust_tokenizer_class = UdopTokenizerFast\n     maxDiff = None\n+    processor_class = UdopProcessor\n \n     def setUp(self):\n         image_processor_map = {\n@@ -70,6 +72,11 @@ def setUp(self):\n \n         self.tokenizer_pretrained_name = \"microsoft/udop-large\"\n \n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizers()[0]\n+        processor = UdopProcessor(image_processor=image_processor, tokenizer=tokenizer)\n+        processor.save_pretrained(self.tmpdirname)\n+\n     def get_tokenizer(self, **kwargs) -> PreTrainedTokenizer:\n         return self.tokenizer_class.from_pretrained(self.tokenizer_pretrained_name, **kwargs)\n \n@@ -85,17 +92,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_default(self):\n         image_processor = self.get_image_processor()\n         tokenizers = self.get_tokenizers()"
        },
        {
            "sha": "c9386a160f843d5aaa7f6522608928251eeec763",
            "filename": "tests/models/vision_text_dual_encoder/test_processor_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 10,
            "deletions": 16,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_processor_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_processor_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_processor_vision_text_dual_encoder.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -18,23 +18,23 @@\n import tempfile\n import unittest\n \n-import numpy as np\n-\n from transformers import BertTokenizerFast\n from transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES, BertTokenizer\n from transformers.testing_utils import require_tokenizers, require_vision\n from transformers.utils import IMAGE_PROCESSOR_NAME, is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n \n-if is_vision_available():\n-    from PIL import Image\n \n+if is_vision_available():\n     from transformers import VisionTextDualEncoderProcessor, ViTImageProcessor\n \n \n @require_tokenizers\n @require_vision\n-class VisionTextDualEncoderProcessorTest(unittest.TestCase):\n+class VisionTextDualEncoderProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = VisionTextDualEncoderProcessor\n+\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n \n@@ -54,6 +54,11 @@ def setUp(self):\n         with open(self.image_processor_file, \"w\", encoding=\"utf-8\") as fp:\n             json.dump(image_processor_map, fp)\n \n+        tokenizer = self.get_tokenizer()\n+        image_processor = self.get_image_processor()\n+        processor = VisionTextDualEncoderProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+        processor.save_pretrained(self.tmpdirname)\n+\n     def get_tokenizer(self, **kwargs):\n         return BertTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n \n@@ -63,17 +68,6 @@ def get_image_processor(self, **kwargs):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-\n-        return image_inputs\n-\n     def test_save_load_pretrained_default(self):\n         tokenizer = self.get_tokenizer()\n         image_processor = self.get_image_processor()"
        },
        {
            "sha": "1343f3bac703e4ad50bf05c4a3b8f732a32f4fc5",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f745e7d3f902601686b83c7cce2660c2a94509f0/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=f745e7d3f902601686b83c7cce2660c2a94509f0",
            "patch": "@@ -44,6 +44,13 @@\n     from transformers import CLIPImageProcessor\n \n \n+def prepare_image_inputs():\n+    \"\"\"This function prepares a list of PIL images\"\"\"\n+    image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n+    image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n+    return image_inputs\n+\n+\n @require_torch\n @require_vision\n @require_torch\n@@ -81,12 +88,8 @@ def get_processor(self):\n \n     @require_vision\n     def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-        \"\"\"\n-        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n-        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n-        return image_inputs\n+        \"\"\"This function prepares a list of PIL images for testing\"\"\"\n+        return prepare_image_inputs()\n \n     def test_processor_to_json_string(self):\n         processor = self.get_processor()"
        }
    ],
    "stats": {
        "total": 444,
        "additions": 140,
        "deletions": 304
    }
}