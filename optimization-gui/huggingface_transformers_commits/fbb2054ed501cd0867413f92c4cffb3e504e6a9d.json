{
    "author": "Cyrilvallez",
    "message": "Offloaded hybrid cache for Llama4 (#37401)\n\n* first try (maybe race condition)\n\n* Update cache_utils.py\n\n* cannot avoid the race condition -> use 2 layers\n\n* Update cache_utils.py\n\n* Update cache_utils.py",
    "sha": "fbb2054ed501cd0867413f92c4cffb3e504e6a9d",
    "files": [
        {
            "sha": "4d239407639a98a5e19a053875dedcdeb56ed7cf",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 112,
            "deletions": 0,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbb2054ed501cd0867413f92c4cffb3e504e6a9d/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbb2054ed501cd0867413f92c4cffb3e504e6a9d/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=fbb2054ed501cd0867413f92c4cffb3e504e6a9d",
            "patch": "@@ -2011,6 +2011,118 @@ def reset(self):\n         self.cumulative_length = [0 for _ in range(len(self.cumulative_length))]\n \n \n+class OffloadedHybridCache(HybridChunkedCache):\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        max_batch_size: int,\n+        max_cache_len: Optional[int] = None,\n+        device: Union[torch.device, str, None] = None,\n+        dtype: torch.dtype = torch.bfloat16,\n+        offload_device: Union[str, torch.device] = torch.device(\"cpu\"),\n+        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n+    ):\n+        super().__init__(config, max_batch_size, max_cache_len, device, dtype, layer_device_map)\n+        self.offload_device = torch.device(offload_device)\n+        # Create new CUDA stream for parallel prefetching.\n+        self._prefetch_stream = torch.cuda.Stream() if torch._C._get_accelerator().type == \"cuda\" else None\n+        # Those will be dynamically created as the other layers (for TP)\n+        self.device_key_cache = None\n+        self.device_value_cache = None\n+        # This gives the index of which on-device full layer to use (we need 2 to avoid race conditions when prefetching)\n+        self.active_device_layer = 0\n+\n+    def initialise_cache_layer(self, layer_idx, key_states):\n+        \"\"\"Overriden to use the correct device if offloaded layer (and pin memory).\"\"\"\n+        if len(self.key_cache) > layer_idx:\n+            return\n+\n+        num_key_value_heads = key_states.shape[1]\n+        device = key_states.device if self.is_sliding[layer_idx] else self.offload_device\n+        pin_memory = not self.is_sliding[layer_idx]\n+        global_cache_shape = (self.max_batch_size, num_key_value_heads, self.max_cache_len, self.head_dim)\n+        sliding_cache_shape = (\n+            self.max_batch_size,\n+            num_key_value_heads,\n+            self.sliding_window,\n+            self.head_dim,\n+        )\n+        # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n+        # breaks when updating the cache.\n+        cache_shape = sliding_cache_shape if self.is_sliding[layer_idx] else global_cache_shape\n+        new_layer_key_cache = torch.zeros(cache_shape, dtype=self._dtype, device=device, pin_memory=pin_memory)\n+        new_layer_value_cache = torch.zeros(cache_shape, dtype=self._dtype, device=device, pin_memory=pin_memory)\n+        torch._dynamo.mark_static_address(new_layer_key_cache)\n+        torch._dynamo.mark_static_address(new_layer_value_cache)\n+        self.key_cache.append(new_layer_key_cache)\n+        self.value_cache.append(new_layer_value_cache)\n+\n+        # Make sure to initialize the on-device layer if it does not already exist\n+        if self.device_key_cache is None and not self.is_sliding[layer_idx]:\n+            self.device_key_cache = []\n+            self.device_value_cache = []\n+            # We need 2 layers to avoid race conditions when prefetching the next one\n+            for _ in range(2):\n+                device_layer_key_cache = torch.zeros(cache_shape, dtype=self._dtype, device=key_states.device)\n+                device_layer_value_cache = torch.zeros(cache_shape, dtype=self._dtype, device=key_states.device)\n+                torch._dynamo.mark_static_address(new_layer_key_cache)\n+                torch._dynamo.mark_static_address(new_layer_value_cache)\n+                self.device_key_cache.append(device_layer_key_cache)\n+                self.device_value_cache.append(device_layer_value_cache)\n+\n+    def _static_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):\n+        # Wait for prefetch stream if needed\n+        if self._prefetch_stream is not None:\n+            torch.cuda.default_stream(key_states.device).wait_stream(self._prefetch_stream)\n+\n+        # Get correct on-device layer\n+        k_out = self.device_key_cache[self.active_device_layer]\n+        v_out = self.device_value_cache[self.active_device_layer]\n+\n+        # Let's prefetch the next layer as soon as possible\n+        self._prefetch_next_layer(layer_idx)\n+\n+        # Copy to on-device layer\n+        k_out[:, :, cache_position] = key_states\n+        v_out[:, :, cache_position] = value_states\n+\n+        # Copy to offloaded device\n+        self.key_cache[layer_idx][:, :, cache_position] = key_states.to(self.offload_device)\n+        self.value_cache[layer_idx][:, :, cache_position] = value_states.to(self.offload_device)\n+\n+        return k_out, v_out\n+\n+    def _prefetch_next_layer(self, layer_idx: int) -> None:\n+        \"\"\"Based on current layer_idx, prefetch next full layer to the device.\"\"\"\n+\n+        # Switch the active layer\n+        self.active_device_layer = 0 if self.active_device_layer == 1 else 1\n+\n+        # Find the next non-sliding layer\n+        try:\n+            next_layer = layer_idx + 1 + self.is_sliding[layer_idx + 1 :].index(False)\n+        # In this case, we are at the last layer, and we go back to prefect the first one\n+        except ValueError:\n+            next_layer = self.is_sliding.index(False)\n+\n+        # Alternate between two on-device caches.\n+        if self._prefetch_stream is not None:\n+            with torch.cuda.stream(self._prefetch_stream):\n+                self._prefetch_layer_in_context(next_layer)\n+        else:\n+            self._prefetch_layer_in_context(next_layer)\n+\n+    def _prefetch_layer_in_context(self, layer_idx: int) -> None:\n+        \"\"\"Performs the actual copy of the layer to device cache.\"\"\"\n+        if len(self.key_cache) >= layer_idx:\n+            self.device_key_cache[self.active_device_layer].copy_(self.key_cache[layer_idx], non_blocking=True)\n+            self.device_value_cache[self.active_device_layer].copy_(self.value_cache[layer_idx], non_blocking=True)\n+        # The layer was not yet initialized\n+        else:\n+            self.device_key_cache[self.active_device_layer].fill_(0.0)\n+            self.device_value_cache[self.active_device_layer].fill_(0.0)\n+\n+\n class MambaCache:\n     \"\"\"\n     Cache for mamba model which does not have attention mechanism and key value states."
        },
        {
            "sha": "f0ceaed5195ddc3d0adb80e79fb3db51c2648b3c",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbb2054ed501cd0867413f92c4cffb3e504e6a9d/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbb2054ed501cd0867413f92c4cffb3e504e6a9d/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=fbb2054ed501cd0867413f92c4cffb3e504e6a9d",
            "patch": "@@ -54,6 +54,7 @@\n         HybridCache,\n         HybridChunkedCache,\n         MambaCache,\n+        OffloadedHybridCache,\n         OffloadedStaticCache,\n         QuantizedCacheConfig,\n         QuantoQuantizedCache,\n@@ -71,6 +72,8 @@\n         \"sliding_window\": SlidingWindowCache,\n         \"hybrid\": HybridCache,\n         \"hybrid_chunked\": HybridChunkedCache,\n+        \"offloaded_hybrid\": OffloadedHybridCache,\n+        \"offloaded_hybrid_chunked\": OffloadedHybridCache,\n         \"mamba\": MambaCache,\n     }\n     QUANT_BACKEND_CLASSES_MAPPING = {\"quanto\": QuantoQuantizedCache, \"HQQ\": HQQQuantizedCache}"
        },
        {
            "sha": "ee5b79e6d3ac7a9757ee6eac149659f92342f9ef",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbb2054ed501cd0867413f92c4cffb3e504e6a9d/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbb2054ed501cd0867413f92c4cffb3e504e6a9d/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=fbb2054ed501cd0867413f92c4cffb3e504e6a9d",
            "patch": "@@ -35,6 +35,7 @@\n     EncoderDecoderCache,\n     HybridChunkedCache,\n     OffloadedCache,\n+    OffloadedHybridCache,\n     QuantizedCacheConfig,\n     StaticCache,\n )\n@@ -1834,7 +1835,9 @@ def _get_cache(\n             not hasattr(self, \"_cache\")\n             or (not isinstance(cache_to_check, cache_cls))\n             or cache_to_check.max_batch_size != batch_size\n-            or isinstance(cache_to_check, HybridChunkedCache)  # due to internal slicing, we always re-init\n+            or isinstance(\n+                cache_to_check, (HybridChunkedCache, OffloadedHybridCache)\n+            )  # due to internal slicing, we always re-init\n         )\n         if cache_implementation != \"mamba\":\n             need_new_cache = need_new_cache or cache_to_check.max_cache_len < max_cache_len"
        }
    ],
    "stats": {
        "total": 120,
        "additions": 119,
        "deletions": 1
    }
}