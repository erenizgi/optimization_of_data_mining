{
    "author": "qubvel",
    "message": "Enable auto task for timm models in pipeline (#35531)\n\n* Enable auto task for timm models\r\n\r\n* Add pipeline test",
    "sha": "657bb14f981c8d7e3ad77fe309bee0951cbf7186",
    "files": [
        {
            "sha": "257f5689b0ed71afd8560aeb183f4e47beb03d47",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/657bb14f981c8d7e3ad77fe309bee0951cbf7186/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/657bb14f981c8d7e3ad77fe309bee0951cbf7186/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=657bb14f981c8d7e3ad77fe309bee0951cbf7186",
            "patch": "@@ -494,7 +494,7 @@ def get_task(model: str, token: Optional[str] = None, **deprecated_kwargs) -> st\n         raise RuntimeError(\n             f\"The model {model} does not seem to have a correct `pipeline_tag` set to infer the task automatically\"\n         )\n-    if getattr(info, \"library_name\", \"transformers\") != \"transformers\":\n+    if getattr(info, \"library_name\", \"transformers\") not in {\"transformers\", \"timm\"}:\n         raise RuntimeError(f\"This model is meant to be used with {info.library_name} not with transformers\")\n     task = info.pipeline_tag\n     return task"
        },
        {
            "sha": "360bbc9371770bb6e330c3d394731695146dab8e",
            "filename": "tests/models/timm_wrapper/test_modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/657bb14f981c8d7e3ad77fe309bee0951cbf7186/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/657bb14f981c8d7e3ad77fe309bee0951cbf7186/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py?ref=657bb14f981c8d7e3ad77fe309bee0951cbf7186",
            "patch": "@@ -17,6 +17,7 @@\n import tempfile\n import unittest\n \n+from transformers import pipeline\n from transformers.testing_utils import (\n     require_bitsandbytes,\n     require_timm,\n@@ -294,6 +295,19 @@ def test_inference_image_classification_head(self):\n         is_close = torch.allclose(resulted_slice, expected_slice, atol=1e-3)\n         self.assertTrue(is_close, f\"Expected {expected_slice}, but got {resulted_slice}\")\n \n+    @slow\n+    def test_inference_with_pipeline(self):\n+        image = prepare_img()\n+        classifier = pipeline(model=\"timm/resnet18.a1_in1k\", device=torch_device)\n+        result = classifier(image)\n+\n+        # verify result\n+        expected_label = \"tabby, tabby cat\"\n+        expected_score = 0.4329\n+\n+        self.assertEqual(result[0][\"label\"], expected_label)\n+        self.assertAlmostEqual(result[0][\"score\"], expected_score, places=3)\n+\n     @slow\n     @require_bitsandbytes\n     def test_inference_image_classification_quantized(self):"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 15,
        "deletions": 1
    }
}