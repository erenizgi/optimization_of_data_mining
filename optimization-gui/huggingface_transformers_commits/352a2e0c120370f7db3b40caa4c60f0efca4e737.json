{
    "author": "remi-or",
    "message": "Benchmark simplification (#42408)\n\n* Renames\n\n* Added the timestamps to request\n\n* Better rename for prompt_ids\n\n* Merged the two timing functions\n\n* Style\n\n* Remove the first timestamp for generate timing\n\n* Fix nit in comment\n\n* Re-introduce timestamps\n\n* Now upload two versions of the results: full and summarized\n\n* Make summarized result more summarized\n\n* Fix wrong file name\n\n* Dumb fix",
    "sha": "352a2e0c120370f7db3b40caa4c60f0efca4e737",
    "files": [
        {
            "sha": "ed47d0b25d638895fb05f5c12982808ef61c6c86",
            "filename": "benchmark_v2/framework/benchmark_runner.py",
            "status": "modified",
            "additions": 77,
            "deletions": 93,
            "changes": 170,
            "blob_url": "https://github.com/huggingface/transformers/blob/352a2e0c120370f7db3b40caa4c60f0efca4e737/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/352a2e0c120370f7db3b40caa4c60f0efca4e737/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_runner.py?ref=352a2e0c120370f7db3b40caa4c60f0efca4e737",
            "patch": "@@ -10,6 +10,7 @@\n from queue import Queue\n from typing import Any\n \n+import numpy as np\n import torch\n from datasets import Dataset\n from huggingface_hub import HfApi\n@@ -208,10 +209,11 @@ def run_benchmark(\n             self.logger.info(f\"Running benchmark scenario: {config.name}\")\n \n             # Quick validation: try one measurement first to see if this scenario works\n-            generate_fn = self.time_generate_batch if config.continuous_batching else self.time_generate\n             flush_memory()\n-            e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics = generate_fn(\n-                max_new_tokens=1, gpu_monitor=None\n+            e2e_latency, timestamps, shape_and_decoded_output, gpu_metrics = self.time_generate(\n+                max_new_tokens=config.num_tokens_to_generate,\n+                use_continuous_batching=config.continuous_batching,\n+                gpu_monitor=None,\n             )\n             if e2e_latency < 0:\n                 self.logger.warning(f\"Skipping config {config.name}: {e2e_latency = } (no GPU monitoring)\")\n@@ -220,18 +222,23 @@ def run_benchmark(\n             # Warmup runs\n             self.logger.info(f\"Warming up with {config.warmup_iterations} iterations...\")\n             for _ in trange(config.warmup_iterations, desc=\"Warmup\"):\n-                _ = generate_fn(max_new_tokens=config.num_tokens_to_generate)\n+                _ = self.time_generate(\n+                    max_new_tokens=config.num_tokens_to_generate,\n+                    use_continuous_batching=config.continuous_batching,\n+                    gpu_monitor=None,\n+                )\n             self.logger.info(\"Warmup over.\")\n \n             # Measurement runs\n             result = BenchmarkResult()\n             self.logger.info(f\"Benchmarking with {config.measurement_iterations} iterations.\")\n             for _ in trange(config.measurement_iterations, desc=\"Benchmarking\"):\n-                e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics = generate_fn(\n+                e2e_latency, timestamps, shape_and_decoded_output, gpu_metrics = self.time_generate(\n                     max_new_tokens=config.num_tokens_to_generate,\n+                    use_continuous_batching=config.continuous_batching,\n                     gpu_monitor=(GPUMonitor(logger=self.logger) if config.gpu_monitoring else None),\n                 )\n-                result.accumulate(e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics)\n+                result.accumulate(e2e_latency, timestamps, shape_and_decoded_output, gpu_metrics)\n             self.logger.info(\"Benchmarking done. Cleaning up.\")\n \n             # Profile if needed\n@@ -249,75 +256,50 @@ def run_benchmark(\n                 \"config\": config,\n             }\n \n-    # TODO: refactor `generate_batch` to handle streaming so we can use it here\n-    def time_generate_batch(\n-        self,\n-        max_new_tokens: int,\n-        gpu_monitor: GPUMonitor | None = None,\n-    ) -> tuple[float, list[float], str, GPURawMetrics | None]:\n-        if gpu_monitor is not None:\n-            gpu_monitor.start()\n-        # Prepare inputs\n-        inputs = self.inputs[\"input_ids\"].tolist()\n-        timestamps = []\n-        last_result_generated_tokens = None\n-        wall_time_0 = time.perf_counter()\n-        # We disable prefix sharing because all prompts are the same\n-        with self.model.continuous_batching_context_manager(allow_prefix_sharing=False) as manager:\n-            manager.add_requests(inputs, max_new_tokens=max_new_tokens, streaming=True)\n-            unfinished_requests = len(inputs)\n-            while unfinished_requests > 0:\n-                # NOTE: I don't like having the extra if stmt here, but hopefully won't degrade perf too much\n-                result = manager.get_result()\n-                if result is not None:\n-                    timestamps.append(time.perf_counter() - wall_time_0)  # FIXME: the timestamps are wrong\n-                    if result.is_finished():\n-                        last_result_generated_tokens = result.generated_tokens\n-                        unfinished_requests -= 1\n-                elif not manager.is_running():\n-                    raise RuntimeError(\"Generation thread exited unexpectedly\")\n-        # Post-processing\n-        wall_time_1 = time.perf_counter()\n-        e2e_latency = wall_time_1 - wall_time_0\n-        gpu_metrics = gpu_monitor.stop_and_collect() if gpu_monitor is not None else None\n-        decoded_output = self.tokenizer.decode(last_result_generated_tokens, skip_special_tokens=True)\n-        shape_and_decoded_output = f\"{(1, len(last_result_generated_tokens))} | {decoded_output}\"\n-        return e2e_latency, timestamps, shape_and_decoded_output, gpu_metrics\n-\n     def time_generate(\n         self,\n         max_new_tokens: int,\n+        use_continuous_batching: bool = False,\n         gpu_monitor: GPUMonitor | None = None,\n     ) -> tuple[float, list[float], str, GPURawMetrics | None]:\n-        \"\"\"Time the latency of a call to model.generate() with the given (inputs) and (max_new_tokens).\"\"\"\n         # Prepare gpu monitoring if needed\n         if gpu_monitor is not None:\n             gpu_monitor.start()\n-        # Prepare streamer\n-        streamer = BenchmarkStreamer()\n+\n         # Generate and time\n-        wall_time_0 = time.perf_counter()\n-        outputs = self.model.generate(\n-            **self.inputs,\n-            max_new_tokens=max_new_tokens,\n-            streamer=streamer,\n-        )\n+        if use_continuous_batching:\n+            inputs = self.inputs[\"input_ids\"].tolist()\n+            wall_time_0 = time.perf_counter()\n+            results = self.model.generate_batch(inputs, allow_prefix_sharing=False, record_timestamps=True)\n+        else:\n+            streamer = BenchmarkStreamer()\n+            wall_time_0 = time.perf_counter()\n+            results = self.model.generate(**self.inputs, streamer=streamer)\n+\n         wall_time_1 = time.perf_counter()\n-        # Stop gpu monitoring if needed\n         gpu_metrics = gpu_monitor.stop_and_collect() if gpu_monitor is not None else None\n-        # Check if generation had the right number of tokens\n+\n+        # Retrieve timestamps and results in a way that allows similar post-processing\n         input_tokens = self.inputs[\"input_ids\"].size(-1)\n-        batch_size, output_tokens = outputs.shape\n-        new_tokens = output_tokens - input_tokens\n-        if new_tokens != max_new_tokens:\n-            raise RuntimeError(f\"Generated {new_tokens} tokens, expected {max_new_tokens}\")\n+        if use_continuous_batching:\n+            timestamps = [result.timestamps for result in results.values()]\n+            results = torch.tensor([result.generated_tokens for result in results.values()])\n+        else:\n+            timestamps = [streamer.timestamps[1:]]  # skip the first timestamp because it's the input tokens\n+            results = results[:, input_tokens:]\n+\n+        # Check if generation had the right number of tokens\n+        if results.size(-1) != max_new_tokens:\n+            raise RuntimeError(f\"Generated {results.size(-1)} tokens, expected {max_new_tokens}\")\n+\n         # Decode outputs\n-        decoded_output = self.tokenizer.decode(outputs[0, input_tokens:], skip_special_tokens=True)\n-        shape_and_decoded_output = f\"{tuple(outputs.shape)} | {decoded_output}\"\n-        # Compute intermediate quantities\n+        decoded_output = self.tokenizer.decode(results[0], skip_special_tokens=True)\n+        shape_and_decoded_output = f\"{tuple(results.shape)} | {decoded_output}\"\n+\n+        # Compute metrics\n         e2e_latency = wall_time_1 - wall_time_0\n-        token_generation_times = [t - wall_time_0 for t in streamer.timestamps[1:]]\n-        return e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics\n+        timestamps = torch.tensor(timestamps).sub(wall_time_0).tolist()\n+        return e2e_latency, timestamps, shape_and_decoded_output, gpu_metrics\n \n     def profile_generate(self, num_tokens_to_profile: int, config_name: str) -> None:\n         \"\"\"Profile the latency of a call to model.generate() with the given (inputs) and (max_new_tokens).\"\"\"\n@@ -431,36 +413,38 @@ def push_results_to_hub(self, dataset_id: str, results: dict[Any, Any], timestam\n                 \"PUSH_TO_HUB_TOKEN is not set, cannot push results to the Hub. When setting dataset_id, please also set the PUSH_TO_HUB_TOKEN environment variable.\"\n             )\n \n+        api = HfApi()\n         n_results = len(results)\n-        self.logger.info(f\"Pushing {n_results} results to: {dataset_id}\")\n-        rows = []\n-        for cfg_hash, entry in results.items():\n-            row = {\n-                \"benchmark_config_hash\": cfg_hash,\n-                \"config\": entry[\"config\"].to_dict(),\n-                \"measurements\": entry[\"measurements\"].to_dict(),\n-                \"metadata\": entry[\"metadata\"].to_dict(),\n-            }\n-            rows.append(row)\n-\n-        ds = Dataset.from_list(rows)\n-        with tempfile.TemporaryDirectory() as tmp:\n-            jsonl_path = os.path.join(tmp, \"data.jsonl\")\n-            with open(jsonl_path, \"w\") as f:\n-                json_lines = []\n-                for ex in ds:\n-                    json_lines.append(json.dumps(ex, ensure_ascii=False))\n-                f.write(\"\\n\".join(json_lines))\n-\n-            api = HfApi()\n-            # NOTE: we expect the repository to already exist\n-            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if not timestamp else timestamp\n-            file_name = f\"benchmark_run_{timestamp}.jsonl\"\n-            api.upload_file(\n-                path_or_fileobj=jsonl_path,\n-                path_in_repo=file_name,\n-                repo_id=dataset_id,\n-                repo_type=\"dataset\",\n-                token=PUSH_TO_HUB_TOKEN,\n-            )\n-        self.logger.info(f\"Successfully uploaded results to: {dataset_id}\")\n+        for summarized in [False, True]:\n+            self.logger.info(f\"Pushing {n_results} results to: {dataset_id} with {summarized = }\")\n+            rows = []\n+            for cfg_hash, entry in results.items():\n+                row = {\n+                    \"benchmark_config_hash\": cfg_hash,\n+                    \"config\": entry[\"config\"].to_dict(),\n+                    \"measurements\": entry[\"measurements\"].to_dict(summarized=summarized),\n+                    \"metadata\": entry[\"metadata\"].to_dict(),\n+                }\n+                rows.append(row)\n+\n+            ds = Dataset.from_list(rows)\n+            with tempfile.TemporaryDirectory() as tmp:\n+                file_name = \"summarized_results\" if summarized else \"full_results\"\n+                jsonl_path = os.path.join(tmp, f\"{file_name}.jsonl\")\n+                with open(jsonl_path, \"w\") as f:\n+                    json_lines = []\n+                    for ex in ds:\n+                        json_lines.append(json.dumps(ex, ensure_ascii=False))\n+                    f.write(\"\\n\".join(json_lines))\n+\n+                # NOTE: we expect the repository to already exist\n+                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if not timestamp else timestamp\n+                file_name = file_name + \"/\" + f\"benchmark_run_{timestamp}.jsonl\"\n+                api.upload_file(\n+                    path_or_fileobj=jsonl_path,\n+                    path_in_repo=file_name,\n+                    repo_id=dataset_id,\n+                    repo_type=\"dataset\",\n+                    token=PUSH_TO_HUB_TOKEN,\n+                )\n+                self.logger.info(f\"Successfully uploaded results to: {dataset_id} with {summarized = }\")"
        },
        {
            "sha": "380acd4664f692b9118812832c92c53c69b44013",
            "filename": "benchmark_v2/framework/data_classes.py",
            "status": "modified",
            "additions": 26,
            "deletions": 18,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/352a2e0c120370f7db3b40caa4c60f0efca4e737/benchmark_v2%2Fframework%2Fdata_classes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/352a2e0c120370f7db3b40caa4c60f0efca4e737/benchmark_v2%2Fframework%2Fdata_classes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fdata_classes.py?ref=352a2e0c120370f7db3b40caa4c60f0efca4e737",
            "patch": "@@ -89,31 +89,35 @@ class BenchmarkResult:\n \n     def __init__(self) -> None:\n         self.e2e_latency = []\n+        self._timestamps = []\n         self.time_to_first_token = []\n         self.inter_token_latency = []\n         self.shape_and_decoded_outputs = []\n         self.gpu_metrics = []\n \n-    def compute_itl(self, token_generation_times: list[float]) -> list[float]:\n-        return (token_generation_times[-1] - token_generation_times[0]) / len(token_generation_times)\n-\n     def accumulate(\n         self,\n         e2e_latency: float,\n-        token_generation_times: list[float],\n+        timestamps: list[float],\n         shape_and_decoded_output: str,\n         gpu_metrics: GPURawMetrics | None,\n     ) -> None:\n         self.e2e_latency.append(e2e_latency)\n-        self.time_to_first_token.append(token_generation_times[0])\n-        # inter-token latency is already an average in itself\n-        self.inter_token_latency.append(self.compute_itl(token_generation_times))\n+        self._timestamps.append(timestamps)\n+        self._accumulate_ttft_and_itl(timestamps)\n         self.shape_and_decoded_outputs.append(shape_and_decoded_output)\n         self.gpu_metrics.append(gpu_metrics)\n \n-    def to_dict(self) -> dict[str, None | int | float]:\n-        # Save GPU metrics as None if it contains only None values\n-        if all(gm is None for gm in self.gpu_metrics):\n+    def _accumulate_ttft_and_itl(self, timestamps: list[float]) -> None:\n+        timestamps = np.array(timestamps)\n+        tftt = np.min(timestamps[:, 0])\n+        itl = np.mean(timestamps[:, -1] - timestamps[:, 0]) / (timestamps.shape[1] - 1)\n+        self.time_to_first_token.append(tftt)\n+        self.inter_token_latency.append(itl)\n+\n+    def to_dict(self, summarized: bool = False) -> dict[str, Any]:\n+        # Save GPU metrics as None if it contains only None values or if we are summarizing\n+        if summarized or all(gm is None for gm in self.gpu_metrics):\n             gpu_metrics = None\n         else:\n             gpu_metrics = [gm.to_dict() for gm in self.gpu_metrics]\n@@ -123,6 +127,7 @@ def to_dict(self) -> dict[str, None | int | float]:\n             \"inter_token_latency\": self.inter_token_latency,\n             \"shape_and_decoded_outputs\": self.shape_and_decoded_outputs,\n             \"gpu_metrics\": gpu_metrics,\n+            \"timestamps\": None if summarized else self._timestamps,\n         }\n \n     @classmethod\n@@ -132,16 +137,19 @@ def from_dict(cls, data: dict[str, None | int | float]) -> \"BenchmarkResult\":\n             gpu_metrics = [None for _ in range(len(data[\"e2e_latency\"]))]\n         else:\n             gpu_metrics = [GPURawMetrics.from_dict(gm) for gm in data[\"gpu_metrics\"]]\n+        # Handle timestamps, which can be saved as None to reduce file size\n+        if data[\"timestamps\"] is None:\n+            timestamps = [None for _ in range(len(data[\"e2e_latency\"]))]\n+        else:\n+            timestamps = data[\"timestamps\"]\n         # Create a new instance and accumulate the data\n         new_instance = cls()\n-        for i in range(len(data[\"e2e_latency\"])):\n-            new_instance.accumulate(\n-                e2e_latency=data[\"e2e_latency\"][i],\n-                time_to_first_token=data[\"time_to_first_token\"][i],\n-                inter_token_latency=data[\"inter_token_latency\"][i],\n-                shape_and_decoded_output=data[\"shape_and_decoded_outputs\"][i],\n-                gpu_metrics=gpu_metrics[i],\n-            )\n+        new_instance.e2e_latency = data[\"e2e_latency\"]\n+        new_instance._timestamps = timestamps\n+        new_instance.time_to_first_token = data[\"time_to_first_token\"]\n+        new_instance.inter_token_latency = data[\"inter_token_latency\"]\n+        new_instance.shape_and_decoded_outputs = data[\"shape_and_decoded_outputs\"]\n+        new_instance.gpu_metrics = gpu_metrics\n         return new_instance\n \n     def get_throughput(self, total_generated_tokens: int) -> list[float]:"
        },
        {
            "sha": "1346a43b387e25013853cc374fa0a15effaaa6dc",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/352a2e0c120370f7db3b40caa4c60f0efca4e737/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/352a2e0c120370f7db3b40caa4c60f0efca4e737/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=352a2e0c120370f7db3b40caa4c60f0efca4e737",
            "patch": "@@ -379,7 +379,7 @@ def mark_blocks_as_complete(self, state: RequestState) -> None:\n         self._block_manager.mark_blocks_as_complete(\n             num_complete_blocks=num_complete_blocks,\n             allocated_blocks=cm.block_table[state.request_id],\n-            prompt_ids=(state.full_prompt_ids + state.static_outputs),\n+            prompt_ids=(state.initial_tokens + state.generated_tokens),\n         )\n \n "
        },
        {
            "sha": "faec08ae1e4dd63184e19e178dfed5e226716b37",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 25,
            "deletions": 12,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/352a2e0c120370f7db3b40caa4c60f0efca4e737/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/352a2e0c120370f7db3b40caa4c60f0efca4e737/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=352a2e0c120370f7db3b40caa4c60f0efca4e737",
            "patch": "@@ -408,9 +408,9 @@ def _handle_request_error(self, error: Exception, state: RequestState) -> None:\n \n         # Include any generated tokens if this is an active request\n         if isinstance(state.request_id, str):\n-            state.static_outputs = self.scheduler.get_active_request_static_outputs(state.request_id)\n+            state.generated_tokens = self.scheduler.get_active_request_static_outputs(state.request_id)\n         else:\n-            state.static_outputs = []\n+            state.generated_tokens = []\n \n         self.metrics.record_request_completion(state.created_time, state.request_id)\n         self.output_queue.put(state.to_generation_output())\n@@ -455,7 +455,7 @@ def prepare_next_batch(self) -> bool:\n         for state in self.requests_in_batch:\n             # First we retrieve the lengths related to the request\n             past_length = state.position_offset\n-            query_length = len(state.prompt_ids)\n+            query_length = len(state.tokens_to_process)\n             seqlens_k = self.cache.get_seqlens_k(state.request_id, past_length, query_length)\n \n             # Then we update the total lengths that are used for slicing\n@@ -467,12 +467,12 @@ def prepare_next_batch(self) -> bool:\n             state.position_offset += query_length\n \n             # Then we accumulate for the object used in the kwargs\n-            input_ids.extend(state.prompt_ids)\n+            input_ids.extend(state.tokens_to_process)\n             position_ids.extend(range(past_length, past_length + query_length))\n             cumulative_seqlens_q.append(cumulative_seqlens_q[-1] + query_length)\n             self.max_seqlen_q = max(self.max_seqlen_q, query_length)\n \n-            if not state.remaining_prompt_ids:\n+            if not state.remaining_prefill_tokens:\n                 logits_indices.append(cumulative_seqlens_q[-1] - 1)\n \n             for layer_type, layer_type_seqlen_k in seqlens_k.items():\n@@ -564,11 +564,11 @@ def update_batch(self) -> None:\n         out_tokens = self._sync()\n         for i, state in enumerate(self.requests_in_batch):\n             # If the request has no remaining prompt ids, it means prefill has already ended or just finished\n-            if len(state.remaining_prompt_ids) == 0:\n+            if len(state.remaining_prefill_tokens) == 0:\n                 self.metrics.record_ttft_metric(state.created_time, state.request_id)\n                 state.status = RequestStatus.DECODING\n                 token = out_tokens[self.logits_indices[i]]\n-                state.prompt_ids = [token]\n+                state.tokens_to_process = [token]\n                 # Update the request and stop if it is complete\n                 is_finished = state.update_and_check_completion(token)\n                 # We mark the completed blocks as such\n@@ -862,6 +862,7 @@ def add_request(\n         request_id: str | None = None,\n         max_new_tokens: int | None = None,\n         streaming: bool = False,\n+        record_timestamps: bool = False,\n     ) -> str:\n         \"\"\"Add a new generation request to the queue.\n \n@@ -883,8 +884,9 @@ def add_request(\n         # NOTE: do we want to handle a case when the user wants token ids returned instead of decoded text?\n         state = RequestState(\n             request_id=request_id,\n-            prompt_ids=list(input_ids),\n-            full_prompt_ids=list(input_ids),\n+            initial_tokens=list(input_ids),\n+            record_timestamps=record_timestamps,\n+            tokens_to_process=list(input_ids),\n             max_new_tokens=max_new_tokens,\n             eos_token_id=self.generation_config.eos_token_id,\n             streaming=streaming,\n@@ -895,10 +897,16 @@ def add_request(\n         return request_id\n \n     def add_requests(\n-        self, inputs: list[list[int]], max_new_tokens: int | None = None, streaming: bool = False\n+        self,\n+        inputs: list[list[int]],\n+        max_new_tokens: int | None = None,\n+        streaming: bool = False,\n+        record_timestamps: bool = False,\n     ) -> None:\n         for input_ids in inputs:\n-            self.add_request(input_ids, max_new_tokens=max_new_tokens, streaming=streaming)\n+            self.add_request(\n+                input_ids, max_new_tokens=max_new_tokens, streaming=streaming, record_timestamps=record_timestamps\n+            )\n \n     def cancel_request(self, request_id: str) -> None:\n         \"\"\"Cancel a request by its ID.\n@@ -1119,6 +1127,8 @@ def generate_batch(\n         progress_bar: bool = True,\n         num_q_cuda_graphs: int = 0,\n         num_kv_cuda_graphs: int = 0,\n+        allow_prefix_sharing: bool = True,\n+        record_timestamps: bool = False,\n         **kwargs,\n     ) -> dict[str, GenerationOutput]:\n         \"\"\"Generate sequences for a batch of prompts using continuous batching.\n@@ -1146,6 +1156,7 @@ def generate_batch(\n             generation_config=generation_config,\n             num_q_cuda_graphs=num_q_cuda_graphs,\n             num_kv_cuda_graphs=num_kv_cuda_graphs,\n+            allow_prefix_sharing=allow_prefix_sharing,\n         )\n         manager.start()\n         results = {}\n@@ -1160,7 +1171,9 @@ def generate_batch(\n                     desc=f\"Solving {num_requests} requests\",\n                     unit=\"request\",\n                 ) as pbar:\n-                    manager.add_requests(inputs=inputs, max_new_tokens=kwargs.get(\"max_new_tokens\"))\n+                    manager.add_requests(\n+                        inputs=inputs, max_new_tokens=kwargs.get(\"max_new_tokens\"), record_timestamps=record_timestamps\n+                    )\n                     finished_count = 0\n                     while finished_count < num_requests:\n                         result = manager.get_result(timeout=1)"
        },
        {
            "sha": "0dd1b0b2ce758522e3b4389edf5ad7c1241c4c54",
            "filename": "src/transformers/generation/continuous_batching/requests.py",
            "status": "modified",
            "additions": 32,
            "deletions": 18,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/352a2e0c120370f7db3b40caa4c60f0efca4e737/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/352a2e0c120370f7db3b40caa4c60f0efca4e737/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py?ref=352a2e0c120370f7db3b40caa4c60f0efca4e737",
            "patch": "@@ -88,7 +88,8 @@ class GenerationOutput:\n     logprobs: list[float] = field(default_factory=list)\n     error: str | None = None\n     status: RequestStatus = RequestStatus.PENDING\n-    created_time: float = field(default_factory=time.time)\n+    created_time: float = field(default_factory=time.perf_counter)\n+    timestamps: list[float] | None = None  # Timestamps of the generated tokens\n \n     def is_finished(self) -> bool:\n         return self.status == RequestStatus.FINISHED\n@@ -115,21 +116,25 @@ class RequestState:\n         error (Optional[str]): Any error message associated with the request. When None, has had no error yet.\n     \"\"\"\n \n-    # Required fields # TODO: come up with better names / not sure prompt_ids and such are not redundant\n+    # Required fields\n     request_id: str\n-    full_prompt_ids: list[int] | None = None  # Full initial prompt\n-    prompt_ids: list[int] | None = None  # Tokens IDs currently being processed\n-    remaining_prompt_ids: list[int] = field(default_factory=list)  # For split requests, prefill left to process\n-    static_outputs: list[int] = field(default_factory=list)  # Generated tokens\n+    initial_tokens: list[int]  # Initial prompt tokens\n+    # Optional fields\n+    record_timestamps: bool = False  # Whether to record timestamps for the generated tokens\n+    # Internal fields\n+    tokens_to_process: list[int] | None = None  # Tokens IDs currently being processed\n+    remaining_prefill_tokens: list[int] = field(default_factory=list)  # For split requests, prefill left to process\n+    generated_tokens: list[int] = field(default_factory=list)  # Generated tokens\n     allocated_blocks: int = 0  # Number of blocks allocated to the request\n     position_offset: int = 0  # Current position in the sequence for position_ids\n     _status: RequestStatus = RequestStatus.PENDING  # Status of the request, hidden behind a property\n     max_new_tokens: int = 20  # Maximum number of new tokens to generate\n     eos_token_id: int = -1  # ID of the end-of-sequence token\n     streaming: bool = False  # Whether to stream tokens as they're generated\n-    created_time: float = field(default_factory=time.time)  # Time the request was created\n+    created_time: float = field(default_factory=time.perf_counter)  # Time the request was created\n     error: str | None = None  # Error message if the request failed\n     lifespan: tuple[float, float] = (-1, -1)  # (time request was no longer pending, time request finished)\n+    _timestamps: list[float] = field(default_factory=list)  # Timestamps of the generated tokens\n \n     @property\n     def status(self) -> RequestStatus:\n@@ -138,14 +143,18 @@ def status(self) -> RequestStatus:\n     @status.setter\n     def status(self, value: RequestStatus):\n         if self._status == RequestStatus.PENDING:\n-            self.lifespan = (time.time(), -1)\n+            self.lifespan = (time.perf_counter(), -1)\n         elif value == RequestStatus.FINISHED:\n-            self.lifespan = (self.lifespan[0], time.time())\n+            self.lifespan = (self.lifespan[0], time.perf_counter())\n             self.log_end_of_request()\n         self._status = value\n \n+    @property\n+    def timestamps(self) -> list[float] | None:\n+        return self._timestamps if self.record_timestamps else None\n+\n     def log_end_of_request(self):\n-        prefill_len = len(self.full_prompt_ids)\n+        prefill_len = len(self.initial_tokens)\n         decode_len = self.generated_len()\n         start_time = self.lifespan[0] - self.created_time\n         end_time = self.lifespan[1] - self.created_time\n@@ -159,7 +168,7 @@ def current_len(self) -> int:\n \n     def generated_len(self) -> int:\n         \"\"\"Get the number of tokens generated so far.\"\"\"\n-        return len(self.static_outputs)\n+        return len(self.generated_tokens)\n \n     # TODO: this logic seems one token off, check it out\n     @traced\n@@ -176,13 +185,17 @@ def update_and_check_completion(self, token_id: int) -> bool:\n         if self.status != RequestStatus.DECODING:\n             return False\n \n+        # If we're recording timestamps, add timestamp to the list\n+        if self.record_timestamps:\n+            self._timestamps.append(time.perf_counter())\n+\n         is_eos = token_id == self.eos_token_id and self.eos_token_id != -1\n         is_max_len = self.generated_len() >= self.max_new_tokens\n \n         # Only add the token if we're not finishing due to max length\n         # (EOS tokens should still be added to the output)\n         if not (is_max_len and not is_eos):\n-            self.static_outputs.extend([token_id])\n+            self.generated_tokens.extend([token_id])\n \n         if is_eos or is_max_len:\n             self.status = RequestStatus.FINISHED\n@@ -194,22 +207,23 @@ def __repr__(self):\n             f\"request_id={self.request_id}\",\n             f\"status={self._status}\",\n             f\"out_tokens={self.generated_len()}\",\n-            f\"query_length={len(self.prompt_ids)}\",\n-            f\"remaining_tokens={len(self.remaining_prompt_ids)}\",\n+            f\"query_length={len(self.tokens_to_process)}\",\n+            f\"remaining_tokens={len(self.remaining_prefill_tokens)}\",\n             f\"kv_length={self.position_offset}\",\n-            f\"full_prompt_length={len(self.full_prompt_ids)}\",\n+            f\"full_prompt_length={len(self.initial_tokens)}\",\n             f\"allocated_blocks={self.allocated_blocks}\",\n-            f\"generated_tokens={self.static_outputs}\",\n+            f\"generated_tokens={self.generated_tokens}\",\n         ]\n         return \"RequestState(\\n\\t\" + \",\\n\\t\".join(msg) + \"\\n)\"\n \n     def to_generation_output(self):\n         \"\"\"Convert the request state to a GenerationOutput object.\"\"\"\n         return GenerationOutput(\n             request_id=self.request_id,\n-            prompt_ids=self.full_prompt_ids,\n+            prompt_ids=self.initial_tokens,\n             status=self.status,\n-            generated_tokens=self.static_outputs,\n+            generated_tokens=self.generated_tokens,\n             logprobs=[],\n             error=self.error,\n+            timestamps=self.timestamps,\n         )"
        },
        {
            "sha": "1ac6019530b6d1fd6693fc2a2f8047d201200ca6",
            "filename": "src/transformers/generation/continuous_batching/scheduler.py",
            "status": "modified",
            "additions": 17,
            "deletions": 15,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/352a2e0c120370f7db3b40caa4c60f0efca4e737/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/352a2e0c120370f7db3b40caa4c60f0efca4e737/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py?ref=352a2e0c120370f7db3b40caa4c60f0efca4e737",
            "patch": "@@ -42,7 +42,9 @@ def add_waiting_request(self, state: RequestState):\n         \"\"\"Adds a request to the waiting list.\"\"\"\n         if self.retain_cache_on_finish and state.request_id in self.active_requests:\n             old_state = self.active_requests.pop(state.request_id)\n-            state.prompt_ids = state.prompt_ids[len(old_state.full_prompt_ids) :]  # XXX: check for indexing error?\n+            state.tokens_to_process = state.tokens_to_process[\n+                len(old_state.initial_tokens) :\n+            ]  # XXX: check for indexing error?\n             state.allocated_blocks = old_state.allocated_blocks\n             state.position_offset = old_state.position_offset\n         self.waiting_requests[state.request_id] = state\n@@ -73,7 +75,7 @@ def finish_request(self, request_id: str, evict_from_cache: bool = True):\n     def get_active_request_static_outputs(self, request_id: str) -> list[int]:\n         \"\"\"Gets generated tokens for an active request.\"\"\"\n         if request_id in self.active_requests:\n-            return self.active_requests[request_id].static_outputs\n+            return self.active_requests[request_id].generated_tokens\n         return []\n \n     @traced\n@@ -113,7 +115,7 @@ def _allocate_blocks_if_needed(self, state: RequestState) -> bool:\n         # 1. we check that the occupancy is less than the requested length\n         # 2. we allocate enough blocks to cover the requested length\n         current_len = state.current_len()\n-        len_next_tokens = len(state.prompt_ids)\n+        len_next_tokens = len(state.tokens_to_process)\n         occupancy = state.allocated_blocks * self.cache.block_size - current_len\n         if occupancy < len_next_tokens or state.allocated_blocks == 0:\n             blocks_needed = ((len_next_tokens - occupancy + 1) // self.cache.block_size) + 1\n@@ -131,23 +133,23 @@ def _prepare_request_for_processing(\n         pending, this is where we look for a prefix match and split the request if found.\"\"\"\n         # If prefix sharing is enabled, we look for a prefix match and split the request if found\n         if self.cache.use_prefix_sharing and state.status == RequestStatus.PENDING:\n-            prefill_length = self.cache.search_prefix_match(state.request_id, state.prompt_ids)\n+            prefill_length = self.cache.search_prefix_match(state.request_id, state.tokens_to_process)\n             if prefill_length > 0:\n                 self.active_requests[state.request_id] = state\n                 request_ids_to_remove_from_waiting.add(state.request_id)\n                 state.status = RequestStatus.SPLIT_PENDING_REMAINDER\n                 # Even if we match the whole request, we keep at least 1 token to start decoding\n-                prefill_length = min(prefill_length, len(state.prompt_ids) - 1)\n-                state.remaining_prompt_ids = state.prompt_ids[prefill_length:]\n-                state.prompt_ids = state.prompt_ids[prefill_length:]\n+                prefill_length = min(prefill_length, len(state.tokens_to_process) - 1)\n+                state.remaining_prefill_tokens = state.tokens_to_process[prefill_length:]\n+                state.tokens_to_process = state.tokens_to_process[prefill_length:]\n                 state.position_offset += prefill_length\n \n         # If the request has a split prefill, the tokens to process are the remaining prompt ids\n         if state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n-            request_tokens = state.remaining_prompt_ids\n+            request_tokens = state.remaining_prefill_tokens\n         # Otherwise, the tokens to process are the prompt ids, which are the full prompt or the last predicted tokens\n         else:\n-            request_tokens = state.prompt_ids\n+            request_tokens = state.tokens_to_process\n \n         if len(request_tokens) < token_budget:\n             # Can process the entire prompt/remainder\n@@ -157,8 +159,8 @@ def _prepare_request_for_processing(\n                 request_ids_to_remove_from_waiting.add(state.request_id)\n             elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n                 state.status = RequestStatus.PREFILLING\n-                state.prompt_ids = state.remaining_prompt_ids\n-                state.remaining_prompt_ids = []\n+                state.tokens_to_process = state.remaining_prefill_tokens\n+                state.remaining_prefill_tokens = []\n         else:\n             # Need to split the request\n             if state.status == RequestStatus.PENDING:\n@@ -167,8 +169,8 @@ def _prepare_request_for_processing(\n                 request_ids_to_remove_from_waiting.add(state.request_id)\n             elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n                 state.status = RequestStatus.PREFILLING_SPLIT\n-            state.remaining_prompt_ids = request_tokens[token_budget:]\n-            state.prompt_ids = request_tokens[:token_budget]\n+            state.remaining_prefill_tokens = request_tokens[token_budget:]\n+            state.tokens_to_process = request_tokens[:token_budget]\n \n \n # TODO: further common-ize the two classes\n@@ -214,7 +216,7 @@ def schedule_batch(self, token_budget: int) -> list[RequestState]:\n                 break\n \n             self._prepare_request_for_processing(state, token_budget, request_ids_to_remove_from_waiting)\n-            request_len = len(state.prompt_ids)\n+            request_len = len(state.tokens_to_process)\n             # If we can't allocate blocks, do not schedule the request and break if the cache is full\n             if not self._allocate_blocks_if_needed(state):\n                 if self.cache.get_num_free_blocks() == 0:\n@@ -280,7 +282,7 @@ def schedule_batch(self, token_budget: int) -> list[RequestState]:\n \n         for state in candidates:\n             self._prepare_request_for_processing(state, token_budget, request_ids_to_remove_from_waiting)\n-            request_len = len(state.prompt_ids)\n+            request_len = len(state.tokens_to_process)\n             # If we can't allocate blocks, do not schedule the request and break if the cache is full\n             if not self._allocate_blocks_if_needed(state):\n                 if self.cache.get_num_free_blocks() == 0:"
        }
    ],
    "stats": {
        "total": 335,
        "additions": 178,
        "deletions": 157
    }
}