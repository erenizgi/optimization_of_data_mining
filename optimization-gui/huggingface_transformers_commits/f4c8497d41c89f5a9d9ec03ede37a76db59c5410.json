{
    "author": "remi-or",
    "message": "Update torchcodec to match torchaudio version (#42288)",
    "sha": "f4c8497d41c89f5a9d9ec03ede37a76db59c5410",
    "files": [
        {
            "sha": "52369945b362fb7dd7123675e6bc6d8376856b6d",
            "filename": "docker/transformers-pytorch-amd-gpu/Dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f4c8497d41c89f5a9d9ec03ede37a76db59c5410/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/f4c8497d41c89f5a9d9ec03ede37a76db59c5410/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile?ref=f4c8497d41c89f5a9d9ec03ede37a76db59c5410",
            "patch": "@@ -34,7 +34,7 @@ RUN python3 -m pip uninstall py3nvml pynvml nvidia-ml-py apex -y\n RUN python3 -m pip uninstall -y kernels\n \n # On ROCm, torchcodec is required to decode audio files and 0.4 or 0.6 fails\n-RUN python3 -m pip install --no-cache-dir \"torchcodec==0.5\"\n+RUN python3 -m pip install --no-cache-dir \"torchcodec==0.7\"\n \n # Install flash attention from source. Tested with commit 6387433156558135a998d5568a9d74c1778666d8\n RUN git clone https://github.com/ROCm/flash-attention/ -b tridao && \\"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}