{
    "author": "echarlaix",
    "message": "Fix ONNX export for sequence classification head  (#36332)\n\n* set dtype to int32\n\n* fix style",
    "sha": "055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
    "files": [
        {
            "sha": "f91cd85779a315f99f6caa5275941509e03192d1",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1151,7 +1151,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "f97adac913a13acc2ef4aee5f60d31571c66fa5a",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -796,7 +796,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "ab38de3a392d60143bf6de9aa70f8fc991585d11",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1234,7 +1234,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "61c27f5f29677dce781fe8961f889d0f19d08ec8",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1363,7 +1363,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "bf835a980556cd88f00e9cb761c0eafe585a5be1",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -967,7 +967,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "e5f73756d890e978aa553d7404e5720249e7f990",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1060,7 +1060,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "fbefb0b1b03ce13ca608281f34f17c93345ec23c",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -976,7 +976,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "0c045094e7dc14fbd26526387faf5ef3b629bd0a",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1400,7 +1400,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "d388f70a777b9351105e2ab1a9f1bdf984be1f03",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1288,7 +1288,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "ff963fd665dfbd372d73caf91a54884ebafb584e",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1121,7 +1121,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "fd7dac0653979278b4ccd7bd2ccba8cc378058c6",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -950,7 +950,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "8e64fd120bab043e0b676031fcc4ffbd77c005b7",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -634,7 +634,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "57feb2c53607a7f3e04e5cbac52bd67115488325",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1263,7 +1263,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "7a46de8dd550aae0b0942d08b85e0e8c93c644f5",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -963,7 +963,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "482ed8177a40225a5d27baba9616f0809a0044ff",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1687,7 +1687,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "872c3628a05d8ce63a889a32296e366bb4467ad4",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1478,7 +1478,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "eb37b60529138327cf9acf3c114d44c436d7ff2b",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -965,7 +965,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "b4ff18cb11770c9026a204b23894f5ac37b8134e",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1042,7 +1042,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "172b2d7b3e2100f8d066f877005ff307e0153876",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1195,7 +1195,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "4de99990396959dd2f9b08c6eba414cfd2d1d5fa",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -685,7 +685,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "c23f35caa09bc455c5ff3e63a1bacd4b981bcc28",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1211,7 +1211,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "f777d95bf896b9701ea0ad5f92ee798df4337a29",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -812,7 +812,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "c13f43f6967ad069597ed31ff8f707a08ed2dfe9",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1322,7 +1322,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "7d6965620b8b9864d023b7d3e6a5b6ab5262bdb2",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1022,7 +1022,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "04500e834c8ebda52d8d57b8f770af734f6c3098",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -939,7 +939,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "c656d56265df2ef73ed8aa48eff1be8e386aed5a",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1068,7 +1068,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "c419a7e534e4bf7dcabf508fd3e2d3d26940d0ba",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1602,7 +1602,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "2a2b45538b92ba01ce4bb891db85ad5c057a588f",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -967,7 +967,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "a0ee19a6a3a2f6a376ed3bc9568bffe63da47ca6",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1444,7 +1444,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "a1806dfe24ad70f3aad42e57b83e1ed716c910b2",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1278,7 +1278,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "a1eb407d6082a8616c610a25c4d6ca3ed9c88b94",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -950,7 +950,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "2a7a13780e13f646696d13541b1c754d6b574106",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1440,7 +1440,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        },
        {
            "sha": "772500b42c393fa4f495f89986254f10567996ad",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=055afdb6bb1f2e4bd5ab4870ba5b3733bb88da4e",
            "patch": "@@ -1877,7 +1877,7 @@ def forward(\n         elif input_ids is not None:\n             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n             last_non_pad_token = -1"
        }
    ],
    "stats": {
        "total": 66,
        "additions": 33,
        "deletions": 33
    }
}