{
    "author": "jackzhxng",
    "message": "Support input_embeds in torch exportable decoders (#39836)\n\n* Support input_embeds in torch exportable decoders\n\n* Hybrid cache update\n\n* Manually change some callsites\n\n* AI changes the rest of the call sites\n\n* Make either input_ids/inputs_embeds mandatory\n\n* Clean up\n\n* Ruff check --fix\n\n* Fix test\n\n* pr review\n\n* Revert config/generation_config changes\n\n* Ruff check",
    "sha": "6121e9e46c4fc4e5c91d9f927aef5490691850cf",
    "files": [
        {
            "sha": "f9bc88eaa13870b3925f1a35f509cd3523fcc566",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 139,
            "deletions": 73,
            "changes": 212,
            "blob_url": "https://github.com/huggingface/transformers/blob/6121e9e46c4fc4e5c91d9f927aef5490691850cf/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6121e9e46c4fc4e5c91d9f927aef5490691850cf/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=6121e9e46c4fc4e5c91d9f927aef5490691850cf",
            "patch": "@@ -198,59 +198,65 @@ class TorchExportableModuleForDecoderOnlyLM(torch.nn.Module):\n     def __init__(\n         self,\n         model: PreTrainedModel,\n-        max_batch_size: int = 1,\n-        max_cache_len: int = 4096,\n     ):\n         \"\"\"\n         Initializes the exportable module with `HybridCache`.\n \n         Args:\n             model (`PreTrainedModel`): The pretrained model to wrap.\n-            max_batch_size (int): Maximum batch size for the cache.\n-            max_cache_len (int): Maximum sequence length for the cache.\n \n         Raises:\n             ValueError: If the model is configured with a unsupported cache implementation.\n         \"\"\"\n         super().__init__()\n \n-        if not hasattr(model.config, \"use_cache\") or model.config.use_cache is False:\n+        config = model.config.get_text_config()\n+        _generation_config = model.generation_config\n+\n+        if not hasattr(config, \"use_cache\") or config.use_cache is False:\n             raise ValueError(\"The model must have caching enabled to be performant.\")\n \n-        if hasattr(model.config, \"layer_types\") and getattr(model.config, \"sliding_window\", None) is not None:\n-            self.model = TorchExportableModuleWithHybridCache(model, max_batch_size, max_cache_len)\n+        if hasattr(config, \"layer_types\") and getattr(config, \"sliding_window\", None) is not None:\n+            self.model = TorchExportableModuleWithHybridCache(model)\n         else:\n             # If `layer_types` is not specified explicitly in the config or `sliding_window` is null,\n             # there is only 1 type of layers, so export will use `StaticCache` by default.\n             logging.info(\n                 \"Using `StaticCache` for export as `layer_types` is not specified or `sliding_window` is `null` in the config.\"\n             )\n-            self.model = TorchExportableModuleWithStaticCache(model, max_batch_size, max_cache_len)\n+            self.model = TorchExportableModuleWithStaticCache(model)\n         # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n         ALL_MASK_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", sdpa_mask_without_vmap)\n         ALL_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n         self.model.model.config._attn_implementation = \"sdpa_without_vmap\"\n \n     def forward(\n         self,\n-        input_ids: torch.Tensor,\n-        cache_position: torch.Tensor,\n+        input_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Forward pass of the module, which is compatible with the ExecuTorch llm runner.\n \n         Args:\n             input_ids (`torch.Tensor`): Tensor representing current input token id to the module.\n+            inputs_embeds (`torch.Tensor`): Tensor representing current input embeddings to the module.\n             cache_position (`torch.Tensor`): Tensor representing current input position in the cache.\n \n         Returns:\n             torch.Tensor: Logits output from the model.\n         \"\"\"\n-        return self.model.forward(input_ids, cache_position)\n+        return self.model.forward(\n+            input_ids=input_ids,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+        )\n \n     def export(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         dynamic_shapes: Optional[dict] = None,\n         strict: Optional[bool] = None,\n@@ -260,14 +266,49 @@ def export(\n \n         Args:\n             input_ids (`Optional[torch.Tensor]`):\n-                Tensor representing current input token id to the module. If not provided, a default tensor will be used.\n+                Tensor representing current input token id to the module. Must specify either this or inputs_embeds.\n+            inputs_embeds (`Optional[torch.Tensor]`):\n+                Tensor representing current input embeddings to the module. Must specify either this or input_ids.\n             cache_position (`Optional[torch.Tensor]`):\n                 Tensor representing current input position in the cache. If not provided, a default tensor will be used.\n             dynamic_shapes (`Optional[dict]`):\n                 Dynamic shapes to use for export if specified.\n             strict(`Optional[bool]`):\n                 Flag to instruct `torch.export` to use `torchdynamo`.\n+\n+        Returns:\n+            torch.export.ExportedProgram: The exported program that can be used for inference.\n+\n+        Examples:\n+            Export with input_ids:\n+            ```python\n+            # Prepare inputs\n+            input_ids = torch.tensor([[1, 2, 3]], dtype=torch.long, device=model.device)\n+            cache_position = torch.arange(input_ids.shape[-1], dtype=torch.long, device=model.device)\n+\n+            # Export\n+            exported = exportable_module.export(\n+                input_ids=input_ids,\n+                cache_position=cache_position\n+            )\n+            ```\n+\n+            Export with inputs_embeds:\n+            ```python\n+            # Prepare embeddings\n+            inputs_embeds = torch.randn(1, 3, 768, device=model.device)  # batch_size=1, seq_len=3, hidden_size=768\n+            cache_position = torch.arange(inputs_embeds.shape[1], dtype=torch.long, device=model.device)\n+\n+            # Export\n+            exported = exportable_module.export(\n+                inputs_embeds=inputs_embeds,\n+                cache_position=cache_position\n+            )\n+            ```\n         \"\"\"\n+        if not (input_ids is None) ^ (inputs_embeds is None):\n+            raise ValueError(\"Need to specify either input_ids or inputs_embeds.\")\n+\n         if hasattr(self.model, \"base_model_prefix\"):\n             base = getattr(self.model, self.model.base_model_prefix, self.model)\n             model_device = base.device\n@@ -279,20 +320,29 @@ def export(\n                 \"TorchExportableModuleForDecoderOnlyLM.export Can't infer device from the model. Set to CPU by default.\"\n             )\n \n-        example_input_ids = (\n-            input_ids if input_ids is not None else torch.tensor([[1]], dtype=torch.long, device=model_device)\n-        )\n-        example_cache_position = (\n-            cache_position if cache_position is not None else torch.tensor([0], dtype=torch.long, device=model_device)\n-        )\n+        if input_ids is not None:\n+            input_kwargs = {\n+                \"input_ids\": input_ids,\n+                \"cache_position\": cache_position\n+                if cache_position is not None\n+                else torch.arange(input_ids.shape[-1], dtype=torch.long, model=model_device),\n+            }\n+        else:  # inputs_embeds\n+            input_kwargs = {\n+                \"inputs_embeds\": inputs_embeds,\n+                \"cache_position\": cache_position\n+                if cache_position is not None\n+                else torch.arange(inputs_embeds.shape[1], dtype=torch.long, model=model_device),\n+            }\n \n         exported_program = torch.export.export(\n             self.model,\n-            args=(example_input_ids, example_cache_position),\n-            kwargs={},\n+            args=(),\n+            kwargs=input_kwargs,\n             dynamic_shapes=dynamic_shapes,\n             strict=strict if strict is not None else True,\n         )\n+\n         return exported_program\n \n     @staticmethod\n@@ -341,7 +391,7 @@ def generate(\n             curr_cache_position = torch.tensor([curr_position], dtype=torch.long, device=device)\n \n             # Forward pass\n-            _ = exported_module(curr_input_ids, curr_cache_position)\n+            _ = exported_module(input_ids=curr_input_ids, cache_position=curr_cache_position)\n             curr_position += 1\n \n         # Generate new tokens\n@@ -351,7 +401,7 @@ def generate(\n             curr_cache_position = torch.tensor([curr_position], dtype=torch.long, device=device)\n \n             # Forward pass to get next token logits\n-            outputs = exported_module(curr_input_ids, curr_cache_position)\n+            outputs = exported_module(input_ids=curr_input_ids, cache_position=curr_cache_position)\n \n             # Get the next token ID\n             if do_sample:\n@@ -418,66 +468,75 @@ class TorchExportableModuleWithStaticCache(torch.nn.Module):\n     def __init__(\n         self,\n         model: PreTrainedModel,\n-        max_batch_size: int = 1,\n-        max_cache_len: int = 4096,\n     ):\n         \"\"\"\n         Initializes the wrapper module with the pretrained model.\n \n         Args:\n             model (`PreTrainedModel`): The pretrained model to wrap. The model must have caching\n-            enabled and use a 'static' caching implementation.\n+                enabled and use a 'static' caching implementation.\n \n         Raises:\n             AssertionError: If the pretrained model does not have caching enabled or if it does\n             not use a 'static' caching implementation in `model.generation_config`.\n         \"\"\"\n         super().__init__()\n \n+        config = model.config.get_text_config()\n+        generation_config = model.generation_config\n+\n         # Sanity checks\n-        if model.generation_config is None:\n-            # Use default generation config if not specified\n-            model.generation_config = GenerationConfig(\n-                use_cache=model.config.use_cache,\n-                cache_implementation=\"static\",\n-                max_length=max_cache_len,\n-                cache_config={\n-                    \"batch_size\": max_batch_size,\n-                    \"max_cache_len\": max_cache_len,\n-                    \"device\": \"cpu\",\n-                },\n+        if generation_config is None:\n+            raise AssertionError(\n+                \"The model must have a generation config to be exported with static caching. \"\n+                \"Please set `generation_config` in `model`.\"\n             )\n-\n-        if not model.generation_config.use_cache:\n+        if \"batch_size\" not in generation_config.cache_config:\n+            raise ValueError(\n+                \"The model's generation config must specify a batch_size in its cache_config. \"\n+                'Try GenerationConfig( ... cache_config={\"batch_size\": 1, ...} ...)'\n+            )\n+        if \"max_cache_len\" not in generation_config.cache_config:\n+            raise ValueError(\n+                \"The model's generation config must specify a max_cache_len in its cache_config. \"\n+                'Try GenerationConfig( ... cache_config={\"max_cache_len\": 4096, ...} ...)'\n+            )\n+        if not generation_config.use_cache:\n             raise AssertionError(\n                 \"The model must have caching enabled to be exported with static caching. \"\n                 \"Please set `generation_config.use_cache=True`.\"\n             )\n-\n-        if model.generation_config.cache_implementation != \"static\":\n+        if generation_config.cache_implementation != \"static\":\n             raise AssertionError(\n                 \"The model must use a 'static' caching implementation to be exported with static caching. \"\n                 \"Please set `generation_config.cache_implementation='static'`.\"\n             )\n \n         self.model = model\n         self.static_cache = StaticCache(\n-            config=self.model.config,\n-            max_batch_size=self.model.generation_config.cache_config.get(\"batch_size\"),\n-            max_cache_len=self.model.generation_config.cache_config.get(\"max_cache_len\"),\n-            device=self.model.generation_config.cache_config.get(\"device\"),\n+            config=config,\n+            max_batch_size=generation_config.cache_config.get(\"batch_size\"),\n+            max_cache_len=generation_config.cache_config.get(\"max_cache_len\"),\n+            device=generation_config.cache_config.get(\"device\"),\n             dtype=self.model.dtype,\n         )\n+\n         for i in range(len(self.static_cache)):\n             self.register_buffer(f\"key_cache_{i}\", self.static_cache.layers[i].keys, persistent=False)\n             self.register_buffer(f\"value_cache_{i}\", self.static_cache.layers[i].values, persistent=False)\n \n-    def forward(self, input_ids: torch.Tensor, cache_position: torch.Tensor):\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+    ):\n         \"\"\"\n         Forward pass of the module, which is compatible with the ExecuTorch runtime.\n \n         Args:\n             input_ids (`torch.Tensor`): Tensor representing current input token id to the module.\n+            inputs_embeds (`torch.Tensor`): Tensor representing current input embeddings to the module.\n             cache_position (`torch.Tensor`): Tensor representing current input position in the cache.\n \n         Returns:\n@@ -493,15 +552,13 @@ def forward(self, input_ids: torch.Tensor, cache_position: torch.Tensor):\n             The adapter matches the model's forward signature with that in `executorch/extension/llm/runner`,\n             ensuring that the exported model can be executed in `ExecuTorch` out-of-the-box.\n         \"\"\"\n-        _, seqlen = input_ids.shape\n-        position_ids = cache_position.unsqueeze(0)\n         past_key_values = self.static_cache\n \n         outs = self.model(\n             input_ids=input_ids,\n-            attention_mask=None,\n-            position_ids=position_ids,\n+            inputs_embeds=inputs_embeds,\n             cache_position=cache_position,\n+            attention_mask=None,\n             past_key_values=past_key_values,\n             use_cache=True,\n         )\n@@ -576,33 +633,45 @@ class TorchExportableModuleWithHybridCache(torch.nn.Module):\n     def __init__(\n         self,\n         model: PreTrainedModel,\n-        max_batch_size: int = 1,\n-        max_cache_len: int = 4096,\n     ):\n         \"\"\"\n         Initializes the exportable module with `HybridCache`.\n \n         Args:\n             model (`PreTrainedModel`): The pretrained model to wrap.\n-            max_batch_size (int): Maximum batch size for the cache.\n-            max_cache_len (int): Maximum sequence length for the cache.\n \n         Raises:\n             AssertionError: If the model doesn't have the expected configuration for HybridCache.\n         \"\"\"\n         super().__init__()\n         self.model = model\n+        config = model.config.get_text_config()\n+        generation_config = model.generation_config\n \n-        # Verify the model is configured for HybridCache\n-        if not self.model.config.use_cache:\n-            raise AssertionError(\"Model must have caching enabled\")\n+        if generation_config is None:\n+            raise AssertionError(\n+                \"The model must have a generation config to be exported with static caching. \"\n+                \"Please set `generation_config` in `model`.\"\n+            )\n+        if \"batch_size\" not in generation_config.cache_config:\n+            raise ValueError(\n+                \"The model's generation config must specify a batch_size in its cache_config. \"\n+                'Try GenerationConfig( ... cache_config={\"batch_size\": 1, ...} ...)'\n+            )\n+        if \"max_cache_len\" not in generation_config.cache_config:\n+            raise ValueError(\n+                \"The model's generation config must specify a max_cache_len in its cache_config. \"\n+                'Try GenerationConfig( ... cache_config={\"max_cache_len\": 4096, ...} ...)'\n+            )\n+        if not config.use_cache:\n+            raise AssertionError(\"Model must have caching enabled.\")\n \n         # Initialize the HybridCache\n         self.cache = HybridCache(\n-            config=self.model.config,\n-            max_batch_size=max_batch_size,\n-            max_cache_len=max_cache_len,\n-            device=self.model.device,\n+            config=config,\n+            max_batch_size=generation_config.cache_config.get(\"batch_size\"),\n+            max_cache_len=generation_config.cache_config.get(\"max_cache_len\"),\n+            device=generation_config.cache_config.get(\"device\"),\n             dtype=self.model.dtype,\n         )\n \n@@ -613,32 +682,29 @@ def __init__(\n \n     def forward(\n         self,\n-        input_ids: torch.Tensor,\n-        cache_position: torch.Tensor,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Forward pass of the module, which is compatible with the ExecuTorch llm runner.\n \n         Args:\n             input_ids (`torch.Tensor`): Tensor representing current input token id to the module.\n+            inputs_embeds (`Optional[torch.Tensor]`): Tensor representing current input embeddings to the module.\n             cache_position (`torch.Tensor`): Tensor representing current input position in the cache.\n \n         Returns:\n             torch.Tensor: Logits output from the model.\n         \"\"\"\n-        batch_size = input_ids.shape[0]\n-\n-        # Generate position_ids from cache_position\n-        position_ids = cache_position.unsqueeze(0).expand(batch_size, -1)\n-\n         # Forward pass with the model\n         outputs = self.model(\n             input_ids=input_ids,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n             attention_mask=None,\n-            position_ids=position_ids,\n             past_key_values=self.cache,\n             use_cache=True,\n-            cache_position=cache_position,\n         )\n \n         # Return only the logits to simplify the export\n@@ -692,8 +758,8 @@ def convert_and_export_with_cache(\n         if is_torch_greater_or_equal(\"2.6.0\"):\n             exported_program = torch.export.export(\n                 TorchExportableModuleWithStaticCache(model),\n-                args=(example_input_ids, example_cache_position),\n-                kwargs={},\n+                args=(),\n+                kwargs={\"input_ids\": example_input_ids, \"cache_position\": example_cache_position},\n                 dynamic_shapes=dynamic_shapes,\n                 strict=strict if strict is not None else True,\n             )\n@@ -710,8 +776,8 @@ def convert_and_export_with_cache(\n             # export API and pre_dispatch=False. Switch to use the public API once the issue is included in 2.5 release.\n             exported_program = torch.export._trace._export(\n                 TorchExportableModuleWithStaticCache(model),\n-                args=(example_input_ids,),\n-                kwargs={\"cache_position\": example_cache_position},\n+                args=(),\n+                kwargs={\"input_ids\": example_input_ids, \"cache_position\": example_cache_position},\n                 pre_dispatch=False,\n                 strict=True,\n             )"
        },
        {
            "sha": "8a1e2ea9eb7fe74d6eec7611f979b3f3389d1402",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=6121e9e46c4fc4e5c91d9f927aef5490691850cf",
            "patch": "@@ -460,7 +460,10 @@ def test_export_static_cache(self):\n         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n \n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n-        exported_program = exportable_module.export()\n+        exported_program = exportable_module.export(\n+            input_ids=prompt_token_ids,\n+            cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+        )\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )"
        },
        {
            "sha": "4a6c326f780c426881c2e8ab1ef02c6dd212600a",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=6121e9e46c4fc4e5c91d9f927aef5490691850cf",
            "patch": "@@ -365,7 +365,10 @@ def test_export_static_cache(self):\n         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n \n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n-        exported_program = exportable_module.export()\n+        exported_program = exportable_module.export(\n+            input_ids=prompt_token_ids,\n+            cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+        )\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )\n@@ -389,7 +392,10 @@ def test_export_hybrid_cache(self):\n         # Export + HybridCache\n         model.eval()\n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n-        exported_program = exportable_module.export()\n+        exported_program = exportable_module.export(\n+            input_ids=torch.tensor([[1]], dtype=torch.long, device=model.device),\n+            cache_position=torch.tensor([0], dtype=torch.long, device=model.device),\n+        )\n \n         # Test generation with the exported model\n         prompt = \"What is the capital of France?\""
        },
        {
            "sha": "46e6c11a57c96b37cb29dfb9acb399bea63a8d53",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=6121e9e46c4fc4e5c91d9f927aef5490691850cf",
            "patch": "@@ -822,7 +822,10 @@ def test_export_text_only_with_hybrid_cache(self):\n         # Export + HybridCache\n         model.eval()\n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n-        exported_program = exportable_module.export()\n+        exported_program = exportable_module.export(\n+            input_ids=torch.tensor([[1]], dtype=torch.long, device=model.device),\n+            cache_position=torch.tensor([0], dtype=torch.long, device=model.device),\n+        )\n         logging.info(f\"\\nExported program: {exported_program}\")\n \n         # Test generation with the exported model"
        },
        {
            "sha": "a6c2c3eee2b6ee56366e3952aae4c91b5b7a497e",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=6121e9e46c4fc4e5c91d9f927aef5490691850cf",
            "patch": "@@ -353,7 +353,10 @@ def test_export_static_cache(self):\n             from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n \n             exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n-            exported_program = exportable_module.export()\n+            exported_program = exportable_module.export(\n+                input_ids=prompt_token_ids,\n+                cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+            )\n             ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n                 exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n             )"
        },
        {
            "sha": "ea23f4e96fdabdba9671b1ca90fb9ad91d5345bb",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=6121e9e46c4fc4e5c91d9f927aef5490691850cf",
            "patch": "@@ -384,7 +384,10 @@ def test_export_static_cache(self):\n         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n \n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n-        exported_program = exportable_module.export()\n+        exported_program = exportable_module.export(\n+            input_ids=prompt_token_ids,\n+            cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+        )\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )"
        },
        {
            "sha": "6887c0c6cd6448eaabdddb4d78faa7306c373dd0",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=6121e9e46c4fc4e5c91d9f927aef5490691850cf",
            "patch": "@@ -417,7 +417,10 @@ def test_export_static_cache(self):\n         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n \n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n-        exported_program = exportable_module.export()\n+        exported_program = exportable_module.export(\n+            input_ids=prompt_token_ids,\n+            cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+        )\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )"
        },
        {
            "sha": "51bd943cf916b3dcd78198dc4a364803b88dc21a",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=6121e9e46c4fc4e5c91d9f927aef5490691850cf",
            "patch": "@@ -303,7 +303,11 @@ def test_export_static_cache(self):\n         strict = version.parse(torch.__version__) != version.parse(\n             \"2.7.0\"\n         )  # Due to https://github.com/pytorch/pytorch/issues/150994\n-        exported_program = exportable_module.export(strict=strict)\n+        exported_program = exportable_module.export(\n+            input_ids=prompt_token_ids,\n+            cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+            strict=strict,\n+        )\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )"
        },
        {
            "sha": "205228073e198901f1ee3ac7e4b8ca2641a070d1",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=6121e9e46c4fc4e5c91d9f927aef5490691850cf",
            "patch": "@@ -293,7 +293,11 @@ def test_export_static_cache(self):\n         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n \n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n-        exported_program = exportable_module.export(strict=strict)\n+        exported_program = exportable_module.export(\n+            input_ids=prompt_token_ids,\n+            cache_position=torch.arange(prompt_token_ids.shape[-1], dtype=torch.long, device=model.device),\n+            strict=strict,\n+        )\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )"
        },
        {
            "sha": "0e33253c08f1a0fd0f782934d2ee19d34ac74a97",
            "filename": "tests/test_executorch.py",
            "status": "added",
            "additions": 129,
            "deletions": 0,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Ftest_executorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Ftest_executorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_executorch.py?ref=6121e9e46c4fc4e5c91d9f927aef5490691850cf",
            "patch": "@@ -0,0 +1,129 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+\n+from transformers import AutoModelForCausalLM, set_seed\n+from transformers.generation.configuration_utils import GenerationConfig\n+from transformers.integrations.executorch import (\n+    TorchExportableModuleForDecoderOnlyLM,\n+    TorchExportableModuleWithHybridCache,\n+    TorchExportableModuleWithStaticCache,\n+)\n+from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_3\n+from transformers.testing_utils import require_torch\n+\n+\n+@require_torch\n+class ExecutorchTest(unittest.TestCase):\n+    def setUp(self):\n+        if not is_torch_greater_or_equal_than_2_3:\n+            self.skipTest(\"torch >= 2.3 is required\")\n+\n+        set_seed(0)\n+        self.model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-LlamaForCausalLM\")\n+        self.model.eval()\n+\n+        # Create generation config with static cache for the model\n+        self.model.generation_config = GenerationConfig(\n+            use_cache=True,\n+            cache_implementation=\"static\",\n+            cache_config={\"batch_size\": 1, \"max_cache_len\": 32, \"device\": \"cpu\"},\n+        )\n+\n+        self.input_ids = torch.tensor([[1, 2, 3]], dtype=torch.long)\n+        self.inputs_embeds = torch.randn(1, 3, self.model.config.hidden_size)\n+        self.cache_position = torch.arange(3, dtype=torch.long)\n+\n+    def test_static_cache_module_forward(self):\n+        \"\"\"Test TorchExportableModuleWithStaticCache forward with both input types\"\"\"\n+        generation_config = GenerationConfig(\n+            use_cache=True,\n+            cache_implementation=\"static\",\n+            cache_config={\"batch_size\": 1, \"max_cache_len\": 32, \"device\": \"cpu\"},\n+        )\n+\n+        # Set generation config on model\n+        self.model.generation_config = generation_config\n+        module = TorchExportableModuleWithStaticCache(self.model)\n+\n+        # Test with input_ids\n+        eager_output_ids = self.model(input_ids=self.input_ids, use_cache=False).logits\n+        wrapped_output_ids = module.forward(input_ids=self.input_ids, cache_position=self.cache_position)\n+        torch.testing.assert_close(eager_output_ids, wrapped_output_ids, atol=1e-4, rtol=1e-4)\n+\n+        # Test with inputs_embeds\n+        eager_output_embeds = self.model(inputs_embeds=self.inputs_embeds, use_cache=False).logits\n+        wrapped_output_embeds = module.forward(inputs_embeds=self.inputs_embeds, cache_position=self.cache_position)\n+        torch.testing.assert_close(eager_output_embeds, wrapped_output_embeds, atol=1e-4, rtol=1e-4)\n+\n+    def test_hybrid_cache_module_forward(self):\n+        \"\"\"Test TorchExportableModuleWithHybridCache forward with both input types\"\"\"\n+        config = self.model.config\n+        config.sliding_window = 16\n+        config.layer_types = [\"full_attention\"] * config.num_hidden_layers\n+\n+        generation_config = GenerationConfig(\n+            use_cache=True,\n+            cache_implementation=\"hybrid\",\n+            cache_config={\"batch_size\": 1, \"max_cache_len\": 32, \"device\": \"cpu\"},\n+        )\n+\n+        # Set generation config on model\n+        self.model.generation_config = generation_config\n+        module = TorchExportableModuleWithHybridCache(self.model)\n+\n+        # Test with input_ids\n+        eager_output_ids = self.model(input_ids=self.input_ids, use_cache=False).logits\n+        wrapped_output_ids = module.forward(input_ids=self.input_ids, cache_position=self.cache_position)\n+        torch.testing.assert_close(eager_output_ids, wrapped_output_ids, atol=1e-4, rtol=1e-4)\n+\n+        # Test with inputs_embeds\n+        eager_output_embeds = self.model(inputs_embeds=self.inputs_embeds, use_cache=False).logits\n+        wrapped_output_embeds = module.forward(inputs_embeds=self.inputs_embeds, cache_position=self.cache_position)\n+        torch.testing.assert_close(eager_output_embeds, wrapped_output_embeds, atol=1e-4, rtol=1e-4)\n+\n+    def test_decoder_only_lm_export_validation(self):\n+        \"\"\"Test TorchExportableModuleForDecoderOnlyLM export validation\"\"\"\n+        module = TorchExportableModuleForDecoderOnlyLM(self.model)\n+\n+        # Should fail with both input_ids and inputs_embeds\n+        with self.assertRaises(ValueError):\n+            module.export(input_ids=self.input_ids, inputs_embeds=self.inputs_embeds)\n+\n+        # Should fail with neither\n+        with self.assertRaises(ValueError):\n+            module.export()\n+\n+    def test_decoder_only_lm_export(self):\n+        \"\"\"Test TorchExportableModuleForDecoderOnlyLM export with both input types\"\"\"\n+        module = TorchExportableModuleForDecoderOnlyLM(self.model)\n+\n+        # Test export with input_ids\n+        exported_program_ids = module.export(input_ids=self.input_ids, cache_position=self.cache_position)\n+        eager_output_ids = self.model(input_ids=self.input_ids, use_cache=False).logits\n+        exported_output_ids = exported_program_ids.module()(\n+            input_ids=self.input_ids, cache_position=self.cache_position\n+        )\n+        torch.testing.assert_close(eager_output_ids, exported_output_ids, atol=1e-4, rtol=1e-4)\n+\n+        # Test export with inputs_embeds\n+        exported_program_embeds = module.export(inputs_embeds=self.inputs_embeds, cache_position=self.cache_position)\n+        eager_output_embeds = self.model(inputs_embeds=self.inputs_embeds, use_cache=False).logits\n+        exported_output_embeds = exported_program_embeds.module()(\n+            inputs_embeds=self.inputs_embeds, cache_position=self.cache_position\n+        )\n+        torch.testing.assert_close(eager_output_embeds, exported_output_embeds, atol=1e-4, rtol=1e-4)"
        },
        {
            "sha": "74b19395a67f3b1a4b10220c8898fbd14604a1b8",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6121e9e46c4fc4e5c91d9f927aef5490691850cf/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=6121e9e46c4fc4e5c91d9f927aef5490691850cf",
            "patch": "@@ -841,8 +841,24 @@ def test_hybrid_cache_exportability(self):\n         model.eval()\n         max_batch_size = 1\n         max_cache_len = 23\n-        exportable_module = TorchExportableModuleForDecoderOnlyLM(model, max_batch_size, max_cache_len)\n-        exported_program = exportable_module.export()\n+        # Set generation config on the model for the hybrid cache model\n+        from transformers.generation.configuration_utils import GenerationConfig\n+\n+        model.generation_config = GenerationConfig(\n+            use_cache=True,\n+            cache_implementation=\"hybrid\",\n+            max_length=max_cache_len,\n+            cache_config={\n+                \"batch_size\": max_batch_size,\n+                \"max_cache_len\": max_cache_len,\n+                \"device\": model.device,\n+            },\n+        )\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+        exported_program = exportable_module.export(\n+            input_ids=torch.tensor([[1]], dtype=torch.long, device=model.device),\n+            cache_position=torch.tensor([0], dtype=torch.long, device=model.device),\n+        )\n         n_g_key_caches = n_g_value_caches = 0\n         for buffer_name, buffer in exported_program.named_buffers():\n             if buffer_name.startswith(\"key_cache\"):"
        }
    ],
    "stats": {
        "total": 408,
        "additions": 324,
        "deletions": 84
    }
}