{
    "author": "gante",
    "message": "[pipelines] add support to `skip_special_tokens` in the main text generation pipelines (#40356)\n\n* add support to skip_special_tokens in pipelines\n\n* add test\n\n* rm redundant",
    "sha": "9c25820978ea1b132bb145258959a852b2de4b7b",
    "files": [
        {
            "sha": "4d7037d52bf8b0b32b3cc212090ba7e469baf120",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c25820978ea1b132bb145258959a852b2de4b7b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c25820978ea1b132bb145258959a852b2de4b7b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=9c25820978ea1b132bb145258959a852b2de4b7b",
            "patch": "@@ -1754,6 +1754,9 @@ def _prepare_generation_config(\n                 for key, model_gen_config_value in model_generation_config.__dict__.items():\n                     if key.startswith(\"_\") or key == \"transformers_version\":  # metadata\n                         continue\n+                    # Don't set `cache_implementation = 'hybrid'` from the model defaults, see #40135\n+                    if key == \"cache_implementation\" and model_generation_config.cache_implementation == \"hybrid\":\n+                        continue\n                     global_default_value = getattr(global_default_generation_config, key, None)\n                     custom_gen_config_value = getattr(generation_config, key, None)\n                     if ("
        },
        {
            "sha": "480b83120da202d350fa66f495cfd43ce31b353a",
            "filename": "src/transformers/pipelines/image_text_to_text.py",
            "status": "modified",
            "additions": 31,
            "deletions": 14,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c25820978ea1b132bb145258959a852b2de4b7b/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c25820978ea1b132bb145258959a852b2de4b7b/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py?ref=9c25820978ea1b132bb145258959a852b2de4b7b",
            "patch": "@@ -203,22 +203,33 @@ def _sanitize_parameters(\n         clean_up_tokenization_spaces=None,\n         stop_sequence=None,\n         continue_final_message=None,\n+        skip_special_tokens=None,\n         **kwargs: Unpack[ProcessingKwargs],\n     ):\n         forward_kwargs = {}\n         preprocess_params = {}\n         postprocess_params = {}\n-        preprocess_params.update(kwargs)\n \n+        # Preprocess params\n+        preprocess_params.update(kwargs)\n         if timeout is not None:\n             preprocess_params[\"timeout\"] = timeout\n-\n         if continue_final_message is not None:\n             preprocess_params[\"continue_final_message\"] = continue_final_message\n \n+        # Forward kwargs\n+        if generate_kwargs is not None:\n+            forward_kwargs[\"generate_kwargs\"] = generate_kwargs\n+        if stop_sequence is not None:\n+            stop_sequence_ids = self.processor.tokenizer.encode(stop_sequence, add_special_tokens=False)\n+            if len(stop_sequence_ids) > 1:\n+                logger.warning_once(\n+                    \"Stopping on a multiple token sequence is not yet supported on transformers. The first token of\"\n+                    \" the stop sequence will be used as the stop sequence string in the interim.\"\n+                )\n+            generate_kwargs[\"eos_token_id\"] = stop_sequence_ids[0]\n         if generate_kwargs is not None:\n             forward_kwargs[\"generate_kwargs\"] = generate_kwargs\n-\n         if max_new_tokens is not None:\n             if \"generate_kwargs\" not in forward_kwargs:\n                 forward_kwargs[\"generate_kwargs\"] = {}\n@@ -229,6 +240,7 @@ def _sanitize_parameters(\n                 )\n             forward_kwargs[\"generate_kwargs\"][\"max_new_tokens\"] = max_new_tokens\n \n+        # Postprocess params\n         if return_full_text is not None and return_type is None:\n             if return_tensors is not None:\n                 raise ValueError(\"`return_full_text` is mutually exclusive with `return_tensors`\")\n@@ -241,14 +253,9 @@ def _sanitize_parameters(\n             postprocess_params[\"continue_final_message\"] = continue_final_message\n         if clean_up_tokenization_spaces is not None:\n             postprocess_params[\"clean_up_tokenization_spaces\"] = clean_up_tokenization_spaces\n-        if stop_sequence is not None:\n-            stop_sequence_ids = self.processor.tokenizer.encode(stop_sequence, add_special_tokens=False)\n-            if len(stop_sequence_ids) > 1:\n-                logger.warning_once(\n-                    \"Stopping on a multiple token sequence is not yet supported on transformers. The first token of\"\n-                    \" the stop sequence will be used as the stop sequence string in the interim.\"\n-                )\n-            generate_kwargs[\"eos_token_id\"] = stop_sequence_ids[0]\n+        if skip_special_tokens is not None:\n+            postprocess_params[\"skip_special_tokens\"] = skip_special_tokens\n+\n         return preprocess_params, forward_kwargs, postprocess_params\n \n     @overload\n@@ -432,7 +439,12 @@ def _forward(self, model_inputs, generate_kwargs=None):\n         return {\"generated_sequence\": generated_sequence, \"prompt_text\": prompt_text, \"input_ids\": input_ids}\n \n     def postprocess(\n-        self, model_outputs, return_type=ReturnType.FULL_TEXT, continue_final_message=None, **postprocess_kwargs\n+        self,\n+        model_outputs,\n+        return_type=ReturnType.FULL_TEXT,\n+        continue_final_message=None,\n+        skip_special_tokens=None,\n+        **postprocess_kwargs,\n     ):\n         input_texts = model_outputs[\"prompt_text\"]\n         input_texts = [input_texts] if isinstance(input_texts, (str, Chat)) else input_texts\n@@ -445,8 +457,13 @@ def postprocess(\n             ]\n \n         # Decode inputs and outputs the same way to remove input text from generated text if present\n-        generated_texts = self.processor.post_process_image_text_to_text(generated_sequence, **postprocess_kwargs)\n-        decoded_inputs = self.processor.post_process_image_text_to_text(input_ids, **postprocess_kwargs)\n+        skip_special_tokens = skip_special_tokens if skip_special_tokens is not None else True\n+        generated_texts = self.processor.post_process_image_text_to_text(\n+            generated_sequence, skip_special_tokens=skip_special_tokens, **postprocess_kwargs\n+        )\n+        decoded_inputs = self.processor.post_process_image_text_to_text(\n+            input_ids, skip_special_tokens=skip_special_tokens, **postprocess_kwargs\n+        )\n \n         # Force consistent behavior for including the input text in the output\n         if return_type in {ReturnType.NEW_TEXT, ReturnType.FULL_TEXT}:"
        },
        {
            "sha": "9df6441e8932bdefda32c6bc1cee1d4c84d73727",
            "filename": "src/transformers/pipelines/text_generation.py",
            "status": "modified",
            "additions": 19,
            "deletions": 14,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c25820978ea1b132bb145258959a852b2de4b7b/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c25820978ea1b132bb145258959a852b2de4b7b/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_generation.py?ref=9c25820978ea1b132bb145258959a852b2de4b7b",
            "patch": "@@ -157,10 +157,11 @@ def _sanitize_parameters(\n         truncation=None,\n         max_length=None,\n         continue_final_message=None,\n+        skip_special_tokens=None,\n         **generate_kwargs,\n     ):\n+        # preprocess kwargs\n         preprocess_params = {}\n-\n         add_special_tokens = False\n         if \"add_special_tokens\" in generate_kwargs:\n             add_special_tokens = preprocess_params[\"add_special_tokens\"] = generate_kwargs.pop(\"add_special_tokens\")\n@@ -193,10 +194,20 @@ def _sanitize_parameters(\n \n         if continue_final_message is not None:\n             preprocess_params[\"continue_final_message\"] = continue_final_message\n-\n         preprocess_params.update(generate_kwargs)\n+\n+        # forward kwargs\n+        if stop_sequence is not None:\n+            stop_sequence_ids = self.tokenizer.encode(stop_sequence, add_special_tokens=False)\n+            generate_kwargs[\"eos_token_id\"] = stop_sequence_ids\n         forward_params = generate_kwargs\n+        if self.assistant_model is not None:\n+            forward_params[\"assistant_model\"] = self.assistant_model\n+        if self.assistant_tokenizer is not None:\n+            forward_params[\"tokenizer\"] = self.tokenizer\n+            forward_params[\"assistant_tokenizer\"] = self.assistant_tokenizer\n \n+        # postprocess kwargs\n         postprocess_params = {}\n         if return_full_text is not None and return_type is None:\n             if return_text is not None:\n@@ -214,16 +225,8 @@ def _sanitize_parameters(\n             postprocess_params[\"clean_up_tokenization_spaces\"] = clean_up_tokenization_spaces\n         if continue_final_message is not None:\n             postprocess_params[\"continue_final_message\"] = continue_final_message\n-\n-        if stop_sequence is not None:\n-            stop_sequence_ids = self.tokenizer.encode(stop_sequence, add_special_tokens=False)\n-            generate_kwargs[\"eos_token_id\"] = stop_sequence_ids\n-\n-        if self.assistant_model is not None:\n-            forward_params[\"assistant_model\"] = self.assistant_model\n-        if self.assistant_tokenizer is not None:\n-            forward_params[\"tokenizer\"] = self.tokenizer\n-            forward_params[\"assistant_tokenizer\"] = self.assistant_tokenizer\n+        if skip_special_tokens is not None:\n+            postprocess_params[\"skip_special_tokens\"] = skip_special_tokens\n \n         return preprocess_params, forward_params, postprocess_params\n \n@@ -462,6 +465,7 @@ def postprocess(\n         return_type=ReturnType.FULL_TEXT,\n         clean_up_tokenization_spaces=True,\n         continue_final_message=None,\n+        skip_special_tokens=None,\n     ):\n         generated_sequence = model_outputs[\"generated_sequence\"][0]\n         input_ids = model_outputs[\"input_ids\"]\n@@ -480,14 +484,15 @@ def postprocess(\n                     if isinstance(v, tf.Tensor) and v.shape[0] == len(generated_sequence):\n                         splitted_keys[k] = v.numpy().tolist()\n \n+        skip_special_tokens = skip_special_tokens if skip_special_tokens is not None else True\n         for idx, sequence in enumerate(generated_sequence):\n             if return_type == ReturnType.TENSORS:\n                 record = {\"generated_token_ids\": sequence}\n             elif return_type in {ReturnType.NEW_TEXT, ReturnType.FULL_TEXT}:\n                 # Decode text\n                 text = self.tokenizer.decode(\n                     sequence,\n-                    skip_special_tokens=True,\n+                    skip_special_tokens=skip_special_tokens,\n                     clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n                 )\n \n@@ -498,7 +503,7 @@ def postprocess(\n                     prompt_length = len(\n                         self.tokenizer.decode(\n                             input_ids[0],\n-                            skip_special_tokens=True,\n+                            skip_special_tokens=skip_special_tokens,\n                             clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n                         )\n                     )"
        },
        {
            "sha": "063d86d65a847230bbd9359f7c635e53f5c4974b",
            "filename": "tests/pipelines/test_pipelines_text_generation.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9c25820978ea1b132bb145258959a852b2de4b7b/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9c25820978ea1b132bb145258959a852b2de4b7b/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_generation.py?ref=9c25820978ea1b132bb145258959a852b2de4b7b",
            "patch": "@@ -540,3 +540,18 @@ def test_pipeline_assisted_generation(self):\n         # It is running assisted generation under the hood (e.g. flags incompatible with assisted gen will crash)\n         with self.assertRaises(ValueError):\n             _ = pipe(prompt, generate_kwargs={\"num_beams\": 2})\n+\n+    @require_torch\n+    def test_pipeline_skip_special_tokens(self):\n+        \"\"\"Tests that we can use `skip_special_tokens=False` to get the special tokens in the output\"\"\"\n+        model_id = \"google/gemma-3-270m-it\"\n+        chat = [{\"role\": \"user\", \"content\": \"What's your name?\"}]\n+        generator = pipeline(\"text-generation\", model=model_id)\n+\n+        # normal pipeline use\n+        output = generator(chat, max_new_tokens=20, do_sample=False)\n+        self.assertNotIn(\"<end_of_turn>\", str(output[0][\"generated_text\"]))\n+\n+        # forcing special tokens to be included in the output\n+        output = generator(chat, max_new_tokens=1000, do_sample=False, skip_special_tokens=False)\n+        self.assertIn(\"<end_of_turn>\", str(output[0][\"generated_text\"]))"
        }
    ],
    "stats": {
        "total": 96,
        "additions": 68,
        "deletions": 28
    }
}