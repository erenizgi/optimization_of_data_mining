{
    "author": "cyyever",
    "message": "Simplify unnecessary Optional typing (#40839)\n\nRemove Optional\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
    "files": [
        {
            "sha": "e62880e7062c2c3a2d9e84f1172dfead34569946",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
            "patch": "@@ -370,7 +370,7 @@ def chroma_filter_bank(\n     tuning: float = 0.0,\n     power: Optional[float] = 2.0,\n     weighting_parameters: Optional[tuple[float, float]] = (5.0, 2.0),\n-    start_at_c_chroma: Optional[bool] = True,\n+    start_at_c_chroma: bool = True,\n ):\n     \"\"\"\n     Creates a chroma filter bank, i.e a linear transformation to project spectrogram bins onto chroma bins.\n@@ -391,7 +391,7 @@ def chroma_filter_bank(\n         weighting_parameters (`tuple[float, float]`, *optional*, defaults to `(5., 2.)`):\n             If specified, apply a Gaussian weighting parameterized by the first element of the tuple being the center and\n             the second element being the Gaussian half-width.\n-        start_at_c_chroma (`float`, *optional*, defaults to `True`):\n+        start_at_c_chroma (`bool`, *optional*, defaults to `True`):\n             If True, the filter bank will start at the 'C' pitch class. Otherwise, it will start at 'A'.\n     Returns:\n         `np.ndarray` of shape `(num_frequency_bins, num_chroma)`\n@@ -627,7 +627,7 @@ def spectrogram(\n     reference: float = 1.0,\n     min_value: float = 1e-10,\n     db_range: Optional[float] = None,\n-    remove_dc_offset: Optional[bool] = None,\n+    remove_dc_offset: bool = False,\n     dtype: np.dtype = np.float32,\n ) -> np.ndarray:\n     \"\"\"\n@@ -838,7 +838,7 @@ def spectrogram_batch(\n     reference: float = 1.0,\n     min_value: float = 1e-10,\n     db_range: Optional[float] = None,\n-    remove_dc_offset: Optional[bool] = None,\n+    remove_dc_offset: bool = False,\n     dtype: np.dtype = np.float32,\n ) -> list[np.ndarray]:\n     \"\"\""
        },
        {
            "sha": "f414ebe11e60788174f3d846e9226144d333963c",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
            "patch": "@@ -141,7 +141,7 @@ class TransformersTranscriptionCreateParams(TranscriptionCreateParamsBase, total\n \n         file: bytes  # Overwritten -- pydantic isn't happy with `typing.IO[bytes]`, present in the original type\n         generation_config: str\n-        stream: Optional[bool] = False\n+        stream: bool = False\n \n     # Contrarily to OpenAI's output types, input types are `TypedDict`, which don't have built-in validation.\n     response_validator = TypeAdapter(TransformersResponseCreateParamsStreaming)\n@@ -600,7 +600,7 @@ def validate_transcription_request(self, request: dict):\n \n     def build_chat_completion_chunk(\n         self,\n-        request_id: Optional[str] = \"\",\n+        request_id: str = \"\",\n         content: Optional[int] = None,\n         model: Optional[str] = None,\n         role: Optional[str] = None,"
        },
        {
            "sha": "b8197b02eb60b9618699ba961a226cac1c9f10bb",
            "filename": "src/transformers/data/datasets/squad.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py?ref=646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
            "patch": "@@ -118,9 +118,9 @@ def __init__(\n         tokenizer: PreTrainedTokenizer,\n         limit_length: Optional[int] = None,\n         mode: Union[str, Split] = Split.train,\n-        is_language_sensitive: Optional[bool] = False,\n+        is_language_sensitive: bool = False,\n         cache_dir: Optional[str] = None,\n-        dataset_format: Optional[str] = \"pt\",\n+        dataset_format: str = \"pt\",\n     ):\n         self.args = args\n         self.is_language_sensitive = is_language_sensitive"
        },
        {
            "sha": "1a91532983f2765a7bd9a40856ab51e479f23d06",
            "filename": "src/transformers/generation/beam_search.py",
            "status": "modified",
            "additions": 15,
            "deletions": 13,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fbeam_search.py?ref=646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
            "patch": "@@ -165,10 +165,10 @@ def __init__(\n         batch_size: int,\n         num_beams: int,\n         device: torch.device,\n-        length_penalty: Optional[float] = 1.0,\n-        do_early_stopping: Optional[Union[bool, str]] = False,\n-        num_beam_hyps_to_keep: Optional[int] = 1,\n-        num_beam_groups: Optional[int] = 1,\n+        length_penalty: float = 1.0,\n+        do_early_stopping: Union[bool, str] = False,\n+        num_beam_hyps_to_keep: int = 1,\n+        num_beam_groups: int = 1,\n         max_length: Optional[int] = None,\n     ):\n         logger.warning_once(\n@@ -214,7 +214,7 @@ def __init__(\n \n     @property\n     def is_done(self) -> bool:\n-        return self._done.all()\n+        return self._done.all().item()\n \n     def process(\n         self,\n@@ -225,8 +225,8 @@ def process(\n         pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n         eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n         beam_indices: Optional[torch.LongTensor] = None,\n-        group_index: Optional[int] = 0,\n-        decoder_prompt_len: Optional[int] = 0,\n+        group_index: int = 0,\n+        decoder_prompt_len: int = 0,\n     ) -> dict[str, torch.Tensor]:\n         # add up to the length which the next_scores is calculated on (including decoder prompt)\n         cur_len = input_ids.shape[-1] + 1\n@@ -460,9 +460,9 @@ def __init__(\n         num_beams: int,\n         constraints: list[Constraint],\n         device: torch.device,\n-        length_penalty: Optional[float] = 1.0,\n-        do_early_stopping: Optional[Union[bool, str]] = False,\n-        num_beam_hyps_to_keep: Optional[int] = 1,\n+        length_penalty: float = 1.0,\n+        do_early_stopping: Union[bool, str] = False,\n+        num_beam_hyps_to_keep: int = 1,\n         max_length: Optional[int] = None,\n     ):\n         logger.warning_once(\n@@ -495,7 +495,7 @@ def __init__(\n \n     @property\n     def is_done(self) -> bool:\n-        return self._done.all()\n+        return self._done.all().item()\n \n     def make_constraint_states(self, n):\n         return [ConstraintListState([constraint.copy() for constraint in self.constraints]) for _ in range(n)]\n@@ -515,7 +515,7 @@ def process(\n         pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n         eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n         beam_indices: Optional[torch.LongTensor] = None,\n-        decoder_prompt_len: Optional[int] = 0,\n+        decoder_prompt_len: int = 0,\n     ) -> tuple[torch.Tensor]:\n         r\"\"\"\n         Args:\n@@ -912,7 +912,9 @@ def finalize(\n \n \n class BeamHypotheses:\n-    def __init__(self, num_beams: int, length_penalty: float, early_stopping: bool, max_length: Optional[int] = None):\n+    def __init__(\n+        self, num_beams: int, length_penalty: float, early_stopping: Union[bool, str], max_length: Optional[int] = None\n+    ):\n         \"\"\"\n         Initialize n-best list of hypotheses.\n         \"\"\""
        },
        {
            "sha": "98a0d14ade1ad4af6373e5a3e92c15ccef56c0d2",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
            "patch": "@@ -1282,11 +1282,11 @@ class WatermarkingConfig(BaseWatermarkingConfig):\n \n     def __init__(\n         self,\n-        greenlist_ratio: Optional[float] = 0.25,\n-        bias: Optional[float] = 2.0,\n-        hashing_key: Optional[int] = 15485863,\n-        seeding_scheme: Optional[str] = \"lefthash\",\n-        context_width: Optional[int] = 1,\n+        greenlist_ratio: float = 0.25,\n+        bias: float = 2.0,\n+        hashing_key: int = 15485863,\n+        seeding_scheme: str = \"lefthash\",\n+        context_width: int = 1,\n     ):\n         self.greenlist_ratio = greenlist_ratio\n         self.bias = bias"
        },
        {
            "sha": "47447b6c936795eb72bfe9c302f14cb06a59e62b",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
            "patch": "@@ -85,7 +85,7 @@ def validate_fast_preprocess_arguments(\n     size: Optional[SizeDict] = None,\n     interpolation: Optional[\"F.InterpolationMode\"] = None,\n     return_tensors: Optional[Union[str, TensorType]] = None,\n-    data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+    data_format: ChannelDimension = ChannelDimension.FIRST,\n ):\n     \"\"\"\n     Checks validity of typically used arguments in an `ImageProcessorFast` `preprocess` method."
        },
        {
            "sha": "4fba01df425a68c5d2caa785a7a345854da33f69",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
            "patch": "@@ -96,7 +96,7 @@ def load_adapter(\n         adapter_name: Optional[str] = None,\n         revision: Optional[str] = None,\n         token: Optional[str] = None,\n-        device_map: Optional[str] = \"auto\",\n+        device_map: str = \"auto\",\n         max_memory: Optional[str] = None,\n         offload_folder: Optional[str] = None,\n         offload_index: Optional[int] = None,"
        },
        {
            "sha": "0bffc4382b1e3ad866c91589eb9b539c959c78b0",
            "filename": "src/transformers/model_debugging_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodel_debugging_utils.py?ref=646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
            "patch": "@@ -269,8 +269,8 @@ def clean(val):\n \n def _attach_debugger_logic(\n     model,\n-    debug_path: Optional[str] = \".\",\n-    do_prune_layers: Optional[bool] = True,\n+    debug_path: str = \".\",\n+    do_prune_layers: bool = True,\n     use_repr: bool = True,\n ):\n     \"\"\"\n@@ -399,8 +399,8 @@ def top_wrapped_forward(*inps, **kws):\n def model_addition_debugger_context(\n     model,\n     debug_path: Optional[str] = None,\n-    do_prune_layers: Optional[bool] = True,\n-    use_repr: Optional[bool] = True,\n+    do_prune_layers: bool = True,\n+    use_repr: bool = True,\n ):\n     \"\"\"\n     # Model addition debugger - context manager for model adders"
        },
        {
            "sha": "0f349278e15c1a32efe42c087c39a1d1b5f1d807",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
            "patch": "@@ -3466,7 +3466,7 @@ def _get_resized_lm_head(\n         self,\n         old_lm_head: nn.Linear,\n         new_num_tokens: Optional[int] = None,\n-        transposed: Optional[bool] = False,\n+        transposed: bool = False,\n         mean_resizing: bool = True,\n     ) -> nn.Linear:\n         \"\"\"\n@@ -3623,7 +3623,7 @@ def _init_added_lm_head_weights_with_mean(\n         old_lm_head_dim,\n         old_num_tokens,\n         added_num_tokens,\n-        transposed=False,\n+        transposed: bool = False,\n     ):\n         if transposed:\n             # Transpose to the desired shape for the function."
        },
        {
            "sha": "fdcf0a2f627f49855c3f24d49fa42c17c0c4d5e6",
            "filename": "src/transformers/pipelines/token_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py?ref=646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
            "patch": "@@ -151,7 +151,7 @@ def _sanitize_parameters(\n         ignore_subwords: Optional[bool] = None,\n         aggregation_strategy: Optional[AggregationStrategy] = None,\n         offset_mapping: Optional[list[tuple[int, int]]] = None,\n-        is_split_into_words: Optional[bool] = False,\n+        is_split_into_words: bool = False,\n         stride: Optional[int] = None,\n         delimiter: Optional[str] = None,\n     ):"
        },
        {
            "sha": "b89e570931526a24555b4ff86ee1797f906f41fc",
            "filename": "src/transformers/tokenization_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Ftokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Ftokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils.py?ref=646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
            "patch": "@@ -587,11 +587,11 @@ def _add_tokens(self, new_tokens: Union[list[str], list[AddedToken]], special_to\n         self._update_total_vocab_size()\n         return added_tokens\n \n-    def _update_trie(self, unique_no_split_tokens: Optional[str] = []):\n+    def _update_trie(self, unique_no_split_tokens: Optional[list[str]] = None):\n         for token in self._added_tokens_decoder.values():\n             if token.content not in self.tokens_trie._tokens:\n                 self.tokens_trie.add(token.content)\n-        for token in unique_no_split_tokens:\n+        for token in unique_no_split_tokens or []:\n             if token not in self.tokens_trie._tokens:\n                 self.tokens_trie.add(token)\n "
        },
        {
            "sha": "69b3ec977241204d0688c4b23d926472c8d27c03",
            "filename": "src/transformers/utils/chat_template_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Futils%2Fchat_template_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Futils%2Fchat_template_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fchat_template_utils.py?ref=646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
            "patch": "@@ -468,9 +468,9 @@ def render_jinja_template(\n     tools: Optional[list[Union[dict, Callable]]] = None,\n     documents: Optional[list[dict[str, str]]] = None,\n     chat_template: Optional[str] = None,\n-    return_assistant_tokens_mask: Optional[bool] = False,\n-    continue_final_message: Optional[bool] = False,\n-    add_generation_prompt: Optional[bool] = False,\n+    return_assistant_tokens_mask: bool = False,\n+    continue_final_message: bool = False,\n+    add_generation_prompt: bool = False,\n     **kwargs,\n ) -> str:\n     if return_assistant_tokens_mask and not re.search(r\"\\{\\%-?\\s*generation\\s*-?\\%\\}\", chat_template):"
        },
        {
            "sha": "6afd42e0a724f6c3d76f85cb5a26e2eec08fcafa",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/646ff51d1ad0a440f2fc63b6daa49c83e3c00932/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=646ff51d1ad0a440f2fc63b6daa49c83e3c00932",
            "patch": "@@ -1882,9 +1882,9 @@ class BitNetQuantConfig(QuantizationConfigMixin):\n     def __init__(\n         self,\n         modules_to_not_convert: Optional[list] = None,\n-        linear_class: Optional[str] = \"bitlinear\",\n-        quantization_mode: Optional[str] = \"offline\",\n-        use_rms_norm: Optional[bool] = False,\n+        linear_class: str = \"bitlinear\",\n+        quantization_mode: str = \"offline\",\n+        use_rms_norm: bool = False,\n         rms_norm_eps: Optional[float] = 1e-6,\n         **kwargs,\n     ):"
        }
    ],
    "stats": {
        "total": 88,
        "additions": 45,
        "deletions": 43
    }
}