{
    "author": "Abhinavexists",
    "message": "Fix Fuyu processor width dimension bug in `_get_num_multimodal_tokens` (#43137)\n\nfix: image padding in image_unpadded_w",
    "sha": "474bbf59b3f4f77091f39625077aa25ac4bd4072",
    "files": [
        {
            "sha": "0fa107ac69330b425019362e46cba2e336894c75",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/474bbf59b3f4f77091f39625077aa25ac4bd4072/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/474bbf59b3f4f77091f39625077aa25ac4bd4072/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=474bbf59b3f4f77091f39625077aa25ac4bd4072",
            "patch": "@@ -625,7 +625,7 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n                 optimal_scale_factor = min(height_scale_factor, width_scale_factor)\n \n                 image_unpadded_h = min(int(image_size[0] * optimal_scale_factor), image_size[0])\n-                image_unpadded_w = min(int(image_size[0] * optimal_scale_factor), image_size[0])\n+                image_unpadded_w = min(int(image_size[1] * optimal_scale_factor), image_size[1])\n \n                 # We can use torch here because Fuyu processor has hard dependency on torch. NOTE: Fuyu can't do multi-image\n                 # thus the below (1, 1, 1) is hardcoded. Same as when calling the processor"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}