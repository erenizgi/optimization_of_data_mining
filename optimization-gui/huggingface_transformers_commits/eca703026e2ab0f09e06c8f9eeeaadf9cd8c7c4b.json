{
    "author": "Shoumik-Gandre",
    "message": "chore: standardize DeBERTa model card (#37409)\n\n* chore: standardize DeBERTa model card\n\n* Apply suggestions from code review in docs\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* fix: Update deberta.md with code cleanup suggestions\n\n* Update docs/source/en/model_doc/deberta.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/deberta.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update deberta.md\n\n* Update deberta.md\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "eca703026e2ab0f09e06c8f9eeeaadf9cd8c7c4b",
    "files": [
        {
            "sha": "5991a0c085b72633bf941c123babaef78ae7adb4",
            "filename": "docs/source/en/model_doc/deberta.md",
            "status": "modified",
            "additions": 64,
            "deletions": 47,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca703026e2ab0f09e06c8f9eeeaadf9cd8c7c4b/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca703026e2ab0f09e06c8f9eeeaadf9cd8c7c4b/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md?ref=eca703026e2ab0f09e06c8f9eeeaadf9cd8c7c4b",
            "patch": "@@ -14,72 +14,89 @@ rendered properly in your Markdown viewer.\n \n -->\n \n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+</div>\n+    </div>\n+</div>\n+\n # DeBERTa\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n-</div>\n+[DeBERTa](https://huggingface.co/papers/2006.03654) improves the pretraining efficiency of BERT and RoBERTa with two key ideas, disentangled attention and an enhanced mask decoder. Instead of mixing everything together like BERT, DeBERTa separates a word's *content* from its *position* and processes them independently. This gives it a clearer sense of what's being said and where in the sentence it's happening.\n+\n+The enhanced mask decoder replaces the traditional softmax decoder to make better predictions.\n+\n+Even with less training data than RoBERTa, DeBERTa manages to outperform it on several benchmarks.\n+\n+You can find all the original DeBERTa checkpoints under the [Microsoft](https://huggingface.co/microsoft?search_models=deberta) organization.\n+\n \n-## Overview\n+> [!TIP]\n+> Click on the DeBERTa models in the right sidebar for more examples of how to apply DeBERTa to different language tasks.\n \n-The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://huggingface.co/papers/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's\n-BERT model released in 2018 and Facebook's RoBERTa model released in 2019.\n+The example below demonstrates how to classify text with [`Pipeline`], [`AutoModel`], and from the command line.\n \n-It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in\n-RoBERTa.\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n-The abstract from the paper is the following:\n+```py\n+import torch\n+from transformers import pipeline\n \n-*Recent progress in pre-trained neural language models has significantly improved the performance of many natural\n-language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with\n-disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the\n-disentangled attention mechanism, where each word is represented using two vectors that encode its content and\n-position, respectively, and the attention weights among words are computed using disentangled matrices on their\n-contents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to\n-predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency\n-of model pretraining and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of\n-the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9%\n-(90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and\n-pre-trained models will be made publicly available at https://github.com/microsoft/DeBERTa.*\n+classifier = pipeline(\n+    task=\"text-classification\",\n+    model=\"microsoft/deberta-base-mnli\",\n+    device=0,\n+)\n \n+classifier({\n+    \"text\": \"A soccer game with multiple people playing.\",\n+    \"text_pair\": \"Some people are playing a sport.\"\n+})\n+```\n \n-This model was contributed by [DeBERTa](https://huggingface.co/DeBERTa). This model TF 2.0 implementation was\n-contributed by [kamalkraj](https://huggingface.co/kamalkraj) . The original code can be found [here](https://github.com/microsoft/DeBERTa).\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n-## Resources\n+```py\n+import torch\n+from transformers import AutoModelForSequenceClassification, AutoTokenizer\n \n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with DeBERTa. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n+model_name = \"microsoft/deberta-base-mnli\"\n+tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base-mnli\")\n+model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-base-mnli\", device_map=\"auto\")\n \n-<PipelineTag pipeline=\"text-classification\"/>\n+inputs = tokenizer(\n+    \"A soccer game with multiple people playing.\",\n+    \"Some people are playing a sport.\",\n+    return_tensors=\"pt\"\n+).to(\"cuda\")\n \n-- A blog post on how to [Accelerate Large Model Training using DeepSpeed](https://huggingface.co/blog/accelerate-deepspeed) with DeBERTa.\n-- A blog post on [Supercharged Customer Service with Machine Learning](https://huggingface.co/blog/supercharge-customer-service-with-machine-learning) with DeBERTa.\n-- [`DebertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb).\n-- [`TFDebertaForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).\n-- [Text classification task guide](../tasks/sequence_classification)\n+with torch.no_grad():\n+    logits = model(**inputs).logits\n+    predicted_class = logits.argmax().item()\n \n-<PipelineTag pipeline=\"token-classification\" />\n+labels = [\"contradiction\", \"neutral\", \"entailment\"]\n+print(f\"The predicted relation is: {labels[predicted_class]}\")\n \n-- [`DebertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb).\n-- [`TFDebertaForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).\n-- [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the ðŸ¤— Hugging Face Course.\n-- [Byte-Pair Encoding tokenization](https://huggingface.co/course/chapter6/5?fw=pt) chapter of the ðŸ¤— Hugging Face Course.\n-- [Token classification task guide](../tasks/token_classification)\n+```\n \n-<PipelineTag pipeline=\"fill-mask\"/>\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n \n-- [`DebertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\n-- [`TFDebertaForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n-- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the ðŸ¤— Hugging Face Course.\n-- [Masked language modeling task guide](../tasks/masked_language_modeling)\n+```bash\n+echo -e '{\"text\": \"A soccer game with multiple people playing.\", \"text_pair\": \"Some people are playing a sport.\"}' | transformers run --task text-classification --model microsoft/deberta-base-mnli --device 0\n+```\n \n-<PipelineTag pipeline=\"question-answering\"/>\n+</hfoption>\n+</hfoptions>\n \n-- [`DebertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).\n-- [`TFDebertaForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n-- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the ðŸ¤— Hugging Face Course.\n-- [Question answering task guide](../tasks/question_answering)\n+## Notes\n+- DeBERTa uses **relative position embeddings**, so it does not require **right-padding** like BERT.\n+- For best results, use DeBERTa on sentence-level or sentence-pair classification tasks like MNLI, RTE, or SST-2.\n+- If you're using DeBERTa for token-level tasks like masked language modeling, make sure to load a checkpoint specifically pretrained or fine-tuned for token-level tasks.\n \n ## DebertaConfig\n "
        }
    ],
    "stats": {
        "total": 111,
        "additions": 64,
        "deletions": 47
    }
}