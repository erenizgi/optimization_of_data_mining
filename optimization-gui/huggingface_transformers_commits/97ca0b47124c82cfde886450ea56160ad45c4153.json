{
    "author": "remi-or",
    "message": "Fix flash-attn for paged_attention when no kernels (#41078)\n\n* Fix non-kernels flash attention paged implementation\n\n* Cover all cases\n\n* Style\n\n* Update src/transformers/integrations/flash_paged.py\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "97ca0b47124c82cfde886450ea56160ad45c4153",
    "files": [
        {
            "sha": "cf5379fc619cadd12a205e24d5592f309604dc8f",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/97ca0b47124c82cfde886450ea56160ad45c4153/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97ca0b47124c82cfde886450ea56160ad45c4153/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=97ca0b47124c82cfde886450ea56160ad45c4153",
            "patch": "@@ -40,7 +40,8 @@ def generate_simple(\n     attn_impl = {\n         \"sdpa_paged\": \"sdpa\",\n         \"eager_paged\": \"eager\",\n-        \"flash_paged\": \"flash_attention_2\",\n+        \"paged_attention\": \"eager\",  # TODO: this does not work on AMD docker\n+        \"flash_paged\": \"flash_attention_2\",  # TODO: this does not work on AMD docker\n     }[attn_impl]\n \n     model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype=torch.bfloat16, attn_implementation=attn_impl)"
        },
        {
            "sha": "1d1db72a760525f2e1cf1b37d29d47a1bd5762e3",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/97ca0b47124c82cfde886450ea56160ad45c4153/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97ca0b47124c82cfde886450ea56160ad45c4153/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=97ca0b47124c82cfde886450ea56160ad45c4153",
            "patch": "@@ -6,11 +6,21 @@\n from ..utils import is_flash_attn_2_available\n \n \n+# For some reason, if we dont assign the function to a variable here, it will be garbage collected\n try:\n     if is_flash_attn_2_available():\n         from flash_attn import flash_attn_varlen_func  # noqa: F401\n-except Exception:\n-    pass\n+\n+        FLASH_ATTN_VARLEN_FUNC = flash_attn_varlen_func\n+    else:\n+        raise RuntimeError(\n+            \"Flash Attention 2 is not installed. Please refer to https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install it\"\n+        )\n+except Exception as e:\n+    msg = repr(e)\n+\n+    def FLASH_ATTN_VARLEN_FUNC(*args, **kwargs):\n+        raise Exception(f\"flash_attn_varlen_func is not available: {msg}\")\n \n \n def paged_attention_forward(\n@@ -63,6 +73,8 @@ def paged_attention_forward(\n \n     if implementation is not None and hasattr(implementation, \"flash_attn_varlen_func\"):\n         flash_attn_varlen_func = implementation.flash_attn_varlen_func\n+    else:\n+        flash_attn_varlen_func = FLASH_ATTN_VARLEN_FUNC\n \n     custom_kwargs = {\"s_aux\": kwargs.get(\"s_aux\")} if \"s_aux\" in kwargs else {}\n "
        }
    ],
    "stats": {
        "total": 19,
        "additions": 16,
        "deletions": 3
    }
}