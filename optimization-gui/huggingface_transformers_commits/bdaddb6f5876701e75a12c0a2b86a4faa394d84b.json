{
    "author": "Wauplin",
    "message": "Document new default shard size + dropped unsafe serialization (#42904)\n\n* Document new default shard size + dropped unsafe serialization\n\n* a\n\n* Update MIGRATION_GUIDE_V5.md\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "bdaddb6f5876701e75a12c0a2b86a4faa394d84b",
    "files": [
        {
            "sha": "1666bb274a10d9c068a8ebcfdc6805760772cc03",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/bdaddb6f5876701e75a12c0a2b86a4faa394d84b/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bdaddb6f5876701e75a12c0a2b86a4faa394d84b/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=bdaddb6f5876701e75a12c0a2b86a4faa394d84b",
            "patch": "@@ -417,6 +417,20 @@ There is a tracker for that here: https://github.com/huggingface/transformers/is\n \n ## Library-wide changes with lesser impact\n \n+### Drop support for `safe_serialization=False`\n+\n+Safetensors is a simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy). It is the preferred file format to store transformers's weights. Prior to transformers `v5`, it was still possible to pass `safe_serialization=False` to fall back to torch's default (and unsafe) file format. This is no longer possible in `v5`. The `safe_serialization` parameter has been removed from all `save_pretrained` and `push_to_hub` methods.\n+\n+If you really want to export weights to another file format, you must save the `model.state_dict()` by yourself.\n+\n+Linked PR: https://github.com/huggingface/transformers/issues/42556\n+\n+### 50GB default shard size\n+\n+The default shard size went up from `5GB` to `50GB`. Main benefit will be to avoid having tens or hundreds of weight files for large models. This change was made possible thanks to the Xet backend allowing us to efficiently serve very large files. Increasing default shard size was a decision that was only taken after *very careful considerations* around optimizations and load speed. Check out the linked PR for benchmark details.\n+\n+Linked PR: https://github.com/huggingface/transformers/issues/42556\n+\n ### `use_auth_token`\n \n The `use_auth_token` argument/parameter is deprecated in favor of `token` everywhere."
        }
    ],
    "stats": {
        "total": 14,
        "additions": 14,
        "deletions": 0
    }
}