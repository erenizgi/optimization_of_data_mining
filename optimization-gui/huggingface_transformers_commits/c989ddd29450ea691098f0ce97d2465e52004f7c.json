{
    "author": "qgallouedec",
    "message": "Simplify and update trl examples (#38772)\n\n* Simplify and update trl examples\n\n* Remove optim_args from SFTConfig in Trainer documentation\n\n* Update docs/source/en/trainer.md\n\n* Apply suggestions from code review\n\n* Update docs/source/en/trainer.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Quentin Gallouédec <qgallouedec@Quentins-MacBook-Pro.local>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "c989ddd29450ea691098f0ce97d2465e52004f7c",
    "files": [
        {
            "sha": "1784d76a4ecb58ee382c3da75c6bcb9dba3a87c6",
            "filename": "docs/source/ar/trainer.md",
            "status": "modified",
            "additions": 42,
            "deletions": 123,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Far%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Far%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftrainer.md?ref=c989ddd29450ea691098f0ce97d2465e52004f7c",
            "patch": "@@ -306,75 +306,45 @@ pip install galore-torch\n ثم أضف ببساطة أحد `[\"galore_adamw\"، \"galore_adafactor\"، \"galore_adamw_8bit\"]` في `optim` جنبًا إلى جنب مع `optim_target_modules`، والتي يمكن أن تكون قائمة من السلاسل أو التعبيرات النمطية regex أو المسار الكامل المطابق لأسماء الوحدات المستهدفة التي تريد تكييفها. فيما يلي مثال على النص البرمجي كامل(تأكد من `pip install trl datasets`):\n \n ```python\n-import torch\n import datasets\n-import trl\n-\n-from transformers import TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n+from trl import SFTConfig, SFTTrainer\n \n train_dataset = datasets.load_dataset('imdb', split='train')\n-\n-args = TrainingArguments(\n-    output_dir=\"./test-galore\"،\n+args = SFTConfig(\n+    output_dir=\"./test-galore\",\n     max_steps=100,\n-    per_device_train_batch_size=2,\n-    optim=\"galore_adamw\"،\n-    optim_target_modules=[r\".*.attn.*\"، r\".*.mlp.*\"]\n+    optim=\"galore_adamw\",\n+    optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\n+    gradient_checkpointing=True,\n )\n-\n-model_id = \"google/gemma-2b\"\n-\n-config = AutoConfig.from_pretrained(model_id)\n-\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_config(config).to(0)\n-\n-trainer = trl.SFTTrainer(\n-    model=model, \n+trainer = SFTTrainer(\n+    model=\"google/gemma-2b\",\n     args=args,\n     train_dataset=train_dataset,\n-    dataset_text_field='text',\n-    max_seq_length=512,\n )\n-\n trainer.train()\n ```\n \n لتمرير معامﻻت إضافية يدعمها  GaLore، يجب عليك تمرير `optim_args` بشكل صحيح، على سبيل المثال:\n \n ```python\n-import torch\n import datasets\n-import trl\n-\n-from transformers import TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n+from trl import SFTConfig, SFTTrainer\n \n train_dataset = datasets.load_dataset('imdb', split='train')\n-\n-args = TrainingArguments(\n+args = SFTConfig(\n     output_dir=\"./test-galore\",\n     max_steps=100,\n-    per_device_train_batch_size=2,\n     optim=\"galore_adamw\",\n     optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\n     optim_args=\"rank=64, update_proj_gap=100, scale=0.10\",\n+    gradient_checkpointing=True,\n )\n-\n-model_id = \"google/gemma-2b\"\n-\n-config = AutoConfig.from_pretrained(model_id)\n-\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_config(config).to(0)\n-\n-trainer = trl.SFTTrainer(\n-    model=model, \n+trainer = SFTTrainer(\n+    model=\"google/gemma-2b\",\n     args=args,\n     train_dataset=train_dataset,\n-    dataset_text_field='text',\n-    max_seq_length=512,\n )\n-\n trainer.train()\n ```\n يمكنك قراءة المزيد حول الطريقة في [المستودع الأصلي](https://github.com/jiaweizzhao/GaLore) أو [الورقة البحثية](https://huggingface.co/papers/2403.03507).\n@@ -386,37 +356,22 @@ trainer.train()\n يمكنك أيضًا إجراء تحسين طبقة تلو الأخرى عن طريق إضافة `layerwise` إلى اسم المُحسِّن كما هو موضح أدناه:\n \n ```python\n-import torch\n import datasets\n-import trl\n-\n-from transformers import TrainingArguments، AutoConfig، AutoTokenizer، AutoModelForCausalLM\n+from trl import SFTConfig, SFTTrainer\n \n-train_dataset = datasets.load_dataset('imdb'، split='train')\n-\n-args = TrainingArguments(\n-    output_dir=\"./test-galore\"،\n-    max_steps=100،\n-    per_device_train_batch_size=2،\n-    optim=\"galore_adamw_layerwise\"،\n-    optim_target_modules=[r\".*.attn.*\"، r\".*.mlp.*\"]\n+train_dataset = datasets.load_dataset('imdb', split='train')\n+args = SFTConfig(\n+    output_dir=\"./test-galore\",\n+    max_steps=100,\n+    optim=\"galore_adamw_layerwise\",\n+    optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\n+    gradient_checkpointing=True,\n )\n-\n-model_id = \"google/gemma-2b\"\n-\n-config = AutoConfig.from_pretrained(model_id)\n-\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_config(config).to(0)\n-\n-trainer = trl.SFTTrainer(\n-    model=model،\n-    args=args،\n-    train_dataset=train_dataset،\n-    dataset_text_field='text'،\n-    max_seq_length=512،\n+trainer = SFTTrainer(\n+    model=\"google/gemma-2b\",\n+    args=args,\n+    train_dataset=train_dataset,\n )\n-\n trainer.train()\n ```\n \n@@ -436,39 +391,21 @@ trainer.train()\n فيما يلي نص برمجي بسيط يوضح كيفية ضبط نموذج [google/gemma-2b](https://huggingface.co/google/gemma-2b) على مجموعة بيانات IMDB في الدقة الكاملة:\n \n ```python\n-import torch\n import datasets\n-from transformers import TrainingArguments، AutoTokenizer، AutoModelForCausalLM\n-import trl\n-\n-train_dataset = datasets.load_dataset('imdb'، split='train')\n+from trl import SFTConfig, SFTTrainer\n \n-args = TrainingArguments(\n-    output_dir=\"./test-lomo\"،\n-    max_steps=100،\n-    per_device_train_batch_size=4،\n-    optim=\"adalomo\"،\n-    gradient_checkpointing=True،\n-    logging_strategy=\"steps\"،\n-    logging_steps=1،\n-    learning_rate=2e-6،\n-    save_strategy=\"no\"،\n-    run_name=\"lomo-imdb\"،\n+train_dataset = datasets.load_dataset('imdb', split='train')\n+args = SFTConfig(\n+    output_dir=\"./test-lomo\",\n+    max_steps=100,\n+    optim=\"adalomo\",\n+    gradient_checkpointing=True,\n )\n-\n-model_id = \"google/gemma-2b\"\n-\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id).to(0)\n-\n-trainer = trl.SFTTrainer(\n-    model=model،\n-    args=args،\n-    train_dataset=train_dataset،\n-    dataset_text_field='text'،\n-    max_seq_length=1024،\n+trainer = SFTTrainer(\n+    model=\"google/gemma-2b\",\n+    args=args,\n+    train_dataset=train_dataset,\n )\n-\n trainer.train()\n ```\n \n@@ -524,39 +461,21 @@ trainer.train()\n \n فيما يلي نص برمجى بسيط لشرح كيفية ضبط [google/gemma-2b](https://huggingface.co/google/gemma-2b) بدقة على مجموعة بيانات IMDB بدقة كاملة:\n ```python\n-import torch\n import datasets\n-from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n-import trl\n+from trl import SFTConfig, SFTTrainer\n \n train_dataset = datasets.load_dataset('imdb', split='train')\n-\n-args = TrainingArguments(\n-    output_dir=\"./test-schedulefree\",\n-    max_steps=1000,\n-    per_device_train_batch_size=4,\n+args = SFTConfig(\n+    output_dir=\"./test-galore\",\n+    max_steps=100,\n     optim=\"schedule_free_adamw\",\n     gradient_checkpointing=True,\n-    logging_strategy=\"steps\",\n-    logging_steps=1,\n-    learning_rate=2e-6,\n-    save_strategy=\"no\",\n-    run_name=\"sfo-imdb\",\n )\n-\n-model_id = \"google/gemma-2b\"\n-\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id).to(0)\n-\n-trainer = trl.SFTTrainer(\n-    model=model, \n+trainer = SFTTrainer(\n+    model=\"google/gemma-2b\",\n     args=args,\n     train_dataset=train_dataset,\n-    dataset_text_field='text',\n-    max_seq_length=1024,\n )\n-\n trainer.train()\n ```\n ## تسريع ومدرب"
        },
        {
            "sha": "9ce98d8516a802df646c8ab0994262a14792bc1f",
            "filename": "docs/source/en/model_doc/mamba.md",
            "status": "modified",
            "additions": 15,
            "deletions": 32,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md?ref=c989ddd29450ea691098f0ce97d2465e52004f7c",
            "patch": "@@ -97,39 +97,22 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n - Mamba stacks `mixer` layers which are equivalent to `Attention` layers. You can find the main logic of Mamba in the `MambaMixer` class.\n - The example below demonstrates how to fine-tune Mamba with [PEFT](https://huggingface.co/docs/peft).\n \n-   ```py\n-   from datasets import load_dataset\n-   from trl import SFTTrainer\n-   from peft import LoraConfig\n-   from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n-   \n-   model_id = \"state-spaces/mamba-130m-hf\"\n-   tokenizer = AutoTokenizer.from_pretrained(model_id)\n-   model = AutoModelForCausalLM.from_pretrained(model_id)\n-   dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n-   training_args = TrainingArguments(\n-       output_dir=\"./results\",\n-       num_train_epochs=3,\n-       per_device_train_batch_size=4,\n-       logging_dir='./logs',\n-       logging_steps=10,\n-       learning_rate=2e-3\n-   )\n-   lora_config =  LoraConfig(\n-           r=8,\n-           target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"],\n-           task_type=\"CAUSAL_LM\",\n-           bias=\"none\"\n-   )\n-   trainer = SFTTrainer(\n-       model=model,\n-       processing_class=tokenizer,\n+  ```py\n+  from datasets import load_dataset\n+  from trl import SFTConfig, SFTTrainer\n+  from peft import LoraConfig\n+\n+  model_id = \"state-spaces/mamba-130m-hf\"\n+  dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n+  training_args = SFTConfig(dataset_text_field=\"quote\")\n+  lora_config =  LoraConfig(target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"])\n+  trainer = SFTTrainer(\n+      model=model_id,\n       args=training_args,\n-       peft_config=lora_config,\n-       train_dataset=dataset,\n-       dataset_text_field=\"quote\",\n-   )\n-   trainer.train()\n+      train_dataset=dataset,\n+      peft_config=lora_config,\n+  )\n+  trainer.train()\n    ```\n \n ## MambaConfig"
        },
        {
            "sha": "a2094070226182ef78bb315c9384503fc8a7d009",
            "filename": "docs/source/en/model_doc/mamba2.md",
            "status": "modified",
            "additions": 7,
            "deletions": 28,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba2.md?ref=c989ddd29450ea691098f0ce97d2465e52004f7c",
            "patch": "@@ -103,40 +103,19 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n - The example below demonstrates how to fine-tune Mamba 2 with [PEFT](https://huggingface.co/docs/peft).\n \n ```python \n-from trl import SFTTrainer\n+from datasets import load_dataset\n from peft import LoraConfig\n-from transformers import AutoTokenizer, Mamba2ForCausalLM, TrainingArguments\n-model_id = 'mistralai/Mamba-Codestral-7B-v0.1'\n-tokenizer = AutoTokenizer.from_pretrained(model_id, revision='refs/pr/9', from_slow=True, legacy=False)\n-tokenizer.pad_token = tokenizer.eos_token\n-tokenizer.padding_side = \"left\" #enforce padding side left\n+from trl import SFTConfig, SFTTrainer\n \n-model = Mamba2ForCausalLM.from_pretrained(model_id, revision='refs/pr/9')\n+model_id = \"mistralai/Mamba-Codestral-7B-v0.1\"\n dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n-# Without CUDA kernels, batch size of 2 occupies one 80GB device\n-# but precision can be reduced.\n-# Experiments and trials welcome!\n-training_args = TrainingArguments(\n-    output_dir=\"./results\",\n-    num_train_epochs=3,\n-    per_device_train_batch_size=2,\n-    logging_dir='./logs',\n-    logging_steps=10,\n-    learning_rate=2e-3\n-)\n-lora_config =  LoraConfig(\n-        r=8,\n-        target_modules=[\"embeddings\", \"in_proj\", \"out_proj\"],\n-        task_type=\"CAUSAL_LM\",\n-        bias=\"none\"\n-)\n+training_args = SFTConfig(dataset_text_field=\"quote\", gradient_checkpointing=True, per_device_train_batch_size=4)\n+lora_config =  LoraConfig(target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"])\n trainer = SFTTrainer(\n-    model=model,\n-    tokenizer=tokenizer,\n+    model=model_id,\n     args=training_args,\n-    peft_config=lora_config,\n     train_dataset=dataset,\n-    dataset_text_field=\"quote\",\n+    peft_config=lora_config,\n )\n trainer.train()\n ```"
        },
        {
            "sha": "56f929884a5c8b8ce172755bb2fa66f82d98bc97",
            "filename": "docs/source/en/trainer.md",
            "status": "modified",
            "additions": 14,
            "deletions": 28,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Fen%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Fen%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftrainer.md?ref=c989ddd29450ea691098f0ce97d2465e52004f7c",
            "patch": "@@ -392,15 +392,15 @@ training_args = TrainingArguments(\n \n [Gradient Low-Rank Projection (GaLore)](https://hf.co/papers/2403.03507) significantly reduces memory usage when training large language models (LLMs). One of GaLores key benefits is *full-parameter* learning, unlike low-rank adaptation methods like [LoRA](https://hf.co/papers/2106.09685), which produces better model performance.\n \n-Install the [GaLore](https://github.com/jiaweizzhao/GaLore) library, [TRL](https://hf.co/docs/trl/index), and [Datasets](https://hf.co/docs/datasets/index).\n+Install the [GaLore](https://github.com/jiaweizzhao/GaLore) and [TRL](https://hf.co/docs/trl/index) libraries.\n \n ```bash\n-pip install galore-torch trl datasets\n+pip install galore-torch trl\n ```\n \n-Pick a GaLore optimizer (`\"galore_adamw\"`, `\"galore_adafactor\"`, `\"galore_adamw_8bit`\") and pass it to the `optim` parameter in [`TrainingArguments`]. Use the `optim_target_modules` parameter to specify which modules to adapt (can be a list of strings, regex, or a full path).\n+Pick a GaLore optimizer (`\"galore_adamw\"`, `\"galore_adafactor\"`, `\"galore_adamw_8bit`\") and pass it to the `optim` parameter in [`trl.SFTConfig`]. Use the `optim_target_modules` parameter to specify which modules to adapt (can be a list of strings, regex, or a full path).\n \n-Extra parameters supported by GaLore, `rank`, `update_proj_gap`, and `scale`, should be passed to the `optim_args` parameter in [`TrainingArguments`].\n+Extra parameters supported by GaLore, `rank`, `update_proj_gap`, and `scale`, should be passed to the `optim_args` parameter in [`trl.SFTConfig`].\n \n The example below enables GaLore with [`~trl.SFTTrainer`] that targets the `attn` and `mlp` layers with regex.\n \n@@ -411,29 +411,22 @@ The example below enables GaLore with [`~trl.SFTTrainer`] that targets the `attn\n <hfoption id=\"GaLore optimizer\">\n \n ```py\n-import torch\n import datasets\n-import trl\n-from transformers import TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n+from trl import SFTConfig, SFTTrainer\n \n train_dataset = datasets.load_dataset('imdb', split='train')\n-args = TrainingArguments(\n+args = SFTConfig(\n     output_dir=\"./test-galore\",\n     max_steps=100,\n-    per_device_train_batch_size=2,\n     optim=\"galore_adamw\",\n     optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\n     optim_args=\"rank=64, update_proj_gap=100, scale=0.10\",\n+    gradient_checkpointing=True,\n )\n-config = AutoConfig.from_pretrained(\"google/gemma-2b\")\n-tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n-model = AutoModelForCausalLM.from_config(\"google/gemma-2b\").to(0)\n-trainer = trl.SFTTrainer(\n-    model=model,\n+trainer = SFTTrainer(\n+    model=\"google/gemma-2b\",\n     args=args,\n     train_dataset=train_dataset,\n-    dataset_text_field='text',\n-    max_seq_length=512,\n )\n trainer.train()\n ```\n@@ -444,29 +437,22 @@ trainer.train()\n Append `layerwise` to the optimizer name to enable layerwise optimization. For example, `\"galore_adamw\"` becomes `\"galore_adamw_layerwise\"`. This feature is still experimental and does not support Distributed Data Parallel (DDP). The code below can only be run on a [single GPU](https://github.com/jiaweizzhao/GaLore?tab=readme-ov-file#train-7b-model-with-a-single-gpu-with-24gb-memory). Other features like gradient clipping and DeepSpeed may not be available out of the box. Feel free to open an [issue](https://github.com/huggingface/transformers/issues) if you encounter any problems!\n \n ```py\n-import torch\n import datasets\n-import trl\n-from transformers import TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n+from trl import SFTConfig, SFTTrainer\n \n train_dataset = datasets.load_dataset('imdb', split='train')\n-args = TrainingArguments(\n+args = SFTConfig(\n     output_dir=\"./test-galore\",\n     max_steps=100,\n-    per_device_train_batch_size=2,\n     optim=\"galore_adamw_layerwise\",\n     optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\n     optim_args=\"rank=64, update_proj_gap=100, scale=0.10\",\n+    gradient_checkpointing=True,\n )\n-config = AutoConfig.from_pretrained(\"google/gemma-2b\")\n-tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n-model = AutoModelForCausalLM.from_config(\"google/gemma-2b\").to(0)\n-trainer = trl.SFTTrainer(\n-    model=model,\n+trainer = SFTTrainer(\n+    model=\"google/gemma-2b\",\n     args=args,\n     train_dataset=train_dataset,\n-    dataset_text_field='text',\n-    max_seq_length=512,\n )\n trainer.train()\n ```"
        },
        {
            "sha": "001ea609932219fb59f944b5f21f6b53e090c2bb",
            "filename": "docs/source/ko/model_doc/mamba.md",
            "status": "modified",
            "additions": 6,
            "deletions": 22,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Fko%2Fmodel_doc%2Fmamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Fko%2Fmodel_doc%2Fmamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fmamba.md?ref=c989ddd29450ea691098f0ce97d2465e52004f7c",
            "patch": "@@ -58,34 +58,18 @@ print(tokenizer.batch_decode(out))\n \n ```python \n from datasets import load_dataset\n-from trl import SFTTrainer\n+from trl import SFTConfig, SFTTrainer\n from peft import LoraConfig\n-from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n+\n model_id = \"state-spaces/mamba-130m-hf\"\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id)\n dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n-training_args = TrainingArguments(\n-    output_dir=\"./results\",\n-    num_train_epochs=3,\n-    per_device_train_batch_size=4,\n-    logging_dir='./logs',\n-    logging_steps=10,\n-    learning_rate=2e-3\n-)\n-lora_config =  LoraConfig(\n-        r=8,\n-        target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"],\n-        task_type=\"CAUSAL_LM\",\n-        bias=\"none\"\n-)\n+training_args = SFTConfig(dataset_text_field=\"quote\")\n+lora_config =  LoraConfig(target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"])\n trainer = SFTTrainer(\n-    model=model,\n-    tokenizer=tokenizer,\n+    model=model_id,\n     args=training_args,\n-    peft_config=lora_config,\n     train_dataset=dataset,\n-    dataset_text_field=\"quote\",\n+    peft_config=lora_config,\n )\n trainer.train()\n ```"
        },
        {
            "sha": "04ef4d070b87b99019f460ee36144e81cd4eab89",
            "filename": "docs/source/ko/model_doc/mamba2.md",
            "status": "modified",
            "additions": 7,
            "deletions": 28,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Fko%2Fmodel_doc%2Fmamba2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Fko%2Fmodel_doc%2Fmamba2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fmamba2.md?ref=c989ddd29450ea691098f0ce97d2465e52004f7c",
            "patch": "@@ -57,40 +57,19 @@ print(tokenizer.batch_decode(out))\n \n 이곳은 미세조정을 위한 초안 스크립트입니다: \n ```python \n-from trl import SFTTrainer\n+from datasets import load_dataset\n from peft import LoraConfig\n-from transformers import AutoTokenizer, Mamba2ForCausalLM, TrainingArguments\n-model_id = 'mistralai/Mamba-Codestral-7B-v0.1'\n-tokenizer = AutoTokenizer.from_pretrained(model_id, revision='refs/pr/9', from_slow=True, legacy=False)\n-tokenizer.pad_token = tokenizer.eos_token\n-tokenizer.padding_side = \"left\" #왼쪽 패딩으로 설정\n+from trl import SFTConfig, SFTTrainer\n \n-model = Mamba2ForCausalLM.from_pretrained(model_id, revision='refs/pr/9')\n+model_id = \"mistralai/Mamba-Codestral-7B-v0.1\"\n dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n-# CUDA 커널없이는, 배치크기 2가 80GB 장치를 하나 차지합니다.\n-# 하지만 정확도는 감소합니다.\n-# 실험과 시도를 환영합니다!\n-training_args = TrainingArguments(\n-    output_dir=\"./results\",\n-    num_train_epochs=3,\n-    per_device_train_batch_size=2,\n-    logging_dir='./logs',\n-    logging_steps=10,\n-    learning_rate=2e-3\n-)\n-lora_config =  LoraConfig(\n-        r=8,\n-        target_modules=[\"embeddings\", \"in_proj\", \"out_proj\"],\n-        task_type=\"CAUSAL_LM\",\n-        bias=\"none\"\n-)\n+training_args = SFTConfig(dataset_text_field=\"quote\", gradient_checkpointing=True, per_device_train_batch_size=4)\n+lora_config =  LoraConfig(target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"])\n trainer = SFTTrainer(\n-    model=model,\n-    tokenizer=tokenizer,\n+    model=model_id,\n     args=training_args,\n-    peft_config=lora_config,\n     train_dataset=dataset,\n-    dataset_text_field=\"quote\",\n+    peft_config=lora_config,\n )\n trainer.train()\n ```"
        },
        {
            "sha": "d753627c86fbb4cfc0a9adba21fc536a0f9e4f93",
            "filename": "docs/source/ko/trainer.md",
            "status": "modified",
            "additions": 23,
            "deletions": 86,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Fko%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c989ddd29450ea691098f0ce97d2465e52004f7c/docs%2Fsource%2Fko%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftrainer.md?ref=c989ddd29450ea691098f0ce97d2465e52004f7c",
            "patch": "@@ -267,75 +267,45 @@ pip install galore-torch\n 그런 다음 `optim`에 `[\"galore_adamw\", \"galore_adafactor\", \"galore_adamw_8bit\"]` 중 하나와 함께 `optim_target_modules`를 추가합니다. 이는 적용하려는 대상 모듈 이름에 해당하는 문자열, 정규 표현식 또는 전체 경로의 목록일 수 있습니다. 아래는 end-to-end 예제 스크립트입니다(필요한 경우 `pip install trl datasets`를 실행):\n \n ```python\n-import torch\n import datasets\n-import trl\n-\n-from transformers import TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n+from trl import SFTConfig, SFTTrainer\n \n train_dataset = datasets.load_dataset('imdb', split='train')\n-\n-args = TrainingArguments(\n+args = SFTConfig(\n     output_dir=\"./test-galore\",\n     max_steps=100,\n-    per_device_train_batch_size=2,\n     optim=\"galore_adamw\",\n-    optim_target_modules=[\"attn\", \"mlp\"]\n+    optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\n+    gradient_checkpointing=True,\n )\n-\n-model_id = \"google/gemma-2b\"\n-\n-config = AutoConfig.from_pretrained(model_id)\n-\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_config(config).to(0)\n-\n-trainer = trl.SFTTrainer(\n-    model=model, \n+trainer = SFTTrainer(\n+    model=\"google/gemma-2b\",\n     args=args,\n     train_dataset=train_dataset,\n-    dataset_text_field='text',\n-    max_seq_length=512,\n )\n-\n trainer.train()\n ```\n \n GaLore가 지원하는 추가 매개변수를 전달하려면 `optim_args`를 설정합니다. 예를 들어:\n \n ```python\n-import torch\n import datasets\n-import trl\n-\n-from transformers import TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n+from trl import SFTConfig, SFTTrainer\n \n train_dataset = datasets.load_dataset('imdb', split='train')\n-\n-args = TrainingArguments(\n+args = SFTConfig(\n     output_dir=\"./test-galore\",\n     max_steps=100,\n-    per_device_train_batch_size=2,\n     optim=\"galore_adamw\",\n-    optim_target_modules=[\"attn\", \"mlp\"],\n+    optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\n     optim_args=\"rank=64, update_proj_gap=100, scale=0.10\",\n+    gradient_checkpointing=True,\n )\n-\n-model_id = \"google/gemma-2b\"\n-\n-config = AutoConfig.from_pretrained(model_id)\n-\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_config(config).to(0)\n-\n-trainer = trl.SFTTrainer(\n-    model=model, \n+trainer = SFTTrainer(\n+    model=\"google/gemma-2b\",\n     args=args,\n     train_dataset=train_dataset,\n-    dataset_text_field='text',\n-    max_seq_length=512,\n )\n-\n trainer.train()\n ```\n \n@@ -348,37 +318,22 @@ trainer.train()\n 다음과 같이 옵티마이저 이름에 `layerwise`를 추가하여 레이어별 최적화를 수행할 수도 있습니다:\n \n ```python\n-import torch\n import datasets\n-import trl\n-\n-from transformers import TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n+from trl import SFTConfig, SFTTrainer\n \n train_dataset = datasets.load_dataset('imdb', split='train')\n-\n-args = TrainingArguments(\n+args = SFTConfig(\n     output_dir=\"./test-galore\",\n     max_steps=100,\n-    per_device_train_batch_size=2,\n     optim=\"galore_adamw_layerwise\",\n-    optim_target_modules=[\"attn\", \"mlp\"]\n+    optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\n+    gradient_checkpointing=True,\n )\n-\n-model_id = \"google/gemma-2b\"\n-\n-config = AutoConfig.from_pretrained(model_id)\n-\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_config(config).to(0)\n-\n-trainer = trl.SFTTrainer(\n-    model=model, \n+trainer = SFTTrainer(\n+    model=\"google/gemma-2b\",\n     args=args,\n     train_dataset=train_dataset,\n-    dataset_text_field='text',\n-    max_seq_length=512,\n )\n-\n trainer.train()\n ```\n \n@@ -398,39 +353,21 @@ LOMO 옵티마이저는 [제한된 자원으로 대형 언어 모델의 전체 \n 다음은 IMDB 데이터셋에서 [google/gemma-2b](https://huggingface.co/google/gemma-2b)를 최대 정밀도로 미세 조정하는 간단한 스크립트입니다:\n \n ```python\n-import torch\n import datasets\n-from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n-import trl\n+from trl import SFTConfig, SFTTrainer\n \n train_dataset = datasets.load_dataset('imdb', split='train')\n-\n-args = TrainingArguments(\n+args = SFTConfig(\n     output_dir=\"./test-lomo\",\n-    max_steps=1000,\n-    per_device_train_batch_size=4,\n+    max_steps=100,\n     optim=\"adalomo\",\n     gradient_checkpointing=True,\n-    logging_strategy=\"steps\",\n-    logging_steps=1,\n-    learning_rate=2e-6,\n-    save_strategy=\"no\",\n-    run_name=\"lomo-imdb\",\n )\n-\n-model_id = \"google/gemma-2b\"\n-\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id).to(0)\n-\n-trainer = trl.SFTTrainer(\n-    model=model, \n+trainer = SFTTrainer(\n+    model=\"google/gemma-2b\",\n     args=args,\n     train_dataset=train_dataset,\n-    dataset_text_field='text',\n-    max_seq_length=1024,\n )\n-\n trainer.train()\n ```\n "
        }
    ],
    "stats": {
        "total": 461,
        "additions": 114,
        "deletions": 347
    }
}