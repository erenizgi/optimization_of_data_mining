{
    "author": "cyyever",
    "message": "Fix typos in documentation (#41087)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "f4a6c65951b0d6c76418b008200df8b57f410cc5",
    "files": [
        {
            "sha": "b32fa8ec43f4d624c7729116c1d1e337ca6af0ee",
            "filename": "docs/source/en/chat_templating.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f4a6c65951b0d6c76418b008200df8b57f410cc5/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f4a6c65951b0d6c76418b008200df8b57f410cc5/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=f4a6c65951b0d6c76418b008200df8b57f410cc5",
            "patch": "@@ -189,7 +189,7 @@ model.generate(**formatted_chat)\n \n ## Model training\n \n-Training a model with a chat template is a good way to ensure the template matches the tokens the model was trained on. Apply the chat template as a preprocessing step to your dataset. Set `add_generation_prompt=False` because the additional tokens to prompt an assistant response aren’t helpful during training.\n+Training a model with a chat template is a good way to ensure the template matches the tokens the model was trained on. Apply the chat template as a preprocessing step to your dataset. Set `add_generation_prompt=False` because the additional tokens to prompt an assistant response aren't helpful during training.\n \n An example of preprocessing a dataset with a chat template is shown below.\n "
        },
        {
            "sha": "f28c09e96b678125b9af2c3f13f8c0a28e7d1842",
            "filename": "docs/source/en/chat_templating_multimodal.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f4a6c65951b0d6c76418b008200df8b57f410cc5/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f4a6c65951b0d6c76418b008200df8b57f410cc5/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md?ref=f4a6c65951b0d6c76418b008200df8b57f410cc5",
            "patch": "@@ -46,7 +46,7 @@ messages = [\n ]\n ```\n \n-Create an [`ImageTextToTextPipeline`] and pass the chat to it. For large models, setting [device_map=“auto”](./models#big-model-inference) helps load the model quicker and automatically places it on the fastest device available. Setting the data type to [auto](./models#model-data-type) also helps save memory and improve speed.\n+Create an [`ImageTextToTextPipeline`] and pass the chat to it. For large models, setting [device_map=\"auto\"](./models#big-model-inference) helps load the model quicker and automatically places it on the fastest device available. Setting the data type to [auto](./models#model-data-type) also helps save memory and improve speed.\n \n ```python\n import torch"
        },
        {
            "sha": "fa8472c253e2614da06e5f71a8fbb23a34a62af7",
            "filename": "docs/source/en/debugging.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f4a6c65951b0d6c76418b008200df8b57f410cc5/docs%2Fsource%2Fen%2Fdebugging.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f4a6c65951b0d6c76418b008200df8b57f410cc5/docs%2Fsource%2Fen%2Fdebugging.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdebugging.md?ref=f4a6c65951b0d6c76418b008200df8b57f410cc5",
            "patch": "@@ -35,7 +35,7 @@ pip install deepspeed\n \n PyTorch comes with its own CUDA toolkit, but to use DeepSpeed with PyTorch, you need to have an identical version of CUDA installed system-wide. For example, if you installed PyTorch with `cudatoolkit==10.2` in your Python environment, then you'll also need to have CUDA 10.2 installed everywhere.\n \n-The exact location can vary from system to system, but `usr/local/cuda-10.2` is the most common location on many Unix systems. When CUDA is correctly set up and added to your `PATH` environment variable, you can find the installation location with the following command.\n+The exact location can vary from system to system, but `/usr/local/cuda-10.2` is the most common location on many Unix systems. When CUDA is correctly set up and added to your `PATH` environment variable, you can find the installation location with the following command.\n \n ```bash\n which nvcc"
        },
        {
            "sha": "7f3caaef33010d59c19904655bd8729b8fc69e24",
            "filename": "docs/source/en/fast_tokenizers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f4a6c65951b0d6c76418b008200df8b57f410cc5/docs%2Fsource%2Fen%2Ffast_tokenizers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f4a6c65951b0d6c76418b008200df8b57f410cc5/docs%2Fsource%2Fen%2Ffast_tokenizers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ffast_tokenizers.md?ref=f4a6c65951b0d6c76418b008200df8b57f410cc5",
            "patch": "@@ -226,7 +226,7 @@ tokenizer = PreTrainedTokenizerFast.from_pretrained(\"config/save/dir\")\n \n <Youtube id=\"Yffk5aydLzg\"/>\n \n-A Transformers model expects the input to be a PyTorch or NumPy tensor. A tokenizers job is to preprocess text into those tensors. Specify the framework tensor type to return with the `return_tensors` parameter.\n+A Transformers model expects the input to be a PyTorch or NumPy tensor. A tokenizer's job is to preprocess text into those tensors. Specify the framework tensor type to return with the `return_tensors` parameter.\n \n ```py\n from transformers import AutoTokenizer"
        },
        {
            "sha": "2c6162ed1ca62d690653ce8d259808fcfe13a293",
            "filename": "docs/source/en/model_memory_anatomy.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f4a6c65951b0d6c76418b008200df8b57f410cc5/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f4a6c65951b0d6c76418b008200df8b57f410cc5/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md?ref=f4a6c65951b0d6c76418b008200df8b57f410cc5",
            "patch": "@@ -24,10 +24,10 @@ Let's start by exploring a motivating example of GPU utilization and the trainin\n we'll need to install a few libraries: \n \n ```bash\n-pip install transformers datasets accelerate nvidia-ml-py3\n+pip install transformers datasets accelerate nvidia-ml-py\n ```\n \n-The `nvidia-ml-py3` library allows us to monitor the memory usage of the models from within Python. You might be familiar \n+The `nvidia-ml-py` library allows us to monitor the memory usage of the models from within Python. You might be familiar \n with the `nvidia-smi` command in the terminal - this library allows to access the same information in Python directly.\n \n Then, we create some dummy data: random token IDs between 100 and 30000 and binary labels for a classifier. "
        },
        {
            "sha": "ed6c2b4a8d1ae8befbfab76297166d3b9a4d2edd",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f4a6c65951b0d6c76418b008200df8b57f410cc5/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f4a6c65951b0d6c76418b008200df8b57f410cc5/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=f4a6c65951b0d6c76418b008200df8b57f410cc5",
            "patch": "@@ -69,7 +69,7 @@ Learn in more detail the concepts underlying 8-bit quantization in the [Gentle I\n \n Set up a [`BitsAndBytesConfig`] and set `load_in_4bit=True` to load a model in 4-bit precision. The [`BitsAndBytesConfig`] is passed to the `quantization_config` parameter in [`~PreTrainedModel.from_pretrained`].\n \n-Allow Accelerate to automatically distribute the model across your available hardware by setting `device_map=“auto”`.\n+Allow Accelerate to automatically distribute the model across your available hardware by setting `device_map=\"auto\"`.\n \n Place all inputs on the same device as the model.\n "
        }
    ],
    "stats": {
        "total": 14,
        "additions": 7,
        "deletions": 7
    }
}