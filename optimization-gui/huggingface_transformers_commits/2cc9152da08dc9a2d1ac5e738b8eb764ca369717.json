{
    "author": "BenjaminBossan",
    "message": "FIX Broken PEFT adapter loading (#42187)\n\n* FIX Broken PEFT adapter loading\n\nFor some time now, loading PEFT adapters directly with transformers is\nbroken when using revisions or subfolders.\n\nTo check, run:\n\nRUN_SLOW=1 pytest tests/peft_integration/test_peft_integration.py -k\ntest_peft_from_pretrained_hub_kwargs\n\nThis PR makes the PEFT tests pass.\n\nThe PR causing this is #41445 (bad commit:\n1ee3b288a62c9de658e8be117d869c2a9b835a7c, previous good comit:\ncad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e). However, that PR also caused\nother errors (see #41604), which is why this error was not immediately\nobvious.\n\n* Fix for adapter_kwargs being None",
    "sha": "2cc9152da08dc9a2d1ac5e738b8eb764ca369717",
    "files": [
        {
            "sha": "c77b3ea7bb23d595c9e2a525acb36c9a19e30018",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/2cc9152da08dc9a2d1ac5e738b8eb764ca369717/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2cc9152da08dc9a2d1ac5e738b8eb764ca369717/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=2cc9152da08dc9a2d1ac5e738b8eb764ca369717",
            "patch": "@@ -654,17 +654,18 @@ def maybe_load_adapters(\n     token_from_adapter_kwargs = adapter_kwargs.pop(\"token\", None)\n \n     if _adapter_model_path is None:\n+        peft_kwargs = adapter_kwargs.copy()\n+        for arg_name in (\"cache_dir\", \"proxies\", \"subfolder\"):  # don't override revision\n+            if (arg_name not in peft_kwargs) and (arg_name in download_kwargs):\n+                peft_kwargs[arg_name] = download_kwargs[arg_name]\n+        if \"commit_hash\" in download_kwargs:\n+            peft_kwargs[\"_commit_hash\"] = download_kwargs[\"commit_hash\"]\n+        peft_kwargs[\"force_download\"] = bool(download_kwargs.get(\"force_download\", False))\n+        peft_kwargs[\"local_files_only\"] = bool(download_kwargs.get(\"local_files_only\", False))\n+        peft_kwargs[\"token\"] = token or token_from_adapter_kwargs\n         _adapter_model_path = find_adapter_config_file(\n             pretrained_model_name_or_path,\n-            cache_dir=download_kwargs.get(\"cache_dir\"),\n-            force_download=bool(download_kwargs.get(\"force_download\", False)),\n-            proxies=download_kwargs.get(\"proxies\"),\n-            token=token or token_from_adapter_kwargs,\n-            revision=download_kwargs.get(\"revision\"),\n-            local_files_only=bool(download_kwargs.get(\"local_files_only\", False)),\n-            subfolder=download_kwargs.get(\"subfolder\", \"\"),\n-            _commit_hash=download_kwargs.get(\"commit_hash\"),\n-            **adapter_kwargs,\n+            **peft_kwargs,\n         )\n \n     if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):"
        },
        {
            "sha": "80656de2fe901d1a074490a6e29f2b92d35cc36b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2cc9152da08dc9a2d1ac5e738b8eb764ca369717/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2cc9152da08dc9a2d1ac5e738b8eb764ca369717/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=2cc9152da08dc9a2d1ac5e738b8eb764ca369717",
            "patch": "@@ -3881,7 +3881,7 @@ def from_pretrained(\n         subfolder = kwargs.pop(\"subfolder\", \"\")\n         commit_hash = kwargs.pop(\"_commit_hash\", None)\n         variant = kwargs.pop(\"variant\", None)\n-        adapter_kwargs = kwargs.pop(\"adapter_kwargs\", {})\n+        adapter_kwargs = (kwargs.pop(\"adapter_kwargs\", {}) or {}).copy()\n         adapter_name = kwargs.pop(\"adapter_name\", \"default\")\n         generation_config = kwargs.pop(\"generation_config\", None)\n         gguf_file = kwargs.pop(\"gguf_file\", None)"
        },
        {
            "sha": "7a2dfc773873edd301e858397ef7dea540d20298",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2cc9152da08dc9a2d1ac5e738b8eb764ca369717/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2cc9152da08dc9a2d1ac5e738b8eb764ca369717/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=2cc9152da08dc9a2d1ac5e738b8eb764ca369717",
            "patch": "@@ -288,8 +288,9 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n         if is_peft_available():\n             if adapter_kwargs is None:\n                 adapter_kwargs = {}\n-                if token is not None:\n-                    adapter_kwargs[\"token\"] = token\n+            adapter_kwargs = adapter_kwargs.copy()  # avoid mutating original\n+            if token is not None:\n+                adapter_kwargs[\"token\"] = token\n \n             maybe_adapter_path = find_adapter_config_file(\n                 pretrained_model_name_or_path, _commit_hash=commit_hash, **adapter_kwargs"
        },
        {
            "sha": "dd33c7bb5b18407f699005c243202c4327659e51",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2cc9152da08dc9a2d1ac5e738b8eb764ca369717/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2cc9152da08dc9a2d1ac5e738b8eb764ca369717/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=2cc9152da08dc9a2d1ac5e738b8eb764ca369717",
            "patch": "@@ -699,20 +699,21 @@ def test_peft_from_pretrained_hub_kwargs(self):\n         with self.assertRaises(OSError):\n             _ = AutoModelForCausalLM.from_pretrained(peft_model_id)\n \n-        adapter_kwargs = {\"revision\": \"test\"}\n-\n         # This should work\n+        adapter_kwargs = {\"revision\": \"test\"}\n         model = AutoModelForCausalLM.from_pretrained(peft_model_id, adapter_kwargs=adapter_kwargs)\n         self.assertTrue(self._check_lora_correctly_converted(model))\n \n+        # note: always create new adapter_kwargs, avoid the test relying on the previous calls possibly mutating them\n+        adapter_kwargs = {\"revision\": \"test\"}\n         model = OPTForCausalLM.from_pretrained(peft_model_id, adapter_kwargs=adapter_kwargs)\n         self.assertTrue(self._check_lora_correctly_converted(model))\n \n         adapter_kwargs = {\"revision\": \"main\", \"subfolder\": \"test_subfolder\"}\n-\n         model = AutoModelForCausalLM.from_pretrained(peft_model_id, adapter_kwargs=adapter_kwargs)\n         self.assertTrue(self._check_lora_correctly_converted(model))\n \n+        adapter_kwargs = {\"revision\": \"main\", \"subfolder\": \"test_subfolder\"}\n         model = OPTForCausalLM.from_pretrained(peft_model_id, adapter_kwargs=adapter_kwargs)\n         self.assertTrue(self._check_lora_correctly_converted(model))\n "
        }
    ],
    "stats": {
        "total": 33,
        "additions": 18,
        "deletions": 15
    }
}