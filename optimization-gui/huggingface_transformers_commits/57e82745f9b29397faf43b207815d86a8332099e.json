{
    "author": "vasqu",
    "message": "[`v5`] Sync Bert and Bart eager attention (#41248)\n\n* remove from modeling files\n\n* remaining changes\n\n* style / copies\n\n* revert deprecated models and fixup some models\n\n* oops\n\n* sync attn impl\n\n* fix style/copies\n\n* fix distilbert\n\n* remove dim check",
    "sha": "57e82745f9b29397faf43b207815d86a8332099e",
    "files": [
        {
            "sha": "e7a00a00777c34814a8dc940fdada6819dd324a3",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -132,7 +132,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "ba10cfa6653ed1c17003b20cff07942958399256",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -44,6 +44,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n+    TransformersKwargs,\n     auto_docstring,\n     is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n@@ -116,6 +117,7 @@ def forward(self, input_ids: torch.Tensor):\n         return super().forward(input_ids) * self.embed_scale\n \n \n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -124,18 +126,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "197adff873edcbbb67f904022331b8a5ffc5fb10",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -133,7 +133,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "abcc29bf8fcb65e76d33857a078af1ea85b44649",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -78,7 +78,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "bb1caf39ef3972f92860d3ba6e75d6b7973ef670",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -43,7 +43,13 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from ...utils import (\n+    TransformersKwargs,\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_bigbird_pegasus import BigBirdPegasusConfig\n \n@@ -1162,7 +1168,7 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -1171,18 +1177,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "f17855b7f86d74bd759e29f3db4a8a6e97b6007c",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -101,18 +101,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "9a50bf7b663ddb2827d4c073535c9ebf9ab057d2",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n+    TransformersKwargs,\n     auto_docstring,\n     is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n@@ -111,7 +112,7 @@ def forward(self, input_ids: torch.Tensor):\n         return super().forward(input_ids) * self.embed_scale\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -120,18 +121,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "8edb02dd0bddd0e7423c9a0e65509a07ff940663",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n+    TransformersKwargs,\n     auto_docstring,\n     is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n@@ -95,7 +96,7 @@ def forward(\n         return super().forward(position_ids)\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -104,18 +105,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "814275c2f50ad7e2643ddebd4d2215cf6c2409d3",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -423,7 +423,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "e509cdab6354c8a55b3f4a579bfdd7d71bafec2d",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -73,7 +73,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "21a60f2cca22e0c54f8dff7be63e19eb14914c20",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -44,7 +44,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_peft_available, is_torch_flex_attn_available\n+from ...utils import TransformersKwargs, auto_docstring, is_peft_available, is_torch_flex_attn_available\n from .configuration_data2vec_audio import Data2VecAudioConfig\n \n \n@@ -184,18 +184,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "376e42c627981c59fc438be3c8d3261606926369",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -178,7 +178,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "e3b5d5b89ccd46fd23544fd5c8c92f5f9e9da898",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -125,7 +125,7 @@ def forward(\n         return embeddings\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -134,18 +134,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "68a5966ac1a8538dae2e10d002e3437868281efe",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -139,7 +139,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "f095f006a8757fc4e31284576f4fd9d84423e4c9",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -149,7 +149,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "2f81adb1c72d4f6e32ebb31b9eb5711887eeeb41",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -273,7 +273,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "7acf88476a80096d79b7a97d9677b8a4d26b915d",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -239,7 +239,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "1569617fa4269ac86dd465f048e390f5a5227cbb",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -201,18 +201,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "bd117cc92a11ab933ac579d20af6785cc3c10eeb",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -36,7 +36,7 @@\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from .configuration_hubert import HubertConfig\n \n \n@@ -244,18 +244,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "f21b1ff483273bc230b3c5d3c595003862711753",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -45,7 +45,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n-from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_informer import InformerConfig\n \n@@ -363,18 +363,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "e11494d96c00a7a94c737094746ef55fcc9a8147",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -43,7 +43,13 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from ...utils import (\n+    TransformersKwargs,\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_m2m_100 import M2M100Config\n \n@@ -187,7 +193,7 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n         return incremental_indices.long() + padding_idx\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -196,18 +202,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "eb88094304633126c2d1896fc7925b0a1543ce7c",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n+    TransformersKwargs,\n     auto_docstring,\n     is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n@@ -112,7 +113,7 @@ def forward(\n         return super().forward(position_ids)\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -121,18 +122,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "95fad17ee7da66c1a70a845252363afc1b497643",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -45,6 +45,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n+    TransformersKwargs,\n     auto_docstring,\n     is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n@@ -123,7 +124,7 @@ def forward(self, input_ids: torch.Tensor):\n         return super().forward(input_ids) * self.embed_scale\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -132,18 +133,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "3eda860067f51c6bdd78a2e0747960c95579fe44",
            "filename": "src/transformers/models/mobilebert/modeling_mobilebert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -145,7 +145,7 @@ def forward(\n         return embeddings\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -154,18 +154,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "b0b74e0e4f6b9deb66c031d5d9f9afea37ae8327",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -54,7 +54,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.deprecation import deprecate_kwarg\n from ..auto.configuration_auto import AutoConfig\n from ..auto.modeling_auto import AutoModel\n@@ -153,7 +153,7 @@ def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n         return self.weights.index_select(0, position_ids.view(-1)).detach()\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -162,18 +162,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "fc49b9114b01693e507223a129414e80b9f0dd1e",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -46,7 +46,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.deprecation import deprecate_kwarg\n from ..auto.configuration_auto import AutoConfig\n from ..auto.modeling_auto import AutoModel, AutoModelForTextEncoding\n@@ -159,7 +159,7 @@ def forward(self, inputs_embeds: torch.Tensor, past_key_values_length: int = 0):\n         return self.weights.index_select(0, position_ids.view(-1)).detach()\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -168,18 +168,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "f771041dc6292fa1eec3e7b13ed6285bd432996b",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -390,6 +390,7 @@ def forward(self, hidden_states: torch.Tensor, padding_mask: Optional[torch.Tens\n         return hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n \n \n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -398,18 +399,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "aab6d27b9806c3234fbb285be3b541432efed45b",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -28,7 +28,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n-from ...utils import auto_docstring, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from .configuration_patchtsmixer import PatchTSMixerConfig\n \n \n@@ -237,7 +237,7 @@ def forward(self, inputs: torch.Tensor):\n         return out\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -246,18 +246,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "66f76f6ff7c205331456820865d7b822bb61a1e7",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -27,14 +27,14 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n-from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging\n from .configuration_patchtst import PatchTSTConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -43,18 +43,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "2eb9ea64d7f343ee5dbcd63ef338e6aca38291ba",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n+    TransformersKwargs,\n     auto_docstring,\n     is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n@@ -111,7 +112,7 @@ def forward(\n         return super().forward(position_ids)\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -120,18 +121,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "18d54ce6566557df49eb8bcfe925dbbd6b9715a7",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -41,7 +41,13 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from ...utils import (\n+    TransformersKwargs,\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_pegasus_x import PegasusXConfig\n \n@@ -129,7 +135,7 @@ def forward(\n         return pe[None].expand(batch_size, -1, -1)\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -138,18 +144,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "e909358d20e73084a45d2ac2d4e24d66210ce5fa",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -46,7 +46,13 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from ...utils import (\n+    TransformersKwargs,\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_plbart import PLBartConfig\n \n@@ -305,18 +311,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "41327056a22fa5d589150b4893f0ffe3e0c0028b",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -179,7 +179,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "c9a1d11ae859a2595be0ec3aa5cf9d05655163a0",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -176,7 +176,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "64857d753c77ffe8c4bb92f568c44734ad9583ae",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -195,7 +195,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "c3aabf7b55b5f60119d52a6b99debf83938c2bdb",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -36,7 +36,7 @@\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from .configuration_sew import SEWConfig\n \n \n@@ -237,18 +237,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "1e4a6abeafa36933a8c7f4b7dc6cd42f39f80e0d",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n+    TransformersKwargs,\n     auto_docstring,\n     is_torch_flex_attn_available,\n     logging,\n@@ -176,7 +177,7 @@ def create_position_ids_from_input_ids(\n         return incremental_indices.long() + padding_idx\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -185,18 +186,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "a0650577670b7169602de692fc5d802aa4ca07a4",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -41,7 +41,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n-from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_time_series_transformer import TimeSeriesTransformerConfig\n \n@@ -278,7 +278,7 @@ def forward(self, x):\n         return self.value_projection(x)\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -287,18 +287,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "72815a5f34aca5d0455062946c0295045d0d6c34",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -44,7 +44,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from .configuration_unispeech import UniSpeechConfig\n \n \n@@ -276,18 +276,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "2566eb7764a64fb8d2d4833d73b349a2d645505f",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -46,7 +46,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_peft_available, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, is_peft_available, is_torch_flex_attn_available, logging\n from .configuration_unispeech_sat import UniSpeechSatConfig\n \n \n@@ -281,18 +281,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "598136db3e97c27aa9fc23414997e74986cbf000",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -47,6 +47,7 @@\n from ...processing_utils import Unpack\n from ...utils import (\n     ModelOutput,\n+    TransformersKwargs,\n     auto_docstring,\n     cached_file,\n     check_torch_load_is_safe,\n@@ -454,7 +455,7 @@ def forward(self, hidden_states):\n         return hidden_states, norm_hidden_states\n \n \n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -463,18 +464,21 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     if scaling is None:\n         scaling = query.size(-1) ** -0.5\n \n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+\n     if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "00d0222fbcd07968368ff6093fab4a48fe8e0ebd",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -73,7 +73,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "9b3dd976abe23175e1f160788d1b7c3288729948",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -181,7 +181,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        },
        {
            "sha": "d362d8dce89a819b103c046029a2e183a00585a8",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57e82745f9b29397faf43b207815d86a8332099e/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=57e82745f9b29397faf43b207815d86a8332099e",
            "patch": "@@ -175,7 +175,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n "
        }
    ],
    "stats": {
        "total": 326,
        "additions": 222,
        "deletions": 104
    }
}