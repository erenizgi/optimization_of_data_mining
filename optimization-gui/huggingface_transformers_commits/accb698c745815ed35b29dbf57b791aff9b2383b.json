{
    "author": "remi-or",
    "message": "[CB] Support the `num_return_sequences` argument (#42921)\n\n* Reformat to make the code pretty\n\n* Allow for multiple decoding sequences in CB\n\n* Style\n\n* Fix a generation config bug\n\n* Add seed to example\n\n* Batch forking\n\n* Cahnge the fixme (for later PR)\n\n* Copy source is optional\n\n* Added a benchmark script for PR\n\n* Added a test and fixed a bug\n\n* Deepcopy and style\n\n* Review compliance\n\n* Style",
    "sha": "accb698c745815ed35b29dbf57b791aff9b2383b",
    "files": [
        {
            "sha": "720dce383485ddb7d4c2ed0d4e476983f4b158d0",
            "filename": "benchmark_v2/benchmark_scripts/continuous_batching_overall.py",
            "status": "added",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb698c745815ed35b29dbf57b791aff9b2383b/benchmark_v2%2Fbenchmark_scripts%2Fcontinuous_batching_overall.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb698c745815ed35b29dbf57b791aff9b2383b/benchmark_v2%2Fbenchmark_scripts%2Fcontinuous_batching_overall.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fbenchmark_scripts%2Fcontinuous_batching_overall.py?ref=accb698c745815ed35b29dbf57b791aff9b2383b",
            "patch": "@@ -0,0 +1,59 @@\n+import re\n+import subprocess\n+from pathlib import Path\n+\n+from tabulate import tabulate\n+\n+\n+SCRIPT_LOCATION = (Path(__file__).parent.parent.parent / \"examples/pytorch/continuous_batching.py\").as_posix()\n+COMMON_ARGS = \"--log-level WARNING --seed 0\".split()\n+\n+\n+def run_and_parse_cb_example(args: list[str]) -> dict:\n+    print(f\"Benchmarking with args: {args}\")\n+    output = subprocess.check_output(\n+        [\"python\", SCRIPT_LOCATION] + args.split() + COMMON_ARGS,\n+        # stderr=subprocess.DEVNULL,\n+    )\n+    pattern = r\"CB generation took: ([\\d.]+) seconds for (\\d+) tokens\\. ([\\d.]+)tok/s\"\n+    match = re.search(pattern, output.decode(\"utf-8\"))\n+    if match is not None:\n+        return {\n+            \"args\": args,\n+            \"time_seconds\": float(match.group(1)),\n+            \"num_tokens\": int(match.group(2)),\n+            \"throughput_tok_per_sec\": float(match.group(3)),\n+        }\n+    return {}\n+\n+\n+if __name__ == \"__main__\":\n+    results = [\n+        {\n+            \"args\": \"Arguments\",\n+            \"time_seconds\": \"Duration (s)\",\n+            \"num_tokens\": \"Generated tokens\",\n+            \"throughput_tok_per_sec\": \"Throughput (tok/s)\",\n+        }\n+    ]\n+\n+    # Benchmark with different number of samples\n+    results.append(run_and_parse_cb_example(\"--samples 10\"))\n+    results.append(run_and_parse_cb_example(\"--samples 50\"))\n+    results.append(run_and_parse_cb_example(\"--samples 100\"))\n+    results.append(run_and_parse_cb_example(\"--samples 500\"))\n+\n+    # Benchmark with compile: default, flash attention 2 and sdpa\n+    results.append(run_and_parse_cb_example(\"--samples 100 --compile\"))\n+    results.append(run_and_parse_cb_example(\"--samples 100 --compile --attn flash_attention_2\"))\n+    results.append(run_and_parse_cb_example(\"--samples 100 --compile --attn sdpa\"))\n+\n+    # Benchmark with parallel decoding\n+    results.append(run_and_parse_cb_example(\"--samples 50 --compile --num-return-sequences 8 --do-sample\"))\n+    results.append(run_and_parse_cb_example(\"--samples 100 --compile --num-return-sequences 4 --do-sample\"))\n+\n+    # Benchmark with prefix sharing\n+    results.append(run_and_parse_cb_example(\"--samples 500 --add-prefix --compile\"))\n+\n+    print()\n+    print(tabulate(results, tablefmt=\"github\"))"
        },
        {
            "sha": "970b390ffbc295a0fd29d93a77f9ff8ed10d1a65",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb698c745815ed35b29dbf57b791aff9b2383b/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb698c745815ed35b29dbf57b791aff9b2383b/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=accb698c745815ed35b29dbf57b791aff9b2383b",
            "patch": "@@ -177,6 +177,7 @@ def batch_generate(\n     parser.add_argument(\"--cuda-graph\", \"-cg\", help=\"Use cuda graphs\", type=str, default=None)\n     parser.add_argument(\"--compile\", action=\"store_true\", help=\"Compile the model using torch.compile\")\n     parser.add_argument(\"--do-sample\", action=\"store_true\", help=\"Activate sampling\")\n+    parser.add_argument(\"--num-return-sequences\", type=int, default=1, help=\"Number of return sequences\")\n \n     # Benchmark parameters\n     parser.add_argument(\"--samples\", type=int, default=500, help=\"Number of samples to generate\")\n@@ -190,6 +191,7 @@ def batch_generate(\n     parser.add_argument(\"--compare\", action=\"store_true\", help=\"Compare CB generation with classic generate\")\n     parser.add_argument(\"--profile\", type=str, default=None)\n     parser.add_argument(\"--metrics\", action=\"store_true\")\n+    parser.add_argument(\"--seed\", type=int, default=None, help=\"Random seed\")\n \n     # Display parameters\n     parser.add_argument(\"--displayed\", type=int, default=0, help=\"Number of samples to display\")\n@@ -210,6 +212,10 @@ def batch_generate(\n         else:\n             args.attn = \"kernels-community/flash-attn3\"\n \n+    # Set seed\n+    if args.seed is not None:\n+        torch.manual_seed(args.seed)\n+\n     # Create model\n     model_id = \"google/gemma-2-2b-it\" if args.sliding_window > 0 else \"meta-llama/Llama-3.1-8B-Instruct\"\n     has_system_role = args.sliding_window == 0\n@@ -272,17 +278,27 @@ def batch_generate(\n         inputs = inputs if isinstance(inputs, list) else inputs[\"input_ids\"]\n         batched_inputs.append(inputs)\n \n+    # If num_return_sequences > 1, automatically enable do_sample with a warning\n+    do_sample = args.do_sample\n+    if args.num_return_sequences != 1 and not args.do_sample:\n+        logger.warning(\n+            f\"num_return_sequences={args.num_return_sequences} > 1, automatically enabling do_sample=True. \"\n+            \"Set --do-sample explicitly to suppress this warning.\"\n+        )\n+        do_sample = True\n+\n     # Prepare generation config\n     generation_cfg = GenerationConfig(\n         max_new_tokens=args.max_new_tokens,\n         use_cuda_graph=use_cuda_graph,\n         eos_token_id=tokenizer.pad_token_id if args.force_max_length else tokenizer.eos_token_id,\n         pad_token_id=tokenizer.pad_token_id,\n-        do_sample=args.do_sample,\n+        do_sample=do_sample,\n         temperature=0.8,\n         top_p=0.9,\n         num_blocks=args.num_blocks,\n         max_batch_tokens=args.max_batch_tokens,\n+        num_return_sequences=args.num_return_sequences,\n     )\n \n     # Add a compile config if requested"
        },
        {
            "sha": "bfae16a70f887e54b1ccf35a713df817214502e2",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 24,
            "deletions": 1,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb698c745815ed35b29dbf57b791aff9b2383b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb698c745815ed35b29dbf57b791aff9b2383b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=accb698c745815ed35b29dbf57b791aff9b2383b",
            "patch": "@@ -210,7 +210,7 @@ def __init__(\n         self.key_cache: list[torch.Tensor] = []\n         self.value_cache: list[torch.Tensor] = []\n         # We add two extra tokens to the cache to handle padding and generally discard unwanted tokens\n-        self.cache_shape = (num_blocks * self.block_size + 2, self.num_key_value_heads, self.head_dim)\n+        self.cache_shape = ((num_blocks + 2) * self.block_size, self.num_key_value_heads, self.head_dim)\n         for _ in range(group_size):\n             new_layer_key_cache = torch.empty(self.cache_shape, dtype=self.dtype, device=self.device)\n             new_layer_value_cache = torch.empty(self.cache_shape, dtype=self.dtype, device=self.device)\n@@ -388,6 +388,29 @@ def mark_shareable_blocks_as_complete(self, state: RequestState) -> None:\n                     prompt_ids=(state.initial_tokens + state.generated_tokens),\n                 )\n \n+    def copy_cache(self, source_blocks: list[int], forked_blocks: list[int]) -> None:\n+        \"\"\"Copy the cache from the source blocks to the forked blocks.\"\"\"\n+        source_blocks = torch.tensor(source_blocks, device=self.device, dtype=torch.int32)\n+        forked_blocks = torch.tensor(forked_blocks, device=self.device, dtype=torch.int32)\n+        for key_cache, value_cache in zip(self.key_cache, self.value_cache):\n+            key_cache = key_cache.view(-1, self.block_size, self.num_key_value_heads, self.head_dim)\n+            value_cache = value_cache.view(-1, self.block_size, self.num_key_value_heads, self.head_dim)\n+            key_cache[forked_blocks] = key_cache[source_blocks]\n+            value_cache[forked_blocks] = value_cache[source_blocks]\n+        # FIXME: consolidate the cache into a single tensor of shape (group_size, 2, *self.k_or_v_cache_shape)\n+        # This will allow for  better .update and a single copy instead of one per cache tensor\n+\n+    def fork_request(self, source_request_id: str, destination_request_ids: list[str]) -> tuple[list[int], list[int]]:\n+        \"\"\"Fork the cache of a request (state) into the one of a list of requests with the given (dst_request_ids).\"\"\"\n+        # These lists will be the accumulators for the source and destination blocks for the cache copy\n+        source_blocks, destination_blocks = [], []\n+        # Main fork loop\n+        for cm in self.group_cache_managers:\n+            src_blocks, dst_blocks = cm.fork_blocks(source_request_id, destination_request_ids, self._block_manager)\n+            source_blocks.extend(src_blocks)\n+            destination_blocks.extend(dst_blocks)\n+        return source_blocks, destination_blocks\n+\n \n # TODO: rework computation with the groups and their sizes\n class PagedAttentionMemoryHandler:"
        },
        {
            "sha": "c2186b00ee6178051c07493f486ae1f338709a9c",
            "filename": "src/transformers/generation/continuous_batching/cache_manager.py",
            "status": "modified",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb698c745815ed35b29dbf57b791aff9b2383b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb698c745815ed35b29dbf57b791aff9b2383b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py?ref=accb698c745815ed35b29dbf57b791aff9b2383b",
            "patch": "@@ -123,6 +123,62 @@ def get_free_blocks(\n         # In both cases, we return the allocated block ids\n         return allocated_block_ids\n \n+    def fork_blocks(\n+        self, parent_blocks: list[int], num_forks: int, shareable: bool, group_id: int\n+    ) -> tuple[list[list[int]], list[int], list[int]]:\n+        \"\"\"Fork a given list of (parent_blocks) as many times as (num_forks). If the blocks are (shareable), we use\n+        reference on the blocks that are complete. Otherwise, we allocate new blocks and keep track of their indices to\n+        later copy the physical cache. For instance, when forking 4 blocks for 2 children:\n+\n+        Parent blocks: [0, 1, 2, 3], with all blocks being complete except the last one (block 3).\n+\n+        ----------------------------------------- IF BLOCKS ARE NOT SHAREABLE -----------------------------------------\n+\n+        Forked blocks lists: [[5, 6, 7, 8], [9, 10, 11, 12]]\n+        Copy source:          [0, 1, 2, 3,   0,  1,  2,  3]\n+                               ↓  ↓  ↓  ↓    ↓   ↓   ↓   ↓\n+        Copy destination:     [5, 6, 7, 8,   9, 10, 11, 12]  → 8 blocks are newly allocated and copied\n+\n+        ----------------------------------------- IF BLOCKS ARE SHAREABLE ---------------------------------------------\n+\n+        Forked blocks lists: [[0, 1, 2, 5], [0, 1, 2, 6]]\n+        Copy source:          [         3,            3]     (block 3 is not complete so it's copied, not referenced)\n+                                        ↓             ↓\n+        Copy destination:     [         5,            6]     → only 2 blocks are newly allocated and copied\n+        \"\"\"\n+        # First phase: reference all complete blocks\n+        forked_by_reference = []\n+\n+        if shareable:\n+            for block_id in parent_blocks:\n+                block = self._id_to_block[block_id]\n+                if block.is_complete:\n+                    forked_by_reference.append(block.id)\n+                    block.ref_count += num_forks\n+                else:\n+                    break\n+\n+        # Early return if we have forked all blocks by reference\n+        blocks_to_copy = len(parent_blocks) - len(forked_by_reference)\n+        if blocks_to_copy == 0:\n+            return [forked_by_reference[:] for _ in range(num_forks)], [], []\n+\n+        # From now on, each child will have its own list of blocks\n+        forked_blocks_lists = []\n+        copy_src = []\n+        copy_dst = []\n+\n+        # Second phase: allocate new blocks if needed\n+        parent_id = forked_by_reference[-1] if forked_by_reference else None\n+        for _ in range(num_forks):\n+            allocated_block_ids = self.get_free_blocks(blocks_to_copy, parent_id, shareable, group_id)\n+            if allocated_block_ids is None:\n+                return None, [], []\n+            forked_blocks_lists.append(forked_by_reference + allocated_block_ids)\n+            copy_src.extend(parent_blocks[-blocks_to_copy:])\n+            copy_dst.extend(allocated_block_ids)\n+        return forked_blocks_lists, copy_src, copy_dst\n+\n     def increase_ref_count(self, block_id: int) -> None:\n         \"\"\"Increases the reference count of a given (block_id).\"\"\"\n         block = self._id_to_block[block_id]\n@@ -243,6 +299,36 @@ def get_write_indices(self, request_id: str, past_length: int, query_length: int\n     def get_seqlens_k(self, request_id: str, past_length: int, query_length: int) -> tuple[str, int]:\n         \"\"\"Returns the attention type of the cache allocator and the key sequence length for the given request_id.\"\"\"\n \n+    def fork_blocks(\n+        self, parent_request_id: str, children_request_ids: list[str], block_manager: BlockManager\n+    ) -> tuple[list[int], list[int]]:\n+        \"\"\"Forks the cache blocks of a (parent_request_id) to a list of (children_request_ids). To manage the blocks,\n+        the (block_manager) is used. When forking, the child's block are either shared with the parent, or they need to\n+        be copied from the parent. Hence we return two lists of blocks that need to be copied: one for the source and\n+        one for the destination.\"\"\"\n+\n+        # Sanity checks\n+        if parent_request_id not in self.block_table:\n+            raise ValueError(f\"No block table found for request {parent_request_id}\")\n+\n+        # Actual forking\n+        parent_blocks = self.block_table[parent_request_id]\n+        list_forked_blocks, copy_src, copy_dst = block_manager.fork_blocks(\n+            parent_blocks=parent_blocks,\n+            num_forks=len(children_request_ids),\n+            shareable=self.uses_block_sharing,\n+            group_id=self._index,\n+        )\n+        if list_forked_blocks is None:\n+            raise ValueError(f\"Failed to fork blocks for request {parent_request_id}\")\n+\n+        # Update the block table for all children requests\n+        for children_request_id, forked_blocks in zip(children_request_ids, list_forked_blocks):\n+            if children_request_id in self.block_table:\n+                raise ValueError(f\"Block table already exists for request {children_request_id}\")\n+            self.block_table[children_request_id] = forked_blocks\n+        return copy_src, copy_dst\n+\n \n class FullAttentionCacheAllocator(CacheAllocator):\n     \"\"\"Cache manager for a group of full attention layers.\"\"\""
        },
        {
            "sha": "a3dc357a34b2ec098204abf72f5b5e8f0b2925a3",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 69,
            "deletions": 32,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb698c745815ed35b29dbf57b791aff9b2383b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb698c745815ed35b29dbf57b791aff9b2383b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=accb698c745815ed35b29dbf57b791aff9b2383b",
            "patch": "@@ -572,13 +572,19 @@ def _maybe_send_output(self, state: RequestState) -> None:\n     def update_batch(self) -> None:\n         \"\"\"Update request states based on generated tokens.\"\"\"\n         new_tokens = self._get_new_tokens(len(self.requests_in_batch))\n-        for i, state in enumerate(self.requests_in_batch):\n+        current_logits_index = 0\n+        for state in self.requests_in_batch:\n             # If the request has no remaining prompt ids, it means prefill has already ended or just finished\n             if len(state.remaining_prefill_tokens) == 0:\n-                self.metrics.record_ttft_metric(state.created_time, state.request_id)\n-                state.status = RequestStatus.DECODING\n-                token = new_tokens[i]\n+                # If there are no generated tokens yet, it means prefill just ended\n+                if state.generated_len() == 0:\n+                    self.metrics.record_ttft_metric(state.created_time, state.request_id)\n+                    state.status = RequestStatus.DECODING\n+\n+                token = new_tokens[current_logits_index]\n                 state.tokens_to_process = [token]\n+                current_logits_index += 1\n+\n                 # Update the request and stop if it is complete\n                 is_finished = state.update_and_check_completion(token)\n                 # We mark the completed blocks as such\n@@ -594,6 +600,27 @@ def update_batch(self) -> None:\n             else:\n                 raise ValueError(f\"Request {state.request_id} is in an unexpected state: {state.status}\")\n \n+        # If some requests need to be forked, we do it now\n+        copy_source, copy_destination = [], []\n+        while self.scheduler._requests_to_fork:\n+            # Get the number of children and reset it so it's not forked again\n+            state = self.scheduler._requests_to_fork.pop()\n+            num_children = state.num_children\n+            state.num_children = 0\n+            # Create the new request and add them to the scheduler\n+            new_request_ids = [f\"{state.request_id}__child#{i}\" for i in range(num_children)]\n+            for new_request_id in new_request_ids:\n+                self.scheduler.active_requests[new_request_id] = state.fork(new_request_id)\n+            # Fork the cache\n+            copy_src, copy_dst = self.cache.fork_request(state.request_id, new_request_ids)\n+            copy_source.extend(copy_src)\n+            copy_destination.extend(copy_dst)\n+            # FIXME: if fork cant be done, create a new pending request without forking instead of crashing everything\n+\n+        # The copy induced by the fork is done in one go (if it's even needed)\n+        if copy_source:\n+            self.cache.copy_cache(copy_source, copy_destination)\n+\n         if self.cache.get_num_free_blocks() == 0:\n             raise ValueError(\"No more free blocks\")\n \n@@ -760,29 +787,35 @@ def __init__(\n             num_kv_padding_intervals: (optional) Number of intervals used to pad the keys/values dimension\n             allow_block_sharing: (optional) Whether to allow block sharing if the model has some full attention layers\n         \"\"\"\n-        # Reloade paged version if necessary\n+        # Reload paged version of the attention implementation if necessary\n         if \"paged|\" not in model.config._attn_implementation:\n             model.set_attn_implementation(f\"paged|{model.config._attn_implementation}\")\n \n+        # Internal arguments\n         self.model = model.eval()\n-        generation_config = model.generation_config if generation_config is None else generation_config\n-        self.generation_config = generation_config\n+        self.manual_eviction = manual_eviction\n+        self._allow_block_sharing = allow_block_sharing\n+        self._use_prefix_sharing = allow_block_sharing  # approximation until the cache is created\n+\n         self.input_queue = queue.Queue(maxsize=max_queue_size)\n         self.output_queue = queue.Queue()\n         self.stop_event = threading.Event()\n-        self.log_prob_generation = getattr(generation_config, \"log_prob_generation\", False)\n+        self.batch_processor: ContinuousBatchProcessor | None = None\n         self._generation_thread = None\n         self._request_counter = 0\n         self._request_lock = threading.Lock()\n-        self.model.generation_config.top_p = None\n+\n+        # Generation config related arguments\n+        generation_config = model.generation_config if generation_config is None else generation_config\n+        self.generation_config = generation_config\n+        self.log_prob_generation = getattr(generation_config, \"log_prob_generation\", False)\n         self.do_sample = getattr(generation_config, \"do_sample\", True)\n         self.logit_processor = self.model._get_logits_processor(generation_config)\n-        self.profile = getattr(generation_config, \"profile\", False)  # TODO: not supported yet\n-        self.manual_eviction = manual_eviction\n-        self.batch_processor: ContinuousBatchProcessor | None = None\n-        self._allow_block_sharing = allow_block_sharing\n-        self._use_prefix_sharing = allow_block_sharing  # approximation until the cache is created\n+        self.num_return_sequences = getattr(generation_config, \"num_return_sequences\", 1)\n+\n+        # self.model.generation_config.top_p = None NOTE: figure out why this was here\n \n+        # Cuda graph behavior is determined below using either user-specified arguments or heuristics\n         self.use_cuda_graph = self._decide_use_cuda_graphs(\n             use_cuda_graph=getattr(generation_config, \"use_cuda_graph\", None),\n             num_q_padding_intervals=num_q_padding_intervals,\n@@ -796,6 +829,7 @@ def __init__(\n             num_kv_padding_intervals if num_kv_padding_intervals > 0 else NUM_KV_PADDING_INTERVALS\n         )\n \n+        # Log probability generation is not supported yet (TODO)\n         if self.log_prob_generation:\n             raise NotImplementedError(\"log_prob_generation is not supported yet\")\n \n@@ -929,6 +963,7 @@ def add_request(\n         state = RequestState(\n             request_id=request_id,\n             initial_tokens=list(input_ids),\n+            num_children=self.num_return_sequences - 1,\n             record_timestamps=record_timestamps,\n             tokens_to_process=list(input_ids),\n             max_new_tokens=max_new_tokens,\n@@ -1226,24 +1261,26 @@ def generate_batch(\n \n         # Initialize manager with the batch inputs\n         results = {}\n-        num_requests = len(inputs)\n-        with (\n-            self.continuous_batching_context_manager(\n-                generation_config=generation_config,\n-                num_q_cuda_graphs=num_q_padding_intervals,\n-                num_kv_cuda_graphs=num_kv_padding_intervals,\n-                allow_block_sharing=allow_block_sharing,\n-                block=True,\n-                timeout=5,\n-            ) as manager,\n-            logging_redirect_tqdm([logger]),\n-            tqdm(\n-                total=num_requests,\n-                disable=(not progress_bar),\n-                desc=f\"Solving {num_requests} requests\",\n-                unit=\"request\",\n-            ) as pbar,\n-        ):\n+        gen_cfg = self.generation_config if generation_config is None else generation_config\n+        num_requests = len(inputs) * gen_cfg.num_return_sequences\n+        # Prepare context managers for the main loop\n+        manager_cm = self.continuous_batching_context_manager(\n+            generation_config=generation_config,\n+            num_q_cuda_graphs=num_q_padding_intervals,\n+            num_kv_cuda_graphs=num_kv_padding_intervals,\n+            allow_block_sharing=allow_block_sharing,\n+            block=True,\n+            timeout=5,\n+        )\n+        logging_cm = logging_redirect_tqdm([logger])\n+        pbar_cm = tqdm(\n+            total=num_requests,\n+            disable=(not progress_bar),\n+            desc=f\"Solving {num_requests} requests\",\n+            unit=\"request\",\n+        )\n+        # Main loop\n+        with manager_cm as manager, logging_cm, pbar_cm as pbar:\n             try:\n                 manager.add_requests(\n                     inputs=inputs, max_new_tokens=kwargs.get(\"max_new_tokens\"), record_timestamps=record_timestamps"
        },
        {
            "sha": "8435164db6d369f9f258e6d193e8bdca0c77fa57",
            "filename": "src/transformers/generation/continuous_batching/requests.py",
            "status": "modified",
            "additions": 28,
            "deletions": 1,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb698c745815ed35b29dbf57b791aff9b2383b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb698c745815ed35b29dbf57b791aff9b2383b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py?ref=accb698c745815ed35b29dbf57b791aff9b2383b",
            "patch": "@@ -101,6 +101,8 @@ class RequestState:\n \n     Attributes:\n         request_id (str): The ID of the generation request.\n+        initial_tokens (list[int]): The initial prompt tokens.\n+        num_children (int): The number of children requests\n         full_prompt_ids (list[int] | None): The tokens IDs of the full prompt.\n         prompt_ids (list[int] | None): The tokens IDs currently being processed.\n         remaining_prompt_ids (list[int]): The tokens IDs remaining to be processed (for split requests).\n@@ -121,6 +123,7 @@ class RequestState:\n     initial_tokens: list[int]  # Initial prompt tokens\n     # Optional fields\n     record_timestamps: bool = False  # Whether to record timestamps for the generated tokens\n+    num_children: int = 0  # Number of children requests\n     # Internal fields\n     tokens_to_process: list[int] | None = None  # Tokens IDs currently being processed\n     remaining_prefill_tokens: list[int] = field(default_factory=list)  # For split requests, prefill left to process\n@@ -181,7 +184,7 @@ def update_and_check_completion(self, token_id: int) -> bool:\n         Returns:\n             bool: True if the request is now complete, False otherwise\n         \"\"\"\n-        # Only update if we're in decoding state\n+        # Only update if we're in decoding state # TODO: seems useless (always true) -- remove this\n         if self.status != RequestStatus.DECODING:\n             return False\n \n@@ -227,3 +230,27 @@ def to_generation_output(self):\n             error=self.error,\n             timestamps=self.timestamps,\n         )\n+\n+    def fork(self, new_request_id: str) -> \"RequestState\":\n+        \"\"\"Fork the request into a new request with the same state expect for request_id, created_time and lifespan.\"\"\"\n+        t = time.perf_counter()\n+        new_request = RequestState(\n+            request_id=new_request_id,\n+            initial_tokens=self.initial_tokens,\n+            num_children=self.num_children,\n+            tokens_to_process=self.tokens_to_process[:],\n+            remaining_prefill_tokens=self.remaining_prefill_tokens[:],\n+            generated_tokens=self.generated_tokens[:],\n+            allocated_blocks=self.allocated_blocks,\n+            position_offset=self.position_offset,\n+            status=self.status,\n+            max_new_tokens=self.max_new_tokens,\n+            eos_token_id=self.eos_token_id,\n+            streaming=self.streaming,\n+            created_time=t,\n+            lifespan=(t, -1),\n+            timestamps=None if self.timestamps is None else self.timestamps[:],\n+            error=self.error,\n+            record_timestamps=self.record_timestamps,\n+        )\n+        return new_request"
        },
        {
            "sha": "42bd607a216f49afc74ce1c75496a5fad4a5ecac",
            "filename": "src/transformers/generation/continuous_batching/scheduler.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb698c745815ed35b29dbf57b791aff9b2383b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb698c745815ed35b29dbf57b791aff9b2383b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py?ref=accb698c745815ed35b29dbf57b791aff9b2383b",
            "patch": "@@ -36,6 +36,7 @@ def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = Fa\n         self.retain_cache_on_finish = retain_cache_on_finish\n         self._cancellation_lock = threading.Lock()\n         self._requests_to_cancel: set[str] = set()\n+        self._requests_to_fork: list[RequestState] = []\n \n     @traced\n     def add_waiting_request(self, state: RequestState):\n@@ -151,8 +152,13 @@ def _prepare_request_for_processing(\n         else:\n             request_tokens = state.tokens_to_process\n \n+        # If the request has one or more children we make sure not to prefill it entrirely\n+        if state.num_children > 0 and token_budget >= len(request_tokens) - 1:\n+            token_budget = len(request_tokens) - 1\n+            self._requests_to_fork.append(state)\n+\n+        # Case: we can process the entire prompt/remainder\n         if len(request_tokens) < token_budget:\n-            # Can process the entire prompt/remainder\n             if state.status == RequestStatus.PENDING:\n                 self.active_requests[state.request_id] = state\n                 state.status = RequestStatus.PREFILLING\n@@ -161,8 +167,9 @@ def _prepare_request_for_processing(\n                 state.status = RequestStatus.PREFILLING\n                 state.tokens_to_process = state.remaining_prefill_tokens\n                 state.remaining_prefill_tokens = []\n+\n+        # Otherwise: we need to split the request\n         else:\n-            # Need to split the request\n             if state.status == RequestStatus.PENDING:\n                 self.active_requests[state.request_id] = state\n                 state.status = RequestStatus.PREFILLING_SPLIT"
        },
        {
            "sha": "0e3c20aff2b67f0279b72285579572ec1e151a69",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb698c745815ed35b29dbf57b791aff9b2383b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb698c745815ed35b29dbf57b791aff9b2383b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=accb698c745815ed35b29dbf57b791aff9b2383b",
            "patch": "@@ -1305,7 +1305,7 @@ def _get_logits_processor(\n         if generation_config.do_sample:\n             # In beam methods, we need to keep at least one non-eos token to explore continuations that might have a\n             # better score (i.e. keep len(list(generation_config._eos_token_tensor)) + 1)\n-            if generation_config.num_beams > 1:\n+            if generation_config.num_beams is not None and generation_config.num_beams > 1:\n                 if isinstance(generation_config._eos_token_tensor, list):\n                     min_tokens_to_keep = len(generation_config._eos_token_tensor) + 1\n                 elif isinstance(generation_config._eos_token_tensor, torch.Tensor):"
        },
        {
            "sha": "3b803708d02bf889d80b9bce6818264d0267eeb6",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 40,
            "deletions": 1,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/accb698c745815ed35b29dbf57b791aff9b2383b/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accb698c745815ed35b29dbf57b791aff9b2383b/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=accb698c745815ed35b29dbf57b791aff9b2383b",
            "patch": "@@ -212,7 +212,6 @@ def _test_continuous_batching_parity(\n         chats = [[{\"role\": \"user\", \"content\": user_message}] for user_message in user_messages]\n         tokenized = [tokenizer.apply_chat_template(chat, add_generation_prompt=True) for chat in chats]\n         input_ids = [(x if isinstance(x, list) else x[\"input_ids\"]) for x in tokenized]\n-        print(f\"{input_ids[0] = } {type(input_ids[0]) = }\")\n \n         # Eager and SDPA implementations get a precision boost to account for the fact that an attention mask is used in\n         # continuous batching but not in generate\n@@ -504,3 +503,43 @@ def test_block_sharing_with_hybrid_model(self) -> None:\n         }).get_expectation()  # fmt: skip\n \n         return self._test_block_sharing(model_id, num_layer_groups, input_msg, expected_generated_tokens)\n+\n+    @parameterized.expand([True, False])\n+    @require_torch_accelerator\n+    def test_num_return_sequences(self, allow_block_sharing: bool) -> None:\n+        model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n+        user_messages = [\n+            \"A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?\"\n+        ]\n+        chats = [[{\"role\": \"user\", \"content\": user_message}] for user_message in user_messages]\n+        tokenized = [tokenizer.apply_chat_template(chat, add_generation_prompt=True) for chat in chats]\n+        input_ids = [(x if isinstance(x, list) else x[\"input_ids\"]) for x in tokenized]\n+\n+        # Generation with continuous batching\n+        model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=\"sdpa\")\n+        model = model.to(torch_device).eval()\n+        model.generation_config.max_new_tokens = 30\n+        model.generation_config.do_sample = False\n+\n+        # Generation with continuous batching\n+        manager_cm = model.continuous_batching_context_manager(\n+            allow_block_sharing=allow_block_sharing, block=True, timeout=5\n+        )\n+        # Main loop\n+        results = []\n+        with manager_cm as manager:\n+            manager.num_return_sequences = 2\n+            manager.add_requests(inputs=input_ids, max_new_tokens=30)\n+            requests_left = 2\n+            while requests_left:\n+                result = manager.get_result(timeout=1)\n+                if result and result.is_finished():\n+                    results.append(result)\n+                    requests_left -= 1\n+                else:\n+                    if not manager.is_running():\n+                        break\n+\n+        self.assertEqual(len(results), 2, f\"Expected 2 results, but got {len(results) = }\")\n+        self.assertEqual(results[0].generated_tokens, results[1].generated_tokens)"
        }
    ],
    "stats": {
        "total": 372,
        "additions": 333,
        "deletions": 39
    }
}