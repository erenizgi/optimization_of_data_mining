{
    "author": "SunMarc",
    "message": "fix hqq due to recent modeling changes (#36771)\n\n* fix-hqq\n\n* style\n\n* test",
    "sha": "3017536ebf3f215130cf4925a574c19fb4854cae",
    "files": [
        {
            "sha": "60d334fdd9be6fccb6710ae5e92aed89c6151558",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3017536ebf3f215130cf4925a574c19fb4854cae/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3017536ebf3f215130cf4925a574c19fb4854cae/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=3017536ebf3f215130cf4925a574c19fb4854cae",
            "patch": "@@ -169,7 +169,12 @@ def check_quantized_param(\n                 and tensor_name != \"bias\"\n             )\n         else:\n-            return isinstance(module, torch.nn.Linear) and tensor_name == \"weight\"\n+            # we need a special path for bias since hqq overwrote load_state_dict for this layer\n+            return (\n+                isinstance(module, torch.nn.Linear)\n+                and tensor_name == \"weight\"\n+                or (isinstance(module, HQQLinear) and tensor_name == \"bias\")\n+            )\n \n     def create_quantized_param(\n         self,\n@@ -194,6 +199,10 @@ def create_quantized_param(\n         parent_module = find_parent(model, layer_name)\n         node = layer_name.split(\".\")[-1]\n \n+        if tensor_name == \"bias\":\n+            # this should already be set\n+            return\n+\n         # set module state_dict\n         module_state_dict = {}\n         for k, v in state_dict.items():"
        },
        {
            "sha": "7335a937086d4f9a089a300920d83dafef39370d",
            "filename": "tests/quantization/hqq/test_hqq.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/3017536ebf3f215130cf4925a574c19fb4854cae/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3017536ebf3f215130cf4925a574c19fb4854cae/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhqq%2Ftest_hqq.py?ref=3017536ebf3f215130cf4925a574c19fb4854cae",
            "patch": "@@ -145,6 +145,28 @@ def test_fp16_quantized_model_multipgpu(self):\n         check_forward(self, hqq_runner.model)\n \n \n+@slow\n+@require_torch_gpu\n+@require_accelerate\n+@require_hqq\n+class HQQTestBias(unittest.TestCase):\n+    def tearDown(self):\n+        cleanup()\n+\n+    def test_fp16_quantized_model(self):\n+        \"\"\"\n+        Simple LLM model testing fp16 with bias\n+        \"\"\"\n+        quant_config = HqqConfig(nbits=8, group_size=64)\n+\n+        hqq_runner = HQQLLMRunner(\n+            model_id=\"facebook/opt-125m\", quant_config=quant_config, compute_dtype=torch.float16, device=torch_device\n+        )\n+\n+        check_hqqlayer(self, hqq_runner.model.model.decoder.layers[0].self_attn.v_proj)\n+        check_forward(self, hqq_runner.model)\n+\n+\n @slow\n @require_torch_gpu\n @require_accelerate"
        }
    ],
    "stats": {
        "total": 33,
        "additions": 32,
        "deletions": 1
    }
}