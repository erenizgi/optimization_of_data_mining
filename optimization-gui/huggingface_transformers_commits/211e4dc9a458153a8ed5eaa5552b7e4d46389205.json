{
    "author": "zucchini-nlp",
    "message": "[chat-template] fix video loading (#37146)\n\n* fix\n\n* add video\n\n* trigger\n\n* push new iamges\n\n* fix tests\n\n* revert\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "211e4dc9a458153a8ed5eaa5552b7e4d46389205",
    "files": [
        {
            "sha": "a13d855a53e7f6d0ff96abe4aa53943e2b6b337d",
            "filename": "docker/torch-light.dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/211e4dc9a458153a8ed5eaa5552b7e4d46389205/docker%2Ftorch-light.dockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/211e4dc9a458153a8ed5eaa5552b7e4d46389205/docker%2Ftorch-light.dockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftorch-light.dockerfile?ref=211e4dc9a458153a8ed5eaa5552b7e4d46389205",
            "patch": "@@ -7,5 +7,5 @@ ENV UV_PYTHON=/usr/local/bin/python\n RUN pip --no-cache-dir install uv && uv venv && uv pip install --no-cache-dir -U pip setuptools\n RUN uv pip install --no-cache-dir 'torch' 'torchvision' 'torchaudio' --index-url https://download.pytorch.org/whl/cpu\n RUN uv pip install --no-deps timm accelerate --extra-index-url https://download.pytorch.org/whl/cpu\n-RUN uv pip install --no-cache-dir librosa \"git+https://github.com/huggingface/transformers.git@${REF}#egg=transformers[sklearn,sentencepiece,vision,testing,tiktoken,num2words]\"\n+RUN uv pip install --no-cache-dir librosa \"git+https://github.com/huggingface/transformers.git@${REF}#egg=transformers[sklearn,sentencepiece,vision,testing,tiktoken,num2words,video]\"\n RUN uv pip uninstall transformers"
        },
        {
            "sha": "b4865263883b1a986d53fa5a82db91e13b969f62",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/211e4dc9a458153a8ed5eaa5552b7e4d46389205/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211e4dc9a458153a8ed5eaa5552b7e4d46389205/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=211e4dc9a458153a8ed5eaa5552b7e4d46389205",
            "patch": "@@ -1382,28 +1382,28 @@ def apply_chat_template(\n                             batch_audios.append(load_audio(fname, sampling_rate=mm_load_kwargs[\"sampling_rate\"]))\n                     else:\n                         for fname in video_fnames:\n-                            if isinstance(fname, (list, tuple)) and isinstance(fname[0], str):\n-                                video = [np.array(load_image(image_fname)).T for image_fname in fname]\n-                                # create a 4D video because `load_video` always returns a 4D array\n-                                video = np.stack(video)\n-                                metadata = None\n-                                audios = None\n-                                logger.warning(\n-                                    \"When loading the video from list of images, we cannot infer metadata such as `fps` or `duration`. \"\n-                                    \"If your model uses this metadata during processing, please load the whole video and let the model sample frames instead.\"\n-                                )\n-                            else:\n-                                video, metadata = load_video(\n-                                    fname,\n-                                    num_frames=mm_load_kwargs[\"num_frames\"],\n-                                    fps=mm_load_kwargs[\"video_fps\"],\n-                                    backend=mm_load_kwargs[\"video_load_backend\"],\n-                                    sample_indices_fn=mm_load_kwargs[\"sample_indices_fn\"],\n-                                )\n-                                audios = load_audio(fname, sampling_rate=mm_load_kwargs[\"sampling_rate\"])\n-                            batch_audios.append(audios)\n-                            videos.append(video)\n-                            video_metadata.append(metadata)\n+                            batch_audios.append(load_audio(fname, sampling_rate=mm_load_kwargs[\"sampling_rate\"]))\n+\n+                    for fname in video_fnames:\n+                        if isinstance(fname, (list, tuple)) and isinstance(fname[0], str):\n+                            video = [np.array(load_image(image_fname)).T for image_fname in fname]\n+                            # create a 4D video because `load_video` always returns a 4D array\n+                            video = np.stack(video)\n+                            metadata = None\n+                            logger.warning(\n+                                \"When loading the video from list of images, we cannot infer metadata such as `fps` or `duration`. \"\n+                                \"If your model uses this metadata during processing, please load the whole video and let the model sample frames instead.\"\n+                            )\n+                        else:\n+                            video, metadata = load_video(\n+                                fname,\n+                                num_frames=mm_load_kwargs[\"num_frames\"],\n+                                fps=mm_load_kwargs[\"video_fps\"],\n+                                backend=mm_load_kwargs[\"video_load_backend\"],\n+                                sample_indices_fn=mm_load_kwargs[\"sample_indices_fn\"],\n+                            )\n+                        videos.append(video)\n+                        video_metadata.append(metadata)\n \n                 # Currently all processors can accept nested list of batches, but not flat list of visuals\n                 # So we'll make a batched list of images and let the processor handle it"
        },
        {
            "sha": "d2dc9252021bde3cf2a420b56ce00bc49cd7d72b",
            "filename": "tests/models/qwen2_5_vl/test_processor_qwen2_5_vl.py",
            "status": "modified",
            "additions": 112,
            "deletions": 0,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/211e4dc9a458153a8ed5eaa5552b7e4d46389205/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211e4dc9a458153a8ed5eaa5552b7e4d46389205/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py?ref=211e4dc9a458153a8ed5eaa5552b7e4d46389205",
            "patch": "@@ -18,6 +18,7 @@\n import unittest\n \n import pytest\n+from huggingface_hub import hf_hub_download\n \n from transformers import AutoProcessor, Qwen2Tokenizer\n from transformers.testing_utils import require_av, require_torch, require_vision\n@@ -326,3 +327,114 @@ def test_kwargs_overrides_custom_image_processor_kwargs(self):\n         self.assertEqual(inputs[self.images_input_name].shape[0], 612)\n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n         self.assertEqual(inputs[self.images_input_name].shape[0], 800)\n+\n+    @require_av\n+    def test_chat_template_video_custom_sampling(self):\n+        \"\"\"\n+        Tests that models can pass their custom callables to sample video indices.\n+        \"\"\"\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        video_file_path = hf_hub_download(\n+            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n+        )\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"video\", \"path\": video_file_path},\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        def dummy_sample_indices_fn(metadata, **fn_kwargs):\n+            # sample only the first two frame always\n+            return [0, 1]\n+\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            sample_indices_fn=dummy_sample_indices_fn,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 14400)\n+\n+    @require_av\n+    def test_chat_template_video_special_processing(self):\n+        \"\"\"\n+        Tests that models can use their own preprocessing to preprocess conversations.\n+        \"\"\"\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        video_file_path = hf_hub_download(\n+            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n+        )\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"video\", \"path\": video_file_path},\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        def _process_messages_for_chat_template(\n+            conversation,\n+            batch_images,\n+            batch_videos,\n+            batch_video_metadata,\n+            **chat_template_kwargs,\n+        ):\n+            # Let us just always return a dummy prompt\n+            new_msg = [\n+                [\n+                    {\n+                        \"role\": \"user\",\n+                        \"content\": [\n+                            {\"type\": \"video\"},  # no need to use path, video is loaded already by this moment\n+                            {\"type\": \"text\", \"text\": \"Dummy prompt for preprocess testing\"},\n+                        ],\n+                    },\n+                ]\n+            ]\n+            return new_msg\n+\n+        processor._process_messages_for_chat_template = _process_messages_for_chat_template\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+\n+        # Check with `in` because we don't know how each template formats the prompt with BOS/EOS/etc\n+        formatted_text = processor.batch_decode(out_dict_with_video[\"input_ids\"], skip_special_tokens=True)[0]\n+        self.assertTrue(\"Dummy prompt for preprocess testing\" in formatted_text)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1756800)"
        },
        {
            "sha": "58dd2c8544d84df6f4a73b653c5d1d165fbca376",
            "filename": "tests/models/qwen2_vl/test_processor_qwen2_vl.py",
            "status": "modified",
            "additions": 112,
            "deletions": 0,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/211e4dc9a458153a8ed5eaa5552b7e4d46389205/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211e4dc9a458153a8ed5eaa5552b7e4d46389205/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py?ref=211e4dc9a458153a8ed5eaa5552b7e4d46389205",
            "patch": "@@ -18,6 +18,7 @@\n import unittest\n \n import pytest\n+from huggingface_hub import hf_hub_download\n \n from transformers import AutoProcessor, Qwen2Tokenizer\n from transformers.testing_utils import require_av, require_torch, require_vision\n@@ -308,6 +309,117 @@ def test_chat_template_video(self):\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 71280)\n \n+    @require_av\n+    def test_chat_template_video_custom_sampling(self):\n+        \"\"\"\n+        Tests that models can pass their custom callables to sample video indices.\n+        \"\"\"\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        video_file_path = hf_hub_download(\n+            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n+        )\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"video\", \"path\": video_file_path},\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        def dummy_sample_indices_fn(metadata, **fn_kwargs):\n+            # sample only the first two frame always\n+            return [0, 1]\n+\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            sample_indices_fn=dummy_sample_indices_fn,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 14400)\n+\n+    @require_av\n+    def test_chat_template_video_special_processing(self):\n+        \"\"\"\n+        Tests that models can use their own preprocessing to preprocess conversations.\n+        \"\"\"\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        video_file_path = hf_hub_download(\n+            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n+        )\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"video\", \"path\": video_file_path},\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        def _process_messages_for_chat_template(\n+            conversation,\n+            batch_images,\n+            batch_videos,\n+            batch_video_metadata,\n+            **chat_template_kwargs,\n+        ):\n+            # Let us just always return a dummy prompt\n+            new_msg = [\n+                [\n+                    {\n+                        \"role\": \"user\",\n+                        \"content\": [\n+                            {\"type\": \"video\"},  # no need to use path, video is loaded already by this moment\n+                            {\"type\": \"text\", \"text\": \"Dummy prompt for preprocess testing\"},\n+                        ],\n+                    },\n+                ]\n+            ]\n+            return new_msg\n+\n+        processor._process_messages_for_chat_template = _process_messages_for_chat_template\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+\n+        # Check with `in` because we don't know how each template formats the prompt with BOS/EOS/etc\n+        formatted_text = processor.batch_decode(out_dict_with_video[\"input_ids\"], skip_special_tokens=True)[0]\n+        self.assertTrue(\"Dummy prompt for preprocess testing\" in formatted_text)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1756800)\n+\n     def test_kwargs_overrides_custom_image_processor_kwargs(self):\n         processor_components = self.prepare_components()\n         processor_components[\"image_processor\"] = self.get_component(\"image_processor\")"
        }
    ],
    "stats": {
        "total": 270,
        "additions": 247,
        "deletions": 23
    }
}