{
    "author": "vasqu",
    "message": "[`Ernie 4.5 Moe`] Fix routing, weights, and update expectations (#42653)\n\n* fix\n\n* fix dtype casting\n\n* add \"small\" model test\n\n* update for a10\n\n* fix\n\n* crap forgot\n\n* proper decorator usage\n\n* add comment\n\n* style\n\n* fix expectation from dtype casting",
    "sha": "4e7cecb24d5ee9e41e488bebba69151faf6e3e4d",
    "files": [
        {
            "sha": "d8af9bb408291988abb880616d15d0dbc869d10c",
            "filename": "src/transformers/conversion_mapping.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e7cecb24d5ee9e41e488bebba69151faf6e3e4d/src%2Ftransformers%2Fconversion_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e7cecb24d5ee9e41e488bebba69151faf6e3e4d/src%2Ftransformers%2Fconversion_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconversion_mapping.py?ref=4e7cecb24d5ee9e41e488bebba69151faf6e3e4d",
            "patch": "@@ -166,6 +166,9 @@ def _build_checkpoint_conversion_mapping():\n     mapping[\"deepseek_v3\"] = mapping[\"qwen2_moe\"].copy()\n     mapping[\"dots1\"] = mapping[\"qwen2_moe\"].copy()\n     mapping[\"ernie4_5_moe\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"ernie4_5_moe\"] += [\n+        WeightRenaming(\"mlp.moe_statics.e_score_correction_bias\", \"mlp.gate.moe_statics.e_score_correction_bias\")\n+    ]\n     mapping[\"glm4_moe\"] = mapping[\"qwen2_moe\"].copy()\n     mapping[\"glm4v_moe\"] = mapping[\"qwen2_moe\"].copy()\n     mapping[\"longcat_flash\"] = mapping[\"qwen2_moe\"].copy()"
        },
        {
            "sha": "0134bbe61a5c9ec6ed0145f0dd4c49a32fe5429f",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e7cecb24d5ee9e41e488bebba69151faf6e3e4d/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e7cecb24d5ee9e41e488bebba69151faf6e3e4d/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=4e7cecb24d5ee9e41e488bebba69151faf6e3e4d",
            "patch": "@@ -373,14 +373,14 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n \n         with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             router_logits = F.linear(hidden_states.float(), self.weight)\n-            router_logits = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            router_top_value, router_indices = torch.topk(self.moe_statics(router_logits), self.top_k, dim=-1)\n-            router_top_value = router_top_value / torch.clamp(\n-                router_top_value.sum(dim=-1, keepdim=True), min=self.norm_min\n+            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+            _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n+            routing_weights = routing_weights / torch.clamp(\n+                routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n-            router_scores = router_top_value\n-        router_scores = router_scores.to(hidden_states.dtype)\n-        return router_logits, router_scores, router_indices\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+        return router_logits, selected_experts, routing_weights\n \n \n class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n@@ -403,7 +403,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         if self.shared_experts is not None:\n             shared_output = self.shared_experts(hidden_states)\n \n-        _, top_k_weights, top_k_index = self.gate(hidden_states)\n+        _, top_k_index, top_k_weights = self.gate(hidden_states)\n         final_hidden_states = self.experts(hidden_states, top_k_index, top_k_weights)\n \n         if self.shared_experts is not None:"
        },
        {
            "sha": "74d6e73ccb83b4b21885ec6b07f591bf5d6800be",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e7cecb24d5ee9e41e488bebba69151faf6e3e4d/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e7cecb24d5ee9e41e488bebba69151faf6e3e4d/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=4e7cecb24d5ee9e41e488bebba69151faf6e3e4d",
            "patch": "@@ -148,14 +148,14 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n \n         with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             router_logits = F.linear(hidden_states.float(), self.weight)\n-            router_logits = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            router_top_value, router_indices = torch.topk(self.moe_statics(router_logits), self.top_k, dim=-1)\n-            router_top_value = router_top_value / torch.clamp(\n-                router_top_value.sum(dim=-1, keepdim=True), min=self.norm_min\n+            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+            _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n+            routing_weights = routing_weights / torch.clamp(\n+                routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n-            router_scores = router_top_value\n-        router_scores = router_scores.to(hidden_states.dtype)\n-        return router_logits, router_scores, router_indices\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+        return router_logits, selected_experts, routing_weights\n \n \n class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n@@ -178,7 +178,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         if self.shared_experts is not None:\n             shared_output = self.shared_experts(hidden_states)\n \n-        _, top_k_weights, top_k_index = self.gate(hidden_states)\n+        _, top_k_index, top_k_weights = self.gate(hidden_states)\n         final_hidden_states = self.experts(hidden_states, top_k_index, top_k_weights)\n \n         if self.shared_experts is not None:"
        },
        {
            "sha": "390065040596f7f6a4cf87581641bd6cff9d54a9",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e7cecb24d5ee9e41e488bebba69151faf6e3e4d/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e7cecb24d5ee9e41e488bebba69151faf6e3e4d/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=4e7cecb24d5ee9e41e488bebba69151faf6e3e4d",
            "patch": "@@ -1091,17 +1091,20 @@ def require_torch_large_gpu(test_case, memory: float = 20):\n     )(test_case)\n \n \n-def require_torch_large_accelerator(test_case, memory: float = 20):\n+def require_torch_large_accelerator(test_case=None, *, memory: float = 20):\n     \"\"\"Decorator marking a test that requires an accelerator with more than `memory` GiB of memory.\"\"\"\n-    if torch_device != \"cuda\" and torch_device != \"xpu\":\n-        return unittest.skip(reason=f\"test requires a GPU or XPU with more than {memory} GiB of memory\")(test_case)\n \n-    torch_accelerator_module = getattr(torch, torch_device)\n+    def memory_decorator(tc):\n+        if torch_device not in (\"cuda\", \"xpu\"):\n+            return unittest.skip(f\"test requires a GPU or XPU with more than {memory} GiB of memory\")(tc)\n \n-    return unittest.skipUnless(\n-        torch_accelerator_module.get_device_properties(0).total_memory / 1024**3 > memory,\n-        f\"test requires a GPU or XPU with more than {memory} GiB of memory\",\n-    )(test_case)\n+        torch_accel = getattr(torch, torch_device)\n+        return unittest.skipUnless(\n+            torch_accel.get_device_properties(0).total_memory / 1024**3 > memory,\n+            f\"test requires a GPU or XPU with more than {memory} GiB of memory\",\n+        )(tc)\n+\n+    return memory_decorator if test_case is None else memory_decorator(test_case)\n \n \n def require_torch_accelerator(test_case):"
        },
        {
            "sha": "e5fb46ea5f221bcb91a063146f352c15ab3e74a9",
            "filename": "tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 43,
            "deletions": 15,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e7cecb24d5ee9e41e488bebba69151faf6e3e4d/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e7cecb24d5ee9e41e488bebba69151faf6e3e4d/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py?ref=4e7cecb24d5ee9e41e488bebba69151faf6e3e4d",
            "patch": "@@ -27,7 +27,6 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_large_accelerator,\n-    require_torch_multi_accelerator,\n     slow,\n     torch_device,\n )\n@@ -130,9 +129,7 @@ def test_load_balancing_loss(self):\n         self.assertNotAlmostEqual(include_padding_result.aux_loss.item(), result.aux_loss.item())\n \n \n-# Run on runners with larger accelerators (for example A10 instead of T4) with a lot of CPU RAM (e.g. g5-12xlarge)\n-@require_torch_multi_accelerator\n-@require_torch_large_accelerator\n+@slow\n @require_torch\n class Ernie4_5_MoeIntegrationTest(unittest.TestCase):\n     @classmethod\n@@ -144,27 +141,58 @@ def tearDownClass(cls):\n         del cls.model\n         cleanup(torch_device, gc_collect=True)\n \n+    def setup(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n     @classmethod\n-    def get_model(cls):\n-        if cls.model is None:\n-            cls.model = Ernie4_5_MoeForCausalLM.from_pretrained(\n-                \"baidu/ERNIE-4.5-21B-A3B-PT\",\n-                device_map=\"auto\",\n-                quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n-            )\n+    def get_large_model(cls):\n+        cls.model = Ernie4_5_MoeForCausalLM.from_pretrained(\n+            \"baidu/ERNIE-4.5-21B-A3B-PT\",\n+            device_map=\"auto\",\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+        )\n+\n+        return cls.model\n+\n+    @classmethod\n+    def get_small_model(cls):\n+        cls.model = Ernie4_5_MoeForCausalLM.from_pretrained(\n+            \"hf-internal-testing/ERNIE-4.5-Small-Moe\",\n+            device_map=\"auto\",\n+            dtype=\"auto\",\n+        )\n \n         return cls.model\n \n+    @require_torch_large_accelerator(memory=48)  # Tested on A100 but requires around 48GiB\n     @require_bitsandbytes\n-    @slow\n     def test_model_21b_a3b_generation(self):\n-        EXPECTED_TEXT_COMPLETION = \"User: Hey, are you conscious? Can you talk to me?\\nAssistant:  I don't have consciousness in the way humans do. I'm a text-based AI created to process and generate responses based on patterns in data.\"  # fmt: skip\n+        EXPECTED_TEXT_COMPLETION = \"User: Hey, are you conscious? Can you talk to me?\\nAssistant: \\nI don't have consciousness in the way humans do. I don't feel emotions, have thoughts, or experience awareness. However, I'm\"  # fmt: skip\n+\n+        model = self.get_large_model()\n+        tokenizer = AutoTokenizer.from_pretrained(\"baidu/ERNIE-4.5-21B-A3B-PT\")\n+        prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        messages = [{\"role\": \"user\", \"content\": prompt}]\n+        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+        model_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n+\n+        generated_ids = model.generate(\n+            model_inputs.input_ids,\n+            max_new_tokens=32,\n+            do_sample=False,\n+        )\n+        text = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip(\"\\n\")\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+\n+    def test_shortened_model_generation(self):\n+        # This is gibberish which is expected as the model are the first x layers of the original 28B model\n+        EXPECTED_TEXT_COMPLETION = 'User: Hey, are you conscious? Can you talk to me?\\nAssistant: 不了的 tongues说话 dagat绵席裹着头phones<mask:11>odikèkèk<mask:11><mask:11>bun褶席席地说起来这么说的话的话retti upside upsideolate疡疡疡'  # fmt: skip\n \n-        model = self.get_model()\n-        tokenizer = AutoTokenizer.from_pretrained(\"baidu/ERNIE-4.5-21B-A3B-PT\", revision=\"refs/pr/11\")\n+        model = self.get_small_model()\n+        tokenizer = AutoTokenizer.from_pretrained(\"baidu/ERNIE-4.5-21B-A3B-PT\")\n         prompt = \"Hey, are you conscious? Can you talk to me?\"\n         messages = [{\"role\": \"user\", \"content\": prompt}]\n         text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
        }
    ],
    "stats": {
        "total": 112,
        "additions": 73,
        "deletions": 39
    }
}