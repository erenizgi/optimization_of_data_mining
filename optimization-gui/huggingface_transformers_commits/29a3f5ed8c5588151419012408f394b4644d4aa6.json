{
    "author": "yao-matrix",
    "message": "switch default xpu tp backend to pytorch built-in XCCL from pytorch 2.8 (#39024)\n\n* switch default xpu tp backend to pytorch built-in XCCL from pytorch 2.8\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* Update docs/source/en/perf_infer_gpu_multi.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update perf_infer_gpu_multi.md\n\n* Update perf_infer_gpu_multi.md\n\n* Update perf_infer_gpu_multi.md\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "29a3f5ed8c5588151419012408f394b4644d4aa6",
    "files": [
        {
            "sha": "d8761befaac66b256ad5fe77c960c8f8b259dc13",
            "filename": "docs/source/en/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/29a3f5ed8c5588151419012408f394b4644d4aa6/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/29a3f5ed8c5588151419012408f394b4644d4aa6/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md?ref=29a3f5ed8c5588151419012408f394b4644d4aa6",
            "patch": "@@ -15,9 +15,9 @@ rendered properly in your Markdown viewer.\n \n # Distributed inference\n \n-When a model doesn't fit on a single GPU, distributed inference with [tensor parallelism](./perf_train_gpu_many#tensor-parallelism) can help. Tensor parallelism shards a model onto multiple GPUs and parallelizes computations such as matrix multiplication. It enables fitting larger model sizes into memory and is faster because each GPU can process a tensor slice.\n+When a model doesn't fit on a single GPU, distributed inference with [tensor parallelism](./perf_train_gpu_many#tensor-parallelism) can help. Tensor parallelism shards a model onto multiple accelerators (CUDA GPU, Intel XPU, etc.) and parallelizes computations such as matrix multiplication. It enables fitting larger model sizes into memory and is faster because each accelerator can process a tensor slice.\n \n-However, tensor parallelism adds communication overhead and should be used on single machine setups with multiple GPUs to take advantage of fast intra-node communication. For multi-node training, it may be more efficient to use pipeline or data parallelism depending on your use case.\n+However, tensor parallelism adds communication overhead and should be used on single machine setups with multiple accelerators to take advantage of fast intra-node communication. For multi-node training, it may be more efficient to use pipeline or data parallelism depending on your use case.\n \n > [!TIP]\n > Refer to the [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism) section on tensor parallelism to learn more.\n@@ -308,4 +308,4 @@ The most important part of DTensor is the `placement` attribute because it tells\n     bias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Replicate()]) # Replicate bias across all GPUs\n     ```\n \n-- `Partial()` - Indicates a tensor is pending a reduction operation (not typically relevant for usage in Transformers).\n\\ No newline at end of file\n+- `Partial()` - Indicates a tensor is pending a reduction operation (not typically relevant for usage in Transformers)."
        },
        {
            "sha": "0ffce54977f4843ec19f4b9159be3b89d86b873b",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/29a3f5ed8c5588151419012408f394b4644d4aa6/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29a3f5ed8c5588151419012408f394b4644d4aa6/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=29a3f5ed8c5588151419012408f394b4644d4aa6",
            "patch": "@@ -57,10 +57,12 @@ def initialize_tensor_parallelism(tp_plan, tp_size=None):\n             local_rank = int(os.environ[\"LOCAL_RANK\"])\n             world_size = int(os.environ[\"WORLD_SIZE\"])\n \n-            backend_map = {\"cuda\": \"nccl\", \"cpu\": \"gloo\", \"xpu\": \"ccl\", \"hpu\": \"hccl\"}\n+            backend_map = {\"cuda\": \"nccl\", \"cpu\": \"gloo\", \"xpu\": \"xccl\", \"hpu\": \"hccl\"}\n             backend = backend_map.get(device_type)\n             if device_type == \"cpu\" and int(os.environ.get(\"CCL_WORKER_COUNT\", 0)):\n                 backend = \"ccl\"\n+            if device_type == \"xpu\" and not is_torch_greater_or_equal(\"2.8\", accept_dev=True):\n+                backend = \"ccl\"\n \n             torch.distributed.init_process_group(backend=backend, rank=rank, world_size=world_size)\n             current_device = getattr(torch, device_type)"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 6,
        "deletions": 4
    }
}