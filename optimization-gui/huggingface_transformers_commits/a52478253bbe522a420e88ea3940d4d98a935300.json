{
    "author": "stevhliu",
    "message": "[docs] Tensor parallelism (#38241)\n\n* updates\n\n* feedback\n\n* badges\n\n* fix?\n\n* fix?\n\n* fix?\n\n* fix?",
    "sha": "a52478253bbe522a420e88ea3940d4d98a935300",
    "files": [
        {
            "sha": "f569a09e588fd8e745cd0967b261793b5832f7bb",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -97,7 +97,7 @@\n     - local: perf_infer_gpu_one\n       title: GPU\n     - local: perf_infer_gpu_multi\n-      title: Distributed GPU inference\n+      title: Distributed inference\n     - local: perf_infer_cpu\n       title: CPU\n     - local: tf_xla"
        },
        {
            "sha": "08087b14c461f8055d745c35f9cc1be345dc2c25",
            "filename": "docs/source/en/model_doc/cohere.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -3,6 +3,7 @@\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n         <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n         <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n     </div>\n </div>\n "
        },
        {
            "sha": "24f649666395fdc40b7be684cb9784456a31055a",
            "filename": "docs/source/en/model_doc/cohere2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -4,6 +4,7 @@\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "63e4d0409fde9112873e1088a1177dd072624568",
            "filename": "docs/source/en/model_doc/gemma.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -23,6 +23,7 @@ rendered properly in your Markdown viewer.\n         \">\n         <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n         <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n     </div>\n </div>\n "
        },
        {
            "sha": "84f11b1eb24f3611415d62d29e958fdb76cf114b",
            "filename": "docs/source/en/model_doc/gemma2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -22,6 +22,7 @@ rendered properly in your Markdown viewer.\n         \">\n         <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n         <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n     </div>\n </div>\n "
        },
        {
            "sha": "4a1618459b04ec0edd938c998816cd3734ad45f2",
            "filename": "docs/source/en/model_doc/glm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -20,6 +20,7 @@ rendered properly in your Markdown viewer.\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "bdc71c2997ae3e26db16d8ffc36802a1593d670a",
            "filename": "docs/source/en/model_doc/granite.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -19,6 +19,7 @@ rendered properly in your Markdown viewer.\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n </div>\n \n # Granite"
        },
        {
            "sha": "183775bcadbcbfda3c391b6a34c3f3e8d8dbb4ab",
            "filename": "docs/source/en/model_doc/llama.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -21,6 +21,7 @@ rendered properly in your Markdown viewer.\n         \">\n         <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n         <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n     </div>\n </div>\n "
        },
        {
            "sha": "a2e697e89d1b7f27cc52971d9c15c48de902d5a6",
            "filename": "docs/source/en/model_doc/llama2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -19,6 +19,7 @@ rendered properly in your Markdown viewer.\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n         <img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n         \">\n+        <img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n     </div>\n </div>\n "
        },
        {
            "sha": "ab5c4862c49c77882f58caa8ac722d5f327325f3",
            "filename": "docs/source/en/model_doc/llama3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama3.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -20,6 +20,7 @@ rendered properly in your Markdown viewer.\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n \">\n+<img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n </div>\n \n ```py3"
        },
        {
            "sha": "07f0919fba3ba0a99137b063da487c11a5b00853",
            "filename": "docs/source/en/model_doc/llama4.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -21,6 +21,7 @@ rendered properly in your Markdown viewer.\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n         <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n     </div>\n </div>\n "
        },
        {
            "sha": "f41a486dbbe8e7846f7cba566c33da80b8ccd928",
            "filename": "docs/source/en/model_doc/mistral.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -22,6 +22,7 @@ rendered properly in your Markdown viewer.\n         \">\n         <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n         <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n     </div>\n </div>\n "
        },
        {
            "sha": "e0688f35befcc72714d93b3c7d748f045a99b3b4",
            "filename": "docs/source/en/model_doc/mixtral.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -20,6 +20,7 @@ rendered properly in your Markdown viewer.\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "efa56ce0af8d212e30836b4d219da911404e1aac",
            "filename": "docs/source/en/model_doc/olmo.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -20,6 +20,7 @@ rendered properly in your Markdown viewer.\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "10f53eb583e85a697b8b84831eb19e28e747df89",
            "filename": "docs/source/en/model_doc/phi.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -18,6 +18,7 @@ rendered properly in your Markdown viewer.\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n         <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n         <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n     </div>\n </div>\n "
        },
        {
            "sha": "77444d7955b5941d4b3fd21f23768e265be55f84",
            "filename": "docs/source/en/model_doc/phi3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi3.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -20,6 +20,7 @@ rendered properly in your Markdown viewer.\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "899d9dddf594e965ad45eb0e4529d688adf9dd81",
            "filename": "docs/source/en/model_doc/qwen2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -19,6 +19,7 @@ rendered properly in your Markdown viewer.\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n         <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n         <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n     </div>\n </div>\n "
        },
        {
            "sha": "b25ff9b7a3bbea6cfe73379fc8f23e84f16a1f02",
            "filename": "docs/source/en/model_doc/qwen2_moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -18,6 +18,7 @@ rendered properly in your Markdown viewer.\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n </div>\n \n # Qwen2MoE"
        },
        {
            "sha": "926cb5bc4ddff99b01395372af07056e6dddfda5",
            "filename": "docs/source/en/model_doc/qwen2_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -19,6 +19,7 @@ rendered properly in your Markdown viewer.\n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "ecb405f4d216b930aba95d58f891793b21b04e2d",
            "filename": "docs/source/en/model_doc/starcoder2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -20,6 +20,7 @@ rendered properly in your Markdown viewer.\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "f269960d3fc6a5e6756ab6b38f42663ae9455806",
            "filename": "docs/source/en/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 190,
            "deletions": 208,
            "changes": 398,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -13,21 +13,19 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Tensor parallelism in transformers\n+# Distributed inference\n \n-[Tensor parallelism](./perf_train_gpu_many#tensor-parallelism) shards a model onto multiple GPUs and parallelizes computations such as matrix multiplication. It enables fitting larger model sizes into memory and is faster because each GPU can process a tensor slice.\n-This document assumes that you are already familiar with the basics of tensor parallelism. If you are not, please refer to the [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism) section on tensor parallelism.\n+When a model doesn't fit on a single GPU, distributed inference with [tensor parallelism](./perf_train_gpu_many#tensor-parallelism) can help. Tensor parallelism shards a model onto multiple GPUs and parallelizes computations such as matrix multiplication. It enables fitting larger model sizes into memory and is faster because each GPU can process a tensor slice.\n \n-> [!TIP]\n-> Tensor parallelism is very communication intensive, therefore it is reccomended to use it on a single machine with multiple GPUs, utilizing fast intra-node communication. For multi-node training, methods as pipeline or data parallelism are more efficient (depending on your use case).\n-\n-Tensor parallelism requires slight changes to the model parameters, therefore in transformers, we support some of the popular models out of the box.\n+However, tensor parallelism adds communication overhead and should be used on single machine setups with multiple GPUs to take advantage of fast intra-node communication. For multi-node training, it may be more efficient to use pipeline or data parallelism depending on your use case.\n \n > [!TIP]\n-> Expand the list below to see which models support tensor parallelism. Open a GitHub issue or pull request to add support for a model not currently below.\n+> Refer to the [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism) section on tensor parallelism to learn more.\n+\n+Check the list below for models that natively support tensor parallelism. Open a GitHub issue or pull request to add support for a model.\n \n <details>\n-<summary>Supported models</summary>\n+<summary>Show supported models</summary>\n \n * [Cohere](./model_doc/cohere) and [Cohere 2](./model_doc/cohere2)\n * [Gemma](./model_doc/gemma) and [Gemma 2](./model_doc/gemma2)\n@@ -43,72 +41,52 @@ Tensor parallelism requires slight changes to the model parameters, therefore in\n \n </details>\n \n-## Using ðŸ¤— transformers\n+This guide shows how to enable tensor parallelism with Transformers and different partitioning strategies.\n \n-Transformers provides a simple interface to use for tensor parallelism. We provide multiple classes implementing different partitioning\n-strategies and a simple entrypoint to parallelize `nn.Module` instance. You won't have to interact with this interface directly, everything is done in `PretrainedModel.from_pretrained` method for you. This section will first talk about the partitioning strategies\n-we support, then the user interface you will be interacting with, and finally it will teach you how to extend it with your own partitioning\n-strategies.\n+## Partitioning a model\n \n-### Partitioning strategies\n+Transformers supports tensor parallelism if a model has a `tp_plan`. There are two plans to partition a model.\n \n-In transformers, partitioning strategies reside in a class `ParallelInterface` which works like a mapping from string to the strategy implementation.\n+- The `auto` tensor parallelism plan partitions a model (see the supported models above) based on a predefined configuration.\n+- You can also manually specify your own partitioning plan and pass it to the `tp_plan` parameter in [`~PreTrainedModel.from_pretrained`].\n \n+<hfoptions id=\"sharding\">\n+<hfoption id=\"auto plan\">\n \n-```python\n-class ParallelInterface(MutableMapping):\n-    \"\"\"\n-    Dict-like object keeping track of allowed attention functions. You can easily add a new attention function\n-    with a call to `register()`. If a model needs to locally overwrite an existing attention function, say `sdpa`,\n-    it needs to declare a new instance of this class inside the `modeling_<model>.py`, and declare it on that instance.\n-    \"\"\"\n-    _global_mapping = {\n-        \"colwise\": ColwiseParallel(),\n-        \"rowwise\": RowwiseParallel(),\n-        \"colwise_rep\": ColwiseParallel(output_layouts=Replicate()),\n-        \"rowwise_rep\": RowwiseParallel(input_layouts=Replicate()),\n-        \"local_colwise\": ColwiseParallel(use_dtensor=False),\n-        \"local_rowwise\": RowwiseParallel(use_dtensor=False),\n-        \"local\": IsolatedParallel(),\n-        \"gather\": GatherParallel(),\n-        \"local_packed_rowwise\": PackedRowwiseParallel(use_dtensor=False),\n-        \"sequence_parallel\": SequenceParallel(),\n-        \"replicate\": ReplicateParallel(),\n-    }\n-```\n-\n-We support the following strategies:\n-\n-- `ColwiseParallel` - A simple column-wise partitioning, being able to handle both weights and biases, does exactly what we've discussed before.\n-- `RowwiseParallel` - Again, row-wise partitioning as dicussed before, supports weights and biases, on top of that it also supports `nn.Embedding` modules.\n-- `SequenceParallel` - Sequence parallel implementation, for support of `LayerNorm` and `Dropout` layers. Also supports Python implementation of `RMSNorm` (see [this](https://github.com/facebookresearch/llama/blob/main/llama/model.py#L34))\n-- `PackedColwiseParallel` - A variant of column-wise partitioning, however it works on packed weights (i.e. `up_proj` and `gate_proj` being packed together). For more details, see [this comment](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108)\n-- `PackedRowwiseParallel` - A variant of row-wise partitioning, works on packed weights, for more details check the comment linked above.\n-- `GatherParallel` - A very simple class, that only makes the outputs of the module to be gathered across devices.\n-- `IsolatedParallel` - This is a special case, where we want to *isolate* the module from the rest of the devices (world). This is used for Experts in MoE layers, basically creating Expert parallelism of sorts.\n-- `ReplicateParallel` - Many `torch.distributed` APIs break if model is partially sharded, so this class is used to replicate the module across all devices.\n+```py\n+import os\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-### Sharding a model\n+# model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" # better to visualize all the possible strategies\n+model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # better for smaller number of GPUs\n \n-We provide two ways to shard a model, first one is to use `auto` tensor parallelism plan, which will automatically shard the model based on our predefined configuration. This requires the model to have predefined tensor parallel plan in transformers.\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=\"auto\")\n+print(model._tp_plan)\n \n-```python\n-from transformers import AutoModelForCausalLM\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n+prompt = \"Can I help\"\n+inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n \n-# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\" # better for smaller number of GPUs\n-model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" # better to visualize all the possible strategies\n+# distributed run\n+outputs = model(inputs)\n+```\n \n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=\"auto\")\n+Launch the inference script above on [torchrun](https://pytorch.org/docs/stable/elastic/run.html) with 4 processes per GPU.\n \n-print(model._tp_plan)\n+```bash\n+torchrun --nproc-per-node 4 demo.py\n ```\n \n-> [!TIP]\n-> For a list of models that support tensor parallelism, see the [Supported models](#supported-models) section above.\n+</hfoption>\n+<hfoption id=\"manual plan\">\n \n-The second way is to manually specify your own partitioning plan.\n+Define a tensor parallel plan for each layer in `tp_plan` and pass it to [`~PreTrainedModel.from_pretrained`]. The example below uses a combination of column and row partitioning. Refer to the [Partitioning strategies](#partitioning-strategies) section to learn about other supported partitioning strategies.\n \n-```python\n+> [!WARNING]\n+> Manually specifying your own partitioning plan requires a good understanding of the model architecture and how the partitioning strategies interact together. If you are not sure about the partitioning strategies, the resulting model can be very slow, even failing or incorrect. Refer to the [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism) to learn more.\n+\n+```py\n from transformers import AutoModelForCausalLM\n \n tp_plan = {\n@@ -120,23 +98,64 @@ tp_plan = {\n }\n \n model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=tp_plan)\n-\n print(model._tp_plan)\n ```\n \n-You might have noticed that there are some special cases in the `ParallelInterface` mapping, let's now talk about them. This will help you understand their purpose and help with extending to other strategies.\n+</hfoption>\n+</hfoptions>\n+\n+## Partitioning strategies\n+\n+All partitioning strategies are defined in the [`ParallelInterface`] class which maps a string to the strategy implementation. You don't need to interact with this class directly since all the strategies are set with `tp_plan` in [`~PreTrainedModel.from_pretrained`], but it is useful for checking what strategies are available.\n+\n+```py\n+class ParallelInterface(MutableMapping):\n+    \"\"\"\n+    Dict-like object keeping track of allowed attention functions. You can easily add a new attention function\n+    with a call to `register()`. If a model needs to locally overwrite an existing attention function, say `sdpa`,\n+    it needs to declare a new instance of this class inside the `modeling_<model>.py`, and declare it on that instance.\n+    \"\"\"\n+    _global_mapping = {\n+        \"colwise\": ColwiseParallel(),\n+        \"rowwise\": RowwiseParallel(),\n+        \"colwise_rep\": ColwiseParallel(output_layouts=Replicate()),\n+        \"rowwise_rep\": RowwiseParallel(input_layouts=Replicate()),\n+        \"local_colwise\": ColwiseParallel(use_dtensor=False),\n+        \"local_rowwise\": RowwiseParallel(use_dtensor=False),\n+        \"local\": IsolatedParallel(),\n+        \"gather\": GatherParallel(),\n+        \"local_packed_rowwise\": PackedRowwiseParallel(use_dtensor=False),\n+        \"sequence_parallel\": SequenceParallel(),\n+        \"replicate\": ReplicateParallel(),\n+    }\n+```\n+\n+Refer to the table below to learn more about each strategy.\n \n-### PackedRowwiseParallel\n-This class is a special case of `RowwiseParallel`, it's used to shard packed weights. Weight packing is a common technique used in models. It's a technique where we pack multiple linear layers into a single, bigger one.\n+| Strategy | Description |\n+|---|---|\n+| `ColwiseParallel` | Column-wise partitioning of weights and biases. |\n+| `RowwiseParallel` | Row-wise partitioning of weights and biases. Also supports partitioning `nn.Embedding` modules. |\n+| `SequenceParallel` | Sequence parallel implementation to support `LayerNorm` and `Dropout` layers. Also supports Python implementation of [RMSNorm](https://github.com/facebookresearch/llama/blob/main/llama/model.py#L34). |\n+| `PackedColwiseParallel` | Variant of `ColwiseParallel` to support packed weights (for example, packing `up_proj` and `gate_proj` together). Refer to the [code](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108) for more details. |\n+| `PackedRowwiseParallel` | Variant of `RowwiseParallel` to support packed weights (refer to the [code](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108) for more details). |\n+| `GatherParallel` | Gather outputs of the module across devices. |\n+| `IsolatedParallel` | Used for Experts in Mixture-of-Experts (MoE) layers to isolates module from other devices. |\n+| `ReplicateParallel` | Replicate modules across all devices to prevent `torch.distributed` APIs from breaking due to a partially sharded model. |\n+\n+### Packed strategies\n+\n+Weight packing packs multiple linear layers into a single, bigger layer. Packed strategies, `PackedColwiseParallel` and `PackedRowwiseParallel`, are used to shard packed weights. The more basic `ColwiseParallel` or `RowwiseParallel` will incorrectly shard the packed weights.\n+\n+The example below packs `up_proj` and `gate_proj` into a single `gate_up_proj` module and requires the `PackedRowwiseParallel` strategy to shard `gate_up_proj`.\n \n-For example in `Llama4` model, we pack `up_proj` and `gate_proj` into a single `gate_up_proj` module.\n ```python\n class Llama4TextExperts(nn.Module):\n     ...\n     self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n ```\n \n-Then in forward, we can use batch matrix multiplication to compute the output of the `gate_up_proj` module.\n+Batch matrix multiplication can be used in the `forward` pass to compute the output of the `gate_up_proj` module.\n \n ```python\n def forward(self, hidden_states):\n@@ -145,185 +164,148 @@ def forward(self, hidden_states):\n     gate, up = gate_up.chunk(2, dim=-1) # Split the output into gate and up\n ```\n \n-In this case, we need to use the `PackedRowwiseParallel` strategy to shard the `gate_up_proj` module, as using a simple `RowwiseParallel` will shard the layers wrongly.\n-\n > [!TIP]\n-> If this is a bit difficult to wrap your head around, check out [this comment](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108) for an amazing visual representation of why `Packed*` needs to be used.\n-\n+> Refer to [this comment](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108) for an visual representation of why `Packed*` needs to be used.\n \n-### `local*` strategies\n+### Local strategies\n \n-You could have noticed that there are `local*` strategies, which use the same layers as `*` strategy, but don't use `DTensor` at all.\n-This is because `DTensor` is not supported for some of the operations: such as `torch.chunk`. Therefore, sometimes we need to use the `local*` strategies, which use vanilla `torch.Tensor` and do some of the distributed logic manually.\n+Local strategies (`local_colwise`, `local_rowwise`, `local_packed_rowwise`) don't use [DTensor](https://docs.pytorch.org/docs/stable/distributed.tensor.html) because it isn't supported for some operations such as [torch.chunk](https://docs.pytorch.org/docs/stable/generated/torch.chunk.html). Instead, local strategies use the basic [torch.Tensor](https://docs.pytorch.org/docs/stable/tensors.html) and performs some of the distributed logic manually.\n \n-<!---\n+<!--\n Readd this when I get the exact error message\n > [!TIP]\n > If you are using a custom partitioning strategy, and it's not working with `... is not supported` error, try using the `local*` strategies to see if they work better.\n -->\n \n-> [!WARNING]\n-> Manually specifying your own partitiong plan requires a good understanding of the model architecture and how the partitioning strategies interact together. If you are not sure about this, the resulting model can be very slow, even failing or incorrect. Again, refer to the [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism) which can teach you everything required.\n+## Custom partitioning strategies\n \n-### Extending the interface with your own partitioning strategies\n+A custom partitioning strategy should inherit from [`TensorParallelLayer`](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py) and implement `partition_tensor`, `_prepare_input_fn` and `_prepare_output_fn`.\n \n-This is a very advanced topic, which requires a good understanding of distributed collectives and the model architecture.\n-Your custom partitioning strategy should inherit from `TensorParallelLayer` defined in [integrations/tensor_parallel.py](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py) and implement: `partition_tensor`, `_prepare_input_fn` and `_prepare_output_fn`. Then it should be registered in the `ParallelInterface` mapping, so our dispatching logic can find it when specified in the `tp_plan`.\n+Then it needs to be registered in the `ParallelInterface` mapping so the dispatching logic can find it when specified in `tp_plan`.\n \n-Let's go through this workflow step by step, on an already existing example: `ColwiseParallel`.\n+The example below shows how to implement `ColwiseParallel` with this workflow.\n \n-1. Inherit from `TensorParallelLayer` and initialization\n+1. Inherit from `TensorParallelLayer`. In the `__init__` method, define `input_layouts` and `output_layouts` to describe how the input and output tensors should be placed on devices. The `desired_input_layouts` attribute is used to specify how the input *should* be placed on devices.\n \n-```python\n-class ColwiseParallel(TensorParallelLayer):\n-    def __init__(\n-        self,\n-        *,\n-        input_layouts: Optional[Placement] = None, # The input layout coming from the previous layer\n-        output_layouts: Optional[Placement] = None, # The output layout we want to achieve\n-        use_local_output: bool = True, # Whether to use local output or not\n-        use_dtensor=True, # Whether to use DTensor or not\n-    ):\n-        self.input_layouts = (input_layouts or Replicate(),) # The input sharding coming from the previous layer\n-        self.output_layouts = (output_layouts or Shard(-1),) # Desired output sharding\n-        self.desired_input_layouts = (Replicate(),) # Desired input sharding, inputs should be replicated across GPUs\n-        self.use_local_output = use_local_output\n-        self.use_dtensor = use_dtensor\n-```\n+    ```python\n+    class ColwiseParallel(TensorParallelLayer):\n+        def __init__(\n+            self,\n+            *,\n+            input_layouts: Optional[Placement] = None, # The input layout coming from the previous layer\n+            output_layouts: Optional[Placement] = None, # The output layout we want to achieve\n+            use_local_output: bool = True, # Whether to use local output or not\n+            use_dtensor=True, # Whether to use DTensor or not\n+        ):\n+            self.input_layouts = (input_layouts or Replicate(),) # The input sharding coming from the previous layer\n+            self.output_layouts = (output_layouts or Shard(-1),) # Desired output sharding\n+            self.desired_input_layouts = (Replicate(),) # Desired input sharding, inputs should be replicated across GPUs\n+            self.use_local_output = use_local_output\n+            self.use_dtensor = use_dtensor\n+    ```\n \n-In the `__init__` method, we define these attributes, where `input_layouts` and `output_layouts` describing, how the input and output tensors should be placed on the devices. `desired_input_layouts` is used to specify, how the input *SHOULD* be placed on the devices.\n+2. Implement the `partition_tensor`, `_prepare_input_fn` and `_prepare_output_fn` methods.\n \n-2a. Implement `partition_tensor` method\n+    The `partition_tensor` method partitions the tensor and fills `empty_param` with the partitioned tensor. Use the utility function `get_tensor_shard` to help you get the correct shard of the original parameter for a given rank and `get_packed_weights` to help with packed weights.\n \n-```python\n-def partition_tensor(\n-    self,\n-    param, # Full tensor of the parameter\n-    empty_param, # Empty tensor of the parameter, will be filled with the partitioned tensor\n-    param_type, # Type of the parameter, `bias` or `weight`\n-    param_casting_dtype, # The type to cast the parameter to\n-    to_contiguous, # Whether to convert the tensor to a contiguous memory layout\n-    rank, # The rank of the current device\n-    device_mesh, # The device mesh\n-) -> nn.Parameter: # Return the partitioned parameter\n-    ...\n-```\n-\n-This method is used to partition the tensor, and fill the `empty_param` with the partitioned tensor.\n-We provide some utility functions to help you with this, such as `get_tensor_shard` which will get you the correct shard of the original parameter for this rank or `get_packed_weights` to help with packed weights.\n-\n-2b. Implement `_prepare_input_fn` and `_prepare_output_fn` methods\n-\n-These methods are used as [`pre-forward`](https://docs.pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_pre_hook.html) and [`forward`](https://docs.pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html) hooks respectively. Their purpose is to re-distribute the inputs and outputs to the desired layout, passed in the `__init__` method.\n-\n-```python\n-def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n-    ...\n-    # Do some custom logic, cast to DTensor etc.\n-    ...\n-    return inputs.redistribute(placements=desired_input_layouts, device_mesh=device_mesh)\n-\n-def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n-    ...\n-    # Do some custom logic, cast to DTensor etc.\n-    ...\n-    return outputs.redistribute(placements=output_layouts, device_mesh=device_mesh)\n-```\n-\n-3. Register the strategy\n-Congratulations! You've implemented your own partitioning strategy. Now, to use it with your own `tp_plan`, you need to register it in the `ParallelInterface` mapping.\n-\n-```python\n-from transformers.integrations.tensor_parallel import ParallelInterface\n-\n-ParallelInterface.register_strategy(\"colwise_custom\", ColwiseParallel)\n-```\n-\n-And now you can use it in your `tp_plan` as such:\n-\n-```python\n-tp_plan = {\n-    \"model.layers.*.self_attn.q_proj\": \"colwise_custom\",\n-    ...\n-}\n-\n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=tp_plan)\n-```\n-\n-\n-## Full example\n-\n-Let's go through a full example of inference with tensor parallelism.\n-```python\n-import os\n-import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-\n-# enable tensor parallelism\n-model = AutoModelForCausalLM.from_pretrained(\n-    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n-    tp_plan=\"auto\",\n-)\n-\n-# prepare input tokens\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n-prompt = \"Can I help\"\n-inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n-\n-# distributed run\n-outputs = model(inputs)\n-```\n-\n-Launch the inference script above on [torchrun](https://pytorch.org/docs/stable/elastic/run.html) with 4 processes per GPU.\n+    ```python\n+    def partition_tensor(\n+        self,\n+        param, # Full tensor of the parameter\n+        empty_param, # Empty tensor of the parameter, will be filled with the partitioned tensor\n+        param_type, # Type of the parameter, `bias` or `weight`\n+        param_casting_dtype, # The type to cast the parameter to\n+        to_contiguous, # Whether to convert the tensor to a contiguous memory layout\n+        rank, # The rank of the current device\n+        device_mesh, # The device mesh\n+    ) -> nn.Parameter: # Return the partitioned parameter\n+        ...\n+    ```\n+\n+    The `_prepare_input_fn` and `_prepare_output_fn` methods are used in the [pre-forward](https://docs.pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_pre_hook.html) and [forward](https://docs.pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html) hooks. They redistribute the inputs and outputs to the desired layout as specified in the `__init__`.\n+\n+    ```python\n+    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n+        ...\n+        # Do some custom logic, cast to DTensor etc.\n+        ...\n+        return inputs.redistribute(placements=desired_input_layouts, device_mesh=device_mesh)\n+    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n+        ...\n+        # Do some custom logic, cast to DTensor etc.\n+        ...\n+        return outputs.redistribute(placements=output_layouts, device_mesh=device_mesh)\n+    ```\n+\n+3. Register the strategy to [`ParallelInterface`] to enable it for use with `tp_plan`.\n+\n+    ```python\n+    from transformers.integrations.tensor_parallel import ParallelInterface\n+\n+    ParallelInterface.register_strategy(\"colwise_custom\", ColwiseParallel)\n+    tp_plan = {\n+        \"model.layers.*.self_attn.q_proj\": \"colwise_custom\",\n+        ...\n+    }\n+    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=tp_plan)\n+    ```\n \n-```bash\n-torchrun --nproc-per-node 4 demo.py\n-```\n+## Benchmarks\n \n-You can benefit from considerable speed ups for inference, especially for inputs with large batch size or long sequences.\n+Tensor parallelism can considerably speedup inference, especially for inputs with large batch sizes or long sequences.\n \n-For a single forward pass on [Llama](./model_doc/llama) with a sequence length of 512 and various batch sizes, you can expect the following speed ups.\n+Refer to the chart below for the expected speedup for a single forward pass on [Llama](./model_doc/llama) with a sequence length of 512.\n \n <div style=\"text-align: center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Meta-Llama-3-8B-Instruct%2C%20seqlen%20%3D%20512%2C%20python%2C%20w_%20compile.png\">\n </div>\n \n-## Tensor parallelism in-depth\n-Our implementation of tensor parallelism is framework-agnostic in design, but the specific implementations we've developed rely on the torch.distributed package. We heavily utilize abstractions such as `DeviceMesh` or `DTensor` to provide a simple and extensible interface to the user.\n+## Design implementation\n+\n+The Transformers tensor parallelism implementation is framework-agnostic, but for specific implementations, we rely on [DeviceMesh](https://docs.pytorch.org/tutorials/recipes/distributed_device_mesh.html) and [DTensor](https://docs.pytorch.org/docs/stable/distributed.tensor.html) from [torch.distributed](https://docs.pytorch.org/tutorials/beginner/dist_overview.html) to provide a simple and extensible interface.\n \n ### DeviceMesh\n-Imagine `DeviceMesh` as a multi-dimensional grid of devices that communicate together. Different parallelization strategies require different types of communication patterns, therefore we can create a `DeviceMesh` with multiple submeshes:\n+\n+Imagine `DeviceMesh` as a multi-dimensional grid of devices that communicate together. Different parallelization strategies require different types of communication patterns, so you can create a `DeviceMesh` with multiple sub-meshes.\n+\n ```python\n from torch.distributed.device_mesh import init_device_mesh\n \n # Create a 1D mesh of 4 GPUs\n device_mesh = init_device_mesh(\"cuda\", (4,), mesh_dim_names=[\"tp\"])\n ```\n-Then, most of the `torch.distributed` defined parallelization strategies can be applied to a mesh itself, or its submesh, automatically handling the communication patterns.\n+\n+Most of the `torch.distributed` defined parallelization strategies can be applied to the mesh itself, or its sub-mesh, and it automatically handles the communication patterns.\n \n ### DTensor\n \n-Abbreviation for Distributed Tensor, `DTensor` is a tensor subclass that handles the distributed logic on-top of the usual tensor operations. Most of the model weights in case of tensor parallelism are stored as `DTensor`s (with some exceptions, more on that later).\n-The most important part of DTensor, that is crucial to understand, is the `placement` attribute. It's an attribute that tells PyTorch how is the tensor placed on the devices of the `DeviceMesh`.\n+`DTensor` (Distributed Tensor) is a tensor subclass that handles the distributed logic on top of the usual tensor operations. Most of the model weights in tensor parallelism are stored as `DTensor`s.\n \n-It can have the following values:\n+The most important part of DTensor is the `placement` attribute because it tells PyTorch how a tensor is placed on the devices in `DeviceMesh`. The `placement` attribute can take the following values.\n \n-- `Shard(dimension)` - Annotates that this `DTensor` is sharded across a given dimension, over the `DeviceMesh` it was constructed under. For example, if we would like to shard weights for column-wise partitioning, we would do:\n-```python\n-weight = ...\n-weight = DTensor.from_local(weight, device_mesh[\"tp\"], placements=[Shard(0)]) # Shard across the 1st (column-wise) dimension\n-bias = ...\n-bias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Shard(-1)]) # Shard across the ONLY dimension\n-```\n+- `Shard(dimension)` - Indicates how a `DTensor` is sharded across a given dimension, over the `DeviceMesh` it was constructed under. The example below demonstrates how to shard weights over different dimensions for column-wise partitioning.\n \n-To give another example, for row-wise partitioning, we would do:\n-```python\n-weight = ...\n-weight = DTensor.from_local(weight, device_mesh[\"tp\"], placements=[Shard(1)]) # Shard across the 2nd (row-wise) dimension\n-bias = ...\n-bias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Replicate()]) # Replicate bias across all GPUs\n-```\n+    ```python\n+    weight = ...\n+    weight = DTensor.from_local(weight, device_mesh[\"tp\"], placements=[Shard(0)]) # Shard across the 1st (column-wise) dimension\n+    bias = ...\n+    bias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Shard(-1)]) # Shard across the ONLY dimension\n+    ```\n+\n+    This example demonstrates how to shard weights over different dimensions for row-wise partitioning.\n+\n+    ```python\n+    weight = ...\n+    weight = DTensor.from_local(weight, device_mesh[\"tp\"], placements=[Shard(1)]) # Shard across the 2nd (row-wise) dimension\n+    bias = ...\n+    bias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Replicate()]) # Replicate bias across all GPUs\n+    ```\n+\n+- `Replicate()` - Indicates a `DTensor` is replicated across the `DeviceMesh`. It only creates a full copy of the tensor on each device.\n+\n+    ```py\n+    bias = ...\n+    bias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Replicate()]) # Replicate bias across all GPUs\n+    ```\n \n-- `Replicate()` - Annotates that this `DTensor` is replicated across the `DeviceMesh`. Very straight-forward, only creates a full copy of the tensor on each device.\n-- `Partial()` - This placement is mostly of no interest to us, it's used to annotate that this tensor is pending a reduction operation.\n+- `Partial()` - Indicates a tensor is pending a reduction operation (not typically relevant for usage in Transformers).\n\\ No newline at end of file"
        },
        {
            "sha": "7fdbb9d8afe391071fc9a3c85d35108b1a4446e5",
            "filename": "docs/source/en/perf_train_gpu_many.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a52478253bbe522a420e88ea3940d4d98a935300/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md?ref=a52478253bbe522a420e88ea3940d4d98a935300",
            "patch": "@@ -91,6 +91,8 @@ Tensor parallelism distributes large tensor computations across multiple GPUs. T\n \n Tensor parallelism is effective for training large models that don't fit into the memory of a single GPU. It is also faster and more efficient because each GPU can process its tensor slice in parallel, and it can be combined with other parallelism methods. Like other parallelism methods though, tensor parallelism adds communication overhead between GPUs.\n \n+Refer to the [Tensor parallelism](./perf_infer_gpu_multi) guide to learn how to use it for inference.\n+\n ## Hybrid parallelism\n \n Parallelism methods can be combined to achieve even greater memory savings and more efficiently train models with billions of parameters."
        }
    ],
    "stats": {
        "total": 421,
        "additions": 212,
        "deletions": 209
    }
}