{
    "author": "zucchini-nlp",
    "message": "a few more breaking changes from raushan",
    "sha": "f77950605e2f2f27613b829559b133c2b9fd8912",
    "files": [
        {
            "sha": "c38be2363905cd59e057bb3ec7f0042e170a6956",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f77950605e2f2f27613b829559b133c2b9fd8912/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f77950605e2f2f27613b829559b133c2b9fd8912/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=f77950605e2f2f27613b829559b133c2b9fd8912",
            "patch": "@@ -127,6 +127,8 @@ model_4bit = AutoModelForCausalLM.from_pretrained(\n \n - Methods to init a nested config such as `from_xxx_config` are deleted. Configs can be init from the `__init__` method in the same way. See [#41314](https://github.com/huggingface/transformers/pull/41314).\n - It is no longer possible to load a config class from a URL file. Configs must be loaded from either a local path or a repo on the Hub. See [#42383](https://github.com/huggingface/transformers/pull/42383).\n+- All parameters for configuring model's rotary embedding are now stored under `mode.rope_parameters`, including the `rope_theta` and `rope_type`. Model's `config.rope_parameters` is a simple dictionaty in most cases, and can also be a nested dict in special cases (i.e. Gemma3 and ModernBert) with different rope parameterization for each layer type. See [#39847](https://github.com/huggingface/transformers/pull/39847)\n+\n \n ## Processing\n \n@@ -154,16 +156,19 @@ Linked PRs:\n - `XXXFeatureExtractors` classes are completely removed in favor of `XXXImageProcessor` class for all vision models (https://github.com/huggingface/transformers/pull/41174)\n - Minor change: `XXXFastImageProcessorKwargs` is removed in favor of `XXXImageProcessorKwargs` which will be shared between fast and slow processors (https://github.com/huggingface/transformers/pull/40931)\n \n+\n ## Modeling\n \n - Some `RotaryEmbeddings` layers will start returning a dict of tuples, in case the model uses several RoPE configurations (Gemma2, ModernBert). Each value will be a tuple of \"cos, sin\" per RoPE type.\n - Config attribute for `RotaryEmbeddings` layer will be unified and accessed via `config.rope_parameters`. Config attr for `rope_theta` might not be accessible anymore for some models, and instead will be in `config.rope_parameters['rope_theta']`. BC will be supported for a while as much as possible, and in the near future we'll gradually move to the new RoPE format  (https://github.com/huggingface/transformers/pull/39847)\n+- Vision Language models will not have a shortcut access to its language and vision component from the generative model via `model.language_model`. It is recommended to either access the module with `model.model.language_model` or `model.get_decoder()`. See [#42156](https://github.com/huggingface/transformers/pull/42156/)\n \n ### Generate\n \n - Old, deprecated output type aliases were removed (e.g. `GreedySearchEncoderDecoderOutput`). We now only have 4 output classes built from the following matrix: decoder-only vs encoder-decoder, uses beams vs doesn't use beams (https://github.com/huggingface/transformers/pull/40998)\n - Removed deprecated classes regarding decoding methods that were moved to the Hub due to low usage (constraints and beam scores) (https://github.com/huggingface/transformers/pull/41223)\n - If `generate` doesn't receive any KV Cache argument, the default cache class used is now defined by the model (as opposed to always being `DynamicCache`) (https://github.com/huggingface/transformers/pull/41505)\n+- Generation parameters are no longer accessible via model's config. If generation paramaters are serialized in `config.json` for any old model, it will be loaded back into model's generation config. Users are expected to access or modify generation parameters only with `model.generation_config.do_sample = True`. \n \n ## Trainer\n \n@@ -209,6 +214,10 @@ Linked PRs:\n \n - `use_cache` in the model config will be set to `False`. You can still change the cache value through `TrainingArguments` `usel_cache` argument if needed. \n \n+## Pipeline\n+\n+- Image text to text pipelines will no longer accept images as a separate argument along with conversation chats. Image data has to be embedded in the chat's \"content\" field. See [#42359](https://github.com/huggingface/transformers/pull/42359)\n+\n ## PushToHubMixin\n \n - removed deprecated `organization` and `repo_url` from `PushToHubMixin`. You must pass a `repo_id` instead."
        }
    ],
    "stats": {
        "total": 9,
        "additions": 9,
        "deletions": 0
    }
}