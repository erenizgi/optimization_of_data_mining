{
    "author": "LysandreJik",
    "message": "Imports",
    "sha": "afc0c04d802e988d09001a25dc5d956128fe0166",
    "files": [
        {
            "sha": "75dfb29f5f30ddb687a5b355436c542c8b8250bc",
            "filename": "src/transformers/cli/serve.py",
            "status": "modified",
            "additions": 18,
            "deletions": 15,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/afc0c04d802e988d09001a25dc5d956128fe0166/src%2Ftransformers%2Fcli%2Fserve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afc0c04d802e988d09001a25dc5d956128fe0166/src%2Ftransformers%2Fcli%2Fserve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fserve.py?ref=afc0c04d802e988d09001a25dc5d956128fe0166",
            "patch": "@@ -11,6 +11,8 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from __future__ import annotations\n+\n import asyncio\n import base64\n import copy\n@@ -34,7 +36,6 @@\n import typer\n from huggingface_hub import model_info\n from huggingface_hub.constants import HF_HUB_OFFLINE\n-from openai.types.chat.chat_completion import Choice\n from tokenizers.decoders import DecodeStream\n \n import transformers\n@@ -52,7 +53,6 @@\n     LogitsProcessorList,\n     TextIteratorStreamer,\n )\n-from ..generation.continuous_batching import ContinuousBatchingManager, RequestStatus\n from ..utils import logging\n \n \n@@ -63,6 +63,8 @@\n         ProcessorMixin,\n     )\n \n+    from ..generation.continuous_batching import ContinuousBatchingManager\n+\n \n if is_librosa_available():\n     import librosa\n@@ -81,6 +83,7 @@\n     from openai.types.audio.transcription import Transcription\n     from openai.types.audio.transcription_create_params import TranscriptionCreateParamsBase\n     from openai.types.chat import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageParam\n+    from openai.types.chat.chat_completion import Choice\n     from openai.types.chat.chat_completion_chunk import (\n         ChatCompletionChunk,\n         ChoiceDelta,\n@@ -234,9 +237,9 @@ class Modality(enum.Enum):\n \n def create_generation_config_from_req(\n     req: dict,\n-    model_generation_config: \"GenerationConfig\",\n+    model_generation_config: GenerationConfig,\n     **kwargs,\n-) -> \"GenerationConfig\":\n+) -> GenerationConfig:\n     \"\"\"\n     Creates a generation config from the parameters of the request. If a generation config is passed in the request,\n     it will be used as a baseline for parameterization. Otherwise, we will use the model's default generation config.\n@@ -313,9 +316,9 @@ class TimedModel:\n \n     def __init__(\n         self,\n-        model: \"PreTrainedModel\",\n+        model: PreTrainedModel,\n         timeout_seconds: int,\n-        processor: Union[\"ProcessorMixin\", \"PreTrainedTokenizerFast\"] | None = None,\n+        processor: Union[ProcessorMixin, PreTrainedTokenizerFast] | None = None,\n     ):\n         self.model = model\n         self._name_or_path = str(model.name_or_path)\n@@ -471,7 +474,7 @@ def __init__(\n             self.load_model_and_processor(model_id_and_revision)\n \n         @asynccontextmanager\n-        async def lifespan(app: \"FastAPI\"):\n+        async def lifespan(app: FastAPI):\n             yield\n             for model in self.loaded_models.values():\n                 model.delete_model()\n@@ -585,7 +588,7 @@ def _validate_request(\n         self,\n         request: dict,\n         schema: TypedDict,\n-        validator: \"TypeAdapter\",\n+        validator: TypeAdapter,\n         unused_fields: set,\n     ):\n         \"\"\"\n@@ -661,7 +664,7 @@ def build_chat_completion_chunk(\n         model: str | None = None,\n         role: str | None = None,\n         finish_reason: str | None = None,\n-        tool_calls: list[\"ChoiceDeltaToolCall\"] | None = None,\n+        tool_calls: list[ChoiceDeltaToolCall] | None = None,\n         decode_stream: DecodeStream | None = None,\n         tokenizer: PreTrainedTokenizerFast | None = None,\n     ) -> ChatCompletionChunk:\n@@ -825,6 +828,8 @@ def continuous_batching_chat_completion(self, req: dict, request_id: str) -> Str\n         )[\"input_ids\"][0]\n \n         def stream_chat_completion(request_id, decode_stream):\n+            from ..generation.continuous_batching import RequestStatus\n+\n             try:\n                 # Emit the assistant role to start the stream. Other chunks won't have a role, as it is implicit\n                 # they come from the assistant.\n@@ -909,7 +914,7 @@ def cancellation_wrapper_buffer(_request_id):\n             return JSONResponse(json_chunk, media_type=\"application/json\")\n \n     @staticmethod\n-    def get_model_modality(model: \"PreTrainedModel\") -> Modality:\n+    def get_model_modality(model: PreTrainedModel) -> Modality:\n         from transformers.models.auto.modeling_auto import (\n             MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n             MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES,\n@@ -1688,7 +1693,7 @@ def is_continuation(self, req: dict) -> bool:\n         self.last_messages = messages\n         return req_continues_last_messages\n \n-    def get_quantization_config(self) -> Optional[\"BitsAndBytesConfig\"]:\n+    def get_quantization_config(self) -> Optional[BitsAndBytesConfig]:\n         \"\"\"\n         Returns the quantization config for the given CLI arguments.\n \n@@ -1787,9 +1792,7 @@ def _load_model_and_data_processor(self, model_id_and_revision: str):\n         logger.info(f\"Loaded model {model_id_and_revision}\")\n         return model, data_processor\n \n-    def load_model_and_processor(\n-        self, model_id_and_revision: str\n-    ) -> tuple[\"PreTrainedModel\", \"PreTrainedTokenizerFast\"]:\n+    def load_model_and_processor(self, model_id_and_revision: str) -> tuple[PreTrainedModel, PreTrainedTokenizerFast]:\n         \"\"\"\n         Loads the text model and processor from the given model ID and revision into the ServeCommand instance.\n \n@@ -1814,7 +1817,7 @@ def load_model_and_processor(\n \n         return model, processor\n \n-    def load_audio_model_and_processor(self, model_id_and_revision: str) -> tuple[\"PreTrainedModel\", \"ProcessorMixin\"]:\n+    def load_audio_model_and_processor(self, model_id_and_revision: str) -> tuple[PreTrainedModel, ProcessorMixin]:\n         \"\"\"\n         Loads the audio model and processor from the given model ID and revision into the ServeCommand instance.\n "
        },
        {
            "sha": "4369b4d84424b060cd1ecf9bc1f89fc012414607",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/afc0c04d802e988d09001a25dc5d956128fe0166/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afc0c04d802e988d09001a25dc5d956128fe0166/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=afc0c04d802e988d09001a25dc5d956128fe0166",
            "patch": "@@ -33,6 +33,7 @@\n from types import ModuleType\n from typing import Any\n \n+import packaging.version\n from packaging import version\n \n from . import logging\n@@ -92,10 +93,14 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> tuple[\n \n @lru_cache\n def is_torch_available() -> bool:\n-    is_available, torch_version = _is_package_available(\"torch\", return_version=True)\n-    if is_available and version.parse(torch_version) < version.parse(\"2.2.0\"):\n-        logger.warning_once(f\"Disabling PyTorch because PyTorch >= 2.2 is required but found {torch_version}\")\n-    return is_available and version.parse(torch_version) >= version.parse(\"2.2.0\")\n+    try:\n+        is_available, torch_version = _is_package_available(\"torch\", return_version=True)\n+        parsed_version = version.parse(torch_version)\n+        if is_available and parsed_version < version.parse(\"2.2.0\"):\n+            logger.warning_once(f\"Disabling PyTorch because PyTorch >= 2.2 is required but found {torch_version}\")\n+        return is_available and version.parse(torch_version) >= version.parse(\"2.2.0\")\n+    except packaging.version.InvalidVersion:\n+        return False\n \n \n @lru_cache"
        }
    ],
    "stats": {
        "total": 46,
        "additions": 27,
        "deletions": 19
    }
}