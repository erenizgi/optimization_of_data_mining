{
    "author": "zucchini-nlp",
    "message": "Paligemma: fix static cache test (#33941)\n\n* fix\r\n\r\n* not flaky anymore + style",
    "sha": "612065efeba61b9ee9f45e80aa4c1368d6d43934",
    "files": [
        {
            "sha": "51349ecf4ecfc874ab66f4bdd4a5d3ea56ceeabc",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/612065efeba61b9ee9f45e80aa4c1368d6d43934/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/612065efeba61b9ee9f45e80aa4c1368d6d43934/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=612065efeba61b9ee9f45e80aa4c1368d6d43934",
            "patch": "@@ -881,9 +881,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You must specify exactly one of input_ids or inputs_embeds\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "49cdd274162092d7d54e7609879fc52beed42b61",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/612065efeba61b9ee9f45e80aa4c1368d6d43934/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/612065efeba61b9ee9f45e80aa4c1368d6d43934/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=612065efeba61b9ee9f45e80aa4c1368d6d43934",
            "patch": "@@ -758,9 +758,7 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You must specify exactly one of input_ids or inputs_embeds\"\n-            )\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once("
        },
        {
            "sha": "5e695f3387d768795dcd804cb18c137e4af1b7eb",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/612065efeba61b9ee9f45e80aa4c1368d6d43934/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/612065efeba61b9ee9f45e80aa4c1368d6d43934/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=612065efeba61b9ee9f45e80aa4c1368d6d43934",
            "patch": "@@ -57,8 +57,8 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     min_dtype: float,\n     cache_position: torch.Tensor,\n     batch_size: int,\n-    is_training: bool,\n-    token_type_ids: torch.Tensor,\n+    is_training: bool = False,\n+    token_type_ids: torch.Tensor = None,\n ):\n     \"\"\"\n     Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n@@ -94,7 +94,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             if is_training:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n             else:\n-                causal_mask = torch.zeros_like(causal_mask)\n+                causal_mask[:, :sequence_length] = 0.0\n \n         causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n         causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n@@ -378,7 +378,7 @@ def _update_causal_mask(\n             if is_training:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n             else:\n-                causal_mask = torch.zeros_like(causal_mask)\n+                causal_mask[:, :sequence_length] = 0.0\n \n         causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n         causal_mask = causal_mask[None, None, :, :].expand(inputs_embeds.shape[0], 1, -1, -1)\n@@ -593,7 +593,6 @@ def prepare_inputs_for_generation(\n \n             dtype = self.get_output_embeddings().weight.dtype\n             min_dtype = torch.finfo(dtype).min\n-            is_training = token_type_ids is not None and kwargs.get(\"labels\", None) is not None\n \n             model_inputs[\"attention_mask\"] = _prepare_4d_causal_attention_mask_with_cache_position(\n                 attention_mask,\n@@ -604,8 +603,6 @@ def prepare_inputs_for_generation(\n                 min_dtype=min_dtype,\n                 cache_position=cache_position,\n                 batch_size=batch_size,\n-                is_training=is_training,\n-                token_type_ids=token_type_ids,\n             )\n \n         model_inputs[\"token_type_ids\"] = token_type_ids"
        },
        {
            "sha": "7d72226e41b2b38f67ea0835f5fefc73da967c8f",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/612065efeba61b9ee9f45e80aa4c1368d6d43934/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/612065efeba61b9ee9f45e80aa4c1368d6d43934/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=612065efeba61b9ee9f45e80aa4c1368d6d43934",
            "patch": "@@ -159,7 +159,8 @@ def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         config, pixel_values = config_and_inputs\n         input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n-        attention_mask = input_ids.ne(1).to(torch_device)\n+        attention_mask = input_ids.ne(self.pad_token_id).to(torch_device)\n+\n         # set the 16 first tokens to be image, and ensure that no other tokens are image tokens\n         # do not change this unless you modified image size or patch size\n         input_ids[input_ids == config.image_token_index] = self.pad_token_id"
        },
        {
            "sha": "6d40359f91703faead28d21a358e950502bc0f1e",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/612065efeba61b9ee9f45e80aa4c1368d6d43934/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/612065efeba61b9ee9f45e80aa4c1368d6d43934/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=612065efeba61b9ee9f45e80aa4c1368d6d43934",
            "patch": "@@ -4868,7 +4868,6 @@ def test_custom_4d_attention_mask(self):\n             normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n             torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n \n-    @is_flaky(max_attempts=10)  # TODO @raushan: this test is VERY flaky on some VLMs, like paligemma\n     def test_static_cache_matches_dynamic(self):\n         \"\"\"\n         Tests that generating with static cache give almost same results as with dynamic cache."
        }
    ],
    "stats": {
        "total": 23,
        "additions": 8,
        "deletions": 15
    }
}