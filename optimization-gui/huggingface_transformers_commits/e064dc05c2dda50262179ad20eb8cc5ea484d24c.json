{
    "author": "ydshieh",
    "message": "[testing] Fix `JetMoeIntegrationTest` (#41377)\n\n* fix\n\n* update\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "e064dc05c2dda50262179ad20eb8cc5ea484d24c",
    "files": [
        {
            "sha": "d189d11144f414ee92055347306ee31922ea13c8",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 19,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/e064dc05c2dda50262179ad20eb8cc5ea484d24c/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e064dc05c2dda50262179ad20eb8cc5ea484d24c/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=e064dc05c2dda50262179ad20eb8cc5ea484d24c",
            "patch": "@@ -13,14 +13,13 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch JetMoe model.\"\"\"\n \n-import gc\n import unittest\n \n import pytest\n \n from transformers import AutoTokenizer, is_torch_available\n from transformers.testing_utils import (\n-    backend_empty_cache,\n+    cleanup,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n@@ -127,41 +126,39 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n @require_torch\n class JetMoeIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     @slow\n     def test_model_8b_logits(self):\n         input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n-        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\")\n+        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         with torch.no_grad():\n             out = model(input_ids).logits.float().cpu()\n         # Expected mean on dim = -1\n-        EXPECTED_MEAN = torch.tensor([[0.2507, -2.7073, -1.3445, -1.9363, -1.7216, -1.7370, -1.9054, -1.9792]])\n+        EXPECTED_MEAN = torch.tensor([[0.1943, -2.7299, -1.3466, -1.9385, -1.7457, -1.7472, -1.8647, -1.8547]])\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n         # slicing logits[0, 0, 0:30]\n-        EXPECTED_SLICE = torch.tensor([-3.3689,  5.9006,  5.7450, -1.7012, -4.7072, -4.7071, -4.7071, -4.7071, -4.7072, -4.7072, -4.7072, -4.7071,  3.8321,  9.1746, -4.7071, -4.7072, -4.7071, -4.7072, -4.7071, -4.7072, -4.7071, -4.7071, -4.7071, -4.7071, -4.7071, -4.7071, -4.7071, -4.7071, -4.7071, -4.7071])  # fmt: skip\n+        EXPECTED_SLICE = torch.tensor([-3.4844, 6.0625, 5.8750, -1.6875, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, 3.8750, 9.3750, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812, -4.7812])  # fmt: skip\n         torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n \n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n-\n     @slow\n     def test_model_8b_generation(self):\n         EXPECTED_TEXT_COMPLETION = \"\"\"My favourite condiment is ....\\nI love ketchup. I love\"\"\"\n         prompt = \"My favourite condiment is \"\n         tokenizer = AutoTokenizer.from_pretrained(\"jetmoe/jetmoe-8b\", use_fast=False)\n-        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\")\n+        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n \n         # greedy generation outputs\n         generated_ids = model.generate(input_ids, max_new_tokens=10, temperature=0)\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n-\n     @slow\n     def test_model_8b_batched_generation(self):\n         EXPECTED_TEXT_COMPLETION = [\n@@ -173,14 +170,10 @@ def test_model_8b_batched_generation(self):\n             \"My favourite \",\n         ]\n         tokenizer = AutoTokenizer.from_pretrained(\"jetmoe/jetmoe-8b\", use_fast=False)\n-        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\")\n+        model = JetMoeForCausalLM.from_pretrained(\"jetmoe/jetmoe-8b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n         input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.model.embed_tokens.weight.device)\n \n         # greedy generation outputs\n         generated_ids = model.generate(**input_ids, max_new_tokens=10, temperature=0)\n         text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n-\n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()"
        }
    ],
    "stats": {
        "total": 31,
        "additions": 12,
        "deletions": 19
    }
}