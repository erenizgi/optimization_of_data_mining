{
    "author": "jla524",
    "message": "Update docs for `sdpa_kernel` (#35410)\n\nupdate: sdp_kernel -> sdpa_kernel",
    "sha": "b5f97977ed762b0a3eb36337eab12d10f0df8891",
    "files": [
        {
            "sha": "364141c8e406b2b35231207410a67612aa5bff75",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b5f97977ed762b0a3eb36337eab12d10f0df8891/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b5f97977ed762b0a3eb36337eab12d10f0df8891/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=b5f97977ed762b0a3eb36337eab12d10f0df8891",
            "patch": "@@ -332,10 +332,11 @@ In that case, you should see a warning message and we will fall back to the (slo\n \n </Tip>\n \n-By default, SDPA selects the most performant kernel available but you can check whether a backend is available in a given setting (hardware, problem size) with [`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel) as a context manager:\n+By default, SDPA selects the most performant kernel available but you can check whether a backend is available in a given setting (hardware, problem size) with [`torch.nn.attention.sdpa_kernel`](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) as a context manager:\n \n ```diff\n import torch\n++ from torch.nn.attention import SDPBackend, sdpa_kernel\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n@@ -344,7 +345,7 @@ model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=to\n input_text = \"Hello my dog is cute and\"\n inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n \n-+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n++ with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n     outputs = model.generate(**inputs)\n \n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n@@ -518,6 +519,7 @@ It is often possible to combine several of the optimization techniques described\n \n ```py\n import torch\n+from torch.nn.attention import SDPBackend, sdpa_kernel\n from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n \n # load model in 4-bit\n@@ -536,7 +538,7 @@ input_text = \"Hello my dog is cute and\"\n inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n \n # enable FlashAttention\n-with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n+with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n     outputs = model.generate(**inputs)\n \n print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 5,
        "deletions": 3
    }
}