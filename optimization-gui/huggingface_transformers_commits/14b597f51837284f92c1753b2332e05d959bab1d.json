{
    "author": "SunMarc",
    "message": "Fix casting dtype for qunatization (#36799)\n\n* fix\n\n* remove print",
    "sha": "14b597f51837284f92c1753b2332e05d959bab1d",
    "files": [
        {
            "sha": "4158c82b4094d7f9f1b4f7d1ba6999b99791a1a0",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/14b597f51837284f92c1753b2332e05d959bab1d/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14b597f51837284f92c1753b2332e05d959bab1d/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=14b597f51837284f92c1753b2332e05d959bab1d",
            "patch": "@@ -732,6 +732,8 @@ def _infer_parameter_dtype(\n         if keep_in_fp32_modules is not None and keep_in_fp32_modules.search(param_name):\n             casting_dtype = torch.float32\n         # Then dtype that was instantiated in the meta model -- note that this respects subconfigs dtypes\n+        elif hf_quantizer is not None:\n+            casting_dtype = model.config._pre_quantization_dtype\n         else:\n             casting_dtype = old_param.dtype\n     return old_param is not None and old_param.is_contiguous(), casting_dtype\n@@ -754,7 +756,6 @@ def _load_state_dict_into_meta_model(\n     keep_in_fp32_modules: Optional[List[str]] = None,\n     unexpected_keys: Optional[List[str]] = None,  # passing `unexpected` for cleanup from quantization items\n     device_mesh: Optional[torch.distributed.device_mesh.DeviceMesh] = None,\n-    weights_only=True,\n ) -> Tuple[Optional[Dict], Optional[Dict]]:\n     \"\"\"Load parameters from `meta_state_dict` into the model. The parameters of the `meta_state_dict` are on the meta\n     device in order to easily infer the shapes and dtypes that they will have. Then proper parameters are then loaded\n@@ -4883,7 +4884,6 @@ def _load_pretrained_model(\n                         keep_in_fp32_modules=keep_in_fp32_modules,\n                         unexpected_keys=unexpected_keys,\n                         device_mesh=device_mesh,\n-                        weights_only=weights_only,\n                     )\n             else:\n                 assign_params = check_support_param_buffer_assignment(model_to_load, state_dict)"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}