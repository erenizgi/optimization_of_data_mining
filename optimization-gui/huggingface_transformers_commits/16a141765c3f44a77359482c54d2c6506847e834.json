{
    "author": "stevhliu",
    "message": "[docs] Fix tp_plan (#41205)\n\nremove manual",
    "sha": "16a141765c3f44a77359482c54d2c6506847e834",
    "files": [
        {
            "sha": "21d1817e302b1619c4535a37b63c8109527bc5d2",
            "filename": "docs/source/en/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 1,
            "deletions": 33,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/16a141765c3f44a77359482c54d2c6506847e834/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/16a141765c3f44a77359482c54d2c6506847e834/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md?ref=16a141765c3f44a77359482c54d2c6506847e834",
            "patch": "@@ -45,13 +45,7 @@ This guide shows how to enable tensor parallelism with Transformers and differen\n \n ## Partitioning a model\n \n-Transformers supports tensor parallelism if a model has a `tp_plan`. There are two plans to partition a model.\n-\n-- The `auto` tensor parallelism plan partitions a model (see the supported models above) based on a predefined configuration.\n-- You can also manually specify your own partitioning plan and pass it to the `tp_plan` parameter in [`~PreTrainedModel.from_pretrained`].\n-\n-<hfoptions id=\"sharding\">\n-<hfoption id=\"auto plan\">\n+Transformers supports tensor parallelism if a model has a `tp_plan`. Set `tp_plan=\"auto\"` to automatically use a tensor parallelism plan based on a model's predefined configuration.\n \n ```py\n import os\n@@ -78,32 +72,6 @@ Launch the inference script above on [torchrun](https://pytorch.org/docs/stable/\n torchrun --nproc-per-node 4 demo.py\n ```\n \n-</hfoption>\n-<hfoption id=\"manual plan\">\n-\n-Define a tensor parallel plan for each layer in `tp_plan` and pass it to [`~PreTrainedModel.from_pretrained`]. The example below uses a combination of column and row partitioning. Refer to the [Partitioning strategies](#partitioning-strategies) section to learn about other supported partitioning strategies.\n-\n-> [!WARNING]\n-> Manually specifying your own partitioning plan requires a good understanding of the model architecture and how the partitioning strategies interact together. If you are not sure about the partitioning strategies, the resulting model can be very slow, even failing or incorrect. Refer to the [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism) to learn more.\n-\n-```py\n-from transformers import AutoModelForCausalLM\n-\n-tp_plan = {\n-    \"model.layers.*.self_attn.q_proj\": \"colwise\",\n-    \"model.layers.*.self_attn.k_proj\": \"colwise\",\n-    \"model.layers.*.self_attn.v_proj\": \"colwise\",\n-    \"model.layers.*.self_attn.o_proj\": \"rowwise\",\n-    ...\n-}\n-\n-model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, tp_plan=tp_plan)\n-print(model._tp_plan)\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n ## Partitioning strategies\n \n All partitioning strategies are defined in the [`ParallelInterface`] class which maps a string to the strategy implementation. You don't need to interact with this class directly since all the strategies are set with `tp_plan` in [`~PreTrainedModel.from_pretrained`], but it is useful for checking what strategies are available."
        }
    ],
    "stats": {
        "total": 34,
        "additions": 1,
        "deletions": 33
    }
}