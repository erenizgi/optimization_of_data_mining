{
    "author": "MekkCyber",
    "message": "[quantization] Dequant fp8 when cuda or xpu not available (#42511)\n\n* up\n\n* style\n\n* add tests\n\n* update",
    "sha": "afd039d1caa1a11dcc03d21bc0bb5c114bf927c2",
    "files": [
        {
            "sha": "16950dfe0d6fa214e420aaa326d9641abb4a54f3",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 34,
            "deletions": 28,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/afd039d1caa1a11dcc03d21bc0bb5c114bf927c2/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afd039d1caa1a11dcc03d21bc0bb5c114bf927c2/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=afd039d1caa1a11dcc03d21bc0bb5c114bf927c2",
            "patch": "@@ -14,8 +14,7 @@\n # limitations under the License.\n \n import re\n-from collections.abc import Sequence\n-from typing import Any\n+from typing import Optional\n \n from ..core_model_loading import ConversionOps\n from ..utils import is_accelerate_available, is_torch_accelerator_available, is_torch_available, logging\n@@ -549,6 +548,9 @@ def replace_with_fp8_linear(\n     quantization_config=None,\n ):\n     \"\"\"Helper function to replace model layers with FP8 versions.\"\"\"\n+    if quantization_config.dequantize:\n+        return model\n+\n     if modules_to_not_convert is None:\n         modules_to_not_convert = []\n     modules_to_not_convert += [\"lm_head\"]\n@@ -652,41 +654,45 @@ def convert(self, input_dict: torch.Tensor, **kwargs) -> dict[str, torch.Tensor]\n class Fp8Dequantize(ConversionOps):\n     \"\"\"Inverse operation of :class:`Fp8Quantize`. Takes a pair (weight, scale) and reconstructs the fp32 tensor.\"\"\"\n \n-    def __init__(self, block_size: tuple[int, int] | None = None):\n-        self.block_size = block_size\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n \n     def convert(\n         self,\n-        value: Sequence[torch.Tensor] | dict[str, torch.Tensor],\n-        *,\n-        context: dict[str, Any],\n-    ) -> torch.Tensor:\n-        if isinstance(value, dict):\n-            tensors = list(value.values())\n-        else:\n-            tensors = list(value) if isinstance(value, Sequence) else [value]\n-        if len(tensors) != 2:\n-            raise ValueError(\"Fp8Dequantize expects exactly two tensors: quantized weights and scales.\")\n-        quantized, scales = tensors\n-        if not isinstance(quantized, torch.Tensor) or not isinstance(scales, torch.Tensor):\n-            raise TypeError(\"Fp8Dequantize expects tensors as inputs.\")\n-\n-        quantized_fp32 = quantized.to(torch.float32)\n-        rows, cols = quantized_fp32.shape[-2:]\n-        block_size = self.block_size\n-        if block_size is None:\n-            quant_config = context.get(\"quantization_config\")\n-            block_size = getattr(quant_config, \"weight_block_size\", None)\n-        if block_size is None:\n-            block_size = (rows, cols)\n+        input_dict: dict[str, torch.Tensor],\n+        model: Optional[torch.nn.Module] = None,\n+        full_layer_name: str | None = None,\n+        missing_keys=None,\n+        **kwargs,\n+    ) -> dict[str, torch.Tensor]:\n+        if len(input_dict) != 2:\n+            # in case of no scales, the weights are not quantized, so we return the weights as is\n+            return {\n+                full_layer_name: input_dict[\"weight$\"][0]\n+                if isinstance(input_dict[\"weight$\"], list)\n+                else input_dict[\"weight$\"]\n+            }\n+        quantized = input_dict[\"weight$\"][0] if isinstance(input_dict[\"weight$\"], list) else input_dict[\"weight$\"]\n+        scales = (\n+            input_dict[\"weight_scale_inv\"][0]\n+            if isinstance(input_dict[\"weight_scale_inv\"], list)\n+            else input_dict[\"weight_scale_inv\"]\n+        )\n+\n+        rows, cols = quantized.shape[-2:]\n+        block_size = self.hf_quantizer.quantization_config.weight_block_size\n+\n         block_m, block_n = block_size\n         if rows % block_m != 0 or cols % block_n != 0:\n             raise ValueError(\n                 f\"Matrix dimensions ({rows}, {cols}) must be divisible by block sizes ({block_m}, {block_n}).\"\n             )\n \n-        reshaped = quantized_fp32.reshape(-1, rows // block_m, block_m, cols // block_n, block_n)\n+        reshaped = quantized.reshape(-1, rows // block_m, block_m, cols // block_n, block_n)\n         expanded_scales = scales.to(torch.float32).reshape(-1, rows // block_m, cols // block_n)\n         expanded_scales = expanded_scales.unsqueeze(-1).unsqueeze(2)\n         dequantized = reshaped * expanded_scales\n-        return dequantized.reshape(quantized_fp32.shape)\n+\n+        return {\n+            full_layer_name: dequantized.reshape(quantized.shape),\n+        }"
        },
        {
            "sha": "c61f0231a436dacf328b5d5df30c8d095e52f2d2",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/afd039d1caa1a11dcc03d21bc0bb5c114bf927c2/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afd039d1caa1a11dcc03d21bc0bb5c114bf927c2/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=afd039d1caa1a11dcc03d21bc0bb5c114bf927c2",
            "patch": "@@ -224,7 +224,15 @@ def merge_quantization_configs(\n         if (\n             isinstance(\n                 quantization_config,\n-                (GPTQConfig, AwqConfig, AutoRoundConfig, FbgemmFp8Config, CompressedTensorsConfig, Mxfp4Config),\n+                (\n+                    GPTQConfig,\n+                    AwqConfig,\n+                    AutoRoundConfig,\n+                    FbgemmFp8Config,\n+                    CompressedTensorsConfig,\n+                    Mxfp4Config,\n+                    FineGrainedFP8Config,\n+                ),\n             )\n             and quantization_config_from_args is not None\n         ):\n@@ -234,7 +242,7 @@ def merge_quantization_configs(\n \n             warning_msg += f\"However, loading attributes (e.g. {list(loading_attr_dict.keys())}) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\"\n \n-        if warning_msg != \"\" and not isinstance(quantization_config, Mxfp4Config):\n+        if warning_msg != \"\" and not isinstance(quantization_config, (Mxfp4Config, FineGrainedFP8Config)):\n             warnings.warn(warning_msg)\n         else:\n             # in the case of mxfp4, we don't want to print the warning message, bit confusing for users"
        },
        {
            "sha": "5cef7130b2853ef2444593ba8d1194010204edae",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 24,
            "deletions": 2,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/afd039d1caa1a11dcc03d21bc0bb5c114bf927c2/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afd039d1caa1a11dcc03d21bc0bb5c114bf927c2/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=afd039d1caa1a11dcc03d21bc0bb5c114bf927c2",
            "patch": "@@ -38,8 +38,15 @@ def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n             raise ImportError(\"Loading an FP8 quantized model requires accelerate (`pip install accelerate`)\")\n \n-        if not (torch.cuda.is_available() or is_torch_xpu_available()):\n-            raise RuntimeError(\"No GPU or XPU found. A GPU or XPU is needed for FP8 quantization.\")\n+        if (not (torch.cuda.is_available() or is_torch_xpu_available())) and not self.quantization_config.dequantize:\n+            if self.pre_quantized:\n+                logger.warning_once(\n+                    \"Using FP8 quantized models requires a GPU or XPU, we will default to dequantizing the model to bf16 since no GPU or XPU is available\"\n+                )\n+                self.quantization_config.dequantize = True\n+                return\n+            else:\n+                raise RuntimeError(\"No GPU or XPU found. A GPU or XPU is needed for FP8 quantization.\")\n \n         if torch.cuda.is_available():\n             compute_capability = torch.cuda.get_device_capability()\n@@ -231,3 +238,18 @@ def get_quantize_ops(self):\n         from ..integrations.finegrained_fp8 import Fp8Quantize\n \n         return Fp8Quantize(self)\n+\n+    def get_weight_conversions(self):\n+        from ..core_model_loading import WeightConverter\n+        from ..integrations.finegrained_fp8 import Fp8Dequantize\n+\n+        if self.pre_quantized and self.quantization_config.dequantize:\n+            return [\n+                # either use the dollar sign, or permute the source patterns to start matching against the scales first\n+                WeightConverter(\n+                    source_patterns=[\"weight$\", \"weight_scale_inv\"],\n+                    target_patterns=\"weight\",\n+                    operations=[Fp8Dequantize(self)],\n+                )\n+            ]\n+        return []"
        },
        {
            "sha": "08774ace200171a7e232604623ed4b37cb3d7d89",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/afd039d1caa1a11dcc03d21bc0bb5c114bf927c2/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afd039d1caa1a11dcc03d21bc0bb5c114bf927c2/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=afd039d1caa1a11dcc03d21bc0bb5c114bf927c2",
            "patch": "@@ -1981,6 +1981,8 @@ class FineGrainedFP8Config(QuantizationConfigMixin):\n             The scheme used for activation, the defaults and only support scheme for now is \"dynamic\".\n         weight_block_size (`typing.tuple[int, int]`, *optional*, defaults to `(128, 128)`):\n             The size of the weight blocks for quantization, default is (128, 128).\n+        dequantize (`bool`, *optional*, defaults to `False`):\n+            Whether to dequantize the model during loading.\n         modules_to_not_convert (`list`, *optional*):\n             A list of module names that should not be converted during quantization.\n     \"\"\"\n@@ -1989,13 +1991,15 @@ def __init__(\n         self,\n         activation_scheme: str = \"dynamic\",\n         weight_block_size: tuple[int, int] = (128, 128),\n+        dequantize: bool = False,\n         modules_to_not_convert: list | None = None,\n         **kwargs,\n     ):\n         self.quant_method = QuantizationMethod.FP8\n         self.modules_to_not_convert = modules_to_not_convert\n         self.activation_scheme = activation_scheme\n         self.weight_block_size = weight_block_size\n+        self.dequantize = dequantize\n         self.post_init()\n \n     def post_init(self):\n@@ -2010,6 +2014,9 @@ def post_init(self):\n         if self.weight_block_size[0] <= 0 or self.weight_block_size[1] <= 0:\n             raise ValueError(\"weight_block_size must be a tuple of two positive integers\")\n \n+    def get_loading_attributes(self):\n+        return {\"dequantize\": self.dequantize}\n+\n \n class QuarkConfig(QuantizationConfigMixin):\n     def __init__("
        },
        {
            "sha": "3e65c955c80b9fcff91fbc50dbe9ad4f01e816ec",
            "filename": "tests/quantization/finegrained_fp8/test_fp8.py",
            "status": "modified",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/afd039d1caa1a11dcc03d21bc0bb5c114bf927c2/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afd039d1caa1a11dcc03d21bc0bb5c114bf927c2/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py?ref=afd039d1caa1a11dcc03d21bc0bb5c114bf927c2",
            "patch": "@@ -15,8 +15,11 @@\n import gc\n import tempfile\n import unittest\n+from contextlib import ExitStack, contextmanager\n+from unittest.mock import patch\n \n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, FineGrainedFP8Config, OPTForCausalLM\n+from transformers.quantizers.quantizer_finegrained_fp8 import FineGrainedFP8HfQuantizer\n from transformers.testing_utils import (\n     backend_empty_cache,\n     get_device_properties,\n@@ -37,6 +40,15 @@\n     from accelerate import init_empty_weights\n \n \n+@contextmanager\n+def _patch_no_accelerator():\n+    with ExitStack() as stack:\n+        stack.enter_context(patch(\"torch.cuda.is_available\", return_value=False))\n+        if hasattr(torch, \"xpu\"):\n+            stack.enter_context(patch(\"torch.xpu.is_available\", return_value=False))\n+        yield\n+\n+\n @require_torch_accelerator\n class FineGrainedFP8ConfigTest(unittest.TestCase):\n     def test_to_dict(self):\n@@ -71,9 +83,11 @@ def test_from_dict(self):\n )\n class FP8QuantizerTest(unittest.TestCase):\n     model_name = \"meta-llama/Llama-3.2-1B\"\n+    quantized_model_name = \"hf-internal-testing/Llama-3.2-1B-Instruct-fp8\"\n     input_text = \"Once upon a time\"\n     max_new_tokens = 10\n     EXPECTED_OUTPUT = \"Once upon a time, there was a man who was very rich.\"\n+    EXPECTED_DEQUANTIZED_OUTPUT = \"Once upon a time, in a small village nestled in the rolling hills\"\n     device_map = torch_device\n     offload_device_map = {\n         \"model.embed_tokens\": 0,\n@@ -152,6 +166,25 @@ def test_quantized_model_conversion(self):\n \n         self.assertEqual(nb_linears - 25, nb_fp8_linear)\n \n+    def test_quantizer_validation_no_accelerator(self):\n+        \"\"\"Test quantizer validation when CUDA/XPU is not available\"\"\"\n+        with _patch_no_accelerator():\n+            config = FineGrainedFP8Config()\n+            quantizer = FineGrainedFP8HfQuantizer(config)\n+            quantizer.pre_quantized = False\n+\n+            with self.assertRaises(RuntimeError):\n+                quantizer.validate_environment()\n+\n+    def test_dequantization_no_accelerator(self):\n+        \"\"\"Test dequantization when CUDA/XPU is not available\"\"\"\n+        with _patch_no_accelerator():\n+            config = FineGrainedFP8Config()\n+            quantizer = FineGrainedFP8HfQuantizer(config)\n+            quantizer.pre_quantized = True\n+            quantizer.validate_environment()\n+            self.assertTrue(quantizer.quantization_config.dequantize)\n+\n     def test_quantized_model(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly\n@@ -162,6 +195,32 @@ def test_quantized_model(self):\n         output_tokens = self.tokenizer.decode(output[0], skip_special_tokens=True)\n         self.assertEqual(output_tokens, self.EXPECTED_OUTPUT)\n \n+    def test_dequantized_model(self):\n+        \"\"\"\n+        Simple test that checks if the dequantized model is working properly\n+        \"\"\"\n+        quantization_config = FineGrainedFP8Config(dequantize=True)\n+        dequantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.quantized_model_name, device_map=self.device_map, quantization_config=quantization_config\n+        )\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(self.device_map)\n+        output = dequantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n+        output_tokens = self.tokenizer.decode(output[0], skip_special_tokens=True)\n+        self.assertEqual(output_tokens, self.EXPECTED_DEQUANTIZED_OUTPUT)\n+        del dequantized_model\n+\n+    def test_dequantize_when_no_accelerator(self):\n+        \"\"\"\n+        Simple test that checks if the dequantized model is working properly when no accelerator is available\n+        \"\"\"\n+        with _patch_no_accelerator():\n+            dequantized_model = AutoModelForCausalLM.from_pretrained(self.quantized_model_name, device_map=\"cpu\")\n+            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(\"cpu\")\n+            output = dequantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n+            output_tokens = self.tokenizer.decode(output[0], skip_special_tokens=True)\n+            self.assertEqual(output_tokens, self.EXPECTED_DEQUANTIZED_OUTPUT)\n+            del dequantized_model\n+\n     def test_save_pretrained(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly after being saved and loaded"
        }
    ],
    "stats": {
        "total": 166,
        "additions": 134,
        "deletions": 32
    }
}