{
    "author": "nikosanto13",
    "message": "fix onnx export of speech foundation models (#34224)\n\n* added expanded attention/padding masks prior to indexing the hidden_states\n\n* consistency fix in WavLMForSequenceClassification\n\n---------\n\nCo-authored-by: Nikos Antoniou <nikosantoniou@Nikos-MacBook-Pro.local>",
    "sha": "ff9141bb85f22e7b200f0fbed76fd3641990ed7b",
    "files": [
        {
            "sha": "801bd19fca3b60038940de554d12a3625aca798f",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=ff9141bb85f22e7b200f0fbed76fd3641990ed7b",
            "patch": "@@ -1421,7 +1421,8 @@ def forward(\n             pooled_output = hidden_states.mean(dim=1)\n         else:\n             padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n-            hidden_states[~padding_mask] = 0.0\n+            expand_padding_mask = padding_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_padding_mask] = 0.0\n             pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n \n         logits = self.classifier(pooled_output)"
        },
        {
            "sha": "f2700836789ebd452cf266498d43362dfe37c394",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=ff9141bb85f22e7b200f0fbed76fd3641990ed7b",
            "patch": "@@ -1629,7 +1629,8 @@ def forward(\n             pooled_output = hidden_states.mean(dim=1)\n         else:\n             padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n-            hidden_states[~padding_mask] = 0.0\n+            expand_padding_mask = padding_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_padding_mask] = 0.0\n             pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n \n         logits = self.classifier(pooled_output)"
        },
        {
            "sha": "8dc3e2297d452508720234133bb1153c4c57c1f4",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=ff9141bb85f22e7b200f0fbed76fd3641990ed7b",
            "patch": "@@ -882,15 +882,15 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n \n         if attention_mask is not None:\n+            expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             if self._use_flash_attention_2:\n                 # make sure padded tokens output 0\n-                hidden_states[~attention_mask] = 0.0\n+                hidden_states[~expand_attention_mask] = 0.0\n                 # 2d mask is passed through the layers\n                 attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n             else:\n                 # make sure padded tokens output 0\n-                hidden_states[~attention_mask] = 0.0\n-\n+                hidden_states[~expand_attention_mask] = 0.0\n                 input_lengths = (attention_mask.long()).sum(-1)\n                 # apply pooling formula to get real output_lengths\n                 output_lengths = input_lengths // self.config.squeeze_factor\n@@ -1473,7 +1473,8 @@ def forward(\n             pooled_output = hidden_states.mean(dim=1)\n         else:\n             padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n-            hidden_states[~padding_mask] = 0.0\n+            expand_padding_mask = padding_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_padding_mask] = 0.0\n             pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n \n         logits = self.classifier(pooled_output)"
        },
        {
            "sha": "2df687f4cc362a97a0f0f1fc5629f9a5a0715cc4",
            "filename": "src/transformers/models/sew_d/modeling_sew_d.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py?ref=ff9141bb85f22e7b200f0fbed76fd3641990ed7b",
            "patch": "@@ -1175,7 +1175,8 @@ def forward(\n             )\n         else:\n             # make sure padded tokens output 0\n-            hidden_states[~attention_mask.bool()] = 0.0\n+            expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_attention_mask.bool()] = 0.0\n \n             input_lengths = (attention_mask.long()).sum(-1)\n             # apply pooling formula to get real output_lengths\n@@ -1721,7 +1722,8 @@ def forward(\n             pooled_output = hidden_states.mean(dim=1)\n         else:\n             padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n-            hidden_states[~padding_mask] = 0.0\n+            expand_padding_mask = padding_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_padding_mask] = 0.0\n             pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n \n         logits = self.classifier(pooled_output)"
        },
        {
            "sha": "f355eb03bdb82fd1f2319abc75c182617d9de2cd",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=ff9141bb85f22e7b200f0fbed76fd3641990ed7b",
            "patch": "@@ -1876,7 +1876,8 @@ def forward(\n             pooled_output = hidden_states.mean(dim=1)\n         else:\n             padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n-            hidden_states[~padding_mask] = 0.0\n+            expand_padding_mask = padding_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_padding_mask] = 0.0\n             pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n \n         logits = self.classifier(pooled_output)"
        },
        {
            "sha": "0fd6e7cb2c04e14f961857c044843b947a8c9b2a",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=ff9141bb85f22e7b200f0fbed76fd3641990ed7b",
            "patch": "@@ -1886,7 +1886,8 @@ def forward(\n             pooled_output = hidden_states.mean(dim=1)\n         else:\n             padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n-            hidden_states[~padding_mask] = 0.0\n+            expand_padding_mask = padding_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_padding_mask] = 0.0\n             pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n \n         logits = self.classifier(pooled_output)"
        },
        {
            "sha": "e4df2e6ae3b71838288ed9dba0677da3af3ff82f",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=ff9141bb85f22e7b200f0fbed76fd3641990ed7b",
            "patch": "@@ -2376,7 +2376,8 @@ def forward(\n             pooled_output = hidden_states.mean(dim=1)\n         else:\n             padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n-            hidden_states[~padding_mask] = 0.0\n+            expand_padding_mask = padding_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_padding_mask] = 0.0\n             pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n \n         logits = self.classifier(pooled_output)"
        },
        {
            "sha": "7774c7a4069d02e3dc1809c899e3841441b1aa0b",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=ff9141bb85f22e7b200f0fbed76fd3641990ed7b",
            "patch": "@@ -1359,7 +1359,8 @@ def forward(\n             pooled_output = hidden_states.mean(dim=1)\n         else:\n             padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n-            hidden_states[~padding_mask] = 0.0\n+            expand_padding_mask = padding_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_padding_mask] = 0.0\n             pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n \n         logits = self.classifier(pooled_output)"
        },
        {
            "sha": "494654a67747546a46959e52aab1cf4dba0564b7",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=ff9141bb85f22e7b200f0fbed76fd3641990ed7b",
            "patch": "@@ -878,7 +878,8 @@ def forward(\n \n         if attention_mask is not None:\n             # make sure padded tokens output 0\n-            hidden_states[~attention_mask] = 0.0\n+            expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_attention_mask] = 0.0\n \n             # extend attention_mask\n             attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n@@ -1791,7 +1792,8 @@ def forward(\n             pooled_output = hidden_states.mean(dim=1)\n         else:\n             padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n-            hidden_states[~padding_mask] = 0.0\n+            expand_padding_mask = padding_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_padding_mask] = 0.0\n             pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n \n         logits = self.classifier(pooled_output)"
        },
        {
            "sha": "3e5e379000537749472b6bea711531e484c0043c",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff9141bb85f22e7b200f0fbed76fd3641990ed7b/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=ff9141bb85f22e7b200f0fbed76fd3641990ed7b",
            "patch": "@@ -691,7 +691,8 @@ def forward(\n \n         if attention_mask is not None:\n             # make sure padded tokens output 0\n-            hidden_states[~attention_mask] = 0.0\n+            expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_attention_mask] = 0\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n         hidden_states = hidden_states + position_embeddings\n@@ -776,7 +777,8 @@ def forward(\n \n         if attention_mask is not None:\n             # make sure padded tokens are not attended to\n-            hidden_states[~attention_mask] = 0\n+            expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_attention_mask] = 0\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n         hidden_states = hidden_states + position_embeddings\n@@ -1508,7 +1510,8 @@ def forward(\n             pooled_output = hidden_states.mean(dim=1)\n         else:\n             padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n-            hidden_states[~padding_mask] = 0.0\n+            expand_padding_mask = padding_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_padding_mask] = 0.0\n             pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n \n         logits = self.classifier(pooled_output)"
        }
    ],
    "stats": {
        "total": 48,
        "additions": 31,
        "deletions": 17
    }
}