{
    "author": "kashif",
    "message": "[Continous Batching] fix do_Sample=True in continuous batching (#40692)\n\n* fix do_Sample=True in continous batching\n\n* added test\n\n* fix top_p\n\n* test\n\n* Update examples/pytorch/continuous_batching.py",
    "sha": "3f7bda4209bd484b75a1441b80ef38668e442f78",
    "files": [
        {
            "sha": "7196dc994204fbb67fed61f387d4ed477bcd1fc9",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f7bda4209bd484b75a1441b80ef38668e442f78/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f7bda4209bd484b75a1441b80ef38668e442f78/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=3f7bda4209bd484b75a1441b80ef38668e442f78",
            "patch": "@@ -229,7 +229,9 @@ def batch_generate(\n         use_cuda_graph=args.use_cuda_graph,\n         eos_token_id=tokenizer.eos_token_id,\n         pad_token_id=tokenizer.pad_token_id,\n-        do_sample=False,\n+        do_sample=True,\n+        temperature=0.8,\n+        top_p=0.9,\n         num_blocks=args.num_blocks,\n         max_batch_tokens=args.max_batch_tokens,\n     )"
        },
        {
            "sha": "1c63507abe93c0399b20d60b5240dc1284578080",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 19,
            "deletions": 4,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f7bda4209bd484b75a1441b80ef38668e442f78/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f7bda4209bd484b75a1441b80ef38668e442f78/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=3f7bda4209bd484b75a1441b80ef38668e442f78",
            "patch": "@@ -631,16 +631,31 @@ def _process_logit(self, batch_data, logits):\n             self.logit_processor.set_continuous_batching_context(\n                 batch_data[\"logits_indices\"], batch_data[\"cu_seq_lens_q\"]\n             )\n-        return self.logit_processor(batch_data[\"input_ids\"], logits)\n+\n+        # Handle shape compatibility: logit processors expect 2D tensors [batch_size, vocab_size]\n+        # but continuous batching always produces 3D tensors [batch_size, seq_len, vocab_size]\n+        batch_size, seq_len, vocab_size = logits.shape\n+        logits_2d = logits.view(batch_size * seq_len, vocab_size)\n+        input_ids_2d = batch_data[\"input_ids\"].view(batch_size * seq_len)\n+\n+        # Process with 2D tensors\n+        processed_logits_2d = self.logit_processor(input_ids_2d, logits_2d)\n+\n+        # Reshape back to 3D\n+        return processed_logits_2d.view(batch_size, seq_len, vocab_size)\n \n     @traced(span_name=\"sampling\")\n     def _sample(self, batch_processor: ContinuousBatchProcessor, probs):\n         if self.do_sample:  # sample\n             probs = nn.functional.softmax(probs, dim=-1)\n-            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(1)\n+            # probs[0] has shape [seq_len, vocab_size], multinomial returns [seq_len, 1]\n+            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(-1)  # Now [seq_len]\n+            # Add batch dimension back to match argmax output\n+            next_tokens = next_tokens.unsqueeze(0)  # Now [1, seq_len]\n         else:\n-            next_tokens = torch.argmax(probs, dim=-1)\n-        tokens = next_tokens.size(1)\n+            next_tokens = torch.argmax(probs, dim=-1)  # Already [1, seq_len]\n+\n+        tokens = next_tokens.size(1)  # Get seq_len dimension\n         batch_processor.output_ids[:, :tokens].copy_(next_tokens)\n \n     def _run_generation_loop(self):"
        },
        {
            "sha": "fbef0d0274adeec01179bd0b1169ae7c73ac1df6",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f7bda4209bd484b75a1441b80ef38668e442f78/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f7bda4209bd484b75a1441b80ef38668e442f78/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=3f7bda4209bd484b75a1441b80ef38668e442f78",
            "patch": "@@ -53,7 +53,7 @@ def paged_attention_forward(\n     k, v = cache.update(k, v, module.layer_idx, **kwargs)\n \n     sliding_window = (-1, -1) if not getattr(module, \"sliding_window\", False) else (module.sliding_window, 0)\n-    if implementation is not None:\n+    if implementation is not None and hasattr(implementation, \"flash_attn_varlen_func\"):\n         flash_attn_varlen_func = implementation.flash_attn_varlen_func\n     custom_kwargs = {\"s_aux\": kwargs.get(\"s_aux\")} if \"s_aux\" in kwargs else {}\n     attn_output = flash_attn_varlen_func("
        },
        {
            "sha": "e7673f5f08cdfe0c93d799cf2b991909d84d28f1",
            "filename": "tests/generation/test_paged_attention.py",
            "status": "modified",
            "additions": 64,
            "deletions": 1,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f7bda4209bd484b75a1441b80ef38668e442f78/tests%2Fgeneration%2Ftest_paged_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f7bda4209bd484b75a1441b80ef38668e442f78/tests%2Fgeneration%2Ftest_paged_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_paged_attention.py?ref=3f7bda4209bd484b75a1441b80ef38668e442f78",
            "patch": "@@ -78,9 +78,72 @@ def test_generate_batch_consistency(self, attn_impl, num_blocks, block_size, max\n         )\n \n         for i, req_id in enumerate(batch_outputs):\n-            generated = self.tokenizer.decode(batch_outputs[req_id].static_outputs, skip_special_tokens=False).strip()\n+            generated = self.tokenizer.decode(\n+                batch_outputs[req_id].generated_tokens, skip_special_tokens=False\n+            ).strip()\n             expected = _EXPECTED_OUTPUTS[i].strip()\n             self.assertTrue(\n                 generated.startswith(expected),\n                 msg=f\"[{attn_impl}] Mismatch in request {i}:\\nExpected start: {expected}\\nGot: {generated}\",\n             )\n+\n+    @parameterized.expand(\n+        [\n+            (\"eager_paged\", 64, 128, 64),\n+            (\"sdpa_paged\", 32, 256, 128),\n+            (\"paged_attention\", 16, 512, 256),\n+            (\"flex_paged\", 64, 128, 64),\n+        ]\n+    )\n+    def test_generate_batch_with_sampling(self, attn_impl, num_blocks, block_size, max_batch_tokens):\n+        \"\"\"Test batch generation with do_sampling=True to verify sampling works correctly.\"\"\"\n+        self.model.config.attn_implementation = attn_impl\n+\n+        generation_config = GenerationConfig(\n+            max_new_tokens=30,\n+            do_sample=True,\n+            top_k=50,\n+            top_p=0.9,\n+            temperature=0.8,\n+            eos_token_id=self.tokenizer.eos_token_id,\n+            pad_token_id=self.tokenizer.pad_token_id,\n+            use_cache=False,\n+            num_blocks=num_blocks,\n+            block_size=block_size,\n+            max_batch_tokens=max_batch_tokens,\n+        )\n+\n+        tokenized = self.tokenizer(_TEST_PROMPTS, truncation=True, max_length=512)  # Use fewer prompts for faster test\n+        batch_inputs = list(tokenized[\"input_ids\"])\n+\n+        start = time.time()\n+        batch_outputs = self.model.generate_batch(\n+            inputs=batch_inputs,\n+            generation_config=generation_config,\n+        )\n+        end = time.time()\n+        print(\n+            f\"\\n[{attn_impl}] Sampling batch took {end - start:.2f}s with config: blocks={num_blocks}, block_size={block_size}, max_batch_tokens={max_batch_tokens}\"\n+        )\n+\n+        # With sampling enabled, we can't check exact outputs, but we should verify:\n+        # 1. All requests completed successfully\n+        # 2. Generated text is non-empty\n+        # 3. Generated text is different from greedy (demonstrating sampling is working)\n+        self.assertEqual(len(batch_outputs), len(batch_inputs), f\"[{attn_impl}] Not all requests completed\")\n+\n+        for i, req_id in enumerate(batch_outputs):\n+            generated = self.tokenizer.decode(\n+                batch_outputs[req_id].generated_tokens, skip_special_tokens=False\n+            ).strip()\n+            self.assertTrue(\n+                len(generated) > 0,\n+                msg=f\"[{attn_impl}] Empty output for request {i}\",\n+            )\n+            # Check that we got at least some tokens generated\n+            generated_tokens = batch_outputs[req_id].generated_tokens\n+            self.assertGreater(\n+                len(generated_tokens),\n+                0,\n+                msg=f\"[{attn_impl}] No tokens generated for request {i}\",\n+            )"
        }
    ],
    "stats": {
        "total": 94,
        "additions": 87,
        "deletions": 7
    }
}