{
    "author": "zhanluxianshen",
    "message": "Fix docs typos. (#35465)\n\nSigned-off-by: zhanluxianshen <zhanluxianshen@163.com>",
    "sha": "b2b04e86e71159259333de2f8da85c08a712880d",
    "files": [
        {
            "sha": "2c4f114dec85cb799a6d241fe27dc9062fb56740",
            "filename": "docs/source/en/fsdp.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2b04e86e71159259333de2f8da85c08a712880d/docs%2Fsource%2Fen%2Ffsdp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2b04e86e71159259333de2f8da85c08a712880d/docs%2Fsource%2Fen%2Ffsdp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ffsdp.md?ref=b2b04e86e71159259333de2f8da85c08a712880d",
            "patch": "@@ -58,7 +58,7 @@ Otherwise, you can choose a size-based wrapping policy where FSDP is applied to\n \n ### Checkpointing\n \n-Intermediate checkpoints should be saved with `fsdp_state_dict_type: SHARDED_STATE_DICT` because saving the full state dict with CPU offloading on rank 0 takes a lot of time and often results in `NCCL Timeout` errors due to indefinite hanging during broadcasting. You can resume training with the sharded state dicts with the [`~accelerate.Accelerator.load_state`]` method.\n+Intermediate checkpoints should be saved with `fsdp_state_dict_type: SHARDED_STATE_DICT` because saving the full state dict with CPU offloading on rank 0 takes a lot of time and often results in `NCCL Timeout` errors due to indefinite hanging during broadcasting. You can resume training with the sharded state dicts with the [`~accelerate.Accelerator.load_state`] method.\n \n ```py\n # directory containing checkpoints"
        },
        {
            "sha": "4688b021f7416e27eeb921b9b28bb5c5eaa69046",
            "filename": "docs/source/zh/fsdp.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2b04e86e71159259333de2f8da85c08a712880d/docs%2Fsource%2Fzh%2Ffsdp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2b04e86e71159259333de2f8da85c08a712880d/docs%2Fsource%2Fzh%2Ffsdp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Ffsdp.md?ref=b2b04e86e71159259333de2f8da85c08a712880d",
            "patch": "@@ -74,7 +74,7 @@ FSDP 是通过包装网络中的每个层来应用的。通常，包装是以嵌\n \n 应该使用 `fsdp_state_dict_type: SHARDED_STATE_DICT` 来保存中间检查点，\n 因为在排名 0 上保存完整状态字典需要很长时间，通常会导致 `NCCL Timeout` 错误，因为在广播过程中会无限期挂起。\n-您可以使用 [`~accelerate.Accelerator.load_state`]` 方法加载分片状态字典以恢复训练。\n+您可以使用 [`~accelerate.Accelerator.load_state`] 方法加载分片状态字典以恢复训练。\n \n ```py\n # 包含检查点的目录"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}