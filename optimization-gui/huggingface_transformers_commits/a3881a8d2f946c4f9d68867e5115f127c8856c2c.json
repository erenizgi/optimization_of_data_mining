{
    "author": "Dhruv88",
    "message": "perf: Remove implicit CPU-GPU syncs due to implicit .item() call (#42433)\n\n* perf: Remove implicit CPU-GPU syncs due to implicit .item() call\n\n* fix: replicated the changes across similar files\n\n* fix: update the newly added nanochat model files\n\n* fix: use input_shape and device instead of input_emdeds properties for imagegpt",
    "sha": "a3881a8d2f946c4f9d68867e5115f127c8856c2c",
    "files": [
        {
            "sha": "6d6d768f53b5878b93833d23b905c8d03977831c",
            "filename": "src/transformers/models/apertus/modeling_apertus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -392,8 +392,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "42d8c099bc958b6446aba7534f3c3d5d17cd272b",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -397,8 +397,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "714585c174242fad66d1ca444cc2a3e91d1a5500",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -726,8 +726,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "bc15a03c1ce89ea1427c0b47e674bbf5a92c952e",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -396,8 +396,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "5d848875359e7be705d6f0c33f1ad6767990a219",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -428,8 +428,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "f94c8a1dace703bbc8bb59f20d35852cb29d03ba",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -731,8 +731,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "e464d3ff44defa78a000baaeb9665ebb1cb15504",
            "filename": "src/transformers/models/cwm/modeling_cwm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -393,8 +393,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "4ba38ee1b02725f10c68211f9d263655c85070bc",
            "filename": "src/transformers/models/cwm/modular_cwm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodular_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodular_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodular_cwm.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -241,8 +241,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "5573ad5678fa57d4f2d451a6c9331d559bb8a0a2",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -512,8 +512,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "d55b3f43c4751c771c21fe6cf39fefd3745026c0",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -604,8 +604,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "5149b670cab5f0f76354b8f512b9712d161a461d",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -651,8 +651,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "6c87e7442733d507fb0f74a2063fc0cb62076b15",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -1223,8 +1223,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "59422fb4d136cdc99e855436be964c3d3c14a87f",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -394,8 +394,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "2ea11c42033d869a12d99019ecc7e04d238a2713",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -412,8 +412,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "f80b00a0bad88e82f67c8850985a168076350002",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -416,8 +416,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "ef702f94fa257cafaaa7cd705a021b26f1ffe56d",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -547,8 +547,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "ba4594240eec1198c445b05d057a676a9edcf7fe",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -395,8 +395,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "af396506b37327139d6d460649b779d1c00ec37f",
            "filename": "src/transformers/models/hunyuan_v1_dense/modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -410,8 +410,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "6e3bb592e722c563d548936cfc3e9e6bb2eb5193",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -504,8 +504,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "0b2288f682c4cc75e123f782407bec093bb7db76",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -493,9 +493,7 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + input_shape[-1], device=device\n-            )\n+            cache_position: torch.Tensor = torch.arange(input_shape[-1], device=device) + past_seen_tokens\n \n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)"
        },
        {
            "sha": "e73e1c776addceab0506413d59aa6b75315b5ae9",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -401,8 +401,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "65a7bc4c5123e030b2c144f0351b1f29a9cdc6e1",
            "filename": "src/transformers/models/longcat_flash/modeling_longcat_flash.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -617,8 +617,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "fb9af42e0a8ef1eca27ec55036e53c0f1e4fd2fe",
            "filename": "src/transformers/models/longcat_flash/modular_longcat_flash.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -394,8 +394,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "7cea9b7e733b310966391abd3214029940940451",
            "filename": "src/transformers/models/nanochat/modeling_nanochat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -405,8 +405,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "d180c45151cec6055afe29b9860d4bba51f5aa70",
            "filename": "src/transformers/models/nanochat/modular_nanochat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodular_nanochat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodular_nanochat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodular_nanochat.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -170,8 +170,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "d530fe95cae28845a11dd6bb8107c491b14590fc",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -398,8 +398,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "5c852ba966e2597e3bf9555448695ec26e83e869",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -403,8 +403,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "8f580896790fd8c41b28dd22a7adcfc1505c6c49",
            "filename": "src/transformers/models/olmo3/modeling_olmo3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -402,8 +402,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "9783f2086f3ecefbb816cac01fd74873b1947405",
            "filename": "src/transformers/models/olmo3/modular_olmo3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -304,8 +304,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        },
        {
            "sha": "9126606a68fb1c2b894695db6efc69ba959740a4",
            "filename": "src/transformers/models/seed_oss/modeling_seed_oss.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3881a8d2f946c4f9d68867e5115f127c8856c2c/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py?ref=a3881a8d2f946c4f9d68867e5115f127c8856c2c",
            "patch": "@@ -401,8 +401,8 @@ def forward(\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position: torch.Tensor = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n             )\n \n         if position_ids is None:"
        }
    ],
    "stats": {
        "total": 120,
        "additions": 59,
        "deletions": 61
    }
}