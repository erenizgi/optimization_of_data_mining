{
    "author": "VladOS95-cyber",
    "message": "Add gguf support for bloom (#33473)\n\n* add bloom arch support for gguf\r\n\r\n* apply format\r\n\r\n* small refactoring, bug fix in GGUF_TENSOR_MAPPING naming\r\n\r\n* optimize bloom GGUF_TENSOR_MAPPING\r\n\r\n* implement reverse reshaping for bloom gguf\r\n\r\n* add qkv weights test\r\n\r\n* add q_8 test for bloom",
    "sha": "9d200cfbeef6e88754eafb5dfba5458ec07e9eba",
    "files": [
        {
            "sha": "5ac7279292bb1586aa2ebbee13901c260dbfff00",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d200cfbeef6e88754eafb5dfba5458ec07e9eba/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d200cfbeef6e88754eafb5dfba5458ec07e9eba/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=9d200cfbeef6e88754eafb5dfba5458ec07e9eba",
            "patch": "@@ -80,6 +80,7 @@ For now the supported model architectures are the architectures that have been v\n - Qwen2\n - Qwen2Moe\n - Phi3\n+- Bloom\n \n ## Example usage\n "
        },
        {
            "sha": "a4fd90f2bfe473d5ac58b1aca0b5e23e41d16f1c",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d200cfbeef6e88754eafb5dfba5458ec07e9eba/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d200cfbeef6e88754eafb5dfba5458ec07e9eba/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=9d200cfbeef6e88754eafb5dfba5458ec07e9eba",
            "patch": "@@ -328,9 +328,11 @@ def converted(self) -> Tokenizer:\n \n \n class GPT2Converter(Converter):\n-    def converted(self) -> Tokenizer:\n-        vocab = self.original_tokenizer.encoder\n-        merges = list(self.original_tokenizer.bpe_ranks.keys())\n+    def converted(self, vocab: Dict[str, int] = None, merges: List[Tuple[str, str]] = None) -> Tokenizer:\n+        if not vocab:\n+            vocab = self.original_tokenizer.encoder\n+        if not merges:\n+            merges = list(self.original_tokenizer.bpe_ranks)\n \n         tokenizer = Tokenizer(\n             BPE(\n@@ -343,9 +345,11 @@ def converted(self) -> Tokenizer:\n             )\n         )\n \n-        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=self.original_tokenizer.add_prefix_space)\n+        add_prefix_space = False\n+        add_prefix_space = getattr(self.original_tokenizer, \"add_prefix_space\", False)\n+        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n         tokenizer.decoder = decoders.ByteLevel()\n-        if self.original_tokenizer.add_bos_token:\n+        if getattr(self.original_tokenizer, \"add_bos_token\", False):\n             bos = self.original_tokenizer.bos_token\n             bos_token_id = self.original_tokenizer.bos_token_id\n             tokenizer.post_processor = processors.TemplateProcessing("
        },
        {
            "sha": "d7a548791e8a70b0bd7f82a277e05930a4abeca9",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 34,
            "deletions": 1,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d200cfbeef6e88754eafb5dfba5458ec07e9eba/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d200cfbeef6e88754eafb5dfba5458ec07e9eba/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=9d200cfbeef6e88754eafb5dfba5458ec07e9eba",
            "patch": "@@ -25,7 +25,7 @@\n from tokenizers.models import BPE\n \n from .. import AddedToken\n-from ..convert_slow_tokenizer import LlamaConverter, Qwen2Converter\n+from ..convert_slow_tokenizer import GPT2Converter, LlamaConverter, Qwen2Converter\n from ..utils import logging\n from ..utils.logging import tqdm\n \n@@ -107,6 +107,19 @@\n         \"output.weight\": \"lm_head.weight\",\n         \"output_norm\": \"model.norm\",\n     },\n+    \"bloom\": {\n+        \"token_embd.weight\": \"transformer.word_embeddings.weight\",\n+        \"token_embd_norm\": \"transformer.word_embeddings_layernorm\",\n+        \"blk\": \"transformer.h\",\n+        \"ffn_up\": \"mlp.dense_h_to_4h\",\n+        \"ffn_down\": \"mlp.dense_4h_to_h\",\n+        \"ffn_norm\": \"post_attention_layernorm\",\n+        \"attn_norm\": \"input_layernorm\",\n+        \"attn_qkv\": \"self_attention.query_key_value\",\n+        \"attn_output\": \"self_attention.dense\",\n+        \"output.weight\": \"lm_head.weight\",\n+        \"output_norm\": \"transformer.ln_f\",\n+    },\n }\n \n \n@@ -183,6 +196,13 @@\n         \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n         \"vocab_size\": \"vocab_size\",\n     },\n+    \"bloom\": {\n+        \"block_count\": \"n_layer\",\n+        \"embedding_length\": \"hidden_size\",\n+        \"attention.head_count\": \"n_head\",\n+        \"vocab_size\": \"vocab_size\",\n+        \"attention.layer_norm_epsilon\": \"layer_norm_epsilon\",\n+    },\n }\n \n GGUF_TOKENIZER_MAPPING = {\n@@ -492,11 +512,24 @@ def converted(self) -> Tokenizer:\n         return tokenizer\n \n \n+class GGUFBloomConverter(GPT2Converter):\n+    def __init__(self, tokenizer_dict):\n+        self.original_tokenizer = GGUFTokenizerSkeleton(tokenizer_dict)\n+        self.additional_kwargs = {}\n+\n+    def converted(self) -> Tokenizer:\n+        vocab = {word: i for i, word in enumerate(self.original_tokenizer.tokens)}\n+        merges = self.original_tokenizer.merges\n+        tokenizer = super().converted(vocab, merges)\n+        return tokenizer\n+\n+\n GGUF_TO_FAST_CONVERTERS = {\n     \"llama\": GGUFLlamaConverter,\n     \"qwen2\": GGUFQwen2Converter,\n     \"qwen2_moe\": GGUFQwen2Converter,\n     \"phi3\": GGUFPhi3Converter,\n+    \"bloom\": GGUFBloomConverter,\n }\n \n "
        },
        {
            "sha": "c2e06624e15714a57294d752d7f83aff7dc139af",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d200cfbeef6e88754eafb5dfba5458ec07e9eba/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d200cfbeef6e88754eafb5dfba5458ec07e9eba/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=9d200cfbeef6e88754eafb5dfba5458ec07e9eba",
            "patch": "@@ -169,6 +169,14 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n                 elif \".attn_k.\" in name:\n                     weights = reverse_permute_weights(weights, num_heads, num_kv_heads)\n \n+            if architecture == \"bloom\" and \"attn_qkv\" in name:\n+                num_heads = parsed_parameters[\"config\"][\"n_head\"]\n+                n_embed = parsed_parameters[\"config\"][\"hidden_size\"]\n+                if \"weight\" in name:\n+                    weights = reverse_reshape_weights(weights, num_heads, n_embed)\n+                else:\n+                    weights = reverse_reshape_bias(weights, num_heads, n_embed)\n+\n             for tensor_name in tensor_key_mapping:\n                 if tensor_name in name:\n                     name = name.replace(tensor_name, tensor_key_mapping[tensor_name])\n@@ -191,3 +199,29 @@ def reverse_permute_weights(weights: np.ndarray, n_head: int, num_kv_heads: Opti\n     dim = weights.shape[0] // n_head // 2\n     w = weights.reshape(n_head, dim, 2, *weights.shape[1:])\n     return w.swapaxes(2, 1).reshape(weights.shape)\n+\n+\n+def reverse_reshape_weights(weights: np.ndarray, n_head: int, n_embed: int):\n+    # Original reshape implementation\n+    # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L972-L985\n+    q, k, v = np.array_split(weights, 3, axis=0)\n+\n+    q = q.reshape(n_head, n_embed // n_head, n_embed)\n+    k = k.reshape(n_head, n_embed // n_head, n_embed)\n+    v = v.reshape(n_head, n_embed // n_head, n_embed)\n+    qkv_weights = np.stack([q, k, v], axis=1)\n+\n+    return qkv_weights.reshape(n_head * 3 * (n_embed // n_head), n_embed)\n+\n+\n+def reverse_reshape_bias(weights: np.ndarray, n_head: int, n_embed: int):\n+    # Original reshape implementation\n+    # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L986-L998\n+    q_bias, k_bias, v_bias = np.array_split(weights, 3)\n+\n+    q_bias = q_bias.reshape(n_head, n_embed // n_head)\n+    k_bias = k_bias.reshape(n_head, n_embed // n_head)\n+    v_bias = v_bias.reshape(n_head, n_embed // n_head)\n+\n+    qkv_bias = np.stack([q_bias, k_bias, v_bias], axis=1).flatten()\n+    return qkv_bias"
        },
        {
            "sha": "3ea7a1a39cd8a572594115c6bfaf86d1e13f812d",
            "filename": "src/transformers/models/bloom/tokenization_bloom_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d200cfbeef6e88754eafb5dfba5458ec07e9eba/src%2Ftransformers%2Fmodels%2Fbloom%2Ftokenization_bloom_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d200cfbeef6e88754eafb5dfba5458ec07e9eba/src%2Ftransformers%2Fmodels%2Fbloom%2Ftokenization_bloom_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Ftokenization_bloom_fast.py?ref=9d200cfbeef6e88754eafb5dfba5458ec07e9eba",
            "patch": "@@ -99,8 +99,8 @@ def __init__(\n         **kwargs,\n     ):\n         super().__init__(\n-            vocab_file,\n-            merges_file,\n+            vocab_file=vocab_file,\n+            merges_file=merges_file,\n             tokenizer_file=tokenizer_file,\n             unk_token=unk_token,\n             bos_token=bos_token,"
        },
        {
            "sha": "13e64677be5c42a39093ec079395ccd68084f284",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 60,
            "deletions": 0,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d200cfbeef6e88754eafb5dfba5458ec07e9eba/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d200cfbeef6e88754eafb5dfba5458ec07e9eba/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=9d200cfbeef6e88754eafb5dfba5458ec07e9eba",
            "patch": "@@ -42,6 +42,8 @@ class GgufIntegrationTests(unittest.TestCase):\n     llama3_model_id = \"NousResearch/Meta-Llama-3-8B-GGUF\"\n     tinyllama_model_id = \"PenutChen/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n     phi3_model_id = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n+    bloom_model_id = \"afrideva/bloom-560m-GGUF\"\n+    original_bloom_model_id = \"bigscience/bloom-560m\"\n \n     # standard quants\n     q4_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_0.gguf\"\n@@ -69,6 +71,8 @@ class GgufIntegrationTests(unittest.TestCase):\n     q4_0_qwen2_model_id = \"qwen1_5-0_5b-chat-q4_0.gguf\"\n     q4_0_qwen2_moe_model_id = \"Qwen1.5-MoE-A2.7B-Chat.Q4_0.gguf\"\n     q4_llama3_model_id = \"Meta-Llama-3-8B-Q4_K_M.gguf\"\n+    fp16_bloom_model_id = \"bloom-560m.fp16.gguf\"\n+    q8_bloom_model_id = \"bloom-560m.q8_0.gguf\"\n     f16_tinyllama_model_id = \"TinyLlama-1.1B-Chat-v1.0.FP16.gguf\"\n \n     example_text = \"Hello\"\n@@ -385,6 +389,62 @@ def test_llama3_q4_0(self):\n         EXPECTED_TEXT = \"Hello, I am interested in [The Park]\\nThe\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n+    def test_bloom_fp16(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.bloom_model_id, gguf_file=self.fp16_bloom_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.bloom_model_id,\n+            gguf_file=self.fp16_bloom_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, I just want to say that I am very\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_bloom_q8_0(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.bloom_model_id, gguf_file=self.q8_bloom_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.bloom_model_id,\n+            gguf_file=self.q8_bloom_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, I just want to say that I am very\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_bloom_weights_conversion_fp16(self):\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.bloom_model_id,\n+            gguf_file=self.fp16_bloom_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+        original_model = AutoModelForCausalLM.from_pretrained(\n+            self.original_bloom_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        quantized_state_dict = quantized_model.state_dict()\n+        original_state_dict = original_model.state_dict()\n+\n+        for (quantized_name, quantized_param), (original_name, original_param) in zip(\n+            quantized_state_dict.items(), original_state_dict.items()\n+        ):\n+            if (\n+                \"self_attention.query_key_value\" in quantized_name\n+                and \"self_attention.query_key_value\" in original_name\n+            ):\n+                self.assertTrue(quantized_param.shape == original_param.shape)\n+                torch.testing.assert_close(quantized_param, original_param)\n+\n     def test_tokenization_xnli(self):\n         import tqdm\n         from datasets import load_dataset"
        }
    ],
    "stats": {
        "total": 148,
        "additions": 140,
        "deletions": 8
    }
}