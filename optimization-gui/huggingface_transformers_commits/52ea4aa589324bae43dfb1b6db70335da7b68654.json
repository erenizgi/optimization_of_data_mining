{
    "author": "jiqing-feng",
    "message": "add xpu path for awq (#34712)\n\n* add xpu path for awq\r\n\r\n* update readme",
    "sha": "52ea4aa589324bae43dfb1b6db70335da7b68654",
    "files": [
        {
            "sha": "91c6ebd40dab4dbfa3e8fdf7ee7d438591589fa2",
            "filename": "docs/source/en/quantization/overview.md",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/52ea4aa589324bae43dfb1b6db70335da7b68654/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52ea4aa589324bae43dfb1b6db70335da7b68654/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Foverview.md?ref=52ea4aa589324bae43dfb1b6db70335da7b68654",
            "patch": "@@ -45,19 +45,19 @@ In short, supporting a wide range of quantization methods allows you to pick the\n \n Use the table below to help you decide which quantization method to use.\n \n-| Quantization method                 | On the fly quantization | CPU | CUDA GPU | RoCm GPU (AMD) | Metal (Apple Silicon) | torch.compile() support | Number of bits | Supports fine-tuning (through PEFT) | Serializable with 游뱅 transformers | 游뱅 transformers support | Link to library                             |\n-|-------------------------------------|-------------------------|-----|----------|----------------|-----------------------|-------------------------|----------------|-------------------------------------|--------------|------------------------|---------------------------------------------|\n-| [AQLM](./aqlm)                                | 游댮                       |  游릭   |     游릭     | 游댮              | 游댮                     | 游릭                      | 1 / 2          | 游릭                                   | 游릭            | 游릭                      | https://github.com/Vahe1994/AQLM            |\n-| [AWQ](./awq) | 游댮                       | 游댮   | 游릭        | 游릭              | 游댮                     | ?                       | 4              | 游릭                                   | 游릭            | 游릭                      | https://github.com/casper-hansen/AutoAWQ    |\n-| [bitsandbytes](./bitsandbytes)     | 游릭            | 游리 *   |     游릭     | 游리 *            | 游댮 **    | 游댮    (soon!)          | 4 / 8          | 游릭                                   | 游릭            | 游릭                      | https://github.com/bitsandbytes-foundation/bitsandbytes |\n-| [compressed-tensors](./compressed_tensors)                        | 游댮                       | 游릭   |     游릭     | 游릭              | 游댮                     | 游댮                       | 1 - 8          | 游릭                                   | 游릭            | 游릭                      | https://github.com/neuralmagic/compressed-tensors |\n-| [EETQ](./eetq)                                | 游릭                       | 游댮   | 游릭        | 游댮              | 游댮                     | ?                       | 8              | 游릭                                   | 游릭            | 游릭                      | https://github.com/NetEase-FuXi/EETQ        |\n-| GGUF / GGML (llama.cpp)             | 游릭                       | 游릭   | 游릭        | 游댮              | 游릭                     | 游댮                       | 1 - 8          | 游댮                                   | [See GGUF section](../gguf)                | [See GGUF section](../gguf)                      | https://github.com/ggerganov/llama.cpp      |\n-| [GPTQ](./gptq)                                | 游댮                       | 游댮   | 游릭        | 游릭              | 游댮                     | 游댮                       | 2 - 3 - 4 - 8          | 游릭                                   | 游릭            | 游릭                      | https://github.com/AutoGPTQ/AutoGPTQ        |\n-| [HQQ](./hqq)                                 | 游릭                       | 游릭    | 游릭        | 游댮              | 游댮                     | 游릭                       | 1 - 8          | 游릭                                   | 游댮            | 游릭                      | https://github.com/mobiusml/hqq/            |\n-| [Quanto](./quanto)                              | 游릭                       | 游릭   | 游릭        | 游댮              | 游릭                     | 游릭                       | 2 / 4 / 8      | 游댮                                   | 游댮            | 游릭                      | https://github.com/huggingface/quanto       |\n-| [FBGEMM_FP8](./fbgemm_fp8.md)                              | 游릭                       | 游댮    | 游릭        | 游댮              | 游댮                      | 游댮                        | 8      | 游댮                                   | 游릭            | 游릭                      | https://github.com/pytorch/FBGEMM       |\n-| [torchao](./torchao.md)                              | 游릭                       |     | 游릭        | 游댮              | partial support (int4 weight only)       |                       | 4 / 8      |                                   | 游릭游댮           | 游릭                      | https://github.com/pytorch/ao       |\n+| Quantization method                 | On the fly quantization | CPU | CUDA GPU | RoCm GPU (AMD) | Metal (Apple Silicon) | Intel GPU | torch.compile() support | Number of bits | Supports fine-tuning (through PEFT) | Serializable with 游뱅 transformers | 游뱅 transformers support | Link to library                             |\n+|-------------------------------------|-------------------------|-----|----------|----------------|-----------------------|-----------|-------------------------|----------------|-------------------------------------|--------------|------------------------|---------------------------------------------|\n+| [AQLM](./aqlm)                                | 游댮                       |  游릭   |     游릭     | 游댮              | 游댮                     | 游댮         | 游릭                      | 1 / 2          | 游릭                                   | 游릭            | 游릭                      | https://github.com/Vahe1994/AQLM            |\n+| [AWQ](./awq) | 游댮                       | 游릭   | 游릭        | 游릭              | 游댮                     | 游릭         | ?                       | 4              | 游릭                                   | 游릭            | 游릭                      | https://github.com/casper-hansen/AutoAWQ    |\n+| [bitsandbytes](./bitsandbytes)     | 游릭            | 游리 *   |     游릭     | 游리 *            | 游댮 **    | 游리 *       | 游댮    (soon!)          | 4 / 8          | 游릭                                   | 游릭            | 游릭                      | https://github.com/bitsandbytes-foundation/bitsandbytes |\n+| [compressed-tensors](./compressed_tensors)                        | 游댮                       | 游릭   |     游릭     | 游릭              | 游댮                     | 游댮         | 游댮                       | 1 - 8          | 游릭                                   | 游릭            | 游릭                      | https://github.com/neuralmagic/compressed-tensors |\n+| [EETQ](./eetq)                                | 游릭                       | 游댮   | 游릭        | 游댮              | 游댮         | 游댮                     | ?                       | 8              | 游릭                                   | 游릭            | 游릭                      | https://github.com/NetEase-FuXi/EETQ        |\n+| GGUF / GGML (llama.cpp)             | 游릭                       | 游릭   | 游릭        | 游댮              | 游릭                     | 游댮         | 游댮                       | 1 - 8          | 游댮                                   | [See GGUF section](../gguf)                | [See GGUF section](../gguf)                      | https://github.com/ggerganov/llama.cpp      |\n+| [GPTQ](./gptq)                                | 游댮                       | 游댮   | 游릭        | 游릭              | 游댮                     | 游댮         | 游댮                       | 2 - 3 - 4 - 8          | 游릭                                   | 游릭            | 游릭                      | https://github.com/AutoGPTQ/AutoGPTQ        |\n+| [HQQ](./hqq)                                 | 游릭                       | 游릭    | 游릭        | 游댮              | 游댮                     | 游댮         | 游릭                       | 1 - 8          | 游릭                                   | 游댮            | 游릭                      | https://github.com/mobiusml/hqq/            |\n+| [Quanto](./quanto)                              | 游릭                       | 游릭   | 游릭        | 游댮              | 游릭                     | 游댮         | 游릭                       | 2 / 4 / 8      | 游댮                                   | 游댮            | 游릭                      | https://github.com/huggingface/quanto       |\n+| [FBGEMM_FP8](./fbgemm_fp8.md)                              | 游릭                       | 游댮    | 游릭        | 游댮              | 游댮                      | 游댮         | 游댮                        | 8      | 游댮                                   | 游릭            | 游릭                      | https://github.com/pytorch/FBGEMM       |\n+| [torchao](./torchao.md)                              | 游릭                       |     | 游릭        | 游댮              | partial support (int4 weight only)       | 游댮         |                       | 4 / 8      |                                   | 游릭游댮           | 游릭                      | https://github.com/pytorch/ao       |\n \n <Tip>\n "
        },
        {
            "sha": "0c14c236d26036d7f14d49b26855719f421db66c",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/52ea4aa589324bae43dfb1b6db70335da7b68654/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52ea4aa589324bae43dfb1b6db70335da7b68654/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=52ea4aa589324bae43dfb1b6db70335da7b68654",
            "patch": "@@ -57,14 +57,14 @@ def validate_environment(self, device_map, **kwargs):\n                 raise RuntimeError(\n                     \"To use IPEX backend, you need autoawq>0.6.2. Please install the latest version or from source.\"\n                 )\n-            if (\n-                device_map is not None\n-                and isinstance(device_map, dict)\n-                and (torch.device(\"cpu\") not in device_map.values() or len(device_map.values()) > 1)\n-            ):\n+            if device_map is None:\n+                logger.warning_once(\n+                    \"You have loaded an AWQ model without setting device_map, please set 'cpu' or 'xpu' or 'auto'\"\n+                )\n+            elif isinstance(device_map, dict) and \"disk\" in device_map.values():\n                 raise ValueError(\n-                    \"You are attempting to load an IPEX version AWQ model with a device_map that contains more than CPU.\"\n-                    \" This is not supported. Please make sure only cpu in the device_map.\"\n+                    \"You are attempting to load an IPEX version AWQ model with a device_map that contains disk device.\"\n+                    \" This is not supported. Please make sure only cpu and xpu in the device_map.\"\n                 )\n         else:\n             if not torch.cuda.is_available():"
        }
    ],
    "stats": {
        "total": 40,
        "additions": 20,
        "deletions": 20
    }
}