{
    "author": "dvrogozh",
    "message": "Fix skip of test_training_gradient_checkpointing (#34723)\n\n19d58d31f has introduced a context manager to manage subtests of\r\ntest_training_gradient_checkpointing. However, test body was not\r\nmoved under \"with\" statement. Thus, while tests are correctly\r\nmarked as skipped, test bodies were still executed. In some cases,\r\nas with llama this caused attribute errors.\r\n\r\nFixes: #34722\r\nFixes: 19d58d31f (\"Add MLLama (#33703)\")\r\n\r\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>",
    "sha": "1c471fc3070a9179f32d147df40b760804f2ef4e",
    "files": [
        {
            "sha": "3ef30fc8ae552838e1ec8c78991808ce2c1b945b",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c471fc3070a9179f32d147df40b760804f2ef4e/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c471fc3070a9179f32d147df40b760804f2ef4e/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=1c471fc3070a9179f32d147df40b760804f2ef4e",
            "patch": "@@ -849,29 +849,29 @@ def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=No\n                 ):\n                     self.skipTest(reason=f\"`supports_gradient_checkpointing` is False for {model_class.__name__}.\")\n \n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            config.use_cache = False\n-            config.return_dict = True\n-            model = model_class(config)\n+                config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+                config.use_cache = False\n+                config.return_dict = True\n+                model = model_class(config)\n \n-            model.to(torch_device)\n-            model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n-            model.train()\n+                model.to(torch_device)\n+                model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n+                model.train()\n \n-            # unfreeze additional layers\n-            for p in model.parameters():\n-                p.requires_grad_(True)\n+                # unfreeze additional layers\n+                for p in model.parameters():\n+                    p.requires_grad_(True)\n \n-            optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n+                optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n \n-            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n-            loss = model(**inputs).loss\n-            loss.backward()\n-            optimizer.step()\n+                inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+                loss = model(**inputs).loss\n+                loss.backward()\n+                optimizer.step()\n \n-            for k, v in model.named_parameters():\n-                if v.requires_grad:\n-                    self.assertTrue(v.grad is not None, f\"{k} in {model_class.__name__} has no gradient!\")\n+                for k, v in model.named_parameters():\n+                    if v.requires_grad:\n+                        self.assertTrue(v.grad is not None, f\"{k} in {model_class.__name__} has no gradient!\")\n \n     def test_training(self):\n         if not self.model_tester.is_training:"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 18,
        "deletions": 18
    }
}