{
    "author": "yonigozlan",
    "message": "Uniformize kwargs for Udop processor and update docs (#33628)\n\n* Add optional kwargs and uniformize udop\r\n\r\n* cleanup Unpack\r\n\r\n* nit Udop",
    "sha": "14561209291255e51c55260306c7d00c159381a5",
    "files": [
        {
            "sha": "6f7b6cf060495ae21f2187430bbd3af95b7db881",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/14561209291255e51c55260306c7d00c159381a5/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14561209291255e51c55260306c7d00c159381a5/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=14561209291255e51c55260306c7d00c159381a5",
            "patch": "@@ -1790,7 +1790,7 @@ def forward(\n         >>> # one can use the various task prefixes (prompts) used during pre-training\n         >>> # e.g. the task prefix for DocVQA is \"Question answering. \"\n         >>> question = \"Question answering. What is the date on the form?\"\n-        >>> encoding = processor(image, question, words, boxes=boxes, return_tensors=\"pt\")\n+        >>> encoding = processor(image, question, text_pair=words, boxes=boxes, return_tensors=\"pt\")\n \n         >>> # autoregressive generation\n         >>> predicted_ids = model.generate(**encoding)"
        },
        {
            "sha": "ddd5d484a98883ca764d9fbdbc1131c5840ea611",
            "filename": "src/transformers/models/udop/processing_udop.py",
            "status": "modified",
            "additions": 91,
            "deletions": 68,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/14561209291255e51c55260306c7d00c159381a5/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14561209291255e51c55260306c7d00c159381a5/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py?ref=14561209291255e51c55260306c7d00c159381a5",
            "patch": "@@ -18,10 +18,38 @@\n \n from typing import List, Optional, Union\n \n+from transformers import logging\n+\n+from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class UdopTextKwargs(TextKwargs, total=False):\n+    word_labels: Optional[Union[List[int], List[List[int]]]]\n+    boxes: Union[List[List[int]], List[List[List[int]]]]\n+\n+\n+class UdopProcessorKwargs(ProcessingKwargs, total=False):\n+    text_kwargs: UdopTextKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"add_special_tokens\": True,\n+            \"padding\": False,\n+            \"truncation\": False,\n+            \"stride\": 0,\n+            \"return_overflowing_tokens\": False,\n+            \"return_special_tokens_mask\": False,\n+            \"return_offsets_mapping\": False,\n+            \"return_length\": False,\n+            \"verbose\": True,\n+        },\n+        \"images_kwargs\": {},\n+    }\n \n \n class UdopProcessor(ProcessorMixin):\n@@ -49,6 +77,8 @@ class UdopProcessor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"LayoutLMv3ImageProcessor\"\n     tokenizer_class = (\"UdopTokenizer\", \"UdopTokenizerFast\")\n+    # For backward compatibility. See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details.\n+    optional_call_args = [\"text_pair\"]\n \n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n@@ -57,28 +87,16 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n-        text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,\n-        boxes: Union[List[List[int]], List[List[List[int]]]] = None,\n-        word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n-        text_target: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n-        text_pair_target: Optional[\n-            Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]\n-        ] = None,\n-        add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = False,\n-        max_length: Optional[int] = None,\n-        stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_overflowing_tokens: bool = False,\n-        return_special_tokens_mask: bool = False,\n-        return_offsets_mapping: bool = False,\n-        return_length: bool = False,\n-        verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-    ) -> BatchEncoding:\n+        # The following is to capture `text_pair` argument that may be passed as a positional argument.\n+        # See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details,\n+        # or this conversation for more context: https://github.com/huggingface/transformers/pull/32544#discussion_r1720208116\n+        # This behavior is only needed for backward compatibility and will be removed in future versions.\n+        #\n+        *args,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[UdopProcessorKwargs],\n+    ) -> BatchFeature:\n         \"\"\"\n         This method first forwards the `images` argument to [`~UdopImageProcessor.__call__`]. In case\n         [`UdopImageProcessor`] was initialized with `apply_ocr` set to `True`, it passes the obtained words and\n@@ -93,6 +111,20 @@ def __call__(\n         Please refer to the docstring of the above two methods for more information.\n         \"\"\"\n         # verify input\n+        output_kwargs = self._merge_kwargs(\n+            UdopProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+            **self.prepare_and_validate_optional_call_args(*args),\n+        )\n+\n+        boxes = output_kwargs[\"text_kwargs\"].pop(\"boxes\", None)\n+        word_labels = output_kwargs[\"text_kwargs\"].pop(\"word_labels\", None)\n+        text_pair = output_kwargs[\"text_kwargs\"].pop(\"text_pair\", None)\n+        return_overflowing_tokens = output_kwargs[\"text_kwargs\"].get(\"return_overflowing_tokens\", False)\n+        return_offsets_mapping = output_kwargs[\"text_kwargs\"].get(\"return_offsets_mapping\", False)\n+        text_target = output_kwargs[\"text_kwargs\"].get(\"text_target\", None)\n+\n         if self.image_processor.apply_ocr and (boxes is not None):\n             raise ValueError(\n                 \"You cannot provide bounding boxes if you initialized the image processor with apply_ocr set to True.\"\n@@ -103,69 +135,47 @@ def __call__(\n                 \"You cannot provide word labels if you initialized the image processor with apply_ocr set to True.\"\n             )\n \n-        if return_overflowing_tokens is True and return_offsets_mapping is False:\n+        if return_overflowing_tokens and not return_offsets_mapping:\n             raise ValueError(\"You cannot return overflowing tokens without returning the offsets mapping.\")\n \n         if text_target is not None:\n             # use the processor to prepare the targets of UDOP\n             return self.tokenizer(\n-                text_target=text_target,\n-                text_pair_target=text_pair_target,\n-                add_special_tokens=add_special_tokens,\n-                padding=padding,\n-                truncation=truncation,\n-                max_length=max_length,\n-                stride=stride,\n-                pad_to_multiple_of=pad_to_multiple_of,\n-                return_token_type_ids=return_token_type_ids,\n-                return_attention_mask=return_attention_mask,\n-                return_overflowing_tokens=return_overflowing_tokens,\n-                return_special_tokens_mask=return_special_tokens_mask,\n-                return_offsets_mapping=return_offsets_mapping,\n-                return_length=return_length,\n-                verbose=verbose,\n-                return_tensors=return_tensors,\n+                **output_kwargs[\"text_kwargs\"],\n             )\n \n         else:\n             # use the processor to prepare the inputs of UDOP\n             # first, apply the image processor\n-            features = self.image_processor(images=images, return_tensors=return_tensors)\n+            features = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+            features_words = features.pop(\"words\", None)\n+            features_boxes = features.pop(\"boxes\", None)\n+\n+            output_kwargs[\"text_kwargs\"].pop(\"text_target\", None)\n+            output_kwargs[\"text_kwargs\"].pop(\"text_pair_target\", None)\n+            output_kwargs[\"text_kwargs\"][\"text_pair\"] = text_pair\n+            output_kwargs[\"text_kwargs\"][\"boxes\"] = boxes if boxes is not None else features_boxes\n+            output_kwargs[\"text_kwargs\"][\"word_labels\"] = word_labels\n \n             # second, apply the tokenizer\n             if text is not None and self.image_processor.apply_ocr and text_pair is None:\n                 if isinstance(text, str):\n                     text = [text]  # add batch dimension (as the image processor always adds a batch dimension)\n-                text_pair = features[\"words\"]\n+                output_kwargs[\"text_kwargs\"][\"text_pair\"] = features_words\n \n             encoded_inputs = self.tokenizer(\n-                text=text if text is not None else features[\"words\"],\n-                text_pair=text_pair if text_pair is not None else None,\n-                boxes=boxes if boxes is not None else features[\"boxes\"],\n-                word_labels=word_labels,\n-                add_special_tokens=add_special_tokens,\n-                padding=padding,\n-                truncation=truncation,\n-                max_length=max_length,\n-                stride=stride,\n-                pad_to_multiple_of=pad_to_multiple_of,\n-                return_token_type_ids=return_token_type_ids,\n-                return_attention_mask=return_attention_mask,\n-                return_overflowing_tokens=return_overflowing_tokens,\n-                return_special_tokens_mask=return_special_tokens_mask,\n-                return_offsets_mapping=return_offsets_mapping,\n-                return_length=return_length,\n-                verbose=verbose,\n-                return_tensors=return_tensors,\n+                text=text if text is not None else features_words,\n+                **output_kwargs[\"text_kwargs\"],\n             )\n \n             # add pixel values\n-            pixel_values = features.pop(\"pixel_values\")\n             if return_overflowing_tokens is True:\n-                pixel_values = self.get_overflowing_images(pixel_values, encoded_inputs[\"overflow_to_sample_mapping\"])\n-            encoded_inputs[\"pixel_values\"] = pixel_values\n+                features[\"pixel_values\"] = self.get_overflowing_images(\n+                    features[\"pixel_values\"], encoded_inputs[\"overflow_to_sample_mapping\"]\n+                )\n+            features.update(encoded_inputs)\n \n-            return encoded_inputs\n+            return features\n \n     # Copied from transformers.models.layoutlmv3.processing_layoutlmv3.LayoutLMv3Processor.get_overflowing_images\n     def get_overflowing_images(self, images, overflow_to_sample_mapping):\n@@ -198,7 +208,20 @@ def decode(self, *args, **kwargs):\n         \"\"\"\n         return self.tokenizer.decode(*args, **kwargs)\n \n+    def post_process_image_text_to_text(self, generated_outputs):\n+        \"\"\"\n+        Post-process the output of the model to decode the text.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+\n+        Returns:\n+            `List[str]`: The decoded text.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n+\n     @property\n-    # Copied from transformers.models.layoutlmv3.processing_layoutlmv3.LayoutLMv3Processor.model_input_names\n     def model_input_names(self):\n-        return [\"input_ids\", \"bbox\", \"attention_mask\", \"pixel_values\"]\n+        return [\"pixel_values\", \"input_ids\", \"bbox\", \"attention_mask\"]"
        },
        {
            "sha": "621b761b5f17a140faf3b2a37afcc7ab85c61077",
            "filename": "tests/models/udop/test_processor_udop.py",
            "status": "modified",
            "additions": 18,
            "deletions": 19,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/14561209291255e51c55260306c7d00c159381a5/tests%2Fmodels%2Fudop%2Ftest_processor_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14561209291255e51c55260306c7d00c159381a5/tests%2Fmodels%2Fudop%2Ftest_processor_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_processor_udop.py?ref=14561209291255e51c55260306c7d00c159381a5",
            "patch": "@@ -12,8 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import json\n-import os\n import shutil\n import tempfile\n import unittest\n@@ -34,7 +32,7 @@\n     require_torch,\n     slow,\n )\n-from transformers.utils import FEATURE_EXTRACTOR_NAME, cached_property, is_pytesseract_available, is_torch_available\n+from transformers.utils import cached_property, is_pytesseract_available, is_torch_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n@@ -55,20 +53,19 @@\n class UdopProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     tokenizer_class = UdopTokenizer\n     rust_tokenizer_class = UdopTokenizerFast\n-    maxDiff = None\n     processor_class = UdopProcessor\n+    maxDiff = None\n \n     def setUp(self):\n-        image_processor_map = {\n-            \"do_resize\": True,\n-            \"size\": 224,\n-            \"apply_ocr\": True,\n-        }\n-\n         self.tmpdirname = tempfile.mkdtemp()\n-        self.feature_extraction_file = os.path.join(self.tmpdirname, FEATURE_EXTRACTOR_NAME)\n-        with open(self.feature_extraction_file, \"w\", encoding=\"utf-8\") as fp:\n-            fp.write(json.dumps(image_processor_map) + \"\\n\")\n+        image_processor = LayoutLMv3ImageProcessor(\n+            do_resize=True,\n+            size=224,\n+            apply_ocr=True,\n+        )\n+        tokenizer = UdopTokenizer.from_pretrained(\"microsoft/udop-large\")\n+        processor = UdopProcessor(image_processor=image_processor, tokenizer=tokenizer)\n+        processor.save_pretrained(self.tmpdirname)\n \n         self.tokenizer_pretrained_name = \"microsoft/udop-large\"\n \n@@ -80,15 +77,15 @@ def setUp(self):\n     def get_tokenizer(self, **kwargs) -> PreTrainedTokenizer:\n         return self.tokenizer_class.from_pretrained(self.tokenizer_pretrained_name, **kwargs)\n \n+    def get_image_processor(self, **kwargs):\n+        return LayoutLMv3ImageProcessor.from_pretrained(self.tmpdirname, **kwargs)\n+\n     def get_rust_tokenizer(self, **kwargs) -> PreTrainedTokenizerFast:\n         return self.rust_tokenizer_class.from_pretrained(self.tokenizer_pretrained_name, **kwargs)\n \n     def get_tokenizers(self, **kwargs) -> List[PreTrainedTokenizerBase]:\n         return [self.get_tokenizer(**kwargs), self.get_rust_tokenizer(**kwargs)]\n \n-    def get_image_processor(self, **kwargs):\n-        return LayoutLMv3ImageProcessor.from_pretrained(self.tmpdirname, **kwargs)\n-\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n@@ -153,7 +150,7 @@ def test_model_input_names(self):\n         input_str = \"lower newer\"\n         image_input = self.prepare_image_inputs()\n \n-        inputs = processor(text=input_str, images=image_input)\n+        inputs = processor(images=image_input, text=input_str)\n \n         self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n \n@@ -472,7 +469,7 @@ def test_processor_case_5(self):\n             question = \"What's his name?\"\n             words = [\"hello\", \"world\"]\n             boxes = [[1, 2, 3, 4], [5, 6, 7, 8]]\n-            input_processor = processor(images[0], question, words, boxes, return_tensors=\"pt\")\n+            input_processor = processor(images[0], question, text_pair=words, boxes=boxes, return_tensors=\"pt\")\n \n             # verify keys\n             expected_keys = [\"attention_mask\", \"bbox\", \"input_ids\", \"pixel_values\"]\n@@ -488,7 +485,9 @@ def test_processor_case_5(self):\n             questions = [\"How old is he?\", \"what's the time\"]\n             words = [[\"hello\", \"world\"], [\"my\", \"name\", \"is\", \"niels\"]]\n             boxes = [[[1, 2, 3, 4], [5, 6, 7, 8]], [[3, 2, 5, 1], [6, 7, 4, 2], [3, 9, 2, 4], [1, 1, 2, 3]]]\n-            input_processor = processor(images, questions, words, boxes, padding=True, return_tensors=\"pt\")\n+            input_processor = processor(\n+                images, questions, text_pair=words, boxes=boxes, padding=True, return_tensors=\"pt\"\n+            )\n \n             # verify keys\n             expected_keys = [\"attention_mask\", \"bbox\", \"input_ids\", \"pixel_values\"]"
        }
    ],
    "stats": {
        "total": 198,
        "additions": 110,
        "deletions": 88
    }
}