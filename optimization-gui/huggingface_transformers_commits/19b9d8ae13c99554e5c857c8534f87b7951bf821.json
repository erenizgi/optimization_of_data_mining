{
    "author": "threewebcode",
    "message": "chore: fix typos in tests directory (#36785)\n\n* chore: fix typos in tests directory\n\n* chore: fix typos in tests directory\n\n* chore: fix typos in tests directory\n\n* chore: fix typos in tests directory\n\n* chore: fix typos in tests directory\n\n* chore: fix typos in tests directory\n\n* chore: fix typos in tests directory",
    "sha": "19b9d8ae13c99554e5c857c8534f87b7951bf821",
    "files": [
        {
            "sha": "7b884b99e702f6c039971deb2b53f24d5fdade2c",
            "filename": "tests/generation/test_beam_constraints.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fgeneration%2Ftest_beam_constraints.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fgeneration%2Ftest_beam_constraints.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_beam_constraints.py?ref=19b9d8ae13c99554e5c857c8534f87b7951bf821",
            "patch": "@@ -43,7 +43,7 @@ def test_input_types(self):\n             DisjunctiveConstraint([torch.LongTensor([1, 2, 4]), torch.LongTensor([1, 2, 3, 4, 5])])\n \n     def test_check_illegal_input(self):\n-        # We can't have constraints that are complete subsets of another. This leads to a preverse\n+        # We can't have constraints that are complete subsets of another. This leads to a perverse\n         # interpretation of \"constraint fulfillment\": does generating [1,2,3] fulfill the constraint?\n         # It would mean that it generated [1,2] which fulfills it, but it's in the middle of potentially\n         # fulfilling [1,2,3,4]. If we believe that [1,2,3] does fulfill the constraint, then the algorithm"
        },
        {
            "sha": "8619fe67850691aa155637e765fe5952aa55d43c",
            "filename": "tests/models/align/test_modeling_align.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_modeling_align.py?ref=19b9d8ae13c99554e5c857c8534f87b7951bf821",
            "patch": "@@ -495,7 +495,7 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # override as the `temperature` parameter initilization is different for ALIGN\n+    # override as the `temperature` parameter initialization is different for ALIGN\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -504,7 +504,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `temperature` is initilized as per the original implementation\n+                    # check if `temperature` is initialized as per the original implementation\n                     if name == \"temperature\":\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "81190d85f8521ae95b234505115d5f79567c6f48",
            "filename": "tests/models/altclip/test_modeling_altclip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py?ref=19b9d8ae13c99554e5c857c8534f87b7951bf821",
            "patch": "@@ -482,15 +482,15 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # override as the `logit_scale` parameter initilization is different for AltCLIP\n+    # override as the `logit_scale` parameter initialization is different for AltCLIP\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         configs_no_init = _config_zero_init(config)\n         for model_class in self.all_model_classes:\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "f9d0fa6438b6c4bf2ab741b88f3d92c69ab9ba82",
            "filename": "tests/models/auto/test_processor_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py?ref=19b9d8ae13c99554e5c857c8534f87b7951bf821",
            "patch": "@@ -186,7 +186,7 @@ def test_processor_from_local_directory_from_model_config(self):\n             model_config.save_pretrained(tmpdirname)\n             # copy relevant files\n             copyfile(SAMPLE_VOCAB, os.path.join(tmpdirname, \"vocab.json\"))\n-            # create emtpy sample processor\n+            # create empty sample processor\n             with open(os.path.join(tmpdirname, FEATURE_EXTRACTOR_NAME), \"w\") as f:\n                 f.write(\"{}\")\n "
        },
        {
            "sha": "d35f5978cdaa70df2a1a75549b125b2e1c164b77",
            "filename": "tests/pipelines/test_pipelines_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_common.py?ref=19b9d8ae13c99554e5c857c8534f87b7951bf821",
            "patch": "@@ -613,7 +613,7 @@ def test_load_default_pipelines_pt(self):\n         set_seed_fn = lambda: torch.manual_seed(0)  # noqa: E731\n         for task in SUPPORTED_TASKS.keys():\n             if task == \"table-question-answering\":\n-                # test table in seperate test due to more dependencies\n+                # test table in separate test due to more dependencies\n                 continue\n \n             self.check_default_pipeline(task, \"pt\", set_seed_fn, self.check_models_equal_pt)\n@@ -631,7 +631,7 @@ def test_load_default_pipelines_tf(self):\n         set_seed_fn = lambda: keras.utils.set_random_seed(0)  # noqa: E731\n         for task in SUPPORTED_TASKS.keys():\n             if task == \"table-question-answering\":\n-                # test table in seperate test due to more dependencies\n+                # test table in separate test due to more dependencies\n                 continue\n \n             self.check_default_pipeline(task, \"tf\", set_seed_fn, self.check_models_equal_tf)"
        },
        {
            "sha": "94c495b0d621c4b9a51b2d87251613a1c2bcad05",
            "filename": "tests/pipelines/test_pipelines_token_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fpipelines%2Ftest_pipelines_token_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fpipelines%2Ftest_pipelines_token_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_token_classification.py?ref=19b9d8ae13c99554e5c857c8534f87b7951bf821",
            "patch": "@@ -778,7 +778,7 @@ def test_word_heuristic_leading_space(self):\n     @require_tf\n     def test_tf_only(self):\n         model_name = \"hf-internal-testing/tiny-random-bert-tf-only\"  # This model only has a TensorFlow version\n-        # We test that if we don't specificy framework='tf', it gets detected automatically\n+        # We test that if we don't specify framework='tf', it gets detected automatically\n         token_classifier = pipeline(task=\"ner\", model=model_name)\n         self.assertEqual(token_classifier.framework, \"tf\")\n "
        },
        {
            "sha": "9b26fb60afad85a87890db5e1bfce2f2b8acef8d",
            "filename": "tests/quantization/bnb/README.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fquantization%2Fbnb%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fquantization%2Fbnb%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2FREADME.md?ref=19b9d8ae13c99554e5c857c8534f87b7951bf821",
            "patch": "@@ -13,7 +13,7 @@ The following is the recipe on how to effectively debug `bitsandbytes` integrati\n \n The following instructions are tested with 2 NVIDIA-Tesla T4 GPUs. To run successfully `bitsandbytes` you would need a 8-bit core tensor supported GPU. Note that Turing, Ampere or newer architectures - e.g. T4, RTX20s RTX30s, A40-A100, A6000 should be supported. \n \n-## Virutal envs\n+## Virtual envs\n \n ```bash\n conda create --name int8-testing python==3.8\n@@ -61,7 +61,7 @@ This happens when some Linear weights are set to the CPU when using `accelerate`\n \n Use the latest version of `accelerate` with a command such as: `pip install -U accelerate` and the problem should be solved.\n \n-### `Parameter has no attribue .CB` \n+### `Parameter has no attribute .CB` \n \n Same solution as above.\n \n@@ -71,7 +71,7 @@ Run your script by pre-pending `CUDA_LAUNCH_BLOCKING=1` and you should observe a\n \n ### `CUDA illegal memory error: an illegal memory access at line...`:\n \n-Check the CUDA verisons with:\n+Check the CUDA versions with:\n ```bash\n nvcc --version\n ```"
        },
        {
            "sha": "f8e356aedbfcce524682bb45d3ce531a28088da4",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=19b9d8ae13c99554e5c857c8534f87b7951bf821",
            "patch": "@@ -179,7 +179,7 @@ def test_memory_footprint(self):\n \n     def test_original_dtype(self):\n         r\"\"\"\n-        A simple test to check if the model succesfully stores the original dtype\n+        A simple test to check if the model successfully stores the original dtype\n         \"\"\"\n         self.assertTrue(hasattr(self.model_4bit.config, \"_pre_quantization_dtype\"))\n         self.assertFalse(hasattr(self.model_fp16.config, \"_pre_quantization_dtype\"))\n@@ -496,8 +496,8 @@ def tearDown(self):\n     def test_pipeline(self):\n         r\"\"\"\n         The aim of this test is to verify that the mixed 4bit is compatible with `pipeline` from transformers. Since\n-        we used pipline for inference speed benchmarking we want to make sure that this feature does not break anything\n-        on pipline.\n+        we used pipeline for inference speed benchmarking we want to make sure that this feature does not break anything\n+        on pipeline.\n         \"\"\"\n         # self._clear_cuda_cache()\n         self.pipe = pipeline("
        },
        {
            "sha": "fa783b3cbe224b5a800789878af49c8382b962dd",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=19b9d8ae13c99554e5c857c8534f87b7951bf821",
            "patch": "@@ -213,7 +213,7 @@ def test_quantization_config_json_serialization(self):\n \n     def test_original_dtype(self):\n         r\"\"\"\n-        A simple test to check if the model succesfully stores the original dtype\n+        A simple test to check if the model successfully stores the original dtype\n         \"\"\"\n         self.assertTrue(hasattr(self.model_8bit.config, \"_pre_quantization_dtype\"))\n         self.assertFalse(hasattr(self.model_fp16.config, \"_pre_quantization_dtype\"))\n@@ -655,8 +655,8 @@ def tearDown(self):\n     def test_pipeline(self):\n         r\"\"\"\n         The aim of this test is to verify that the mixed int8 is compatible with `pipeline` from transformers. Since\n-        we used pipline for inference speed benchmarking we want to make sure that this feature does not break anything\n-        on pipline.\n+        we used pipeline for inference speed benchmarking we want to make sure that this feature does not break anything\n+        on pipeline.\n         \"\"\"\n         # self._clear_cuda_cache()\n         self.pipe = pipeline("
        },
        {
            "sha": "386866f713447c718b2718d180aa192b9d934e8a",
            "filename": "tests/quantization/gptq/test_gptq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fquantization%2Fgptq%2Ftest_gptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fquantization%2Fgptq%2Ftest_gptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fgptq%2Ftest_gptq.py?ref=19b9d8ae13c99554e5c857c8534f87b7951bf821",
            "patch": "@@ -167,7 +167,7 @@ def test_device_and_dtype_assignment(self):\n \n     def test_original_dtype(self):\n         r\"\"\"\n-        A simple test to check if the model succesfully stores the original dtype\n+        A simple test to check if the model successfully stores the original dtype\n         \"\"\"\n         self.assertTrue(hasattr(self.quantized_model.config, \"_pre_quantization_dtype\"))\n         self.assertFalse(hasattr(self.model_fp16.config, \"_pre_quantization_dtype\"))\n@@ -261,7 +261,7 @@ def test_serialization(self):\n                 if self.device_map == \"cpu\":\n                     quant_type = \"ipex\" if is_ipex_available() else \"torch\"\n                 else:\n-                    # We expecte tritonv2 to be used here, because exllama backend doesn't support packing https://github.com/ModelCloud/GPTQModel/issues/1354\n+                    # We expect tritonv2 to be used here, because exllama backend doesn't support packing https://github.com/ModelCloud/GPTQModel/issues/1354\n                     # TODO: Remove this once GPTQModel exllama kernels supports packing\n                     quant_type = \"tritonv2\"\n                 quantized_model_from_saved = AutoModelForCausalLM.from_pretrained(\n@@ -433,7 +433,7 @@ def test_quantized_layers_type(self):\n                 \"exllamav2\",\n             )\n         else:\n-            # We expecte tritonv2 to be used here, because exllama backend doesn't support packing https://github.com/ModelCloud/GPTQModel/issues/1354\n+            # We expect tritonv2 to be used here, because exllama backend doesn't support packing https://github.com/ModelCloud/GPTQModel/issues/1354\n             # TODO: Remove this once GPTQModel exllama kernels supports packing\n             self.assertEqual(\n                 self.quantized_model.model.layers[0].self_attn.k_proj.QUANT_TYPE,\n@@ -458,7 +458,7 @@ def check_inference_correctness(self, model):\n \n     def test_generate_quality(self):\n         \"\"\"\n-        Simple test to check the quality of the model by comapring the the generated tokens with the expected tokens\n+        Simple test to check the quality of the model by comparing the the generated tokens with the expected tokens\n         \"\"\"\n         self.check_inference_correctness(self.quantized_model)\n "
        },
        {
            "sha": "ece8af7c684ce2ab12735de7ce1b9559731dca40",
            "filename": "tests/quantization/higgs/test_higgs.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py?ref=19b9d8ae13c99554e5c857c8534f87b7951bf821",
            "patch": "@@ -184,7 +184,7 @@ def test_save_pretrained_multi_gpu(self):\n             output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n             self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n-    @unittest.skip(\"This will almost surely OOM. Enable when swithed to a smaller model\")\n+    @unittest.skip(\"This will almost surely OOM. Enable when switched to a smaller model\")\n     def test_dequantize(self):\n         \"\"\"\n         Test the ability to dequantize a model"
        },
        {
            "sha": "e6d723e67851f159f653540ae781a69acaf09bc4",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19b9d8ae13c99554e5c857c8534f87b7951bf821/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=19b9d8ae13c99554e5c857c8534f87b7951bf821",
            "patch": "@@ -202,7 +202,7 @@ class TorchAoGPUTest(TorchAoTest):\n \n     def test_int4wo_offload(self):\n         \"\"\"\n-        Simple test that checks if the quantized model int4 wieght only is working properly with cpu/disk offload\n+        Simple test that checks if the quantized model int4 weight only is working properly with cpu/disk offload\n         \"\"\"\n \n         device_map_offload = {\n@@ -254,7 +254,7 @@ def test_int4wo_offload(self):\n     @require_torch_multi_gpu\n     def test_int4wo_quant_multi_gpu(self):\n         \"\"\"\n-        Simple test that checks if the quantized model int4 wieght only is working properly with multiple GPUs\n+        Simple test that checks if the quantized model int4 weight only is working properly with multiple GPUs\n         set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUS\n         \"\"\"\n "
        }
    ],
    "stats": {
        "total": 50,
        "additions": 25,
        "deletions": 25
    }
}