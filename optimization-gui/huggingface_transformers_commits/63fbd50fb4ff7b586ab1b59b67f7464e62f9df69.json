{
    "author": "RyanMullins",
    "message": "fix: dict[RopeParameters] to dict[str, RopeParameters] (#41963)",
    "sha": "63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
    "files": [
        {
            "sha": "b9892eaf8b6183d18f869636663348d019e125b1",
            "filename": "src/transformers/models/arcee/configuration_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -136,7 +136,7 @@ def __init__(\n         bos_token_id: Optional[int] = 128000,\n         eos_token_id: Optional[int] = 128001,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,"
        },
        {
            "sha": "09fbe14d291fbb9c05ae6422bfe31cdb4c83b1cd",
            "filename": "src/transformers/models/arcee/modular_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -137,7 +137,7 @@ def __init__(\n         bos_token_id: Optional[int] = 128000,\n         eos_token_id: Optional[int] = 128001,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,"
        },
        {
            "sha": "192d7877667902d44bcab84131dd1047794b7c9d",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -134,7 +134,7 @@ def __init__(\n         eos_token_id: Optional[int] = 2,\n         pretraining_tp: Optional[int] = 1,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,"
        },
        {
            "sha": "0473ad6ac40704e85e22da8fbb2949b8c02b1df7",
            "filename": "src/transformers/models/bitnet/configuration_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -117,7 +117,7 @@ def __init__(\n         tie_word_embeddings: Optional[bool] = False,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[str] = 0.0,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size"
        },
        {
            "sha": "7459346645ea5ff93a4aba680e50f8304c23ff71",
            "filename": "src/transformers/models/blt/configuration_blt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -44,7 +44,7 @@ def __init__(\n         rms_norm_eps: Optional[float] = 1e-5,\n         dropout: Optional[float] = 0.0,\n         max_position_embeddings: Optional[int] = 24576,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         hidden_act: Optional[str] = \"silu\",\n         intermediate_size: Optional[int] = 2816,\n         initializer_range: Optional[float] = 0.02,\n@@ -99,7 +99,7 @@ def __init__(\n         rms_norm_eps: Optional[float] = 1e-5,\n         dropout: Optional[float] = 0.0,\n         max_position_embeddings: Optional[int] = 24576,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         hidden_act: Optional[str] = \"silu\",\n         intermediate_size: Optional[int] = 2816,\n         initializer_range: Optional[float] = 0.02,\n@@ -150,7 +150,7 @@ def __init__(\n         rms_norm_eps: Optional[float] = 1e-5,\n         dropout: Optional[float] = 0.0,\n         max_position_embeddings: Optional[int] = 4096,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         hidden_act: Optional[str] = \"silu\",\n         intermediate_size: Optional[int] = 5632,\n         initializer_range: Optional[float] = 0.02,\n@@ -231,7 +231,7 @@ def __init__(\n         rms_norm_eps: Optional[float] = 1e-5,\n         dropout: Optional[float] = 0.0,\n         intermediate_size: Optional[int] = 2048,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         initializer_range: Optional[float] = 0.02,\n         **kwargs,\n     ):\n@@ -356,7 +356,7 @@ def __init__(\n         global_config: Optional[dict] = None,\n         tie_word_embeddings: Optional[bool] = False,\n         initializer_range: Optional[float] = 0.02,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         **kwargs,\n     ):\n         # Basic model configuration"
        },
        {
            "sha": "bfa8a9f334696ae0f6019626dbabdf38b7d9dd65",
            "filename": "src/transformers/models/chameleon/configuration_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -203,7 +203,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[int] = False,\n         attention_dropout: Optional[float] = 0.0,\n         model_parallel_size: Optional[int] = 1,"
        },
        {
            "sha": "18afd5fd32e954fe523f84a883239fff9782b323",
            "filename": "src/transformers/models/cohere/configuration_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -139,7 +139,7 @@ def __init__(\n         bos_token_id: Optional[int] = 5,\n         eos_token_id: Optional[int] = 255001,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         use_qk_norm: Optional[bool] = False,"
        },
        {
            "sha": "910dc6dcb80a40065130bfa56f14901104f4756d",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -138,7 +138,7 @@ def __init__(\n         bos_token_id: Optional[int] = 5,\n         eos_token_id: Optional[int] = 255001,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         sliding_window: Optional[int] = 4096,"
        },
        {
            "sha": "af9fa871f3919eb66ca43ca7846c8bc9b196231f",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -162,7 +162,7 @@ def __init__(\n         bos_token_id: Optional[int] = 5,\n         eos_token_id: Optional[int] = 255001,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         sliding_window: Optional[int] = 4096,"
        },
        {
            "sha": "ce1ad2dd5993924398678fe6771f8d2ebb28988d",
            "filename": "src/transformers/models/csm/configuration_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -122,7 +122,7 @@ def __init__(\n         pad_token_id: Optional[int] = None,\n         bos_token_id: Optional[int] = None,\n         eos_token_id: Optional[int] = None,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,\n@@ -291,7 +291,7 @@ def __init__(\n         eos_token_id: Optional[int] = None,\n         audio_token_id: Optional[int] = 128002,\n         audio_eos_token_id: Optional[int] = 128003,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,"
        },
        {
            "sha": "82182c49bd3fbcb0fbfeb67902ca8d518b8cd09e",
            "filename": "src/transformers/models/dbrx/configuration_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -189,7 +189,7 @@ def __init__(\n         use_cache: Optional[bool] = True,\n         initializer_range: Optional[float] = 0.02,\n         output_router_logits: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         **kwargs: Any,\n     ):\n         if attn_config is None:"
        },
        {
            "sha": "aad76507d3a6de4869b43ee68d5622c59c2945a0",
            "filename": "src/transformers/models/deepseek_v2/configuration_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -154,7 +154,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,"
        },
        {
            "sha": "7e60d5c858b3e90761ccee1696232d4abafbb716",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -167,7 +167,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,"
        },
        {
            "sha": "f90c5e175ba5d031cc7bf530081f12df5c55163c",
            "filename": "src/transformers/models/deepseek_v3/configuration_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -186,7 +186,7 @@ def __init__(\n         eos_token_id: Optional[int] = 1,\n         pretraining_tp: Optional[int] = 1,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         rope_interleave: Optional[bool] = True,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,"
        },
        {
            "sha": "cbfb5fea51609306d59f22c5ab1ede776d026b89",
            "filename": "src/transformers/models/diffllama/configuration_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -118,7 +118,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         lambda_std_dev: Optional[float] = 0.1,"
        },
        {
            "sha": "db2d3014d9783c1dd195cebb2597daf563fdb38f",
            "filename": "src/transformers/models/doge/configuration_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -148,7 +148,7 @@ def __init__(\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n         max_position_embeddings: Optional[int] = 2048,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         num_attention_heads: Optional[int] = 8,\n         num_key_value_heads: Optional[int] = None,\n         attention_bias: Optional[bool] = False,"
        },
        {
            "sha": "fd71f7479f6bd6263b145bcc1bbeca63e4379635",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -176,7 +176,7 @@ def __init__(\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n         max_position_embeddings: Optional[int] = 2048,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         num_attention_heads: Optional[int] = 8,\n         num_key_value_heads: Optional[int] = None,\n         attention_bias: Optional[bool] = False,"
        },
        {
            "sha": "a5755ad0a45ff31d2742083f1a5ca6c40fdc4d54",
            "filename": "src/transformers/models/dots1/configuration_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -159,7 +159,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-6,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         routed_scaling_factor: Optional[float] = 1.0,"
        },
        {
            "sha": "346eff50e9f23e6889e0013d736f13ff0de1040e",
            "filename": "src/transformers/models/ernie4_5/configuration_ernie4_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -125,7 +125,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         use_bias: Optional[bool] = False,\n         head_dim: Optional[int] = 128,\n         **kwargs,"
        },
        {
            "sha": "19ed1853db3368e91a7455415e092a513f64215d",
            "filename": "src/transformers/models/ernie4_5_moe/configuration_ernie4_5_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -161,7 +161,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-5,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         use_bias: Optional[int] = False,\n         moe_intermediate_size: Optional[int] = 1536,\n         moe_k: Optional[int] = 6,"
        },
        {
            "sha": "4dab03fb931456ceea301811d1e299408cf49051",
            "filename": "src/transformers/models/evolla/configuration_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -203,7 +203,7 @@ def __init__(\n         hidden_act: Optional[str] = \"silu\",  # llama activation function\n         max_position_embeddings: Optional[int] = 8192,  # llama rope max length\n         rms_norm_eps: Optional[int] = 1e-05,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,"
        },
        {
            "sha": "a968bcc6f07b8dccaff8ab82ef7e8c1c2230ed66",
            "filename": "src/transformers/models/exaone4/configuration_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -143,7 +143,7 @@ def __init__(\n         bos_token_id: Optional[int] = 0,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_dropout: Optional[float] = 0.0,\n         sliding_window: Optional[int] = 4096,\n         sliding_window_pattern: Optional[int] = 4,"
        },
        {
            "sha": "4ddc3466ffd96ed3d3d49d8dddb97a93b9e91399",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -176,7 +176,7 @@ def __init__(\n         bos_token_id: Optional[int] = 0,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_dropout: Optional[float] = 0.0,\n         sliding_window: Optional[int] = 4096,\n         sliding_window_pattern: Optional[int] = 4,"
        },
        {
            "sha": "3e7b437954dcf70d1010a1dda75d9f608fc586ae",
            "filename": "src/transformers/models/falcon/configuration_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -128,7 +128,7 @@ def __init__(\n         parallel_attn: Optional[bool] = True,\n         bias: Optional[bool] = False,\n         max_position_embeddings: Optional[int] = 2048,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         bos_token_id: Optional[int] = 11,\n         eos_token_id: Optional[int] = 11,\n         ffn_hidden_size: Optional[int] = None,"
        },
        {
            "sha": "6ba590f1502581410a371bc726f995e9df159ffb",
            "filename": "src/transformers/models/falcon_h1/configuration_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -164,7 +164,7 @@ def __init__(\n         mamba_norm_before_gate: Optional[bool] = True,\n         mamba_rms_norm: Optional[bool] = False,\n         projectors_bias: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         lm_head_multiplier: Optional[float] = 1.0,\n         embedding_multiplier: Optional[float] = 1.0,\n         mlp_multipliers: Optional[int] = None,"
        },
        {
            "sha": "515301b93c0c20f118be6e0c9ed9d3a12a936fbe",
            "filename": "src/transformers/models/flex_olmo/configuration_flex_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -141,7 +141,7 @@ def __init__(\n         bos_token_id: Optional[int] = None,\n         eos_token_id: Optional[int] = 100257,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         num_experts_per_tok: Optional[int] = 5,"
        },
        {
            "sha": "f6cee224c0ee80649b39f33f1679c507022db8aa",
            "filename": "src/transformers/models/flex_olmo/modular_flex_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -152,7 +152,7 @@ def __init__(\n         bos_token_id: Optional[int] = None,\n         eos_token_id: Optional[int] = 100257,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         num_experts_per_tok: Optional[int] = 5,"
        },
        {
            "sha": "bbe4a5ec22d8109017b7a731908d38ab14a728c4",
            "filename": "src/transformers/models/fuyu/configuration_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -118,7 +118,7 @@ def __init__(\n         layer_norm_eps: Optional[int] = 1e-5,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         qk_layernorm: Optional[bool] = True,\n         hidden_dropout: Optional[float] = 0.0,\n         attention_dropout: Optional[float] = 0.0,"
        },
        {
            "sha": "a2c6ac12f0089a35387faaa0b83257e260e9bb54",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -131,7 +131,7 @@ def __init__(\n         eos_token_id: Optional[int] = 1,\n         bos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         use_bidirectional_attention: Optional[bool] = None,"
        },
        {
            "sha": "aa64cc9e63e882bd3031f90af2fdb2c8660d9848",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -158,7 +158,7 @@ def __init__(\n         eos_token_id: Optional[int] = 1,\n         bos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         use_bidirectional_attention: Optional[bool] = None,"
        },
        {
            "sha": "460fb7000354c31a52cb549e2077b8717acba718",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -142,7 +142,7 @@ def __init__(\n         eos_token_id: Optional[int] = 1,\n         bos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         query_pre_attn_scalar: Optional[int] = 256,"
        },
        {
            "sha": "4e36cc22e030b7a579e8e7b37711b38455c6df96",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -171,7 +171,7 @@ def __init__(\n         eos_token_id: Optional[int] = 1,\n         bos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         query_pre_attn_scalar: Optional[int] = 256,"
        },
        {
            "sha": "796822cf4e37b50fadc0b1c0221d2ff859c1fe14",
            "filename": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -177,7 +177,7 @@ def __init__(\n         pad_token_id: int = 0,\n         eos_token_id: int = 1,\n         bos_token_id: int = 2,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: bool = False,\n         attention_dropout: float = 0.0,\n         sliding_window: int = 512,"
        },
        {
            "sha": "6d431e9acc550629ce20f25c08dfe7cee389fe49",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -187,7 +187,7 @@ def __init__(\n         pad_token_id: int = 0,\n         eos_token_id: int = 1,\n         bos_token_id: int = 2,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: bool = False,\n         attention_dropout: float = 0.0,\n         sliding_window: int = 512,"
        },
        {
            "sha": "e0d2c3d6492a46af67dac86c481be2420c5a217a",
            "filename": "src/transformers/models/glm/configuration_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -121,7 +121,7 @@ def __init__(\n         rms_norm_eps: Optional[float] = 0.00000015625,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         pad_token_id: Optional[int] = 151329,\n         eos_token_id: Optional[list[int]] = [151329, 151336, 151338],\n         bos_token_id: Optional[int] = None,"
        },
        {
            "sha": "43e6323b0060a4d46fad4934f3995c588c7129d6",
            "filename": "src/transformers/models/glm4/configuration_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -122,7 +122,7 @@ def __init__(\n         rms_norm_eps: Optional[float] = 0.00000015625,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         pad_token_id: Optional[int] = 151329,\n         eos_token_id: Optional[list[int]] = [151329, 151336, 151338],\n         bos_token_id: Optional[int] = None,"
        },
        {
            "sha": "33d9afd756e5c49f0496770af58b401606e50358",
            "filename": "src/transformers/models/glm4_moe/configuration_glm4_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -152,7 +152,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-5,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         moe_intermediate_size: Optional[int] = 1408,"
        },
        {
            "sha": "471d06d69ff9d59dca8db7488ae659cbc07b89a1",
            "filename": "src/transformers/models/glm4_moe/modular_glm4_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -166,7 +166,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-5,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         moe_intermediate_size: Optional[int] = 1408,"
        },
        {
            "sha": "c8f2ef75ca71524615e3bb149ca27ae7aa5cde05",
            "filename": "src/transformers/models/glm4v/configuration_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -220,7 +220,7 @@ def __init__(\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         image_token_id: Optional[int] = None,\n         video_token_id: Optional[int] = None,\n         **kwargs,"
        },
        {
            "sha": "8ae513b63d449309010e2057241a708592a74550",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -257,7 +257,7 @@ def __init__(\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         image_token_id: Optional[int] = None,\n         video_token_id: Optional[int] = None,\n         **kwargs,"
        },
        {
            "sha": "05a9a58089dd8df10b15d0f9989c9ea3d12b3fac",
            "filename": "src/transformers/models/glm4v_moe/configuration_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -242,7 +242,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-5,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = True,\n         attention_dropout: Optional[float] = 0.0,\n         moe_intermediate_size: Optional[int] = 1408,"
        },
        {
            "sha": "9e7557c9ecf55be538aeb4bab01aa6d14c0074ec",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -183,7 +183,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-5,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = True,\n         attention_dropout: Optional[float] = 0.0,\n         moe_intermediate_size: Optional[int] = 1408,"
        },
        {
            "sha": "744e0316146c4af0b59709b62607a65c35df9424",
            "filename": "src/transformers/models/gpt_neox/configuration_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -131,7 +131,7 @@ def __init__(\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = False,\n         use_parallel_residual: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = True,\n         **kwargs,\n     ):"
        },
        {
            "sha": "409232145f2a53ddf5f75eb376b8d382b724ccad",
            "filename": "src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -100,7 +100,7 @@ def __init__(\n         use_cache: Optional[bool] = True,\n         bos_token_id: Optional[int] = 31996,\n         eos_token_id: Optional[int] = 31999,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_dropout: Optional[float] = 0.1,\n         hidden_dropout: Optional[float] = 0.0,\n         **kwargs,"
        },
        {
            "sha": "97d3eca0aafe42bf9c61d11b0e870b00f446de1b",
            "filename": "src/transformers/models/granite/configuration_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -141,7 +141,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,"
        },
        {
            "sha": "98460ec8a363f74c672b5b0a7d0ff15fbd6fce34",
            "filename": "src/transformers/models/granitemoe/configuration_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -130,7 +130,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         embedding_multiplier: Optional[float] = 1.0,"
        },
        {
            "sha": "9a58272ec4287b518ef779b5a61af52e88d31ee3",
            "filename": "src/transformers/models/granitemoehybrid/configuration_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -147,7 +147,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         embedding_multiplier: Optional[float] = 1.0,"
        },
        {
            "sha": "b94545710e352ea67f04fa9a8c38bdaf0c184683",
            "filename": "src/transformers/models/granitemoeshared/configuration_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -132,7 +132,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         embedding_multiplier: Optional[float] = 1.0,"
        },
        {
            "sha": "3f3ee841991f99965cb18cddc6520690ec08607a",
            "filename": "src/transformers/models/helium/configuration_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -124,7 +124,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-8,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         pad_token_id: Optional[int] = 3,\n         eos_token_id: Optional[int] = 2,\n         bos_token_id: Optional[int] = 1,"
        },
        {
            "sha": "3dfa5388d1f7f1fdfb8bc1239d8d3338bdcd04e2",
            "filename": "src/transformers/models/hunyuan_v1_dense/configuration_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -116,7 +116,7 @@ def __init__(\n         eod_token_id: Optional[int] = 3,\n         pretraining_tp: Optional[int] = 1,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         head_dim: Optional[int] = None,"
        },
        {
            "sha": "5ee86b218ae0cdbd74e1bb4d9648216796028afb",
            "filename": "src/transformers/models/hunyuan_v1_moe/configuration_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -127,7 +127,7 @@ def __init__(\n         sep_token_id: Optional[int] = 4,\n         pretraining_tp: Optional[int] = 1,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         num_experts: Union[int, list] = 1,"
        },
        {
            "sha": "43a7b069a32e2d65a48ba6ea0ed318d1381a803e",
            "filename": "src/transformers/models/jetmoe/configuration_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -119,7 +119,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         rms_norm_eps: Optional[int] = 1e-6,\n         initializer_range: Optional[float] = 0.01,\n         attention_dropout: Optional[float] = 0.0,"
        },
        {
            "sha": "05c901d96dd4eab5562a83a54dda6e65f293c91b",
            "filename": "src/transformers/models/kyutai_speech_to_text/configuration_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -129,7 +129,7 @@ def __init__(\n         num_attention_heads: Optional[int] = 32,\n         num_key_value_heads: Optional[int] = None,\n         max_position_embeddings: Optional[int] = 750,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         hidden_act: Optional[str] = \"silu\",\n         head_dim: Optional[int] = None,\n         initializer_range: Optional[float] = 0.02,"
        },
        {
            "sha": "6ee32698cc856c2daae49a0ed139668531d294c3",
            "filename": "src/transformers/models/lfm2/configuration_lfm2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -117,7 +117,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         conv_bias: Optional[bool] = False,\n         conv_L_cache: Optional[int] = 3,\n         block_multiple_of: Optional[int] = 256,"
        },
        {
            "sha": "add6c8ee2f74c04a9d3b86f51c2265de821bc8e4",
            "filename": "src/transformers/models/llama/configuration_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -143,7 +143,7 @@ def __init__(\n         eos_token_id: Optional[int] = 2,\n         pretraining_tp: Optional[int] = 1,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,"
        },
        {
            "sha": "a37301a177412277a24b0d7a3090bce6d9fbed04",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -104,7 +104,7 @@ def __init__(\n         multi_modal_projector_bias: Optional[bool] = False,\n         projector_dropout: Optional[float] = 0.0,\n         attention_dropout: Optional[float] = 0.0,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         **kwargs,\n     ):\n         self.hidden_size = hidden_size\n@@ -290,7 +290,7 @@ def __init__(\n         output_router_logits=False,\n         router_aux_loss_coef=0.001,\n         router_jitter_noise=0.0,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         no_rope_layers=None,\n         no_rope_layer_interval=4,\n         attention_chunk_size=8192,"
        },
        {
            "sha": "6163a0cad7852354b7e7528f3ec31e96496fd093",
            "filename": "src/transformers/models/longcat_flash/configuration_longcat_flash.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -157,7 +157,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         ffn_hidden_size: Optional[int] = 12288,"
        },
        {
            "sha": "5453817e3ea4ea3f125a8c996b96a47506546662",
            "filename": "src/transformers/models/mimi/configuration_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -179,7 +179,7 @@ def __init__(\n         norm_eps: Optional[int] = 1e-5,\n         use_cache: Optional[bool] = False,\n         use_streaming: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         sliding_window: Optional[int] = 250,\n         attention_dropout: Optional[float] = 0.0,\n         layer_scale_initial_scale: Optional[float] = 0.01,"
        },
        {
            "sha": "b99a61a277ea80a96d111f43aca5bb7c3356b0c1",
            "filename": "src/transformers/models/minimax/configuration_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -176,7 +176,7 @@ def __init__(\n         output_router_logits: Optional[bool] = False,\n         router_aux_loss_coef: Optional[float] = 0.001,\n         router_jitter_noise: Optional[float] = 0.0,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         layer_types: Optional[list[str]] = None,\n         block_size: Optional[int] = 256,\n         full_attn_alpha_factor: Optional[int] = 1,"
        },
        {
            "sha": "d1bbb96bb5c1b74231c74dce5437459a55727127",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -200,7 +200,7 @@ def __init__(\n         output_router_logits: Optional[bool] = False,\n         router_aux_loss_coef: Optional[float] = 0.001,\n         router_jitter_noise: Optional[float] = 0.0,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         layer_types: Optional[list[str]] = None,\n         block_size: Optional[int] = 256,\n         full_attn_alpha_factor: Optional[int] = 1,"
        },
        {
            "sha": "0fac55d26e2a89c4fc7d0dda68b6ab61f0dfc88f",
            "filename": "src/transformers/models/mistral/configuration_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -136,7 +136,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         sliding_window: Optional[int] = 4096,\n         attention_dropout: Optional[float] = 0.0,\n         **kwargs,"
        },
        {
            "sha": "6784b7eb5f195edae2f2bc183f4fe1ce5ae44d12",
            "filename": "src/transformers/models/mixtral/configuration_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -158,7 +158,7 @@ def __init__(\n         output_router_logits: Optional[bool] = False,\n         router_aux_loss_coef: Optional[float] = 0.001,\n         router_jitter_noise: Optional[float] = 0.0,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size"
        },
        {
            "sha": "b3a045ae324a330da77f2ef0a22e1333ee660d8b",
            "filename": "src/transformers/models/modernbert/configuration_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -154,7 +154,7 @@ def __init__(\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         layer_types: Optional[list[str]] = None,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         local_attention: Optional[int] = 128,\n         embedding_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,"
        },
        {
            "sha": "131a01e6db5c5c3c37d1b608fc6adcc60be6e3d6",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -181,7 +181,7 @@ def __init__(\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         layer_types: Optional[list[str]] = None,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         local_attention: Optional[int] = 128,\n         embedding_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,"
        },
        {
            "sha": "be60950fa59373f90eec4e7d885be836c0857633",
            "filename": "src/transformers/models/modernbert_decoder/configuration_modernbert_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fconfiguration_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fconfiguration_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fconfiguration_modernbert_decoder.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -154,7 +154,7 @@ def __init__(\n         local_attention: Optional[int] = 128,\n         global_attn_every_n_layers: Optional[int] = 3,\n         layer_types: Optional[list[str]] = None,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         **kwargs,\n     ):\n         super().__init__("
        },
        {
            "sha": "e7935b9f21593db6192727ec6964addc2118dfa3",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -174,7 +174,7 @@ def __init__(\n         local_attention: Optional[int] = 128,\n         global_attn_every_n_layers: Optional[int] = 3,\n         layer_types: Optional[list[str]] = None,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         **kwargs,\n     ):\n         super().__init__("
        },
        {
            "sha": "e04909e1f7eb42265861ffa48858027b29b979aa",
            "filename": "src/transformers/models/moonshine/configuration_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -141,7 +141,7 @@ def __init__(\n         initializer_range: Optional[float] = 0.02,\n         decoder_start_token_id: Optional[int] = 1,\n         use_cache: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         partial_rotary_factor: Optional[float] = 0.9,\n         is_encoder_decoder: Optional[bool] = True,\n         attention_bias: Optional[bool] = False,"
        },
        {
            "sha": "bb66a7916f00898e4624ac66302249f445974572",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -164,7 +164,7 @@ def __init__(\n         initializer_range: Optional[float] = 0.02,\n         decoder_start_token_id: Optional[int] = 1,\n         use_cache: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         partial_rotary_factor: Optional[float] = 0.9,\n         is_encoder_decoder: Optional[bool] = True,\n         attention_bias: Optional[bool] = False,"
        },
        {
            "sha": "fea1a7cff98521bdbb9deb6c7153f9cd89f449c4",
            "filename": "src/transformers/models/moshi/configuration_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -252,7 +252,7 @@ def __init__(\n         num_key_value_heads: Optional[int] = None,\n         audio_vocab_size: Optional[int] = None,\n         max_position_embeddings: Optional[int] = 3000,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         hidden_act: Optional[str] = \"silu\",\n         head_dim: Optional[int] = None,\n         initializer_range: Optional[float] = 0.02,"
        },
        {
            "sha": "c5f888ac6d36af41885f0c75fc93f07726b0886b",
            "filename": "src/transformers/models/nemotron/configuration_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -122,7 +122,7 @@ def __init__(\n         bos_token_id: Optional[int] = 2,\n         eos_token_id: Optional[int] = 3,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         partial_rotary_factor: Optional[float] = 0.5,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,"
        },
        {
            "sha": "f01e33ead00a79bb7907cf754ce352a3cd1f4df2",
            "filename": "src/transformers/models/olmo/configuration_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -134,7 +134,7 @@ def __init__(\n         bos_token_id: Optional[int] = None,\n         eos_token_id: Optional[int] = 50279,\n         tie_word_embeddings: Optional[int] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         clip_qkv: Optional[bool] = None,"
        },
        {
            "sha": "3ba97d4f162bdab4771bfba388422bf8a19ff97d",
            "filename": "src/transformers/models/olmo2/configuration_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -135,7 +135,7 @@ def __init__(\n         bos_token_id: Optional[int] = None,\n         eos_token_id: Optional[int] = 50279,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         rms_norm_eps: Optional[int] = 1e-5,"
        },
        {
            "sha": "12705dce6e8c8fab7dfdda0ec0d40d43373eb073",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -150,7 +150,7 @@ def __init__(\n         bos_token_id: Optional[int] = None,\n         eos_token_id: Optional[int] = 50279,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         rms_norm_eps: Optional[int] = 1e-5,"
        },
        {
            "sha": "6e3f5594cbb574e3ed06289b7013fc5b4220e676",
            "filename": "src/transformers/models/olmo3/configuration_olmo3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -135,7 +135,7 @@ def __init__(\n         bos_token_id: Optional[int] = None,\n         eos_token_id: Optional[int] = 50279,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         rms_norm_eps: Optional[float] = 1e-5,"
        },
        {
            "sha": "d8bec6e9f15db8c3081ab6861db503ea372eb107",
            "filename": "src/transformers/models/olmo3/modular_olmo3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -151,7 +151,7 @@ def __init__(\n         bos_token_id: Optional[int] = None,\n         eos_token_id: Optional[int] = 50279,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         rms_norm_eps: Optional[float] = 1e-5,"
        },
        {
            "sha": "511d7968fb78d6166e5c36ad300fde3c44c2daee",
            "filename": "src/transformers/models/olmoe/configuration_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -122,7 +122,7 @@ def __init__(\n         bos_token_id: Optional[int] = None,\n         eos_token_id: Optional[int] = 50279,\n         tie_word_embeddings: Optional[int] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         clip_qkv: Optional[bool] = None,"
        },
        {
            "sha": "f9dbe11580b2ddd53edd43e2b058f3b23c593a57",
            "filename": "src/transformers/models/persimmon/configuration_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -98,7 +98,7 @@ def __init__(\n         layer_norm_eps: Optional[int] = 1e-5,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         qk_layernorm: Optional[bool] = True,\n         hidden_dropout: Optional[float] = 0.0,\n         attention_dropout: Optional[float] = 0.0,"
        },
        {
            "sha": "5476cb1b6c7c7251ef4f9fd8db2c66133b16d919",
            "filename": "src/transformers/models/phi/configuration_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -137,7 +137,7 @@ def __init__(\n         layer_norm_eps: Optional[int] = 1e-5,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         partial_rotary_factor: Optional[float] = 0.5,\n         qk_layernorm: Optional[bool] = False,\n         bos_token_id: Optional[int] = 1,"
        },
        {
            "sha": "35eb2df30c9d2aefe492a532eb15de6d9ebc5b96",
            "filename": "src/transformers/models/phi3/configuration_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -139,7 +139,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-5,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         partial_rotary_factor: Optional[float] = 1.0,\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 32000,"
        },
        {
            "sha": "46c104d027a7ef2fbd7da54dca6a91a41c122ce1",
            "filename": "src/transformers/models/phi4_multimodal/configuration_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -366,7 +366,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-5,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         partial_rotary_factor: Optional[int] = 1,\n         bos_token_id: Optional[int] = 199999,\n         eos_token_id: Optional[list[int]] = [199999, 200020],"
        },
        {
            "sha": "9095c4375c7e2ed0f81f77990a9cfe9dfa03856f",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -388,7 +388,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-5,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         partial_rotary_factor: Optional[int] = 1,\n         bos_token_id: Optional[int] = 199999,\n         eos_token_id: Optional[list[int]] = [199999, 200020],"
        },
        {
            "sha": "f7a9b528211f22eb751b3d4fb8eb4de3eb582aa4",
            "filename": "src/transformers/models/phimoe/configuration_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -128,7 +128,7 @@ def __init__(\n         bos_token_id: Optional[int] = 1,\n         eos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[int] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         sliding_window: Optional[int] = None,\n         attention_dropout: Optional[float] = 0.0,\n         num_experts_per_tok: Optional[int] = 2,"
        },
        {
            "sha": "62c179b20edc95c2fbd5149ead871db285d93c66",
            "filename": "src/transformers/models/pixtral/configuration_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -86,7 +86,7 @@ def __init__(\n         patch_size: Optional[int] = 16,\n         hidden_act: Optional[str] = \"gelu\",\n         attention_dropout: Optional[float] = 0.0,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         initializer_range: Optional[float] = 0.02,\n         **kwargs,\n     ):"
        },
        {
            "sha": "bda8bb8abfc75f7efcd79c8a4e42e9540a1c8bdc",
            "filename": "src/transformers/models/qwen2/configuration_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -129,7 +129,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-6,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         use_sliding_window: Optional[bool] = False,\n         sliding_window: Optional[int] = 4096,\n         max_window_layers: Optional[int] = 28,"
        },
        {
            "sha": "af96e9a3163f2a2c713ce0164a589f24fb420059",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -322,7 +322,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-6,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         use_sliding_window: Optional[bool] = False,\n         sliding_window: Optional[int] = 32768,\n         max_window_layers: Optional[int] = 28,\n@@ -650,7 +650,7 @@ def __init__(\n         sliding_window=32768,\n         max_window_layers=28,\n         attention_dropout=0.0,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         position_id_per_seconds=25,\n         seconds_per_chunk=2,\n         audio_start_token_id=151647,\n@@ -781,7 +781,7 @@ def __init__(\n         ff_mult=2,\n         emb_dim=512,\n         head_dim=64,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         max_position_embeddings=32768,\n         block_size=24,\n         look_ahead_layers=[10],"
        },
        {
            "sha": "329e1b798dd6b124a37f380e253b3437316ebde1",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -355,7 +355,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-6,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         use_sliding_window: Optional[bool] = False,\n         sliding_window: Optional[int] = 32768,\n         max_window_layers: Optional[int] = 28,\n@@ -683,7 +683,7 @@ def __init__(\n         sliding_window=32768,\n         max_window_layers=28,\n         attention_dropout=0.0,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         position_id_per_seconds=25,\n         seconds_per_chunk=2,\n         audio_start_token_id=151647,\n@@ -814,7 +814,7 @@ def __init__(\n         ff_mult=2,\n         emb_dim=512,\n         head_dim=64,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         max_position_embeddings=32768,\n         block_size=24,\n         look_ahead_layers=[10],"
        },
        {
            "sha": "5469b3226f3bb44156e2875714fc7ca52539af02",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -179,7 +179,7 @@ def __init__(\n         max_window_layers: Optional[int] = 80,\n         layer_types: Optional[list[str]] = None,\n         attention_dropout: Optional[float] = 0.0,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size"
        },
        {
            "sha": "256d663d3114a296bc1013d31df87a80a491ca9d",
            "filename": "src/transformers/models/qwen2_moe/configuration_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -150,7 +150,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-6,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         use_sliding_window: Optional[bool] = False,\n         sliding_window: Optional[int] = 4096,\n         max_window_layers: Optional[int] = 28,"
        },
        {
            "sha": "62fb4a815a4cf64a68591e41e6a112273cf63fe4",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -167,7 +167,7 @@ def __init__(\n         max_window_layers: Optional[int] = 80,\n         layer_types: Optional[list[str]] = None,\n         attention_dropout: Optional[float] = 0.0,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size"
        },
        {
            "sha": "a1cf6a1ea861519eee72d0b2988c914e1d879204",
            "filename": "src/transformers/models/qwen3/configuration_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -134,7 +134,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-6,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         use_sliding_window: Optional[bool] = False,\n         sliding_window: Optional[int] = 4096,"
        },
        {
            "sha": "5043a3f38a070742068bf5fb586c082b6e8a98c1",
            "filename": "src/transformers/models/qwen3_moe/configuration_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -148,7 +148,7 @@ def __init__(\n         rms_norm_eps: Optional[int] = 1e-6,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         use_sliding_window: Optional[bool] = False,\n         sliding_window: Optional[int] = 4096,"
        },
        {
            "sha": "1e5df811d86626896b5d6622328cb23a8550aaa2",
            "filename": "src/transformers/models/qwen3_next/configuration_qwen3_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -165,7 +165,7 @@ def __init__(\n         rms_norm_eps: Optional[float] = 1e-6,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         partial_rotary_factor: Optional[float] = 0.25,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,"
        },
        {
            "sha": "4c2b86d4da20c1e59e6a09ee8c7e6cf3a03486d1",
            "filename": "src/transformers/models/qwen3_omni_moe/configuration_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -295,7 +295,7 @@ def __init__(\n         rms_norm_eps: Optional[float] = 1e-6,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         sliding_window: Optional[int] = None,\n         attention_dropout: Optional[int] = 0,\n@@ -738,7 +738,7 @@ def __init__(\n         rms_norm_eps: Optional[float] = 0.000001,\n         use_cache: Optional[int] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         sliding_window: Optional[int] = None,\n         attention_dropout: Optional[int] = 0,"
        },
        {
            "sha": "7b96007aa6ef04be9644930450188928573bef5b",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -169,7 +169,7 @@ def __init__(\n         rms_norm_eps: Optional[float] = 1e-6,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         sliding_window: Optional[int] = None,\n         attention_dropout: Optional[int] = 0,\n@@ -380,7 +380,7 @@ def __init__(\n         rms_norm_eps: Optional[float] = 0.000001,\n         use_cache: Optional[int] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         sliding_window: Optional[int] = None,\n         attention_dropout: Optional[int] = 0,"
        },
        {
            "sha": "546a3da5bb7b007b158afb8ea3053966225f973d",
            "filename": "src/transformers/models/qwen3_vl/configuration_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -146,7 +146,7 @@ def __init__(\n         rms_norm_eps: Optional[float] = 1e-6,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         **kwargs,"
        },
        {
            "sha": "7758a23e2970efa1be0ece5506fb83b882110a60",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -187,7 +187,7 @@ def __init__(\n         rms_norm_eps: Optional[float] = 1e-6,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         **kwargs,"
        },
        {
            "sha": "130044ee099d2a5ceb22f0c630ced86fdce1e5a0",
            "filename": "src/transformers/models/recurrent_gemma/configuration_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconfiguration_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconfiguration_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconfiguration_recurrent_gemma.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -120,7 +120,7 @@ def __init__(\n         bos_token_id: Optional[int] = 2,\n         hidden_activation: Optional[str] = \"gelu_pytorch_tanh\",\n         partial_rotary_factor: Optional[float] = 0.5,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         block_types: Optional[list[str]] = (\"recurrent\", \"recurrent\", \"attention\"),\n         attention_dropout: Optional[float] = 0.0,\n         num_key_value_heads: Optional[int] = None,"
        },
        {
            "sha": "240cb03bac771ea6c24d0cfa55b2eee4d8f25863",
            "filename": "src/transformers/models/seed_oss/configuration_seed_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fconfiguration_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fconfiguration_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fconfiguration_seed_oss.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -139,7 +139,7 @@ def __init__(\n         eos_token_id: Optional[int] = 2,\n         pretraining_tp: Optional[int] = 1,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = True,\n         attention_out_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.1,"
        },
        {
            "sha": "04e8e78e575c4b1058d82c66446edb73dc94d9f7",
            "filename": "src/transformers/models/smollm3/configuration_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -140,7 +140,7 @@ def __init__(\n         pad_token_id: Optional[int] = 128004,\n         bos_token_id: Optional[int] = 128000,\n         eos_token_id: Optional[int] = 128001,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         use_sliding_window: Optional[bool] = False,\n         sliding_window: Optional[int] = None,\n         no_rope_layers: Optional[int] = None,"
        },
        {
            "sha": "e5551d414c1bb26e7703f2fff49d627bf35e1c9c",
            "filename": "src/transformers/models/smollm3/modular_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -157,7 +157,7 @@ def __init__(\n         pad_token_id: Optional[int] = 128004,\n         bos_token_id: Optional[int] = 128000,\n         eos_token_id: Optional[int] = 128001,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         use_sliding_window: Optional[bool] = False,\n         sliding_window: Optional[int] = None,\n         no_rope_layers: Optional[int] = None,"
        },
        {
            "sha": "0efdcd94adcdffa9cf958092b599f5b00ecd7893",
            "filename": "src/transformers/models/stablelm/configuration_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -119,7 +119,7 @@ def __init__(\n         layer_norm_eps: Optional[float] = 1.0e-5,\n         use_cache: Optional[bool] = True,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         use_qkv_bias: Optional[bool] = False,\n         qk_layernorm: Optional[bool] = False,\n         use_parallel_residual: Optional[bool] = False,"
        },
        {
            "sha": "cb34ad1d91578e2c580f75f9a674bee202eca038",
            "filename": "src/transformers/models/starcoder2/configuration_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -131,7 +131,7 @@ def __init__(\n         use_cache: Optional[bool] = True,\n         bos_token_id: Optional[int] = 50256,\n         eos_token_id: Optional[int] = 50256,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         sliding_window: Optional[int] = None,\n         attention_dropout: Optional[float] = 0.0,\n         residual_dropout: Optional[float] = 0.0,"
        },
        {
            "sha": "6e6f9784e951f513ddd4455b3fdfc83d6360bc1e",
            "filename": "src/transformers/models/t5gemma/configuration_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -140,7 +140,7 @@ def __init__(\n         eos_token_id: Optional[int] = 1,\n         bos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         query_pre_attn_scalar: Optional[int] = 256,"
        },
        {
            "sha": "86ecf53ae6e4ced63940066a82289a8d4e936cb0",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -159,7 +159,7 @@ def __init__(\n         eos_token_id: Optional[int] = 1,\n         bos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         query_pre_attn_scalar: Optional[int] = 256,"
        },
        {
            "sha": "0a784c02c1e6319292a64fd4c4c32ca2a86878af",
            "filename": "src/transformers/models/vaultgemma/configuration_vaultgemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -140,7 +140,7 @@ def __init__(\n         eos_token_id: Optional[int] = 1,\n         bos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         query_pre_attn_scalar: Optional[int] = 256,"
        },
        {
            "sha": "a0a9fc2076921f0c359930bc79189fb1e38c5071",
            "filename": "src/transformers/models/vaultgemma/modular_vaultgemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -121,7 +121,7 @@ def __init__(\n         eos_token_id: Optional[int] = 1,\n         bos_token_id: Optional[int] = 2,\n         tie_word_embeddings: Optional[bool] = True,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         query_pre_attn_scalar: Optional[int] = 256,"
        },
        {
            "sha": "4d6c92439da523ed9f3305e59f60eb421ff78fb6",
            "filename": "src/transformers/models/zamba2/configuration_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/63fbd50fb4ff7b586ab1b59b67f7464e62f9df69/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py?ref=63fbd50fb4ff7b586ab1b59b67f7464e62f9df69",
            "patch": "@@ -162,7 +162,7 @@ def __init__(\n         use_shared_attention_adapter: Optional[bool] = False,\n         adapter_rank: Optional[int] = 128,\n         use_mem_rope: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         initializer_range: Optional[float] = 0.02,\n         rms_norm_eps: Optional[int] = 1e-5,\n         use_cache: Optional[bool] = True,"
        }
    ],
    "stats": {
        "total": 238,
        "additions": 119,
        "deletions": 119
    }
}