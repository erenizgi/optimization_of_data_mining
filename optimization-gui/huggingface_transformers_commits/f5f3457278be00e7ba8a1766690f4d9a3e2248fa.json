{
    "author": "ydshieh",
    "message": "Try to remove `pickle` - `BloomTokenizerFast` (#41466)\n\n* pickle 1\n\n* pickle 1\n\n* pickle 1\n\n* pickle 1\n\n* pickle 1\n\n* pickle 1\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "f5f3457278be00e7ba8a1766690f4d9a3e2248fa",
    "files": [
        {
            "sha": "04ed9d701da43d2f141cf997fd59ff055676e888",
            "filename": "src/transformers/models/bloom/tokenization_bloom_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 11,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5f3457278be00e7ba8a1766690f4d9a3e2248fa/src%2Ftransformers%2Fmodels%2Fbloom%2Ftokenization_bloom_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5f3457278be00e7ba8a1766690f4d9a3e2248fa/src%2Ftransformers%2Fmodels%2Fbloom%2Ftokenization_bloom_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Ftokenization_bloom_fast.py?ref=f5f3457278be00e7ba8a1766690f4d9a3e2248fa",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Tokenization classes for Bloom.\"\"\"\n \n-import pickle\n from typing import Optional\n \n from ...tokenization_utils_base import BatchEncoding\n@@ -110,16 +109,11 @@ def __init__(\n             clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n-        # TODO @ArthurZucker this can only work one way for now, to update later-on. Tests should also properly\n-        # check this as they were green before.\n-        pre_tok_state = pickle.dumps(self.backend_tokenizer.pre_tokenizer)\n-        decoder_state = pickle.dumps(self.backend_tokenizer.decoder)\n-\n-        if add_prefix_space:\n-            pre_tok_state = pre_tok_state.replace(b'\"add_prefix_space\":false', b'\"add_prefix_space\": true')\n-            decoder_state = decoder_state.replace(b'\"add_prefix_space\":false', b'\"add_prefix_space\": true')\n-        self.backend_tokenizer.pre_tokenizer = pickle.loads(pre_tok_state)\n-        self.backend_tokenizer.decoder = pickle.loads(decoder_state)\n+        # This is a `tokenizers.pre_tokenizers.Sequence`\n+        for pre_tokenizer in self.backend_tokenizer.pre_tokenizer:\n+            if hasattr(pre_tokenizer, \"add_prefix_space\"):\n+                pre_tokenizer.add_prefix_space = add_prefix_space\n+        self.backend_tokenizer.decoder.add_prefix_space = add_prefix_space\n \n         self.add_prefix_space = add_prefix_space\n "
        },
        {
            "sha": "99def0e8aa7377df280cc7eb0b06e541b42acacc",
            "filename": "src/transformers/models/cohere/tokenization_cohere_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 11,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5f3457278be00e7ba8a1766690f4d9a3e2248fa/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5f3457278be00e7ba8a1766690f4d9a3e2248fa/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py?ref=f5f3457278be00e7ba8a1766690f4d9a3e2248fa",
            "patch": "@@ -15,7 +15,6 @@\n \n # This file is based on the tokenization_llama_fast.py file in transformers\n \n-import pickle\n from typing import Literal, Union\n \n from tokenizers import processors\n@@ -147,16 +146,11 @@ def __init__(\n         self.grounded_generation_template = kwargs.pop(\"grounded_generation_template\", None)\n         self.tool_use_template = kwargs.pop(\"tool_use_template\", None)\n \n-        # TODO @ArthurZucker this can only work one way for now, to update later-on. Tests should also properly\n-        # check this as they were green before.\n-        pre_tok_state = pickle.dumps(self.backend_tokenizer.pre_tokenizer)\n-        decoder_state = pickle.dumps(self.backend_tokenizer.decoder)\n-\n-        if add_prefix_space:\n-            pre_tok_state = pre_tok_state.replace(b'\"add_prefix_space\":false', b'\"add_prefix_space\": true')\n-            decoder_state = decoder_state.replace(b'\"add_prefix_space\":false', b'\"add_prefix_space\": true')\n-        self.backend_tokenizer.pre_tokenizer = pickle.loads(pre_tok_state)\n-        self.backend_tokenizer.decoder = pickle.loads(decoder_state)\n+        # This is a `tokenizers.pre_tokenizers.Sequence`\n+        for pre_tokenizer in self.backend_tokenizer.pre_tokenizer:\n+            if hasattr(pre_tokenizer, \"add_prefix_space\"):\n+                pre_tokenizer.add_prefix_space = add_prefix_space\n+        self.backend_tokenizer.decoder.add_prefix_space = add_prefix_space\n \n         self.add_prefix_space = add_prefix_space\n "
        }
    ],
    "stats": {
        "total": 32,
        "additions": 10,
        "deletions": 22
    }
}