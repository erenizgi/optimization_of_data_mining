{
    "author": "gante",
    "message": "[cache] add a test to confirm we can use cache at train time (#35709)\n\n* add test\r\n\r\n* augment test as suggested\r\n\r\n* Update tests/utils/test_modeling_utils.py\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* rerun tests\r\n\r\n---------\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "aeeceb99167ad95f5e1e55400ad894fe101c2e18",
    "files": [
        {
            "sha": "63f8e7ec4658343cef54f7af61c0ebf0e6f68244",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/aeeceb99167ad95f5e1e55400ad894fe101c2e18/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aeeceb99167ad95f5e1e55400ad894fe101c2e18/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=aeeceb99167ad95f5e1e55400ad894fe101c2e18",
            "patch": "@@ -37,6 +37,7 @@\n     AutoModel,\n     AutoModelForImageClassification,\n     AutoModelForSequenceClassification,\n+    DynamicCache,\n     LlavaForConditionalGeneration,\n     OwlViTForObjectDetection,\n     PretrainedConfig,\n@@ -1790,6 +1791,43 @@ def test_load_model_with_state_dict_only_low_cpu_mem_usage(self):\n         )\n         self.assertTrue(check_models_equal(model, model_loaded))\n \n+    def test_cache_when_needed_at_train_time(self):\n+        \"\"\"\n+        Some fine-tuning methods require the use of cache, like prefix tuning in PEFT. This test checks that a cache\n+        is at train time used if we request it. Related issue: #35648\n+        \"\"\"\n+        model = AutoModelForCausalLM.from_pretrained(TINY_MISTRAL)\n+        tokenizer = AutoTokenizer.from_pretrained(TINY_MISTRAL)\n+        model_inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n+\n+        # By default it is not training, we have to set it\n+        self.assertFalse(model.training)\n+        model.train()\n+\n+        # If we set `use_cache=True` while training, then a cache is returned\n+        model_outputs = model(**model_inputs, use_cache=True)\n+        self.assertIsInstance(model_outputs.past_key_values, DynamicCache)\n+        self.assertTrue(model.training)\n+\n+        # simulate injecting virtual tokens like in prefix tuning\n+        num_virtual_tokens = 3\n+        past_key_values = [torch.randn(2, 1, 2, num_virtual_tokens, 8)] * 2\n+        past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+        model_inputs[\"attention_mask\"] = torch.cat(\n+            (\n+                model_inputs[\"attention_mask\"],\n+                torch.ones(1, num_virtual_tokens).to(model_inputs[\"attention_mask\"].device),\n+            ),\n+            dim=1,\n+        )\n+        model_outputs = model(**model_inputs, past_key_values=past_key_values, use_cache=True)\n+        self.assertTrue(model.training)\n+\n+        # We can also disable the cache to skip a few operations, if the training loop doesn't need cache\n+        model_outputs = model(**model_inputs, use_cache=False)\n+        self.assertIsNone(model_outputs.past_key_values)\n+        self.assertTrue(model.training)\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 38,
        "additions": 38,
        "deletions": 0
    }
}