{
    "author": "rjgleaton",
    "message": "Add Whole Word Masking and Padding Strategy to DataCollatorForLanguageModeling (#39485)\n\n* Add whole word masking\n\n* Vectorize whole word masking functions\n\n* Unit test whole word masking\n\n* Remove support for TF in whole word masking",
    "sha": "2b8a7e82b5d0bc21fb1ecafe4412ae2600728acf",
    "files": [
        {
            "sha": "d9b198b510871f7f097f3efc403dd850bc301264",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 123,
            "deletions": 256,
            "changes": 379,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b8a7e82b5d0bc21fb1ecafe4412ae2600728acf/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b8a7e82b5d0bc21fb1ecafe4412ae2600728acf/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=2b8a7e82b5d0bc21fb1ecafe4412ae2600728acf",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import multiprocessing as mp\n-import random\n import warnings\n from collections.abc import Mapping\n from dataclasses import dataclass\n@@ -22,7 +21,6 @@\n \n import numpy as np\n \n-from ..models.bert import BertTokenizer, BertTokenizerFast\n from ..tokenization_utils_base import PreTrainedTokenizerBase\n from ..utils import PaddingStrategy\n \n@@ -630,6 +628,8 @@ class DataCollatorForLanguageModeling(DataCollatorMixin):\n             Whether or not to use masked language modeling. If set to `False`, the labels are the same as the inputs\n             with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked\n             tokens and the value to predict for the masked token.\n+        whole_word_mask (`bool`, *optional*, defaults to `False`):\n+            Whether or not to mask whole words instead of individual tokens.\n         mlm_probability (`float`, *optional*, defaults to 0.15):\n             The probability with which to (randomly) mask tokens in the input, when `mlm` is set to `True`.\n         mask_replace_prob (`float`, *optional*, defaults to 0.8):\n@@ -681,6 +681,7 @@ class DataCollatorForLanguageModeling(DataCollatorMixin):\n \n     tokenizer: PreTrainedTokenizerBase\n     mlm: bool = True\n+    whole_word_mask: bool = False\n     mlm_probability: Optional[float] = 0.15\n     mask_replace_prob: float = 0.8\n     random_replace_prob: float = 0.1\n@@ -698,6 +699,11 @@ def __post_init__(self):\n             if self.mlm_probability is None or self.mlm_probability < 0 or self.mlm_probability > 1:\n                 raise ValueError(\"mlm_probability should be between 0 and 1.\")\n             self.mlm_probability = float(self.mlm_probability)\n+        elif self.whole_word_mask:\n+            raise ValueError(\n+                \"Whole word masking can only be used with mlm=True.\"\n+                \"If you want to use whole word masking, please set mlm=True.\"\n+            )\n         if self.mask_replace_prob + self.random_replace_prob > 1:\n             raise ValueError(\"The sum of mask_replace_prob and random_replace_prob should not exceed 1\")\n         if self.mask_replace_prob < 0 or self.mask_replace_prob > 1:\n@@ -708,6 +714,21 @@ def __post_init__(self):\n         self.mask_replace_prob = float(self.mask_replace_prob)\n         self.random_replace_prob = float(self.random_replace_prob)\n \n+        if self.whole_word_mask:\n+            if not self.tokenizer.is_fast:\n+                warnings.warn(\n+                    \"Whole word masking depends on offset mapping which is only natively available with fast tokenizers.\",\n+                    UserWarning,\n+                )\n+\n+            if self.mask_replace_prob < 1:\n+                warnings.warn(\n+                    \"Random token replacement is not supported with whole word masking.\",\n+                    \"Setting mask_replace_prob to 1.\",\n+                )\n+                self.mask_replace_prob = 1\n+                self.random_replace_prob = 0\n+\n         self.generator = None\n \n     def get_generator(self, seed):\n@@ -762,9 +783,10 @@ def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> d\n \n         # If special token mask has been preprocessed, pop it from the dict.\n         special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n+        offset_mapping = batch.pop(\"offset_mapping\", None)\n         if self.mlm:\n             batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n-                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n+                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask, offset_mapping=offset_mapping\n             )\n         else:\n             labels = batch[\"input_ids\"].clone()\n@@ -773,9 +795,11 @@ def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> d\n             batch[\"labels\"] = labels\n         return batch\n \n-    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> tuple[Any, Any]:\n+    def torch_mask_tokens(\n+        self, inputs: Any, special_tokens_mask: Optional[Any] = None, offset_mapping: Optional[Any] = None\n+    ) -> tuple[Any, Any]:\n         \"\"\"\n-        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n+        Prepare masked tokens inputs/labels for masked language modeling.\n         \"\"\"\n         import torch\n \n@@ -786,12 +810,24 @@ def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = No\n             special_tokens_mask = [\n                 self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n             ]\n-            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n+\n+        if self.whole_word_mask:\n+            word_ids, no_mask_mask = self._calc_word_ids_and_prob_mask(\n+                to_numpy(offset_mapping), to_numpy(special_tokens_mask)\n+            )\n+            no_mask_mask = torch.tensor(no_mask_mask, dtype=torch.bool)\n         else:\n-            special_tokens_mask = special_tokens_mask.bool()\n+            no_mask_mask = (\n+                special_tokens_mask.bool()\n+                if isinstance(special_tokens_mask, torch.Tensor)\n+                else torch.tensor(special_tokens_mask, dtype=torch.bool)\n+            )\n \n-        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n+        probability_matrix.masked_fill_(no_mask_mask, value=0.0)\n         masked_indices = torch.bernoulli(probability_matrix, generator=self.generator).bool()\n+        if self.whole_word_mask:\n+            masked_indices = torch.BoolTensor(self._whole_word_mask(word_ids, masked_indices))\n+\n         labels[~masked_indices] = -100  # We only compute loss on masked tokens\n \n         # mask_replace_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n@@ -841,9 +877,10 @@ def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> d\n \n         # If special token mask has been preprocessed, pop it from the dict.\n         special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n+        offset_mapping = batch.pop(\"offset_mapping\", None)\n         if self.mlm:\n             batch[\"input_ids\"], batch[\"labels\"] = self.numpy_mask_tokens(\n-                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n+                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask, offset_mapping=offset_mapping\n             )\n         else:\n             labels = np.copy(batch[\"input_ids\"])\n@@ -852,9 +889,14 @@ def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> d\n             batch[\"labels\"] = labels\n         return batch\n \n-    def numpy_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> tuple[Any, Any]:\n+    def numpy_mask_tokens(\n+        self,\n+        inputs: Any,\n+        special_tokens_mask: Optional[Any] = None,\n+        offset_mapping: Optional[Any] = None,\n+    ) -> tuple[Any, Any]:\n         \"\"\"\n-        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n+        Prepare masked tokens inputs/labels for masked language modeling.\n         \"\"\"\n         labels = np.copy(inputs)\n         # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n@@ -863,16 +905,28 @@ def numpy_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = No\n             special_tokens_mask = [\n                 self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n             ]\n-            special_tokens_mask = np.array(special_tokens_mask, dtype=bool)\n+\n+        if self.whole_word_mask:\n+            word_ids, no_mask_mask = self._calc_word_ids_and_prob_mask(\n+                to_numpy(offset_mapping), to_numpy(special_tokens_mask)\n+            )\n         else:\n-            special_tokens_mask = special_tokens_mask.astype(bool)\n+            no_mask_mask = (\n+                special_tokens_mask.astype(bool)\n+                if isinstance(special_tokens_mask, np.ndarray)\n+                else np.array(special_tokens_mask, dtype=bool)\n+            )\n \n-        probability_matrix[special_tokens_mask] = 0\n+        probability_matrix[no_mask_mask] = 0\n         # Numpy doesn't have bernoulli, so we use a binomial with 1 trial\n         if self.generator:\n             masked_indices = self.generator.binomial(1, probability_matrix, size=probability_matrix.shape).astype(bool)\n         else:\n             masked_indices = np.random.binomial(1, probability_matrix, size=probability_matrix.shape).astype(bool)\n+\n+        if self.whole_word_mask:\n+            masked_indices = self._whole_word_mask(word_ids, masked_indices)\n+\n         labels[~masked_indices] = -100  # We only compute loss on masked tokens\n \n         # mask_replace_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n@@ -917,276 +971,89 @@ def numpy_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = No\n         # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n         return inputs, labels\n \n-\n-@dataclass\n-class DataCollatorForWholeWordMask(DataCollatorForLanguageModeling):\n-    \"\"\"\n-    Data collator used for language modeling that masks entire words.\n-\n-    - collates batches of tensors, honoring their tokenizer's pad_token\n-    - preprocesses batches for masked language modeling\n-\n-    <Tip>\n-\n-    This collator relies on details of the implementation of subword tokenization by [`BertTokenizer`], specifically\n-    that subword tokens are prefixed with *##*. For tokenizers that do not adhere to this scheme, this collator will\n-    produce an output that is roughly equivalent to [`.DataCollatorForLanguageModeling`].\n-\n-    </Tip>\"\"\"\n-\n-    def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n-        if self.seed and self.generator is None:\n-            # If we have a seed, we need to create a generator object. Subsequent calls to this function will use the same generator.\n-            # If no seed supplied, we will use the global RNG\n-            self.create_rng()\n-\n-        if isinstance(examples[0], Mapping):\n-            input_ids = [e[\"input_ids\"] for e in examples]\n-        else:\n-            input_ids = examples\n-            examples = [{\"input_ids\": e} for e in examples]\n-\n-        batch_input = _torch_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n-\n-        mask_labels = []\n-        for e in examples:\n-            ref_tokens = []\n-            for id in tolist(e[\"input_ids\"]):\n-                token = self.tokenizer._convert_id_to_token(id)\n-                ref_tokens.append(token)\n-\n-            # For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-> [喜，##欢]\n-            if \"chinese_ref\" in e:\n-                ref_pos = tolist(e[\"chinese_ref\"])\n-                len_seq = len(e[\"input_ids\"])\n-                for i in range(len_seq):\n-                    if i in ref_pos:\n-                        ref_tokens[i] = \"##\" + ref_tokens[i]\n-            mask_labels.append(self._whole_word_mask(ref_tokens))\n-        batch_mask = _torch_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n-        inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)\n-        return {\"input_ids\": inputs, \"labels\": labels}\n-\n-    def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n-        if self.seed and self.generator is None:\n-            # If we have a seed, we need to create a generator object. Subsequent calls to this function will use the same generator.\n-            # If no seed supplied, we will use the global RNG\n-            self.create_rng()\n-\n-        if isinstance(examples[0], Mapping):\n-            input_ids = [e[\"input_ids\"] for e in examples]\n-        else:\n-            input_ids = examples\n-            examples = [{\"input_ids\": e} for e in examples]\n-\n-        batch_input = _numpy_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n-\n-        mask_labels = []\n-        for e in examples:\n-            ref_tokens = []\n-            for id in tolist(e[\"input_ids\"]):\n-                token = self.tokenizer._convert_id_to_token(id)\n-                ref_tokens.append(token)\n-\n-            # For Chinese tokens, we need extra inf to mark sub-word, e.g [喜,欢]-> [喜，##欢]\n-            if \"chinese_ref\" in e:\n-                ref_pos = tolist(e[\"chinese_ref\"])\n-                len_seq = len(e[\"input_ids\"])\n-                for i in range(len_seq):\n-                    if i in ref_pos:\n-                        ref_tokens[i] = \"##\" + ref_tokens[i]\n-            mask_labels.append(self._whole_word_mask(ref_tokens))\n-        batch_mask = _numpy_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n-        inputs, labels = self.numpy_mask_tokens(batch_input, batch_mask)\n-        return {\"input_ids\": inputs, \"labels\": labels}\n-\n-    def _shuffle(self, cand_indexes):\n-        # if no seed, just use random's shuffle\n-        if self.seed is None:\n-            random.shuffle(cand_indexes)\n-            return cand_indexes\n-\n-        # if seed is provided, use the generator to shuffle\n-        if self.return_tensors == \"pt\":\n-            import torch\n-\n-            indices = torch.randperm(len(cand_indexes), generator=self.generator)\n-            return [cand_indexes[i] for i in indices]\n-\n-        elif self.return_tensors == \"np\":\n-            self.generator.shuffle(cand_indexes)\n-            return cand_indexes\n-\n-    def _whole_word_mask(self, input_tokens: list[str], max_predictions=512):\n+    @staticmethod\n+    def _calc_word_ids_and_prob_mask(\n+        offsets: np.ndarray[np.ndarray[tuple[int, int]]], special_tokens_mask: np.ndarray[np.ndarray[int]]\n+    ) -> tuple[np.ndarray[np.ndarray[int]], np.ndarray[np.ndarray[int]]]:\n         \"\"\"\n-        Get 0/1 labels for masked tokens with whole word mask proxy\n+        Map tokens to word ids and create mask of tokens to not mask.\n+        Tokens that are part of the same word will have the same word id and we will only\n+        set a mask probability for the first token of each word.\n         \"\"\"\n-        if not isinstance(self.tokenizer, (BertTokenizer, BertTokenizerFast)):\n-            warnings.warn(\n-                \"DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. \"\n-                \"Please refer to the documentation for more information.\"\n-            )\n \n-        cand_indexes = []\n-        for i, token in enumerate(input_tokens):\n-            if token == \"[CLS]\" or token == \"[SEP]\":\n-                continue\n+        token_starts = offsets[:, :, 0]\n+        token_ends = offsets[:, :, 1]\n \n-            if len(cand_indexes) >= 1 and token.startswith(\"##\"):\n-                cand_indexes[-1].append(i)\n-            else:\n-                cand_indexes.append([i])\n-\n-        cand_indexes = self._shuffle(cand_indexes)\n-        num_to_predict = min(max_predictions, max(1, int(round(len(input_tokens) * self.mlm_probability))))\n-        masked_lms = []\n-        covered_indexes = set()\n-        for index_set in cand_indexes:\n-            if len(masked_lms) >= num_to_predict:\n-                break\n-            # If adding a whole-word mask would exceed the maximum number of\n-            # predictions, then just skip this candidate.\n-            if len(masked_lms) + len(index_set) > num_to_predict:\n-                continue\n-            for index in index_set:\n-                covered_indexes.add(index)\n-                masked_lms.append(index)\n-\n-        if len(covered_indexes) != len(masked_lms):\n-            raise ValueError(\"Length of covered_indexes is not equal to length of masked_lms.\")\n-        mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_tokens))]\n-        return mask_labels\n-\n-    def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]:\n-        \"\"\"\n-        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n-        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n-        \"\"\"\n-        import torch\n+        prev_token_ends = np.roll(token_ends, 1, axis=1)\n+        prev_token_ends[:, 0] = -1  # First token has no previous token\n \n-        if self.tokenizer.mask_token is None:\n-            raise ValueError(\n-                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n-                \" --mlm flag if you want to use this tokenizer.\"\n-            )\n-        labels = inputs.clone()\n-        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n-\n-        probability_matrix = mask_labels\n-\n-        special_tokens_mask = [\n-            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n-        ]\n-        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n-        if self.tokenizer.pad_token is not None:\n-            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n-            probability_matrix.masked_fill_(padding_mask, value=0.0)\n-\n-        masked_indices = probability_matrix.bool()\n-        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n-\n-        # mask_replace_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = (\n-            torch.bernoulli(torch.full(labels.shape, self.mask_replace_prob), generator=self.generator).bool()\n-            & masked_indices\n-        )\n-        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n+        prev_token_special = np.roll(special_tokens_mask, 1, axis=1)\n+        prev_token_special[:, 0] = 0\n \n-        if self.mask_replace_prob == 1 or self.random_replace_prob == 0:\n-            return inputs, labels\n+        # Not special token AND (gap from previous or previous token was special)\n+        special_tokens_mask = special_tokens_mask.astype(bool)\n+        is_new_word = (~special_tokens_mask) & ((token_starts != prev_token_ends) | (prev_token_special == 1))\n \n-        remaining_prob = 1 - self.mask_replace_prob\n-        # scaling the random_replace_prob to the remaining probability for example if\n-        # mask_replace_prob = 0.8 and random_replace_prob = 0.1,\n-        # then random_replace_prob_scaled = 0.1 / 0.2 = 0.5\n-        random_replace_prob_scaled = self.random_replace_prob / remaining_prob\n+        word_ids = np.cumsum(is_new_word, axis=1)\n+        word_ids[special_tokens_mask] = -1\n \n-        # random_replacement_prob% of the time, we replace masked input tokens with random word\n-        indices_random = (\n-            torch.bernoulli(torch.full(labels.shape, random_replace_prob_scaled), generator=self.generator).bool()\n-            & masked_indices\n-            & ~indices_replaced\n-        )\n-        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long, generator=self.generator)\n-        inputs[indices_random] = random_words[indices_random]\n+        prob_mask = ~is_new_word\n \n-        # The rest of the time ((1-random_replacement_prob-mask_replace_prob)% of the time) we keep the masked input tokens unchanged\n-        return inputs, labels\n+        return word_ids, prob_mask\n \n-    def numpy_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]:\n+    @staticmethod\n+    def _whole_word_mask(word_ids: np.ndarray[np.ndarray[int]], mask: Any) -> Any:\n         \"\"\"\n-        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n-        'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n+        Mask whole words based on word ids and mask.\n         \"\"\"\n-        if self.tokenizer.mask_token is None:\n-            raise ValueError(\n-                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the\"\n-                \" --mlm flag if you want to use this tokenizer.\"\n-            )\n-        labels = np.copy(inputs)\n-        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n+        mask = to_numpy(mask)\n \n-        masked_indices = mask_labels.astype(bool)\n+        valid_ids = word_ids != -1\n \n-        special_tokens_mask = [\n-            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n-        ]\n-        masked_indices[np.array(special_tokens_mask, dtype=bool)] = 0\n-        if self.tokenizer.pad_token is not None:\n-            padding_mask = labels == self.tokenizer.pad_token_id\n-            masked_indices[padding_mask] = 0\n-\n-        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n-\n-        # mask_replacement_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        if self.generator:\n-            indices_replaced = (\n-                self.generator.binomial(1, self.mask_replace_prob, size=labels.shape).astype(bool) & masked_indices\n-            )\n-        else:\n-            indices_replaced = (\n-                np.random.binomial(1, self.mask_replace_prob, size=labels.shape).astype(bool) & masked_indices\n-            )\n-        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n+        # Create 3D mask where [batch, token_i, token_j] is True if token_i and token_j are the same word\n+        same_word = (word_ids[:, :, None] == word_ids[:, None, :]) & valid_ids[:, :, None] & valid_ids[:, None, :]\n \n-        if self.mask_replace_prob == 1 or self.random_replace_prob == 0:\n-            return inputs, labels\n+        # For each token, set True if any token in the same word is masked\n+        return np.any(same_word & mask[:, None, :], axis=2)\n \n-        remaining_prob = 1 - self.mask_replace_prob\n-        # scaling the random_replace_prob to the remaining probability for example if\n-        # mask_replace_prob = 0.8 and random_replace_prob = 0.1,\n-        # then random_replace_prob_scaled = 0.1 / 0.2 = 0.5\n-        random_replace_prob_scaled = self.random_replace_prob / remaining_prob\n \n-        if self.generator:\n-            indices_random = (\n-                self.generator.binomial(1, random_replace_prob_scaled, size=labels.shape).astype(bool)\n-                & masked_indices\n-                & ~indices_replaced\n-            )\n-            random_words = self.generator.integers(low=0, high=len(self.tokenizer), size=labels.shape, dtype=np.int64)\n-        else:\n-            indices_random = (\n-                np.random.binomial(1, random_replace_prob_scaled, size=labels.shape).astype(bool)\n-                & masked_indices\n-                & ~indices_replaced\n-            )\n-            random_words = np.random.randint(low=0, high=len(self.tokenizer), size=labels.shape, dtype=np.int64)\n+@dataclass\n+class DataCollatorForWholeWordMask(DataCollatorForLanguageModeling):\n+    \"\"\"\n+    Data collator used for language modeling that masks entire words.\n \n-        inputs[indices_random] = random_words[indices_random]\n+    - collates batches of tensors, honoring their tokenizer's pad_token\n+    - preprocesses batches for masked language modeling\n+    \"\"\"\n \n-        # The rest of the time ((1-mask_replace_prob-random_replace_prob)% of the time) we keep the masked input tokens unchanged\n-        return inputs, labels\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"DataCollatorForWholeWordMask is deprecated and will be removed in a future version, you can now use \"\n+            \"DataCollatorForLanguageModeling with whole_word_mask=True instead.\",\n+            FutureWarning,\n+        )\n+        super().__init__(*args, **kwargs)\n+        self.mlm = True  # Force masked language modeling\n+        self.whole_word_mask = True  # Force whole word masking\n \n \n-def tolist(x):\n+def tolist(x) -> list[Any]:\n     if isinstance(x, list):\n         return x\n     elif hasattr(x, \"numpy\"):\n         x = x.numpy()\n     return x.tolist()\n \n \n+def to_numpy(x) -> np.ndarray[Any]:\n+    if isinstance(x, np.ndarray):\n+        return x\n+    elif hasattr(x, \"detach\"):\n+        return x.detach().cpu().numpy()\n+    else:\n+        return np.array(x)\n+\n+\n @dataclass\n class DataCollatorForSOP(DataCollatorForLanguageModeling):\n     \"\"\""
        },
        {
            "sha": "b5cbb5ecea28443fdf1efc72a64b9fd133518df9",
            "filename": "tests/trainer/test_data_collator.py",
            "status": "modified",
            "additions": 232,
            "deletions": 109,
            "changes": 341,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b8a7e82b5d0bc21fb1ecafe4412ae2600728acf/tests%2Ftrainer%2Ftest_data_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b8a7e82b5d0bc21fb1ecafe4412ae2600728acf/tests%2Ftrainer%2Ftest_data_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_data_collator.py?ref=2b8a7e82b5d0bc21fb1ecafe4412ae2600728acf",
            "patch": "@@ -21,6 +21,7 @@\n \n from transformers import (\n     BertTokenizer,\n+    BertTokenizerFast,\n     DataCollatorForLanguageModeling,\n     DataCollatorForPermutationLanguageModeling,\n     DataCollatorForSeq2Seq,\n@@ -525,99 +526,120 @@ def test_data_collator_for_language_modeling_with_seed(self):\n         self.assertFalse(torch.all(batch_3_labels == batch_5_labels))\n \n     def test_data_collator_for_whole_word_mask(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n+        tokenizer = BertTokenizerFast(self.vocab_file)\n+\n+        input_tokens = [f\"token_{i}\" for i in range(8)]\n+        tokenizer.add_tokens(input_tokens)\n+        features = [tokenizer(\" \".join(input_tokens), return_offsets_mapping=True) for _ in range(2)]\n+\n         data_collator = DataCollatorForWholeWordMask(tokenizer, return_tensors=\"pt\")\n \n-        features = [{\"input_ids\": list(range(10))}, {\"input_ids\": list(range(10))}]\n         batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape, torch.Size((2, 10)))\n-        self.assertEqual(batch[\"labels\"].shape, torch.Size((2, 10)))\n+        self.assertEqual(batch[\"input_ids\"].shape, (2, 10))\n+        self.assertEqual(batch[\"labels\"].shape, (2, 10))\n \n         # Features can already be tensors\n-        features = [{\"input_ids\": np.arange(10)}, {\"input_ids\": np.arange(10)}]\n+        features = [\n+            tokenizer(\" \".join(input_tokens), return_offsets_mapping=True).convert_to_tensors(\"np\") for _ in range(2)\n+        ]\n         batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape, torch.Size((2, 10)))\n-        self.assertEqual(batch[\"labels\"].shape, torch.Size((2, 10)))\n+        self.assertEqual(batch[\"input_ids\"].shape, (2, 10))\n+        self.assertEqual(batch[\"labels\"].shape, (2, 10))\n+\n+        if is_torch_available():\n+            # Features can already be tensors\n+            features = [\n+                tokenizer(\" \".join(input_tokens), return_offsets_mapping=True).convert_to_tensors(\"pt\")\n+                for _ in range(2)\n+            ]\n+            data_collator = DataCollatorForWholeWordMask(tokenizer, return_tensors=\"pt\")\n+            batch = data_collator(features)\n+            self.assertEqual(batch[\"input_ids\"].shape, torch.Size((2, 10)))\n+            self.assertEqual(batch[\"labels\"].shape, torch.Size((2, 10)))\n \n     def test_data_collator_for_whole_word_mask_with_seed(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        features = [{\"input_ids\": list(range(1000))}, {\"input_ids\": list(range(1000))}]\n+        tokenizer = BertTokenizerFast(self.vocab_file)\n+\n+        input_tokens = [f\"token_{i}\" for i in range(998)]\n+        tokenizer.add_tokens(input_tokens)\n+        features = [tokenizer(\" \".join(input_tokens), return_offsets_mapping=True) for _ in range(2)]\n \n         # check if seed is respected between two different DataCollatorForWholeWordMask instances\n-        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=42)\n+        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=42, return_tensors=\"np\")\n         batch_1 = data_collator(features)\n-        self.assertEqual(batch_1[\"input_ids\"].shape, torch.Size((2, 1000)))\n-        self.assertEqual(batch_1[\"labels\"].shape, torch.Size((2, 1000)))\n+        self.assertEqual(batch_1[\"input_ids\"].shape, (2, 1000))\n+        self.assertEqual(batch_1[\"labels\"].shape, (2, 1000))\n \n-        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=42)\n+        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=42, return_tensors=\"np\")\n         batch_2 = data_collator(features)\n-        self.assertEqual(batch_2[\"input_ids\"].shape, torch.Size((2, 1000)))\n-        self.assertEqual(batch_2[\"labels\"].shape, torch.Size((2, 1000)))\n+        self.assertEqual(batch_2[\"input_ids\"].shape, (2, 1000))\n+        self.assertEqual(batch_2[\"labels\"].shape, (2, 1000))\n \n-        self.assertTrue(torch.all(batch_1[\"input_ids\"] == batch_2[\"input_ids\"]))\n-        self.assertTrue(torch.all(batch_1[\"labels\"] == batch_2[\"labels\"]))\n+        self.assertTrue(np.all(batch_1[\"input_ids\"] == batch_2[\"input_ids\"]))\n+        self.assertTrue(np.all(batch_1[\"labels\"] == batch_2[\"labels\"]))\n \n         # check if seed is respected in multiple workers situation\n-        features = [{\"input_ids\": list(range(1000))} for _ in range(10)]\n-        dataloader = torch.utils.data.DataLoader(\n-            features,\n-            batch_size=2,\n-            num_workers=2,\n-            generator=torch.Generator().manual_seed(42),\n-            collate_fn=DataCollatorForWholeWordMask(tokenizer, seed=42),\n-        )\n-\n-        batch_3_input_ids = []\n-        batch_3_labels = []\n-        for batch in dataloader:\n-            batch_3_input_ids.append(batch[\"input_ids\"])\n-            batch_3_labels.append(batch[\"labels\"])\n-\n-        batch_3_input_ids = torch.stack(batch_3_input_ids)\n-        batch_3_labels = torch.stack(batch_3_labels)\n-        self.assertEqual(batch_3_input_ids.shape, torch.Size((5, 2, 1000)))\n-        self.assertEqual(batch_3_labels.shape, torch.Size((5, 2, 1000)))\n-\n-        dataloader = torch.utils.data.DataLoader(\n-            features,\n-            batch_size=2,\n-            num_workers=2,\n-            collate_fn=DataCollatorForWholeWordMask(tokenizer, seed=42),\n-        )\n-\n-        batch_4_input_ids = []\n-        batch_4_labels = []\n-        for batch in dataloader:\n-            batch_4_input_ids.append(batch[\"input_ids\"])\n-            batch_4_labels.append(batch[\"labels\"])\n-        batch_4_input_ids = torch.stack(batch_4_input_ids)\n-        batch_4_labels = torch.stack(batch_4_labels)\n-        self.assertEqual(batch_4_input_ids.shape, torch.Size((5, 2, 1000)))\n-        self.assertEqual(batch_4_labels.shape, torch.Size((5, 2, 1000)))\n+        if is_torch_available():\n+            features = [tokenizer(\" \".join(input_tokens), return_offsets_mapping=True) for _ in range(10)]\n+            dataloader = torch.utils.data.DataLoader(\n+                features,\n+                batch_size=2,\n+                num_workers=2,\n+                generator=torch.Generator().manual_seed(42),\n+                collate_fn=DataCollatorForWholeWordMask(tokenizer, seed=42),\n+            )\n \n-        self.assertTrue(torch.all(batch_3_input_ids == batch_4_input_ids))\n-        self.assertTrue(torch.all(batch_3_labels == batch_4_labels))\n+            batch_3_input_ids = []\n+            batch_3_labels = []\n+            for batch in dataloader:\n+                batch_3_input_ids.append(batch[\"input_ids\"])\n+                batch_3_labels.append(batch[\"labels\"])\n+\n+            batch_3_input_ids = torch.stack(batch_3_input_ids)\n+            batch_3_labels = torch.stack(batch_3_labels)\n+            self.assertEqual(batch_3_input_ids.shape, torch.Size((5, 2, 1000)))\n+            self.assertEqual(batch_3_labels.shape, torch.Size((5, 2, 1000)))\n+\n+            dataloader = torch.utils.data.DataLoader(\n+                features,\n+                batch_size=2,\n+                num_workers=2,\n+                collate_fn=DataCollatorForWholeWordMask(tokenizer, seed=42),\n+            )\n \n-        # try with different seed\n-        dataloader = torch.utils.data.DataLoader(\n-            features,\n-            batch_size=2,\n-            num_workers=2,\n-            collate_fn=DataCollatorForWholeWordMask(tokenizer, seed=43),\n-        )\n+            batch_4_input_ids = []\n+            batch_4_labels = []\n+            for batch in dataloader:\n+                batch_4_input_ids.append(batch[\"input_ids\"])\n+                batch_4_labels.append(batch[\"labels\"])\n+            batch_4_input_ids = torch.stack(batch_4_input_ids)\n+            batch_4_labels = torch.stack(batch_4_labels)\n+            self.assertEqual(batch_4_input_ids.shape, torch.Size((5, 2, 1000)))\n+            self.assertEqual(batch_4_labels.shape, torch.Size((5, 2, 1000)))\n+\n+            self.assertTrue(torch.all(batch_3_input_ids == batch_4_input_ids))\n+            self.assertTrue(torch.all(batch_3_labels == batch_4_labels))\n+\n+            # try with different seed\n+            dataloader = torch.utils.data.DataLoader(\n+                features,\n+                batch_size=2,\n+                num_workers=2,\n+                collate_fn=DataCollatorForWholeWordMask(tokenizer, seed=43),\n+            )\n \n-        batch_5_input_ids = []\n-        batch_5_labels = []\n-        for batch in dataloader:\n-            batch_5_input_ids.append(batch[\"input_ids\"])\n-            batch_5_labels.append(batch[\"labels\"])\n-        batch_5_input_ids = torch.stack(batch_5_input_ids)\n-        batch_5_labels = torch.stack(batch_5_labels)\n-        self.assertEqual(batch_5_input_ids.shape, torch.Size((5, 2, 1000)))\n-        self.assertEqual(batch_5_labels.shape, torch.Size((5, 2, 1000)))\n+            batch_5_input_ids = []\n+            batch_5_labels = []\n+            for batch in dataloader:\n+                batch_5_input_ids.append(batch[\"input_ids\"])\n+                batch_5_labels.append(batch[\"labels\"])\n+            batch_5_input_ids = torch.stack(batch_5_input_ids)\n+            batch_5_labels = torch.stack(batch_5_labels)\n+            self.assertEqual(batch_5_input_ids.shape, torch.Size((5, 2, 1000)))\n+            self.assertEqual(batch_5_labels.shape, torch.Size((5, 2, 1000)))\n \n-        self.assertFalse(torch.all(batch_3_input_ids == batch_5_input_ids))\n-        self.assertFalse(torch.all(batch_3_labels == batch_5_labels))\n+            self.assertFalse(torch.all(batch_3_input_ids == batch_5_input_ids))\n+            self.assertFalse(torch.all(batch_3_labels == batch_5_labels))\n \n     def test_plm(self):\n         tokenizer = BertTokenizer(self.vocab_file)\n@@ -929,24 +951,23 @@ def test_language_modelling_collator_immutability(self):\n                 )\n \n     def test_whole_world_masking_collator_immutability(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n+        tokenizer = BertTokenizerFast(self.vocab_file)\n \n-        features_base = [\n-            {\"input_ids\": list(range(10)), \"labels\": (1,)},\n-            {\"input_ids\": list(range(10)), \"labels\": (1,)},\n-        ]\n-        whole_word_masking_collator = DataCollatorForWholeWordMask(tokenizer, return_tensors=\"pt\")\n+        input_tokens = [f\"token_{i}\" for i in range(8)]\n+        tokenizer.add_tokens(input_tokens)\n+        original_data = [tokenizer(\" \".join(input_tokens), return_offsets_mapping=True) for _ in range(2)]\n+        for feature in original_data:\n+            feature[\"labels\"] = (1,)\n \n-        for datatype_input, datatype_label in [(list, list), (np.array, np.array)]:\n-            self._validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-                collator=whole_word_masking_collator,\n-                base_data=features_base,\n-                input_key=\"input_ids\",\n-                input_datatype=datatype_input,\n-                label_key=\"labels\",\n-                label_datatype=datatype_label,\n-                ignore_label=True,\n-            )\n+        batch_data = [tokenizer(\" \".join(input_tokens), return_offsets_mapping=True) for _ in range(2)]\n+        for feature in batch_data:\n+            feature[\"labels\"] = (1,)\n+\n+        whole_word_masking_collator = DataCollatorForWholeWordMask(tokenizer)\n+\n+        self._validate_original_data_against_collated_data(\n+            collator=whole_word_masking_collator, original_data=original_data, batch_data=batch_data\n+        )\n \n     def test_permutation_language_modelling_collator_immutability(self):\n         tokenizer = BertTokenizer(self.vocab_file)\n@@ -1400,23 +1421,31 @@ def test_data_collator_for_language_modeling_with_seed(self):\n         self.assertFalse(np.all(batch_1[\"labels\"] == batch_3[\"labels\"]))\n \n     def test_data_collator_for_whole_word_mask(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n+        tokenizer = BertTokenizerFast(self.vocab_file)\n         data_collator = DataCollatorForWholeWordMask(tokenizer, return_tensors=\"np\")\n \n-        features = [{\"input_ids\": list(range(10))}, {\"input_ids\": list(range(10))}]\n+        input_tokens = [f\"token_{i}\" for i in range(8)]\n+        tokenizer.add_tokens(input_tokens)\n+        features = [tokenizer(\" \".join(input_tokens), return_offsets_mapping=True) for _ in range(2)]\n+\n         batch = data_collator(features)\n         self.assertEqual(batch[\"input_ids\"].shape, (2, 10))\n         self.assertEqual(batch[\"labels\"].shape, (2, 10))\n \n         # Features can already be tensors\n-        features = [{\"input_ids\": np.arange(10)}, {\"input_ids\": np.arange(10)}]\n+        features = [\n+            tokenizer(\" \".join(input_tokens), return_offsets_mapping=True).convert_to_tensors(\"np\") for _ in range(2)\n+        ]\n         batch = data_collator(features)\n         self.assertEqual(batch[\"input_ids\"].shape, (2, 10))\n         self.assertEqual(batch[\"labels\"].shape, (2, 10))\n \n     def test_data_collator_for_whole_word_mask_with_seed(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        features = [{\"input_ids\": list(range(1000))}, {\"input_ids\": list(range(1000))}]\n+        tokenizer = BertTokenizerFast(self.vocab_file)\n+\n+        input_tokens = [f\"token_{i}\" for i in range(998)]\n+        tokenizer.add_tokens(input_tokens)\n+        features = [tokenizer(\" \".join(input_tokens), return_offsets_mapping=True) for _ in range(2)]\n \n         # check if seed is respected between two different DataCollatorForWholeWordMask instances\n         data_collator = DataCollatorForWholeWordMask(tokenizer, seed=42, return_tensors=\"np\")\n@@ -1755,24 +1784,23 @@ def test_language_modelling_collator_immutability(self):\n                 )\n \n     def test_whole_world_masking_collator_immutability(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n+        tokenizer = BertTokenizerFast(self.vocab_file)\n+\n+        input_tokens = [f\"token_{i}\" for i in range(8)]\n+        tokenizer.add_tokens(input_tokens)\n+        original_data = [tokenizer(\" \".join(input_tokens), return_offsets_mapping=True) for _ in range(2)]\n+        for feature in original_data:\n+            feature[\"labels\"] = (1,)\n+\n+        batch_data = [tokenizer(\" \".join(input_tokens), return_offsets_mapping=True) for _ in range(2)]\n+        for feature in batch_data:\n+            feature[\"labels\"] = (1,)\n \n-        features_base = [\n-            {\"input_ids\": list(range(10)), \"labels\": (1,)},\n-            {\"input_ids\": list(range(10)), \"labels\": (1,)},\n-        ]\n         whole_word_masking_collator = DataCollatorForWholeWordMask(tokenizer, return_tensors=\"np\")\n \n-        for datatype_input, datatype_label in [(list, list), (np.array, np.array)]:\n-            self._validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-                collator=whole_word_masking_collator,\n-                base_data=features_base,\n-                input_key=\"input_ids\",\n-                input_datatype=datatype_input,\n-                label_key=\"labels\",\n-                label_datatype=datatype_label,\n-                ignore_label=True,\n-            )\n+        self._validate_original_data_against_collated_data(\n+            collator=whole_word_masking_collator, original_data=original_data, batch_data=batch_data\n+        )\n \n     def test_permutation_language_modelling_collator_immutability(self):\n         tokenizer = BertTokenizer(self.vocab_file)\n@@ -1842,3 +1870,98 @@ def test_sentence_order_prediction_collator_immutability(self):\n         self._validate_original_data_against_collated_data(\n             collator=sop_collator, original_data=features_original, batch_data=features_batch\n         )\n+\n+\n+class DataCollatorForLanguageModelingUnitTest(unittest.TestCase):\n+    def test__calc_word_ids_and_prob_mask(self):\n+        offsets = np.array(\n+            [\n+                [(0, 0), (0, 3), (3, 4), (5, 6), (6, 7), (8, 9)],\n+                [(0, 0), (0, 3), (3, 4), (5, 6), (6, 7), (0, 0)],\n+                [(0, 0), (0, 3), (3, 4), (0, 0), (6, 7), (0, 0)],\n+                [(1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7)],\n+                [(1, 1), (2, 2), (3, 4), (5, 6), (7, 8), (9, 10)],\n+                [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)],\n+            ]\n+        )\n+\n+        special_tokens_mask = np.array(\n+            [\n+                [1, 0, 0, 0, 0, 0],\n+                [1, 0, 0, 0, 0, 1],\n+                [1, 0, 0, 1, 0, 1],\n+                [0, 0, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0],\n+                [1, 1, 1, 1, 1, 1],\n+            ]\n+        )\n+\n+        output_word_ids, output_prob_mask = DataCollatorForLanguageModeling._calc_word_ids_and_prob_mask(\n+            offsets, special_tokens_mask\n+        )\n+\n+        expected_word_ids = np.array(\n+            [\n+                [-1, 1, 1, 2, 2, 3],\n+                [-1, 1, 1, 2, 2, -1],\n+                [-1, 1, 1, -1, 2, -1],\n+                [1, 1, 1, 1, 1, 1],\n+                [1, 2, 3, 4, 5, 6],\n+                [-1, -1, -1, -1, -1, -1],\n+            ]\n+        )\n+\n+        expected_prob_mask = np.array(\n+            [\n+                [1, 0, 1, 0, 1, 0],\n+                [1, 0, 1, 0, 1, 1],\n+                [1, 0, 1, 1, 0, 1],\n+                [0, 1, 1, 1, 1, 1],\n+                [0, 0, 0, 0, 0, 0],\n+                [1, 1, 1, 1, 1, 1],\n+            ]\n+        )\n+\n+        np.testing.assert_array_equal(output_word_ids, expected_word_ids)\n+        np.testing.assert_array_equal(output_prob_mask, expected_prob_mask)\n+\n+    def test__whole_word_mask(self):\n+        word_ids = np.array(\n+            [\n+                [-1, 1, 1, 2, 2, 3],\n+                [-1, 1, 1, 2, 2, -1],\n+                [-1, 1, 1, -1, 2, -1],\n+                [1, 1, 1, 1, 1, 1],\n+                [1, 2, 3, 4, 5, 6],\n+                [1, 2, 3, 4, 5, 6],\n+                [-1, -1, -1, -1, -1, -1],\n+            ]\n+        )\n+\n+        mask = np.array(\n+            [\n+                [0, 1, 0, 0, 0, 0],\n+                [0, 1, 0, 1, 0, 0],\n+                [0, 0, 0, 0, 1, 0],\n+                [1, 0, 0, 0, 0, 0],\n+                [0, 0, 0, 0, 0, 0],\n+                [0, 1, 0, 1, 0, 1],\n+                [0, 0, 0, 0, 0, 0],\n+            ]\n+        ).astype(bool)\n+\n+        output_mask = DataCollatorForLanguageModeling._whole_word_mask(word_ids, mask)\n+\n+        expected_mask = np.array(\n+            [\n+                [0, 1, 1, 0, 0, 0],\n+                [0, 1, 1, 1, 1, 0],\n+                [0, 0, 0, 0, 1, 0],\n+                [1, 1, 1, 1, 1, 1],\n+                [0, 0, 0, 0, 0, 0],\n+                [0, 1, 0, 1, 0, 1],\n+                [0, 0, 0, 0, 0, 0],\n+            ]\n+        ).astype(bool)\n+\n+        np.testing.assert_array_equal(output_mask, expected_mask)"
        }
    ],
    "stats": {
        "total": 720,
        "additions": 355,
        "deletions": 365
    }
}