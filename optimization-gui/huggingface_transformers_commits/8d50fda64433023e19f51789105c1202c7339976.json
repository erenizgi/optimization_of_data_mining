{
    "author": "eljandoubi",
    "message": "Remove FSDP wrapping from sub-models. (#34452)\n\n* Remove FSDP wrapping from sub-models.\r\n\r\n* solve conflict trainer.py\r\n\r\n* make fixup\r\n\r\n* add unit test for fsdp_auto_wrap_policy when using auto_find_batch_size\r\n\r\n* put back extract_model_from_parallel\r\n\r\n* use transformers unwrap_model",
    "sha": "8d50fda64433023e19f51789105c1202c7339976",
    "files": [
        {
            "sha": "1603a4ec215557fbf02d13955a8f7e087745d04d",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d50fda64433023e19f51789105c1202c7339976/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d50fda64433023e19f51789105c1202c7339976/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=8d50fda64433023e19f51789105c1202c7339976",
            "patch": "@@ -66,7 +66,7 @@\n from .integrations.deepspeed import deepspeed_init, deepspeed_load_checkpoint, is_deepspeed_available\n from .integrations.tpu import tpu_spmd_dataloader\n from .modelcard import TrainingSummary\n-from .modeling_utils import PreTrainedModel, load_sharded_checkpoint\n+from .modeling_utils import PreTrainedModel, load_sharded_checkpoint, unwrap_model\n from .models.auto.modeling_auto import (\n     MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n     MODEL_MAPPING_NAMES,\n@@ -2277,8 +2277,11 @@ def _inner_training_loop(\n         # FSDP-XLA, SageMaker MP/DP, DataParallel, IPEX\n         use_accelerator_prepare = True if model is self.model else False\n \n-        # configure fsdp plugin for qlora if any\n-        if use_accelerator_prepare:\n+        if use_accelerator_prepare and self.is_fsdp_enabled:\n+            # In case of auto_find_batch_size=True\n+            # Remove FSDP wrapping from sub-models.\n+            self.model = unwrap_model(self.model, recursive=True)\n+            # configure fsdp plugin for qlora if any\n             self._fsdp_qlora_plugin_updates()\n \n         if delay_optimizer_creation:"
        },
        {
            "sha": "eca6a30664f045ca9e99fc068230c6bde390c9cd",
            "filename": "tests/trainer/test_trainer_fsdp.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d50fda64433023e19f51789105c1202c7339976/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d50fda64433023e19f51789105c1202c7339976/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_fsdp.py?ref=8d50fda64433023e19f51789105c1202c7339976",
            "patch": "@@ -117,6 +117,33 @@ def test_trainer(self):\n         execute_subprocess_async(cmd, env=self.get_env())\n         # successful return here == success - any errors would have caused an error in the sub-call\n \n+    class TestFSDPTrainerWrap(TestCasePlus):\n+        @require_accelerate\n+        @require_torch_multi_gpu\n+        @require_fsdp\n+        def test_trainer(self):\n+            output_dir = self.get_auto_remove_tmp_dir()\n+            cmd = [\n+                \"accelerate\",\n+                \"launch\",\n+                \"--use_fsdp\",\n+                \"--main_process_port\",\n+                f\"{get_torch_dist_unique_port()}\",\n+                \"--num_processes\",\n+                f\"{torch.cuda.device_count()}\",\n+                \"--fsdp_transformer_layer_cls_to_wrap\",\n+                \"GPT2Block\",\n+                f\"{self.test_file_dir}/test_trainer_fsdp.py\",\n+                \"--output_dir\",\n+                f\"{output_dir}\",\n+                \"--report_to\",\n+                \"none\",\n+                \"--auto_find_batch_size\",\n+                \"True\",\n+            ]\n+            execute_subprocess_async(cmd, env=self.get_env())\n+            # successful return here == success - any errors would have caused an error in the sub-call\n+\n \n if __name__ == \"__main__\":\n     parser = HfArgumentParser((Seq2SeqTrainingArguments,))"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 33,
        "deletions": 3
    }
}