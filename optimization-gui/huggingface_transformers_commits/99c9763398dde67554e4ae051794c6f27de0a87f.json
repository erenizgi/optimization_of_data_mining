{
    "author": "Phoenix-Shen",
    "message": "Fixed a bug calculating cross entropy loss in `JetMoeForCausalLM` (#37830)\n\nfix: :bug: Fixed a bug in calculating Cross Entropy loss in JetMoeForCausalLM\n\nIn the original code, we shift the logits and pass shift_logits into the self.loss_function, but in self.loss_function, the shift_logits will be shifted again, so we are actually doing \"next next token prediction\", which is incorrect. I have removed the logits shifting before calling self.loss_function.\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "99c9763398dde67554e4ae051794c6f27de0a87f",
    "files": [
        {
            "sha": "4eb834a0f6c04053b17067e4fa69df3287461ea5",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/99c9763398dde67554e4ae051794c6f27de0a87f/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99c9763398dde67554e4ae051794c6f27de0a87f/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=99c9763398dde67554e4ae051794c6f27de0a87f",
            "patch": "@@ -1205,19 +1205,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Ensure tensors are on the same device\n-            shift_labels = shift_labels.to(shift_logits.device)\n             loss = self.loss_function(\n-                shift_logits,\n-                shift_labels,\n+                logits,\n+                labels,\n                 vocab_size=self.config.vocab_size,\n                 **kwargs,\n             )"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 2,
        "deletions": 12
    }
}