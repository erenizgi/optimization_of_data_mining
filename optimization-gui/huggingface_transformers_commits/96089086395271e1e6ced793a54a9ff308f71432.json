{
    "author": "Cyrilvallez",
    "message": "Correct warm-up with fp8 (#37670)\n\n* start clean warmup for quantizers\n\n* style\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "96089086395271e1e6ced793a54a9ff308f71432",
    "files": [
        {
            "sha": "0dbc97781efe709d3ad1731f8545a1ce0085e437",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/96089086395271e1e6ced793a54a9ff308f71432/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/96089086395271e1e6ced793a54a9ff308f71432/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=96089086395271e1e6ced793a54a9ff308f71432",
            "patch": "@@ -4866,7 +4866,7 @@ def _load_pretrained_model(\n         # Warmup cuda to load the weights much faster on devices\n         if device_map is not None and not is_hqq_or_quark:\n             expanded_device_map = expand_device_map(device_map, expected_keys)\n-            caching_allocator_warmup(model_to_load, expanded_device_map, factor=2 if hf_quantizer is None else 4)\n+            caching_allocator_warmup(model_to_load, expanded_device_map, hf_quantizer)\n \n         error_msgs = []\n         # Iterate on all the shards to load the weights\n@@ -5871,7 +5871,7 @@ def is_accelerator_device(device: Union[str, int, torch.device]) -> bool:\n         return torch.device(device).type not in [\"meta\", \"cpu\"]\n \n \n-def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict, factor=2):\n+def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict, hf_quantizer: Optional[HfQuantizer]):\n     \"\"\"This function warm-ups the caching allocator based on the size of the model tensors that will reside on each\n     device. It allows to have one large call to Malloc, instead of recursively calling it later when loading\n     the model, which is actually the loading speed botteneck.\n@@ -5890,6 +5890,8 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict,\n     - Loading speed bottleneck is now almost only tensor copy (i.e. changing the dtype) and moving the tensors to the devices.\n     However, we cannot really improve on those aspects obviously, as the data needs to be moved/copied in the end.\n     \"\"\"\n+    factor = 2 if hf_quantizer is None else hf_quantizer.get_cuda_warm_up_factor()\n+\n     # Remove disk, cpu and meta devices, and cast to proper torch.device\n     accelerator_device_map = {\n         param: torch.device(device) for param, device in expanded_device_map.items() if is_accelerator_device(device)"
        },
        {
            "sha": "d5ae46a0afbb7a04799d2e9e88bfb158990e5ea3",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/96089086395271e1e6ced793a54a9ff308f71432/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/96089086395271e1e6ced793a54a9ff308f71432/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=96089086395271e1e6ced793a54a9ff308f71432",
            "patch": "@@ -252,6 +252,17 @@ def dequantize(self, model):\n \n         return model\n \n+    def get_cuda_warm_up_factor(self):\n+        \"\"\"\n+        The factor to be used in `caching_allocator_warmup` to get the number of bytes to pre-allocate to warm up cuda.\n+        A factor of 2 means we allocate all bytes in the empty model (since we allocate in fp16), a factor of 4 means\n+        we allocate half the memory of the weights residing in the empty model, etc...\n+        \"\"\"\n+        # By default we return 4, i.e. half the model size (this corresponds to the case where the model is not\n+        # really pre-processed, i.e. we do not have the info that weights are going to be 8 bits before actual\n+        # weight loading)\n+        return 4\n+\n     def _dequantize(self, model):\n         raise NotImplementedError(\n             f\"{self.quantization_config.quant_method} has no implementation of `dequantize`, please raise an issue on GitHub.\""
        },
        {
            "sha": "76f6f9221c3c6868da04735bed57ddd87b2460c2",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/96089086395271e1e6ced793a54a9ff308f71432/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/96089086395271e1e6ced793a54a9ff308f71432/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=96089086395271e1e6ced793a54a9ff308f71432",
            "patch": "@@ -200,3 +200,7 @@ def is_serializable(self, safe_serialization=None):\n     @property\n     def is_trainable(self) -> bool:\n         return False\n+\n+    def get_cuda_warm_up_factor(self):\n+        # Pre-processing is done cleanly, so we can allocate everything here\n+        return 2"
        }
    ],
    "stats": {
        "total": 21,
        "additions": 19,
        "deletions": 2
    }
}