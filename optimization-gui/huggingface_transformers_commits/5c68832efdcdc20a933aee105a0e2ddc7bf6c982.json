{
    "author": "vasqu",
    "message": "[`GPT OSS`] Fix false flag (#43120)\n\nfix",
    "sha": "5c68832efdcdc20a933aee105a0e2ddc7bf6c982",
    "files": [
        {
            "sha": "26bd50a53a5fb4e9a9c26b85f325f1af4f7edbbc",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c68832efdcdc20a933aee105a0e2ddc7bf6c982/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c68832efdcdc20a933aee105a0e2ddc7bf6c982/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=5c68832efdcdc20a933aee105a0e2ddc7bf6c982",
            "patch": "@@ -434,7 +434,7 @@ class GptOssPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = False\n-    _supports_flex_attn = False\n+    _supports_flex_attn = True\n \n     _can_compile_fullgraph = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "f5c5e272b2965f163985422a83e6fbd219c616d7",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c68832efdcdc20a933aee105a0e2ddc7bf6c982/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c68832efdcdc20a933aee105a0e2ddc7bf6c982/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=5c68832efdcdc20a933aee105a0e2ddc7bf6c982",
            "patch": "@@ -354,7 +354,6 @@ def forward(\n class GptOssPreTrainedModel(LlamaPreTrainedModel):\n     _keep_in_fp32_modules = [\"post_attention_layernorm\", \"input_layernorm\", \"norm\"]\n     _supports_sdpa = False\n-    _supports_flex_attn = False\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(GptOssTopKRouter, index=0),\n         \"hidden_states\": GptOssDecoderLayer,"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 1,
        "deletions": 2
    }
}