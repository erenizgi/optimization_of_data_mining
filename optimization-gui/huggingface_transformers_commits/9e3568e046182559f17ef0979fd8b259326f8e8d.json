{
    "author": "stevhliu",
    "message": "[docs] Expert parallelism (#42409)\n\n* draft\n\n* ep\n\n* toctree\n\n* warning",
    "sha": "9e3568e046182559f17ef0979fd8b259326f8e8d",
    "files": [
        {
            "sha": "4344ea8c558b38e0f0672c3fc815a3b37981732b",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e3568e046182559f17ef0979fd8b259326f8e8d/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e3568e046182559f17ef0979fd8b259326f8e8d/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=9e3568e046182559f17ef0979fd8b259326f8e8d",
            "patch": "@@ -80,18 +80,20 @@\n       title: Kernels in transformers\n     - local: perf_torch_compile\n       title: torch.compile\n+    - local: perf_infer_gpu_multi\n+      title: Tensor parallelism\n+    - local: expert_parallelism\n+      title: Expert parallelism\n+    - local: kv_cache\n+      title: Cache strategies\n+    - local: cache_explanation\n+      title: Caching\n     - local: perf_infer_gpu_one\n       title: GPU\n-    - local: perf_infer_gpu_multi\n-      title: Distributed inference\n     - local: perf_infer_cpu\n       title: CPU\n     - local: llm_optims\n       title: Optimizing inference\n-    - local: cache_explanation\n-      title: Caching\n-    - local: kv_cache\n-      title: KV cache strategies\n     - local: llm_tutorial_optimization\n       title: Getting the most out of LLMs\n     title: Optimization"
        },
        {
            "sha": "1628b6fda0cd74787c8408ccfcacbd5fa7410eaa",
            "filename": "docs/source/en/expert_parallelism.md",
            "status": "added",
            "additions": 50,
            "deletions": 0,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e3568e046182559f17ef0979fd8b259326f8e8d/docs%2Fsource%2Fen%2Fexpert_parallelism.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e3568e046182559f17ef0979fd8b259326f8e8d/docs%2Fsource%2Fen%2Fexpert_parallelism.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fexpert_parallelism.md?ref=9e3568e046182559f17ef0979fd8b259326f8e8d",
            "patch": "@@ -0,0 +1,50 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Expert parallelism\n+\n+[Expert parallelism](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=expert_parallelism) is a parallelism strategy for [mixture-of-experts (MoE) models](https://huggingface.co/blog/moe). Each expert's feedforward layer lives on a different hardware accelerator. A router dispatches tokens to the appropriate experts and gathers the results. This approach scales models to far larger parameter counts without increasing computation cost because each token activates only a few experts.\n+\n+## DistributedConfig\n+\n+> [!WARNING]\n+> The [`DistributedConfig`] API is experimental and its usage may change in the future.\n+\n+Enable expert parallelism with the [`DistributedConfig`] class and the `enable_expert_parallel` argument.\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers.distributed.configuration_utils import DistributedConfig\n+\n+distributed_config = DistributedConfig(enable_expert_parallel=True)\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"openai/gpt-oss-120b\",\n+    dtype=\"auto\",\n+    distributed_config=distributed_config,\n+)\n+```\n+\n+> [!TIP]\n+> Expert parallelism automatically enables [tensor parallelism](./perf_infer_gpu_multi) for attention layers.\n+\n+This argument switches to the `ep_plan` (expert parallel plan) defined in each MoE model's config file. The [`GroupedGemmParallel`] class splits expert weights so each device loads only its local experts. The `ep_router` routes tokens to experts and an all-reduce operation combines their outputs.\n+\n+Launch your inference script with [torchrun](https://pytorch.org/docs/stable/elastic/run.html) and specify how many devices to use. The number of devices must evenly divide the total number of experts.\n+\n+```zsh\n+torchrun --nproc-per-node 8 your_script.py\n+```"
        },
        {
            "sha": "2b27ea7c9219a6f81d2aafb3a8677a2e3c9d2fc4",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e3568e046182559f17ef0979fd8b259326f8e8d/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e3568e046182559f17ef0979fd8b259326f8e8d/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=9e3568e046182559f17ef0979fd8b259326f8e8d",
            "patch": "@@ -14,7 +14,7 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# KV cache strategies\n+# Cache strategies\n \n The key-value (KV) vectors are used to calculate attention scores. For autoregressive models, KV scores are calculated *every* time because the model predicts one token at a time. Each prediction depends on the previous tokens, which means the model performs the same computations each time.\n "
        },
        {
            "sha": "a6e67eba40a6273a91c4755f53c5cfdb74329e73",
            "filename": "docs/source/en/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 43,
            "deletions": 44,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e3568e046182559f17ef0979fd8b259326f8e8d/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e3568e046182559f17ef0979fd8b259326f8e8d/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md?ref=9e3568e046182559f17ef0979fd8b259326f8e8d",
            "patch": "@@ -13,16 +13,11 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Distributed inference\n+# Tensor parallelism\n \n-When a model doesn't fit on a single GPU, distributed inference with [tensor parallelism](./perf_train_gpu_many#tensor-parallelism) can help. Tensor parallelism shards a model onto multiple accelerators (CUDA GPU, Intel XPU, etc.) and parallelizes computations such as matrix multiplication. It enables fitting larger model sizes into memory and is faster because each accelerator can process a tensor slice.\n+[Tensor parallelism](./perf_train_gpu_many#tensor-parallelism) slices a model layer into pieces so multiple hardware accelerators work on it simultaneously. This lets you run models that exceed a single GPU's memory capacity and achieve higher throughput. You'll need fast intra-node communication because GPUs exchange partial results at each layer.\n \n-However, tensor parallelism adds communication overhead and should be used on single machine setups with multiple accelerators to take advantage of fast intra-node communication. For multi-node training, it may be more efficient to use pipeline or data parallelism depending on your use case.\n-\n-> [!TIP]\n-> Refer to the [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism) section on tensor parallelism to learn more.\n-\n-Check the list below for models that natively support tensor parallelism. Open a GitHub issue or pull request to add support for a model.\n+The list below shows models with native tensor parallelism support. Open a GitHub issue or pull request to add support for a model.\n \n <details>\n <summary>Show supported models</summary>\n@@ -41,13 +36,13 @@ Check the list below for models that natively support tensor parallelism. Open a\n \n </details>\n \n-This guide shows how to enable tensor parallelism with Transformers and different partitioning strategies.\n+This guide covers enabling tensor parallelism in Transformers and the available partitioning strategies.\n \n ## Partitioning a model\n \n-Transformers supports tensor parallelism if a model has a `tp_plan`. There are two ways to partition a model.\n+Transformers enables tensor parallelism when a model has a `tp_plan`. Choose from two partitioning methods.\n \n-- Set `tp_plan=\"auto\"` to automatically use a tensor parallelism plan based on a model's predefined configuration.\n+- Set `tp_plan=\"auto\"` for an automatic plan based on the model's predefined configuration.\n - Define and pass a manual `tp_plan`.\n \n <hfoptions id=\"tp_plan\">\n@@ -70,7 +65,7 @@ inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n outputs = model(inputs)\n ```\n \n-Launch the inference script above on [torchrun](https://pytorch.org/docs/stable/elastic/run.html) with 4 processes per GPU.\n+Launch the inference script with [torchrun](https://pytorch.org/docs/stable/elastic/run.html). Use 4 processes per GPU.\n \n ```bash\n torchrun --nproc-per-node 4 demo.py\n@@ -79,9 +74,9 @@ torchrun --nproc-per-node 4 demo.py\n </hfoption>\n <hfoption id=\"manual plan\">\n \n-Define a tensor parallel plan for each layer in `tp_plan` and pass it to [`~PreTrainedModel.from_pretrained`]. The example below uses column and row partitioning. See the [Partitioning strategies](#partitioning-strategies) section for other supported strategies.\n+Define a tensor parallel plan for each layer in `tp_plan`. Pass it to [`~PreTrainedModel.from_pretrained`]. The example below uses column and row partitioning. See the [Partitioning strategies](#partitioning-strategies) section for other supported strategies.\n \n-Manual partitioning requires deep understanding of model architecture and strategy interactions. Poor partitioning choices create slow models that fail or produce incorrect results. The [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism) explains partitioning strategies in detail.\n+Manual partitioning requires a deep understanding of model architecture and strategy interactions. Poor partitioning choices create slow models that fail or produce incorrect results. The [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism) explains partitioning strategies in detail.\n \n ```py\n from transformers import AutoModelForCausalLM\n@@ -103,7 +98,7 @@ print(model.tp_plan)\n \n ## Partitioning strategies\n \n-All partitioning strategies are defined in the [`ParallelInterface`] class which maps a string to the strategy implementation. You don't need to interact with this class directly since all the strategies are set with `tp_plan` in [`~PreTrainedModel.from_pretrained`], but it is useful for checking what strategies are available.\n+The [`ParallelInterface`] class defines all partitioning strategies. It maps a string to the strategy implementation. You don't need to interact with this class directly since you set strategies with `tp_plan` in [`~PreTrainedModel.from_pretrained`]. It's useful for checking available strategies.\n \n ```py\n class ParallelInterface(MutableMapping):\n@@ -127,22 +122,22 @@ class ParallelInterface(MutableMapping):\n     }\n ```\n \n-Refer to the table below to learn more about each strategy.\n+The table below describes each strategy.\n \n | Strategy | Description |\n |---|---|\n-| `ColwiseParallel` | Column-wise partitioning of weights and biases. |\n-| `RowwiseParallel` | Row-wise partitioning of weights and biases. Also supports partitioning `nn.Embedding` modules. |\n-| `SequenceParallel` | Sequence parallel implementation to support `LayerNorm` and `Dropout` layers. Also supports Python implementation of [RMSNorm](https://github.com/facebookresearch/llama/blob/main/llama/model.py#L34). |\n-| `PackedColwiseParallel` | Variant of `ColwiseParallel` to support packed weights (for example, packing `up_proj` and `gate_proj` together). Refer to the [code](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108) for more details. |\n-| `PackedRowwiseParallel` | Variant of `RowwiseParallel` to support packed weights (refer to the [code](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108) for more details). |\n-| `GatherParallel` | Gather outputs of the module across devices. |\n-| `IsolatedParallel` | Used for Experts in Mixture-of-Experts (MoE) layers to isolates module from other devices. |\n-| `ReplicateParallel` | Replicate modules across all devices to prevent `torch.distributed` APIs from breaking due to a partially sharded model. |\n+| `ColwiseParallel` | Partitions weights and biases column-wise. |\n+| `RowwiseParallel` | Partitions weights and biases row-wise. Supports `nn.Embedding` modules partitioning. |\n+| `SequenceParallel` | Sequence parallel implementation to support `LayerNorm` and `Dropout` layers. Supports Python implementation of [RMSNorm](https://github.com/facebookresearch/llama/blob/main/llama/model.py#L34). |\n+| `PackedColwiseParallel` | A variant of `ColwiseParallel` that supports packed weights (for example, packing `up_proj` and `gate_proj` together). Refer to the [code](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108) for more details. |\n+| `PackedRowwiseParallel` | A variant of `RowwiseParallel` that supports packed weights (refer to the [code](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108) for more details). |\n+| `GatherParallel` | Gathers module outputs across devices. |\n+| `IsolatedParallel` | Isolates a module from other devices. Used for Experts in Mixture-of-Experts (MoE) layers. |\n+| `ReplicateParallel` | Replicates modules across all devices. Prevents `torch.distributed` APIs from breaking due to a partially sharded model. |\n \n ### Packed strategies\n \n-Weight packing packs multiple linear layers into a single, bigger layer. Packed strategies, `PackedColwiseParallel` and `PackedRowwiseParallel`, are used to shard packed weights. The more basic `ColwiseParallel` or `RowwiseParallel` will incorrectly shard the packed weights.\n+Weight packing combines multiple linear layers into a single, larger layer. The `PackedColwiseParallel` and `PackedRowwiseParallel` strategies shard packed weights correctly. Basic `ColwiseParallel` or `RowwiseParallel` strategies shard packed weights incorrectly.\n \n The example below packs `up_proj` and `gate_proj` into a single `gate_up_proj` module and requires the `PackedRowwiseParallel` strategy to shard `gate_up_proj`.\n \n@@ -152,7 +147,7 @@ class Llama4TextExperts(nn.Module):\n     self.gate_up_proj = nn.Parameter(torch.zeros(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n ```\n \n-Batch matrix multiplication can be used in the `forward` pass to compute the output of the `gate_up_proj` module.\n+Use batch matrix multiplication in the `forward` pass to compute the output of the `gate_up_proj` module.\n \n ```python\n def forward(self, hidden_states):\n@@ -162,11 +157,11 @@ def forward(self, hidden_states):\n ```\n \n > [!TIP]\n-> Refer to [this comment](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108) for an visual representation of why `Packed*` needs to be used.\n+> See [this comment](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108) for a visual representation of why `Packed*` needs to be used.\n \n ### Local strategies\n \n-Local strategies (`local_colwise`, `local_rowwise`, `local_packed_rowwise`) don't use [DTensor](https://docs.pytorch.org/docs/stable/distributed.tensor.html) because it isn't supported for some operations such as [torch.chunk](https://docs.pytorch.org/docs/stable/generated/torch.chunk.html). Instead, local strategies use the basic [torch.Tensor](https://docs.pytorch.org/docs/stable/tensors.html) and performs some of the distributed logic manually.\n+Local strategies (`local_colwise`, `local_rowwise`, `local_packed_rowwise`) don't use [DTensor](https://docs.pytorch.org/docs/stable/distributed.tensor.html) because it lacks support for some operations like [torch.chunk](https://docs.pytorch.org/docs/stable/generated/torch.chunk.html). Instead, local strategies use the basic [torch.Tensor](https://docs.pytorch.org/docs/stable/tensors.html) and perform distributed logic manually.\n \n <!--\n Readd this when I get the exact error message\n@@ -176,13 +171,13 @@ Readd this when I get the exact error message\n \n ## Custom partitioning strategies\n \n-A custom partitioning strategy should inherit from [`TensorParallelLayer`](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py) and implement `partition_tensor`, `_prepare_input_fn` and `_prepare_output_fn`.\n+Inherit from [TensorParallelLayer](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py) to create a custom partitioning strategy. Implement `partition_tensor`, `_prepare_input_fn` and `_prepare_output_fn`.\n \n-Then it needs to be registered in the `ParallelInterface` mapping so the dispatching logic can find it when specified in `tp_plan`.\n+Register the strategy in the `ParallelInterface` mapping so the dispatching logic finds it when specified in `tp_plan`.\n \n The example below shows how to implement `ColwiseParallel` with this workflow.\n \n-1. Inherit from `TensorParallelLayer`. In the `__init__` method, define `input_layouts` and `output_layouts` to describe how the input and output tensors should be placed on devices. The `desired_input_layouts` attribute is used to specify how the input *should* be placed on devices.\n+1. Inherit from `TensorParallelLayer`. In the `__init__` method, define `input_layouts` and `output_layouts` to describe how the input and output tensors should be placed on devices. The `desired_input_layouts` attribute is used to specify *how* the input should be placed on devices.\n \n     ```python\n     class ColwiseParallel(TensorParallelLayer):\n@@ -201,7 +196,7 @@ The example below shows how to implement `ColwiseParallel` with this workflow.\n             self.use_dtensor = use_dtensor\n     ```\n \n-2. Implement the `partition_tensor`, `_prepare_input_fn` and `_prepare_output_fn` methods.\n+2. Implement the `partition_tensor`, `_prepare_input_fn`, and `_prepare_output_fn` methods.\n \n     The `partition_tensor` method partitions the tensor and fills `empty_param` with the partitioned tensor. Use the utility function `get_tensor_shard` to help you get the correct shard of the original parameter for a given rank and `get_packed_weights` to help with packed weights.\n \n@@ -249,21 +244,21 @@ The example below shows how to implement `ColwiseParallel` with this workflow.\n \n ## Benchmarks\n \n-Tensor parallelism can considerably speedup inference, especially for inputs with large batch sizes or long sequences.\n+Tensor parallelism significantly speeds up inference, especially for large batch sizes or long sequences.\n \n-Refer to the chart below for the expected speedup for a single forward pass on [Llama](./model_doc/llama) with a sequence length of 512.\n+This chart shows the expected speedup for a single forward pass on [Llama](./model_doc/llama) with a sequence length of 512.\n \n <div style=\"text-align: center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Meta-Llama-3-8B-Instruct%2C%20seqlen%20%3D%20512%2C%20python%2C%20w_%20compile.png\">\n </div>\n \n ## Design implementation\n \n-The Transformers tensor parallelism implementation is framework-agnostic, but for specific implementations, we rely on [DeviceMesh](https://docs.pytorch.org/tutorials/recipes/distributed_device_mesh.html) and [DTensor](https://docs.pytorch.org/docs/stable/distributed.tensor.html) from [torch.distributed](https://docs.pytorch.org/tutorials/beginner/dist_overview.html) to provide a simple and extensible interface.\n+Transformers implements tensor parallelism in a framework-agnostic way. It relies on [DeviceMesh](https://docs.pytorch.org/tutorials/recipes/distributed_device_mesh.html) and [DTensor](https://docs.pytorch.org/docs/stable/distributed.tensor.html) from [torch.distributed](https://docs.pytorch.org/tutorials/beginner/dist_overview.html) to provide a simple, extensible interface.\n \n ### DeviceMesh\n \n-Imagine `DeviceMesh` as a multi-dimensional grid of devices that communicate together. Different parallelization strategies require different types of communication patterns, so you can create a `DeviceMesh` with multiple sub-meshes.\n+`DeviceMesh` creates a multi-dimensional grid of devices that communicate together. Different parallelization strategies require different communication patterns. Create a `DeviceMesh` with multiple sub-meshes to handle these patterns.\n \n ```python\n from torch.distributed.device_mesh import init_device_mesh\n@@ -272,15 +267,15 @@ from torch.distributed.device_mesh import init_device_mesh\n device_mesh = init_device_mesh(\"cuda\", (4,), mesh_dim_names=[\"tp\"])\n ```\n \n-Most of the `torch.distributed` defined parallelization strategies can be applied to the mesh itself, or its sub-mesh, and it automatically handles the communication patterns.\n+Most `torch.distributed` parallelization strategies apply to the mesh itself or its sub-mesh. The mesh automatically handles communication patterns.\n \n ### DTensor\n \n-`DTensor` (Distributed Tensor) is a tensor subclass that handles the distributed logic on top of the usual tensor operations. Most of the model weights in tensor parallelism are stored as `DTensor`s.\n+`DTensor` (Distributed Tensor) handles distributed logic on top of usual tensor operations. Most model weights in tensor parallelism are stored as `DTensor`s.\n \n-The most important part of DTensor is the `placement` attribute because it tells PyTorch how a tensor is placed on the devices in `DeviceMesh`. The `placement` attribute can take the following values.\n+The `placement` attribute tells PyTorch how to place a tensor on devices in `DeviceMesh`. It accepts the following values:\n \n-- `Shard(dimension)` - Indicates how a `DTensor` is sharded across a given dimension, over the `DeviceMesh` it was constructed under. The example below demonstrates how to shard weights over different dimensions for column-wise partitioning.\n+- `Shard(dimension)` shards a `DTensor` across a given dimension over the `DeviceMesh` it was constructed under. The example below shows how to shard weights over different dimensions for column-wise partitioning.\n \n     ```python\n     weight = ...\n@@ -289,7 +284,7 @@ The most important part of DTensor is the `placement` attribute because it tells\n     bias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Shard(-1)]) # Shard across the ONLY dimension\n     ```\n \n-    This example demonstrates how to shard weights over different dimensions for row-wise partitioning.\n+    This example shows how to shard weights over different dimensions for row-wise partitioning.\n \n     ```python\n     weight = ...\n@@ -298,15 +293,19 @@ The most important part of DTensor is the `placement` attribute because it tells\n     bias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Replicate()]) # Replicate bias across all GPUs\n     ```\n \n-- `Replicate()` - Indicates a `DTensor` is replicated across the `DeviceMesh`. It only creates a full copy of the tensor on each device.\n+- `Replicate()` replicates a `DTensor` across the `DeviceMesh`. It creates a full copy of the tensor on each device.\n \n     ```py\n     bias = ...\n     bias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Replicate()]) # Replicate bias across all GPUs\n     ```\n \n-- `Partial()` - Indicates a tensor is pending a reduction operation (not typically relevant for usage in Transformers).\n+- `Partial()` indicates a tensor is pending a reduction operation (not typically relevant for Transformers usage).\n \n ## Resources\n \n-Read the [Tensor Parallelism (TP) in Transformers: 5 Minutes to Understand](https://huggingface.co/blog/qgallouedec/tp) blog post for a quick overview of tensor parallelism and learn how column and row parallel setups differ.\n\\ No newline at end of file\n+- The [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism) section on tensor parallelism provides more details.\n+\n+- Check the [expert parallelism](./expert_parallelism) guide if you're using a mixture-of-experts (MoE) model. These models support tensor parallelism and expert parallelism.\n+\n+- Read the [Tensor Parallelism (TP) in Transformers: 5 Minutes to Understand](https://huggingface.co/blog/qgallouedec/tp) blog post for a quick overview of tensor parallelism and learn how column and row parallel setups differ.\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 153,
        "additions": 102,
        "deletions": 51
    }
}