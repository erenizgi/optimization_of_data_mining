{
    "author": "reedrya",
    "message": "Update HuBERT model card according to template (#39742)\n\n* Update HuBERT model card according to template\n\nStandardized HuBERT doc, added ASR examples, Flash Attention 2 support, and quantization section.\n\n* Address review comments and changes requested to hubert.md\n\n* Update hubert.md\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "b59140b696346005320f461cb4b558d475234c06",
    "files": [
        {
            "sha": "7396565087f8723860c5068261d1c33ba4485c43",
            "filename": "docs/source/en/model_doc/hubert.md",
            "status": "modified",
            "additions": 79,
            "deletions": 52,
            "changes": 131,
            "blob_url": "https://github.com/huggingface/transformers/blob/b59140b696346005320f461cb4b558d475234c06/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b59140b696346005320f461cb4b558d475234c06/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md?ref=b59140b696346005320f461cb4b558d475234c06",
            "patch": "@@ -14,88 +14,115 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Hubert\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n+# HuBERT\n+\n+[HuBERT](https://huggingface.co/papers/2106.07447) is a self-supervised speech model to cluster aligned target labels for BERT-like prediction loss and applying the prediction loss only over masked regions to force the model to learn both acoustic and language modeling over continuous inputs. It addresses the challenges of multiple sound units per utterance, no lexicon during pre-training, and variable-length sound units without explicit segmentation.\n+\n+You can find all the original HuBERT checkpoints under the [HuBERT](https://huggingface.co/collections/facebook/hubert-651fca95d57549832161e6b6) collection.\n \n-Hubert was proposed in [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://huggingface.co/papers/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan\n-Salakhutdinov, Abdelrahman Mohamed.\n+> [!TIP]\n+> This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n+>\n+> Click on the HuBERT models in the right sidebar for more examples of how to apply HuBERT to different audio tasks.\n \n-The abstract from the paper is the following:\n+The example below demonstrates how to automatically transcribe speech into text with [`Pipeline`] or the [`AutoModel`] class.\n \n-*Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are\n-multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training\n-phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we\n-propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an\n-offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our\n-approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined\n-acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised\n-clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means\n-teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the\n-state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h,\n-10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER\n-reduction on the more challenging dev-other and test-other evaluation subsets.*\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n-This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n+```python\n+import torch\n+from transformers import pipeline\n \n-# Usage tips\n+pipeline = pipeline(\n+    task=\"automatic-speech-recognition\",\n+    model=\"facebook/hubert-large-ls960-ft\",\n+    torch_dtype=torch.float16,\n+    device=0\n+)\n \n-- Hubert is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n-- Hubert model was fine-tuned using connectionist temporal classification (CTC) so the model output has to be decoded\n-  using [`Wav2Vec2CTCTokenizer`].\n-- The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`  \n+pipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\")\n+```\n \n-## Using Flash Attention 2\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n-Flash Attention 2 is an faster, optimized version of the model.\n+```python\n+import torch\n+from transformers import AutoProcessor, AutoModelForCTC\n+from datasets import load_dataset\n \n-### Installation \n+dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\").sort(\"id\")\n+sampling_rate = dataset.features[\"audio\"].sampling_rate\n \n-First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the [official documentation](https://github.com/Dao-AILab/flash-attention#installation-and-features). If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered [above](https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer).\n+processor = AutoProcessor.from_pretrained(\"facebook/hubert-base-ls960\")\n+model = AutoModelForCTC.from_pretrained(\"facebook/hubert-base-ls960\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n \n-Next, [install](https://github.com/Dao-AILab/flash-attention#installation-and-features) the latest version of Flash Attention 2:\n+inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n+with torch.no_grad():\n+    logits = model(**inputs).logits\n+predicted_ids = torch.argmax(logits, dim=-1)\n \n-```bash\n-pip install -U flash-attn --no-build-isolation\n+transcription = processor.batch_decode(predicted_ids)\n+print(transcription[0])\n ```\n \n-### Usage\n+</hfoption>\n+</hfoptions>\n \n-Below is an expected speedup diagram comparing the pure inference time between the native implementation in transformers of `facebook/hubert-large-ls960-ft`, the flash-attention-2 and the sdpa (scale-dot-product-attention) version. We show the average speedup obtained on the `librispeech_asr` `clean` validation split: \n+## Quantization\n \n-```python\n->>> from transformers import HubertModel\n->>> import torch\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision.\n+Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n->>> model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(\"cuda\")\n-...\n-```\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to quantize the weights to 4-bits.\n+\n+```python\n+import torch\n+from transformers import AutoProcessor, AutoModelForCTC, BitsAndBytesConfig\n+from datasets import load_dataset\n \n-### Expected speedups\n+bnb_config = BitsAndBytesConfig(\n+    load_in_8bit=True,\n+    llm_int8_threshold=6.0\n+)\n \n-Below is an expected speedup diagram comparing the pure inference time between the native implementation in transformers of the `facebook/hubert-large-ls960-ft` model and the flash-attention-2 and sdpa (scale-dot-product-attention) versions. . We show the average speedup obtained on the `librispeech_asr` `clean` validation split: \n+dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\").sort(\"id\")\n+sampling_rate = dataset.features[\"audio\"].sampling_rate\n \n+processor = AutoProcessor.from_pretrained(\"facebook/hubert-base-ls960\")\n+model = AutoModelForCTC.from_pretrained(\"facebook/hubert-base-ls960\", quantization_config=bnb_config, torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n \n-<div style=\"text-align: center\">\n-<img src=\"https://huggingface.co/datasets/kamilakesbi/transformers_image_doc/resolve/main/data/Hubert_speedup.png\">\n-</div>\n+inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n+with torch.no_grad():\n+    logits = model(**inputs).logits\n+predicted_ids = torch.argmax(logits, dim=-1)\n \n+transcription = processor.batch_decode(predicted_ids)\n+print(transcription[0])\n+```\n \n-## Resources\n+## Notes\n \n-- [Audio classification task guide](../tasks/audio_classification)\n-- [Automatic speech recognition task guide](../tasks/asr)\n+- HuBERT models expect raw audio input as a 1D float array sampled at 16kHz.\n+- If you want to use a `head_mask`, use the model with `attn_implementation=\"eager\"`.\n+  ```python\n+  model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\", attn_implementation=\"eager\")\n+  ```\n \n ## HubertConfig\n \n [[autodoc]] HubertConfig\n+    - all\n \n <frameworkcontent>\n <pt>"
        }
    ],
    "stats": {
        "total": 131,
        "additions": 79,
        "deletions": 52
    }
}