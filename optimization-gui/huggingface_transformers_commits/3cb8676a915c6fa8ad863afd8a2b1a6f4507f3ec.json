{
    "author": "SunMarc",
    "message": "Fix CI by tweaking torchao tests (#34832)",
    "sha": "3cb8676a915c6fa8ad863afd8a2b1a6f4507f3ec",
    "files": [
        {
            "sha": "ac81864e50869b199f6409dd78b1ac3c3c512e8a",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3cb8676a915c6fa8ad863afd8a2b1a6f4507f3ec/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3cb8676a915c6fa8ad863afd8a2b1a6f4507f3ec/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=3cb8676a915c6fa8ad863afd8a2b1a6f4507f3ec",
            "patch": "@@ -1264,8 +1264,13 @@ def post_init(self):\n         r\"\"\"\n         Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n         \"\"\"\n-        if not version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.4.0\"):\n-            raise ValueError(\"Requires torchao 0.4.0 version and above\")\n+        if is_torchao_available():\n+            if not version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.4.0\"):\n+                raise ValueError(\"Requires torchao 0.4.0 version and above\")\n+        else:\n+            raise ValueError(\n+                \"TorchAoConfig requires torchao to be installed, please install with `pip install torchao`\"\n+            )\n \n         _STR_TO_METHOD = self._get_torchao_quant_type_to_method()\n         if self.quant_type not in _STR_TO_METHOD.keys():"
        },
        {
            "sha": "d0263f45f18075d7adb660c8c88f11347731eaba",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3cb8676a915c6fa8ad863afd8a2b1a6f4507f3ec/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3cb8676a915c6fa8ad863afd8a2b1a6f4507f3ec/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=3cb8676a915c6fa8ad863afd8a2b1a6f4507f3ec",
            "patch": "@@ -246,12 +246,13 @@ class TorchAoSerializationTest(unittest.TestCase):\n     # TODO: investigate why we don't have the same output as the original model for this test\n     SERIALIZED_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n-    quant_config = TorchAoConfig(\"int4_weight_only\", group_size=32)\n+    quant_scheme, quant_scheme_kwargs = \"int4_weight_only\", {\"group_size\": 32}\n     device = \"cuda:0\"\n \n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n+        cls.quant_config = TorchAoConfig(cls.quant_scheme, **cls.quant_scheme_kwargs)\n         cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n             cls.model_name,\n             torch_dtype=torch.bfloat16,\n@@ -290,21 +291,21 @@ def test_serialization_expected_output(self):\n \n \n class TorchAoSerializationW8A8Test(TorchAoSerializationTest):\n-    quant_config = TorchAoConfig(\"int8_dynamic_activation_int8_weight\")\n+    quant_scheme, quant_scheme_kwargs = \"int8_dynamic_activation_int8_weight\", {}\n     ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n     device = \"cuda:0\"\n \n \n class TorchAoSerializationW8Test(TorchAoSerializationTest):\n-    quant_config = TorchAoConfig(\"int8_weight_only\")\n+    quant_scheme, quant_scheme_kwargs = \"int8_weight_only\", {}\n     ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n     device = \"cuda:0\"\n \n \n class TorchAoSerializationW8A8CPUTest(TorchAoSerializationTest):\n-    quant_config = TorchAoConfig(\"int8_dynamic_activation_int8_weight\")\n+    quant_scheme, quant_scheme_kwargs = \"int8_dynamic_activation_int8_weight\", {}\n     ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n     device = \"cpu\"\n@@ -318,7 +319,7 @@ def test_serialization_expected_output_cuda(self):\n \n \n class TorchAoSerializationW8CPUTest(TorchAoSerializationTest):\n-    quant_config = TorchAoConfig(\"int8_weight_only\")\n+    quant_scheme, quant_scheme_kwargs = \"int8_weight_only\", {}\n     ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n     device = \"cpu\""
        }
    ],
    "stats": {
        "total": 20,
        "additions": 13,
        "deletions": 7
    }
}