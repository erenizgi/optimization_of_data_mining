{
    "author": "zucchini-nlp",
    "message": "Bart: new cache format (#35314)\n\n* bart compile\n\n* add mbart\n\n* some more models touched by fix-copies\n\n* more\n\n* more models\n\n* even more models\n\n* fix copies\n\n* fix tests\n\n* fix copies\n\n* fix\n\n* biogpt accepts position ids now (breaking?)\n\n* fix failing non-slow tests\n\n* fix some tests\n\n* should not be removed\n\n* small update\n\n* Update src/transformers/models/bart/modeling_bart.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* update for last `main`\n\n* fix copies\n\n* clone `update_causal_mask` from llama\n\n* tmp\n\n* fixup\n\n* why? how?\n\n* fix bart tests\n\n* dont skip test\n\n* address comments\n\n* fix tests\n\n* fix\n\n* fixup and delete the file\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "01ad9f4b493726a159429c86595fbcf0ac5419ed",
    "files": [
        {
            "sha": "bb922203055301679a09c601be8a074b2f4ee6b2",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -28,7 +28,7 @@\n if is_sklearn_available():\n     from sklearn.metrics import roc_curve\n \n-from ..cache_utils import DynamicCache\n+from ..cache_utils import Cache\n from ..pytorch_utils import isin_mps_friendly\n from .logits_process import LogitsProcessorList, MinLengthLogitsProcessor, SuppressTokensLogitsProcessor\n \n@@ -1183,7 +1183,9 @@ def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor,\n def _crop_past_key_values(model, past_key_values, max_length):\n     \"\"\"Crops the past key values up to a certain maximum length.\"\"\"\n     new_past = []\n-    if model.config.is_encoder_decoder:\n+    if isinstance(past_key_values, Cache):\n+        past_key_values.crop(max_length)\n+    elif model.config.is_encoder_decoder:\n         for idx in range(len(past_key_values)):\n             new_past.append(\n                 (\n@@ -1204,8 +1206,6 @@ def _crop_past_key_values(model, past_key_values, max_length):\n         else:\n             for idx in range(len(past_key_values)):\n                 past_key_values[idx] = past_key_values[idx][:, :, :max_length, :]\n-    elif isinstance(past_key_values, DynamicCache):\n-        past_key_values.crop(max_length)\n     elif past_key_values is not None:\n         for idx in range(len(past_key_values)):\n             if past_key_values[idx] != ([], []):"
        },
        {
            "sha": "9f7f1515a27821a1a4665ad14f4c483673ffb84c",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -370,13 +370,16 @@ def _init_weight(self):\n         self.weight = nn.Parameter(out, requires_grad=False)\n \n     @torch.no_grad()\n-    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:\n+    def forward(\n+        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ) -> torch.Tensor:\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n-        bsz, seq_len = input_ids_shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        )\n-        return super().forward(positions)\n+        if position_ids is None:\n+            bsz, seq_len = input_ids_shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            )\n+        return super().forward(position_ids)\n \n \n # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesValueEmbedding with TimeSeries->Autoformer"
        },
        {
            "sha": "3fb6c8b1c4b4634850da3cc41ba88cda59b99945",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 384,
            "deletions": 219,
            "changes": 603,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -25,12 +25,12 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n-    _prepare_4d_causal_attention_mask,\n-    _prepare_4d_causal_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n@@ -43,10 +43,21 @@\n     Seq2SeqSequenceClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from .configuration_bart import BartConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n@@ -81,15 +92,18 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         self.offset = 2\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n \n-    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n+    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: torch.Tensor = None):\n         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n \n-        bsz, seq_len = input_ids.shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        ).expand(bsz, -1)\n+        if position_ids is None:\n+            bsz, seq_len = input_ids.shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            ).expand(bsz, -1)\n+        else:\n+            position_ids = position_ids.unsqueeze(0)\n \n-        return super().forward(positions + self.offset)\n+        return super().forward(position_ids + self.offset)\n \n \n class BartScaledWordEmbedding(nn.Embedding):\n@@ -117,6 +131,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[BartConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -133,73 +148,74 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -213,10 +229,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -278,73 +291,73 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n-    def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        # BartFlashAttention2 attention does not support output_attentions\n+        if output_attentions:\n+            raise ValueError(\n+                \"BartSdpaAttention2 attention does not support `output_attentions`. \"\n+                \"Use the argument `attn_implementation='eager'` when loading the model.\"\n+            )\n+\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, q_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self._reshape(self.q_proj(hidden_states), -1, bsz)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim)\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0].transpose(1, 2)\n-            value_states = past_key_value[1].transpose(1, 2)\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._reshape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0].transpose(1, 2), key_states], dim=1)\n-            value_states = torch.cat([past_key_value[1].transpose(1, 2), value_states], dim=1)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states.transpose(1, 2), value_states.transpose(1, 2))\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value[0].shape[-2]\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n+\n+        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n+        # to be able to avoid many of these transpose/reshape/view.\n+        key_states = key_states.transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n \n         # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n         # therefore the input hidden states gets silently casted in float32. Hence, we need\n         # cast them back in the correct dtype just to be sure everything works as expected.\n         # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n         # in fp32. (LlamaRMSNorm handles it correctly)\n-\n         input_dtype = query_states.dtype\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n@@ -379,21 +392,19 @@ def forward(\n         attn_output = attn_output.reshape(bsz, q_len, -1)\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, None, past_key_value\n \n \n class BartSdpaAttention(BartAttention):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         if output_attentions:\n@@ -408,6 +419,7 @@ def forward(\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -417,74 +429,72 @@ def forward(\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        query_states = self._shape(query_states, tgt_len, bsz)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n+\n+        causal_mask = None\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n+        is_causal = True if self.is_causal and causal_mask is None and tgt_len > 1 else False\n \n         # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n         # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n+            attn_mask=causal_mask,\n             dropout_p=self.dropout if self.training else 0.0,\n             is_causal=is_causal,\n         )\n \n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n \n         # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n         # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n+        attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n         attn_output = self.out_proj(attn_output)\n \n         return attn_output, None, past_key_value\n@@ -498,7 +508,7 @@ def forward(\n \n \n class BartEncoderLayer(nn.Module):\n-    def __init__(self, config: BartConfig):\n+    def __init__(self, config: BartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -507,6 +517,7 @@ def __init__(self, config: BartConfig):\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.dropout = config.dropout\n@@ -568,7 +579,7 @@ def forward(\n \n \n class BartDecoderLayer(nn.Module):\n-    def __init__(self, config: BartConfig):\n+    def __init__(self, config: BartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -579,6 +590,7 @@ def __init__(self, config: BartConfig):\n             is_decoder=True,\n             is_causal=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -591,6 +603,7 @@ def __init__(self, config: BartConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -605,9 +618,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -626,47 +640,42 @@ def forward(\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         residual = hidden_states\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.activation_fn(self.fc1(hidden_states))\n@@ -682,7 +691,7 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n@@ -721,6 +730,8 @@ class BartPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -732,6 +743,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n     @property\n     def dummy_inputs(self):\n@@ -743,6 +757,131 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n class PretrainedBartModel(BartPreTrainedModel):\n     def __init_subclass__(self):\n@@ -792,7 +931,7 @@ def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = No\n             config.max_position_embeddings,\n             embed_dim,\n         )\n-        self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])\n+        self.layers = nn.ModuleList([BartEncoderLayer(config, layer_idx=i) for i in range(config.encoder_layers)])\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n         self._use_sdpa = config._attn_implementation == \"sdpa\"\n         self.layernorm_embedding = nn.LayerNorm(embed_dim)\n@@ -976,7 +1115,7 @@ def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = No\n             config.max_position_embeddings,\n             config.d_model,\n         )\n-        self.layers = nn.ModuleList([BartDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList([BartDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n         self._use_sdpa = config._attn_implementation == \"sdpa\"\n \n@@ -1006,6 +1145,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -1071,6 +1211,9 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1079,43 +1222,59 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input = input_ids\n-            input_shape = input.shape\n-            input_ids = input_ids.view(-1, input_shape[-1])\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-            input = inputs_embeds[:, :, -1]\n-        else:\n-            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if input_ids is not None:\n+            input_ids = input_ids.view(-1, input_ids.shape[-1])\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input)\n-\n-        if self._use_flash_attention_2:\n-            # 2d mask is passed through the layers\n-            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-        elif self._use_sdpa and not output_attentions and cross_attn_head_mask is None:\n-            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-            # the manual implementation that requires a 4D causal mask in all cases.\n-            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask,\n-                input_shape,\n-                inputs_embeds,\n-                past_key_values_length,\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-        else:\n-            # 4d mask is passed through the layers\n-            attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        batch_size, seq_length = inputs_embeds.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n             )\n \n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n+\n+        self_attn_cache = (\n+            past_key_values.self_attention_cache\n+            if isinstance(past_key_values, EncoderDecoderCache)\n+            else past_key_values\n+        )\n+        causal_mask = self._update_causal_mask(\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            self_attn_cache,\n+            output_attentions,\n+        )\n+\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n             if self._use_flash_attention_2:\n@@ -1127,35 +1286,28 @@ def forward(\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n                     inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n+                    tgt_len=seq_length,\n                 )\n             else:\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n                 )\n \n         # embed positions\n-        positions = self.embed_positions(input, past_key_values_length)\n-        positions = positions.to(inputs_embeds.device)\n+        position_ids = self.embed_positions(input, past_key_values_length, position_ids=cache_position)\n+        position_ids = position_ids.to(inputs_embeds.device)\n \n-        hidden_states = inputs_embeds + positions\n+        hidden_states = inputs_embeds + position_ids\n         hidden_states = self.layernorm_embedding(hidden_states)\n \n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1175,39 +1327,39 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    attention_mask,\n+                    causal_mask,\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n                     head_mask[idx] if head_mask is not None else None,\n                     cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=attention_mask,\n+                    attention_mask=causal_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     cross_attn_layer_head_mask=(\n                         cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                     ),\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    cache_position=cache_position,\n                 )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -1220,6 +1372,9 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n@@ -1296,6 +1451,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1379,6 +1535,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1467,6 +1624,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1569,6 +1727,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         lm_logits = self.lm_head(outputs[0])\n@@ -1651,6 +1810,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1708,6 +1868,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         hidden_states = outputs[0]  # last hidden state\n \n@@ -1795,6 +1956,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1844,6 +2006,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = outputs[0]\n@@ -1959,6 +2122,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n@@ -2008,6 +2172,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         logits = self.lm_head(outputs[0])"
        },
        {
            "sha": "2106c07e7dfde6998d3070600473c729ca1d5598",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -1340,7 +1340,6 @@ def set_attention_type(self, value: str):\n         attn_weights.value = self.self.value\n         attn_weights.key = self.self.key\n         self.self = attn_weights\n-        self.attention_type = value\n         if not self.training:\n             self.self.eval()\n "
        },
        {
            "sha": "a1d972ec1acb429f8a289fce8d044484c1667567",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 293,
            "deletions": 106,
            "changes": 399,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -24,8 +24,12 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n+    _prepare_4d_attention_mask,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -36,10 +40,21 @@\n     Seq2SeqSequenceClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from .configuration_bigbird_pegasus import BigBirdPegasusConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _EXPECTED_OUTPUT_SHAPE = [1, 7, 1024]\n@@ -69,13 +84,15 @@ class BigBirdPegasusLearnedPositionalEmbedding(nn.Embedding):\n     def __init__(self, num_embeddings: int, embedding_dim: int):\n         super().__init__(num_embeddings, embedding_dim)\n \n-    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):\n-        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n-        bsz, seq_len = input_ids_shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        )\n-        return super().forward(positions)\n+    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: torch.Tensor = None):\n+        \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n+\n+        if position_ids is None:\n+            bsz, seq_len = input_ids_shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            )\n+        return super().forward(position_ids)\n \n \n # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->BigBirdPegasus\n@@ -1114,7 +1131,6 @@ def set_attention_type(self, value: str):\n         if value == self.attention_type:\n             return\n \n-        self.attention_type = value\n         if value == \"original_full\":\n             # copy all weights to new full attention class\n             attn_weights = BigBirdPegasusSelfAttention(self.config)\n@@ -1136,7 +1152,6 @@ def forward(\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n-        past_key_value=None,\n         output_attentions=False,\n         band_mask=None,\n         from_mask=None,\n@@ -1147,12 +1162,11 @@ def forward(\n         # Expand dims to enable multiplication in the self-attention module\n         head_mask = head_mask.reshape(1, -1, 1, 1) if head_mask is not None else None\n \n-        if self.config.attention_type == \"original_full\":\n+        if self.attention_type == \"original_full\":\n             self_outputs = self.self(\n                 hidden_states,\n                 attention_mask,\n                 head_mask,\n-                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n         else:\n@@ -1178,6 +1192,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[BigBirdPegasusConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -1194,73 +1209,74 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -1274,10 +1290,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -1412,7 +1425,7 @@ def set_attention_type(self, value: str):\n \n \n class BigBirdPegasusDecoderLayer(nn.Module):\n-    def __init__(self, config: BigBirdPegasusConfig):\n+    def __init__(self, config: BigBirdPegasusConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n         self.self_attn = BigBirdPegasusDecoderAttention(\n@@ -1421,6 +1434,7 @@ def __init__(self, config: BigBirdPegasusConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             bias=config.use_bias,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -1433,6 +1447,7 @@ def __init__(self, config: BigBirdPegasusConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             bias=config.use_bias,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -1448,9 +1463,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -1469,47 +1485,43 @@ def forward(\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n             # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.final_layer_norm(hidden_states)\n@@ -1525,7 +1537,7 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n@@ -1563,6 +1575,8 @@ class BigBirdPegasusPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"BigBirdPegasusEncoderLayer\", \"BigBirdPegasusDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_param_buffer_assignment = False\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -1574,6 +1588,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n     @property\n     def dummy_inputs(self):\n@@ -1585,6 +1602,131 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n class BigBirdPegasusEncoder(BigBirdPegasusPreTrainedModel):\n     \"\"\"\n@@ -1914,7 +2056,9 @@ def __init__(self, config: BigBirdPegasusConfig, embed_tokens: Optional[nn.Embed\n             config.max_position_embeddings,\n             config.d_model,\n         )\n-        self.layers = nn.ModuleList([BigBirdPegasusDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList(\n+            [BigBirdPegasusDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)]\n+        )\n         self.layernorm_embedding = nn.LayerNorm(config.d_model)\n \n         self.gradient_checkpointing = False\n@@ -1941,6 +2085,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ):\n         r\"\"\"\n         Args:\n@@ -2006,6 +2151,9 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -2014,54 +2162,80 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_shape = input_ids.size()\n-            input_ids = input_ids.view(-1, input_shape[-1])\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if input_ids is not None:\n+            input_ids = input_ids.view(-1, input_ids.shape[-1])\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        attention_mask = _prepare_4d_causal_attention_mask(\n-            attention_mask, input_shape, inputs_embeds, past_key_values_length\n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        batch_size, seq_length = inputs_embeds.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n+            )\n+\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n+\n+        self_attn_cache = (\n+            past_key_values.self_attention_cache\n+            if isinstance(past_key_values, EncoderDecoderCache)\n+            else past_key_values\n+        )\n+        causal_mask = self._update_causal_mask(\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            self_attn_cache,\n+            output_attentions,\n         )\n \n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n             # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n             encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n             )\n \n         # embed positions\n-        positions = self.embed_positions(input_shape, past_key_values_length)\n-        positions = positions.to(inputs_embeds.device)\n-\n-        hidden_states = inputs_embeds + positions\n-\n+        position_ids = cache_position.unsqueeze(0)\n+        position_ids = self.embed_positions(\n+            (batch_size, seq_length), past_key_values_length, position_ids=position_ids\n+        )\n+        position_ids = position_ids.to(inputs_embeds.device)\n+        hidden_states = inputs_embeds + position_ids\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -2080,39 +2254,39 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    attention_mask,\n+                    causal_mask,\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n                     head_mask[idx] if head_mask is not None else None,\n                     cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=attention_mask,\n+                    attention_mask=causal_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     cross_attn_layer_head_mask=(\n                         cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                     ),\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    cache_position=cache_position,\n                 )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -2127,6 +2301,9 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n@@ -2198,6 +2375,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -2269,6 +2447,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -2359,6 +2538,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -2432,6 +2612,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         lm_logits = self.lm_head(outputs[0])\n@@ -2514,6 +2695,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -2559,6 +2741,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         hidden_states = outputs[0]  # last hidden state\n \n@@ -2646,6 +2829,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -2683,6 +2867,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = outputs[0]\n@@ -2794,6 +2979,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n@@ -2842,6 +3028,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         logits = self.lm_head(outputs[0])"
        },
        {
            "sha": "900a8fe178500892d7186dcf7fae9b183663a3c1",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 320,
            "deletions": 159,
            "changes": 479,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -23,8 +23,11 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_causal_attention_mask, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n+)\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -34,11 +37,19 @@\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n     logging,\n )\n from .configuration_biogpt import BioGptConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -55,17 +66,23 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         self.offset = 2\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n \n-    def forward(self, attention_mask: torch.LongTensor, past_key_values_length: int = 0):\n+    def forward(\n+        self,\n+        attention_mask: torch.LongTensor,\n+        past_key_values_length: int = 0,\n+        position_ids: Optional[torch.Tensor] = None,\n+    ):\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n-        attention_mask = attention_mask.long()\n+        if position_ids is None:\n+            attention_mask = attention_mask.long()\n \n-        # create positions depending on attention_mask\n-        positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1\n+            # create positions depending on attention_mask\n+            positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1\n \n-        # cut positions if `past_key_values_length` is > 0\n-        positions = positions[:, past_key_values_length:]\n+            # cut positions if `past_key_values_length` is > 0\n+            position_ids = positions[:, past_key_values_length:]\n \n-        return super().forward(positions + self.offset)\n+        return super().forward(position_ids + self.offset)\n \n \n # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->BioGpt\n@@ -95,6 +112,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[BioGptConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -111,73 +129,74 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -191,10 +210,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -247,10 +263,11 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         if output_attentions:\n@@ -265,6 +282,7 @@ def forward(\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -274,74 +292,72 @@ def forward(\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        query_states = self._shape(query_states, tgt_len, bsz)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n+\n+        causal_mask = None\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n+        is_causal = True if self.is_causal and causal_mask is None and tgt_len > 1 else False\n \n         # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n         # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n+            attn_mask=causal_mask,\n             dropout_p=self.dropout if self.training else 0.0,\n             is_causal=is_causal,\n         )\n \n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n \n         # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n         # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n+        attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n         attn_output = self.out_proj(attn_output)\n \n         return attn_output, None, past_key_value\n@@ -354,7 +370,7 @@ def forward(\n \n \n class BioGptDecoderLayer(nn.Module):\n-    def __init__(self, config: BioGptConfig):\n+    def __init__(self, config: BioGptConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n \n@@ -364,6 +380,7 @@ def __init__(self, config: BioGptConfig):\n             dropout=config.attention_probs_dropout_prob,\n             is_decoder=True,\n             is_causal=True,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.hidden_dropout_prob\n         self.activation_fn = ACT2FN[config.hidden_act]\n@@ -380,9 +397,10 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -398,21 +416,22 @@ def forward(\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n@@ -433,7 +452,7 @@ def forward(\n             outputs += (self_attn_weights,)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n@@ -444,6 +463,8 @@ class BioGptPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"biogpt\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -461,6 +482,131 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n @auto_docstring\n class BioGptModel(BioGptPreTrainedModel):\n@@ -478,7 +624,7 @@ def __init__(self, config: BioGptConfig):\n         )\n         self.embed_positions = BioGptLearnedPositionalEmbedding(config.max_position_embeddings, self.embed_dim)\n \n-        self.layers = nn.ModuleList([BioGptDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layers = nn.ModuleList([BioGptDecoderLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.layer_norm = nn.LayerNorm(self.embed_dim)\n \n         self.gradient_checkpointing = False\n@@ -501,9 +647,11 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n         use_cache: Optional[bool] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -514,64 +662,71 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input = input_ids\n-            input_shape = input.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-            input = inputs_embeds[:, :, -1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if input_ids is not None:\n+            input_ids = input_ids.view(-1, input_ids.shape[-1])\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input)\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(\n-                (inputs_embeds.shape[0], inputs_embeds.shape[1] + past_key_values_length),\n-                dtype=torch.bool,\n-                device=inputs_embeds.device,\n+        # initialize past_key_values\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-        elif attention_mask.shape[1] != past_key_values_length + input_shape[1]:\n-            raise ValueError(\n-                f\"The provided attention mask has length {attention_mask.shape[1]}, but its length should be \"\n-                f\"{past_key_values_length + input_shape[1]} (sum of the lengths of current and past inputs)\"\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        batch_size, seq_length = inputs_embeds.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n             )\n \n-        # embed positions\n-        positions = self.embed_positions(attention_mask, past_key_values_length)\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n \n-        if self._use_sdpa and not output_attentions and head_mask is None:\n-            # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n-            # the manual implementation that requires a 4D causal mask in all cases.\n-            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n-            )\n-        else:\n-            attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n-            )\n+        self_attn_cache = (\n+            past_key_values.self_attention_cache\n+            if isinstance(past_key_values, EncoderDecoderCache)\n+            else past_key_values\n+        )\n+        causal_mask = self._update_causal_mask(\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            self_attn_cache,\n+            output_attentions,\n+        )\n \n-        hidden_states = inputs_embeds + positions\n+        # embed positions\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n \n-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        position_ids = self.embed_positions(attention_mask, past_key_values_length, position_ids=position_ids)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n+        hidden_states = inputs_embeds + position_ids\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n@@ -582,32 +737,32 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    attention_mask,\n+                    causal_mask,\n                     head_mask[idx] if head_mask is not None else None,\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=attention_mask,\n+                    attention_mask=causal_mask,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    cache_position=cache_position,\n                 )\n \n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -619,6 +774,8 @@ def forward(\n         hidden_states = self.layer_norm(hidden_states)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n@@ -668,9 +825,11 @@ def forward(\n         past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -688,9 +847,11 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n+            position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = outputs[0]"
        },
        {
            "sha": "ede764a513c290fde024ce0c1e066a7b0e0d2d28",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 287,
            "deletions": 99,
            "changes": 386,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -26,8 +26,12 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n+    _prepare_4d_attention_mask,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -36,11 +40,22 @@\n     Seq2SeqModelOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from ..blenderbot_small import BlenderbotSmallForConditionalGeneration, BlenderbotSmallModel\n from .configuration_blenderbot import BlenderbotConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -69,13 +84,16 @@ class BlenderbotLearnedPositionalEmbedding(nn.Embedding):\n     def __init__(self, num_embeddings: int, embedding_dim: int):\n         super().__init__(num_embeddings, embedding_dim)\n \n-    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):\n+    def forward(\n+        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ):\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n-        bsz, seq_len = input_ids_shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        )\n-        return super().forward(positions)\n+        if position_ids is None:\n+            bsz, seq_len = input_ids_shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            )\n+        return super().forward(position_ids)\n \n \n # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->Blenderbot\n@@ -105,6 +123,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[BlenderbotConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -121,73 +140,74 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -201,10 +221,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -325,7 +342,7 @@ def forward(\n \n # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Blenderbot, MBART->BLENDERBOT\n class BlenderbotDecoderLayer(nn.Module):\n-    def __init__(self, config: BlenderbotConfig):\n+    def __init__(self, config: BlenderbotConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -336,6 +353,7 @@ def __init__(self, config: BlenderbotConfig):\n             is_decoder=True,\n             is_causal=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -348,6 +366,7 @@ def __init__(self, config: BlenderbotConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -362,9 +381,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -383,47 +403,43 @@ def forward(\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n             # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.final_layer_norm(hidden_states)\n@@ -439,7 +455,7 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n@@ -449,6 +465,8 @@ class BlenderbotPreTrainedModel(PreTrainedModel):\n     config_class = BlenderbotConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -460,6 +478,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n     @property\n     def dummy_inputs(self):\n@@ -472,6 +493,131 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n class BlenderbotEncoder(BlenderbotPreTrainedModel):\n     \"\"\"\n@@ -674,7 +820,9 @@ def __init__(self, config: BlenderbotConfig, embed_tokens: Optional[nn.Embedding\n             config.max_position_embeddings,\n             config.d_model,\n         )\n-        self.layers = nn.ModuleList([BlenderbotDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList(\n+            [BlenderbotDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)]\n+        )\n         self.layer_norm = nn.LayerNorm(config.d_model)\n \n         self.gradient_checkpointing = False\n@@ -701,6 +849,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ):\n         r\"\"\"\n         Args:\n@@ -767,6 +916,9 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -775,52 +927,79 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        ## retrieve input_ids and inputs_embeds\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_shape = input_ids.size()\n-            input_ids = input_ids.view(-1, input_shape[-1])\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if input_ids is not None:\n+            input_ids = input_ids.view(-1, input_ids.shape[-1])\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        attention_mask = _prepare_4d_causal_attention_mask(\n-            attention_mask, input_shape, inputs_embeds, past_key_values_length\n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        batch_size, seq_length = inputs_embeds.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n+            )\n+\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n+\n+        self_attn_cache = (\n+            past_key_values.self_attention_cache\n+            if isinstance(past_key_values, EncoderDecoderCache)\n+            else past_key_values\n+        )\n+        causal_mask = self._update_causal_mask(\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            self_attn_cache,\n+            output_attentions,\n         )\n \n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n             # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n             encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n             )\n \n         # embed positions\n-        positions = self.embed_positions(input_shape, past_key_values_length)\n-\n-        hidden_states = inputs_embeds + positions\n+        position_ids = self.embed_positions(\n+            (batch_size, seq_length), past_key_values_length, position_ids=cache_position\n+        )\n \n+        hidden_states = inputs_embeds + position_ids\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -839,39 +1018,39 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    attention_mask,\n+                    causal_mask,\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n                     head_mask[idx] if head_mask is not None else None,\n                     cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=attention_mask,\n+                    attention_mask=causal_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     cross_attn_layer_head_mask=(\n                         cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                     ),\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    cache_position=cache_position,\n                 )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -887,6 +1066,9 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n@@ -963,6 +1145,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1041,6 +1224,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1139,6 +1323,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1225,6 +1410,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n \n@@ -1326,6 +1512,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n@@ -1375,6 +1562,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         logits = self.lm_head(outputs[0])"
        },
        {
            "sha": "a6fa6cda00e5d6da91d359d027f5bfa1bfd91b28",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 291,
            "deletions": 102,
            "changes": 393,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -24,8 +24,12 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n+    _prepare_4d_attention_mask,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -34,10 +38,21 @@\n     Seq2SeqModelOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from .configuration_blenderbot_small import BlenderbotSmallConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -67,13 +82,16 @@ class BlenderbotSmallLearnedPositionalEmbedding(nn.Embedding):\n     def __init__(self, num_embeddings: int, embedding_dim: int):\n         super().__init__(num_embeddings, embedding_dim)\n \n-    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):\n+    def forward(\n+        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ):\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n-        bsz, seq_len = input_ids_shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        )\n-        return super().forward(positions)\n+        if position_ids is None:\n+            bsz, seq_len = input_ids_shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            )\n+        return super().forward(position_ids)\n \n \n # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->BlenderbotSmall\n@@ -89,6 +107,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[BlenderbotSmallConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -105,73 +124,74 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -185,10 +205,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -237,7 +254,7 @@ def forward(\n \n # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->BlenderbotSmall, BART->BLENDERBOT_SMALL\n class BlenderbotSmallEncoderLayer(nn.Module):\n-    def __init__(self, config: BlenderbotSmallConfig):\n+    def __init__(self, config: BlenderbotSmallConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -246,6 +263,7 @@ def __init__(self, config: BlenderbotSmallConfig):\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.dropout = config.dropout\n@@ -314,7 +332,7 @@ def forward(\n \n # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->BlenderbotSmall, BART->BLENDERBOT_SMALL\n class BlenderbotSmallDecoderLayer(nn.Module):\n-    def __init__(self, config: BlenderbotSmallConfig):\n+    def __init__(self, config: BlenderbotSmallConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -325,6 +343,7 @@ def __init__(self, config: BlenderbotSmallConfig):\n             is_decoder=True,\n             is_causal=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -337,6 +356,7 @@ def __init__(self, config: BlenderbotSmallConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -351,9 +371,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -372,47 +393,42 @@ def forward(\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         residual = hidden_states\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.activation_fn(self.fc1(hidden_states))\n@@ -428,7 +444,7 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n@@ -438,6 +454,8 @@ class BlenderbotSmallPreTrainedModel(PreTrainedModel):\n     config_class = BlenderbotSmallConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -449,6 +467,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n     @property\n     def dummy_inputs(self):\n@@ -461,6 +482,131 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n class BlenderbotSmallEncoder(BlenderbotSmallPreTrainedModel):\n     \"\"\"\n@@ -657,7 +803,9 @@ def __init__(self, config: BlenderbotSmallConfig, embed_tokens: Optional[nn.Embe\n             config.max_position_embeddings,\n             config.d_model,\n         )\n-        self.layers = nn.ModuleList([BlenderbotSmallDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList(\n+            [BlenderbotSmallDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)]\n+        )\n         self.layernorm_embedding = nn.LayerNorm(config.d_model)\n \n         self.gradient_checkpointing = False\n@@ -684,6 +832,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        cache_position=None,\n     ):\n         r\"\"\"\n         Args:\n@@ -749,6 +898,9 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -757,55 +909,83 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_shape = input_ids.size()\n-            input_ids = input_ids.view(-1, input_shape[-1])\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if input_ids is not None:\n+            input_ids = input_ids.view(-1, input_ids.shape[-1])\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        inputs_embeds = inputs_embeds * self.embed_scale\n+\n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        batch_size, seq_length = inputs_embeds.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n+            )\n+\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n \n-        attention_mask = _prepare_4d_causal_attention_mask(\n-            attention_mask, input_shape, inputs_embeds, past_key_values_length\n+        self_attn_cache = (\n+            past_key_values.self_attention_cache\n+            if isinstance(past_key_values, EncoderDecoderCache)\n+            else past_key_values\n+        )\n+        causal_mask = self._update_causal_mask(\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            self_attn_cache,\n+            output_attentions,\n         )\n \n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n             # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n             encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n             )\n \n         # embed positions\n-        positions = self.embed_positions(input_shape, past_key_values_length)\n+        position_ids = self.embed_positions(\n+            (batch_size, seq_length), past_key_values_length, position_ids=cache_position\n+        )\n \n         # BlenderbotSmall applies layer norm on hidden_states\n         inputs_embeds = self.layernorm_embedding(inputs_embeds)\n-        hidden_states = inputs_embeds + positions\n-\n+        hidden_states = inputs_embeds + position_ids\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -824,39 +1004,39 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    attention_mask,\n+                    causal_mask,\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n                     head_mask[idx] if head_mask is not None else None,\n                     cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=attention_mask,\n+                    attention_mask=causal_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     cross_attn_layer_head_mask=(\n                         cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                     ),\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    cache_position=cache_position,\n                 )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -869,6 +1049,9 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n@@ -932,6 +1115,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1010,6 +1194,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1093,6 +1278,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1179,6 +1365,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n \n@@ -1280,6 +1467,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n@@ -1329,6 +1517,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         logits = self.lm_head(outputs[0])"
        },
        {
            "sha": "8b728a19dff793a8f7d43ae8ac06ebed057922ce",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 16,
            "deletions": 9,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -21,7 +21,10 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_causal_attention_mask,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -251,13 +254,16 @@ def _init_weight(self):\n         self.weight = nn.Parameter(out, requires_grad=False)\n \n     @torch.no_grad()\n-    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:\n+    def forward(\n+        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ) -> torch.Tensor:\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n-        bsz, seq_len = input_ids_shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        )\n-        return super().forward(positions)\n+        if position_ids is None:\n+            bsz, seq_len = input_ids_shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            )\n+        return super().forward(position_ids)\n \n \n # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesValueEmbedding with TimeSeries->Info\n@@ -270,7 +276,7 @@ def forward(self, x):\n         return self.value_projection(x)\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Informer\n+# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->Informer\n class InformerAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -1045,7 +1051,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesTransformerDecoder with TimeSeriesTransformer->Informer,TimeSeriesTransformerConfig->InformerConfig,time-series-transformer->informer,Transformer->Informer,TimeSeries->Informer\n class InformerDecoder(InformerPreTrainedModel):\n     \"\"\"\n     Informer decoder consisting of *config.decoder_layers* layers. Each layer is a\n@@ -1403,6 +1408,7 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.decoder\n \n+    # Ignore copy\n     @auto_docstring\n     def forward(\n         self,\n@@ -1654,6 +1660,7 @@ def output_distribution(self, params, loc=None, scale=None, trailing_n=None) ->\n             sliced_params = [p[:, -trailing_n:] for p in params]\n         return self.distribution_output.distribution(sliced_params, loc=loc, scale=scale)\n \n+    # Ignore copy\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "1da696ae0386fffd2ae4875b00ccc353dd87afe2",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 364,
            "deletions": 197,
            "changes": 561,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -22,14 +22,14 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n-    _prepare_4d_causal_attention_mask,\n-    _prepare_4d_causal_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n@@ -39,10 +39,21 @@\n     Seq2SeqModelOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from .configuration_m2m_100 import M2M100Config\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n@@ -186,6 +197,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[M2M100Config] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -202,73 +214,74 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -282,10 +295,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -348,73 +358,73 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n-    def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        # M2M100FlashAttention2 attention does not support output_attentions\n+        if output_attentions:\n+            raise ValueError(\n+                \"M2M100SdpaAttention2 attention does not support `output_attentions`. \"\n+                \"Use the argument `attn_implementation='eager'` when loading the model.\"\n+            )\n+\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, q_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self._reshape(self.q_proj(hidden_states), -1, bsz)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim)\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0].transpose(1, 2)\n-            value_states = past_key_value[1].transpose(1, 2)\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._reshape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0].transpose(1, 2), key_states], dim=1)\n-            value_states = torch.cat([past_key_value[1].transpose(1, 2), value_states], dim=1)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states.transpose(1, 2), value_states.transpose(1, 2))\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value[0].shape[-2]\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n+\n+        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n+        # to be able to avoid many of these transpose/reshape/view.\n+        key_states = key_states.transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n \n         # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n         # therefore the input hidden states gets silently casted in float32. Hence, we need\n         # cast them back in the correct dtype just to be sure everything works as expected.\n         # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n         # in fp32. (LlamaRMSNorm handles it correctly)\n-\n         input_dtype = query_states.dtype\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n@@ -449,10 +459,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, q_len, -1)\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, None, past_key_value\n \n \n # Copied from transformers.models.bart.modeling_bart.BartSdpaAttention with Bart->M2M100\n@@ -461,10 +468,11 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         if output_attentions:\n@@ -479,6 +487,7 @@ def forward(\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -488,74 +497,72 @@ def forward(\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        query_states = self._shape(query_states, tgt_len, bsz)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n+\n+        causal_mask = None\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n+        is_causal = True if self.is_causal and causal_mask is None and tgt_len > 1 else False\n \n         # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n         # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n+            attn_mask=causal_mask,\n             dropout_p=self.dropout if self.training else 0.0,\n             is_causal=is_causal,\n         )\n \n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n \n         # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n         # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n+        attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n         attn_output = self.out_proj(attn_output)\n \n         return attn_output, None, past_key_value\n@@ -639,7 +646,7 @@ def forward(\n \n # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->M2M100, MBART->M2M100\n class M2M100DecoderLayer(nn.Module):\n-    def __init__(self, config: M2M100Config):\n+    def __init__(self, config: M2M100Config, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -650,6 +657,7 @@ def __init__(self, config: M2M100Config):\n             is_decoder=True,\n             is_causal=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -662,6 +670,7 @@ def __init__(self, config: M2M100Config):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -676,9 +685,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -697,47 +707,43 @@ def forward(\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n             # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.final_layer_norm(hidden_states)\n@@ -753,7 +759,7 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n@@ -766,6 +772,9 @@ class M2M100PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"M2M100EncoderLayer\", \"M2M100DecoderLayer\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_cache_class = True\n+    # Doesn't support `compile` (dynamic control flow). Can be fixed but low usage model\n+    _supports_static_cache = False\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -777,6 +786,134 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n \n \n class M2M100Encoder(M2M100PreTrainedModel):\n@@ -995,7 +1132,7 @@ def __init__(self, config: M2M100Config, embed_tokens: Optional[nn.Embedding] =\n             config.d_model,\n             self.padding_idx,\n         )\n-        self.layers = nn.ModuleList([M2M100DecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList([M2M100DecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n         self._use_sdpa = config._attn_implementation == \"sdpa\"\n         self.layer_norm = nn.LayerNorm(config.d_model)\n@@ -1018,6 +1155,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ):\n         r\"\"\"\n         Args:\n@@ -1083,6 +1221,9 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1091,41 +1232,59 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_shape = input_ids.size()\n-            input_ids = input_ids.view(-1, input_shape[-1])\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if input_ids is not None:\n+            input_ids = input_ids.view(-1, input_ids.shape[-1])\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        if self._use_flash_attention_2:\n-            # 2d mask is passed through the layers\n-            combined_attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-        elif self._use_sdpa and not output_attentions and cross_attn_head_mask is None:\n-            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-            # the manual implementation that requires a 4D causal mask in all cases.\n-            combined_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask,\n-                input_shape,\n-                inputs_embeds,\n-                past_key_values_length,\n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-        else:\n-            # 4d mask is passed through the layers\n-            combined_attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        batch_size, seq_length = inputs_embeds.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n             )\n \n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n+\n+        self_attn_cache = (\n+            past_key_values.self_attention_cache\n+            if isinstance(past_key_values, EncoderDecoderCache)\n+            else past_key_values\n+        )\n+        causal_mask = self._update_causal_mask(\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            self_attn_cache,\n+            output_attentions,\n+        )\n+\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n             if self._use_flash_attention_2:\n@@ -1137,20 +1296,21 @@ def forward(\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n                     inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n+                    tgt_len=seq_length,\n                 )\n             else:\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=seq_length,\n                 )\n \n         # embed positions\n         positions = self.embed_positions(input_ids, inputs_embeds, past_key_values_length)\n         positions = positions.to(inputs_embeds.device)\n \n         hidden_states = inputs_embeds + positions\n-\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n         if self.gradient_checkpointing and self.training:\n@@ -1164,7 +1324,7 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if output_attentions else None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1187,34 +1347,34 @@ def forward(\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n \n-                past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         decoder_layer.__call__,\n                         hidden_states,\n-                        combined_attention_mask,\n+                        causal_mask,\n                         encoder_hidden_states,\n                         encoder_attention_mask,\n                         head_mask[idx] if head_mask is not None else None,\n                         cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n                         None,\n                         output_attentions,\n                         use_cache,\n+                        cache_position,\n                     )\n                 else:\n                     layer_outputs = decoder_layer(\n                         hidden_states,\n-                        attention_mask=combined_attention_mask,\n+                        attention_mask=causal_mask,\n                         encoder_hidden_states=encoder_hidden_states,\n                         encoder_attention_mask=encoder_attention_mask,\n                         layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                         cross_attn_layer_head_mask=(\n                             cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                         ),\n-                        past_key_value=past_key_value,\n+                        past_key_value=past_key_values,\n                         output_attentions=output_attentions,\n                         use_cache=use_cache,\n+                        cache_position=cache_position,\n                     )\n \n                 hidden_states = layer_outputs[0]\n@@ -1223,7 +1383,7 @@ def forward(\n                 continue\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -1236,6 +1396,9 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n@@ -1305,6 +1468,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple[torch.Tensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1367,6 +1531,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1432,6 +1597,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1499,6 +1665,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         lm_logits = self.lm_head(outputs[0])\n "
        },
        {
            "sha": "ee52dd1be069ec58aed7e9de9f154b3730c6ea64",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 292,
            "deletions": 103,
            "changes": 395,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -25,8 +25,12 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n+    _prepare_4d_attention_mask,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -35,10 +39,21 @@\n     Seq2SeqModelOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from .configuration_marian import MarianConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -81,13 +96,16 @@ def _init_weight(self):\n         self.weight = nn.Parameter(out, requires_grad=False)\n \n     @torch.no_grad()\n-    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:\n+    def forward(\n+        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ) -> torch.Tensor:\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n-        bsz, seq_len = input_ids_shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        )\n-        return super().forward(positions)\n+        if position_ids is None:\n+            bsz, seq_len = input_ids_shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            )\n+        return super().forward(position_ids)\n \n \n # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Marian\n@@ -103,6 +121,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[MarianConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -119,73 +138,74 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -199,10 +219,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -251,7 +268,7 @@ def forward(\n \n # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->Marian, BART->MARIAN\n class MarianEncoderLayer(nn.Module):\n-    def __init__(self, config: MarianConfig):\n+    def __init__(self, config: MarianConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -260,6 +277,7 @@ def __init__(self, config: MarianConfig):\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.dropout = config.dropout\n@@ -325,7 +343,7 @@ def forward(\n \n # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->Marian, BART->MARIAN\n class MarianDecoderLayer(nn.Module):\n-    def __init__(self, config: MarianConfig):\n+    def __init__(self, config: MarianConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -336,6 +354,7 @@ def __init__(self, config: MarianConfig):\n             is_decoder=True,\n             is_causal=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -348,6 +367,7 @@ def __init__(self, config: MarianConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -362,9 +382,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -383,47 +404,42 @@ def forward(\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         residual = hidden_states\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.activation_fn(self.fc1(hidden_states))\n@@ -439,7 +455,7 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n@@ -449,6 +465,8 @@ class MarianPreTrainedModel(PreTrainedModel):\n     config_class = MarianConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Embedding, MarianSinusoidalPositionalEmbedding]):\n         std = self.config.init_std\n@@ -462,6 +480,134 @@ def _init_weights(self, module: Union[nn.Linear, nn.Embedding, MarianSinusoidalP\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n \n     @property\n     def dummy_inputs(self):\n@@ -670,7 +816,7 @@ def __init__(self, config: MarianConfig, embed_tokens: Optional[nn.Embedding] =\n         self.embed_positions = MarianSinusoidalPositionalEmbedding(\n             config.max_position_embeddings, config.d_model, self.padding_idx\n         )\n-        self.layers = nn.ModuleList([MarianDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList([MarianDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n \n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n@@ -696,6 +842,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -761,6 +908,9 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -769,53 +919,83 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_shape = input_ids.size()\n-            input_ids = input_ids.view(-1, input_shape[-1])\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if input_ids is not None:\n+            input_ids = input_ids.view(-1, input_ids.shape[-1])\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        # Important to apply outside of the above `if`, in case user passes `embeds`\n+        inputs_embeds = inputs_embeds * self.embed_scale\n+\n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        batch_size, seq_length = inputs_embeds.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n+            )\n \n-        attention_mask = _prepare_4d_causal_attention_mask(\n-            attention_mask, input_shape, inputs_embeds, past_key_values_length\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n+\n+        self_attn_cache = (\n+            past_key_values.self_attention_cache\n+            if isinstance(past_key_values, EncoderDecoderCache)\n+            else past_key_values\n+        )\n+        causal_mask = self._update_causal_mask(\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            self_attn_cache,\n+            output_attentions,\n         )\n \n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n             # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n             encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                encoder_attention_mask,\n+                inputs_embeds.dtype,\n+                tgt_len=seq_length,\n             )\n \n         # embed positions\n-        positions = self.embed_positions(input_shape, past_key_values_length)\n-\n-        hidden_states = inputs_embeds + positions\n-\n+        position_ids = self.embed_positions(\n+            (batch_size, seq_length), past_key_values_length, position_ids=cache_position\n+        )\n+        hidden_states = inputs_embeds + position_ids\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -833,39 +1013,39 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    attention_mask,\n+                    causal_mask,\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n                     head_mask[idx] if head_mask is not None else None,\n                     cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=attention_mask,\n+                    attention_mask=causal_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     cross_attn_layer_head_mask=(\n                         cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                     ),\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    cache_position=cache_position,\n                 )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -878,6 +1058,9 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n@@ -996,6 +1179,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Seq2SeqModelOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1078,6 +1262,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1252,6 +1437,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Seq2SeqLMOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1326,6 +1512,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n \n@@ -1430,6 +1617,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n@@ -1479,6 +1667,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         logits = self.lm_head(outputs[0])"
        },
        {
            "sha": "1cc334db7e0f44c3f2ce6546b9601a3d992a7252",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 378,
            "deletions": 213,
            "changes": 591,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -24,12 +24,12 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n-    _prepare_4d_causal_attention_mask,\n-    _prepare_4d_causal_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n@@ -42,10 +42,21 @@\n     Seq2SeqSequenceClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from .configuration_mbart import MBartConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n@@ -85,15 +96,18 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         self.offset = 2\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n \n-    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n+    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: torch.Tensor = None):\n         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n \n-        bsz, seq_len = input_ids.shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        ).expand(bsz, -1)\n+        if position_ids is None:\n+            bsz, seq_len = input_ids.shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            ).expand(bsz, -1)\n+        else:\n+            position_ids = position_ids.unsqueeze(0)\n \n-        return super().forward(positions + self.offset)\n+        return super().forward(position_ids + self.offset)\n \n \n # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->MBart\n@@ -123,6 +137,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[MBartConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -139,73 +154,74 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -219,10 +235,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -285,73 +298,73 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n-    def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        # MBartFlashAttention2 attention does not support output_attentions\n+        if output_attentions:\n+            raise ValueError(\n+                \"MBartSdpaAttention2 attention does not support `output_attentions`. \"\n+                \"Use the argument `attn_implementation='eager'` when loading the model.\"\n+            )\n+\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, q_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self._reshape(self.q_proj(hidden_states), -1, bsz)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim)\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0].transpose(1, 2)\n-            value_states = past_key_value[1].transpose(1, 2)\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._reshape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0].transpose(1, 2), key_states], dim=1)\n-            value_states = torch.cat([past_key_value[1].transpose(1, 2), value_states], dim=1)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states.transpose(1, 2), value_states.transpose(1, 2))\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value[0].shape[-2]\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n+\n+        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n+        # to be able to avoid many of these transpose/reshape/view.\n+        key_states = key_states.transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n \n         # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n         # therefore the input hidden states gets silently casted in float32. Hence, we need\n         # cast them back in the correct dtype just to be sure everything works as expected.\n         # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n         # in fp32. (LlamaRMSNorm handles it correctly)\n-\n         input_dtype = query_states.dtype\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n@@ -386,10 +399,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, q_len, -1)\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, None, past_key_value\n \n \n # Copied from transformers.models.bart.modeling_bart.BartSdpaAttention with Bart->MBart\n@@ -398,10 +408,11 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         if output_attentions:\n@@ -416,6 +427,7 @@ def forward(\n                 past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -425,74 +437,72 @@ def forward(\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        query_states = self._shape(query_states, tgt_len, bsz)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n+\n+        causal_mask = None\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n+        is_causal = True if self.is_causal and causal_mask is None and tgt_len > 1 else False\n \n         # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n         # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n+            attn_mask=causal_mask,\n             dropout_p=self.dropout if self.training else 0.0,\n             is_causal=is_causal,\n         )\n \n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n \n         # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n         # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n+        attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n         attn_output = self.out_proj(attn_output)\n \n         return attn_output, None, past_key_value\n@@ -574,7 +584,7 @@ def forward(\n \n \n class MBartDecoderLayer(nn.Module):\n-    def __init__(self, config: MBartConfig):\n+    def __init__(self, config: MBartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -585,6 +595,7 @@ def __init__(self, config: MBartConfig):\n             is_decoder=True,\n             is_causal=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -597,6 +608,7 @@ def __init__(self, config: MBartConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -611,9 +623,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -632,47 +645,43 @@ def forward(\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n             # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.final_layer_norm(hidden_states)\n@@ -688,7 +697,7 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n@@ -726,6 +735,8 @@ class MBartPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MBartDecoderLayer\", \"MBartEncoderLayer\", \"MBartAttention\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -737,6 +748,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n     @property\n     def dummy_inputs(self):\n@@ -748,6 +762,131 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n class MBartEncoder(MBartPreTrainedModel):\n     \"\"\"\n@@ -965,7 +1104,7 @@ def __init__(self, config: MBartConfig, embed_tokens: Optional[nn.Embedding] = N\n             config.max_position_embeddings,\n             config.d_model,\n         )\n-        self.layers = nn.ModuleList([MBartDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList([MBartDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n         self.config = config\n \n         self.layernorm_embedding = nn.LayerNorm(config.d_model)\n@@ -995,6 +1134,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -1060,6 +1200,9 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1069,42 +1212,58 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input = input_ids\n-            input_shape = input.size()\n-            input_ids = input_ids.view(-1, input_shape[-1])\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-            input = inputs_embeds[:, :, -1]\n-        else:\n-            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if input_ids is not None:\n+            input_ids = input_ids.view(-1, input_ids.shape[-1])\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            # 2d mask is passed through the layers\n-            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-        elif self.config._attn_implementation == \"sdpa\" and not output_attentions and cross_attn_head_mask is None:\n-            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-            # the manual implementation that requires a 4D causal mask in all cases.\n-            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask,\n-                input_shape,\n-                inputs_embeds,\n-                past_key_values_length,\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-        else:\n-            # 4d mask is passed through the layers\n-            attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        batch_size, seq_length = inputs_embeds.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n             )\n \n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n+\n+        self_attn_cache = (\n+            past_key_values.self_attention_cache\n+            if isinstance(past_key_values, EncoderDecoderCache)\n+            else past_key_values\n+        )\n+        causal_mask = self._update_causal_mask(\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            self_attn_cache,\n+            output_attentions,\n+        )\n+\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n             if self.config._attn_implementation == \"flash_attention_2\":\n@@ -1116,34 +1275,27 @@ def forward(\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n                     inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n+                    tgt_len=seq_length,\n                 )\n             else:\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n                 )\n \n         # embed positions\n-        positions = self.embed_positions(input, past_key_values_length)\n+        position_ids = self.embed_positions(input, past_key_values_length, position_ids=cache_position)\n \n-        hidden_states = inputs_embeds + positions.to(inputs_embeds.device)\n+        hidden_states = inputs_embeds + position_ids.to(inputs_embeds.device)\n         hidden_states = self.layernorm_embedding(hidden_states)\n \n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1162,39 +1314,39 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    attention_mask,\n+                    causal_mask,\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n                     head_mask[idx] if head_mask is not None else None,\n                     cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=attention_mask,\n+                    attention_mask=causal_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     cross_attn_layer_head_mask=(\n                         cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                     ),\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    cache_position=cache_position,\n                 )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -1209,6 +1361,9 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n@@ -1278,6 +1433,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Seq2SeqModelOutput, Tuple[torch.FloatTensor]]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1350,6 +1506,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1433,6 +1590,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Seq2SeqLMOutput, Tuple[torch.FloatTensor]]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1529,6 +1687,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n \n@@ -1609,6 +1768,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1666,6 +1826,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         hidden_states = outputs[0]  # last hidden state\n \n@@ -1754,6 +1915,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1803,6 +1965,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = outputs[0]\n@@ -1915,6 +2078,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n@@ -1964,6 +2128,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         logits = self.lm_head(outputs[0])"
        },
        {
            "sha": "00a0afd62ad5086321c731cf533de150dc19bda8",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -146,7 +146,7 @@ def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n         return self.weights.index_select(0, position_ids.view(-1)).detach()\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Musicgen\n+# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->Musicgen\n class MusicgenAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -305,7 +305,7 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->Musicgen\n+# Copied from transformers.models.hubert.modeling_hubert.HubertFlashAttention2 with Hubert->Musicgen\n class MusicgenFlashAttention2(MusicgenAttention):\n     \"\"\"\n     Musicgen flash attention module. This module inherits from `MusicgenAttention` as the weights of the module stays\n@@ -589,7 +589,7 @@ def __init__(self, config: MusicgenDecoderConfig):\n         self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=False)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer.forward\n+    # copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "c5328d064769e0d0bf201dd7db310648755450b2",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -159,7 +159,7 @@ def forward(self, inputs_embeds: torch.Tensor, past_key_values_length: int = 0):\n         return self.weights.index_select(0, position_ids.view(-1)).detach()\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->MusicgenMelody\n+# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->MusicgenMelody\n class MusicgenMelodyAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -318,7 +318,7 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->MusicgenMelody\n+# Copied from transformers.models.hubert.modeling_hubert.HubertFlashAttention2 with Hubert->MusicgenMelody\n class MusicgenMelodyFlashAttention2(MusicgenMelodyAttention):\n     \"\"\"\n     MusicgenMelody flash attention module. This module inherits from `MusicgenMelodyAttention` as the weights of the module stays\n@@ -441,7 +441,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartSdpaAttention with Bart->MusicgenMelody\n+# Copied from transformers.models.hubert.modeling_hubert.HubertSdpaAttention with Hubert->MusicgenMelody\n class MusicgenMelodySdpaAttention(MusicgenMelodyAttention):\n     def forward(\n         self,"
        },
        {
            "sha": "29874999e052b381ae0b2bb659a74ee723cc15e0",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 15,
            "deletions": 9,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -25,7 +25,10 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_causal_attention_mask,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -60,27 +63,30 @@ def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start\n     return shifted_input_ids\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartLearnedPositionalEmbedding with Bart->MVP\n+# Copied from transformers.models.bart.modeling_bart.BartLearnedPositionalEmbedding with Bart->Mvp\n class MvpLearnedPositionalEmbedding(nn.Embedding):\n     \"\"\"\n     This module learns positional embeddings up to a fixed maximum size.\n     \"\"\"\n \n     def __init__(self, num_embeddings: int, embedding_dim: int):\n-        # MVP is set up so that if padding_idx is specified then offset the embedding ids by 2\n+        # Mvp is set up so that if padding_idx is specified then offset the embedding ids by 2\n         # and adjust num_embeddings appropriately. Other models don't have this hack\n         self.offset = 2\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n \n-    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n+    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: torch.Tensor = None):\n         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n \n-        bsz, seq_len = input_ids.shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        ).expand(bsz, -1)\n+        if position_ids is None:\n+            bsz, seq_len = input_ids.shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            ).expand(bsz, -1)\n+        else:\n+            position_ids = position_ids.unsqueeze(0)\n \n-        return super().forward(positions + self.offset)\n+        return super().forward(position_ids + self.offset)\n \n \n class MvpAttention(nn.Module):"
        },
        {
            "sha": "3729fd6e18155ca30f8685494c08e634046ffb05",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -460,7 +460,7 @@ def forward(self, hidden_states: torch.Tensor, padding_mask: Optional[torch.Tens\n         return hidden_states, (router_probs, top_1_expert_index)\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->NllbMoe,key_value_states->encoder_hidden_states\n+# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->NllbMoe,key_value_states->encoder_hidden_states\n class NllbMoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n "
        },
        {
            "sha": "c10a55b1b02482966ee0ee84a677a806cf683544",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 22,
            "deletions": 50,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_patchtsmixer import PatchTSMixerConfig\n \n \n@@ -247,6 +248,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[PatchTSMixerConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -263,15 +265,23 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n+    # Ignore copy\n+    @deprecate_kwarg(\"key_value_states\", version=\"4.55\")\n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55\")\n+    @deprecate_kwarg(\"cache_position\", version=\"4.55\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -280,56 +290,21 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        key_states = self.k_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -343,10 +318,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -390,7 +362,7 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights_reshaped, None\n \n \n class PatchMixerBlock(nn.Module):"
        },
        {
            "sha": "57b69b6b5ebcedfb771e0b1634b95e7e93bb5712",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -32,7 +32,7 @@\n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->PatchTST\n+# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->PatchTST\n class PatchTSTAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n "
        },
        {
            "sha": "9f57f79df4ed49009ae3d7a7b2cc70454791a285",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 283,
            "deletions": 100,
            "changes": 383,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -25,8 +25,12 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n+    _prepare_4d_attention_mask,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -35,10 +39,21 @@\n     Seq2SeqModelOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from .configuration_pegasus import PegasusConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -82,13 +97,16 @@ def _init_weight(self):\n         self.weight = nn.Parameter(out, requires_grad=False)\n \n     @torch.no_grad()\n-    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:\n+    def forward(\n+        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ) -> torch.Tensor:\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n-        bsz, seq_len = input_ids_shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        )\n-        return super().forward(positions)\n+        if position_ids is None:\n+            bsz, seq_len = input_ids_shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            )\n+        return super().forward(position_ids)\n \n \n # Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Pegasus\n@@ -104,6 +122,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[PegasusConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -120,73 +139,74 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -200,10 +220,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -324,7 +341,7 @@ def forward(\n \n # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Pegasus, MBART->PEGASUS\n class PegasusDecoderLayer(nn.Module):\n-    def __init__(self, config: PegasusConfig):\n+    def __init__(self, config: PegasusConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -335,6 +352,7 @@ def __init__(self, config: PegasusConfig):\n             is_decoder=True,\n             is_causal=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -347,6 +365,7 @@ def __init__(self, config: PegasusConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -361,9 +380,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -382,47 +402,43 @@ def forward(\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n             # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.final_layer_norm(hidden_states)\n@@ -438,7 +454,7 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n@@ -461,6 +477,134 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n \n \n class PegasusEncoder(PegasusPreTrainedModel):\n@@ -692,7 +836,7 @@ def __init__(self, config: PegasusConfig, embed_tokens: Optional[nn.Embedding] =\n             config.d_model,\n             self.padding_idx,\n         )\n-        self.layers = nn.ModuleList([PegasusDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList([PegasusDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n         self.layer_norm = nn.LayerNorm(config.d_model)\n \n         self.gradient_checkpointing = False\n@@ -749,6 +893,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        cache_position=None,\n     ):\n         r\"\"\"\n         Args:\n@@ -814,6 +959,9 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -822,53 +970,79 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_shape = input_ids.size()\n-            input_ids = input_ids.view(-1, input_shape[-1])\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if input_ids is not None:\n+            input_ids = input_ids.view(-1, input_ids.shape[-1])\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        # important to apply scale outside of `if` in case users pass `embeds`\n+        inputs_embeds = inputs_embeds * self.embed_scale\n+\n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        batch_size, seq_length = inputs_embeds.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n+            )\n+\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n \n-        attention_mask = _prepare_4d_causal_attention_mask(\n-            attention_mask, input_shape, inputs_embeds, past_key_values_length\n+        self_attn_cache = (\n+            past_key_values.self_attention_cache\n+            if isinstance(past_key_values, EncoderDecoderCache)\n+            else past_key_values\n+        )\n+        causal_mask = self._update_causal_mask(\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            self_attn_cache,\n+            output_attentions,\n         )\n \n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n             # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n             encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n             )\n \n         # embed positions\n-        positions = self.embed_positions(input_shape, past_key_values_length)\n-\n+        positions = self.embed_positions((batch_size, seq_length), past_key_values_length, position_ids=cache_position)\n         hidden_states = inputs_embeds + positions\n-\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -887,39 +1061,39 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    attention_mask,\n+                    causal_mask,\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n                     head_mask[idx] if head_mask is not None else None,\n                     cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=attention_mask,\n+                    attention_mask=causal_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     cross_attn_layer_head_mask=(\n                         cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                     ),\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    cache_position=cache_position,\n                 )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -934,6 +1108,9 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n@@ -1020,6 +1197,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1099,6 +1277,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1205,6 +1384,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1280,6 +1460,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n \n@@ -1406,6 +1587,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n@@ -1455,6 +1637,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         logits = self.lm_head(outputs[0])"
        },
        {
            "sha": "c9554255e47002c44665165ff5bd93ec4ad286fc",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 283,
            "deletions": 103,
            "changes": 386,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -25,19 +25,34 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n+    _prepare_4d_attention_mask,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n     Seq2SeqLMOutput,\n     Seq2SeqModelOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from .configuration_pegasus_x import PegasusXConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -99,20 +114,24 @@ def __init__(self, embed_dim, max_scale: int = 10000.0):\n         self.max_scale = max_scale\n \n     @torch.no_grad()\n-    def forward(self, input_embeds: torch.Tensor, past_key_values_length: int = 0) -> torch.Tensor:\n+    def forward(\n+        self, input_embeds: torch.Tensor, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ) -> torch.Tensor:\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n         batch_size, seq_len = input_embeds.shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=input_embeds.device\n-        )[:, None]\n+        if position_ids is None:\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=input_embeds.device\n+            )[:, None]\n+\n         pe = torch.zeros((seq_len, self.embed_dim), device=input_embeds.device, dtype=input_embeds.dtype)\n         half_d_feature = self.embed_dim // 2\n         div_term = torch.exp(\n             torch.arange(half_d_feature, device=input_embeds.device, dtype=torch.int64).type_as(input_embeds)\n             * -(np.log(float(self.max_scale)) / (half_d_feature - 1))\n         )\n-        pe[:, :half_d_feature] = torch.sin(positions * div_term)\n-        pe[:, half_d_feature:] = torch.cos(positions * div_term)\n+        pe[:, :half_d_feature] = torch.sin(position_ids * div_term)\n+        pe[:, half_d_feature:] = torch.cos(position_ids * div_term)\n         return pe[None].expand(batch_size, -1, -1)\n \n \n@@ -129,6 +148,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[PegasusXConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -145,73 +165,74 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -225,10 +246,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -625,7 +643,7 @@ def unpad_local_tokens(cls, padded_hidden_states, block_size):\n \n \n class PegasusXDecoderLayer(nn.Module):\n-    def __init__(self, config: PegasusXConfig):\n+    def __init__(self, config: PegasusXConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -635,6 +653,7 @@ def __init__(self, config: PegasusXConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             bias=False,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -647,6 +666,7 @@ def __init__(self, config: PegasusXConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             bias=False,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -659,9 +679,10 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -677,45 +698,40 @@ def forward(\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n             use_cache: Whether to us KV cache for decoding\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.final_layer_norm(hidden_states)\n@@ -731,7 +747,7 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n@@ -742,6 +758,8 @@ class PegasusXPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [r\"PegasusXEncoderLayer\", r\"PegasusXDecoderLayer\"]\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -751,6 +769,134 @@ def _init_weights(self, module):\n                 module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n \n \n class PegasusXEncoder(PegasusXPreTrainedModel):\n@@ -990,7 +1136,7 @@ def __init__(self, config: PegasusXConfig, embed_tokens: Optional[nn.Embedding]\n             )\n \n         self.embed_positions = PegasusXSinusoidalPositionalEmbedding(config.d_model)\n-        self.layers = nn.ModuleList([PegasusXDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList([PegasusXDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n         self.layer_norm = nn.LayerNorm(config.d_model)\n \n         self.gradient_checkpointing = False\n@@ -1015,6 +1161,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        cache_position=None,\n     ):\n         r\"\"\"\n         Args:\n@@ -1069,6 +1216,9 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1077,55 +1227,78 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_shape = input_ids.size()\n-            input_ids = input_ids.view(-1, input_shape[-1])\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if input_ids is not None:\n+            input_ids = input_ids.view(-1, input_ids.shape[-1])\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        attention_mask = _prepare_4d_causal_attention_mask(\n-            attention_mask, input_shape, inputs_embeds, past_key_values_length\n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        batch_size, seq_length = inputs_embeds.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n+            )\n+\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n+\n+        self_attn_cache = (\n+            past_key_values.self_attention_cache\n+            if isinstance(past_key_values, EncoderDecoderCache)\n+            else past_key_values\n+        )\n+        causal_mask = self._update_causal_mask(\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            self_attn_cache,\n+            output_attentions,\n         )\n \n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n             # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n             encoder_attention_mask = _prepare_4d_attention_mask(\n-                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n             )\n \n         # embed positions\n-        positions = self.embed_positions(inputs_embeds, past_key_values_length)\n-\n-        positions = positions.to(inputs_embeds.device)\n-\n-        hidden_states = inputs_embeds + positions\n-\n+        position_ids = cache_position.unsqueeze(1)\n+        position_ids = self.embed_positions(inputs_embeds, past_key_values_length, position_ids)\n+        position_ids = position_ids.to(inputs_embeds.device)\n+        hidden_states = inputs_embeds + position_ids\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n@@ -1136,33 +1309,33 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    attention_mask,\n+                    causal_mask,\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=attention_mask,\n+                    attention_mask=causal_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    cache_position=cache_position,\n                 )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -1177,6 +1350,9 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n@@ -1264,6 +1440,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1334,6 +1511,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1419,6 +1597,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1464,6 +1643,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         lm_logits = self.lm_head(outputs[0])\n "
        },
        {
            "sha": "c82cfb906976a2777ecbe31aefeac3cba54ae03f",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 287,
            "deletions": 121,
            "changes": 408,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -24,12 +24,12 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n+    AttentionMaskConverter,\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n-    _prepare_4d_causal_attention_mask,\n-    _prepare_4d_causal_attention_mask_for_sdpa,\n )\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -40,10 +40,21 @@\n     Seq2SeqSequenceClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from .configuration_plbart import PLBartConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -80,15 +91,18 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         self.offset = 2\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n \n-    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n+    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: torch.Tensor = None):\n         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n \n-        bsz, seq_len = input_ids.shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        ).expand(bsz, -1)\n+        if position_ids is None:\n+            bsz, seq_len = input_ids.shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            ).expand(bsz, -1)\n+        else:\n+            position_ids = position_ids.unsqueeze(0)\n \n-        return super().forward(positions + self.offset)\n+        return super().forward(position_ids + self.offset)\n \n \n # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->PLBart\n@@ -118,6 +132,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[PLBartConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -134,73 +149,74 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -214,10 +230,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -266,7 +279,7 @@ def forward(\n \n # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->PLBart, BART->PLBART\n class PLBartEncoderLayer(nn.Module):\n-    def __init__(self, config: PLBartConfig):\n+    def __init__(self, config: PLBartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -275,6 +288,7 @@ def __init__(self, config: PLBartConfig):\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.dropout = config.dropout\n@@ -341,7 +355,7 @@ def forward(\n \n # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->PLBart, BART->PLBART\n class PLBartDecoderLayer(nn.Module):\n-    def __init__(self, config: PLBartConfig):\n+    def __init__(self, config: PLBartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -352,6 +366,7 @@ def __init__(self, config: PLBartConfig):\n             is_decoder=True,\n             is_causal=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -364,6 +379,7 @@ def __init__(self, config: PLBartConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -378,9 +394,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -399,47 +416,42 @@ def forward(\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         residual = hidden_states\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.activation_fn(self.fc1(hidden_states))\n@@ -455,7 +467,7 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n@@ -491,6 +503,8 @@ class PLBartPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PLBartDecoderLayer\", \"PLBartEncoderLayer\"]\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -502,6 +516,134 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        sequence_length = input_tensor.shape[1]\n+        if using_compilable_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n \n \n # Copied from transformers.models.bart.modeling_bart.BartEncoder with Bart->PLBart\n@@ -537,7 +679,7 @@ def __init__(self, config: PLBartConfig, embed_tokens: Optional[nn.Embedding] =\n             config.max_position_embeddings,\n             embed_dim,\n         )\n-        self.layers = nn.ModuleList([PLBartEncoderLayer(config) for _ in range(config.encoder_layers)])\n+        self.layers = nn.ModuleList([PLBartEncoderLayer(config, layer_idx=i) for i in range(config.encoder_layers)])\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n         self._use_sdpa = config._attn_implementation == \"sdpa\"\n         self.layernorm_embedding = nn.LayerNorm(embed_dim)\n@@ -722,7 +864,7 @@ def __init__(self, config: PLBartConfig, embed_tokens: Optional[nn.Embedding] =\n             config.max_position_embeddings,\n             config.d_model,\n         )\n-        self.layers = nn.ModuleList([PLBartDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList([PLBartDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n         self._use_sdpa = config._attn_implementation == \"sdpa\"\n \n@@ -752,6 +894,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -817,6 +960,9 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -825,43 +971,59 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input = input_ids\n-            input_shape = input.shape\n-            input_ids = input_ids.view(-1, input_shape[-1])\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-            input = inputs_embeds[:, :, -1]\n-        else:\n-            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if input_ids is not None:\n+            input_ids = input_ids.view(-1, input_ids.shape[-1])\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input)\n-\n-        if self._use_flash_attention_2:\n-            # 2d mask is passed through the layers\n-            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-        elif self._use_sdpa and not output_attentions and cross_attn_head_mask is None:\n-            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n-            # the manual implementation that requires a 4D causal mask in all cases.\n-            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask,\n-                input_shape,\n-                inputs_embeds,\n-                past_key_values_length,\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-        else:\n-            # 4d mask is passed through the layers\n-            attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        batch_size, seq_length = inputs_embeds.size()[:-1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n             )\n \n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n+\n+        self_attn_cache = (\n+            past_key_values.self_attention_cache\n+            if isinstance(past_key_values, EncoderDecoderCache)\n+            else past_key_values\n+        )\n+        causal_mask = self._update_causal_mask(\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            self_attn_cache,\n+            output_attentions,\n+        )\n+\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n             if self._use_flash_attention_2:\n@@ -873,35 +1035,28 @@ def forward(\n                 encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n                     encoder_attention_mask,\n                     inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n+                    tgt_len=seq_length,\n                 )\n             else:\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length\n                 )\n \n         # embed positions\n-        positions = self.embed_positions(input, past_key_values_length)\n-        positions = positions.to(inputs_embeds.device)\n+        position_ids = self.embed_positions(input, past_key_values_length, position_ids=cache_position)\n+        position_ids = position_ids.to(inputs_embeds.device)\n \n-        hidden_states = inputs_embeds + positions\n+        hidden_states = inputs_embeds + position_ids\n         hidden_states = self.layernorm_embedding(hidden_states)\n \n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -921,39 +1076,39 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    attention_mask,\n+                    causal_mask,\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n                     head_mask[idx] if head_mask is not None else None,\n                     cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n                     None,\n                     output_attentions,\n                     use_cache,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=attention_mask,\n+                    attention_mask=causal_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                     cross_attn_layer_head_mask=(\n                         cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                     ),\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    cache_position=cache_position,\n                 )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -966,6 +1121,9 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n@@ -1034,6 +1192,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple[torch.Tensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1108,6 +1267,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1190,6 +1350,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1267,6 +1428,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         lm_logits = self.lm_head(outputs[0])\n         lm_logits = lm_logits + self.final_logits_bias.to(lm_logits.device)\n@@ -1348,6 +1510,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1405,6 +1568,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         hidden_states = outputs[0]  # last hidden state\n \n@@ -1522,6 +1686,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n@@ -1571,6 +1736,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         logits = self.lm_head(outputs[0])"
        },
        {
            "sha": "e4208fc8cfd4810b50e33044ecf61d76ab582e8a",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -127,7 +127,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    # Copied from transformers.models.bart.modeling_bart.BartAttention._shape with BART->whisper\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "1fdccf728463e38395d4ef38f997d45617efc503",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -67,13 +67,16 @@ def _init_weight(self):\n         self.weight = nn.Parameter(out, requires_grad=False)\n \n     @torch.no_grad()\n-    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:\n+    def forward(\n+        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ) -> torch.Tensor:\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n-        bsz, seq_len = input_ids_shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        )\n-        return super().forward(positions)\n+        if position_ids is None:\n+            bsz, seq_len = input_ids_shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            )\n+        return super().forward(position_ids)\n \n \n def load_tf_weights_in_roformer(model, config, tf_checkpoint_path):"
        },
        {
            "sha": "42f3e4b577c31189a6f7c03f8dfc74383427eadc",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -28,7 +28,10 @@\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_causal_attention_mask,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1003,6 +1006,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[SeamlessM4TConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -1019,6 +1023,13 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)"
        },
        {
            "sha": "c62b97fb89ce6a5ea32e96fcf2a8d1affb3b72fd",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -909,6 +909,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[SeamlessM4Tv2Config] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -925,6 +926,13 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)"
        },
        {
            "sha": "b4d8584fe0cea34031ac0473e40d28ae5f4c0c92",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 66,
            "deletions": 159,
            "changes": 225,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -31,6 +31,7 @@\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_sew import SEWConfig\n \n \n@@ -385,6 +386,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[SEWConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -401,15 +403,23 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n+    # Ignore copy\n+    @deprecate_kwarg(\"key_value_states\", version=\"4.55\")\n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55\")\n+    @deprecate_kwarg(\"cache_position\", version=\"4.55\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -418,56 +428,21 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        key_states = self.k_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -481,10 +456,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -528,7 +500,7 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights_reshaped, None\n \n \n # Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->SEW\n@@ -547,9 +519,10 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n-    def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n-\n+    # Ignore copy\n+    @deprecate_kwarg(\"key_value_states\", version=\"4.55\")\n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55\")\n+    @deprecate_kwarg(\"cache_position\", version=\"4.55\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -558,62 +531,30 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n+        # SEWFlashAttention2 attention does not support output_attentions\n+        if output_attentions:\n+            raise ValueError(\n+                \"SEWSdpaAttention2 attention does not support `output_attentions`. \"\n+                \"Use the argument `attn_implementation='eager'` when loading the model.\"\n+            )\n \n         bsz, q_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self._reshape(self.q_proj(hidden_states), -1, bsz)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0].transpose(1, 2)\n-            value_states = past_key_value[1].transpose(1, 2)\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._reshape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0].transpose(1, 2), key_states], dim=1)\n-            value_states = torch.cat([past_key_value[1].transpose(1, 2), value_states], dim=1)\n-        else:\n-            # self_attention\n-            key_states = self._reshape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._reshape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states.transpose(1, 2), value_states.transpose(1, 2))\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value[0].shape[-2]\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim)\n+\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim)\n+        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim)\n \n         # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n         # therefore the input hidden states gets silently casted in float32. Hence, we need\n         # cast them back in the correct dtype just to be sure everything works as expected.\n         # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n         # in fp32. (LlamaRMSNorm handles it correctly)\n-\n         input_dtype = query_states.dtype\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n@@ -648,14 +589,13 @@ def forward(\n         attn_output = attn_output.reshape(bsz, q_len, -1)\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, None, None\n \n \n class SEWSdpaAttention(SEWAttention):\n-    # Copied from transformers.models.bart.modeling_bart.BartSdpaAttention.forward with Bart->SEW\n+    @deprecate_kwarg(\"key_value_states\", version=\"4.55\")\n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55\")\n+    @deprecate_kwarg(\"cache_position\", version=\"4.55\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -664,6 +604,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         if output_attentions:\n@@ -675,89 +616,55 @@ def forward(\n             return super().forward(\n                 hidden_states,\n                 key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n                 output_attentions=output_attentions,\n             )\n \n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        query_states = self._shape(query_states, tgt_len, bsz)\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        causal_mask = None\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n+        is_causal = True if self.is_causal and causal_mask is None and tgt_len > 1 else False\n \n         # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n         # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n+            attn_mask=causal_mask,\n             dropout_p=self.dropout if self.training else 0.0,\n             is_causal=is_causal,\n         )\n \n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n \n         # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n         # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n+        attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, None, None\n \n \n SEW_ATTENTION_CLASSES = {"
        },
        {
            "sha": "8db0674633fb304f3f0da4986be2d35b7553565e",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -161,7 +161,7 @@ def create_position_ids_from_input_ids(\n         return incremental_indices.long() + padding_idx\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Speech2Text\n+# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->Speech2Text\n class Speech2TextAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -392,7 +392,7 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Speech2Text, MBART->SPEECH_TO_TEXT\n+# copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Speech2Text, MBART->SPEECH_TO_TEXT\n class Speech2TextDecoderLayer(nn.Module):\n     def __init__(self, config: Speech2TextConfig):\n         super().__init__()"
        },
        {
            "sha": "4936ae56366dae5bf44151d80a9dc25ca0301d59",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 104,
            "deletions": 80,
            "changes": 184,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -32,10 +33,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n-from ...utils import (\n-    auto_docstring,\n-    logging,\n-)\n+from ...utils import auto_docstring, logging\n from .configuration_time_series_transformer import TimeSeriesTransformerConfig\n \n \n@@ -246,13 +244,16 @@ def _init_weight(self):\n         self.weight = nn.Parameter(out, requires_grad=False)\n \n     @torch.no_grad()\n-    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -> torch.Tensor:\n+    def forward(\n+        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+    ) -> torch.Tensor:\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n-        bsz, seq_len = input_ids_shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        )\n-        return super().forward(positions)\n+        if position_ids is None:\n+            bsz, seq_len = input_ids_shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            )\n+        return super().forward(position_ids)\n \n \n class TimeSeriesValueEmbedding(nn.Module):\n@@ -277,6 +278,7 @@ def __init__(\n         bias: bool = True,\n         is_causal: bool = False,\n         config: Optional[TimeSeriesTransformerConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -293,73 +295,74 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will lead to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        query_states = self.q_proj(hidden_states).view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states * self.scaling\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n         key_states = key_states.reshape(*proj_shape)\n         value_states = value_states.reshape(*proj_shape)\n \n@@ -373,10 +376,7 @@ def forward(\n             )\n \n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n+            attention_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n \n@@ -425,7 +425,7 @@ def forward(\n \n # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->TimeSeriesTransformer, BART->TIME_SERIES_TRANSFORMER\n class TimeSeriesTransformerEncoderLayer(nn.Module):\n-    def __init__(self, config: TimeSeriesTransformerConfig):\n+    def __init__(self, config: TimeSeriesTransformerConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -434,6 +434,7 @@ def __init__(self, config: TimeSeriesTransformerConfig):\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.dropout = config.dropout\n@@ -502,7 +503,7 @@ def forward(\n \n # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->TimeSeriesTransformer, with BART->TIME_SERIES_TRANSFORMER\n class TimeSeriesTransformerDecoderLayer(nn.Module):\n-    def __init__(self, config: TimeSeriesTransformerConfig):\n+    def __init__(self, config: TimeSeriesTransformerConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -513,6 +514,7 @@ def __init__(self, config: TimeSeriesTransformerConfig):\n             is_decoder=True,\n             is_causal=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -525,6 +527,7 @@ def __init__(self, config: TimeSeriesTransformerConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -539,9 +542,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -560,47 +564,42 @@ def forward(\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         residual = hidden_states\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.activation_fn(self.fc1(hidden_states))\n@@ -616,7 +615,7 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n@@ -799,7 +798,9 @@ def __init__(self, config: TimeSeriesTransformerConfig):\n         self.embed_positions = TimeSeriesSinusoidalPositionalEmbedding(\n             config.context_length + config.prediction_length, config.d_model\n         )\n-        self.layers = nn.ModuleList([TimeSeriesTransformerDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList(\n+            [TimeSeriesTransformerDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)]\n+        )\n         self.layernorm_embedding = nn.LayerNorm(config.d_model)\n \n         self.gradient_checkpointing = False\n@@ -819,6 +820,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -876,6 +878,9 @@ def forward(\n                 for more detail.\n             return_dict (`bool`, *optional*):\n                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+                cache in the correct position and to infer the complete sequence length.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -885,9 +890,22 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         input_shape = inputs_embeds.size()[:-1]\n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + input_shape[1], device=inputs_embeds.device\n+            )\n \n         attention_mask = _prepare_4d_causal_attention_mask(\n             attention_mask, input_shape, inputs_embeds, past_key_values_length\n@@ -916,7 +934,7 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -936,8 +954,6 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n@@ -961,14 +977,15 @@ def forward(\n                     cross_attn_layer_head_mask=(\n                         cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                     ),\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    cache_position=cache_position,\n                 )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -981,6 +998,9 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n@@ -1155,6 +1175,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Seq2SeqTSModelOutput, Tuple]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):\n@@ -1324,6 +1345,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1406,6 +1428,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Seq2SeqTSModelOutput, Tuple]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):\n@@ -1577,6 +1600,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         prediction_loss = None"
        },
        {
            "sha": "152243da0845cecdb9f5a747e6e8758448faccb0",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 13,
            "deletions": 7,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -24,7 +24,10 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_causal_attention_mask,\n+)\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n@@ -46,15 +49,18 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         self.offset = 2\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n \n-    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n+    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: torch.Tensor = None):\n         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n \n-        bsz, seq_len = input_ids.shape[:2]\n-        positions = torch.arange(\n-            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n-        ).expand(bsz, -1)\n+        if position_ids is None:\n+            bsz, seq_len = input_ids.shape[:2]\n+            position_ids = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n+            ).expand(bsz, -1)\n+        else:\n+            position_ids = position_ids.unsqueeze(0)\n \n-        return super().forward(positions + self.offset)\n+        return super().forward(position_ids + self.offset)\n \n \n # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->TrOCR"
        },
        {
            "sha": "7eb38da9acbe1a186c22362bddead77a83ea356d",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -465,7 +465,7 @@ def forward(self, hidden_states):\n         return hidden_states, norm_hidden_states\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Wav2Vec2\n+# Copied from transformers.models.hubert.modeling_hubert.HubertAttention with Hubert->Wav2Vec2\n class Wav2Vec2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -624,7 +624,7 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->Wav2Vec2\n+# Copied from transformers.models.hubert.modeling_hubert.HubertFlashAttention2 with Hubert->Wav2Vec2\n class Wav2Vec2FlashAttention2(Wav2Vec2Attention):\n     \"\"\"\n     Wav2Vec2 flash attention module. This module inherits from `Wav2Vec2Attention` as the weights of the module stays\n@@ -748,7 +748,7 @@ def forward(\n \n \n class Wav2Vec2SdpaAttention(Wav2Vec2Attention):\n-    # Copied from transformers.models.bart.modeling_bart.BartSdpaAttention.forward with Bart->Wav2Vec2\n+    # Copied from transformers.models.hubert.modeling_hubert.HubertSdpaAttention.forward with Hubert->Wav2Vec2\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "2bf387de82a68b38fdc6b02bd647a8ccaf68abac",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 17,
            "deletions": 12,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -259,10 +259,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    # Copied from transformers.models.bart.modeling_bart.BartAttention._shape with BART->whisper\n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -281,7 +277,9 @@ def forward(\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self._shape(self.q_proj(hidden_states) * self.scaling, tgt_len, bsz)\n+        query_states = self.q_proj(hidden_states) * self.scaling\n+        query_states = query_states.view(bsz, tgt_len, self.num_heads, self.head_dim)\n+        query_states = query_states.transpose(1, 2).contiguous()\n \n         if past_key_value is not None:\n             is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -299,8 +297,10 @@ def forward(\n             key_states = past_key_value.key_cache[self.layer_idx]\n             value_states = past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_states = self._shape(self.k_proj(current_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(current_states), -1, bsz)\n+            key_states = self.k_proj(current_states).view(bsz, -1, self.num_heads, self.head_dim)\n+            value_states = self.v_proj(current_states).view(bsz, -1, self.num_heads, self.head_dim)\n+            key_states = key_states.transpose(1, 2).contiguous()\n+            value_states = value_states.transpose(1, 2).contiguous()\n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n@@ -398,8 +398,10 @@ def forward(\n             key_states = past_key_value.key_cache[self.layer_idx]\n             value_states = past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_states = self._shape(self.k_proj(current_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(current_states), -1, bsz)\n+            key_states = self.k_proj(current_states).view(bsz, tgt_len, self.num_heads, self.head_dim)\n+            value_states = self.v_proj(current_states).view(bsz, tgt_len, self.num_heads, self.head_dim)\n+            key_states = key_states.transpose(1, 2).contiguous()\n+            value_states = value_states.transpose(1, 2).contiguous()\n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n@@ -496,7 +498,8 @@ def forward(\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n-        query_states = self._shape(self.q_proj(hidden_states), tgt_len, bsz)\n+        query_states = self.q_proj(hidden_states).view(bsz, tgt_len, self.num_heads, self.head_dim)\n+        query_states = query_states.transpose(1, 2).contiguous()\n \n         if past_key_value is not None:\n             is_updated = past_key_value.is_updated.get(self.layer_idx)\n@@ -514,8 +517,10 @@ def forward(\n             key_states = past_key_value.key_cache[self.layer_idx]\n             value_states = past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_states = self._shape(self.k_proj(current_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(current_states), -1, bsz)\n+            key_states = self.k_proj(current_states).view(bsz, -1, self.num_heads, self.head_dim)\n+            value_states = self.v_proj(current_states).view(bsz, -1, self.num_heads, self.head_dim)\n+            key_states = key_states.transpose(1, 2).contiguous()\n+            value_states = value_states.transpose(1, 2).contiguous()\n             if past_key_value is not None:\n                 # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None"
        },
        {
            "sha": "dc1e268cbe4a4e0d2c6e5ad971fbd20f99500c5e",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -282,7 +282,7 @@ def __init__(self, config: XGLMConfig):\n         self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer.forward\n+    # copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "783e7cb3a7a7437d3ef5e8101b61e9c99e5fb340",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -2144,6 +2144,7 @@ def test_generate_compile_model_forward(self):\n             compile_config._compile_all_devices = True  # force compilation (e.g. fast CI, CPU)\n \n             generation_kwargs = {\n+                \"use_cache\": True,\n                 \"do_sample\": False,\n                 \"max_new_tokens\": 5,\n                 \"return_dict_in_generate\": True,"
        },
        {
            "sha": "34fbcd80f9bf327b8fc9edbbb5b39e2494d5f595",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -87,7 +87,7 @@ def __init__(\n         hidden_act=\"gelu\",\n         hidden_dropout_prob=0.1,\n         attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=20,\n+        max_position_embeddings=50,\n         eos_token_id=2,\n         pad_token_id=1,\n         bos_token_id=0,\n@@ -1164,8 +1164,7 @@ def test_cnn_summarization_same_as_fairseq(self):\n             [FRANCE_ARTICLE, SHORTER_ARTICLE, IRAN_ARTICLE, ARTICLE_SUBWAY],\n             max_length=1024,\n             padding=\"max_length\",\n-            truncation_strategy=\"only_first\",\n-            truncation=True,\n+            truncation=\"only_first\",\n             return_tensors=\"pt\",\n         )\n \n@@ -1301,7 +1300,7 @@ def __init__(\n         decoder_layers=2,\n         encoder_attention_heads=4,\n         decoder_attention_heads=4,\n-        max_position_embeddings=30,\n+        max_position_embeddings=50,\n         is_encoder_decoder=False,\n         pad_token_id=0,\n         bos_token_id=1,\n@@ -1365,6 +1364,7 @@ def prepare_config_and_inputs(self):\n             decoder_start_token_id=self.decoder_start_token_id,\n             max_position_embeddings=self.max_position_embeddings,\n             is_encoder_decoder=self.is_encoder_decoder,\n+            forced_eos_token_id=None,\n         )\n \n         return (\n@@ -1465,9 +1465,9 @@ def create_and_check_decoder_model_attention_mask_past(\n \n         # get two different outputs\n         output_from_no_past = model(next_input_ids, attention_mask=attn_mask)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, attention_mask=attn_mask, past_key_values=past_key_values)[\n-            \"last_hidden_state\"\n-        ]\n+        output_from_past = model(\n+            next_tokens, attention_mask=attn_mask, past_key_values=past_key_values, use_cache=True\n+        )[\"last_hidden_state\"]\n \n         # select random slice\n         random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()"
        },
        {
            "sha": "34e633757acdb8d58e88d323ed98fb90d85b138f",
            "filename": "tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -611,7 +611,7 @@ def __init__(\n         decoder_layers=2,\n         encoder_attention_heads=4,\n         decoder_attention_heads=4,\n-        max_position_embeddings=30,\n+        max_position_embeddings=50,\n         is_encoder_decoder=False,\n         pad_token_id=0,\n         bos_token_id=1,\n@@ -767,7 +767,7 @@ def create_and_check_decoder_model_attention_mask_past(\n \n         # get two different outputs\n         output_from_no_past = model(next_input_ids)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, past_key_values=past_key_values)[\"last_hidden_state\"]\n+        output_from_past = model(next_tokens, past_key_values=past_key_values, use_cache=True)[\"last_hidden_state\"]\n \n         # select random slice\n         random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()"
        },
        {
            "sha": "232f7176b23c5b19a49eb12c3a29902cec3f6b03",
            "filename": "tests/models/biogpt/test_modeling_biogpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -320,6 +320,7 @@ def test_batch_generation(self):\n         # Define PAD Token = EOS Token = 50256\n         tokenizer.pad_token = tokenizer.eos_token\n         model.config.pad_token_id = model.config.eos_token_id\n+        model.generation_config.pad_token_id = model.generation_config.eos_token_id\n \n         # use different length sentences to test batching\n         sentences = [\n@@ -333,10 +334,11 @@ def test_batch_generation(self):\n         outputs = model.generate(\n             input_ids=input_ids,\n             attention_mask=inputs[\"attention_mask\"].to(torch_device),\n+            max_new_tokens=10,\n         )\n \n         inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_non_padded = model.generate(input_ids=inputs_non_padded)\n+        output_non_padded = model.generate(input_ids=inputs_non_padded, max_new_tokens=10)\n \n         num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)"
        },
        {
            "sha": "83a5c73ff7b76fe1eae2a0800c91d108237f427a",
            "filename": "tests/models/blenderbot/test_modeling_blenderbot.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -356,7 +356,7 @@ def __init__(\n         decoder_layers=2,\n         encoder_attention_heads=4,\n         decoder_attention_heads=4,\n-        max_position_embeddings=30,\n+        max_position_embeddings=50,\n         is_encoder_decoder=False,\n         pad_token_id=0,\n         bos_token_id=1,\n@@ -500,9 +500,9 @@ def create_and_check_decoder_model_attention_mask_past(\n \n         # get two different outputs\n         output_from_no_past = model(next_input_ids, attention_mask=attn_mask)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, past_key_values=past_key_values, attention_mask=attn_mask)[\n-            \"last_hidden_state\"\n-        ]\n+        output_from_past = model(\n+            next_tokens, past_key_values=past_key_values, attention_mask=attn_mask, use_cache=True\n+        )[\"last_hidden_state\"]\n \n         # select random slice\n         random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()"
        },
        {
            "sha": "5e9163376a2c4308f8dad7a070389ffde794ec45",
            "filename": "tests/models/blenderbot_small/test_modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -366,7 +366,7 @@ def __init__(\n         decoder_layers=2,\n         encoder_attention_heads=4,\n         decoder_attention_heads=4,\n-        max_position_embeddings=30,\n+        max_position_embeddings=50,\n         is_encoder_decoder=False,\n         pad_token_id=0,\n         bos_token_id=1,\n@@ -509,9 +509,9 @@ def create_and_check_decoder_model_attention_mask_past(\n \n         # get two different outputs\n         output_from_no_past = model(next_input_ids, attention_mask=attn_mask)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, past_key_values=past_key_values, attention_mask=attn_mask)[\n-            \"last_hidden_state\"\n-        ]\n+        output_from_past = model(\n+            next_tokens, past_key_values=past_key_values, attention_mask=attn_mask, use_cache=True\n+        )[\"last_hidden_state\"]\n \n         # select random slice\n         random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()"
        },
        {
            "sha": "92fddb499891d412f939996bc2605335f6ca4322",
            "filename": "tests/models/informer/test_modeling_informer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -170,8 +170,9 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n \n         embed_positions = InformerSinusoidalPositionalEmbedding(\n             config.context_length + config.prediction_length, config.d_model\n-        ).to(torch_device)\n+        )\n         embed_positions._init_weight()\n+        embed_positions = embed_positions.to(torch_device)\n         self.parent.assertTrue(torch.equal(model.encoder.embed_positions.weight, embed_positions.weight))\n         self.parent.assertTrue(torch.equal(model.decoder.embed_positions.weight, embed_positions.weight))\n "
        },
        {
            "sha": "c264de826c20a1bb9b194ba030faafd20f5fd794",
            "filename": "tests/models/m2m_100/test_modeling_m2m_100.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -82,7 +82,7 @@ def __init__(\n         attention_probs_dropout_prob=0.1,\n         encoder_layerdrop=0.0,\n         decoder_layerdrop=0.0,\n-        max_position_embeddings=20,\n+        max_position_embeddings=50,\n         eos_token_id=2,\n         pad_token_id=1,\n         bos_token_id=0,\n@@ -426,7 +426,7 @@ def test_flash_attn_2_seq_to_seq_generation(self):\n         Overwriting the common test as the test is flaky on tiny models\n         \"\"\"\n         model = M2M100ForConditionalGeneration.from_pretrained(\n-            \"facebook/m2m100_418M\", attn_implementation=\"flash_attention_2\"\n+            \"facebook/m2m100_418M\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\"\n         ).to(torch_device)\n \n         tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"fr\", tgt_lang=\"en\")"
        },
        {
            "sha": "a53627852f5d113318a987bcd2395489a0ed34e6",
            "filename": "tests/models/marian/test_modeling_marian.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -99,7 +99,7 @@ def __init__(\n         hidden_act=\"gelu\",\n         hidden_dropout_prob=0.1,\n         attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=20,\n+        max_position_embeddings=100,\n         eos_token_id=2,\n         pad_token_id=1,\n         bos_token_id=0,\n@@ -653,7 +653,7 @@ def __init__(\n         decoder_layers=2,\n         encoder_attention_heads=4,\n         decoder_attention_heads=4,\n-        max_position_embeddings=30,\n+        max_position_embeddings=100,\n         is_encoder_decoder=False,\n         pad_token_id=0,\n         bos_token_id=1,\n@@ -796,9 +796,9 @@ def create_and_check_decoder_model_attention_mask_past(\n \n         # get two different outputs\n         output_from_no_past = model(next_input_ids, attention_mask=attn_mask)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, attention_mask=attn_mask, past_key_values=past_key_values)[\n-            \"last_hidden_state\"\n-        ]\n+        output_from_past = model(\n+            next_tokens, attention_mask=attn_mask, past_key_values=past_key_values, use_cache=True\n+        )[\"last_hidden_state\"]\n \n         # select random slice\n         random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()"
        },
        {
            "sha": "3b328cfe9e018977befa2e85e4804fa7bcea7527",
            "filename": "tests/models/mbart/test_modeling_mbart.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -538,7 +538,7 @@ def __init__(\n         decoder_layers=2,\n         encoder_attention_heads=4,\n         decoder_attention_heads=4,\n-        max_position_embeddings=30,\n+        max_position_embeddings=50,\n         is_encoder_decoder=False,\n         pad_token_id=0,\n         bos_token_id=1,\n@@ -681,9 +681,9 @@ def create_and_check_decoder_model_attention_mask_past(\n \n         # get two different outputs\n         output_from_no_past = model(next_input_ids, attention_mask=attn_mask)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, attention_mask=attn_mask, past_key_values=past_key_values)[\n-            \"last_hidden_state\"\n-        ]\n+        output_from_past = model(\n+            next_tokens, attention_mask=attn_mask, past_key_values=past_key_values, use_cache=True\n+        )[\"last_hidden_state\"]\n \n         # select random slice\n         random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()"
        },
        {
            "sha": "2b7a127d48289f949f493d108af6b5c9c2c4641d",
            "filename": "tests/models/pegasus/test_modeling_pegasus.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -542,9 +542,9 @@ def create_and_check_decoder_model_attention_mask_past(\n \n         # get two different outputs\n         output_from_no_past = model(next_input_ids, attention_mask=attn_mask)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, attention_mask=attn_mask, past_key_values=past_key_values)[\n-            \"last_hidden_state\"\n-        ]\n+        output_from_past = model(\n+            next_tokens, attention_mask=attn_mask, past_key_values=past_key_values, use_cache=True\n+        )[\"last_hidden_state\"]\n \n         # select random slice\n         random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()"
        },
        {
            "sha": "2ffdc4636756317c1b0b9594870915770700f72d",
            "filename": "tests/models/pegasus_x/test_modeling_pegasus_x.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -78,7 +78,7 @@ def __init__(\n         hidden_act=\"gelu\",\n         hidden_dropout_prob=0.1,\n         attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=20,\n+        max_position_embeddings=50,\n         eos_token_id=2,\n         pad_token_id=1,\n         bos_token_id=0,\n@@ -676,7 +676,7 @@ def __init__(\n         decoder_layers=2,\n         encoder_attention_heads=4,\n         decoder_attention_heads=4,\n-        max_position_embeddings=30,\n+        max_position_embeddings=50,\n         is_encoder_decoder=False,\n         pad_token_id=0,\n         bos_token_id=1,\n@@ -819,7 +819,7 @@ def create_and_check_decoder_model_attention_mask_past(\n \n         # get two different outputs\n         output_from_no_past = model(next_input_ids)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, past_key_values=past_key_values)[\"last_hidden_state\"]\n+        output_from_past = model(next_tokens, past_key_values=past_key_values, use_cache=True)[\"last_hidden_state\"]\n \n         # select random slice\n         random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()"
        },
        {
            "sha": "569a63d435e50f086bb41c3092d4b5b305ea5e1a",
            "filename": "tests/models/plbart/test_modeling_plbart.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -496,7 +496,7 @@ def __init__(\n         decoder_layers=2,\n         encoder_attention_heads=4,\n         decoder_attention_heads=4,\n-        max_position_embeddings=30,\n+        max_position_embeddings=50,\n         is_encoder_decoder=False,\n         pad_token_id=0,\n         bos_token_id=1,\n@@ -634,9 +634,9 @@ def create_and_check_decoder_model_attention_mask_past(\n \n         # get two different outputs\n         output_from_no_past = model(next_input_ids, attention_mask=attn_mask)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, attention_mask=attn_mask, past_key_values=past_key_values)[\n-            \"last_hidden_state\"\n-        ]\n+        output_from_past = model(\n+            next_tokens, attention_mask=attn_mask, past_key_values=past_key_values, use_cache=True\n+        )[\"last_hidden_state\"]\n \n         # select random slice\n         random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()"
        },
        {
            "sha": "338047e95a17c85c53bf89cf50c86a81e18105c8",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01ad9f4b493726a159429c86595fbcf0ac5419ed/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=01ad9f4b493726a159429c86595fbcf0ac5419ed",
            "patch": "@@ -735,6 +735,7 @@ def check_determinism(first, second):\n             model = model_class(config)\n             model.to(torch_device)\n             model.eval()\n+            print(model_class)\n             with torch.no_grad():\n                 first = model(**self._prepare_for_class(inputs_dict, model_class))[0]\n                 second = model(**self._prepare_for_class(inputs_dict, model_class))[0]\n@@ -4130,6 +4131,9 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n             if \"position_ids\" not in inspect.signature(model.forward).parameters:\n                 self.skipTest(\"Model does not support position_ids\")\n \n+            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n+                continue  # this model doesn't accept position ids as input\n+\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n \n@@ -4268,7 +4272,16 @@ def test_flash_attn_2_from_config(self):\n             dummy_input = torch.LongTensor([[0, 2, 3, 4], [0, 2, 3, 4]]).to(torch_device)\n             dummy_attention_mask = torch.LongTensor([[1, 1, 1, 1], [0, 1, 1, 1]]).to(torch_device)\n \n-            _ = fa2_model(input_ids=dummy_input, attention_mask=dummy_attention_mask)\n+            if config.is_encoder_decoder:\n+                _ = fa2_model(\n+                    input_ids=dummy_input,\n+                    attention_mask=dummy_attention_mask,\n+                    decoder_input_ids=dummy_input.clone(),\n+                    decoder_attention_mask=dummy_attention_mask.clone(),\n+                )\n+            else:\n+                _ = fa2_model(input_ids=dummy_input, attention_mask=dummy_attention_mask)\n+\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 fa2_model.save_pretrained(tmpdirname)\n                 model_from_pretrained = model_class.from_pretrained(tmpdirname)\n@@ -4327,8 +4340,10 @@ def test_custom_4d_attention_mask(self):\n             set_config_for_less_flaky_test(config)\n             if getattr(config, \"sliding_window\", 0) is not None and getattr(config, \"sliding_window\", 0) > 0:\n                 self.skipTest(f\"{model_class.__name__} with sliding window attention is not supported by this test\")\n-            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n+            model = model_class(config).to(device=torch_device, dtype=torch.float32).eval()\n             set_model_for_less_flaky_test(model)\n+            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n+                continue  # model doesn't accept position ids and probably has special way to model positions\n \n             if \"position_ids\" not in inspect.signature(model.forward).parameters:\n                 continue  # this model doesn't accept position ids as input"
        }
    ],
    "stats": {
        "total": 5755,
        "additions": 3832,
        "deletions": 1923
    }
}