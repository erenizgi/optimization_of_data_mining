{
    "author": "gante",
    "message": "ðŸš¨ [generate] update paligemma mask updates (and other assisted generation-related fixes) (#40917)\n\n* tmp\n\n* fix modular inheritance\n\n* nit\n\n* paligemma 1 doesn't have swa\n\n* use same pattern as in models with hybrid layers\n\n* PR comments\n\n* helium also needs layer_typed (bc it relies on gemma)\n\n* paligemma/gemma3: same mask creation fn in fwd and generate\n\n* propagate changes to helium (gemma-based)\n\n* tmp commit\n\n* slow paligemma tests passing, let's see what breaks\n\n* fix test_left_padding_compatibility\n\n* tmp commit\n\n* tmp commit\n\n* rebase error\n\n* docs\n\n* reduce diff\n\n* like this?\n\n* t5gemma\n\n* better comment\n\n* shorter diff\n\n* exception\n\n* ffs type\n\n* optional\n\n* shorter modular_gemma.py\n\n* helium model actually needs no changes -- the tester is the issue\n\n* t5gemma modular config\n\n* a few more modular; paligemma BC\n\n* fix processor issues?\n\n* rm config exception\n\n* lift warning in gemma",
    "sha": "869735d37d0f929311ac6611728c482a4414ba8c",
    "files": [
        {
            "sha": "65668f667b0e8fab1395f318818556603773d5a2",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -1073,8 +1073,8 @@ def create_masks_for_generate(\n     **kwargs,\n ):\n     \"\"\"\n-    This function mimics how we create the masks in the `modeling_xxx.py` files, and is used in `generate` in order\n-    to easily create the masks in advance, when we compile the forwards with Static caches.\n+    This function mimics how we create the masks in the `modeling_xxx.py` files, and is used in places like `generate`\n+    in order to easily create the masks in advance, when we compile the forwards with Static caches.\n \n     Args:\n         config (`PretrainedConfig`):"
        },
        {
            "sha": "8136f560f18ee231818fe767542c7adbe272d777",
            "filename": "src/transformers/models/colpali/modular_colpali.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -136,7 +136,7 @@ def __call__(\n         )\n         suffix = output_kwargs[\"text_kwargs\"].pop(\"suffix\", None)\n \n-        return_token_type_ids = suffix is not None\n+        return_token_type_ids = True\n \n         if text is None and images is None:\n             raise ValueError(\"Either text or images must be provided\")\n@@ -167,7 +167,7 @@ def __call__(\n \n             inputs = self.tokenizer(\n                 input_strings,\n-                return_token_type_ids=False,\n+                return_token_type_ids=return_token_type_ids,\n                 **output_kwargs[\"text_kwargs\"],\n             )\n \n@@ -197,7 +197,7 @@ def __call__(\n \n             batch_query = self.tokenizer(\n                 texts_query,\n-                return_token_type_ids=False,\n+                return_token_type_ids=return_token_type_ids,\n                 **output_kwargs[\"text_kwargs\"],\n             )\n "
        },
        {
            "sha": "1d76a74e1ab81afcc9506db8bb5cab774114ac4f",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -177,7 +177,7 @@ def __call__(\n         )\n         suffix = output_kwargs[\"text_kwargs\"].pop(\"suffix\", None)\n \n-        return_token_type_ids = suffix is not None\n+        return_token_type_ids = True\n \n         if text is None and images is None:\n             raise ValueError(\"Either text or images must be provided\")\n@@ -208,7 +208,7 @@ def __call__(\n \n             inputs = self.tokenizer(\n                 input_strings,\n-                return_token_type_ids=False,\n+                return_token_type_ids=return_token_type_ids,\n                 **output_kwargs[\"text_kwargs\"],\n             )\n \n@@ -238,7 +238,7 @@ def __call__(\n \n             batch_query = self.tokenizer(\n                 texts_query,\n-                return_token_type_ids=False,\n+                return_token_type_ids=return_token_type_ids,\n                 **output_kwargs[\"text_kwargs\"],\n             )\n \n@@ -262,6 +262,12 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n             vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n         return MultiModalData(**vision_data)\n \n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names + [\"token_type_ids\", \"labels\"]\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(tokenizer_input_names + image_processor_input_names)\n+\n     @property\n     def query_augmentation_token(self) -> str:\n         \"\"\""
        },
        {
            "sha": "e8f7e057247c0861da841757cb229de7c8fda780",
            "filename": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -247,6 +247,18 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+\n+        # ColQwen doesn't process videos. Make a copy of list when removing\n+        # otherwise `self.feature_extractor.model_input_names` is also modified\n+        image_processor_input_names = [\n+            name for name in image_processor_input_names if name not in [\"pixel_values_videos\", \"video_grid_thw\"]\n+        ]\n+        return tokenizer_input_names + image_processor_input_names\n+\n     @property\n     def query_augmentation_token(self) -> str:\n         \"\"\"\n@@ -385,17 +397,5 @@ def score_retrieval(\n \n         return torch.cat(scores, dim=0)\n \n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-\n-        # ColQwen doesn't process videos. Make a copy of list when removing\n-        # otherwise `self.feature_extractor.model_input_names` is also modified\n-        image_processor_input_names = [\n-            name for name in image_processor_input_names if name not in [\"pixel_values_videos\", \"video_grid_thw\"]\n-        ]\n-        return tokenizer_input_names + image_processor_input_names\n-\n \n __all__ = [\"ColQwen2Processor\"]"
        },
        {
            "sha": "7910f27dcfed9599cd237cc65814c224f0b35bb6",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -19,7 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n \n \n class GemmaConfig(PretrainedConfig):\n@@ -30,6 +30,7 @@ class GemmaConfig(PretrainedConfig):\n     e.g. [google/gemma-7b](https://huggingface.co/google/gemma-7b)\n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n+\n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the Gemma model. Defines the number of different tokens that can be represented by the\n@@ -77,6 +78,11 @@ class GemmaConfig(PretrainedConfig):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+        use_bidirectional_attention (`bool`, *optional*):\n+            If True, the model will attend to all text tokens instead of using a causal mask.\n+\n     ```python\n     >>> from transformers import GemmaModel, GemmaConfig\n     >>> # Initializing a Gemma gemma-7b style configuration\n@@ -125,6 +131,8 @@ def __init__(\n         rope_theta=10000.0,\n         attention_bias=False,\n         attention_dropout=0.0,\n+        layer_types=None,\n+        use_bidirectional_attention=None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -142,6 +150,12 @@ def __init__(\n         self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        self.use_bidirectional_attention = use_bidirectional_attention\n+\n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\"full_attention\" for _ in range(self.num_hidden_layers)]\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "ef0a688d4608e03dde39168dd8fac96c2b5f0cbc",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 15,
            "deletions": 10,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -198,7 +198,7 @@ def __init__(self, config: GemmaConfig, layer_idx: int):\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.is_causal = True\n+        self.is_causal = not getattr(config, \"use_bidirectional_attention\", False)\n \n         self.q_proj = nn.Linear(\n             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n@@ -268,6 +268,7 @@ def __init__(self, config: GemmaConfig, layer_idx: int):\n         self.mlp = GemmaMLP(config)\n         self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.attention_type = config.layer_types[layer_idx]\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n@@ -379,14 +380,18 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = create_causal_mask(\n-            config=self.config,\n-            input_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-            cache_position=cache_position,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=inputs_embeds,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                    position_ids=position_ids,\n+                )\n+            }\n \n         # embed positions\n         hidden_states = inputs_embeds\n@@ -403,7 +408,7 @@ def forward(\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,"
        },
        {
            "sha": "df07b721fb1036e066953f8ce6558258aab3cf78",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 44,
            "deletions": 10,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -20,14 +20,16 @@\n from torch import nn\n \n from ...cache_utils import Cache, DynamicCache\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import TransformersKwargs, logging\n from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaDecoderLayer,\n     LlamaForCausalLM,\n     LlamaForSequenceClassification,\n     LlamaForTokenClassification,\n@@ -58,6 +60,7 @@ class GemmaConfig(PretrainedConfig):\n     e.g. [google/gemma-7b](https://huggingface.co/google/gemma-7b)\n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n+\n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the Gemma model. Defines the number of different tokens that can be represented by the\n@@ -105,6 +108,11 @@ class GemmaConfig(PretrainedConfig):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+        use_bidirectional_attention (`bool`, *optional*):\n+            If True, the model will attend to all text tokens instead of using a causal mask.\n+\n     ```python\n     >>> from transformers import GemmaModel, GemmaConfig\n     >>> # Initializing a Gemma gemma-7b style configuration\n@@ -153,6 +161,8 @@ def __init__(\n         rope_theta=10000.0,\n         attention_bias=False,\n         attention_dropout=0.0,\n+        layer_types=None,\n+        use_bidirectional_attention=None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -170,6 +180,12 @@ def __init__(\n         self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n+        self.use_bidirectional_attention = use_bidirectional_attention\n+\n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\"full_attention\" for _ in range(self.num_hidden_layers)]\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         super().__init__(\n             pad_token_id=pad_token_id,\n@@ -368,6 +384,20 @@ class GemmaRotaryEmbedding(LlamaRotaryEmbedding):\n     pass\n \n \n+class GemmaAttention(LlamaAttention):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: GemmaConfig, layer_idx: int):\n+        super().__init__()\n+        self.is_causal = not getattr(config, \"use_bidirectional_attention\", False)\n+\n+\n+class GemmaDecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: GemmaConfig, layer_idx: int):\n+        super().__init__()\n+        self.attention_type = config.layer_types[layer_idx]\n+\n+\n class GemmaPreTrainedModel(LlamaPreTrainedModel):\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n@@ -407,14 +437,18 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = create_causal_mask(\n-            config=self.config,\n-            input_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-            cache_position=cache_position,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=inputs_embeds,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                    position_ids=position_ids,\n+                )\n+            }\n \n         # embed positions\n         hidden_states = inputs_embeds\n@@ -431,7 +465,7 @@ def forward(\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,"
        },
        {
            "sha": "58749515169e56730d7143c916e9690c33539d67",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -30,6 +30,7 @@ class Gemma2Config(PretrainedConfig):\n     e.g. [google/gemma2-7b](https://huggingface.co/google/gemma2-7b)\n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n+\n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the Gemma2 model. Defines the number of different tokens that can be represented by the\n@@ -88,6 +89,8 @@ class Gemma2Config(PretrainedConfig):\n             scaling factor when applying tanh softcapping on the logits.\n         attn_logit_softcapping (`float`, *optional*, defaults to 50.0):\n             scaling factor when applying tanh softcapping on the attention scores.\n+        use_bidirectional_attention (`bool`, *optional*):\n+            If True, the model will attend to all text tokens instead of using a causal mask.\n \n     ```python\n     >>> from transformers import Gemma2Model, Gemma2Config\n@@ -142,6 +145,7 @@ def __init__(\n         layer_types=None,\n         final_logit_softcapping=30.0,\n         attn_logit_softcapping=50.0,\n+        use_bidirectional_attention=None,\n         **kwargs,\n     ):\n         super().__init__(\n@@ -171,6 +175,7 @@ def __init__(\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n+        self.use_bidirectional_attention = use_bidirectional_attention\n \n         if self.layer_types is None:\n             self.layer_types = ["
        },
        {
            "sha": "2a218338384a4f37dc7f8c90e8a3bb934e0e24ed",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -211,7 +211,7 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = config.query_pre_attn_scalar**-0.5\n         self.attention_dropout = self.config.attention_dropout\n-        self.is_causal = True\n+        self.is_causal = not getattr(config, \"use_bidirectional_attention\", False)\n \n         self.q_proj = nn.Linear(\n             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias"
        },
        {
            "sha": "12024bdcb7b9b022c181bc8555663725e93f0b6e",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -55,6 +55,7 @@ class Gemma2Config(PretrainedConfig):\n     e.g. [google/gemma2-7b](https://huggingface.co/google/gemma2-7b)\n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n+\n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the Gemma2 model. Defines the number of different tokens that can be represented by the\n@@ -113,6 +114,8 @@ class Gemma2Config(PretrainedConfig):\n             scaling factor when applying tanh softcapping on the logits.\n         attn_logit_softcapping (`float`, *optional*, defaults to 50.0):\n             scaling factor when applying tanh softcapping on the attention scores.\n+        use_bidirectional_attention (`bool`, *optional*):\n+            If True, the model will attend to all text tokens instead of using a causal mask.\n \n     ```python\n     >>> from transformers import Gemma2Model, Gemma2Config\n@@ -167,6 +170,7 @@ def __init__(\n         layer_types=None,\n         final_logit_softcapping=30.0,\n         attn_logit_softcapping=50.0,\n+        use_bidirectional_attention=None,\n         **kwargs,\n     ):\n         super().__init__(\n@@ -196,6 +200,7 @@ def __init__(\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n+        self.use_bidirectional_attention = use_bidirectional_attention\n \n         if self.layer_types is None:\n             self.layer_types = [\n@@ -258,7 +263,7 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.attention_dropout = self.config.attention_dropout\n-        self.is_causal = True\n+        self.is_causal = not getattr(config, \"use_bidirectional_attention\", False)\n         self.scaling = config.query_pre_attn_scalar**-0.5\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n "
        },
        {
            "sha": "893d0626dfd73bd4a01435554907c47f62cee2bc",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -38,6 +38,7 @@ class Gemma3TextConfig(PretrainedConfig):\n     e.g. [google/gemma3_text-7b](https://huggingface.co/google/gemma3_text-7b)\n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n+\n     Args:\n         vocab_size (`int`, *optional*, defaults to 262208):\n             Vocabulary size of the Gemma3Text model. Defines the number of different tokens that can be represented by the\n@@ -135,8 +136,9 @@ class Gemma3TextConfig(PretrainedConfig):\n                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         rope_local_base_freq (float, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings for local attention.\n-        use_bidirectional_attention (`bool`, *optional*, defaults to `False`): If True, the model will attend to all\n-            text tokens instead of using a causal mask. This does not change behavior for vision tokens.\n+        use_bidirectional_attention (`bool`, *optional*, defaults to `False`):\n+            If True, the model will attend to all text tokens instead of using a causal mask. This does not change\n+            behavior for vision tokens.\n \n     ```python\n     >>> from transformers import Gemma3TextModel, Gemma3TextConfig"
        },
        {
            "sha": "ed9c83180059b8f18ee329b3acdc20042e5907d0",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 73,
            "deletions": 63,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -729,7 +729,6 @@ def forward(self, vision_outputs: torch.Tensor):\n def token_type_ids_mask_function(\n     token_type_ids: Optional[torch.Tensor],\n     image_group_ids: Optional[torch.Tensor],\n-    tokens_per_image: int,\n ) -> Optional[Callable]:\n     \"\"\"\n     This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n@@ -759,6 +758,57 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n     return inner_mask\n \n \n+def create_causal_mask_mapping(\n+    config: PretrainedConfig,\n+    input_embeds: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    cache_position: torch.Tensor,\n+    past_key_values: Optional[Cache],\n+    position_ids: Optional[torch.Tensor],\n+    token_type_ids: Optional[torch.Tensor] = None,\n+    pixel_values: Optional[torch.FloatTensor] = None,\n+    is_training: bool = False,\n+    **kwargs,\n+) -> dict:\n+    \"\"\"\n+    Overwrites the base `create_masks_for_generate` with `token_type_ids` masking to create the causal mask mapping\n+    for all kinds of forward passes. Gemma3 uses a bidirectional mask for images.\n+\n+    Uses `pixel_values` as an optional input to disambiguate edge cases.\n+    \"\"\"\n+    if is_training and token_type_ids is None:\n+        raise ValueError(\"`token_type_ids` is required as a model input when training\")\n+\n+    mask_kwargs = {\n+        \"config\": config.get_text_config(),\n+        \"input_embeds\": input_embeds,\n+        \"attention_mask\": attention_mask,\n+        \"cache_position\": cache_position,\n+        \"past_key_values\": past_key_values,\n+        \"position_ids\": position_ids,\n+    }\n+    # NOTE: this `may_have_image_input` logic is not flawless, it fails when we're using a cache eagerly initialized\n+    # (e.g. compiled prefill) AND `pixel_values` are not provided (i.e. the image data is provided through other\n+    # means). Determining prefill in that case requires checking data values, which is not compile-compatible.\n+    may_have_image_input = past_key_values is None or not past_key_values.is_initialized or pixel_values is not None\n+    if token_type_ids is not None and may_have_image_input:\n+        # We need to pass an additional mask function to account for token type ids, and it needs to be an `or` (to\n+        # undo the causal masking)\n+\n+        # First find where a new image block starts: 1 if image and previous not image\n+        # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n+        is_image = (token_type_ids == 1).to(cache_position.device)\n+        is_previous_image = nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n+        new_image_start = is_image & ~is_previous_image\n+        image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n+        image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+        mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n+            token_type_ids.to(cache_position.device), image_group_ids\n+        )\n+\n+    return create_masks_for_generate(**mask_kwargs)\n+\n+\n @auto_docstring(\n     custom_intro=\"\"\"\n     The Base Gemma3 model which consists of a vision backbone and a language model without language modeling head.,\n@@ -914,45 +964,17 @@ def forward(\n \n         # It may already have been prepared by e.g. `generate`\n         if not isinstance(causal_mask_mapping := attention_mask, dict):\n-            # Prepare mask arguments\n-            mask_kwargs = {\n-                \"config\": self.config.get_text_config(),\n-                \"input_embeds\": inputs_embeds,\n-                \"attention_mask\": attention_mask,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"position_ids\": position_ids,\n-            }\n-            # NOTE: this `is_prefill` logic is not flawless, it fails when we're using a cache eagerly initialized\n-            # (e.g. compiled prefill) AND `pixel_values` are not provided. Determining prefill in that case requires\n-            # checking data values, which is not compile-compatible.\n-            is_prefill = (\n-                not use_cache\n-                or past_key_values is None\n-                or not past_key_values.is_initialized\n-                or pixel_values is not None\n+            causal_mask_mapping = create_causal_mask_mapping(\n+                self.config,\n+                inputs_embeds,\n+                attention_mask,\n+                cache_position,\n+                past_key_values,\n+                position_ids,\n+                token_type_ids,\n+                pixel_values,\n+                is_training=self.training,\n             )\n-            if token_type_ids is not None and is_prefill:\n-                # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n-\n-                # First find where a new image block starts: 1 if image and previous not image\n-                # The images cannot attend to future images, but can attend to all prev images and to itself\n-                # bidirectionally\n-                is_image = (token_type_ids == 1).to(cache_position.device)\n-                new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n-                image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-                image_group_ids = torch.where(\n-                    is_image, image_group_ids, torch.full_like(token_type_ids, -1, device=is_image.device)\n-                )\n-                mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n-                    token_type_ids.to(cache_position.device), image_group_ids, self.config.mm_tokens_per_image\n-                )\n-\n-            # Create the masks\n-            causal_mask_mapping = {\n-                \"full_attention\": create_causal_mask(**mask_kwargs),\n-                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n-            }\n \n         outputs = self.language_model(\n             attention_mask=causal_mask_mapping,\n@@ -1201,30 +1223,18 @@ def create_masks_for_generate(\n         token_type_ids: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> dict:\n-        # Prepare mask arguments\n-        mask_kwargs = {\n-            \"config\": config.get_text_config(),\n-            \"input_embeds\": input_embeds,\n-            \"attention_mask\": attention_mask,\n-            \"cache_position\": cache_position,\n-            \"past_key_values\": past_key_values,\n-            \"position_ids\": position_ids,\n-        }\n-        # Add the token type ids mask for generate as well\n-        if token_type_ids is not None and input_embeds.shape[1] != 1:\n-            # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n-\n-            # First find where a new image block starts: 1 if image and previous not image\n-            # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n-            is_image = (token_type_ids == 1).to(cache_position.device)\n-            new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n-            image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-            image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n-            mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n-                token_type_ids.to(cache_position.device), image_group_ids, config.mm_tokens_per_image\n-            )\n-\n-        return create_masks_for_generate(**mask_kwargs)\n+        # Uses the overwritten `create_masks_for_generate` with `token_type_ids` masking\n+        return create_causal_mask_mapping(\n+            config,\n+            input_embeds,\n+            attention_mask,\n+            cache_position,\n+            past_key_values,\n+            position_ids,\n+            token_type_ids,\n+            pixel_values=kwargs.get(\"pixel_values\"),\n+            **{k: v for k, v in kwargs.items() if k != \"pixel_values\"},\n+        )\n \n \n class Gemma3ForSequenceClassification(Gemma3PreTrainedModel):"
        },
        {
            "sha": "0f995c5498ac4e9fb9be703cd42e55a6b20dda11",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 61,
            "deletions": 110,
            "changes": 171,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -48,6 +48,7 @@\n     PaliGemmaForConditionalGeneration,\n     PaliGemmaModel,\n     PaligemmaModelOutputWithPast,\n+    token_type_ids_mask_function,\n )\n from ..siglip import SiglipVisionConfig\n \n@@ -63,6 +64,7 @@ class Gemma3TextConfig(Gemma2Config, PretrainedConfig):\n     e.g. [google/gemma3_text-7b](https://huggingface.co/google/gemma3_text-7b)\n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n+\n     Args:\n         vocab_size (`int`, *optional*, defaults to 262208):\n             Vocabulary size of the Gemma3Text model. Defines the number of different tokens that can be represented by the\n@@ -160,8 +162,9 @@ class Gemma3TextConfig(Gemma2Config, PretrainedConfig):\n                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         rope_local_base_freq (float, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings for local attention.\n-        use_bidirectional_attention (`bool`, *optional*, defaults to `False`): If True, the model will attend to all\n-            text tokens instead of using a causal mask. This does not change behavior for vision tokens.\n+        use_bidirectional_attention (`bool`, *optional*, defaults to `False`):\n+            If True, the model will attend to all text tokens instead of using a causal mask. This does not change\n+            behavior for vision tokens.\n \n     ```python\n     >>> from transformers import Gemma3TextModel, Gemma3TextConfig\n@@ -721,37 +724,55 @@ def forward(self, vision_outputs: torch.Tensor):\n         return projected_vision_outputs.type_as(vision_outputs)\n \n \n-def token_type_ids_mask_function(\n-    token_type_ids: Optional[torch.Tensor],\n-    image_group_ids: Optional[torch.Tensor],\n-    tokens_per_image: int,\n-) -> Optional[Callable]:\n-    \"\"\"\n-    This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n-    not start and end indices.\n+def create_causal_mask_mapping(\n+    config: PretrainedConfig,\n+    input_embeds: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    cache_position: torch.Tensor,\n+    past_key_values: Optional[Cache],\n+    position_ids: Optional[torch.Tensor],\n+    token_type_ids: Optional[torch.Tensor] = None,\n+    pixel_values: Optional[torch.FloatTensor] = None,\n+    is_training: bool = False,\n+    **kwargs,\n+) -> dict:\n     \"\"\"\n-    # Do not return an additional mask in this case\n-    if token_type_ids is None:\n-        return None\n-\n-    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n-        # If it's 1 for both query and key/value, we are in an image block\n-        # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length\n-        # Since vmap doesn't support `if statement` we workaround it with `torch.where`\n-        safe_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)\n-        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_idx]\n-        token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)\n+    Overwrites the base `create_masks_for_generate` with `token_type_ids` masking to create the causal mask mapping\n+    for all kinds of forward passes. Gemma3 uses a bidirectional mask for images.\n \n-        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_idx]\n-        image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)\n-\n-        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids_at_kv_idx == 1)\n-        same_image_block = image_group_ids[batch_idx, q_idx] == image_group_ids_at_kv_idx\n-\n-        # This is bidirectional attention whenever we are dealing with image tokens\n-        return is_image_block & same_image_block\n+    Uses `pixel_values` as an optional input to disambiguate edge cases.\n+    \"\"\"\n+    if is_training and token_type_ids is None:\n+        raise ValueError(\"`token_type_ids` is required as a model input when training\")\n+\n+    mask_kwargs = {\n+        \"config\": config.get_text_config(),\n+        \"input_embeds\": input_embeds,\n+        \"attention_mask\": attention_mask,\n+        \"cache_position\": cache_position,\n+        \"past_key_values\": past_key_values,\n+        \"position_ids\": position_ids,\n+    }\n+    # NOTE: this `may_have_image_input` logic is not flawless, it fails when we're using a cache eagerly initialized\n+    # (e.g. compiled prefill) AND `pixel_values` are not provided (i.e. the image data is provided through other\n+    # means). Determining prefill in that case requires checking data values, which is not compile-compatible.\n+    may_have_image_input = past_key_values is None or not past_key_values.is_initialized or pixel_values is not None\n+    if token_type_ids is not None and may_have_image_input:\n+        # We need to pass an additional mask function to account for token type ids, and it needs to be an `or` (to\n+        # undo the causal masking)\n+\n+        # First find where a new image block starts: 1 if image and previous not image\n+        # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n+        is_image = (token_type_ids == 1).to(cache_position.device)\n+        is_previous_image = nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n+        new_image_start = is_image & ~is_previous_image\n+        image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n+        image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+        mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n+            token_type_ids.to(cache_position.device), image_group_ids\n+        )\n \n-    return inner_mask\n+    return create_masks_for_generate(**mask_kwargs)\n \n \n class Gemma3Model(PaliGemmaModel):\n@@ -776,9 +797,6 @@ def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         image_features = self.multi_modal_projector(vision_outputs)\n         return image_features\n \n-    def _update_causal_mask(self, **super_kwargs):\n-        raise AttributeError(\"We don't want to inherit it\")\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -835,45 +853,17 @@ def forward(\n \n         # It may already have been prepared by e.g. `generate`\n         if not isinstance(causal_mask_mapping := attention_mask, dict):\n-            # Prepare mask arguments\n-            mask_kwargs = {\n-                \"config\": self.config.get_text_config(),\n-                \"input_embeds\": inputs_embeds,\n-                \"attention_mask\": attention_mask,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"position_ids\": position_ids,\n-            }\n-            # NOTE: this `is_prefill` logic is not flawless, it fails when we're using a cache eagerly initialized\n-            # (e.g. compiled prefill) AND `pixel_values` are not provided. Determining prefill in that case requires\n-            # checking data values, which is not compile-compatible.\n-            is_prefill = (\n-                not use_cache\n-                or past_key_values is None\n-                or not past_key_values.is_initialized\n-                or pixel_values is not None\n+            causal_mask_mapping = create_causal_mask_mapping(\n+                self.config,\n+                inputs_embeds,\n+                attention_mask,\n+                cache_position,\n+                past_key_values,\n+                position_ids,\n+                token_type_ids,\n+                pixel_values,\n+                is_training=self.training,\n             )\n-            if token_type_ids is not None and is_prefill:\n-                # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n-\n-                # First find where a new image block starts: 1 if image and previous not image\n-                # The images cannot attend to future images, but can attend to all prev images and to itself\n-                # bidirectionally\n-                is_image = (token_type_ids == 1).to(cache_position.device)\n-                new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n-                image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-                image_group_ids = torch.where(\n-                    is_image, image_group_ids, torch.full_like(token_type_ids, -1, device=is_image.device)\n-                )\n-                mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n-                    token_type_ids.to(cache_position.device), image_group_ids, self.config.mm_tokens_per_image\n-                )\n-\n-            # Create the masks\n-            causal_mask_mapping = {\n-                \"full_attention\": create_causal_mask(**mask_kwargs),\n-                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n-            }\n \n         outputs = self.language_model(\n             attention_mask=causal_mask_mapping,\n@@ -1065,45 +1055,6 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    def _prepare_4d_causal_attention_mask_with_cache_position(self, **super_kwargs):\n-        raise AttributeError(\"We don't want to inherit it\")\n-\n-    @staticmethod\n-    def create_masks_for_generate(\n-        config: PretrainedConfig,\n-        input_embeds: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor],\n-        cache_position: torch.Tensor,\n-        past_key_values: Optional[Cache],\n-        position_ids: Optional[torch.Tensor],\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        **kwargs,\n-    ) -> dict:\n-        # Prepare mask arguments\n-        mask_kwargs = {\n-            \"config\": config.get_text_config(),\n-            \"input_embeds\": input_embeds,\n-            \"attention_mask\": attention_mask,\n-            \"cache_position\": cache_position,\n-            \"past_key_values\": past_key_values,\n-            \"position_ids\": position_ids,\n-        }\n-        # Add the token type ids mask for generate as well\n-        if token_type_ids is not None and input_embeds.shape[1] != 1:\n-            # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n-\n-            # First find where a new image block starts: 1 if image and previous not image\n-            # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n-            is_image = (token_type_ids == 1).to(cache_position.device)\n-            new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n-            image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-            image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n-            mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n-                token_type_ids.to(cache_position.device), image_group_ids, config.mm_tokens_per_image\n-            )\n-\n-        return create_masks_for_generate(**mask_kwargs)\n-\n \n class Gemma3ForSequenceClassification(Gemma3PreTrainedModel):\n     _checkpoint_conversion_mapping = {"
        },
        {
            "sha": "580afd43de1f3dcac3d9c6a0143f07a583575652",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -2472,9 +2472,6 @@ def get_audio_features(\n         audio_outputs, audio_mask = self.audio_tower(input_features, input_features_mask)\n         return self.embed_audio(inputs_embeds=audio_outputs), audio_mask\n \n-    def _update_causal_mask(self, **super_kwargs):\n-        raise AttributeError(\"We don't want to inherit it\")\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -2668,8 +2665,8 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    def _prepare_4d_causal_attention_mask_with_cache_position(self, **super_kwargs):\n-        raise AttributeError(\"Do not inherit _prepare_4d_causal_attention_mask_with_cache_position from PaliGemma\")\n+    def create_masks_for_generate(self, **super_kwargs):\n+        raise AttributeError(\"Do not inherit create_masks_for_generate from PaliGemma\")\n \n \n __all__ = ["
        },
        {
            "sha": "bee324fbb729f406d650c63a45126df7d9bbd019",
            "filename": "src/transformers/models/helium/configuration_helium.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -25,6 +25,7 @@ class HeliumConfig(PretrainedConfig):\n     e.g. [kyutai/helium-2b](https://huggingface.co/kyutai/helium-2b)\n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n+\n     Args:\n         vocab_size (`int`, *optional*, defaults to 48000):\n             Vocabulary size of the Helium model. Defines the number of different tokens that can be represented by the\n@@ -74,6 +75,7 @@ class HeliumConfig(PretrainedConfig):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         mlp_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n+\n     ```python\n     >>> from transformers import HeliumModel, HeliumConfig\n     >>> # Initializing a Helium 2b style configuration"
        },
        {
            "sha": "941543c2c9da1a5965a4439755c41e408fb9a37b",
            "filename": "src/transformers/models/paligemma/configuration_paligemma.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -120,6 +120,12 @@ def __init__(\n                 is_encoder_decoder=False,\n                 vocab_size=vocab_size,\n             )\n+\n+        # BC: `use_bidirectional_attention` was originally unset in PaliGemma1 (backbone = Gemma1) AND PaliGemma2\n+        # (backbone = Gemma2). Both PaliGemmas want to default to True.\n+        if self.text_config.use_bidirectional_attention is None:\n+            self.text_config.use_bidirectional_attention = True\n+\n         self.text_config.num_image_tokens = (self.vision_config.image_size // self.vision_config.patch_size) ** 2\n         self.vision_config.projection_dim = projection_dim\n         super().__init__(**kwargs)"
        },
        {
            "sha": "7e5d9f8332e3c644e783572473e4e4cae19e73c3",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 143,
            "deletions": 137,
            "changes": 280,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -15,13 +15,15 @@\n \"\"\"PyTorch PaliGemmamodel.\"\"\"\n \n from dataclasses import dataclass\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n \n-from ...cache_utils import Cache, StaticCache\n+from ...cache_utils import Cache\n+from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationMixin\n+from ...masking_utils import create_masks_for_generate\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n@@ -97,6 +99,109 @@ def forward(self, image_features):\n         return hidden_states\n \n \n+def token_type_ids_mask_function(\n+    token_type_ids: Optional[torch.Tensor],\n+    image_group_ids: Optional[torch.Tensor],\n+) -> Optional[Callable]:\n+    \"\"\"\n+    This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n+    not start and end indices.\n+    \"\"\"\n+    # Do not return an additional mask in this case\n+    if token_type_ids is None:\n+        return None\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        # If it's 1 for both query and key/value, we are in an image block\n+        # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length\n+        # Since vmap doesn't support `if statement` we workaround it with `torch.where`\n+        safe_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)\n+        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_idx]\n+        token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)\n+\n+        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_idx]\n+        image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)\n+\n+        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids_at_kv_idx == 1)\n+        same_image_block = image_group_ids[batch_idx, q_idx] == image_group_ids_at_kv_idx\n+\n+        # This is bidirectional attention whenever we are dealing with image tokens\n+        return is_image_block & same_image_block\n+\n+    return inner_mask\n+\n+\n+def create_causal_mask_mapping(\n+    config: PretrainedConfig,\n+    input_embeds: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    cache_position: torch.Tensor,\n+    past_key_values: Optional[Cache],\n+    position_ids: Optional[torch.Tensor],\n+    token_type_ids: Optional[torch.Tensor] = None,\n+    pixel_values: Optional[torch.FloatTensor] = None,\n+    is_training: bool = False,\n+    **kwargs,\n+) -> dict:\n+    \"\"\"\n+    Overwrites the base `create_masks_for_generate` with `token_type_ids` masking to create the causal mask mapping\n+    for all kinds of forward passes. Paligemma uses a bidirectional mask on the prompt tokens.\n+\n+    Uses `pixel_values` as an optional input to disambiguate edge cases.\n+    \"\"\"\n+    if is_training and token_type_ids is None:\n+        raise ValueError(\"`token_type_ids` is required as a model input when training\")\n+\n+    mask_kwargs = {\n+        \"config\": config.get_text_config(),\n+        \"input_embeds\": input_embeds,\n+        \"attention_mask\": attention_mask,\n+        \"cache_position\": cache_position,\n+        \"past_key_values\": past_key_values,\n+        \"position_ids\": position_ids,\n+    }\n+    # NOTE: this `is_prompt` logic is not flawless, it fails when we're using a cache eagerly initialized\n+    # (e.g. compiled prefill) AND `pixel_values` are not provided (i.e. the image data is provided through other\n+    # means). Determining prefill in that case requires checking data values, which is not compile-compatible.\n+    maybe_is_prompt = past_key_values is None or not past_key_values.is_initialized or pixel_values is not None\n+\n+    if maybe_is_prompt:\n+        if token_type_ids is not None:\n+            # The logic bellow was originally written for Gemma3, where `token_type_ids` is reversed. Let's reverse\n+            # it to then use exactly the same logic.\n+            token_type_ids = 1 - token_type_ids\n+        else:\n+            logger.warning_once(\n+                \"The input may be the prompt, but `token_type_ids` is not provided. We recommend \"\n+                \"passing `token_type_ids` to the model to prevent bad attention masking.\"\n+            )\n+            # BC: when NOT training, use bidirectional mask if sequence length > 1. Otherwise, use the default causal\n+            # mask. This is incorrect in some advanced use cases, hence the warning above.\n+            # NOTE: this branch can't be reached when training because `token_type_ids` is required as a model input.\n+            if input_embeds.shape[1] > 1:\n+                token_type_ids = torch.ones_like(input_embeds)[:, :, 0]\n+\n+    # Logic originally copied from Gemma3. It holds up for Paligemma as well because Paligemma assumes up to one image\n+    # per prompt AND we reverse `token_type_ids` above. Gemma3 uses a bidirectional mask for images, tagged through\n+    # `token_type_ids` 1s.\n+    if token_type_ids is not None and maybe_is_prompt:\n+        # We need to pass an additional mask function to account for token type ids, and it needs to be an `or` (to\n+        # undo the causal masking)\n+\n+        # First find where a new image block starts: 1 if image and previous not image\n+        # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n+        is_image = (token_type_ids == 1).to(cache_position.device)\n+        is_previous_image = nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n+        new_image_start = is_image & ~is_previous_image\n+        image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n+        image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+        mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n+            token_type_ids.to(cache_position.device), image_group_ids\n+        )\n+\n+    return create_masks_for_generate(**mask_kwargs)\n+\n+\n @auto_docstring\n class PaliGemmaPreTrainedModel(PreTrainedModel):\n     config: PaliGemmaConfig\n@@ -159,75 +264,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask,\n-        token_type_ids=None,\n-        past_key_values=None,\n-        cache_position=None,\n-        input_tensor=None,\n-        is_training: Optional[bool] = None,\n-    ):\n-        if self.config.text_config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        is_training = is_training if is_training is not None else self.training\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        min_dtype = torch.finfo(self.text_config_dtype).min\n-        if input_tensor is None:\n-            input_tensor = attention_mask\n-\n-        inputs_lead_dim, sequence_length = input_tensor.shape[:2]\n-        if using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else cache_position[0] + sequence_length + 1\n-            )\n-\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            return attention_mask\n-\n-        causal_mask = torch.full(\n-            (sequence_length, target_length),\n-            fill_value=min_dtype,\n-            dtype=self.text_config_dtype,\n-            device=cache_position.device,\n-        )\n-        # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n-        if sequence_length != 1:\n-            if is_training:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            else:\n-                causal_mask[:, :sequence_length] = 0.0\n-\n-        causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(inputs_lead_dim, 1, -1, -1)\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            mask_length = attention_mask.shape[-1]\n-\n-            # First unmask prefix tokens during training\n-            if is_training:\n-                if token_type_ids is None:\n-                    raise ValueError(\"Token type ids must be provided during training\")\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    token_type_ids[:, None, None, :].to(causal_mask.device) == 0, 0\n-                )\n-\n-            # Then apply padding mask (will mask pad tokens)\n-            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(causal_mask.device)\n-            padding_mask = padding_mask == 0\n-            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                padding_mask, min_dtype\n-            )\n-\n-        return causal_mask\n-\n     def get_image_features(self, pixel_values: torch.FloatTensor):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n@@ -324,8 +360,6 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        is_training = token_type_ids is not None and labels is not None\n-\n         # Replace image id with PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n@@ -355,11 +389,22 @@ def forward(\n             )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            causal_mask_mapping = create_causal_mask_mapping(\n+                self.config,\n+                inputs_embeds,\n+                attention_mask,\n+                cache_position,\n+                past_key_values,\n+                position_ids,\n+                token_type_ids,\n+                pixel_values,\n+                is_training=self.training,\n+            )\n+\n         outputs = self.language_model(\n-            attention_mask=causal_mask,\n+            attention_mask=causal_mask_mapping,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -550,76 +595,37 @@ def prepare_inputs_for_generation(\n         # position_ids in Paligemma are 1-indexed\n         if model_inputs.get(\"position_ids\") is not None:\n             model_inputs[\"position_ids\"] += 1\n+\n         # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n         # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always\n         if cache_position[0] == 0:\n             model_inputs[\"pixel_values\"] = pixel_values\n-        is_training = token_type_ids is not None and labels is not None\n-        is_static_hybrid_cache = isinstance(past_key_values, StaticCache) and any(past_key_values.is_sliding)\n-        if cache_position[0] == 0 and is_static_hybrid_cache:\n-            input_tensor = inputs_embeds if inputs_embeds is not None else input_ids\n-            causal_mask = self.model._update_causal_mask(\n-                attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training\n-            )\n-            model_inputs[\"attention_mask\"] = causal_mask\n \n         return model_inputs\n \n     @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n+    def create_masks_for_generate(\n+        config: PretrainedConfig,\n+        input_embeds: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor],\n         cache_position: torch.Tensor,\n-        batch_size: int,\n+        past_key_values: Optional[Cache],\n+        position_ids: Optional[torch.Tensor],\n+        token_type_ids: Optional[torch.Tensor] = None,\n         **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n+    ) -> dict:\n+        # Uses the overwritten `create_masks_for_generate` with `token_type_ids` masking\n+        return create_causal_mask_mapping(\n+            config,\n+            input_embeds,\n+            attention_mask,\n+            cache_position,\n+            past_key_values,\n+            position_ids,\n+            token_type_ids,\n+            pixel_values=kwargs.get(\"pixel_values\"),\n+            **{k: v for k, v in kwargs.items() if k != \"pixel_values\"},\n+        )\n \n \n __all__ = [\"PaliGemmaForConditionalGeneration\", \"PaliGemmaPreTrainedModel\", \"PaliGemmaModel\"]"
        },
        {
            "sha": "c4c618a4d9580615f972c40199c033e018a4786e",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -217,7 +217,7 @@ def __call__(\n         )\n         suffix = output_kwargs[\"text_kwargs\"].pop(\"suffix\", None)\n \n-        return_token_type_ids = suffix is not None\n+        return_token_type_ids = True\n \n         if images is None:\n             raise ValueError(\"`images` are expected as arguments to a `PaliGemmaProcessor` instance.\")\n@@ -299,6 +299,7 @@ def __call__(\n \n         return_data = {**inputs, \"pixel_values\": pixel_values}\n \n+        # TODO: ideally we would control label generation separately, now that we always return token_type_ids.\n         if return_token_type_ids:\n             labels = np.array(inputs[\"input_ids\"])\n             labels[np.array(inputs[\"token_type_ids\"]) == 0] = -100\n@@ -330,5 +331,11 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n             vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n         return MultiModalData(**vision_data)\n \n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names + [\"token_type_ids\", \"labels\"]\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(tokenizer_input_names + image_processor_input_names)\n+\n \n __all__ = [\"PaliGemmaProcessor\"]"
        },
        {
            "sha": "76ad99132056da2f6a243b1902c4cb8096bb002c",
            "filename": "src/transformers/models/t5gemma/configuration_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -32,6 +32,7 @@ class T5GemmaModuleConfig(PretrainedConfig):\n     e.g. [google/t5_gemma_module-7b](https://huggingface.co/google/t5_gemma_module-7b)\n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n+\n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the T5GemmaModule model. Defines the number of different tokens that can be represented by the"
        },
        {
            "sha": "336e67ce42b6b433e79fc6a701d39fc5d035d4f3",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 5,
            "deletions": 76,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -505,81 +505,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return logits\n \n \n-class T5GemmaAttention(nn.Module):\n-    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n-\n-    def __init__(self, config: T5GemmaConfig, layer_idx: int):\n-        super().__init__()\n-        self.config = config\n-        self.layer_idx = layer_idx\n-        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n-        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n-        self.scaling = config.query_pre_attn_scalar**-0.5\n-        self.attention_dropout = self.config.attention_dropout\n-        self.is_causal = True\n-\n-        self.q_proj = nn.Linear(\n-            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n-        )\n-        self.k_proj = nn.Linear(\n-            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n-        )\n-        self.v_proj = nn.Linear(\n-            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n-        )\n-        self.o_proj = nn.Linear(\n-            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n-        )\n-        self.attn_logit_softcapping = self.config.attn_logit_softcapping\n-        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n-\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        input_shape = hidden_states.shape[:-1]\n-        hidden_shape = (*input_shape, -1, self.head_dim)\n-\n-        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n-        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n-        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_values is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        attention_interface: Callable = eager_attention_forward\n-        if self.config._attn_implementation != \"eager\":\n-            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n-\n-        attn_output, attn_weights = attention_interface(\n-            self,\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            dropout=self.attention_dropout if self.training else 0.0,\n-            scaling=self.scaling,\n-            sliding_window=self.sliding_window,\n-            softcap=self.attn_logit_softcapping,\n-            **kwargs,\n-        )\n-\n-        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-        return attn_output, attn_weights\n-\n-\n @auto_docstring\n class T5GemmaPreTrainedModel(PreTrainedModel):\n     config: T5GemmaConfig\n@@ -595,7 +520,11 @@ class T5GemmaPreTrainedModel(PreTrainedModel):\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": T5GemmaDecoderLayer,\n-        \"attentions\": T5GemmaAttention,\n+        \"attentions\": [\n+            OutputRecorder(T5GemmaSelfAttention, index=1, layer_name=\"self_attn\"),\n+            OutputRecorder(T5GemmaSelfAttention, index=1, layer_name=\"cross_attn\"),\n+            OutputRecorder(T5GemmaCrossAttention, index=1, layer_name=\"cross_attn\"),\n+        ],\n     }\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "dafa2217d0624b169223908b8a6f8c4cfd64cd0a",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 142,
            "deletions": 1,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -61,7 +61,140 @@\n \n \n class T5GemmaModuleConfig(Gemma2Config):\n-    pass\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`T5GemmaModuleModel`]. It is used to instantiate an T5GemmaModule\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the T5GemmaModule-7B.\n+    e.g. [google/t5_gemma_module-7b](https://huggingface.co/google/t5_gemma_module-7b)\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 256000):\n+            Vocabulary size of the T5GemmaModule model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`T5GemmaModuleModel`]\n+        hidden_size (`int`, *optional*, defaults to 2304):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 9216):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 26):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        head_dim (`int`, *optional*, defaults to 256):\n+            The attention head dimension.\n+        hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the decoder. Will default to `\"gelu_pytorch_tanh\"`\n+            if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"` activation function.\n+        max_position_embeddings (`int`, *optional*, defaults to 8192):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        eos_token_id (`int`, *optional*, defaults to 1):\n+            End of stream token id.\n+        bos_token_id (`int`, *optional*, defaults to 2):\n+            Beginning of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        query_pre_attn_scalar (`float`, *optional*, defaults to 256):\n+            scaling factor used on the attention scores\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            in T5GemmaModule, every other layer uses sliding window attention. This is the size of the sliding window.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+        final_logit_softcapping (`float`, *optional*, defaults to 30.0):\n+            scaling factor when applying tanh softcapping on the logits.\n+        attn_logit_softcapping (`float`, *optional*, defaults to 50.0):\n+            scaling factor when applying tanh softcapping on the attention scores.\n+\n+    ```python\n+    >>> from transformers import T5GemmaModuleModel, T5GemmaModuleConfig\n+    >>> # Initializing a T5GemmaModule t5_gemma_module-7b style configuration\n+    >>> configuration = T5GemmaModuleConfig()\n+    >>> # Initializing a model from the t5_gemma_module-7b style configuration\n+    >>> model = T5GemmaModuleModel(configuration)\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    def __init__(\n+        self,\n+        vocab_size=256000,\n+        hidden_size=2304,\n+        intermediate_size=9216,\n+        num_hidden_layers=26,\n+        num_attention_heads=8,\n+        num_key_value_heads=4,\n+        head_dim=256,\n+        hidden_activation=\"gelu_pytorch_tanh\",\n+        max_position_embeddings=8192,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=0,\n+        eos_token_id=1,\n+        bos_token_id=2,\n+        tie_word_embeddings=True,\n+        rope_theta=10000.0,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        query_pre_attn_scalar=256,\n+        sliding_window=4096,\n+        layer_types=None,\n+        final_logit_softcapping=30.0,\n+        attn_logit_softcapping=50.0,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            vocab_size=vocab_size,\n+            hidden_size=hidden_size,\n+            intermediate_size=intermediate_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            num_key_value_heads=num_key_value_heads,\n+            head_dim=head_dim,\n+            hidden_activation=hidden_activation,\n+            max_position_embeddings=max_position_embeddings,\n+            initializer_range=initializer_range,\n+            rms_norm_eps=rms_norm_eps,\n+            use_cache=use_cache,\n+            pad_token_id=pad_token_id,\n+            eos_token_id=eos_token_id,\n+            bos_token_id=bos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            rope_theta=rope_theta,\n+            attention_bias=attention_bias,\n+            attention_dropout=attention_dropout,\n+            query_pre_attn_scalar=query_pre_attn_scalar,\n+            sliding_window=sliding_window,\n+            layer_types=layer_types,\n+            final_logit_softcapping=final_logit_softcapping,\n+            attn_logit_softcapping=attn_logit_softcapping,\n+            **kwargs,\n+        )\n+\n+        del self.use_bidirectional_attention\n \n \n class T5GemmaConfig(PretrainedConfig):\n@@ -477,6 +610,14 @@ class T5GemmaPreTrainedModel(Gemma2PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"T5GemmaEncoderLayer\", \"T5GemmaDecoderLayer\"]\n+    _can_record_outputs = {\n+        \"hidden_states\": T5GemmaDecoderLayer,\n+        \"attentions\": [\n+            OutputRecorder(T5GemmaSelfAttention, index=1, layer_name=\"self_attn\"),\n+            OutputRecorder(T5GemmaSelfAttention, index=1, layer_name=\"cross_attn\"),\n+            OutputRecorder(T5GemmaCrossAttention, index=1, layer_name=\"cross_attn\"),\n+        ],\n+    }\n \n     def _init_weights(self, module):\n         # TODO: support initialization for encoders and decoders separately(?)"
        },
        {
            "sha": "488ce47e896d9a4deccaeb7dc29ba5c7635233ae",
            "filename": "src/transformers/models/vaultgemma/configuration_vaultgemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -30,6 +30,7 @@ class VaultGemmaConfig(PretrainedConfig):\n     e.g. [google/vaultgemma-7b](https://huggingface.co/google/vaultgemma-7b)\n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n+\n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the VaultGemma model. Defines the number of different tokens that can be represented by the"
        },
        {
            "sha": "5eb641a55563de36f040281319c192ae0e0bdb0c",
            "filename": "src/transformers/models/vaultgemma/modular_vaultgemma.py",
            "status": "modified",
            "additions": 150,
            "deletions": 1,
            "changes": 151,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -19,13 +19,162 @@\n \n from ...cache_utils import Cache\n from ..gemma2.configuration_gemma2 import Gemma2Config\n-from ..gemma2.modeling_gemma2 import Gemma2DecoderLayer, Gemma2ForCausalLM\n+from ..gemma2.modeling_gemma2 import Gemma2Attention, Gemma2DecoderLayer, Gemma2ForCausalLM, Gemma2MLP, Gemma2RMSNorm\n \n \n class VaultGemmaConfig(Gemma2Config):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`VaultGemmaModel`]. It is used to instantiate an VaultGemma\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the VaultGemma-7B.\n+    e.g. [google/vaultgemma-7b](https://huggingface.co/google/vaultgemma-7b)\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 256000):\n+            Vocabulary size of the VaultGemma model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`VaultGemmaModel`]\n+        hidden_size (`int`, *optional*, defaults to 2304):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 9216):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 26):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        head_dim (`int`, *optional*, defaults to 256):\n+            The attention head dimension.\n+        hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the decoder. Will default to `\"gelu_pytorch_tanh\"`\n+            if not specified. `\"gelu_pytorch_tanh\"` uses an approximation of the `\"gelu\"` activation function.\n+        max_position_embeddings (`int`, *optional*, defaults to 8192):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        eos_token_id (`int`, *optional*, defaults to 1):\n+            End of stream token id.\n+        bos_token_id (`int`, *optional*, defaults to 2):\n+            Beginning of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        query_pre_attn_scalar (`float`, *optional*, defaults to 256):\n+            scaling factor used on the attention scores\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            in VaultGemma, every other layer uses sliding window attention. This is the size of the sliding window.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+        final_logit_softcapping (`float`, *optional*, defaults to 30.0):\n+            scaling factor when applying tanh softcapping on the logits.\n+        attn_logit_softcapping (`float`, *optional*, defaults to 50.0):\n+            scaling factor when applying tanh softcapping on the attention scores.\n+\n+    ```python\n+    >>> from transformers import VaultGemmaModel, VaultGemmaConfig\n+    >>> # Initializing a VaultGemma vaultgemma-7b style configuration\n+    >>> configuration = VaultGemmaConfig()\n+    >>> # Initializing a model from the vaultgemma-7b style configuration\n+    >>> model = VaultGemmaModel(configuration)\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    def __init__(\n+        self,\n+        vocab_size=256000,\n+        hidden_size=2304,\n+        intermediate_size=9216,\n+        num_hidden_layers=26,\n+        num_attention_heads=8,\n+        num_key_value_heads=4,\n+        head_dim=256,\n+        hidden_activation=\"gelu_pytorch_tanh\",\n+        max_position_embeddings=8192,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=0,\n+        eos_token_id=1,\n+        bos_token_id=2,\n+        tie_word_embeddings=True,\n+        rope_theta=10000.0,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        query_pre_attn_scalar=256,\n+        sliding_window=4096,\n+        layer_types=None,\n+        final_logit_softcapping=30.0,\n+        attn_logit_softcapping=50.0,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            vocab_size=vocab_size,\n+            hidden_size=hidden_size,\n+            intermediate_size=intermediate_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            num_key_value_heads=num_key_value_heads,\n+            head_dim=head_dim,\n+            hidden_activation=hidden_activation,\n+            max_position_embeddings=max_position_embeddings,\n+            initializer_range=initializer_range,\n+            rms_norm_eps=rms_norm_eps,\n+            use_cache=use_cache,\n+            pad_token_id=pad_token_id,\n+            eos_token_id=eos_token_id,\n+            bos_token_id=bos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            rope_theta=rope_theta,\n+            attention_bias=attention_bias,\n+            attention_dropout=attention_dropout,\n+            query_pre_attn_scalar=query_pre_attn_scalar,\n+            sliding_window=sliding_window,\n+            layer_types=layer_types,\n+            final_logit_softcapping=final_logit_softcapping,\n+            attn_logit_softcapping=attn_logit_softcapping,\n+            **kwargs,\n+        )\n+\n+        del self.use_bidirectional_attention\n+\n+\n+class VaultGemmaRMSNorm(Gemma2RMSNorm):\n+    pass\n+\n+\n+class VaultGemmaMLP(Gemma2MLP):\n     pass\n \n \n+class VaultGemmaAttention(Gemma2Attention):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: VaultGemmaConfig, layer_idx: int):\n+        super().__init__()\n+        self.is_causal = True\n+\n+\n class VaultGemmaDecoderLayer(Gemma2DecoderLayer):\n     def __init__(self, **super_kwargs):\n         super().__init__(**super_kwargs)"
        },
        {
            "sha": "ed58403a53d0a74407661b57138b5e215d8636e5",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -744,8 +744,15 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n         for model_class in self.all_generative_model_classes:\n             if model_class._is_stateful:\n                 self.skipTest(reason=\"Stateful models don't support assisted generation\")\n-            if any(model_name in model_class.__name__.lower() for model_name in [\"reformer\"]):\n-                self.skipTest(reason=\"Won't fix: old model with different cache format\")\n+            old_models = [  # models that we won't commit resources fixing because they are old and have little usage\n+                # reformer: has a different cache format\n+                \"reformer\",\n+                # imagegpt: the output lm head uses `vocab_size - 1` tokens, so the `NoBadWordsLogitsProcessor` used\n+                # by prompt lookup may fail\n+                \"imagegpt\",\n+            ]\n+            if any(model_name in model_class.__name__.lower() for model_name in old_models):\n+                self.skipTest(reason=\"Won't fix: old model\")\n             if any(\n                 model_name in model_class.__name__.lower()\n                 for model_name in ["
        },
        {
            "sha": "026dae1e8697fba00a33d0745e045d00ee5d471e",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -884,6 +884,7 @@ def test_resize_embeddings_untied(self):\n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n             model = model_class(config).to(torch_device)\n+            model.eval()\n \n             # if no output embeddings -> leave test\n             if model.get_output_embeddings() is None:"
        },
        {
            "sha": "2f4c849a1e35da66297988ccbb65cc5eca963850",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -338,6 +338,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n+            model.eval()\n             curr_input_dict = copy.deepcopy(input_dict)  # the below tests modify dict in-place\n             _ = model(**curr_input_dict)  # successful forward with no modifications\n "
        },
        {
            "sha": "f00566ccfc1e459c28ac43100fe667cc034d1193",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -173,6 +173,7 @@ def prepare_config_and_inputs_for_common(self):\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n             \"labels\": input_ids,\n+            \"token_type_ids\": torch.zeros_like(input_ids),\n         }\n         return config, inputs_dict\n \n@@ -189,6 +190,7 @@ class ColPaliForRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = True\n     test_head_masking = False\n+    additional_model_inputs = [\"token_type_ids\"]\n \n     def setUp(self):\n         self.model_tester = ColPaliForRetrievalModelTester(self)"
        },
        {
            "sha": "ed63fd2410c9e474bf9f2f225639f54d03facee2",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -282,6 +282,7 @@ class Gemma3Vision2TextModelTest(ModelTesterMixin, GenerationTesterMixin, unitte\n     test_missing_keys = False\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n+    additional_model_inputs = [\"token_type_ids\"]\n \n     # MP works but offload doesn't work when the SigLIP MultiheadAttention is offloaded\n     # TODO: One potential solution would be to add to set preload_module_classes = [\"SiglipMultiheadAttentionPoolingHead\"]"
        },
        {
            "sha": "67a9734fd8668b75c0eff17ef91ef0b480aa7f95",
            "filename": "tests/models/helium/test_modeling_helium.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -24,8 +24,7 @@\n     torch_device,\n )\n \n-from ...test_configuration_common import ConfigTester\n-from ..gemma.test_modeling_gemma import GemmaModelTest, GemmaModelTester\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n@@ -39,17 +38,17 @@\n     )\n \n \n-class HeliumModelTester(GemmaModelTester):\n+class HeliumModelTester(CausalLMModelTester):\n     if is_torch_available():\n         config_class = HeliumConfig\n-        model_class = HeliumModel\n-        for_causal_lm_class = HeliumForCausalLM\n-        for_sequence_class = HeliumForSequenceClassification\n-        for_token_class = HeliumForTokenClassification\n+        base_model_class = HeliumModel\n+        causal_lm_class = HeliumForCausalLM\n+        sequence_classification_class = HeliumForSequenceClassification\n+        token_classification_class = HeliumForTokenClassification\n \n \n @require_torch\n-class HeliumModelTest(GemmaModelTest, unittest.TestCase):\n+class HeliumModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (HeliumModel, HeliumForCausalLM, HeliumForSequenceClassification, HeliumForTokenClassification)\n         if is_torch_available()\n@@ -66,15 +65,12 @@ class HeliumModelTest(GemmaModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n+    model_tester_class = HeliumModelTester\n     test_headmasking = False\n     test_pruning = False\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n \n-    def setUp(self):\n-        self.model_tester = HeliumModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=HeliumConfig, hidden_size=37)\n-\n \n @slow\n # @require_torch_gpu"
        },
        {
            "sha": "6c1f1686515ca5b7dbb554784dc54787f4c2e91e",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -297,6 +297,7 @@ def test_resize_embeddings_untied(self):\n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n             model = model_class(config).to(torch_device)\n+            model.eval()\n \n             # if no output embeddings -> leave test\n             if model.get_output_embeddings() is None:\n@@ -480,6 +481,7 @@ def test_resize_embeddings_untied(self):\n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n             model = model_class(config).to(torch_device)\n+            model.eval()\n \n             # Check that resizing the token embeddings with a larger vocab size increases the model's vocab size\n             model_vocab_size = config.text_config.vocab_size"
        },
        {
            "sha": "73417318658b7ef4ffaa5bbfcdd81997779a98f4",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -287,6 +287,7 @@ def test_resize_embeddings_untied(self):\n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n             model = model_class(config).to(torch_device)\n+            model.eval()\n \n             # if no output embeddings -> leave test\n             if model.get_output_embeddings() is None:\n@@ -446,6 +447,7 @@ def test_resize_embeddings_untied(self):\n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n             model = model_class(config).to(torch_device)\n+            model.eval()\n \n             # Check that resizing the token embeddings with a larger vocab size increases the model's vocab size\n             model_vocab_size = config.text_config.vocab_size"
        },
        {
            "sha": "d1e599fa4e0032fe9cf5140e0045b73fe52c0b50",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -206,6 +206,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n+            model.eval()\n             curr_input_dict = copy.deepcopy(input_dict)  # in=place modifications further\n             _ = model(**curr_input_dict)  # successful forward with no modifications\n "
        },
        {
            "sha": "a476d34ffc398adb59aee5a7c5ea9df4faeaea71",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -231,6 +231,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n+            model.eval()\n             curr_input_dict = copy.deepcopy(input_dict)  # in=place modifications further\n             _ = model(**curr_input_dict)  # successful forward with no modifications\n "
        },
        {
            "sha": "332fdfa59e75548896dd3411d96041ff129777ea",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -244,6 +244,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n+            model.eval()\n             curr_input_dict = copy.deepcopy(input_dict)  # in=place modifications further\n             _ = model(**curr_input_dict)  # successful forward with no modifications\n "
        },
        {
            "sha": "1924be5b0713133c7a4f7ec74f1d6615434dc82f",
            "filename": "tests/models/moonshine/test_modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -398,6 +398,7 @@ def test_resize_embeddings_untied(self):\n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n             model = model_class(config).to(torch_device)\n+            model.eval()\n \n             # if no output embeddings -> leave test\n             if model.get_output_embeddings() is None:"
        },
        {
            "sha": "6a02a3f31e0e456eb3e09638aa830701f3a1ebda",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -189,6 +189,7 @@ class PaliGemmaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n         else ()\n     )\n     pipeline_model_mapping = {\"image-text-to-text\": PaliGemmaForConditionalGeneration}\n+    additional_model_inputs = [\"token_type_ids\"]\n     fx_compatible = False\n     test_pruning = False\n     test_torchscript = False\n@@ -209,6 +210,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n+            model.eval()\n             curr_input_dict = copy.deepcopy(input_dict)  # in=place modifications further\n             _ = model(**curr_input_dict)  # successful forward with no modifications\n \n@@ -555,7 +557,7 @@ def test_integration_detection_bug(self):\n             {\n                 (\"rocm\", (9, 5)): \"detect shoe\\n<loc0051><loc0309><loc0708><loc0644> shoe\",\n                 (None, None): \"detect shoe\\n<loc0051><loc0309><loc0708><loc0646> shoe\",\n-                (\"cuda\", 8): \"detect shoe\\n<loc0045><loc0309><loc0708><loc0646> shoe\",\n+                (\"cuda\", 8): \"detect shoe\\n<loc0051><loc0309><loc0708><loc0646> shoe\",\n             }\n         )  # fmt: skip\n         EXPECTED_DECODED_TEXT = expected_decoded_texts.get_expectation()"
        },
        {
            "sha": "ffb61c2146b2793174077e8b0cd3fb3db144d0c0",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -16,9 +16,6 @@\n import copy\n import unittest\n \n-import pytest\n-from parameterized import parameterized\n-\n from transformers import (\n     PaliGemmaConfig,\n     PaliGemmaForConditionalGeneration,\n@@ -192,6 +189,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n+            model.eval()\n             curr_input_dict = copy.deepcopy(input_dict)  # in=place modifications further\n             _ = model(**curr_input_dict)  # successful forward with no modifications\n \n@@ -271,12 +269,6 @@ def test_feed_forward_chunking(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n-    @parameterized.expand([(\"random\",), (\"same\",)])\n-    @pytest.mark.generate\n-    @unittest.skip(\"Paligemma2 does not seem to be compatible with assisted decoding\")\n-    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n     @unittest.skip(\"Paligemma position ids are 1 indexed\")\n     def test_eager_padding_matches_padding_free_with_position_ids(self):\n         pass"
        },
        {
            "sha": "79c74c93a68204e929f12f21df340e1dc4c5bd95",
            "filename": "tests/models/perception_lm/test_modeling_perception_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fperception_lm%2Ftest_modeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fperception_lm%2Ftest_modeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperception_lm%2Ftest_modeling_perception_lm.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -253,6 +253,7 @@ def test_mismatching_num_image_tokens(self):\n             if model_class == PerceptionLMModel:\n                 continue\n             model = model_class(config).to(torch_device)\n+            model.eval()\n             _ = model(**input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text"
        },
        {
            "sha": "0acda0ddac3d0116f8366a20d1374ba01ae20caf",
            "filename": "tests/models/pix2struct/test_modeling_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -627,6 +627,7 @@ def test_resize_embeddings_untied(self):\n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n             model = model_class(config).to(torch_device)\n+            model.eval()\n \n             # if no output embeddings -> leave test\n             if model.get_output_embeddings() is None:"
        },
        {
            "sha": "6cf1b0fa1078eac085aa2fdb44083504fcd11b2f",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -242,6 +242,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n+            model.eval()\n             _ = model(**input_dict)  # successful forward with no modifications\n             curr_input_dict = copy.deepcopy(input_dict)\n "
        },
        {
            "sha": "898b98658ecc7658d11ffe3e311b352b07ae509d",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -234,6 +234,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n+            model.eval()\n             curr_input_dict = copy.deepcopy(input_dict)\n             _ = model(**curr_input_dict)  # successful forward with no modifications\n "
        },
        {
            "sha": "888d9eb7661825ea389d768a7575c8dfa275f3fa",
            "filename": "tests/models/qwen3_vl/test_modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fqwen3_vl%2Ftest_modeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fqwen3_vl%2Ftest_modeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl%2Ftest_modeling_qwen3_vl.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -201,6 +201,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n+            model.eval()\n             _ = model(**input_dict)  # successful forward with no modifications\n             curr_input_dict = copy.deepcopy(input_dict)\n "
        },
        {
            "sha": "d5e971041931ea53c272a91fcf5266cc94af4bca",
            "filename": "tests/models/qwen3_vl_moe/test_modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -202,6 +202,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n+            model.eval()\n             _ = model(**input_dict)  # successful forward with no modifications\n             curr_input_dict = copy.deepcopy(input_dict)\n "
        },
        {
            "sha": "dd449672551bd114f1b9f765dffeb05600d043cc",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -284,6 +284,7 @@ def test_resize_embeddings_untied(self):\n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n             model = model_class(config).to(torch_device)\n+            model.eval()\n \n             # if no output embeddings -> leave test\n             if model.get_output_embeddings() is None:\n@@ -475,6 +476,7 @@ def test_resize_embeddings_untied(self):\n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n             model = model_class(config).to(torch_device)\n+            model.eval()\n \n             # Check that resizing the token embeddings with a larger vocab size increases the model's vocab size\n             model_vocab_size = config.text_config.vocab_size"
        },
        {
            "sha": "f8ac098f929622cdf1d64245cb092292fb9bf87f",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -581,6 +581,7 @@ def test_resize_embeddings_untied(self):\n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n             model = model_class(config).to(torch_device)\n+            model.eval()\n \n             # if no output embeddings -> leave test\n             if model.get_output_embeddings() is None:"
        },
        {
            "sha": "654c397c951ea84f5ad843e628b25251198f6888",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -625,6 +625,7 @@ def test_resize_embeddings_untied(self):\n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n             model = model_class(config).to(torch_device)\n+            model.eval()\n \n             # if no output embeddings -> leave test\n             if model.get_output_embeddings() is None:"
        },
        {
            "sha": "8bdb878843734e5e36db3fddef420272ae55f7e8",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -353,6 +353,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n+            model.eval()\n             curr_input_dict = copy.deepcopy(input_dict)\n             _ = model(**curr_input_dict)  # successful forward with no modifications\n "
        },
        {
            "sha": "bf7b43dd4580e1b0a6c5cff81e43e32d686dc47e",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -202,6 +202,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n+            model.eval()\n             curr_input_dict = copy.deepcopy(input_dict)  # in=place modifications further\n             _ = model(**curr_input_dict)  # successful forward with no modifications\n "
        },
        {
            "sha": "83fbfce52b4bf56229a63620b2dff012d780d768",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -810,6 +810,7 @@ def test_resize_embeddings_untied(self):\n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n             model = model_class(config).to(torch_device)\n+            model.eval()\n \n             # if no output embeddings -> leave test\n             if model.get_output_embeddings() is None:"
        },
        {
            "sha": "a8493f87d8e88b986e35cd8226f04a3e7fad6e71",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/869735d37d0f929311ac6611728c482a4414ba8c/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=869735d37d0f929311ac6611728c482a4414ba8c",
            "patch": "@@ -2372,6 +2372,7 @@ def test_resize_embeddings_untied(self):\n                     model = model_class(config)\n             else:\n                 model = model_class(config).to(torch_device)\n+            model.eval()\n \n             # if no output embeddings -> leave test\n             if model.get_output_embeddings() is None:"
        }
    ],
    "stats": {
        "total": 1221,
        "additions": 758,
        "deletions": 463
    }
}