{
    "author": "Cyrilvallez",
    "message": "Simplify dtype instantiation (#42825)\n\n* fix\n\n* simplify\n\n* align tests\n\n* fix\n\n* oupsi - super bad traceback\n\n* fix\n\n* fix\n\n* fix\n\n* simplify\n\n* fix\n\n* comment",
    "sha": "64a7cc82a681803d6bdc9e96a1a212cebc5655d7",
    "files": [
        {
            "sha": "551174fa18d67504f2f1a66f0668f5ac0040cac1",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 75,
            "deletions": 121,
            "changes": 196,
            "blob_url": "https://github.com/huggingface/transformers/blob/64a7cc82a681803d6bdc9e96a1a212cebc5655d7/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64a7cc82a681803d6bdc9e96a1a212cebc5655d7/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=64a7cc82a681803d6bdc9e96a1a212cebc5655d7",
            "patch": "@@ -233,23 +233,28 @@ def set_zero3_state():\n         _is_ds_init_called = False\n \n \n-def restore_default_dtype(func):\n+@contextmanager\n+def local_torch_dtype(dtype: torch.dtype, model_class_name: str | None = None):\n     \"\"\"\n-    Decorator to restore the default torch dtype\n-    at the end of the function. Serves\n-    as a backup in case calling the function raises\n-    an error after the function has changed the default dtype but before it could restore it.\n+    Locally change the torch default dtype to `dtype`, and restore the old one upon exiting the context.\n+    If `model_class_name` is provided, it's used to provide a more helpful error message if `dtype` is not valid.\n     \"\"\"\n+    # Just a more helping error before we set `torch.set_default_dtype` later on which would crash in this case\n+    if not dtype.is_floating_point:\n+        if model_class_name is not None:\n+            error_message = (\n+                f\"{model_class_name} cannot be instantiated under `dtype={dtype}` as it's not a floating-point dtype\"\n+            )\n+        else:\n+            error_message = f\"Cannot set `{dtype}` as torch's default as it's not a floating-point dtype\"\n+        raise ValueError(error_message)\n \n-    @wraps(func)\n-    def _wrapper(*args, **kwargs):\n-        old_dtype = torch.get_default_dtype()\n-        try:\n-            return func(*args, **kwargs)\n-        finally:\n-            torch.set_default_dtype(old_dtype)\n-\n-    return _wrapper\n+    original_dtype = torch.get_default_dtype()\n+    try:\n+        torch.set_default_dtype(dtype)\n+        yield\n+    finally:\n+        torch.set_default_dtype(original_dtype)\n \n \n def get_torch_context_manager_or_global_device():\n@@ -696,23 +701,21 @@ def _get_resolved_checkpoint_files(\n \n \n def _get_dtype(\n-    cls,\n     dtype: Optional[Union[str, torch.dtype, dict]],\n     checkpoint_files: Optional[list[str]],\n     config: PreTrainedConfig,\n     sharded_metadata: Optional[dict],\n     state_dict: Optional[dict],\n     weights_only: bool,\n-) -> tuple[PreTrainedConfig, Optional[torch.dtype], Optional[torch.dtype]]:\n+) -> tuple[PreTrainedConfig, torch.dtype]:\n     \"\"\"Find the correct `dtype` to use based on provided arguments. Also update the `config` based on the\n     inferred dtype. We do the following:\n-    1. If dtype is not None, we use that dtype\n-    2. If dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\n-        weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\n-    we also may have config.dtype available, but we won't rely on it till v5\n+    1. If dtype is \"auto\", we try to read the config, else auto-detect dtype from the loaded state_dict, by checking\n+    its first weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\n+    2. Else, use the dtype provided as a dict or str\n     \"\"\"\n-    dtype_orig = None\n     is_sharded = sharded_metadata is not None\n+    asked_dtype = dtype\n \n     if dtype is not None:\n         if isinstance(dtype, str):\n@@ -736,43 +739,46 @@ def _get_dtype(\n                     )\n             elif hasattr(torch, dtype):\n                 dtype = getattr(torch, dtype)\n-                config.dtype = dtype\n-                for sub_config_key in config.sub_configs:\n-                    if (sub_config := getattr(config, sub_config_key)) is not None:\n-                        sub_config.dtype = dtype\n-        elif isinstance(dtype, torch.dtype):\n-            config.dtype = dtype\n-            for sub_config_key in config.sub_configs:\n-                if (sub_config := getattr(config, sub_config_key)) is not None:\n-                    sub_config.dtype = dtype\n-        elif isinstance(dtype, dict):\n-            for key, curr_dtype in dtype.items():\n-                if hasattr(config, key):\n-                    value = getattr(config, key)\n-                    curr_dtype = curr_dtype if not isinstance(curr_dtype, str) else getattr(torch, curr_dtype)\n-                    value.dtype = curr_dtype\n-            # main torch dtype for modules that aren't part of any sub-config\n-            dtype = dtype.get(\"\")\n-            dtype = dtype if not isinstance(dtype, str) else getattr(torch, dtype)\n-            config.dtype = dtype\n-            if dtype is None:\n-                dtype = torch.float32\n-        else:\n+            else:\n+                raise ValueError(\n+                    \"`dtype` provided as a `str` can only be `'auto'`, or a string representation of a valid `torch.dtype`\"\n+                )\n+\n+            # cast it to a proper `torch.dtype` object\n+            dtype = getattr(torch, dtype) if isinstance(dtype, str) else dtype\n+        elif not isinstance(dtype, (dict, torch.dtype)):\n             raise ValueError(\n                 f\"`dtype` can be one of: `torch.dtype`, `'auto'`, a string of a valid `torch.dtype` or a `dict` with valid `dtype` \"\n                 f\"for each sub-config in composite configs, but received {dtype}\"\n             )\n+    else:\n+        # set torch.get_default_dtype() (usually fp32) as the default dtype if `None` is provided\n+        dtype = torch.get_default_dtype()\n \n-        dtype_orig = cls._set_default_dtype(dtype)\n+    # Get the main dtype\n+    if isinstance(dtype, dict):\n+        main_dtype = dtype.get(\"\", torch.get_default_dtype())\n+        main_dtype = getattr(torch, main_dtype) if isinstance(main_dtype, str) else main_dtype\n     else:\n-        # set fp32 as the default dtype for BC\n-        default_dtype = torch.get_default_dtype()\n-        config.dtype = default_dtype\n-        for key in config.sub_configs:\n-            if (sub_config := getattr(config, key)) is not None:\n-                sub_config.dtype = default_dtype\n-    dtype = dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)\n-    return config, dtype, dtype_orig\n+        main_dtype = dtype\n+\n+    # Set it on the config and subconfigs\n+    config.dtype = main_dtype\n+    for sub_config_key in config.sub_configs:\n+        if (sub_config := getattr(config, sub_config_key)) is not None:\n+            # The dtype was \"auto\" -> try to read the subconfig dtype value if any\n+            if asked_dtype == \"auto\":\n+                sub_dtype = getattr(sub_config, \"dtype\", main_dtype)\n+                sub_dtype = getattr(torch, sub_dtype) if isinstance(sub_dtype, str) else sub_dtype\n+            # The dtype was provided as a dict, try to see if we match the subconfig name\n+            elif isinstance(dtype, dict):\n+                sub_dtype = dtype.get(sub_config_key, main_dtype)\n+                sub_dtype = getattr(torch, sub_dtype) if isinstance(sub_dtype, str) else sub_dtype\n+            else:\n+                sub_dtype = main_dtype\n+            sub_config.dtype = sub_dtype\n+\n+    return config, main_dtype\n \n \n class PipelineParallel(Enum):\n@@ -798,10 +804,10 @@ def dtype(self) -> torch.dtype:\n         \"\"\"\n         `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n         \"\"\"\n-        dtype = self._dtype or next(param.dtype for param in self.parameters() if param.is_floating_point())\n-        if isinstance(dtype, str):\n-            if hasattr(torch, dtype):\n-                dtype = getattr(torch, dtype)\n+        # Use config dtype, if it's a proper `torch.dtype`\n+        config_dtype = getattr(self.config, \"dtype\", None)\n+        config_dtype = config_dtype if isinstance(config_dtype, torch.dtype) else None\n+        dtype = config_dtype or next(param.dtype for param in self.parameters() if param.is_floating_point())\n         return dtype\n \n     def invert_attention_mask(self, encoder_attention_mask: Tensor) -> Tensor:\n@@ -1081,7 +1087,6 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n     _keep_in_fp32_modules_strict = None\n \n     dtype_plan: Optional[dict[str, torch.dtype]] = None\n-    _dtype: Optional[Union[str, torch.dtype]] = torch.get_default_dtype()\n \n     # a list of `re` patterns of `state_dict` keys that should be removed from the list of missing\n     # keys we find (keys inside the model but not in the checkpoint) and avoid unnecessary warnings.\n@@ -1226,8 +1231,6 @@ def __init__(self, config: PreTrainedConfig, *inputs, **kwargs):\n                 f\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n             )\n         self.config = config\n-        default_dtype = torch.get_default_dtype()\n-        self._dtype = default_dtype\n \n         # Check the attention implementation is supported, or set it if not yet set (on the internal attr, to avoid\n         # setting it recursively)\n@@ -1400,7 +1403,6 @@ def add_model_tags(self, tags: Union[list[str], str]) -> None:\n                 self.model_tags.append(tag)\n \n     @classmethod\n-    @restore_default_dtype\n     def _from_config(cls, config, **kwargs):\n         \"\"\"\n         All context managers that the model should be initialized under go here.\n@@ -1409,9 +1411,6 @@ def _from_config(cls, config, **kwargs):\n             dtype (`torch.dtype`, *optional*):\n                 Override the default `dtype` and load the model under this dtype.\n         \"\"\"\n-        # when we init a model from within another model (e.g. VLMs) and dispatch on FA2\n-        # a warning is raised that dtype should be fp16. Since we never pass dtype from within\n-        # modeling code, we can try to infer it here same way as done in `from_pretrained`\n         # For BC on the old `torch_dtype`\n         dtype = kwargs.pop(\"dtype\", config.dtype)\n         if (torch_dtype := kwargs.pop(\"torch_dtype\", None)) is not None:\n@@ -1421,67 +1420,27 @@ def _from_config(cls, config, **kwargs):\n         if isinstance(dtype, str):\n             dtype = getattr(torch, dtype)\n \n-        # override default dtype if needed\n-        dtype_orig = None\n-        if dtype is not None:\n-            dtype_orig = cls._set_default_dtype(dtype)\n-\n         # If passing `attn_implementation` as kwargs, respect it (it will be applied recursively on subconfigs)\n         if \"attn_implementation\" in kwargs:\n             config._attn_implementation = kwargs.pop(\"attn_implementation\")\n \n+        init_contexts = []\n+        if dtype is not None:\n+            init_contexts.append(local_torch_dtype(dtype, cls.__name__))\n         if is_deepspeed_zero3_enabled() and not _is_quantized and not _is_ds_init_called:\n             logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n             # this immediately partitions the model across all gpus, to avoid the overhead in time\n             # and memory copying it on CPU or each GPU first\n             import deepspeed\n \n-            init_contexts = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()]\n-            with ContextManagers(init_contexts):\n-                model = cls(config, **kwargs)\n+            init_contexts.extend([deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()])\n \n-        else:\n+        # Instantiate the model\n+        with ContextManagers(init_contexts):\n             model = cls(config, **kwargs)\n \n-        # restore default dtype if it was modified\n-        if dtype_orig is not None:\n-            torch.set_default_dtype(dtype_orig)\n-\n         return model\n \n-    @classmethod\n-    def _set_default_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n-        \"\"\"\n-        Change the default dtype and return the previous one. This is needed when wanting to instantiate the model\n-        under specific dtype.\n-\n-        Args:\n-            dtype (`torch.dtype`):\n-                a floating dtype to set to.\n-\n-        Returns:\n-            `torch.dtype`: the original `dtype` that can be used to restore `torch.set_default_dtype(dtype)` if it was\n-            modified. If it wasn't, returns `None`.\n-\n-        Note `set_default_dtype` currently only works with floating-point types and asserts if for example,\n-        `torch.int64` is passed. So if a non-float `dtype` is passed this functions will throw an exception.\n-        \"\"\"\n-        if isinstance(dtype, str):\n-            if hasattr(torch, dtype):\n-                dtype = getattr(torch, dtype)\n-            else:\n-                raise ValueError(f\"Received an invalid string dtype: {dtype}\")\n-        if not dtype.is_floating_point:\n-            raise ValueError(\n-                f\"Can't instantiate {cls.__name__} model under dtype={dtype} since it is not a floating point dtype\"\n-            )\n-\n-        logger.info(f\"Instantiating {cls.__name__} model under default dtype {dtype}.\")\n-        dtype_orig = torch.get_default_dtype()\n-        torch.set_default_dtype(dtype)\n-        cls._dtype = dtype\n-        return dtype_orig\n-\n     @property\n     def base_model(self) -> nn.Module:\n         \"\"\"\n@@ -3537,19 +3496,21 @@ def float(self, *args):\n             return super().float(*args)\n \n     @classmethod\n-    def get_init_context(cls, is_quantized: bool, _is_ds_init_called: bool):\n+    def get_init_context(cls, dtype: torch.dtype, is_quantized: bool, _is_ds_init_called: bool):\n+        # Need to instantiate with correct dtype\n+        init_contexts = [local_torch_dtype(dtype, cls.__name__)]\n         if is_deepspeed_zero3_enabled():\n             import deepspeed\n \n-            init_contexts = [no_init_weights()]\n+            init_contexts.append(no_init_weights())\n             # We cannot initialize the model on meta device with deepspeed when not quantized\n             if not is_quantized and not _is_ds_init_called:\n                 logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n                 init_contexts.extend([deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()])\n             elif is_quantized:\n                 init_contexts.extend([init_empty_weights(), set_quantized_state()])\n         else:\n-            init_contexts = [no_init_weights(), init_empty_weights()]\n+            init_contexts.extend([no_init_weights(), init_empty_weights()])\n \n         return init_contexts\n \n@@ -3583,7 +3544,6 @@ def set_use_kernels(self, use_kernels, kernel_config):\n             self.use_kernels = False\n \n     @classmethod\n-    @restore_default_dtype\n     def from_pretrained(\n         cls: type[SpecificPreTrainedModelType],\n         pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n@@ -3963,12 +3923,10 @@ def from_pretrained(\n             ]\n \n         # Find the correct dtype based on current state\n-        config, dtype, dtype_orig = _get_dtype(\n-            cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only\n-        )\n+        config, dtype = _get_dtype(dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only)\n \n         config.name_or_path = pretrained_model_name_or_path\n-        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)\n+        model_init_context = cls.get_init_context(dtype, is_quantized, _is_ds_init_called)\n         config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.\n         with ContextManagers(model_init_context):\n             # Let's make sure we don't run the init function of buffer modules\n@@ -3997,10 +3955,6 @@ def from_pretrained(\n         if device_map is not None:\n             device_map = _get_device_map(model, device_map, max_memory, hf_quantizer)\n \n-        # restore default dtype\n-        if dtype_orig is not None:\n-            torch.set_default_dtype(dtype_orig)\n-\n         # Finalize model weight initialization\n         model, missing_keys, unexpected_keys, mismatched_keys, offload_index, error_msgs = cls._load_pretrained_model(\n             model,"
        },
        {
            "sha": "9b74a295f6c6666ea6a9706fac9213acbf8da07a",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 20,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/64a7cc82a681803d6bdc9e96a1a212cebc5655d7/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64a7cc82a681803d6bdc9e96a1a212cebc5655d7/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=64a7cc82a681803d6bdc9e96a1a212cebc5655d7",
            "patch": "@@ -1872,22 +1872,19 @@ def test_restore_default_dtype_from_pretrained(self):\n         # set default type to float32\n         torch.set_default_dtype(torch.float32)\n \n-        # Mock injection point which is right after the call to `_set_default_dtype`\n-        original_set_default_dtype = MistralForCausalLM._set_default_dtype\n+        # Mock injection point which is right after the call to `torch.set_default_dtype`\n+        original_set_default_dtype = torch.set_default_dtype\n \n         def debug(*args, **kwargs):\n             # call the method as usual, than raise a RuntimeError\n             original_set_default_dtype(*args, **kwargs)\n             raise RuntimeError\n \n-        with mock.patch(\n-            \"transformers.models.mistral.modeling_mistral.MistralForCausalLM._set_default_dtype\",\n-            side_effect=debug,\n-        ):\n+        with patch(\"torch.set_default_dtype\", new=debug):\n             with self.assertRaises(RuntimeError):\n                 _ = AutoModelForCausalLM.from_pretrained(TINY_MISTRAL, device_map=\"auto\", dtype=torch.float16)\n         # default should still be float32\n-        assert torch.get_default_dtype() == torch.float32\n+        self.assertTrue(torch.get_default_dtype() == torch.float32)\n         torch.set_default_dtype(old_dtype)\n \n     def test_restore_default_dtype_from_config(self):\n@@ -1899,29 +1896,23 @@ def test_restore_default_dtype_from_config(self):\n         # set default type to float32\n         torch.set_default_dtype(torch.float32)\n \n-        config = AutoConfig.from_pretrained(\n-            TINY_MISTRAL,\n-        )\n+        config = AutoConfig.from_pretrained(TINY_MISTRAL)\n \n-        # Mock injection point which is right after the call to `_set_default_dtype`\n-        original_set_default_dtype = MistralForCausalLM._set_default_dtype\n+        # Mock injection point which is right after the call to `torch.set_default_dtype`\n+        original_set_default_dtype = torch.set_default_dtype\n \n         def debug(*args, **kwargs):\n             # call the method as usual, than raise a RuntimeError\n             original_set_default_dtype(*args, **kwargs)\n             raise RuntimeError\n \n-        with mock.patch(\n-            \"transformers.models.mistral.modeling_mistral.MistralForCausalLM._set_default_dtype\",\n-            side_effect=debug,\n-        ):\n+        with patch(\"torch.set_default_dtype\", new=debug):\n             with self.assertRaises(RuntimeError):\n                 config.dtype = torch.float16\n-                _ = AutoModelForCausalLM.from_config(\n-                    config,\n-                )\n+                _ = AutoModelForCausalLM.from_config(config)\n+\n         # default should still be float32\n-        assert torch.get_default_dtype() == torch.float32\n+        self.assertTrue(torch.get_default_dtype() == torch.float32)\n         torch.set_default_dtype(old_dtype)\n \n     def test_unknown_quantization_config(self):"
        }
    ],
    "stats": {
        "total": 227,
        "additions": 86,
        "deletions": 141
    }
}