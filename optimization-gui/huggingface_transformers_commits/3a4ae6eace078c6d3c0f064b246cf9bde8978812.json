{
    "author": "Cyrilvallez",
    "message": "Refactor/fix Cohere2 (#35594)\n\n* refactor/fix cohere2\r\n\r\n* add kwargs\r\n\r\n* tests\r\n\r\n* remove func and import it",
    "sha": "3a4ae6eace078c6d3c0f064b246cf9bde8978812",
    "files": [
        {
            "sha": "8fa99c06a06e2adce285c78d2c3b1ad0fd6c3699",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 81,
            "deletions": 185,
            "changes": 266,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a4ae6eace078c6d3c0f064b246cf9bde8978812/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a4ae6eace078c6d3c0f064b246cf9bde8978812/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=3a4ae6eace078c6d3c0f064b246cf9bde8978812",
            "patch": "@@ -19,8 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -31,23 +30,18 @@\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_cohere2 import Cohere2Config\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n _CONFIG_FOR_DOC = \"Cohere2Config\"\n \n@@ -139,6 +133,32 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n def rotate_half(x):\n     # Split and rotate. Note that this function is different from e.g. Llama.\n     x1 = x[..., ::2]\n@@ -177,155 +197,31 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed.to(dtype=dtype), k_embed.to(dtype=dtype)\n \n \n-def eager_attention_forward(\n-    config: Cohere2Config,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    **_kwargs,\n-) -> Tuple[torch.Tensor, torch.Tensor]:\n-    key_states = repeat_kv(key, config.num_key_value_groups)\n-    value_states = repeat_kv(value, config.num_key_value_groups)\n-\n-    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) / math.sqrt(config.head_dim)\n-\n-    if mask is not None:  # no matter the length, we just slice it\n-        causal_mask = mask[:, :, :, : key_states.shape[-2]]\n-        attn_weights = attn_weights + causal_mask\n-\n-    # upcast attention to fp32\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n-    attn_weights = nn.functional.dropout(attn_weights, p=config.attention_dropout, training=config.training)\n-    attn_output = torch.matmul(attn_weights, value_states)\n-    attn_output = attn_output.transpose(1, 2).contiguous()\n-    return attn_output, attn_weights\n-\n-\n-def flash_attention_forward(\n-    config: Cohere2Config,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    target_dtype: torch.dtype = torch.float16,\n-    **_kwargs,\n-) -> Tuple[torch.Tensor, None]:\n-    if mask is not None:\n-        seq_len = mask.shape[1]\n-        query = query[:, :, :seq_len]\n-        value = value[:, :, :seq_len]\n-\n-    # TODO: These transpose are quite inefficient but Flash Attention requires the layout\n-    # [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor rotary embedding\n-    query_states = query.transpose(1, 2)\n-    key_states = key.transpose(1, 2)\n-    value_states = value.transpose(1, 2)\n-\n-    dropout_rate = config.attention_dropout if config.training else 0.0\n-\n-    input_dtype = query_states.dtype\n-    if input_dtype == torch.float32:\n-        query_states = query_states.to(target_dtype)\n-        key_states = key_states.to(target_dtype)\n-        value_states = value_states.to(target_dtype)\n-\n-    attn_output = _flash_attention_forward(\n-        query_states,\n-        key_states,\n-        value_states,\n-        mask,\n-        seq_len,\n-        dropout=dropout_rate,\n-        is_causal=config.is_causal,\n-        sliding_window=config.sliding_window,\n-        use_top_left_mask=config._flash_attn_uses_top_left_mask,\n-    )\n-\n-    return attn_output, None\n-\n-\n-def sdpa_attention_forward(\n-    config: Cohere2Config,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    **_kwargs,\n-) -> Tuple[torch.Tensor, None]:\n-    key = repeat_kv(key, config.num_key_value_groups)\n-    value = repeat_kv(value, config.num_key_value_groups)\n-\n-    causal_mask = mask\n-    if mask is not None:\n-        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n-\n-    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-    if query.device.type == \"cuda\" and causal_mask is not None:\n-        query = query.contiguous()\n-        key = key.contiguous()\n-        value = value.contiguous()\n-\n-    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-    is_causal = True if causal_mask is None and query.shape[1] > 1 else False\n-\n-    attn_output = torch.nn.functional.scaled_dot_product_attention(\n-        query,\n-        key,\n-        value,\n-        attn_mask=causal_mask,\n-        dropout_p=config.attention_dropout if config.training else 0.0,\n-        is_causal=is_causal,\n-    )\n-\n-    attn_output = attn_output.transpose(1, 2).contiguous()\n-    return attn_output, None\n-\n-\n-COHERE2_ATTENTION_FUNCTION = {\n-    \"flash_attention_2\": flash_attention_forward,\n-    \"eager\": eager_attention_forward,\n-    \"sdpa\": sdpa_attention_forward,\n-}\n-\n-\n class Cohere2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = config.head_dim\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n \n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n-\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n         self.sliding_window = (\n             config.sliding_window if (self.layer_idx + 1) % self.config.sliding_window_pattern != 0 else None\n         )\n@@ -334,25 +230,19 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         cos, sin = position_embeddings\n-\n         if self.sliding_window is not None:\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n@@ -365,23 +255,31 @@ def forward(\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        if output_attentions and self.config._attn_implementation in [\"sdpa\", \"flash_attention_2\"]:\n-            logger.warning_once(\"Setting `attention_type` to `eager` because `output_attentions=True`\")\n-            attention_type = \"eager\"\n-        else:\n-            attention_type = self.config._attn_implementation\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output, attn_weights = COHERE2_ATTENTION_FUNCTION[attention_type](\n-            self, query_states, key_states, value_states, attention_mask, output_attentions=output_attentions\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class Cohere2MLP(nn.Module):\n@@ -416,10 +314,11 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -430,13 +329,13 @@ def forward(\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n         \"\"\"\n@@ -460,14 +359,15 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states_attention, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states_attention, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         # Fully Connected\n@@ -481,9 +381,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -653,6 +550,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -727,6 +625,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -740,16 +639,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = past_key_values if use_cache else None\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     @torch.no_grad()\n     def _update_causal_mask("
        },
        {
            "sha": "145905287acde498d1f84da79ed2059859d1f591",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 62,
            "deletions": 188,
            "changes": 250,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a4ae6eace078c6d3c0f064b246cf9bde8978812/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a4ae6eace078c6d3c0f064b246cf9bde8978812/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=3a4ae6eace078c6d3c0f064b246cf9bde8978812",
            "patch": "@@ -13,39 +13,37 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import math\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n import torch.utils.checkpoint\n \n from ...cache_utils import Cache, HybridCache\n from ...configuration_utils import PretrainedConfig\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n )\n from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n from ...utils import (\n-    is_flash_attn_2_available,\n     logging,\n )\n from ..cohere.modeling_cohere import (\n+    CohereAttention,\n     CohereDecoderLayer,\n     CohereForCausalLM,\n     CohereLayerNorm,\n     CoherePreTrainedModel,\n     CohereRotaryEmbedding,\n     apply_rotary_pos_emb,\n-    repeat_kv,\n+    eager_attention_forward,\n )\n from ..gemma2.modeling_gemma2 import Gemma2Model\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -240,155 +238,31 @@ class Cohere2LayerNorm(CohereLayerNorm):\n     pass\n \n \n-def eager_attention_forward(\n-    config: Cohere2Config,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    **_kwargs,\n-) -> Tuple[torch.Tensor, torch.Tensor]:\n-    key_states = repeat_kv(key, config.num_key_value_groups)\n-    value_states = repeat_kv(value, config.num_key_value_groups)\n-\n-    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) / math.sqrt(config.head_dim)\n-\n-    if mask is not None:  # no matter the length, we just slice it\n-        causal_mask = mask[:, :, :, : key_states.shape[-2]]\n-        attn_weights = attn_weights + causal_mask\n-\n-    # upcast attention to fp32\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n-    attn_weights = nn.functional.dropout(attn_weights, p=config.attention_dropout, training=config.training)\n-    attn_output = torch.matmul(attn_weights, value_states)\n-    attn_output = attn_output.transpose(1, 2).contiguous()\n-    return attn_output, attn_weights\n-\n-\n-def flash_attention_forward(\n-    config: Cohere2Config,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    target_dtype: torch.dtype = torch.float16,\n-    **_kwargs,\n-) -> Tuple[torch.Tensor, None]:\n-    if mask is not None:\n-        seq_len = mask.shape[1]\n-        query = query[:, :, :seq_len]\n-        value = value[:, :, :seq_len]\n-\n-    # TODO: These transpose are quite inefficient but Flash Attention requires the layout\n-    # [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor rotary embedding\n-    query_states = query.transpose(1, 2)\n-    key_states = key.transpose(1, 2)\n-    value_states = value.transpose(1, 2)\n-\n-    dropout_rate = config.attention_dropout if config.training else 0.0\n-\n-    input_dtype = query_states.dtype\n-    if input_dtype == torch.float32:\n-        query_states = query_states.to(target_dtype)\n-        key_states = key_states.to(target_dtype)\n-        value_states = value_states.to(target_dtype)\n-\n-    attn_output = _flash_attention_forward(\n-        query_states,\n-        key_states,\n-        value_states,\n-        mask,\n-        seq_len,\n-        dropout=dropout_rate,\n-        is_causal=config.is_causal,\n-        sliding_window=config.sliding_window,\n-        use_top_left_mask=config._flash_attn_uses_top_left_mask,\n-    )\n-\n-    return attn_output, None\n-\n-\n-def sdpa_attention_forward(\n-    config: Cohere2Config,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    **_kwargs,\n-) -> Tuple[torch.Tensor, None]:\n-    key = repeat_kv(key, config.num_key_value_groups)\n-    value = repeat_kv(value, config.num_key_value_groups)\n-\n-    causal_mask = mask\n-    if mask is not None:\n-        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n-\n-    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-    if query.device.type == \"cuda\" and causal_mask is not None:\n-        query = query.contiguous()\n-        key = key.contiguous()\n-        value = value.contiguous()\n-\n-    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-    is_causal = True if causal_mask is None and query.shape[1] > 1 else False\n-\n-    attn_output = torch.nn.functional.scaled_dot_product_attention(\n-        query,\n-        key,\n-        value,\n-        attn_mask=causal_mask,\n-        dropout_p=config.attention_dropout if config.training else 0.0,\n-        is_causal=is_causal,\n-    )\n-\n-    attn_output = attn_output.transpose(1, 2).contiguous()\n-    return attn_output, None\n-\n-\n-COHERE2_ATTENTION_FUNCTION = {\n-    \"flash_attention_2\": flash_attention_forward,\n-    \"eager\": eager_attention_forward,\n-    \"sdpa\": sdpa_attention_forward,\n-}\n-\n-\n-class Cohere2Attention(nn.Module):\n+class Cohere2Attention(CohereAttention, nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n-        super().__init__()\n+        nn.Module.__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = config.head_dim\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n \n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n-\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n         self.sliding_window = (\n             config.sliding_window if (self.layer_idx + 1) % self.config.sliding_window_pattern != 0 else None\n         )\n@@ -397,25 +271,19 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         cos, sin = position_embeddings\n-\n         if self.sliding_window is not None:\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n@@ -428,23 +296,31 @@ def forward(\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        if output_attentions and self.config._attn_implementation in [\"sdpa\", \"flash_attention_2\"]:\n-            logger.warning_once(\"Setting `attention_type` to `eager` because `output_attentions=True`\")\n-            attention_type = \"eager\"\n-        else:\n-            attention_type = self.config._attn_implementation\n-\n-        attn_output, attn_weights = COHERE2_ATTENTION_FUNCTION[attention_type](\n-            self, query_states, key_states, value_states, attention_mask, output_attentions=output_attentions\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class Cohere2DecoderLayer(CohereDecoderLayer):\n@@ -460,10 +336,11 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -474,13 +351,13 @@ def forward(\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n         \"\"\"\n@@ -504,14 +381,15 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states_attention, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states_attention, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         # Fully Connected\n@@ -525,9 +403,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -559,6 +434,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -633,6 +509,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -646,16 +523,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = past_key_values if use_cache else None\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n \n class Cohere2ForCausalLM(CohereForCausalLM):"
        },
        {
            "sha": "8b995d1a08ed88d13506d850d2e4ca23a515bc40",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a4ae6eace078c6d3c0f064b246cf9bde8978812/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a4ae6eace078c6d3c0f064b246cf9bde8978812/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=3a4ae6eace078c6d3c0f064b246cf9bde8978812",
            "patch": "@@ -548,6 +548,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -633,6 +634,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "f73b9ea840aec8a6535e0738c7e39102e8e6c7fb",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a4ae6eace078c6d3c0f064b246cf9bde8978812/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a4ae6eace078c6d3c0f064b246cf9bde8978812/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=3a4ae6eace078c6d3c0f064b246cf9bde8978812",
            "patch": "@@ -378,6 +378,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -463,6 +464,7 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "144846772fd54c83c4a54d068c2a0a07e6389edc",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a4ae6eace078c6d3c0f064b246cf9bde8978812/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a4ae6eace078c6d3c0f064b246cf9bde8978812/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=3a4ae6eace078c6d3c0f064b246cf9bde8978812",
            "patch": "@@ -201,7 +201,6 @@ def setUpClass(cls):\n             cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n     @require_read_token\n-    @unittest.skip(\"Cohere2 has not been released yet\")\n     def test_model_bf16(self):\n         model_id = \"CohereForAI/command-r7b-12-2024\"\n         EXPECTED_TEXTS = [\n@@ -222,7 +221,6 @@ def test_model_bf16(self):\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     @require_read_token\n-    @unittest.skip(\"Cohere2 has not been released yet\")\n     def test_model_fp16(self):\n         model_id = \"CohereForAI/command-r7b-12-2024\"\n         EXPECTED_TEXTS = [\n@@ -243,7 +241,6 @@ def test_model_fp16(self):\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     @require_read_token\n-    @unittest.skip(\"Cohere2 has not been released yet\")\n     def test_model_pipeline_bf16(self):\n         # See https://github.com/huggingface/transformers/pull/31747 -- pipeline was broken for Cohere2 before this PR\n         model_id = \"CohereForAI/command-r7b-12-2024\"\n@@ -269,7 +266,6 @@ def test_model_pipeline_bf16(self):\n     @require_torch_gpu\n     @mark.flash_attn_test\n     @slow\n-    @unittest.skip(\"Cohere2 has not been released yet\")\n     def test_model_flash_attn(self):\n         # See https://github.com/huggingface/transformers/issues/31953 --- flash attn was generating garbage for Gemma2, especially in long context\n         model_id = \"CohereForAI/command-r7b-12-2024\"\n@@ -291,7 +287,6 @@ def test_model_flash_attn(self):\n \n     @slow\n     @require_read_token\n-    @unittest.skip(\"Cohere2 has not been released yet\")\n     def test_export_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.5.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.5 to run.\")"
        }
    ],
    "stats": {
        "total": 525,
        "additions": 147,
        "deletions": 378
    }
}