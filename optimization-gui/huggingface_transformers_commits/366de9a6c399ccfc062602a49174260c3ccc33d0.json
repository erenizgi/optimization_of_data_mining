{
    "author": "a4lg",
    "message": "Fix a typo in GGML integration of Qwen2 MoE (#42650)\n\nJust like `\"qwen3_moe\"` below, name of the key must be `\"qwen2_moe\"`\nwith one underscore.\n\nSigned-off-by: Tsukasa OI <floss_llm@irq.a4lg.com>",
    "sha": "366de9a6c399ccfc062602a49174260c3ccc33d0",
    "files": [
        {
            "sha": "ec08846fa11db13f97ea8d124aacc31dfeadac0f",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/366de9a6c399ccfc062602a49174260c3ccc33d0/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/366de9a6c399ccfc062602a49174260c3ccc33d0/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=366de9a6c399ccfc062602a49174260c3ccc33d0",
            "patch": "@@ -76,7 +76,7 @@\n         \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n         \"vocab_size\": \"vocab_size\",\n     },\n-    \"qwen2moe\": {\n+    \"qwen2_moe\": {\n         \"context_length\": \"max_position_embeddings\",\n         \"block_count\": \"num_hidden_layers\",\n         \"feed_forward_length\": \"intermediate_size\","
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}