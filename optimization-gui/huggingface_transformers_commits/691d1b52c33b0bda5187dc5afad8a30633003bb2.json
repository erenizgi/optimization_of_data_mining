{
    "author": "seanswyi",
    "message": "Fix/best model checkpoint fix (#35885)\n\n* Set best_model_checkpoint only when ckpt exists.\n\nRather than set it explicitly without checking if the checkpoint directory even exists as before, now we moved the setting logic inside of _save_checkpoint and are only setting it if it exists.\n\n* Added best_global_step to TrainerState.\n\n* Added tests for best_model_checkpoint.\n\n* Fixed hard-coded values in test to prevent fail.\n\n* Added helper func and removed hard-coded best_step.\n\n* Added side effect patch generator for _eval.\n\n* Added evaluate side effect func.\n\n* Removed erroneous patching.\n\n* Fixed minor bug.\n\n* Applied Ruff.\n\n* Fixed Ruff problem in make style.\n\n* Used Trainer.set_initial_training_values.",
    "sha": "691d1b52c33b0bda5187dc5afad8a30633003bb2",
    "files": [
        {
            "sha": "d75c2d778fb8efb329b96c6f10384bcaf1813168",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 30,
            "deletions": 1,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/691d1b52c33b0bda5187dc5afad8a30633003bb2/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/691d1b52c33b0bda5187dc5afad8a30633003bb2/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=691d1b52c33b0bda5187dc5afad8a30633003bb2",
            "patch": "@@ -38,7 +38,7 @@\n from functools import wraps\n from io import StringIO\n from pathlib import Path\n-from typing import Callable, Dict, Iterable, Iterator, List, Optional, Union\n+from typing import Callable, Dict, Generator, Iterable, Iterator, List, Optional, Union\n from unittest import mock\n from unittest.mock import patch\n \n@@ -48,6 +48,7 @@\n from huggingface_hub import delete_repo\n from packaging import version\n \n+from transformers import Trainer\n from transformers import logging as transformers_logging\n \n from .integrations import (\n@@ -1440,6 +1441,34 @@ def get_tests_dir(append_path=None):\n         return tests_dir\n \n \n+def get_steps_per_epoch(trainer: Trainer) -> int:\n+    training_args = trainer.args\n+    train_dataloader = trainer.get_train_dataloader()\n+\n+    initial_training_values = trainer.set_initial_training_values(\n+        args=training_args,\n+        dataloader=train_dataloader,\n+        total_train_batch_size=training_args.per_device_train_batch_size,\n+    )\n+    steps_per_epoch = initial_training_values[1]\n+\n+    return steps_per_epoch\n+\n+\n+def evaluate_side_effect_factory(\n+    side_effect_values: List[Dict[str, float]],\n+) -> Generator[Dict[str, float], None, None]:\n+    \"\"\"\n+    Function that returns side effects for the _evaluate method.\n+    Used when we're unsure of exactly how many times _evaluate will be called.\n+    \"\"\"\n+    for side_effect_value in side_effect_values:\n+        yield side_effect_value\n+\n+    while True:\n+        yield side_effect_values[-1]\n+\n+\n #\n # Helper functions for dealing with testing text outputs\n # The original code came from:"
        },
        {
            "sha": "bea32918b617b63fb6338524584e51c1fb6959e8",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/691d1b52c33b0bda5187dc5afad8a30633003bb2/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/691d1b52c33b0bda5187dc5afad8a30633003bb2/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=691d1b52c33b0bda5187dc5afad8a30633003bb2",
            "patch": "@@ -3178,12 +3178,10 @@ def _determine_best_metric(self, metrics, trial):\n                 self.state.best_metric = float(\"-inf\") if self.args.greater_is_better else float(\"inf\")\n \n             if operator(metric_value, self.state.best_metric):\n-                run_dir = self._get_output_dir(trial=trial)\n-                checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\n-                output_dir = os.path.join(run_dir, checkpoint_folder)\n-\n                 self.state.best_metric = metric_value\n-                self.state.best_model_checkpoint = output_dir\n+\n+                if self.args.save_strategy in [SaveStrategy.STEPS, SaveStrategy.EPOCH]:\n+                    self.state.best_global_step = self.state.global_step\n \n                 is_new_best_metric = True\n \n@@ -3204,6 +3202,13 @@ def _save_checkpoint(self, model, trial):\n         output_dir = os.path.join(run_dir, checkpoint_folder)\n         self.save_model(output_dir, _internal_call=True)\n \n+        if self.args.save_strategy in [SaveStrategy.STEPS, SaveStrategy.EPOCH] and self.state.best_global_step:\n+            best_checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.best_global_step}\"\n+            best_checkpoint_dir = os.path.join(run_dir, best_checkpoint_folder)\n+\n+            if os.path.exists(best_checkpoint_dir):\n+                self.state.best_model_checkpoint = best_checkpoint_dir\n+\n         if not self.args.save_only_model:\n             # Save optimizer and scheduler\n             self._save_optimizer_and_scheduler(output_dir)"
        },
        {
            "sha": "027fce086c534edfcaa5a4a83e720f59e269edf8",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/691d1b52c33b0bda5187dc5afad8a30633003bb2/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/691d1b52c33b0bda5187dc5afad8a30633003bb2/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=691d1b52c33b0bda5187dc5afad8a30633003bb2",
            "patch": "@@ -74,6 +74,9 @@ class TrainerState:\n             The list of logs done since the beginning of training.\n         best_metric (`float`, *optional*):\n             When tracking the best model, the value of the best metric encountered so far.\n+        best_global_step (`int`, *optional*):\n+            When tracking the best model, the step at which the best metric was encountered.\n+            Used for setting `best_model_checkpoint`.\n         best_model_checkpoint (`str`, *optional*):\n             When tracking the best model, the value of the name of the checkpoint for the best model encountered so\n             far.\n@@ -103,6 +106,7 @@ class TrainerState:\n     total_flos: float = 0\n     log_history: List[Dict[str, float]] = None\n     best_metric: Optional[float] = None\n+    best_global_step: Optional[int] = None\n     best_model_checkpoint: Optional[str] = None\n     is_local_process_zero: bool = True\n     is_world_process_zero: bool = True"
        },
        {
            "sha": "c4c90d5dcbb4c57760bb9c1cabf32eb07113f841",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 187,
            "deletions": 0,
            "changes": 187,
            "blob_url": "https://github.com/huggingface/transformers/blob/691d1b52c33b0bda5187dc5afad8a30633003bb2/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/691d1b52c33b0bda5187dc5afad8a30633003bb2/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=691d1b52c33b0bda5187dc5afad8a30633003bb2",
            "patch": "@@ -62,8 +62,10 @@\n     TemporaryHubRepo,\n     TestCasePlus,\n     backend_device_count,\n+    evaluate_side_effect_factory,\n     execute_subprocess_async,\n     get_gpu_count,\n+    get_steps_per_epoch,\n     get_tests_dir,\n     is_staging_test,\n     require_accelerate,\n@@ -4710,6 +4712,191 @@ def test_metric_for_best_model_behavior(self):\n             )\n             self.assertTrue(trainer.args.metric_for_best_model == \"loss\")\n \n+    def test_best_model_checkpoint_behavior(self):\n+        # Case 1. Never evaluated, save_total_limit > 1 and save_steps == 1.\n+        # Both best_metric and best_model_checkpoint should be None.\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            trainer = get_regression_trainer(\n+                output_dir=tmpdir,\n+                eval_strategy=\"steps\",\n+                save_strategy=\"steps\",\n+                save_steps=1,\n+                metric_for_best_model=\"accuracy\",\n+                greater_is_better=True,\n+            )\n+            trainer.train()\n+\n+            assert trainer.state.best_metric is None\n+            assert trainer.state.best_model_checkpoint is None\n+            assert len(os.listdir(tmpdir)) == trainer.state.global_step\n+\n+        # Case 2. Never evaluated and save_total_limit == 1.\n+        # Both best_metric and best_model_checkpoint should be None.\n+        # Only the last checkpoint should remain.\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            trainer = get_regression_trainer(\n+                output_dir=tmpdir,\n+                eval_strategy=\"steps\",\n+                save_strategy=\"steps\",\n+                save_steps=1,\n+                metric_for_best_model=\"accuracy\",\n+                greater_is_better=True,\n+                save_total_limit=1,\n+            )\n+            trainer.train()\n+\n+            num_steps = trainer.state.global_step\n+\n+            assert trainer.state.best_metric is None\n+            assert trainer.state.best_model_checkpoint is None\n+            assert len(os.listdir(tmpdir)) == 1\n+\n+            ckpt = os.path.join(tmpdir, f\"{PREFIX_CHECKPOINT_DIR}-{num_steps}\")\n+            assert os.path.isdir(ckpt)\n+            assert os.listdir(tmpdir)[0] == f\"{PREFIX_CHECKPOINT_DIR}-{num_steps}\"\n+\n+        # Case 3. eval_strategy == save_strategy.\n+        # best_model_checkpoint should be at epoch 1.\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            trainer = get_regression_trainer(\n+                output_dir=tmpdir,\n+                eval_strategy=\"epoch\",\n+                save_strategy=\"epoch\",\n+                metric_for_best_model=\"accuracy\",\n+                compute_metrics=AlmostAccuracy(),\n+                greater_is_better=True,\n+                load_best_model_at_end=False,\n+            )\n+\n+            with patch.object(\n+                trainer,\n+                \"_evaluate\",\n+                side_effect=evaluate_side_effect_factory(\n+                    [\n+                        {\"eval_accuracy\": 0.59},\n+                        {\"eval_accuracy\": 0.57},\n+                        {\"eval_accuracy\": 0.55},\n+                    ]\n+                ),\n+            ):\n+                trainer.train()\n+\n+            steps_per_epoch = get_steps_per_epoch(trainer)\n+\n+            assert trainer.state.best_metric == 0.59\n+            assert trainer.state.best_global_step == steps_per_epoch\n+\n+            best_ckpt = os.path.join(tmpdir, f\"{PREFIX_CHECKPOINT_DIR}-{trainer.state.best_global_step}\")\n+            assert trainer.state.best_model_checkpoint == best_ckpt\n+\n+            assert len(os.listdir(tmpdir)) == trainer.state.num_train_epochs\n+\n+        # Case 4. eval_strategy != save_strategy.\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            trainer = get_regression_trainer(\n+                output_dir=tmpdir,\n+                eval_strategy=\"epoch\",\n+                save_strategy=\"steps\",\n+                save_steps=1,\n+                metric_for_best_model=\"accuracy\",\n+                compute_metrics=AlmostAccuracy(),\n+                greater_is_better=True,\n+                load_best_model_at_end=False,\n+            )\n+\n+            with patch.object(\n+                trainer,\n+                \"_evaluate\",\n+                side_effect=evaluate_side_effect_factory(\n+                    [\n+                        {\"eval_accuracy\": 0.59},\n+                        {\"eval_accuracy\": 0.57},\n+                        {\"eval_accuracy\": 0.55},\n+                    ]\n+                ),\n+            ):\n+                trainer.train()\n+\n+            steps_per_epoch = get_steps_per_epoch(trainer)\n+\n+            assert trainer.state.best_metric == 0.59\n+            assert trainer.state.best_global_step == steps_per_epoch\n+\n+            best_ckpt = os.path.join(tmpdir, f\"{PREFIX_CHECKPOINT_DIR}-{trainer.state.best_global_step}\")\n+            assert trainer.state.best_model_checkpoint == best_ckpt\n+\n+            assert len(os.listdir(tmpdir)) == trainer.state.global_step\n+\n+        # Case 5. Multiple checkpoints, save_total_limit == 1.\n+        # Best metric is found at step 1 and that checkpoint should be saved.\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            trainer = get_regression_trainer(\n+                output_dir=tmpdir,\n+                eval_strategy=\"steps\",\n+                eval_steps=1,\n+                save_strategy=\"steps\",\n+                save_steps=1,\n+                metric_for_best_model=\"accuracy\",\n+                compute_metrics=AlmostAccuracy(),\n+                greater_is_better=True,\n+                save_total_limit=1,\n+            )\n+\n+            with patch.object(\n+                trainer,\n+                \"_evaluate\",\n+                side_effect=evaluate_side_effect_factory(\n+                    [\n+                        {\"eval_accuracy\": 0.90},\n+                        {\"eval_accuracy\": 0.80},\n+                        {\"eval_accuracy\": 0.70},\n+                    ]\n+                ),\n+            ):\n+                trainer.train()\n+\n+            assert trainer.state.best_metric == 0.90\n+            assert trainer.state.best_global_step == 1\n+\n+            best_ckpt = os.path.join(tmpdir, f\"{PREFIX_CHECKPOINT_DIR}-{trainer.state.best_global_step}\")\n+            assert trainer.state.best_model_checkpoint == best_ckpt\n+\n+            assert len(os.listdir(tmpdir)) == 1\n+\n+        # Case 6. Saving happens more often and eval/save mismatch.\n+        # `best_model_checkpoint` should be None due to a step mismatch.\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            trainer = get_regression_trainer(\n+                output_dir=tmpdir,\n+                eval_strategy=\"steps\",\n+                eval_steps=3,\n+                save_strategy=\"steps\",\n+                save_steps=2,\n+                metric_for_best_model=\"accuracy\",\n+                compute_metrics=AlmostAccuracy(),\n+                greater_is_better=True,\n+            )\n+\n+            with patch.object(\n+                trainer,\n+                \"_evaluate\",\n+                side_effect=evaluate_side_effect_factory(\n+                    [\n+                        {\"eval_accuracy\": 0.90},\n+                        {\"eval_accuracy\": 0.80},\n+                        {\"eval_accuracy\": 0.70},\n+                    ]\n+                ),\n+            ):\n+                trainer.train()\n+\n+            assert trainer.state.best_metric == 0.90\n+            assert trainer.state.best_global_step == 3\n+\n+            assert trainer.state.best_model_checkpoint is None\n+\n+            assert len(os.listdir(tmpdir)) == trainer.state.global_step // 2\n+\n \n @require_torch\n @is_staging_test"
        }
    ],
    "stats": {
        "total": 237,
        "additions": 231,
        "deletions": 6
    }
}