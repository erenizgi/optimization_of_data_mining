{
    "author": "yonigozlan",
    "message": "[SAM2] Fix inconsistent results with original implementation with input boxes (#40800)\n\n* Fix inconsistencies with box input inference with original repo\n\n* remove print\n\n* always pad\n\n* fix modular",
    "sha": "f384bb8ad5eb21fa87ab31faa355c79aae0b925e",
    "files": [
        {
            "sha": "c05a3019c279711a775ccf80273da18d2f0665df",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f384bb8ad5eb21fa87ab31faa355c79aae0b925e/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f384bb8ad5eb21fa87ab31faa355c79aae0b925e/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=f384bb8ad5eb21fa87ab31faa355c79aae0b925e",
            "patch": "@@ -960,9 +960,8 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ) -> MetaClip2Output:\n         r\"\"\"\n-        Args:\n-            return_loss (`bool`, *optional*):\n-                Whether or not to return the contrastive loss.\n+        return_loss (`bool`, *optional*):\n+            Whether or not to return the contrastive loss.\n \n         Examples:\n "
        },
        {
            "sha": "4d5a536ab93f963c1debdeae3a096eef6c6e4091",
            "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f384bb8ad5eb21fa87ab31faa355c79aae0b925e/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f384bb8ad5eb21fa87ab31faa355c79aae0b925e/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py?ref=f384bb8ad5eb21fa87ab31faa355c79aae0b925e",
            "patch": "@@ -551,9 +551,8 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ):\n         r\"\"\"\n-        Args:\n-            return_loss (`bool`, *optional*):\n-                Whether or not to return the contrastive loss.\n+        return_loss (`bool`, *optional*):\n+            Whether or not to return the contrastive loss.\n \n         Examples:\n "
        },
        {
            "sha": "ef16466d344cffe17233c177e7ee1647c3f6a64a",
            "filename": "src/transformers/models/sam2/modeling_sam2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f384bb8ad5eb21fa87ab31faa355c79aae0b925e/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f384bb8ad5eb21fa87ab31faa355c79aae0b925e/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py?ref=f384bb8ad5eb21fa87ab31faa355c79aae0b925e",
            "patch": "@@ -793,13 +793,14 @@ def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -\n \n     def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n         \"\"\"Embeds box prompts.\"\"\"\n-        boxes = boxes + 0.5  # Shift to center of pixel\n-        batch_size, nb_boxes = boxes.shape[:2]\n-        coords = boxes.reshape(batch_size, nb_boxes, 2, 2)\n-        input_shape = (self.input_image_size, self.input_image_size)\n-        corner_embedding = self.shared_embedding(coords, input_shape)\n+        boxes += 0.5  # Shift to center of pixel\n+        coords = boxes.view(*boxes.shape[:2], 2, 2)\n+        # add padding point for consistency with the original implementation\n+        coords = torch.nn.functional.pad(coords, (0, 0, 0, 1), mode=\"constant\", value=0)\n+        corner_embedding = self.shared_embedding(coords, (self.input_image_size, self.input_image_size))\n         corner_embedding[:, :, 0, :] += self.point_embed.weight[2]\n         corner_embedding[:, :, 1, :] += self.point_embed.weight[3]\n+        corner_embedding[:, :, 2, :] = self.not_a_point_embed.weight.expand_as(corner_embedding[:, :, 2, :])\n         return corner_embedding\n \n     def forward("
        },
        {
            "sha": "5fff232a839ce8c903423f96651945c3e1dfedba",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f384bb8ad5eb21fa87ab31faa355c79aae0b925e/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f384bb8ad5eb21fa87ab31faa355c79aae0b925e/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=f384bb8ad5eb21fa87ab31faa355c79aae0b925e",
            "patch": "@@ -882,13 +882,14 @@ def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -\n \n     def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n         \"\"\"Embeds box prompts.\"\"\"\n-        boxes = boxes + 0.5  # Shift to center of pixel\n-        batch_size, nb_boxes = boxes.shape[:2]\n-        coords = boxes.reshape(batch_size, nb_boxes, 2, 2)\n-        input_shape = (self.input_image_size, self.input_image_size)\n-        corner_embedding = self.shared_embedding(coords, input_shape)\n+        boxes += 0.5  # Shift to center of pixel\n+        coords = boxes.view(*boxes.shape[:2], 2, 2)\n+        # add padding point for consistency with the original implementation\n+        coords = torch.nn.functional.pad(coords, (0, 0, 0, 1), mode=\"constant\", value=0)\n+        corner_embedding = self.shared_embedding(coords, (self.input_image_size, self.input_image_size))\n         corner_embedding[:, :, 0, :] += self.point_embed.weight[2]\n         corner_embedding[:, :, 1, :] += self.point_embed.weight[3]\n+        corner_embedding[:, :, 2, :] = self.not_a_point_embed.weight.expand_as(corner_embedding[:, :, 2, :])\n         return corner_embedding\n \n "
        },
        {
            "sha": "f4c1261d67794e46f3a195b6e82ddecc045ee9e4",
            "filename": "src/transformers/models/sam2_video/modeling_sam2_video.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f384bb8ad5eb21fa87ab31faa355c79aae0b925e/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f384bb8ad5eb21fa87ab31faa355c79aae0b925e/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py?ref=f384bb8ad5eb21fa87ab31faa355c79aae0b925e",
            "patch": "@@ -1224,13 +1224,14 @@ def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -\n \n     def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n         \"\"\"Embeds box prompts.\"\"\"\n-        boxes = boxes + 0.5  # Shift to center of pixel\n-        batch_size, nb_boxes = boxes.shape[:2]\n-        coords = boxes.reshape(batch_size, nb_boxes, 2, 2)\n-        input_shape = (self.input_image_size, self.input_image_size)\n-        corner_embedding = self.shared_embedding(coords, input_shape)\n+        boxes += 0.5  # Shift to center of pixel\n+        coords = boxes.view(*boxes.shape[:2], 2, 2)\n+        # add padding point for consistency with the original implementation\n+        coords = torch.nn.functional.pad(coords, (0, 0, 0, 1), mode=\"constant\", value=0)\n+        corner_embedding = self.shared_embedding(coords, (self.input_image_size, self.input_image_size))\n         corner_embedding[:, :, 0, :] += self.point_embed.weight[2]\n         corner_embedding[:, :, 1, :] += self.point_embed.weight[3]\n+        corner_embedding[:, :, 2, :] = self.not_a_point_embed.weight.expand_as(corner_embedding[:, :, 2, :])\n         return corner_embedding\n \n     def forward("
        },
        {
            "sha": "a6584f034064772c4712ce364086f0d3c48a62bb",
            "filename": "tests/models/sam2/test_modeling_sam2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/f384bb8ad5eb21fa87ab31faa355c79aae0b925e/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f384bb8ad5eb21fa87ab31faa355c79aae0b925e/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py?ref=f384bb8ad5eb21fa87ab31faa355c79aae0b925e",
            "patch": "@@ -901,7 +901,7 @@ def test_inference_batched_images_batched_boxes(self):\n         self.assertEqual(outputs.pred_masks.shape, (2, 4, 1, 256, 256))\n         torch.testing.assert_close(\n             outputs.iou_scores,\n-            torch.tensor([[[0.9873], [0.9264], [0.9496], [0.9208]], [[0.9445], [0.9496], [0.9497], [0.9481]]]).to(\n+            torch.tensor([[[0.9904], [0.9689], [0.9770], [0.9079]], [[0.9739], [0.9816], [0.9838], [0.9781]]]).to(\n                 torch_device\n             ),\n             atol=1e-4,\n@@ -912,16 +912,16 @@ def test_inference_batched_images_batched_boxes(self):\n             torch.tensor(\n                 [\n                     [\n-                        [[[-7.6204, -11.9286], [-8.7747, -10.5662]]],\n-                        [[[-17.1070, -23.4025], [-20.9608, -19.5600]]],\n-                        [[[-20.5766, -29.4410], [-26.0739, -24.3225]]],\n-                        [[[-19.7201, -29.0836], [-24.4915, -23.6377]]],\n+                        [[[-11.1540, -18.3994], [-12.4230, -17.4403]]],\n+                        [[[-19.3144, -29.3947], [-24.6341, -24.1144]]],\n+                        [[[-24.2983, -37.6470], [-31.6659, -31.0893]]],\n+                        [[[-25.4313, -44.0231], [-34.0903, -34.7447]]],\n                     ],\n                     [\n-                        [[[-18.5259, -23.5202], [-25.1906, -17.2518]]],\n-                        [[[-20.1214, -25.4215], [-25.7877, -19.1169]]],\n-                        [[[-21.0878, -24.7938], [-27.5625, -19.2650]]],\n-                        [[[-20.5210, -22.5343], [-26.0968, -17.7544]]],\n+                        [[[-22.5539, -30.4633], [-32.8940, -21.6813]]],\n+                        [[[-23.6637, -31.3489], [-32.5095, -22.4442]]],\n+                        [[[-25.2987, -30.9999], [-34.6243, -24.1717]]],\n+                        [[[-26.3150, -30.5313], [-35.0152, -24.0271]]],\n                     ],\n                 ]\n             ).to(torch_device),"
        }
    ],
    "stats": {
        "total": 61,
        "additions": 31,
        "deletions": 30
    }
}