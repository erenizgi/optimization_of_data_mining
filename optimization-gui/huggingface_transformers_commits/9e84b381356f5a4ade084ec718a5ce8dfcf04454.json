{
    "author": "threewebcode",
    "message": "chore: enhance message descriptions in parameters,comments,logs and docstrings (#36554)\n\n* chore: enhance message descriptons in parameters,comments,logs and docstrings\n\n* chore: enhance message descriptons in parameters,comments,logs and docstrings\n\n* Update src/transformers/hf_argparser.py\n\n* Update src/transformers/keras_callbacks.py\n\n---------\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>",
    "sha": "9e84b381356f5a4ade084ec718a5ce8dfcf04454",
    "files": [
        {
            "sha": "3e7effc4abf895b2a89c11ee79af0d63ec2b9941",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -191,7 +191,7 @@ class PretrainedConfig(PushToHubMixin):\n             v5.\n         loss_type (`str`, *optional*):\n             The type of loss that the model should use. It should be in `LOSS_MAPPING`'s keys, otherwise the loss will\n-            be automatically infered from the model architecture.\n+            be automatically inferred from the model architecture.\n     \"\"\"\n \n     model_type: str = \"\"\n@@ -254,7 +254,7 @@ def __init__(self, **kwargs):\n             if num_labels is not None and len(self.id2label) != num_labels:\n                 logger.warning(\n                     f\"You passed along `num_labels={num_labels}` with an incompatible id to label map: \"\n-                    f\"{self.id2label}. The number of labels wil be overwritten to {self.num_labels}.\"\n+                    f\"{self.id2label}. The number of labels will be overwritten to {self.num_labels}.\"\n                 )\n             self.id2label = {int(key): value for key, value in self.id2label.items()}\n             # Keys are always strings in JSON so convert ids to int here.\n@@ -1094,7 +1094,7 @@ def _get_non_default_generation_parameters(self) -> Dict[str, Any]:\n                 is_default_in_config = is_default_generation_value = None\n                 parameter_value = getattr(self_decoder_config, parameter_name)\n                 # Three cases in which is okay for the model config to hold generation config parameters:\n-                # 1. The parameter is set to `None`, effectivelly delegating its value to the generation config\n+                # 1. The parameter is set to `None`, effectively delegating its value to the generation config\n                 if parameter_value is None:\n                     continue\n                 # 2. If we have a default config, then the instance should hold the same generation defaults"
        },
        {
            "sha": "33c87bb35bea8a0bb6cfd64c35b750b6aabdaff4",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -1727,5 +1727,5 @@ def convert_slow_tokenizer(transformer_tokenizer, from_tiktoken=False) -> Tokeni\n             raise ValueError(\n                 f\"Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path \"\n                 f\"with a SentencePiece tokenizer.model file.\"\n-                f\"Currently available slow->fast convertors: {list(SLOW_TO_FAST_CONVERTERS.keys())}\"\n+                f\"Currently available slow->fast converters: {list(SLOW_TO_FAST_CONVERTERS.keys())}\"\n             )"
        },
        {
            "sha": "627869551d482ec0f3b53e1dc7d8b61626ae87db",
            "filename": "src/transformers/hf_argparser.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fhf_argparser.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fhf_argparser.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fhf_argparser.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -201,7 +201,7 @@ def _parse_dataclass_field(parser: ArgumentParser, field: dataclasses.Field):\n             else:\n                 kwargs[\"required\"] = True\n         elif field.type is bool or field.type == Optional[bool]:\n-            # Copy the currect kwargs to use to instantiate a `no_*` complement argument below.\n+            # Copy the correct kwargs to use to instantiate a `no_*` complement argument below.\n             # We do not initialize it here because the `no_*` alternative must be instantiated after the real argument\n             bool_kwargs = copy(kwargs)\n "
        },
        {
            "sha": "22f0b8e62c8f752e1f608515a5c9c2d4d2f3a5f3",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -585,7 +585,7 @@ def center_to_corners_format(bboxes_center: TensorType) -> TensorType:\n \n     center format: contains the coordinate for the center of the box and its width, height dimensions\n         (center_x, center_y, width, height)\n-    corners format: contains the coodinates for the top-left and bottom-right corners of the box\n+    corners format: contains the coordinates for the top-left and bottom-right corners of the box\n         (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n     \"\"\"\n     # Function is used during model forward pass, so we use the input framework if possible, without"
        },
        {
            "sha": "fec1e9dbc0969c5c1bd63066d5fc65b820aadad0",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -545,7 +545,7 @@ def default_sample_indices_fn(metadata: VideoMetadata, num_frames=None, fps=None\n \n     Args:\n         metadata (`VideoMetadata`):\n-            `VideoMetadata` object containing metadat about the video, such as \"total_num_frames\" or \"fps\".\n+            `VideoMetadata` object containing metadata about the video, such as \"total_num_frames\" or \"fps\".\n         num_frames (`int`, *optional*):\n             Number of frames to sample uniformly.\n         fps (`int`, *optional*):"
        },
        {
            "sha": "e335027e76f800c2469f57d4ef41ca2e89d29bc3",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -137,9 +137,9 @@ def prepare_fa2_from_position_ids(query, key, value, position_ids):\n     \"\"\"\n     This function returns necessary arguments to call `flash_attn_varlen_func`.\n     All three query, key, value states will be flattened.\n-    Cummulative lengths of each examples in the batch will be extracted from position_ids.\n+    Cumulative lengths of each examples in the batch will be extracted from position_ids.\n \n-    NOTE: ideally cummulative lengths should be prepared at the data collator stage\n+    NOTE: ideally cumulative lengths should be prepared at the data collator stage\n \n     Arguments:\n         query (`torch.Tensor`):\n@@ -268,7 +268,7 @@ def _flash_attention_forward(\n         softmax_scale (`float`, *optional*):\n             The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n         use_top_left_mask (`bool`, defaults to `False`):\n-            flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference.\n+            flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference.\n         softcap (`float`, *optional*):\n             Softcap for the attention logits, used e.g. in gemma2.\n         deterministic (`bool`, *optional*):\n@@ -374,9 +374,9 @@ class FlashAttentionKwargs(TypedDict, total=False):\n \n     Attributes:\n         cu_seq_lens_q (`torch.LongTensor`, *optional*)\n-            Gets cumlative sequence length for query state.\n+            Gets cumulative sequence length for query state.\n         cu_seq_lens_k (`torch.LongTensor`, *optional*)\n-            Gets cumlative sequence length for key state.\n+            Gets cumulative sequence length for key state.\n         max_length_q (`int`, *optional*):\n             Maximum sequence length for query state.\n         max_length_k (`int`, *optional*):"
        },
        {
            "sha": "273243db4a91cba78d05f7864ffc946624129ef5",
            "filename": "src/transformers/modeling_flax_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_utils.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -367,7 +367,7 @@ def to_bf16(self, params: Union[Dict, FrozenDict], mask: Any = None):\n \n     def to_fp32(self, params: Union[Dict, FrozenDict], mask: Any = None):\n         r\"\"\"\n-        Cast the floating-point `parmas` to `jax.numpy.float32`. This method can be used to explicitly convert the\n+        Cast the floating-point `params` to `jax.numpy.float32`. This method can be used to explicitly convert the\n         model parameters to fp32 precision. This returns a new `params` tree and does not cast the `params` in place.\n \n         Arguments:\n@@ -394,7 +394,7 @@ def to_fp32(self, params: Union[Dict, FrozenDict], mask: Any = None):\n \n     def to_fp16(self, params: Union[Dict, FrozenDict], mask: Any = None):\n         r\"\"\"\n-        Cast the floating-point `parmas` to `jax.numpy.float16`. This returns a new `params` tree and does not cast the\n+        Cast the floating-point `params` to `jax.numpy.float16`. This returns a new `params` tree and does not cast the\n         `params` in place.\n \n         This method can be used on GPU to explicitly convert the model parameters to float16 precision to do full\n@@ -510,7 +510,7 @@ def can_generate(cls) -> bool:\n             `bool`: Whether this model can generate sequences with `.generate()`.\n         \"\"\"\n         # Detects whether `prepare_inputs_for_generation` has been overwritten, which is a requirement for generation.\n-        # Alternativelly, the model can also have a custom `generate` function.\n+        # Alternatively, the model can also have a custom `generate` function.\n         if \"GenerationMixin\" in str(cls.prepare_inputs_for_generation) and \"GenerationMixin\" in str(cls.generate):\n             return False\n         return True\n@@ -968,7 +968,7 @@ def from_pretrained(\n             )\n             cls._missing_keys = missing_keys\n \n-        # Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\n+        # Mismatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\n         # matching the weights in the model.\n         mismatched_keys = []\n         for key in state.keys():"
        },
        {
            "sha": "0f21f80e04d423e89be0f924ed1d7406b15a2e74",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -373,7 +373,7 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n     # to add this patch to ensure things work correctly on our side.\n     if \"llama\" in architecture and \"mistral\" in model_name:\n         updated_architecture = \"mistral\"\n-    # FIXME: Currnetly this implementation is only for flan-t5 architecture.\n+    # FIXME: Currently this implementation is only for flan-t5 architecture.\n     # It needs to be developed for supporting legacy t5.\n     elif \"t5\" in architecture or \"t5encoder\" in architecture:\n         parsed_parameters[\"config\"][\"is_gated_act\"] = True\n@@ -437,7 +437,7 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n             logger.info(f\"Some keys were not parsed and added into account {gguf_key} | {value}\")\n \n     # retrieve config vocab_size from tokenizer\n-    # Pleas refer to https://github.com/huggingface/transformers/issues/32526 for more details\n+    # Please refer to https://github.com/huggingface/transformers/issues/32526 for more details\n     if \"vocab_size\" not in parsed_parameters[\"config\"]:\n         tokenizer_parameters = parsed_parameters[\"tokenizer\"]\n         if \"tokens\" in tokenizer_parameters:"
        },
        {
            "sha": "b6632978fe7477fbb66c91db258df64904c671a7",
            "filename": "src/transformers/modeling_tf_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_utils.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -795,7 +795,7 @@ def load_tf_shard(model, model_layer_map, resolved_archive_file, ignore_mismatch\n         ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`): Whether to ignore the mismatched keys\n \n     Returns:\n-        `keras.models.Model`: Three lists, one for the layers that were found and succesfully restored (from the\n+        `keras.models.Model`: Three lists, one for the layers that were found and successfully restored (from the\n         shard file), one for the mismatched layers, and another one for the unexpected layers.\n     \"\"\"\n     saved_weight_names_set = set()\n@@ -868,7 +868,7 @@ def load_tf_shard(model, model_layer_map, resolved_archive_file, ignore_mismatch\n                 f\"Unable to load weights from TF checkpoint file for '{resolved_archive_file}' \"\n                 f\"at '{resolved_archive_file}'. \"\n                 \"If you tried to load a TF model from a sharded checkpoint, you should try converting the model \"\n-                \"by loading it in pytorch and saving it localy. A convertion script should be realeased soon.\"\n+                \"by loading it in pytorch and saving it locally. A convertion script should be released soon.\"\n             )\n \n \n@@ -1391,7 +1391,7 @@ def can_generate(cls) -> bool:\n             `bool`: Whether this model can generate sequences with `.generate()`.\n         \"\"\"\n         # Detects whether `prepare_inputs_for_generation` has been overwritten, which is a requirement for generation.\n-        # Alternativelly, the model can also have a custom `generate` function.\n+        # Alternatively, the model can also have a custom `generate` function.\n         if \"GenerationMixin\" in str(cls.prepare_inputs_for_generation) and \"GenerationMixin\" in str(cls.generate):\n             return False\n         return True"
        },
        {
            "sha": "45a6915211d0d6886edbffc7d4d9b69cf3a34007",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -1324,7 +1324,7 @@ def _find_mismatched_keys(\n                     and state_dict[checkpoint_key].numel() * 2 == model_state_dict[model_key].numel()\n                 ):\n                     # This skips size mismatches for 4-bit weights. Two 4-bit values share an 8-bit container, causing size differences.\n-                    # Without matching with module type or paramter type it seems like a practical way to detect valid 4bit weights.\n+                    # Without matching with module type or parameter type it seems like a practical way to detect valid 4bit weights.\n                     pass\n                 else:\n                     mismatched_keys.append(\n@@ -1616,7 +1616,7 @@ def _autoset_attn_implementation(\n             3. SDPA implementation, if available and supported by the model type. (`LlamaSdpaAttention` for example)\n             4. The default model's implementation otherwise (`LlamaAttention` for example) .\n         \"\"\"\n-        # Here we use config._attn_implementation_internal to check whether the attention implementation was explicitely set by the user.\n+        # Here we use config._attn_implementation_internal to check whether the attention implementation was explicitly set by the user.\n         # The property `PretrainedConfig._attn_implementation` is never `None`, for backward compatibility (always fall back on \"eager\").\n         # The `hasattr` here is used as some Transformers tests for some reason do not call PretrainedConfig __init__ (e.g. test_no_super_init_config_and_model)\n         requested_attn_implementation = None\n@@ -2207,7 +2207,7 @@ def resize_token_embeddings(\n         if new_num_tokens is None and pad_to_multiple_of is None:\n             return model_embeds\n \n-        # Since we are basically resuing the same old embeddings with new weight values, gathering is required\n+        # Since we are basically reusing the same old embeddings with new weight values, gathering is required\n         is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n             import deepspeed\n@@ -2574,7 +2574,7 @@ def _init_added_embeddings_weights_with_mean(\n                 sample_shape=(added_num_tokens,)\n             ).to(old_embeddings.weight.dtype)\n         else:\n-            # Otherwise, just initialize with the mean. because distribtion will not be created.\n+            # Otherwise, just initialize with the mean. because distribution will not be created.\n             new_embeddings.weight.data[-1 * added_num_tokens :, :] = (\n                 mean_embeddings[None, :].repeat(added_num_tokens, 1).to(old_embeddings.weight.dtype)\n             )\n@@ -2593,7 +2593,7 @@ def _init_added_lm_head_weights_with_mean(\n             new_lm_head.weight.data = new_lm_head.weight.data.T\n             old_lm_head.weight.data = old_lm_head.weight.data.T\n \n-        # The same initilization logic as Embeddings.\n+        # The same initialization logic as Embeddings.\n         self._init_added_embeddings_weights_with_mean(\n             old_lm_head, new_lm_head, old_lm_head_dim, old_num_tokens, added_num_tokens\n         )\n@@ -2740,7 +2740,7 @@ def gradient_checkpointing_disable(self):\n         \"\"\"\n         if self.supports_gradient_checkpointing:\n             # For old GC format (transformers < 4.35.0) for models that live on the Hub\n-            # we will fall back to the overwritten `_set_gradient_checkpointing` methid\n+            # we will fall back to the overwritten `_set_gradient_checkpointing` method\n             _is_using_old_format = \"value\" in inspect.signature(self._set_gradient_checkpointing).parameters\n             if not _is_using_old_format:\n                 self._set_gradient_checkpointing(enable=False)\n@@ -2979,7 +2979,7 @@ def save_pretrained(\n                 if ignore_key in state_dict.keys():\n                     del state_dict[ignore_key]\n \n-        # Rename state_dict keys before saving to file. Do nothing unless overriden in a particular model.\n+        # Rename state_dict keys before saving to file. Do nothing unless overridden in a particular model.\n         # (initially introduced with TimmWrapperModel to remove prefix and make checkpoints compatible with timm)\n         state_dict = self._fix_state_dict_keys_on_save(state_dict)\n \n@@ -4998,7 +4998,7 @@ def _load_pretrained_model(\n                     shard_file, is_quantized=is_quantized, map_location=\"meta\", weights_only=weights_only\n                 )\n \n-                # Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\n+                # Mismatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\n                 # matching the weights in the model.\n                 mismatched_keys += _find_mismatched_keys(\n                     state_dict,\n@@ -5321,13 +5321,13 @@ def tensor_parallel(self, device_mesh):\n         \"\"\"\n         Tensor parallelize the model across the given device mesh. This function is a helper to be called after the model\n         was already loaded in memory, note however that this means that each process will first initialize the whole model,\n-        then parallelize it accross devices. Thus there is a huge waste of GPU memory, and this can lead to OOM at loading time.\n+        then parallelize it across devices. Thus there is a huge waste of GPU memory, and this can lead to OOM at loading time.\n \n-        Calling `from_pretrained(..., tp_plan=\"auto\")` is prefered, and will parallelize module-by-module during initialization,\n+        Calling `from_pretrained(..., tp_plan=\"auto\")` is preferred, and will parallelize module-by-module during initialization,\n         so that the expected per-device memory spike at loading time is not larger than the final model size on each device.\n         Tensor parallelize the model across the given device mesh. This function is a helper to be called after the model\n         was already loaded in memory, note however that this means that each process will first initialize the whole model,\n-        then parallelize it accross devices. Thus there is a huge waste of GPU memory, and this can lead to OOM at loading time.\n+        then parallelize it across devices. Thus there is a huge waste of GPU memory, and this can lead to OOM at loading time.\n \n         Args:\n             device_mesh (`torch.distributed.DeviceMesh`):\n@@ -5869,7 +5869,7 @@ def unwrap_model(model: nn.Module, recursive: bool = False) -> nn.Module:\n \n def expand_device_map(device_map, param_names, start_prefix):\n     \"\"\"\n-    Expand a device map to return the correspondance parameter name to device.\n+    Expand a device map to return the correspondence parameter name to device.\n     \"\"\"\n     new_device_map = {}\n     param_names = [p[len(start_prefix) :] for p in param_names if p.startswith(start_prefix)]"
        },
        {
            "sha": "614cbe8d76b2eac9b18b56522f0d38dadba23e93",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -901,7 +901,7 @@ def _merge_kwargs(\n                 ```python\n                 tokenizer = tokenizer_class(..., {\"padding\": \"max_length\"})\n                 image_processor = image_processor_class(...)\n-                processor(tokenizer, image_processor) # will pass max_length unless overriden by kwargs at call\n+                processor(tokenizer, image_processor) # will pass max_length unless overridden by kwargs at call\n                 ```\n             4) defaults kwargs specified at processor level have lowest priority.\n                 ```python\n@@ -1205,7 +1205,7 @@ def _process_messages_for_chat_template(\n         video models might want to specify in the prompt the duration of video or which frame indices at which timestamps\n         were sampled. This information cannot be accessed before the video is loaded.\n \n-        For most models it is a no-op, and must be overriden by model processors which require special processing.\n+        For most models it is a no-op, and must be overridden by model processors which require special processing.\n \n         Args:\n             conversation (`List[Dict, str, str]`):\n@@ -1372,7 +1372,7 @@ def apply_chat_template(\n         if tokenize:\n             # Tokenizer's `apply_chat_template` never adds special tokens when tokenizing\n             # But processor's `apply_chat_template` didn't have an option to tokenize, so users had to format the prompt\n-            # and pass it to the processor. Users thus never worried about special tokens relying on processor hadnling\n+            # and pass it to the processor. Users thus never worried about special tokens relying on processor handling\n             # everything internally. The below line is to keep BC for that and be able to work with model that have\n             # special tokens in the template (consistent with tokenizers). We dont want to raise warning, it will flood command line\n             # without actionable solution for users"
        },
        {
            "sha": "4ca7822fa5d09db0f4919cee030ca85d5ee69bee",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -2407,7 +2407,7 @@ class SubprocessCallException(Exception):\n def run_command(command: List[str], return_stdout=False):\n     \"\"\"\n     Runs `command` with `subprocess.check_output` and will potentially return the `stdout`. Will also properly capture\n-    if an error occured while running `command`\n+    if an error occurred while running `command`\n     \"\"\"\n     try:\n         output = subprocess.check_output(command, stderr=subprocess.STDOUT)\n@@ -2541,7 +2541,7 @@ def wrapper(*args, **kwargs):\n                     requests.exceptions.RequestException,\n                 ) as err:\n                     logger.error(\n-                        f\"Test failed with {err} at try {retry_count}/{max_attempts} as it couldn't connect to the specied Hub repository.\"\n+                        f\"Test failed with {err} at try {retry_count}/{max_attempts} as it couldn't connect to the specified Hub repository.\"\n                     )\n                     if wait_before_retry is not None:\n                         time.sleep(wait_before_retry)\n@@ -2661,7 +2661,7 @@ def wrapper(*args, **kwargs):\n The following contains utils to run the documentation tests without having to overwrite any files.\n \n The `preprocess_string` function adds `# doctest: +IGNORE_RESULT` markers on the fly anywhere a `load_dataset` call is\n-made as a print would otherwise fail the corresonding line.\n+made as a print would otherwise fail the corresponding line.\n \n To skip cuda tests, make sure to call `SKIP_CUDA_DOCTEST=1 pytest --doctest-modules <path_to_files_to_test>\n \"\"\""
        },
        {
            "sha": "d20522fc15a485240f84c598d046ddea929bd507",
            "filename": "src/transformers/tokenization_utils_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_fast.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -708,7 +708,7 @@ def _save_pretrained(\n             added_tokens_file = os.path.join(\n                 save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + ADDED_TOKENS_FILE\n             )\n-            # make sure to be foward compatible\n+            # make sure to be forward compatible\n             added_vocab = {tok: index for tok, index in self.added_tokens_encoder.items() if index >= self.vocab_size}\n             if added_vocab:\n                 with open(added_tokens_file, \"w\", encoding=\"utf-8\") as f:"
        },
        {
            "sha": "0eca0f40d0ae6c03129437db7da049675ed4ad3f",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -2266,7 +2266,7 @@ def _inner_training_loop(\n                 (self.model_wrapped,) = release_memory(self.model_wrapped)\n                 self.model_wrapped = self.model\n \n-                # Check for DeepSpeed *after* the intial pass and modify the config\n+                # Check for DeepSpeed *after* the initial pass and modify the config\n                 if self.is_deepspeed_enabled:\n                     # Temporarily unset `self.args.train_batch_size`\n                     original_bs = self.args.per_device_train_batch_size\n@@ -2826,7 +2826,7 @@ def _load_from_checkpoint(self, resume_from_checkpoint, model=None):\n                     # Checkpoint must have been saved with the old smp api.\n                     if hasattr(self.args, \"fp16\") and self.args.fp16 is True:\n                         logger.warning(\n-                            \"Enabling FP16 and loading from smp < 1.10 checkpoint together is not suppported.\"\n+                            \"Enabling FP16 and loading from smp < 1.10 checkpoint together is not supported.\"\n                         )\n                     state_dict = torch.load(\n                         weights_file,\n@@ -4091,7 +4091,7 @@ def evaluate(\n             A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n             dictionary also contains the epoch number which comes from the training state.\n         \"\"\"\n-        # handle multipe eval datasets\n+        # handle multiple eval datasets\n         override = eval_dataset is not None\n         eval_dataset = eval_dataset if override else self.eval_dataset\n         if isinstance(eval_dataset, dict):"
        },
        {
            "sha": "9a5eecd7824c335a89b98d1b4c03513e60ca85e8",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -88,7 +88,7 @@ class TrainerState:\n             impact the way data will be logged in TensorBoard.\n         stateful_callbacks (`List[StatefulTrainerCallback]`, *optional*):\n             Callbacks attached to the `Trainer` that should have their states be saved or restored.\n-            Relevent callbacks should implement a `state` and `from_state` function.\n+            Relevant callbacks should implement a `state` and `from_state` function.\n     \"\"\"\n \n     epoch: Optional[float] = None"
        },
        {
            "sha": "9118e9bc8197a998dda62de34e5c2ae4543854c4",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -1231,8 +1231,8 @@ class AcceleratorConfig:\n             all workers.\n         use_seedable_sampler (`bool`, *optional*, defaults to `True`):\n             Whether or not use a fully seedable random sampler ([`accelerate.data_loader.SeedableRandomSampler`]). Ensures\n-            training results are fully reproducable using a different sampling technique. While seed-to-seed results\n-            may differ, on average the differences are neglible when using multiple different seeds to compare. Should\n+            training results are fully reproducible using a different sampling technique. While seed-to-seed results\n+            may differ, on average the differences are negligible when using multiple different seeds to compare. Should\n             also be ran with [`~utils.set_seed`] for the best results.\n         gradient_accumulation_kwargs (`dict`, *optional*):\n             Additional kwargs to configure gradient accumulation, see [`accelerate.utils.GradientAccumulationPlugin`].\n@@ -1284,8 +1284,8 @@ class AcceleratorConfig:\n         default=True,\n         metadata={\n             \"help\": \"Whether or not use a fully seedable random sampler ([`accelerate.data_loader.SeedableRandomSampler`]).\"\n-            \"Ensures training results are fully reproducable using a different sampling technique. \"\n-            \"While seed-to-seed results may differ, on average the differences are neglible when using\"\n+            \"Ensures training results are fully reproducible using a different sampling technique. \"\n+            \"While seed-to-seed results may differ, on average the differences are negligible when using\"\n             \"multiple different seeds to compare. Should also be ran with [`~utils.set_seed`] for the best results.\"\n         },\n     )"
        },
        {
            "sha": "e0730dae2725bf9d50b6b1586b9a5b59ee611ce8",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e84b381356f5a4ade084ec718a5ce8dfcf04454/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=9e84b381356f5a4ade084ec718a5ce8dfcf04454",
            "patch": "@@ -542,7 +542,7 @@ class TrainingArguments:\n                      all-gathers.\n                 - use_orig_params (`bool`, *optional*, defaults to `True`)\n                     If `\"True\"`, allows non-uniform `requires_grad` during init, which means support for interspersed\n-                    frozen and trainable paramteres. Useful in cases such as parameter-efficient fine-tuning. Please\n+                    frozen and trainable parameters. Useful in cases such as parameter-efficient fine-tuning. Please\n                     refer this\n                     [blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019\n                 - sync_module_states (`bool`, *optional*, defaults to `True`)\n@@ -604,8 +604,8 @@ class TrainingArguments:\n                     all workers.\n                 - use_seedable_sampler (`bool`, *optional*, defaults to `True`):\n                     Whether or not use a fully seedable random sampler ([`accelerate.data_loader.SeedableRandomSampler`]). Ensures\n-                    training results are fully reproducable using a different sampling technique. While seed-to-seed results\n-                    may differ, on average the differences are neglible when using multiple different seeds to compare. Should\n+                    training results are fully reproducible using a different sampling technique. While seed-to-seed results\n+                    may differ, on average the differences are negligible when using multiple different seeds to compare. Should\n                     also be ran with [`~utils.set_seed`] for the best results.\n                 - use_configured_state (`bool`, *optional*, defaults to `False`):\n                     Whether or not to use a pre-configured `AcceleratorState` or `PartialState` defined before calling `TrainingArguments`.\n@@ -1278,7 +1278,7 @@ class TrainingArguments:\n         default=None,\n         metadata={\n             \"help\": (\n-                \"Config to be used with the internal Accelerator object initializtion. The value is either a \"\n+                \"Config to be used with the internal Accelerator object initialization. The value is either a \"\n                 \"accelerator json config file (e.g., `accelerator_config.json`) or an already loaded json file as `dict`.\"\n             )\n         },\n@@ -1528,7 +1528,7 @@ class TrainingArguments:\n     neftune_noise_alpha: Optional[float] = field(\n         default=None,\n         metadata={\n-            \"help\": \"Activates neftune noise embeddings into the model. NEFTune has been proven to drastically improve model performances for instrcution fine-tuning. Check out the original paper here: https://arxiv.org/abs/2310.05914 and the original code here: https://github.com/neelsjain/NEFTune. Only supported for `PreTrainedModel` and `PeftModel` classes.\"\n+            \"help\": \"Activates neftune noise embeddings into the model. NEFTune has been proven to drastically improve model performances for instruction fine-tuning. Check out the original paper here: https://arxiv.org/abs/2310.05914 and the original code here: https://github.com/neelsjain/NEFTune. Only supported for `PreTrainedModel` and `PeftModel` classes.\"\n         },\n     )\n \n@@ -1584,7 +1584,7 @@ def __post_init__(self):\n         # Parse in args that could be `dict` sent in from the CLI as a string\n         for field in _VALID_DICT_FIELDS:\n             passed_value = getattr(self, field)\n-            # We only want to do this if the str starts with a bracket to indiciate a `dict`\n+            # We only want to do this if the str starts with a bracket to indicate a `dict`\n             # else its likely a filename if supported\n             if isinstance(passed_value, str) and passed_value.startswith(\"{\"):\n                 loaded_dict = json.loads(passed_value)\n@@ -1849,7 +1849,7 @@ def __post_init__(self):\n                     torch.backends.cudnn.allow_tf32 = True\n             else:\n                 logger.warning(\n-                    \"The speedups for torchdynamo mostly come wih GPU Ampere or higher and which is not detected here.\"\n+                    \"The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\"\n                 )\n         if self.framework == \"pt\" and is_torch_available() and self.tf32 is not None:\n             if self.tf32:"
        }
    ],
    "stats": {
        "total": 110,
        "additions": 55,
        "deletions": 55
    }
}