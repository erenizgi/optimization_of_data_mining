{
    "author": "ydshieh",
    "message": "Update after #36962 (#36965)\n\nupdate\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "69632aadb70cbe3121ee3852b461fb1ccb6cbc37",
    "files": [
        {
            "sha": "416bc2564ecb5021795b0e68063572fca8331db5",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/69632aadb70cbe3121ee3852b461fb1ccb6cbc37/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69632aadb70cbe3121ee3852b461fb1ccb6cbc37/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=69632aadb70cbe3121ee3852b461fb1ccb6cbc37",
            "patch": "@@ -631,8 +631,10 @@ def __init__(self) -> None:\n     def prefetch_layer(self, layer_idx: int):\n         \"Starts prefetching the next layer cache\"\n         if layer_idx < len(self):\n-            with self.prefetch_stream if is_torch_greater_or_equal(\"2.7\", accept_dev=True) else torch.cuda.stream(\n+            with (\n                 self.prefetch_stream\n+                if is_torch_greater_or_equal(\"2.7\", accept_dev=True)\n+                else torch.cuda.stream(self.prefetch_stream)\n             ):\n                 # Prefetch next layer tensors to GPU\n                 device = self.original_device[layer_idx]"
        },
        {
            "sha": "0daa90c564386bb603aca7b32ffa99211a42c7e0",
            "filename": "src/transformers/models/qwen2_audio/processing_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/69632aadb70cbe3121ee3852b461fb1ccb6cbc37/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69632aadb70cbe3121ee3852b461fb1ccb6cbc37/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py?ref=69632aadb70cbe3121ee3852b461fb1ccb6cbc37",
            "patch": "@@ -129,7 +129,7 @@ def __call__(\n         if audio is not None:\n             # ensure we have as much audios as audio tokens\n             num_audio_tokens = sum(sample.count(self.audio_token) for sample in text)\n-            num_audios = 1 if type(audio) == np.ndarray else len(audio)\n+            num_audios = 1 if type(audio) is np.ndarray else len(audio)\n             if num_audio_tokens != num_audios:\n                 raise ValueError(\n                     f\"Found {num_audio_tokens} {self.audio_token} token{'s' if num_audio_tokens > 1 else ''} in provided text but received {num_audios} audio{'s' if num_audios > 1 else ''}\""
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}