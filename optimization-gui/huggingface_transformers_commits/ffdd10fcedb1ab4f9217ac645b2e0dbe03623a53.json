{
    "author": "shanjiaz",
    "message": "Allow compression on meta device (#39039)\n\n* disable gradient calculation for int weights\n\nSigned-off-by: shanjiaz <zsjwpianpian@gmail.com>\n\n* Update src/transformers/quantizers/quantizer_compressed_tensors.py\n\nCo-authored-by: Kyle Sayers <kylesayrs@gmail.com>\n\n* updated model procession before/after weight loading\n\nSigned-off-by: shanjiaz <zsjwpianpian@gmail.com>\n\n* fix style\n\nSigned-off-by: shanjiaz <zsjwpianpian@gmail.com>\n\n* reformat\n\nSigned-off-by: shanjiaz <zsjwpianpian@gmail.com>\n\n* fix style\n\nSigned-off-by: shanjiaz <zsjwpianpian@gmail.com>\n\n---------\n\nSigned-off-by: shanjiaz <zsjwpianpian@gmail.com>\nCo-authored-by: Kyle Sayers <kylesayrs@gmail.com>",
    "sha": "ffdd10fcedb1ab4f9217ac645b2e0dbe03623a53",
    "files": [
        {
            "sha": "803c55775214cb5babfedfb29ca501f46de465cb",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 8,
            "deletions": 60,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/ffdd10fcedb1ab4f9217ac645b2e0dbe03623a53/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ffdd10fcedb1ab4f9217ac645b2e0dbe03623a53/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=ffdd10fcedb1ab4f9217ac645b2e0dbe03623a53",
            "patch": "@@ -13,9 +13,6 @@\n # limitations under the License.\n \n \n-import os\n-import re\n-\n from ..utils import is_compressed_tensors_available, is_torch_available, logging\n from ..utils.quantization_config import CompressedTensorsConfig\n from .base import HfQuantizer\n@@ -55,45 +52,6 @@ def __init__(self, quantization_config: CompressedTensorsConfig, **kwargs):\n         self.run_compressed = quantization_config.run_compressed\n         self.quantization_config = quantization_config\n \n-    def update_missing_keys_after_loading(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n-        \"\"\"\n-        Update missing keys after loading the model. This is necessary for compressed tensors\n-        to load the model correctly. We expect weights to be present in missing keys.\n-        The weight's are re-constructed by ModelCompressor in _process_model_after_weight_loading\n-\n-        This function cleans up expected missing keys and returns the remaining missing keys\n-        \"\"\"\n-\n-        if self.run_compressed:\n-            return missing_keys\n-\n-        # We expect some keys to be missing for\n-        # compressed models\n-        # This is fine as the weights are reconstructed by ModelCompressor\n-        # in _process_model_after_weight_loading\n-\n-        expected_missing_keys = self.compressor.get_missing_module_keys(model)\n-        return [\n-            key for key in missing_keys if not any(re.match(f\".*{pattern}\", key) for pattern in expected_missing_keys)\n-        ]\n-\n-    def update_unexpected_keys(self, model, unexpected_keys: list[str], prefix: str) -> list[str]:\n-        \"\"\"\n-        Override this method if you want to adjust the `unexpected_keys`.\n-\n-        Args:\n-            unexpected_keys (`list[str]`, *optional*):\n-                The list of unexpected keys in the checkpoint compared to the state dict of the model\n-        \"\"\"\n-\n-        if self.run_compressed:\n-            return unexpected_keys\n-\n-        # We expect some unexpected keys in model\n-        # safetensors file for compressed models\n-        keys_to_ignore = self.compressor.get_unexpected_file_keys(model)\n-        return [key for key in unexpected_keys if not any(re.match(f\".*{pattern}\", key) for pattern in keys_to_ignore)]\n-\n     def validate_environment(self, *args, **kwargs):\n         if not is_compressed_tensors_available():\n             raise ImportError(\n@@ -117,31 +75,21 @@ def _process_model_before_weight_loading(self, model, **kwargs):\n \n         ct_quantization_config = self.compressor.quantization_config\n \n-        if self.run_compressed:\n-            apply_quantization_config(model, ct_quantization_config, run_compressed=True)\n-        elif not self.quantization_config.is_quantization_compressed:\n-            apply_quantization_config(model, ct_quantization_config)\n+        # Always initialize compressed wrappers to match the checkpoint\n+        apply_quantization_config(model, ct_quantization_config, self.run_compressed)\n+        if (\n+            self.quantization_config.is_quantization_compressed\n+            or self.quantization_config.is_sparsification_compressed\n+        ):\n+            self.compressor.compress_model(model=model)\n \n     def _process_model_after_weight_loading(self, model, **kwargs):\n         \"\"\"Decompress loaded model if necessary - need for qat\"\"\"\n \n         if (\n             self.quantization_config.is_quantization_compressed and not self.run_compressed\n         ) or self.quantization_config.is_sparsification_compressed:\n-            config = kwargs.get(\"config\")\n-            cache_path = config._name_or_path\n-\n-            if not os.path.exists(cache_path):\n-                from transformers.utils import cached_file\n-\n-                config_file_path = cached_file(cache_path, \"config.json\")\n-                cache_path = os.path.sep.join(config_file_path.split(os.path.sep)[:-1])\n-\n-            if self.quantization_config.is_quantization_compressed and not self.run_compressed:\n-                from compressed_tensors.quantization import QuantizationStatus\n-\n-                self.compressor.quantization_config.quantization_status = QuantizationStatus.FROZEN\n-            self.compressor.decompress(model_path=cache_path, model=model)\n+            self.compressor.decompress_model(model=model)\n \n     def update_tp_plan(self, config):\n         additional_plan = {"
        }
    ],
    "stats": {
        "total": 68,
        "additions": 8,
        "deletions": 60
    }
}