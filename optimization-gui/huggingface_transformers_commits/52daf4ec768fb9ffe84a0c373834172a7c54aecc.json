{
    "author": "jmamou",
    "message": "ðŸš¨ðŸš¨ Setting default behavior of assisted decoding (#33657)",
    "sha": "52daf4ec768fb9ffe84a0c373834172a7c54aecc",
    "files": [
        {
            "sha": "fb3120b3ce69e72dc187156276c495ddd1597037",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52daf4ec768fb9ffe84a0c373834172a7c54aecc/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52daf4ec768fb9ffe84a0c373834172a7c54aecc/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=52daf4ec768fb9ffe84a0c373834172a7c54aecc",
            "patch": "@@ -159,6 +159,8 @@ def __init__(\n         self.generation_config.return_dict_in_generate = True\n         self.generation_config.output_scores = True\n         self.generation_config.assistant_confidence_threshold = self.assistant_confidence_threshold\n+        # this flag allow us set the confidence stopping criteria for assistant model generation.\n+        self.generation_config.is_assistant = True\n \n         # avoid unnecessary warnings that min_length is larger than max_new_tokens\n         # remove the `MinLengthLogitsProcessor` if exists (NOTE: no need to check for `MinNewTokensLogitsProcessor`)"
        },
        {
            "sha": "60e9323dcb2a26523ee5163d2457ec93bfbe2c55",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/52daf4ec768fb9ffe84a0c373834172a7c54aecc/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52daf4ec768fb9ffe84a0c373834172a7c54aecc/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=52daf4ec768fb9ffe84a0c373834172a7c54aecc",
            "patch": "@@ -338,19 +338,20 @@ class GenerationConfig(PushToHubMixin):\n             (e.g. multilingual models with different target languages in one batch)\n \n         > Generation parameters exclusive to assistant generation\n-\n-        num_assistant_tokens (`int`, *optional*, defaults to 5):\n+        is_assistant (`bool`, *optional*, defaults to `False`):\n+            Whether the model is an assistant (draft) model.\n+        num_assistant_tokens (`int`, *optional*, defaults to 20):\n             Defines the number of _speculative tokens_ that shall be generated by the assistant model before being\n             checked by the target model at each iteration. Higher values for `num_assistant_tokens` make the generation\n             more _speculative_ : If the assistant model is performant larger speed-ups can be reached, if the assistant\n             model requires lots of corrections, lower speed-ups are reached.\n-        num_assistant_tokens_schedule (`str`, *optional*, defaults to `\"heuristic\"`):\n+        num_assistant_tokens_schedule (`str`, *optional*, defaults to `\"constant\"`):\n             Defines the schedule at which max assistant tokens shall be changed during inference.\n             - `\"heuristic\"`: When all speculative tokens are correct, increase `num_assistant_tokens` by 2 else\n               reduce by 1. `num_assistant_tokens` value is persistent over multiple generation calls with the same assistant model.\n             - `\"heuristic_transient\"`: Same as `\"heuristic\"` but `num_assistant_tokens` is reset to its initial value after each generation call.\n             - `\"constant\"`: `num_assistant_tokens` stays unchanged during generation\n-        assistant_confidence_threshold (`float`, *optional*):\n+        assistant_confidence_threshold (`float`, *optional*, defaults to 0.4):\n             The confidence threshold for the assistant model. If the assistant model's confidence in its prediction for the current token is lower\n             than this threshold, the assistant model stops the current token generation iteration, even if the number of _speculative tokens_\n             (defined by `num_assistant_tokens`) is not yet reached. It is an unsupervised version of the dynamic speculation lookahead\n@@ -452,9 +453,10 @@ def __init__(self, **kwargs):\n         self.decoder_start_token_id = kwargs.pop(\"decoder_start_token_id\", None)\n \n         # Assistant generation\n-        self.num_assistant_tokens = kwargs.pop(\"num_assistant_tokens\", 5)\n-        self.num_assistant_tokens_schedule = kwargs.pop(\"num_assistant_tokens_schedule\", \"heuristic\")\n-        self.assistant_confidence_threshold = kwargs.pop(\"assistant_confidence_threshold\", None)\n+        self.is_assistant = False\n+        self.num_assistant_tokens = kwargs.pop(\"num_assistant_tokens\", 20)\n+        self.num_assistant_tokens_schedule = kwargs.pop(\"num_assistant_tokens_schedule\", \"constant\")\n+        self.assistant_confidence_threshold = kwargs.pop(\"assistant_confidence_threshold\", 0.4)\n \n         # Prompt lookup decoding\n         self.prompt_lookup_num_tokens = kwargs.pop(\"prompt_lookup_num_tokens\", None)"
        },
        {
            "sha": "aedd1674df883d42dc4ab37db2050de88844ea7c",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/52daf4ec768fb9ffe84a0c373834172a7c54aecc/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52daf4ec768fb9ffe84a0c373834172a7c54aecc/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=52daf4ec768fb9ffe84a0c373834172a7c54aecc",
            "patch": "@@ -953,7 +953,8 @@ def _get_stopping_criteria(\n         if generation_config._eos_token_tensor is not None:\n             criteria.append(EosTokenCriteria(eos_token_id=generation_config._eos_token_tensor))\n         if (\n-            generation_config.assistant_confidence_threshold is not None\n+            generation_config.is_assistant\n+            and generation_config.assistant_confidence_threshold is not None\n             and generation_config.assistant_confidence_threshold > 0\n         ):\n             criteria.append("
        },
        {
            "sha": "3f8e99a3347ef5b54e7a50d816061f368e8fdb0c",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/52daf4ec768fb9ffe84a0c373834172a7c54aecc/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52daf4ec768fb9ffe84a0c373834172a7c54aecc/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=52daf4ec768fb9ffe84a0c373834172a7c54aecc",
            "patch": "@@ -2069,6 +2069,7 @@ def test_assisted_decoding_with_num_logits_to_keep(self):\n                 \"assistant_model\": assistant_model,\n             }\n \n+            assistant_model.generation_config.assistant_confidence_threshold = None\n             # Setting num_logits_to_keep at 0 keeps all logits (old behavior)\n             with_all_logits = model.generate(\n                 input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict, num_logits_to_keep=0\n@@ -3098,6 +3099,16 @@ def test_length_warning_assisted_generation(self):\n             )\n             self.assertEqual(len(warning_list), 0)\n \n+    def test_default_assisted_generation(self):\n+        # Initialize the GenerationConfig object\n+        config = GenerationConfig()\n+\n+        # Check the default values\n+        self.assertEqual(config.num_assistant_tokens, 20)\n+        self.assertEqual(config.num_assistant_tokens_schedule, \"constant\")\n+        self.assertEqual(config.assistant_confidence_threshold, 0.4)\n+        self.assertEqual(config.is_assistant, False)\n+\n     def test_generated_length_assisted_generation(self):\n         # PT-only test: TF doesn't support assisted decoding yet.\n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)"
        }
    ],
    "stats": {
        "total": 32,
        "additions": 24,
        "deletions": 8
    }
}