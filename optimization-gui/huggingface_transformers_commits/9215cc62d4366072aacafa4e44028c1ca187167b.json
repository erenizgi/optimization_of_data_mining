{
    "author": "Rocketknight1",
    "message": "Try working around the processor registration bugs (#36184)\n\n* Try working around the processor registration bugs\n\n* oops\n\n* Update error message\n\n* Clarify error\n\n* Docstring docstring docstring\n\n* The extra content is indexed by config class, so let's grab some values out of there\n\n* Commit my confusion as a TODO\n\n* Resolve my confusion\n\n* Cleanup and mostly revert to the original\n\n* Better autoclass fallback\n\n* Don't nest f-strings you lunatic\n\n* Clearer error message\n\n* Less getattr()\n\n* Revert a lot of changes to try a different approach!\n\n* Try the global registry\n\n* Check the dynamic list as well as the transformers root\n\n* Move the dynamic list somewhere safer\n\n* Move the dynamic list somewhere even safer\n\n* More import cleanup\n\n* Simplify all the register_for_auto_class methods\n\n* Set _auto_class in the register() methods\n\n* Stop setting the cls attribute in register()\n\n* Restore specifying the model class for Model derivatives only\n\n* Fix accidentally taking the .__class__ of a class\n\n* Revert register_for_auto_class changes\n\n* Fix get_possibly_dynamic_module\n\n* No more ALL_CUSTOM_CLASSES\n\n* Fix up get_possibly_dynamic_module as well\n\n* Revert unnecessary formatting changes\n\n* Trigger tests",
    "sha": "9215cc62d4366072aacafa4e44028c1ca187167b",
    "files": [
        {
            "sha": "c65219b0bc69ab4b2b37882d2e2830531584a459",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9215cc62d4366072aacafa4e44028c1ca187167b/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9215cc62d4366072aacafa4e44028c1ca187167b/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=9215cc62d4366072aacafa4e44028c1ca187167b",
            "patch": "@@ -382,6 +382,6 @@ def register(config_class, processor_class, exist_ok=False):\n         Args:\n             config_class ([`PretrainedConfig`]):\n                 The configuration corresponding to the model to register.\n-            processor_class ([`FeatureExtractorMixin`]): The processor to register.\n+            processor_class ([`ProcessorMixin`]): The processor to register.\n         \"\"\"\n         PROCESSOR_MAPPING.register(config_class, processor_class, exist_ok=exist_ok)"
        },
        {
            "sha": "7d838b1ad29b8151149d5d7e8bb2d6b3bd827392",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9215cc62d4366072aacafa4e44028c1ca187167b/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9215cc62d4366072aacafa4e44028c1ca187167b/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=9215cc62d4366072aacafa4e44028c1ca187167b",
            "patch": "@@ -990,6 +990,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n             f\"Model type should be one of {', '.join(c.__name__ for c in TOKENIZER_MAPPING.keys())}.\"\n         )\n \n+    @staticmethod\n     def register(config_class, slow_tokenizer_class=None, fast_tokenizer_class=None, exist_ok=False):\n         \"\"\"\n         Register a new tokenizer in this mapping."
        },
        {
            "sha": "9872887fe58a66e3389e5afdf5d3ce7698fd9927",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 37,
            "deletions": 5,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/9215cc62d4366072aacafa4e44028c1ca187167b/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9215cc62d4366072aacafa4e44028c1ca187167b/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=9215cc62d4366072aacafa4e44028c1ca187167b",
            "patch": "@@ -69,7 +69,7 @@\n \n logger = logging.get_logger(__name__)\n \n-# Dynamically import the Transformers module to grab the attribute classes of the processor form their names.\n+# Dynamically import the Transformers module to grab the attribute classes of the processor from their names.\n transformers_module = direct_transformers_import(Path(__file__).parent)\n \n \n@@ -470,9 +470,9 @@ def __init__(self, *args, **kwargs):\n             # Nothing is ever going to be an instance of \"AutoXxx\", in that case we check the base class.\n             class_name = AUTO_TO_BASE_CLASS_MAPPING.get(class_name, class_name)\n             if isinstance(class_name, tuple):\n-                proper_class = tuple(getattr(transformers_module, n) for n in class_name if n is not None)\n+                proper_class = tuple(self.get_possibly_dynamic_module(n) for n in class_name if n is not None)\n             else:\n-                proper_class = getattr(transformers_module, class_name)\n+                proper_class = self.get_possibly_dynamic_module(class_name)\n \n             if not isinstance(arg, proper_class):\n                 raise TypeError(\n@@ -1100,11 +1100,19 @@ def register_for_auto_class(cls, auto_class=\"AutoProcessor\"):\n \n     @classmethod\n     def _get_arguments_from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n+        \"\"\"\n+        Identify and instantiate the subcomponents of Processor classes, like image processors and\n+        tokenizers. This method uses the Processor attributes like `tokenizer_class` to figure out what class those\n+        subcomponents should be. Note that any subcomponents must either be library classes that are accessible in\n+        the `transformers` root, or they must be custom code that has been registered with the relevant autoclass,\n+        via methods like `AutoTokenizer.register()`. If neither of these conditions are fulfilled, this method\n+        will be unable to find the relevant subcomponent class and will raise an error.\n+        \"\"\"\n         args = []\n         for attribute_name in cls.attributes:\n             class_name = getattr(cls, f\"{attribute_name}_class\")\n             if isinstance(class_name, tuple):\n-                classes = tuple(getattr(transformers_module, n) if n is not None else None for n in class_name)\n+                classes = tuple(cls.get_possibly_dynamic_module(n) if n is not None else None for n in class_name)\n                 if attribute_name == \"image_processor\":\n                     # TODO: @yoni, change logic in v4.50 (when use_fast set to True by default)\n                     use_fast = kwargs.get(\"use_fast\", None)\n@@ -1121,11 +1129,35 @@ def _get_arguments_from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n                 else:\n                     attribute_class = classes[0]\n             else:\n-                attribute_class = getattr(transformers_module, class_name)\n+                attribute_class = cls.get_possibly_dynamic_module(class_name)\n \n             args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))\n         return args\n \n+    @staticmethod\n+    def get_possibly_dynamic_module(module_name):\n+        if hasattr(transformers_module, module_name):\n+            return getattr(transformers_module, module_name)\n+        lookup_locations = [\n+            transformers_module.IMAGE_PROCESSOR_MAPPING,\n+            transformers_module.TOKENIZER_MAPPING,\n+            transformers_module.FEATURE_EXTRACTOR_MAPPING,\n+        ]\n+        for lookup_location in lookup_locations:\n+            for custom_class in lookup_location._extra_content.values():\n+                if isinstance(custom_class, tuple):\n+                    for custom_subclass in custom_class:\n+                        if custom_subclass is not None and custom_subclass.__name__ == module_name:\n+                            return custom_subclass\n+                elif custom_class is not None and custom_class.__name__ == module_name:\n+                    return custom_class\n+        else:\n+            raise ValueError(\n+                f\"Could not find module {module_name} in `transformers`. If this is a custom class, \"\n+                f\"it should be registered using the relevant `AutoClass.register()` function so that \"\n+                f\"other functions can find it!\"\n+            )\n+\n     @property\n     def model_input_names(self):\n         first_attribute = getattr(self, self.attributes[0])"
        },
        {
            "sha": "41a2815e4da7a3a9e9afb7716a2c210b1a2da4cf",
            "filename": "tests/models/auto/test_processor_auto.py",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/9215cc62d4366072aacafa4e44028c1ca187167b/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9215cc62d4366072aacafa4e44028c1ca187167b/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py?ref=9215cc62d4366072aacafa4e44028c1ca187167b",
            "patch": "@@ -354,6 +354,40 @@ def __init__(self, feature_extractor, tokenizer, processor_attr_1=1, processor_a\n             if CustomConfig in PROCESSOR_MAPPING._extra_content:\n                 del PROCESSOR_MAPPING._extra_content[CustomConfig]\n \n+    def test_dynamic_processor_with_specific_dynamic_subcomponents(self):\n+        class NewFeatureExtractor(Wav2Vec2FeatureExtractor):\n+            pass\n+\n+        class NewTokenizer(BertTokenizer):\n+            pass\n+\n+        class NewProcessor(ProcessorMixin):\n+            feature_extractor_class = \"NewFeatureExtractor\"\n+            tokenizer_class = \"NewTokenizer\"\n+\n+            def __init__(self, feature_extractor, tokenizer):\n+                super().__init__(feature_extractor, tokenizer)\n+\n+        try:\n+            AutoConfig.register(\"custom\", CustomConfig)\n+            AutoFeatureExtractor.register(CustomConfig, NewFeatureExtractor)\n+            AutoTokenizer.register(CustomConfig, slow_tokenizer_class=NewTokenizer)\n+            AutoProcessor.register(CustomConfig, NewProcessor)\n+            # If remote code is not set, the default is to use local classes.\n+            processor = AutoProcessor.from_pretrained(\n+                \"hf-internal-testing/test_dynamic_processor\",\n+            )\n+            self.assertEqual(processor.__class__.__name__, \"NewProcessor\")\n+        finally:\n+            if \"custom\" in CONFIG_MAPPING._extra_content:\n+                del CONFIG_MAPPING._extra_content[\"custom\"]\n+            if CustomConfig in FEATURE_EXTRACTOR_MAPPING._extra_content:\n+                del FEATURE_EXTRACTOR_MAPPING._extra_content[CustomConfig]\n+            if CustomConfig in TOKENIZER_MAPPING._extra_content:\n+                del TOKENIZER_MAPPING._extra_content[CustomConfig]\n+            if CustomConfig in PROCESSOR_MAPPING._extra_content:\n+                del PROCESSOR_MAPPING._extra_content[CustomConfig]\n+\n     def test_auto_processor_creates_tokenizer(self):\n         processor = AutoProcessor.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n         self.assertEqual(processor.__class__.__name__, \"BertTokenizerFast\")"
        }
    ],
    "stats": {
        "total": 79,
        "additions": 73,
        "deletions": 6
    }
}