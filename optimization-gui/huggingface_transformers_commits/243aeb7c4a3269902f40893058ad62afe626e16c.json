{
    "author": "lenglaender",
    "message": "Fix Gradient Checkpointing for Deberta & Deberta-V2 using PEFT / Adapters (#35898)\n\nReplace In-Place Operations for Deberta and Deberta-V2",
    "sha": "243aeb7c4a3269902f40893058ad62afe626e16c",
    "files": [
        {
            "sha": "eba94b57ee9d08fb945df3c3082eb86d1a338c58",
            "filename": "src/transformers/models/deberta/modeling_deberta.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/243aeb7c4a3269902f40893058ad62afe626e16c/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/243aeb7c4a3269902f40893058ad62afe626e16c/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py?ref=243aeb7c4a3269902f40893058ad62afe626e16c",
            "patch": "@@ -418,10 +418,10 @@ def forward(self, input_ids=None, token_type_ids=None, position_ids=None, mask=N\n \n         embeddings = inputs_embeds\n         if self.position_biased_input:\n-            embeddings += position_embeddings\n+            embeddings = embeddings + position_embeddings\n         if self.token_type_embeddings is not None:\n             token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-            embeddings += token_type_embeddings\n+            embeddings = embeddings + token_type_embeddings\n \n         if self.embed_proj is not None:\n             embeddings = self.embed_proj(embeddings)"
        },
        {
            "sha": "b02628ed6929ee610906f7075c6ecbd0ad990bf6",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/243aeb7c4a3269902f40893058ad62afe626e16c/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/243aeb7c4a3269902f40893058ad62afe626e16c/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=243aeb7c4a3269902f40893058ad62afe626e16c",
            "patch": "@@ -551,10 +551,10 @@ def forward(self, input_ids=None, token_type_ids=None, position_ids=None, mask=N\n \n         embeddings = inputs_embeds\n         if self.position_biased_input:\n-            embeddings += position_embeddings\n+            embeddings = embeddings + position_embeddings\n         if self.token_type_embeddings is not None:\n             token_type_embeddings = self.token_type_embeddings(token_type_ids)\n-            embeddings += token_type_embeddings\n+            embeddings = embeddings + token_type_embeddings\n \n         if self.embed_proj is not None:\n             embeddings = self.embed_proj(embeddings)"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}