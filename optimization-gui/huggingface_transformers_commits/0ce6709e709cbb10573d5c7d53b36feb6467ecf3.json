{
    "author": "ved1beta",
    "message": "deci gguf support (#38669)\n\n* deci gguf support\n\n* make style\n\n* tests for deci\n\n* try except removed\n\n* style\n\n* try except removed",
    "sha": "0ce6709e709cbb10573d5c7d53b36feb6467ecf3",
    "files": [
        {
            "sha": "3be36e26d9e831a10fdc86980be09463519df21f",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ce6709e709cbb10573d5c7d53b36feb6467ecf3/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ce6709e709cbb10573d5c7d53b36feb6467ecf3/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=0ce6709e709cbb10573d5c7d53b36feb6467ecf3",
            "patch": "@@ -250,6 +250,18 @@\n         \"attention.sliding_window\": \"sliding_window\",\n         \"vocab_size\": \"vocab_size\",\n     },\n+    \"deci\": {\n+        \"context_length\": \"max_position_embeddings\",\n+        \"block_count\": \"num_hidden_layers\",\n+        \"feed_forward_length\": \"intermediate_size\",\n+        \"embedding_length\": \"hidden_size\",\n+        \"rope.dimension_count\": None,\n+        \"rope.freq_base\": \"rope_theta\",\n+        \"attention.head_count\": \"num_attention_heads\",\n+        \"attention.head_count_kv\": \"num_key_value_heads\",\n+        \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n+        \"vocab_size\": \"vocab_size\",\n+    },\n }\n \n GGUF_TOKENIZER_MAPPING = {\n@@ -716,6 +728,8 @@ def converted(self) -> Tokenizer:\n     \"nemotron\": GGUFGPTConverter,\n     \"gemma2\": GGUFGemmaConverter,\n     \"gemma3_text\": GGUFGemmaConverter,\n+    \"deci\": GGUFLlamaConverter,\n+    \"decilm\": GGUFLlamaConverter,\n }\n \n "
        },
        {
            "sha": "6fcc8d8fafde157575d575acc7ac306ac0fc1b8f",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 83,
            "deletions": 0,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ce6709e709cbb10573d5c7d53b36feb6467ecf3/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ce6709e709cbb10573d5c7d53b36feb6467ecf3/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=0ce6709e709cbb10573d5c7d53b36feb6467ecf3",
            "patch": "@@ -335,6 +335,10 @@ class GgufModelTests(unittest.TestCase):\n     q4_0_gemma3_qat_model_id = \"gemma-3-1b-it-q4_0.gguf\"\n     bf16_gemma3_text_model_id = \"gemma-3-1b-it-BF16.gguf\"\n     bf16_gemma3_vision_model_id = \"gemma-3-4b-it-BF16.gguf\"\n+    deci_original_model_id = \"Deci/DeciLM-7B\"\n+    deci_model_id = \"Deci/DeciLM-7B-instruct-GGUF\"\n+    q8_0_deci_model_id = \"decilm-7b-uniform-gqa-q8_0.gguf\"\n+    fp16_deci_model_id = \"decilm-7b-uniform-gqa-f16.gguf\"\n     q8_0_qwen3_model_id = \"Qwen3-0.6B-Q8_0.gguf\"\n     q4_k_m_qwen3moe_model_id = \"Qwen3-30B-A3B-Q4_K_M.gguf\"\n \n@@ -960,6 +964,85 @@ def test_gemma3_vision_weights_conversion_bf16(self):\n             else:\n                 raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n \n+    def test_deci_q8_0(self):\n+        \"\"\"Test Deci model loading and inference with Q4_0 quantization.\"\"\"\n+        tokenizer = AutoTokenizer.from_pretrained(self.deci_model_id, gguf_file=self.q8_0_deci_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.deci_model_id,\n+            gguf_file=self.q8_0_deci_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, I am a language model developed\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_deci_weights_conversion_fp16(self):\n+        \"\"\"Test that GGUF Deci model weights match the original model weights.\"\"\"\n+        original_model_id = \"Deci/DeciLM-7B\"\n+        original_model = AutoModelForCausalLM.from_pretrained(\n+            original_model_id,\n+            torch_dtype=torch.float16,\n+            trust_remote_code=True,\n+            device_map=\"auto\",\n+        )\n+        # You need to have an FP16 version of your GGUF model for accurate comparison\n+\n+        converted_model = AutoModelForCausalLM.from_pretrained(\n+            self.deci_model_id,\n+            gguf_file=self.fp16_deci_model_id,\n+            torch_dtype=torch.float16,\n+            device_map=\"auto\",\n+        )\n+\n+        converted_state_dict = converted_model.state_dict()\n+        original_state_dict = original_model.state_dict()\n+\n+        for layer_name, original_params in original_state_dict.items():\n+            if layer_name in converted_state_dict:\n+                self.assertTrue(original_params.shape == converted_state_dict[layer_name].shape)\n+                torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n+            else:\n+                raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n+\n+    def test_deci_config_mapping(self):\n+        \"\"\"Test that Deci GGUF config mapping is correctly applied.\"\"\"\n+        from transformers.integrations.ggml import GGUF_CONFIG_MAPPING\n+\n+        self.assertIn(\"deci\", GGUF_CONFIG_MAPPING)\n+\n+        deci_mapping = GGUF_CONFIG_MAPPING[\"deci\"]\n+\n+        expected_mappings = {\n+            \"context_length\": \"max_position_embeddings\",\n+            \"block_count\": \"num_hidden_layers\",\n+            \"feed_forward_length\": \"intermediate_size\",\n+            \"embedding_length\": \"hidden_size\",\n+            \"rope.freq_base\": \"rope_theta\",\n+            \"attention.head_count\": \"num_attention_heads\",\n+            \"attention.head_count_kv\": \"num_key_value_heads\",\n+            \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n+            \"vocab_size\": \"vocab_size\",\n+        }\n+\n+        for gguf_key, transformers_key in expected_mappings.items():\n+            self.assertEqual(deci_mapping[gguf_key], transformers_key)\n+\n+        self.assertIsNone(deci_mapping[\"rope.dimension_count\"])\n+\n+    def test_deci_architecture_mapping(self):\n+        \"\"\"Test that Deci architectures are mapped to GGUFLlamaConverter.\"\"\"\n+        from transformers.integrations.ggml import GGUF_TO_FAST_CONVERTERS, GGUFLlamaConverter\n+\n+        self.assertIn(\"deci\", GGUF_TO_FAST_CONVERTERS)\n+        self.assertIn(\"decilm\", GGUF_TO_FAST_CONVERTERS)\n+\n+        self.assertEqual(GGUF_TO_FAST_CONVERTERS[\"deci\"], GGUFLlamaConverter)\n+        self.assertEqual(GGUF_TO_FAST_CONVERTERS[\"decilm\"], GGUFLlamaConverter)\n+\n     @require_read_token\n     @unittest.skipUnless(is_gguf_available(\"0.16.0\"), \"test requires gguf version >= 0.16.0\")\n     def test_qwen3_q8_0(self):"
        }
    ],
    "stats": {
        "total": 97,
        "additions": 97,
        "deletions": 0
    }
}