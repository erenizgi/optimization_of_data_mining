{
    "author": "ArthurZucker",
    "message": "Add llama4 (#37307)\n\n* remove one of the last deps\n\n* update fast image processor after refactor\n\n* styling\n\n* more quality of life improvements\n\n* nit\n\n* update\n\n* cleanups\n\n* some cleanups\n\n* vllm updates\n\n* update fake image token\n\n* [convert] Fix typo\n\n* [convert] Strip extraneous bytes from shards\n\n* [convert] Minor fixes\n\n* [convert] Use num_experts\n\n* multi-image fixes in modeling + processor\n\n* fixup size\n\n* 128 experts\n\n* Use default rope\n\n* Unfuse mlp\n\n* simplify a lot inputs embeds merging\n\n* remove .item() :eyes:\n\n* fix from review\n\n* Address feedback\n\n* Use None \"default\" for rope_scaling. Add eot.\n\n* set seed\n\n* return aspect ratios and bug fixes\n\n* Moe 128 rebased (#8)\n\n* 128 experts\n\n* Use default rope\n\n* Unfuse mlp\n\n* Address feedback\n\n* Use None \"default\" for rope_scaling. Add eot.\n\n* Meta/llama quant compat (#7)\n\n* add quant compatible model & conversion code for llama4\n\n* fix a few issues\n\n* fix a few issues\n\n* minor type mapping fix\n\n---------\n\nCo-authored-by: Lu Fang <fanglu@fb.com>\n\n* use a new config parameter to determine which model definition to use for MoE\n\n---------\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\nCo-authored-by: Lu Fang <fanglu@fb.com>\n\n* un-comment write_tokenizer from converting script\n\n* remove un-used imports\n\n* [llama4] Pop aspect_ratios from image processor output in Llama4Processor\n\nSigned-off-by: Jon Swenson <jmswen@gmail.com>\n\n* Fix parameter_count name\n\n* Update src/transformers/models/llama4/configuration_llama4.py\n\n* nit\n\n* Add changes for no_rope, moe_layers, chunked attention. Just need to test all\n\n* Update src/transformers/models/llama4/image_processing_llama4_fast.py\n\n* nit\n\n* fix post merge with main\n\n* support flex attention\n\n* fixes\n\n* fix\n\n* add layer\n\n* small updates\n\n* rebase and delete llm_compressor\n\n* nit\n\n* [llama4/mm] Add back <|image|> token that delimits global tile\n\n* [llama4/mm] Fix Llama 4 image processing unit tests\n\n* add explicit dtype\n\nSigned-off-by: Jon Swenson <jmswen@gmail.com>\n\n* sdpa works\n\n* comment todo small\n\n* fix model loading\n\nSigned-off-by: Zijing Liu <liuzijing2014@gmail.com>\n\n* revert\n\n* nits\n\n* small fix for TP on 1 node\n\n* Read new params from config\n\n* Add <|eom|>\n\n* lol don't know how this got here\n\n* adding fp8\n\n* Save processor, fix chat template\n\n* style\n\n* Add boi/eoi tokens\n\nWe don't use them.\n\n* fixes for now flex seems to work :)\n\n* updates\n\n* nits\n\n* updates\n\n* missking keys\n\n* add context parallel\n\n* update\n\n* update\n\n* fix\n\n* nits\n\n* add worldsize and make eager attn work for vision\n\n* Ignore new key present in base models\n\n* add tp_plan\n\n* fix nope\n\nSigned-off-by: Zijing Liu <liuzijing2014@gmail.com>\n\n* minor fix\n\nSigned-off-by: Zijing Liu <liuzijing2014@gmail.com>\n\n* Clean up Llama4 vision model\n\n* current updates\n\n* add support for `attn_temperature_tuning`\n\n* add floor scale\n\n* add missing attn scales\n\n* push what works, dirty trick for the device synch\n\n* oups\n\n* Fix pad_token_id\n\nSee\nhttps://huggingface.co/ll-re/Llama-4-Scout-17B-16E/discussions/2/files\nConfirmed in the original codebase.\n\n* fix causallml loading\n\n* rm\n\n* fix tied-weights\n\n* fix sdpa\n\n* push current version\n\n* should work with both short and long\n\n* add compressed_tensos & fix fbgemm tp\n\n* Fix flex impl\n\n* style\n\n* chunking\n\n* try to revert the potentially breaking change\n\n* fix auto factory\n\n* fix shapes in general\n\n* rm processing\n\n* commit cache utils cleanup\n\n* Fix context length\n\n* fix\n\n* allocate\n\n* update tp_plan\n\n* fix SDPA!\n\n* Add support for sparse `Llama4TextMoe` layer from the kernel hub\n\n* cleanup\n\n* better merge\n\n* update\n\n* still broken fixing now\n\n* nits\n\n* revert print\n\n* Write max_position_embeddings and max_model_length\n\n* Update modeling_llama4.py\n\n* Save attention_chunk_size\n\n* Sync eos terminators\n\n* Read initializer_range\n\n* style\n\n* remove `dict`\n\n* fix\n\n* eager should use `chunked_attention_mask`\n\n* revert\n\n* fixup\n\n* fix config\n\n* Revert \"Merge pull request #36 from huggingface/sparse-llama4-moe\"\n\nThis reverts commit ccda19f050867dd42ea143c5de60f3dec81375f0, reversing\nchanges made to a515579aed8c0fe9bf529b6c40446a289406d5d6.\n\n* Fix typo and remove warning with compiled flex and chunked prefill\n\n* Fix MoE vs FF (#41)\n\n* fix\n\n* Use correct no_rope_layers if provided one is empty list\n\n* update tests\n\n* fix\n\n* skipping some tests\n\n* fix fp8 loading\n\nSigned-off-by: Zijing Liu <liuzijing2014@gmail.com>\n\n* fix text geneartion pipeline\n\nSigned-off-by: Zijing Liu <liuzijing2014@gmail.com>\n\n* eager needs 4D mask\n\n* fix\n\n* Some cleanup\n\n* fix\n\n* update\n\n* fix\n\n* replace correctly module\n\n* patch\n\n* modulelist\n\n* update\n\n* update\n\n* clean up\n\n* Don't move to `cuda:0` in distributed mode\n\n* restrict to compressed tensors for now\n\n* rm print\n\n* Docs!\n\n* Fixes\n\n* Update docs/source/en/model_doc/llama4.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Fixes\n\n* cuda graph fix\n\n* revert some stuff\n\n* fixup\n\n* styling\n\n* Update src/transformers/models/llama4/modeling_llama4.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fixup\n\n* commit licence, cleanup here and there and style\n\n* more styling changes\n\n* fix dummies\n\n* fix and clean docstrings\n\n* remove comment\n\n* remove warning\n\n* Only fast image processor is supported\n\n* nit\n\n* trigger CI\n\n* fix issue with flex encoder\n\n* fix dynamic cache\n\n* Code quality\n\n* Code quality\n\n* fix more tests for now\n\n* Code quality\n\n* Code quality\n\n* Nuke bunch of failing stuff\n\n* Code quality\n\n* Code quality\n\n* cleanup removal of slow image processor\n\n* ruff fix fast image processor\n\n* fix\n\n* fix styling\n\n* Docs\n\n* Repo consistency\n\n* Repo consistency\n\n* fix sliding window issue\n\n* separate llama cache\n\n* styling\n\n* Repo consistency\n\n* Repo consistency\n\n* push waht works\n\n* L4 Repo consistency\n\n* Docs\n\n* fix last last alst alst alst alstsaltlsltlaslt\n\n---------\n\nSigned-off-by: Jon Swenson <jmswen@gmail.com>\nSigned-off-by: Zijing Liu <liuzijing2014@gmail.com>\nCo-authored-by: yonigozlan <yoni.gozlan10@gmail.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\nCo-authored-by: Pablo Montalvo <pablo.montalvo.leroux@gmail.com>\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>\nCo-authored-by: Keyun Tong <tongkeyun@gmail.com>\nCo-authored-by: Zijing Liu <liuzijing2014@users.noreply.github.com>\nCo-authored-by: Lu Fang <fanglu@fb.com>\nCo-authored-by: Zijing Liu <liuzijing2014@gmail.com>\nCo-authored-by: Jon Swenson <jmswen@gmail.com>\nCo-authored-by: jmswen <jmswen@users.noreply.github.com>\nCo-authored-by: MekkCyber <mekk.cyber@gmail.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\nCo-authored-by: Mohit Sharma <mohit21sharma.ms@gmail.com>\nCo-authored-by: Yong Hoon Shin <yhshin@meta.com>\nCo-authored-by: Marc Sun <marc@huggingface.co>\nCo-authored-by: drisspg <drisspguessous@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\nCo-authored-by: Daniël de Kok <me@danieldk.eu>\nCo-authored-by: Lysandre <hi@lysand.re>\nCo-authored-by: Ye (Charlotte) Qi <ye.charlotte.qi@gmail.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
    "files": [
        {
            "sha": "6c4b7498b3da86a4ac120d8053f8e3b6be743238",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -507,6 +507,8 @@\n         title: Llama2\n       - local: model_doc/llama3\n         title: Llama3\n+      - local: model_doc/llama4\n+        title: Llama4\n       - local: model_doc/longformer\n         title: Longformer\n       - local: model_doc/longt5"
        },
        {
            "sha": "8e2cd3a2786f7ad1bd8df00c436edda87427d25a",
            "filename": "docs/source/en/model_doc/llama4.md",
            "status": "added",
            "additions": 442,
            "deletions": 0,
            "changes": 442,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -0,0 +1,442 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Llama4\n+\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+    </div>\n+</div>\n+\n+Llama 4, developed by Meta, introduces a new auto-regressive Mixture-of-Experts (MoE) architecture.\n+This generation includes two models:\n+- The highly capable Llama 4 Maverick with 17B active parameters out of ~400B total, with 128 experts.\n+- The efficient Llama 4 Scout also  has 17B active parameters out of ~109B total, using just 16 experts.\n+-\n+Both models leverage early fusion for native multimodality, enabling them to process text and image inputs.\n+Maverick and Scout are both trained on up to 40 trillion tokens on data encompassing 200 languages\n+(with specific fine-tuning support for 12 languages including Arabic, Spanish, German, and Hindi).\n+\n+For deployment, Llama 4 Scout is designed for accessibility, fitting on a single server-grade GPU via\n+on-the-fly 4-bit or 8-bitint4 quantization, while Maverick is available in BF16 and FP8 formats.\n+These models are released under the custom Llama 4 Community License Agreement, available on the model repositories.\n+\n+You can find all the original Llama checkpoints under the [meta-llama](https://huggingface.co/meta-llama) organization.\n+\n+> [!TIP]\n+> The Llama 4 family of models comes in two flavors: 109B, and 402B parameters. Both of these flavors are extremely\n+> large and won't fit on your run-of-the-mill device. See below for some examples to reduce the memory usage of the\n+> model.\n+>\n+> For the download to be faster and more resilient, we recommend installing the `hf_xet` dependency as followed:\n+> `pip install transformers[hf_xet]`\n+\n+The examples below demonstrates how to generate with [`Pipeline`] or the [`AutoModel`]. We additionally add an example\n+showcasing how to toggle the right attributes to enable very long-context generations, as some flavors of Llama 4\n+have context lengths going up to 10 million tokens.\n+\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+from transformers import pipeline\n+import torch\n+\n+model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": \"what is the recipe of mayonnaise?\"},\n+]\n+\n+pipe = pipeline(\n+    \"text-generation\",\n+    model=model_id,\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16\n+)\n+\n+output = pipe(messages, do_sample=False, max_new_tokens=200)\n+print(output[0][\"generated_text\"][-1][\"content\"])\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel - Text only\">\n+\n+```py\n+from transformers import AutoTokenizer, Llama4ForConditionalGeneration\n+import torch\n+\n+model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": \"Who are you?\"},\n+]\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16\n+)\n+\n+outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n+outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])\n+print(outputs[0])\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel - Multimodal\">\n+\n+```py\n+from transformers import AutoProcessor, Llama4ForConditionalGeneration\n+import torch\n+\n+model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n+\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+)\n+\n+img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"url\": img_url},\n+            {\"type\": \"text\", \"text\": \"Describe this image in two sentences.\"},\n+        ]\n+    },\n+]\n+\n+inputs = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+).to(model.device)\n+\n+outputs = model.generate(\n+    **inputs,\n+    max_new_tokens=256,\n+)\n+\n+response = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])[0]\n+print(response)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel - Multimodal with multiple images\">\n+\n+```py\n+from transformers import AutoProcessor, Llama4ForConditionalGeneration\n+import torch\n+\n+model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n+\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+)\n+\n+url1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n+url2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/cat_style_layout.png\"\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"url\": url1},\n+            {\"type\": \"image\", \"url\": url2},\n+            {\"type\": \"text\", \"text\": \"Can you describe how these two images are similar, and how they differ?\"},\n+        ]\n+    },\n+]\n+\n+inputs = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+).to(model.device)\n+\n+outputs = model.generate(\n+    **inputs,\n+    max_new_tokens=256,\n+)\n+\n+response = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])[0]\n+print(response)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel - Long context\">\n+\n+Beware: the example below uses both `device_map=\"auto\"` and flex-attention.\n+Please use `torchrun` to run this example in tensor-parallel mode.\n+\n+We will work to enable running with `device_map=\"auto\"` and flex-attention without\n+tensor-parallel in the future.\n+\n+```py\n+from transformers import Llama4ForConditionalGeneration, AutoTokenizer\n+import torch\n+import time\n+\n+file = \"very_long_context_prompt.txt\"\n+model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n+\n+with open(file, \"r\") as f:\n+    very_long_text = \"\\n\".join(f.readlines())\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    attn_implementation=\"flex_attention\",\n+    torch_dtype=torch.bfloat16\n+)\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": f\"Look at the following texts: [{very_long_text}]\\n\\n\\n\\nWhat are the books, and who wrote them? Make me a nice list.\"},\n+]\n+input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n+\n+torch.cuda.synchronize()\n+start = time.time()\n+out = model.generate(\n+    input_ids.to(model.device),\n+    prefill_chunk_size=2048*8,\n+    max_new_tokens=300,\n+    cache_implementation=\"hybrid\",\n+)\n+print(time.time()-start)\n+print(tokenizer.batch_decode(out[:, input_ids.shape[-1]:]))\n+print(f\"{torch.cuda.max_memory_allocated(model.device) / 1024**3:.2f} GiB\")\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## Efficiency; how to get the best out of llama 4\n+\n+### The Attention methods\n+\n+Updating the default attention function can significantly improve compute performance as well as memory usage. Refer to the [Attention Interface](../attention_interface) overview for an in-depth explanation of our interface.\n+\n+As of release, the Llama 4 model supports the following attention methods: `eager`, `flex_attention`, `sdpa`. We recommend using `flex_attention` for best results.\n+Switching attention mechanism is done at the model initialization step:\n+\n+\n+<hfoptions id=\"Attention\">\n+<hfoption id=\"Flex Attention\">\n+\n+Setting Flex Attention ensures the best results with the very long context the model can handle.\n+\n+> [!TIP] Beware: the example below uses both `device_map=\"auto\"` and flex-attention.\n+> Please use `torchrun` to run this example in tensor-parallel mode.\n+>\n+> We will work to enable running with `device_map=\"auto\"` and flex-attention without\n+> tensor-parallel in the future.\n+\n+```py\n+from transformers import Llama4ForConditionalGeneration\n+import torch\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    attn_implementation=\"flex_attention\",\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+)\n+```\n+</hfoption>\n+<hfoption id=\"SDPA\">\n+The `sdpa` attention method is generally more compute-efficient than the `eager` method.\n+\n+```py\n+from transformers import Llama4ForConditionalGeneration\n+import torch\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    attn_implementation=\"sdpa\",\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+)\n+```\n+</hfoption>\n+<hfoption id=\"Eager\">\n+The `eager` attention method is set by default, so no need for anything different when loading the model:\n+\n+```py\n+from transformers import Llama4ForConditionalGeneration\n+import torch\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+)\n+```\n+</hfoption>\n+</hfoptions>\n+\n+\n+### Quantization\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for available quantization backends.\n+At time of release, both FBGEMM and LLM-Compressor are supported; more quantization methods will be supported in the days that follow the release.\n+\n+See below for examples using both:\n+\n+\n+\n+Here is an example loading an BF16 model in FP8 using the FBGEMM approach:\n+\n+<hfoptions id=\"Quantization\">\n+<hfoption id=\"FBGEMM\">\n+\n+```python\n+from transformers import AutoTokenizer, Llama4ForConditionalGeneration, FbgemmFp8Config\n+import torch\n+\n+model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": \"Who are you?\"},\n+]\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+    quantization_config=FbgemmFp8Config()\n+)\n+\n+outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n+outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])\n+print(outputs[0])\n+```\n+\n+</hfoption>\n+<hfoption id=\"LLM-Compressor\">\n+\n+To use the LLM-Compressor technique, we recommend leveraging the pre-quantized FP8 checkpoint available with the release:\n+\n+```python\n+from transformers import AutoTokenizer, Llama4ForConditionalGeneration\n+import torch\n+\n+model_id = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": \"Who are you?\"},\n+]\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    tp_plan=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+)\n+\n+outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n+outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])\n+print(outputs[0])\n+```\n+</hfoption>\n+</hfoptions>\n+\n+### Offloading\n+\n+Enabling CPU-offloading means that components of the model might be moved to CPU instead of GPU in case the GPU-memory available isn't sufficient to load the entire model.\n+At inference, different components will be loaded/unloaded from/to the GPU on the fly. This ensures that the model can be loaded on smaller machines as long as the CPU-memory is sufficient.\n+However, this also slows down inference as it adds communication overhead.\n+\n+In order to enable CPU-offloading, you simply need to specify the `device_map` to `auto` at model load:\n+\n+```py\n+from transformers import Llama4ForConditionalGeneration\n+import torch\n+\n+model = Llama4ForConditionalGeneration.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+)\n+```\n+\n+## Llama4Config\n+\n+[[autodoc]] Llama4Config\n+\n+## Llama4TextConfig\n+\n+[[autodoc]] Llama4TextConfig\n+\n+## Llama4VisionConfig\n+\n+[[autodoc]] Llama4VisionConfig\n+\n+## Llama4Processor\n+\n+[[autodoc]] Llama4Processor\n+\n+## Llama4ImageProcessorFast\n+\n+[[autodoc]] Llama4ImageProcessorFast\n+\n+## Llama4ForConditionalGeneration\n+\n+[[autodoc]] Llama4ForConditionalGeneration\n+- forward\n+\n+## Llama4ForCausalLM\n+\n+[[autodoc]] Llama4ForCausalLM\n+- forward\n+\n+## Llama4TextModel\n+\n+[[autodoc]] Llama4TextModel\n+- forward\n+\n+## Llama4ForCausalLM\n+\n+[[autodoc]] Llama4ForCausalLM\n+- forward\n+\n+## Llama4VisionModel\n+\n+[[autodoc]] Llama4VisionModel\n+- forward"
        },
        {
            "sha": "3b525f7f154824fd2a39d6f893d43f24ddd98043",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -562,6 +562,12 @@\n     \"models.levit\": [\"LevitConfig\"],\n     \"models.lilt\": [\"LiltConfig\"],\n     \"models.llama\": [\"LlamaConfig\"],\n+    \"models.llama4\": [\n+        \"Llama4Config\",\n+        \"Llama4Processor\",\n+        \"Llama4TextConfig\",\n+        \"Llama4VisionConfig\",\n+    ],\n     \"models.llava\": [\n         \"LlavaConfig\",\n         \"LlavaProcessor\",\n@@ -1354,6 +1360,7 @@\n     _import_structure[\"models.detr\"].append(\"DetrImageProcessorFast\")\n     _import_structure[\"models.gemma3\"].append(\"Gemma3ImageProcessorFast\")\n     _import_structure[\"models.got_ocr2\"].append(\"GotOcr2ImageProcessorFast\")\n+    _import_structure[\"models.llama4\"].append(\"Llama4ImageProcessorFast\")\n     _import_structure[\"models.llava\"].append(\"LlavaImageProcessorFast\")\n     _import_structure[\"models.llava_next\"].append(\"LlavaNextImageProcessorFast\")\n     _import_structure[\"models.llava_onevision\"].append(\"LlavaOnevisionImageProcessorFast\")\n@@ -2510,6 +2517,15 @@\n             \"GlmPreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.llama4\"].extend(\n+        [\n+            \"Llama4ForCausalLM\",\n+            \"Llama4ForConditionalGeneration\",\n+            \"Llama4TextModel\",\n+            \"Llama4VisionModel\",\n+            \"Llama4PreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.glpn\"].extend(\n         [\n             \"GLPNForDepthEstimation\",\n@@ -5807,6 +5823,12 @@\n     from .models.levit import LevitConfig\n     from .models.lilt import LiltConfig\n     from .models.llama import LlamaConfig\n+    from .models.llama4 import (\n+        Llama4Config,\n+        Llama4Processor,\n+        Llama4TextConfig,\n+        Llama4VisionConfig,\n+    )\n     from .models.llava import (\n         LlavaConfig,\n         LlavaProcessor,\n@@ -6646,6 +6668,7 @@\n         from .models.detr import DetrImageProcessorFast\n         from .models.gemma3 import Gemma3ImageProcessorFast\n         from .models.got_ocr2 import GotOcr2ImageProcessorFast\n+        from .models.llama4 import Llama4ImageProcessorFast\n         from .models.llava import LlavaImageProcessorFast\n         from .models.llava_next import LlavaNextImageProcessorFast\n         from .models.llava_onevision import LlavaOnevisionImageProcessorFast\n@@ -7827,6 +7850,13 @@\n             LlamaModel,\n             LlamaPreTrainedModel,\n         )\n+        from .models.llama4 import (\n+            Llama4ForCausalLM,\n+            Llama4ForConditionalGeneration,\n+            Llama4PreTrainedModel,\n+            Llama4TextModel,\n+            Llama4VisionModel,\n+        )\n         from .models.llava import (\n             LlavaForConditionalGeneration,\n             LlavaPreTrainedModel,"
        },
        {
            "sha": "cd2a71196028837551ae583d55c876ea30d72191",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 194,
            "deletions": 0,
            "changes": 194,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -1811,6 +1811,200 @@ def reset(self):\n             self.value_cache[layer_idx].zero_()\n \n \n+class HybridChunkedCache(Cache):\n+    \"\"\"\n+    Hybrid Cache class to be used with `torch.compile` for Gemma2 models that alternate between a local sliding window attention\n+    and global attention in every other layer. Under the hood, Hybrid Cache leverages [\"SlidingWindowCache\"] for sliding window attention\n+    and [\"StaticCache\"] for global attention. For more information, see the documentation of each subcomponeent cache class.\n+\n+    Parameters:\n+        config (`PretrainedConfig):\n+            The configuration file defining the shape-related attributes required to initialize the static cache.\n+        max_batch_size (`int`):\n+            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a\n+            smaller batch size is used.\n+        max_cache_len (`int`, *optional*):\n+            The maximum sequence length with which the model will be used.\n+        device (`torch.device` or `str`, *optional*):\n+            The device on which the cache should be initialized. If you're using more than 1 computation device, you\n+            should pass the `layer_device_map` argument instead.\n+        dtype (torch.dtype, *optional*, defaults to `torch.bfloat16`):\n+            The default `dtype` to use when initializing the layer.\n+        layer_device_map (`Optional[Dict[int, Union[str, torch.device, int]]]]`, *optional*):\n+            Mapping between the layers and its device. This is required when you are manually initializing the cache\n+            and the model is split between different gpus. You can know which layers mapped to which device by\n+            checking the associated device_map: `model.hf_device_map`.\n+\n+    Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, HybridCache\n+\n+        >>> model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n+\n+        >>> inputs = tokenizer(text=\"My name is Gemma\", return_tensors=\"pt\")\n+\n+        >>> # Prepare a cache class and pass it to model's forward\n+        >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n+        >>> max_generated_length = inputs.input_ids.shape[1] + 10\n+        >>> past_key_values = HybridCache(config=model.config, max_batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n+        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n+        >>> outputs.past_key_values # access cache filled with key/values from generation\n+        HybridCache()\n+        ```\n+    \"\"\"\n+\n+    # TODO (joao): dive deeper into gemma2 and paligemma -- there are reports of speed loss with compilation. Revert\n+    # ALL changes from the PR that commented the line below when reactivating it.\n+    # is_compileable = True\n+\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        max_batch_size: int,\n+        max_cache_len: Optional[int] = None,\n+        device: Union[torch.device, str, None] = None,\n+        dtype: torch.dtype = torch.bfloat16,\n+        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n+    ) -> None:\n+        super().__init__()\n+        if not hasattr(config, \"sliding_window\") or config.sliding_window is None:\n+            self.sliding_window = getattr(config.get_text_config(), \"attention_chunk_size\", 8192)\n+        else:\n+            self.sliding_window = config.sliding_window\n+        self.max_cache_len = max_cache_len\n+        self.max_batch_size = max_batch_size\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self._dtype = dtype\n+\n+        if hasattr(config.get_text_config(), \"no_rope_layers\"):\n+            self.is_sliding = config.no_rope_layers\n+        else:\n+            layer_switch = getattr(config, \"sliding_window_pattern\", 2)\n+            self.is_sliding = [bool((i + 1) % layer_switch) for i in range(config.num_hidden_layers)]\n+\n+        self.key_cache: List[torch.Tensor] = []\n+        self.value_cache: List[torch.Tensor] = []\n+        self.cumulative_length = [0 for _ in range(config.num_hidden_layers)]\n+\n+    def initialise_cache_layer(self, layer_idx, key_states):\n+        if len(self.key_cache) > layer_idx:\n+            return\n+\n+        num_key_value_heads = key_states.shape[1]\n+        device = key_states.device\n+        global_cache_shape = (self.max_batch_size, num_key_value_heads, self.max_cache_len, self.head_dim)\n+        sliding_cache_shape = (\n+            self.max_batch_size,\n+            num_key_value_heads,\n+            self.sliding_window,\n+            self.head_dim,\n+        )\n+        # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n+        # breaks when updating the cache.\n+        cache_shape = sliding_cache_shape if self.is_sliding[layer_idx] else global_cache_shape\n+        new_layer_key_cache = torch.zeros(cache_shape, dtype=self._dtype, device=device)\n+        new_layer_value_cache = torch.zeros(cache_shape, dtype=self._dtype, device=device)\n+        torch._dynamo.mark_static_address(new_layer_key_cache)\n+        torch._dynamo.mark_static_address(new_layer_value_cache)\n+        self.key_cache.append(new_layer_key_cache)\n+        self.value_cache.append(new_layer_value_cache)\n+\n+    def _sliding_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):\n+        cumulative_length = self.cumulative_length[layer_idx]\n+        is_full = cumulative_length >= max_cache_len\n+        if is_full:\n+            full_key_states = torch.cat((k_out[:, :, 1:, :], key_states), dim=-2)\n+            full_value_states = torch.cat((v_out[:, :, 1:, :], value_states), dim=-2)\n+        elif not is_full and cumulative_length + key_states.shape[2] > max_cache_len:\n+            full_key_states = torch.cat((k_out[:, :, :cumulative_length, :], key_states), dim=-2)\n+            full_value_states = torch.cat((v_out[:, :, :cumulative_length, :], value_states), dim=-2)\n+        else:\n+            self.key_cache[layer_idx].index_copy_(2, cache_position, key_states)\n+            self.value_cache[layer_idx].index_copy_(2, cache_position, value_states)\n+            self.cumulative_length[layer_idx] += key_states.shape[-2]\n+            return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+        self.key_cache[layer_idx].copy_(full_key_states[:, :, -max_cache_len:, :])\n+        self.value_cache[layer_idx].copy_(full_value_states[:, :, -max_cache_len:, :])\n+        self.cumulative_length[layer_idx] += key_states.shape[-2]\n+        # we should return the whole states instead of k_out, v_out to take the whole prompt\n+        # into consideration when building kv cache instead of just throwing away tokens outside of the window\n+        return full_key_states, full_value_states\n+\n+    def _static_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):\n+        k_out[:, :, cache_position] = key_states\n+        v_out[:, :, cache_position] = value_states\n+\n+        self.key_cache[layer_idx] = k_out\n+        self.value_cache[layer_idx] = v_out\n+        return k_out, v_out\n+\n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        layer_idx: int,\n+        cache_kwargs: Optional[Dict[str, Any]] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        if cache_kwargs is None:\n+            cache_kwargs = {}\n+        cache_position = cache_kwargs.get(\"cache_position\")\n+        self.initialise_cache_layer(layer_idx, key_states)\n+\n+        # These two `if` blocks are only reached in multigpu and if `layer_device_map` is not passed. They are used\n+        # when the cache is initialized in the forward pass (e.g. Gemma2)\n+        if self.key_cache[layer_idx].device != key_states.device:\n+            self.key_cache[layer_idx] = self.key_cache[layer_idx].to(key_states.device)\n+        if self.value_cache[layer_idx].device != value_states.device:\n+            self.value_cache[layer_idx] = self.value_cache[layer_idx].to(value_states.device)\n+\n+        k_out = self.key_cache[layer_idx]\n+        v_out = self.value_cache[layer_idx]\n+        key_states = key_states.to(k_out.dtype)\n+        value_states = value_states.to(v_out.dtype)\n+\n+        if self.is_sliding[layer_idx]:\n+            update_fn = self._sliding_update\n+        else:\n+            update_fn = self._static_update\n+\n+        return update_fn(\n+            cache_position,\n+            layer_idx,\n+            key_states,\n+            value_states,\n+            k_out,\n+            v_out,\n+            k_out.shape[2],\n+        )\n+\n+    def get_max_cache_shape(self) -> Optional[int]:\n+        return self.max_cache_len\n+\n+    def get_seq_length(self, layer_idx: Optional[int] = 0):\n+        # Occupied cache == any slot in the 3rd dim (sequence length) holds a non-zero value. To save on compute, let's\n+        # limit the check to the first batch member and head dimension.\n+        # TODO: deprecate this function in favor of `cache_position`\n+        if layer_idx != 0:\n+            raise ValueError(\n+                \"`get_seq_length` on `HybridCache` may get inconsistent results depending on the layer index. \"\n+                \"Using the `layer_idx` argument is not supported.\"\n+            )\n+        if len(self.key_cache) == 0:\n+            return 0\n+        return (self.key_cache[layer_idx][0, 0].any(dim=-1)).sum()\n+\n+    def reset(self):\n+        \"\"\"Resets the cache values while preserving the objects\"\"\"\n+        for layer_idx in range(len(self.key_cache)):\n+            # In-place ops prevent breaking the static address\n+            self.key_cache[layer_idx].zero_()\n+            self.value_cache[layer_idx].zero_()\n+        self.cumulative_length = [0 for _ in range(len(self.cumulative_length))]\n+\n+\n class MambaCache:\n     \"\"\"\n     Cache for mamba model which does not have attention mechanism and key value states."
        },
        {
            "sha": "d5991cae8f9b7627b7925511046a09819f5027d2",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -801,18 +801,19 @@ def __iter__(self):\n \n     def to_diff_dict(self) -> dict[str, Any]:\n         \"\"\"\n-        Removes all attributes from config which correspond to the default config attributes for better readability and\n-        serializes to a Python dictionary.\n+        Removes all attributes from the configuration that correspond to the default config attributes for\n+        better readability, while always retaining the `config` attribute from the class. Serializes to a\n+        Python dictionary.\n \n         Returns:\n-            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance,\n+            Dict[str, Any]: Dictionary of all the attributes that make up this configuration instance.\n         \"\"\"\n         config_dict = self.to_dict()\n \n-        # get the default config dict\n+        # Get the default config dict (from a fresh PreTrainedConfig instance)\n         default_config_dict = PretrainedConfig().to_dict()\n \n-        # get class specific config dict\n+        # Get class-specific config dict if not part of a composition\n         class_config_dict = self.__class__().to_dict() if not self.is_composition else {}\n \n         serializable_config_dict = {}\n@@ -847,8 +848,7 @@ def to_diff_dict(self) -> dict[str, Any]:\n                 if not isinstance(self.quantization_config, dict)\n                 else self.quantization_config\n             )\n-\n-            # pop the `_pre_quantization_dtype` as torch.dtypes are not serializable.\n+            # Pop the `_pre_quantization_dtype` as torch.dtypes are not serializable.\n             _ = serializable_config_dict.pop(\"_pre_quantization_dtype\", None)\n \n         self.dict_torch_dtype_to_str(serializable_config_dict)"
        },
        {
            "sha": "c743480d786b71a7dcf2e8f8896db69dba588851",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -52,6 +52,7 @@\n     from ..cache_utils import (\n         HQQQuantizedCache,\n         HybridCache,\n+        HybridChunkedCache,\n         MambaCache,\n         OffloadedStaticCache,\n         QuantizedCacheConfig,\n@@ -69,6 +70,7 @@\n         \"offloaded_static\": OffloadedStaticCache,\n         \"sliding_window\": SlidingWindowCache,\n         \"hybrid\": HybridCache,\n+        \"hybrid_chunked\": HybridChunkedCache,\n         \"mamba\": MambaCache,\n     }\n     QUANT_BACKEND_CLASSES_MAPPING = {\"quanto\": QuantoQuantizedCache, \"HQQ\": HQQQuantizedCache}\n@@ -416,6 +418,7 @@ def __init__(self, **kwargs):\n             if isinstance(self.cache_config, dict):\n                 self.cache_config = cache_config_class.from_dict(self.cache_config)\n         self.return_legacy_cache = kwargs.pop(\"return_legacy_cache\", None)\n+        self.prefill_chunk_size = kwargs.pop(\"prefill_chunk_size\", None)\n \n         # Parameters for manipulation of the model output logits\n         self.temperature = kwargs.pop(\"temperature\", 1.0)"
        },
        {
            "sha": "bc00e29ba55255b228f91199ee029f1fdd8c1713",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 48,
            "deletions": 1,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -1830,6 +1830,9 @@ def _get_cache(\n \n         Returns the resulting cache object.\n         \"\"\"\n+        if cache_implementation == \"hybrid\" and \"llama4\" in getattr(self.config, \"model_type\", \"\"):\n+            cache_implementation = \"hybrid_chunked\"\n+\n         cache_cls: Cache = NEED_SETUP_CACHE_CLASSES_MAPPING[cache_implementation]\n         requires_cross_attention_cache = (\n             self.config.is_encoder_decoder or model_kwargs.get(\"encoder_outputs\") is not None\n@@ -3405,7 +3408,12 @@ def _sample(\n                 os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n                 model_forward = self.get_compiled_call(generation_config.compile_config)\n \n-        is_prefill = True\n+        if generation_config.prefill_chunk_size is not None:\n+            model_kwargs = self._prefill_chunking(input_ids, generation_config, **model_kwargs)\n+            is_prefill = False\n+        else:\n+            is_prefill = True\n+\n         while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n             # prepare model inputs\n             model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n@@ -4855,6 +4863,45 @@ def _assisted_decoding(\n         else:\n             return input_ids\n \n+    def _prefill_chunking(self, input_ids: torch.LongTensor, generation_config: GenerationConfig, **model_kwargs):\n+        # Even if we are not compiling the forward, flex is always compiled when used. With chunk prefill, we may\n+        # end up needing just a bit more graphs than the default (which is 8). Doing this avoids very cryptic warnings\n+        torch._dynamo.config.cache_size_limit = 64\n+\n+        chunk_size = generation_config.prefill_chunk_size\n+        # Only chunk up the token just before last, so that decoding is completely performed outside this function\n+        # (here we simply prefill the cache)\n+        input_chunks = torch.split(input_ids[:, :-1], chunk_size, dim=-1)\n+\n+        if \"past_key_values\" not in model_kwargs:\n+            raise ValueError(\"Cannot use prefill chunkink without a cache\")\n+\n+        model_forward = self.get_compiled_call(generation_config.compile_config)\n+        attention_mask = model_kwargs.pop(\"attention_mask\", None)\n+\n+        past_length = 0\n+        for input_chunk in input_chunks:\n+            current_length = past_length + input_chunk.shape[-1]\n+            # Prepare inputs\n+            if attention_mask is not None:\n+                model_kwargs[\"attention_mask\"] = attention_mask[:, :current_length]\n+            model_kwargs[\"cache_position\"] = torch.arange(\n+                past_length, current_length, dtype=torch.long, device=input_chunk.device\n+            )\n+            model_kwargs[\"position_ids\"] = model_kwargs[\"cache_position\"].unsqueeze(0)\n+            model_inputs = self.prepare_inputs_for_generation(input_chunk, **model_kwargs)\n+\n+            outputs = model_forward(**model_inputs, return_dict=True)\n+\n+            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n+            past_length = current_length\n+\n+        model_kwargs[\"attention_mask\"] = attention_mask\n+        model_kwargs[\"cache_position\"] = model_kwargs[\"cache_position\"][-1:] + 1\n+        _ = model_kwargs.pop(\"position_ids\", None)\n+\n+        return model_kwargs\n+\n \n def _speculative_sampling(\n     candidate_input_ids,"
        },
        {
            "sha": "8d03c5cf790ee8bb3eac6448e8927445132d2758",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -53,7 +53,7 @@\n         \"unset_hf_deepspeed_config\",\n     ],\n     \"eetq\": [\"replace_with_eetq_linear\"],\n-    \"fbgemm_fp8\": [\"FbgemmFp8Linear\", \"replace_with_fbgemm_fp8_linear\"],\n+    \"fbgemm_fp8\": [\"FbgemmFp8Linear\", \"FbgemmFp8Llama4TextExperts\", \"replace_with_fbgemm_fp8_linear\"],\n     \"finegrained_fp8\": [\"FP8Linear\", \"replace_with_fp8_linear\"],\n     \"fsdp\": [\"is_fsdp_managed_module\"],\n     \"ggml\": [\n@@ -192,7 +192,7 @@\n         unset_hf_deepspeed_config,\n     )\n     from .eetq import replace_with_eetq_linear\n-    from .fbgemm_fp8 import FbgemmFp8Linear, replace_with_fbgemm_fp8_linear\n+    from .fbgemm_fp8 import FbgemmFp8Linear, FbgemmFp8Llama4TextExperts, replace_with_fbgemm_fp8_linear\n     from .finegrained_fp8 import FP8Linear, replace_with_fp8_linear\n     from .fsdp import is_fsdp_managed_module\n     from .ggml import ("
        },
        {
            "sha": "752227914d98b40e919ab34e26e88868a9334c61",
            "filename": "src/transformers/integrations/compressed_tensors.py",
            "status": "added",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fintegrations%2Fcompressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fintegrations%2Fcompressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fcompressed_tensors.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -0,0 +1,54 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from transformers.utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn as nn\n+\n+from transformers.models.llama4.modeling_llama4 import Llama4TextMLP\n+\n+\n+def skip(*args, **kwargs):\n+    pass\n+\n+\n+class CompressedExpertsLinear(nn.Module):\n+    \"\"\"\n+    A module that implements a compressed version of a list of expert modules.\n+    This is specifically designed to work with Llama4TextExperts in MoE layers.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        # Skip random weight initialization for experts. Otherwise,\n+        # the init of this module would take over minutes. For a model\n+        # with tens of layers of experts, it would easily take over 20 minutes.\n+        nn.init.kaiming_uniform_ = skip\n+        nn.init.uniform_ = skip\n+        nn.init.normal_ = skip\n+        super().__init__()\n+        self.num_experts = config.num_local_experts\n+        self.expert_modules = nn.ModuleList([Llama4TextMLP(config) for _ in range(self.num_experts)])\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+    ) -> torch.Tensor:\n+        hidden_states = hidden_states.reshape(self.num_experts, -1, hidden_states.shape[-1])\n+        expert_routed_out_list = []\n+        for expert_idx in range(self.num_experts):\n+            expert_routed_out_list.append(self.expert_modules[expert_idx](hidden_states[expert_idx]))\n+        routed_out = torch.cat(expert_routed_out_list, dim=0)\n+        return routed_out"
        },
        {
            "sha": "5cca37f51510eeb21b94133a152225c27a080d7f",
            "filename": "src/transformers/integrations/fbgemm_fp8.py",
            "status": "modified",
            "additions": 134,
            "deletions": 11,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from ..activations import ACT2FN\n from ..utils import is_accelerate_available, is_fbgemm_gpu_available, is_torch_available, logging\n \n \n@@ -28,36 +29,36 @@\n logger = logging.get_logger(__name__)\n \n \n-class FbgemmFp8Linear(torch.nn.Module):\n+class FbgemmFp8Linear(torch.nn.Linear):\n     def __init__(self, in_features, out_features, bias, weight_dtype=torch.float32):\n-        super().__init__()\n+        super().__init__(in_features, out_features, bias)\n         self.in_features = in_features\n         self.out_features = out_features\n \n-        self.register_buffer(\"weight\", torch.zeros((out_features, in_features), dtype=torch.float8_e4m3fn))\n-        self.register_buffer(\"weight_scale\", torch.zeros((out_features, 1), dtype=weight_dtype))\n+        self.weight = torch.nn.Parameter(torch.zeros((out_features, in_features), dtype=torch.float8_e4m3fn))\n+        self.weight_scale = torch.nn.Parameter(torch.zeros((out_features, 1), dtype=weight_dtype))\n         self.register_buffer(\"input_scale_ub\", torch.zeros([1], dtype=torch.float), persistent=False)\n \n         if bias:\n-            self.register_buffer(\"bias\", torch.zeros((self.out_features), dtype=weight_dtype))\n+            self.bias = torch.nn.Parameter(torch.zeros((self.out_features), dtype=weight_dtype))\n         else:\n             self.bias = None\n \n     def forward(self, x):\n-        num_tokens = None\n         # quantize_fp8_per_row will squash the leading dimensions, so save the desired shape here\n         output_shape = (*x.shape[:-1], -1)\n         # x_quantized and x_scale are not necessarily on the same device as x, this is an issue.\n         # https://github.com/pytorch/FBGEMM/blob/e08af8539c391437f447173863df0f3f6f6f1855/fbgemm_gpu/experimental/gen_ai/src/quantize/quantize.cu#L1237C3-L1237C45\n         x_quantized, x_scale = torch.ops.fbgemm.quantize_fp8_per_row(\n-            x.view(-1, x.shape[-1]), num_tokens, self.input_scale_ub\n+            x.view(-1, x.shape[-1]), scale_ub=self.input_scale_ub\n         )\n         # moving x_quantized, x_scale here creates glibberish output ... However, if we move the output, it works\n         # x_quantized, x_scale = x_quantized.to(x.device), x_scale.to(x.device)\n \n         # The computation still happens on the device where self.weight is even if x_quantized is not on the same device as self.weight\n+        weight_scale_float32 = self.weight_scale.to(torch.float32)\n         output = torch.ops.fbgemm.f8f8bf16_rowwise(\n-            x_quantized, self.weight, x_scale, self.weight_scale, use_fast_accum=True\n+            x_quantized, self.weight, x_scale, weight_scale_float32, use_fast_accum=True\n         )\n         output = output + self.bias if self.bias is not None else output\n         # Hacky for now, we have the output to the device of x\n@@ -67,19 +68,110 @@ def forward(self, x):\n         return output\n \n \n+class FbgemmFp8Llama4TextExperts(nn.Module):\n+    def __init__(self, config, dtype=torch.float32):\n+        super().__init__()\n+        self.num_experts = config.num_local_experts\n+        self.intermediate_size = config.intermediate_size\n+        self.hidden_size = config.hidden_size\n+        self.expert_dim = self.intermediate_size\n+        self.act_fn = ACT2FN[config.hidden_act]\n+        # Register FP8 buffers for gate_up_proj\n+        self.gate_up_proj = torch.nn.Parameter(\n+            torch.zeros((self.num_experts, self.hidden_size, 2 * self.expert_dim), dtype=torch.float8_e4m3fn)\n+        )\n+        self.gate_up_proj_scale = torch.nn.Parameter(\n+            torch.zeros((self.num_experts, 1, self.expert_dim * 2), dtype=torch.float32)\n+        )\n+        # Register FP8 buffers for down_proj\n+        self.down_proj = torch.nn.Parameter(\n+            torch.zeros((self.num_experts, self.expert_dim, self.hidden_size), dtype=torch.float8_e4m3fn)\n+        )\n+        self.down_proj_scale = torch.nn.Parameter(\n+            torch.zeros((self.num_experts, self.hidden_size, 1), dtype=torch.float32)\n+        )\n+        # Register input scale upper bound\n+        self.register_buffer(\"input_scale_ub\", torch.zeros([1], dtype=torch.float), persistent=False)\n+\n+    def forward(self, hidden_states):\n+        \"\"\"\n+        Args:\n+            hidden_states (torch.Tensor): (batch_size * token_num, hidden_size)\n+        Returns:\n+            torch.Tensor: (batch_size * token_num, hidden_size)\n+        \"\"\"\n+        # Reshape hidden states for expert computation\n+        hidden_states = hidden_states.view(self.num_experts, -1, self.hidden_size)\n+        num_tokens = None\n+\n+        # Pre-allocate tensor for all expert outputs with same shape as hidden_states\n+        next_states = torch.empty_like(hidden_states)\n+\n+        for i in range(self.num_experts):\n+            # Extract expert's hidden states\n+            expert_hidden = hidden_states[i]\n+            expert_hidden_reshaped = expert_hidden.reshape(-1, self.hidden_size)\n+            # Quantize for this expert\n+            expert_quantized, expert_scale = torch.ops.fbgemm.quantize_fp8_per_row(\n+                expert_hidden_reshaped, num_tokens, self.input_scale_ub\n+            )\n+            sharded_expert_dim = self.gate_up_proj.shape[-1] // 2\n+            gate_up_proj_scale_float32 = self.gate_up_proj_scale.to(torch.float32)\n+\n+            gate = torch.ops.fbgemm.f8f8bf16_rowwise(\n+                expert_quantized,\n+                self.gate_up_proj[i].transpose(0, 1)[:sharded_expert_dim].contiguous(),\n+                expert_scale,\n+                gate_up_proj_scale_float32[i][0][:sharded_expert_dim].view(-1, 1).contiguous(),\n+                use_fast_accum=True,\n+            )\n+\n+            up = torch.ops.fbgemm.f8f8bf16_rowwise(\n+                expert_quantized,\n+                self.gate_up_proj[i].transpose(0, 1)[sharded_expert_dim:].contiguous(),\n+                expert_scale,\n+                gate_up_proj_scale_float32[i][0][sharded_expert_dim:].view(-1, 1).contiguous(),\n+                use_fast_accum=True,\n+            )\n+\n+            activated = up * self.act_fn(gate)\n+\n+            activated_quantized, activated_scale = torch.ops.fbgemm.quantize_fp8_per_row(\n+                activated, num_tokens, self.input_scale_ub\n+            )\n+\n+            down_proj_scale_float32 = self.down_proj_scale.to(torch.float32)\n+            expert_output = torch.ops.fbgemm.f8f8bf16_rowwise(\n+                activated_quantized,\n+                self.down_proj[i].transpose(0, 1).contiguous(),\n+                activated_scale,\n+                down_proj_scale_float32[i].view(-1, 1).contiguous(),\n+                use_fast_accum=True,\n+            )\n+\n+            next_states[i] = expert_output\n+        next_states = next_states.to(hidden_states.device)\n+        return next_states.view(-1, self.hidden_size)\n+\n+\n def _replace_with_fbgemm_fp8_linear(\n     model,\n     modules_to_not_convert=None,\n     current_key_name=None,\n     quantization_config=None,\n     has_been_replaced=False,\n     pre_quantized=False,\n+    config=None,\n+    tp_plan=None,\n ):\n     \"\"\"\n     Private method that wraps the recursion for module replacement.\n \n     Returns the converted model and a boolean that indicates if the conversion has been successfull or not.\n     \"\"\"\n+\n+    import re\n+\n     if current_key_name is None:\n         current_key_name = []\n \n@@ -105,9 +197,27 @@ def _replace_with_fbgemm_fp8_linear(\n                     # Force requires grad to False to avoid unexpected errors\n                     model._modules[name].requires_grad_(False)\n                 # set non persistant buffer outside of init_empty_weights\n+                model._modules[name].input_scale_ub = torch.tensor(\n+                    [quantization_config.activation_scale_ub],\n+                    dtype=torch.float,\n+                )\n+        if module.__class__.__name__ == \"Llama4TextExperts\" and name not in modules_to_not_convert:\n+            current_key_name_str = \".\".join(current_key_name)\n+            if not any(\n+                (key + \".\" in current_key_name_str) or (key == current_key_name_str) for key in modules_to_not_convert\n+            ):\n+                with init_empty_weights(include_buffers=True):\n+                    tp_plan[re.sub(r\"\\d+\", \"*\", current_key_name_str + \".gate_up_proj_scale\")] = tp_plan[\n+                        re.sub(r\"\\d+\", \"*\", current_key_name_str + \".gate_up_proj\")\n+                    ]\n+                    tp_plan[re.sub(r\"\\d+\", \"*\", current_key_name_str + \".down_proj_scale\")] = None\n+                    model._modules[name] = FbgemmFp8Llama4TextExperts(\n+                        config.text_config,\n+                    )\n                 model._modules[name].input_scale_ub = torch.tensor(\n                     [quantization_config.activation_scale_ub], dtype=torch.float\n                 )\n+\n         if len(list(module.children())) > 0:\n             _, has_been_replaced = _replace_with_fbgemm_fp8_linear(\n                 module,\n@@ -116,14 +226,22 @@ def _replace_with_fbgemm_fp8_linear(\n                 quantization_config,\n                 has_been_replaced=has_been_replaced,\n                 pre_quantized=pre_quantized,\n+                config=config,\n+                tp_plan=tp_plan,\n             )\n         # Remove the last key for recursion\n         current_key_name.pop(-1)\n     return model, has_been_replaced\n \n \n def replace_with_fbgemm_fp8_linear(\n-    model, modules_to_not_convert=None, current_key_name=None, quantization_config=None, pre_quantized=False\n+    model,\n+    modules_to_not_convert=None,\n+    current_key_name=None,\n+    quantization_config=None,\n+    pre_quantized=False,\n+    config=None,\n+    tp_plan=None,\n ):\n     \"\"\"\n     A helper function to replace all `torch.nn.Linear` modules by `FbgemmFp8Linear` modules.\n@@ -151,9 +269,14 @@ def replace_with_fbgemm_fp8_linear(\n         modules_to_not_convert.extend(quantization_config.modules_to_not_convert)\n     modules_to_not_convert = list(set(modules_to_not_convert))\n     model, has_been_replaced = _replace_with_fbgemm_fp8_linear(\n-        model, modules_to_not_convert, current_key_name, quantization_config, pre_quantized=pre_quantized\n+        model,\n+        modules_to_not_convert,\n+        current_key_name,\n+        quantization_config,\n+        pre_quantized=pre_quantized,\n+        config=config,\n+        tp_plan=tp_plan,\n     )\n-\n     if not has_been_replaced:\n         logger.warning(\n             \"You are loading your model using FP8 quantization but no linear modules were found in your model.\""
        },
        {
            "sha": "1aa146e4a40737edd100059f05208da1a6d5e3dc",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 61,
            "deletions": 17,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -34,10 +34,7 @@\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import (\n-        BlockMask,\n-        flex_attention,\n-    )\n+    from torch.nn.attention.flex_attention import BlockMask, flex_attention\n     from torch.nn.attention.flex_attention import (\n         create_block_mask as create_block_causal_mask_flex,\n     )\n@@ -64,14 +61,23 @@ def __init__(self):\n         Initialize or update the singleton instance.\n         \"\"\"\n         if self._is_flex_compiled is False:\n-            self._compiled_flex_attention = torch.compile(flex_attention, dynamic=False)\n+            self._compiled_flex_attention = torch.compile(flex_attention, backend=\"inductor\")\n             self._is_flex_compiled = True\n \n     def __call__(self):\n         return self._compiled_flex_attention\n \n \n-def make_flex_block_causal_mask(attention_mask_2d: torch.Tensor) -> \"BlockMask\":\n+Offset = Union[torch.Tensor, int]\n+\n+\n+def make_flex_block_causal_mask(\n+    attention_mask_2d: torch.Tensor,\n+    attention_chunk_size: Optional[int] = None,\n+    query_length=None,\n+    key_length=None,\n+    offsets: Optional[Tuple[Offset, Offset]] = None,\n+) -> \"BlockMask\":\n     \"\"\"\n     Create a block causal document mask for a batch of sequences, both packed and unpacked.\n     Create Block causal logic and passing it into :func:`torch.nn.attention.flex_attention.create_block_mask`.\n@@ -94,10 +100,13 @@ def make_flex_block_causal_mask(attention_mask_2d: torch.Tensor) -> \"BlockMask\":\n     Returns:\n         BlockMask\n     \"\"\"\n+    attention_mask_2d = torch.nn.functional.pad(attention_mask_2d, value=0, pad=(0, key_length))\n     device = attention_mask_2d.device\n+    document_ids = attention_mask_2d.clone()\n \n-    document_ids = attention_mask_2d\n-    batch_size, total_seq_len = document_ids.shape\n+    if attention_chunk_size is not None:\n+        # we create an arange, then we just // by chunk size to get [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]\n+        document_ids = (document_ids.fill_(1).cumsum(-1) - 1) // (attention_chunk_size)\n \n     # Instead of passing a tensor mask, flex attention requires a mask_mod function\n     # that determines which elements of QK^T should be included in the attention\n@@ -112,18 +121,30 @@ def causal_mask_mod(batch_idx, head_idx, q_idx, kv_idx):\n         See :func:`~torchtune.modules.attention_utils.create_block_causal_mask`\n         for an illustration.\n         \"\"\"\n-        causal_mask = q_idx >= kv_idx\n+        causal_mask = q_idx >= kv_idx  # not valid when decoding\n         document_mask = document_ids[batch_idx, q_idx] == document_ids[batch_idx, kv_idx]\n-        padding_mask = document_ids[batch_idx, q_idx] > 0\n-        return causal_mask & document_mask & padding_mask\n-\n+        padding_mask = attention_mask_2d[batch_idx, q_idx] > 0\n+        final_mask = causal_mask & padding_mask & document_mask\n+        return final_mask\n+\n+    if offsets is not None:\n+        q_offset = offsets[0]\n+        kv_offset = offsets[1]\n+\n+        def mask_mod(batch_idx, head_idx, q_idx, kv_idx):\n+            offset_q = q_idx + q_offset\n+            offset_kv = kv_idx + kv_offset\n+            return causal_mask_mod(batch_idx, head_idx, offset_q, offset_kv)\n+    else:\n+        mask_mod = causal_mask_mod\n     return create_block_causal_mask_flex(\n-        mask_mod=causal_mask_mod,\n-        B=batch_size,\n+        mask_mod=mask_mod,\n+        B=1,\n         H=None,  # attention head\n-        Q_LEN=total_seq_len,\n-        KV_LEN=total_seq_len,\n+        Q_LEN=query_length,\n+        KV_LEN=key_length,\n         device=device,\n+        _compile=True,\n     )\n \n \n@@ -144,6 +165,18 @@ def compile_friendly_flex_attention(\n     )\n \n \n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n def flex_attention_forward(\n     module: torch.nn.Module,\n     query: torch.Tensor,\n@@ -174,14 +207,25 @@ def score_mod(score, batch_idx, head_idx, q_idx, kv_idx):\n             score = score + head_mask[batch_idx][head_idx][0][0]\n         return score\n \n+    enable_gqa = True\n+    num_local_query_heads = query.shape[1]\n+\n+    # When running TP this helps:\n+    if not ((num_local_query_heads & (num_local_query_heads - 1)) == 0):\n+        key = repeat_kv(key, query.shape[1] // key.shape[1])\n+        value = repeat_kv(value, query.shape[1] // value.shape[1])\n+        enable_gqa = False\n+\n+    kernel_options = kwargs.get(\"kernel_options\", None)\n     attn_output, attention_weights = compile_friendly_flex_attention(\n         query,\n         key,\n         value,\n         score_mod=score_mod,\n         block_mask=block_mask,\n-        enable_gqa=True,\n+        enable_gqa=enable_gqa,\n         scale=scaling,\n+        kernel_options=kernel_options,\n         # Last time checked on PyTorch == 2.5.1: Flex Attention always computes the lse regardless.\n         # For simplification, we thus always return it as no additional computations are introduced.\n         return_lse=True,"
        },
        {
            "sha": "9c924c048ad52929a2d0f890a22295d5a56ef505",
            "filename": "src/transformers/integrations/sdpa_attention.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -31,7 +31,7 @@ def sdpa_attention_forward(\n         value = repeat_kv(value, module.num_key_value_groups)\n \n     causal_mask = attention_mask\n-    if attention_mask is not None:\n+    if attention_mask is not None and causal_mask.ndim == 4:\n         causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n \n     # SDPA with memory-efficient backend is bugged with non-contiguous inputs and custom attn_mask for some torch versions"
        },
        {
            "sha": "34b21444fe35faed6487caf8f655c14e660487d4",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 140,
            "deletions": 18,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -61,6 +61,21 @@ def _blocks_to_block_sizes(total_size: int, blocks: Union[int, List[int]]) -> Li\n         return [single_size] * blocks\n \n \n+str_to_torch_dtype = {\n+    \"BOOL\": torch.bool,\n+    \"U8\": torch.uint8,\n+    \"I8\": torch.int8,\n+    \"I16\": torch.int16,\n+    \"F16\": torch.float16,\n+    \"BF16\": torch.bfloat16,\n+    \"I32\": torch.int32,\n+    \"F32\": torch.float32,\n+    \"F64\": torch.float64,\n+    \"I64\": torch.int64,\n+    \"F8_E4M3\": torch.float8_e4m3fn,\n+}\n+\n+\n def get_packed_weights(param, empty_param, device_mesh, rank, dim):\n     \"\"\"\n     When weights are packed (gate_up_proj), we need to make sure each shard gets its correct share.\n@@ -106,6 +121,12 @@ def get_packed_weights(param, empty_param, device_mesh, rank, dim):\n         tensors_slices += range(block_offset + start, block_offset + stop)\n         block_offset += block_size\n \n+    slice_dtype = slice_.get_dtype()\n+    # Handle F8_E4M3 dtype by converting to float16 before slicing\n+    # Without upcasting, the slicing causes : RuntimeError: \"index_cpu\" not implemented for 'Float8_e4m3fn'\n+    if slice_dtype == \"F8_E4M3\":\n+        slice_ = slice_[...].to(torch.float16)\n+\n     if dim == 0:\n         tensor = slice_[tensors_slices, ...]\n     elif dim == 1 or dim == -2:\n@@ -114,7 +135,7 @@ def get_packed_weights(param, empty_param, device_mesh, rank, dim):\n         tensor = slice_[..., tensors_slices]\n     else:\n         raise ValueError(f\"Unsupported dim {dim}, only dim 0, 1 or 2 are supported\")\n-    return tensor\n+    return tensor.to(str_to_torch_dtype[slice_dtype])\n \n \n def get_tensor_shard(param, empty_param, device_mesh, rank, dim):\n@@ -199,11 +220,12 @@ def __init__(\n     @staticmethod\n     def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n         if isinstance(inputs[0], DTensor):\n-            inputs[0] = inputs[0].to_local()\n+            inputs = inputs[0].to_local()\n         return inputs\n \n     @staticmethod\n     def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n+        # this op cannot be asynch, otherwise it completely breaks the outputs of models\n         torch.distributed.all_reduce(outputs[0], op=torch.distributed.ReduceOp.SUM, async_op=False)\n         return outputs\n \n@@ -266,7 +288,7 @@ def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_\n \n         # transform the input layouts to the desired layouts of ColwiseParallel\n         if input_layouts != desired_input_layouts:\n-            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=True)\n+            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=False)\n         return input_tensor\n \n     def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n@@ -291,7 +313,7 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n     def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n         # outputs is a shard on last dimension DTensor, i.e. Shard(-1)\n         if outputs.placements != output_layouts:\n-            outputs = outputs.redistribute(placements=output_layouts, async_op=True)\n+            outputs = outputs.redistribute(placements=output_layouts, async_op=False)\n         # back to local tensor\n         return outputs.to_local() if use_local_output else outputs\n \n@@ -343,16 +365,6 @@ def __init__(\n         self.use_local_output = use_local_output\n         self.use_dtensor = use_dtensor\n \n-    @staticmethod\n-    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n-        input_tensor = inputs[0]\n-        if not isinstance(input_tensor, DTensor):\n-            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n-\n-        if input_layouts != desired_input_layouts:\n-            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=True)\n-        return input_tensor\n-\n     def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n         # Rowwise shard weight to Shard(1), bias to Replicate(), weight be Shard(1)\n         # means Rowwise as nn.Linear is input * weight^T + bias, where\n@@ -371,13 +383,29 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n             parameter = DTensor.from_local(parameter, device_mesh, shard, run_check=False)\n         return nn.Parameter(parameter)\n \n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n+        if hasattr(mod, \"bias\") and mod.bias is not None:\n+            mod._bias = mod.bias\n+            mod.bias = None\n+\n+        input_tensor = inputs[0]\n+        if not isinstance(input_tensor, DTensor):\n+            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n+\n+        if input_layouts != desired_input_layouts:\n+            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=True)\n+        return input_tensor\n+\n     @staticmethod\n     def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n         # Rowwise sharding produces partial output, depending on output layouts:\n         # 1. to replicate -> allreduce\n         # 2. to shard -> reduce_scatter\n         if outputs.placements != output_layouts:\n             outputs = outputs.redistribute(placements=output_layouts, async_op=True)\n+        if hasattr(mod, \"_bias\"):\n+            outputs += mod._bias\n         # back to local tensor if use_local_output is True\n         return outputs.to_local() if use_local_output else outputs\n \n@@ -418,6 +446,90 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n         return nn.Parameter(parameter)\n \n \n+class SequenceParallel(TensorParallelLayer):\n+    \"\"\"\n+    SequenceParallel replicates a compatible ``nn.Module`` parameters and runs the sharded computation with\n+    input sharded on the sequence dimension. This currently supports ``nn.LayerNorm``, ``nn.Dropout``, and the\n+    `RMSNorm python implementation <https://github.com/facebookresearch/llama/blob/main/llama/model.py#L34>`__\n+\n+    This style implements the operation that is described in the paper\n+    `Reducing Activation Recomputation in Large Transformer Models <https://arxiv.org/abs/2205.05198>`__\n+\n+    If the input passed in to this ``nn.Module`` is a :class:`torch.Tensor`, it assumes that the input is already sharded\n+    on the sequence dimension and converts the input to a :class:`DTensor` sharded on the sequence dimension. If the input\n+    passed in to this ``nn.Module`` is already a :class:`DTensor` but is not sharded on the sequence dimension, it would\n+    redistribute the input to be sharded on the sequence dimension.\n+\n+    The output of the ``nn.Module`` will be sharded on the sequence dimension.\n+\n+    Keyword Args:\n+        sequence_dim (int, optional):\n+            The sequence dimension of the input tensor for the ``nn.Module``, this is used to annotate the input tensor to\n+            become a DTensor that is sharded on the sequence dimension, default: 1.\n+        use_local_output (bool, optional):\n+            Whether to use local :class:`torch.Tensor` instead of :class:`DTensor` for the module output, default: False.\n+    Returns:\n+        A :class:`ParallelStyle` object that represents Sequence Parallel of the ``nn.Module``.\n+\n+    Example::\n+        >>> # xdoctest: +SKIP(failing)\n+        >>> from torch.distributed.tensor.parallel import parallelize_module, SequenceParallel\n+        >>> from torch.distributed.device_mesh import init_device_mesh\n+        >>> ...\n+        >>> m = Model(...)  # m is a nn.Module that contains a \"norm\" nn.LayerNorm submodule\n+        >>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n+        >>>\n+        >>> # By default, the input of the \"norm\" will be converted to DTensor that shards on the sequence dim\n+        >>> # and the output of \"norm\" will return a sharded on sequence dimension :class:`DTensor`.\n+        >>>\n+        >>> sharded_mod = parallelize_module(m, tp_mesh, {\"norm\": SequenceParallel()}),\n+        >>> ...\n+\n+    .. note:: SequenceParallel style assumes ones initialization if there are weights in the nn.Module (i.e.\n+        ``nn.LayerNorm`` or ``RMSNorm``, and they by default have ones initialization). If you have custom\n+        inits for the weights on those modules, you need to broadcast the weights before/after parallelizing\n+        to ensure that they are replicated.\n+    \"\"\"\n+\n+    def __init__(self, *, sequence_dim: int = 1, use_local_output: bool = False, use_dtensor=False):\n+        super().__init__()\n+        self.input_layouts = (Replicate(),)\n+        self.desired_input_layouts = (Shard(1),)\n+        self.output_layouts = (Replicate(),)\n+        self.use_local_output = use_local_output\n+        self.use_dtensor = True\n+        self.sequence_sharding = (Shard(sequence_dim),)\n+        self.use_local_output = use_local_output\n+\n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n+        input_tensor = inputs[0]\n+        if not isinstance(input_tensor, DTensor):\n+            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n+        if input_layouts != desired_input_layouts:\n+            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=True)\n+        return input_tensor\n+\n+    @staticmethod\n+    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n+        outputs = outputs.redistribute(\n+            placements=(Replicate(),), async_op=True\n+        )  # maybe we have to replicate ? because next layer is not sharded\n+        return outputs.to_local()  # if use_local_output else outputs\n+\n+    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+        # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n+        # means Colwise as Linear is input * weight^T + bias, where\n+        # weight would become Shard(1)\n+        parameter = param[:]\n+        parameter = parameter.to(param_casting_dtype)\n+        if to_contiguous:\n+            parameter = parameter.contiguous()\n+        if self.use_dtensor:\n+            parameter = DTensor.from_local(parameter, device_mesh, [Replicate()], run_check=False)\n+        return nn.Parameter(parameter)\n+\n+\n SUPPORTED_TP_STYLES = {\n     \"colwise\",\n     \"rowwise\",\n@@ -428,6 +540,7 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n     \"local\",\n     \"gather\",\n     \"local_packed_rowwise\",\n+    \"sequence_parallel\",\n }\n \n \n@@ -459,6 +572,8 @@ def translate_to_torch_parallel_style(style: str):\n         return GatherParallel()\n     elif style == \"local_packed_rowwise\":\n         return PackedRowwiseParallel(use_dtensor=False)\n+    elif style == \"sequence_parallel\":\n+        return SequenceParallel()\n     else:\n         raise ValueError(f\"Unsupported parallel style value: {style}\")\n \n@@ -518,6 +633,7 @@ def shard_and_distribute_module(\n     tp_plan = model._tp_plan\n     module_to_tp = model.get_submodule(param_name)\n     current_module_plan = None\n+    rank = int(rank)\n     generic_param_name = re.sub(r\"\\d+\", \"*\", parameter_name)\n     if generic_param_name in tp_plan:\n         current_module_plan = tp_plan[generic_param_name]\n@@ -531,12 +647,18 @@ def shard_and_distribute_module(\n         module_to_tp._is_hooked = True\n \n     if current_module_plan is not None:\n-        tp_layer = translate_to_torch_parallel_style(current_module_plan)\n-        param = tp_layer.partition_tensor(\n-            param, empty_param, param_type, param_casting_dtype, is_contiguous, rank, device_mesh\n-        )\n+        try:\n+            tp_layer = translate_to_torch_parallel_style(current_module_plan)\n+            param = tp_layer.partition_tensor(\n+                param, empty_param, param_type, param_casting_dtype, is_contiguous, rank, device_mesh\n+            )\n+        except NotImplementedError as e:\n+            print(\n+                f\"Trying to prepare {parameter_name}, but it's not supported. Corresponding module: {module_to_tp} Fix it's TP plan, current layer: {tp_layer} : {e}\"\n+            )\n     else:\n         # TODO log no plan modules in set\n+        # print(\"No plan for\", parameter_name,end =\"\\n\")\n         param = param[...].to(param_casting_dtype)\n         if is_contiguous:\n             param = param.contiguous()"
        },
        {
            "sha": "6a3286cbc948eb37398ceebefff12ac3adf8ed07",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 13,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -484,6 +484,7 @@ def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n     \"F32\": torch.float32,\n     \"F64\": torch.float64,\n     \"I64\": torch.int64,\n+    \"F8_E4M3\": torch.float8_e4m3fn,\n }\n \n if is_torch_greater_or_equal(\"2.1.0\"):\n@@ -1914,16 +1915,11 @@ def post_init(self):\n                     )\n \n         # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n-        if self.base_model is self:\n-            self._pp_plan = (\n-                self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else None\n-            )\n-            self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n-        else:\n-            self._tp_plan = self._tp_plan or {}\n-            for name, module in self.named_children():\n-                if plan := getattr(module, \"_tp_plan\", None):\n-                    self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.items()})\n+        self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else None\n+        self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n+        for name, module in self.named_children():\n+            if plan := getattr(module, \"_tp_plan\", None):\n+                self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n \n         if self._tp_plan is not None and is_torch_greater_or_equal(\"2.3\"):\n             for _, v in self._tp_plan.items():\n@@ -4054,6 +4050,7 @@ def from_pretrained(\n                 import sys\n \n                 sys.stdout = open(os.devnull, \"w\")\n+                sys.stderr = open(os.devnull, \"w\")\n             # This is the easiest way to dispatch to the current process device\n             device_map = tp_device\n             # Assuming sharding the model onto the world\n@@ -4238,6 +4235,7 @@ def from_pretrained(\n             )\n             torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n             device_map = hf_quantizer.update_device_map(device_map)\n+            config = hf_quantizer.update_tp_plan(config)\n \n             # In order to ensure popular quantization methods are supported. Can be disable with `disable_telemetry`\n             if hasattr(hf_quantizer.quantization_config.quant_method, \"value\"):\n@@ -4370,9 +4368,8 @@ def from_pretrained(\n \n         if hf_quantizer is not None:\n             hf_quantizer.preprocess_model(\n-                model=model, device_map=device_map, keep_in_fp32_modules=model._keep_in_fp32_modules\n+                model=model, device_map=device_map, keep_in_fp32_modules=model._keep_in_fp32_modules, config=config\n             )\n-\n             # We store the original dtype for quantized models as we cannot easily retrieve it\n             # once the weights have been quantized\n             # Note that once you have loaded a quantized model, you can't change its dtype so this will\n@@ -4901,7 +4898,7 @@ def _load_pretrained_model(\n                         name,\n                         casting_dtype,\n                         to_contiguous,\n-                        tp_device.index,\n+                        os.environ[\"RANK\"],\n                         device_mesh,\n                     )\n \n@@ -5174,6 +5171,8 @@ def get_compiled_call(self, compile_config: CompileConfig):\n         want to use compiled version to avoid recomputing the graph with new shapes) and iterative decoding\n         (where we want the speed-ups of compiled version with static shapes).\"\"\"\n         # Only reset it if not present or different from previous config\n+        if \"llama4\" in self.config.model_type:  # TODO try to enable for FULL COMPILE HYBRID CACHE SUPPORT\n+            return self.__call__\n         default_config = getattr(self.generation_config, \"compile_config\", CompileConfig())\n         if (\n             not hasattr(self, \"_compiled_call\")"
        },
        {
            "sha": "08cec64b41eeb8f7121ec5e4f1e959f15dd7b333",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -148,6 +148,7 @@\n     levit,\n     lilt,\n     llama,\n+    llama4,\n     llava,\n     llava_next,\n     llava_next_video,"
        },
        {
            "sha": "d21a16beb124c85e92182f569196be07a3d21a85",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -544,10 +544,6 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n             if kwargs_orig.get(\"quantization_config\", None) is not None:\n                 kwargs[\"quantization_config\"] = kwargs_orig[\"quantization_config\"]\n \n-        # AutoClass-specific config manipulation\n-        config = copy.deepcopy(config)\n-        config = cls._prepare_config_for_auto_class(config)\n-\n         has_remote_code = hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map\n         has_local_code = type(config) in cls._model_mapping.keys()\n         trust_remote_code = resolve_trust_remote_code(\n@@ -570,6 +566,8 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n             )\n         elif type(config) in cls._model_mapping.keys():\n             model_class = _get_model_class(config, cls._model_mapping)\n+            if model_class.config_class == config.sub_configs.get(\"text_config\", None):\n+                config = config.get_text_config()\n             return model_class.from_pretrained(\n                 pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n             )"
        },
        {
            "sha": "759b7ad3d98a182cc61f2fea466ee79b94bd8f06",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -170,6 +170,8 @@\n         (\"levit\", \"LevitConfig\"),\n         (\"lilt\", \"LiltConfig\"),\n         (\"llama\", \"LlamaConfig\"),\n+        (\"llama4\", \"Llama4Config\"),\n+        (\"llama4_text\", \"Llama4TextConfig\"),\n         (\"llava\", \"LlavaConfig\"),\n         (\"llava_next\", \"LlavaNextConfig\"),\n         (\"llava_next_video\", \"LlavaNextVideoConfig\"),\n@@ -519,6 +521,8 @@\n         (\"llama\", \"LLaMA\"),\n         (\"llama2\", \"Llama2\"),\n         (\"llama3\", \"Llama3\"),\n+        (\"llama4\", \"Llama4\"),\n+        (\"llama4_text\", \"Llama4ForCausalLM\"),\n         (\"llava\", \"LLaVa\"),\n         (\"llava_next\", \"LLaVA-NeXT\"),\n         (\"llava_next_video\", \"LLaVa-NeXT-Video\"),\n@@ -776,6 +780,7 @@\n         (\"rt_detr_resnet\", \"rt_detr\"),\n         (\"granitevision\", \"llava_next\"),\n         (\"sam_vision_model\", \"sam\"),\n+        (\"llama4_text\", \"llama4\"),\n     ]\n )\n "
        },
        {
            "sha": "2f9d42fcdb7290b617d13e29c166a070b89f6db5",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -104,6 +104,7 @@\n             (\"layoutlmv2\", (\"LayoutLMv2ImageProcessor\",)),\n             (\"layoutlmv3\", (\"LayoutLMv3ImageProcessor\",)),\n             (\"levit\", (\"LevitImageProcessor\",)),\n+            (\"llama4\", (\"Llama4ImageProcessor\", \"Llama4ImageProcessorFast\")),\n             (\"llava\", (\"LlavaImageProcessor\", \"LlavaImageProcessorFast\")),\n             (\"llava_next\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),\n             (\"llava_next_video\", (\"LlavaNextVideoImageProcessor\",)),"
        },
        {
            "sha": "d33d0f20d5406755d4617c4e9c49cebad8f29abc",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 6,
            "deletions": 25,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -17,7 +17,6 @@\n import warnings\n from collections import OrderedDict\n \n-from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n from .auto_factory import (\n     _BaseAutoBackboneClass,\n@@ -161,6 +160,7 @@\n         (\"levit\", \"LevitModel\"),\n         (\"lilt\", \"LiltModel\"),\n         (\"llama\", \"LlamaModel\"),\n+        (\"llama4\", \"Llama4ForConditionalGeneration\"),\n         (\"longformer\", \"LongformerModel\"),\n         (\"longt5\", \"LongT5Model\"),\n         (\"luke\", \"LukeModel\"),\n@@ -547,6 +547,8 @@\n         (\"jamba\", \"JambaForCausalLM\"),\n         (\"jetmoe\", \"JetMoeForCausalLM\"),\n         (\"llama\", \"LlamaForCausalLM\"),\n+        (\"llama4\", \"Llama4ForCausalLM\"),\n+        (\"llama4_text\", \"Llama4ForCausalLM\"),\n         (\"mamba\", \"MambaForCausalLM\"),\n         (\"mamba2\", \"Mamba2ForCausalLM\"),\n         (\"marian\", \"MarianForCausalLM\"),\n@@ -634,6 +636,7 @@\n         (\"ijepa\", \"IJepaModel\"),\n         (\"imagegpt\", \"ImageGPTModel\"),\n         (\"levit\", \"LevitModel\"),\n+        (\"llama4\", \"Llama4VisionModel\"),\n         (\"mllama\", \"MllamaVisionModel\"),\n         (\"mobilenet_v1\", \"MobileNetV1Model\"),\n         (\"mobilenet_v2\", \"MobileNetV2Model\"),\n@@ -849,6 +852,7 @@\n         (\"idefics3\", \"Idefics3ForConditionalGeneration\"),\n         (\"instructblip\", \"InstructBlipForConditionalGeneration\"),\n         (\"kosmos-2\", \"Kosmos2ForConditionalGeneration\"),\n+        (\"llama4\", \"Llama4ForConditionalGeneration\"),\n         (\"llava\", \"LlavaForConditionalGeneration\"),\n         (\"llava_next\", \"LlavaNextForConditionalGeneration\"),\n         (\"llava_onevision\", \"LlavaOnevisionForConditionalGeneration\"),\n@@ -1492,6 +1496,7 @@\n         (\"emu3\", \"Emu3TextModel\"),\n         (\"flaubert\", \"FlaubertModel\"),\n         (\"ibert\", \"IBertModel\"),\n+        (\"llama4\", \"Llama4TextModel\"),\n         (\"longformer\", \"LongformerModel\"),\n         (\"mllama\", \"MllamaTextModel\"),\n         (\"mobilebert\", \"MobileBertModel\"),\n@@ -1678,30 +1683,6 @@ class _AutoModelWithLMHead(_BaseAutoModelClass):\n class AutoModelForCausalLM(_BaseAutoModelClass):\n     _model_mapping = MODEL_FOR_CAUSAL_LM_MAPPING\n \n-    @classmethod\n-    def _prepare_config_for_auto_class(cls, config: PretrainedConfig) -> PretrainedConfig:\n-        \"\"\"\n-        Additional autoclass-specific config post-loading manipulation. In this specific autoclass, if the config has\n-        a nested text decoder section, uses that section instead.\n-\n-        Under the hood, multimodal models mapped by AutoModelForCausalLM assume the text decoder receives its own\n-        config, rather than the config for the whole model. This is used e.g. to load the text-only part of a VLM.\n-        \"\"\"\n-        possible_text_config_names = (\"decoder\", \"generator\", \"text_config\")\n-        text_config_names = []\n-        for text_config_name in possible_text_config_names:\n-            if hasattr(config, text_config_name):\n-                text_config_names += [text_config_name]\n-\n-        text_config = config.get_text_config(decoder=True)\n-        if text_config_names and type(text_config) in cls._model_mapping.keys():\n-            warnings.warn(\n-                \"Loading a multimodal model with `AutoModelForCausalLM` is deprecated and will be removed in v5. \"\n-                \"`AutoModelForCausalLM` will be used to load only the text-to-text generation module.\",\n-                FutureWarning,\n-            )\n-        return config\n-\n \n AutoModelForCausalLM = auto_class_update(AutoModelForCausalLM, head_doc=\"causal language modeling\")\n "
        },
        {
            "sha": "6e655edff1c982a9b78ae4dbb60957508d515b46",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -77,6 +77,7 @@\n         (\"kosmos-2\", \"Kosmos2Processor\"),\n         (\"layoutlmv2\", \"LayoutLMv2Processor\"),\n         (\"layoutlmv3\", \"LayoutLMv3Processor\"),\n+        (\"llama4\", \"Llama4Processor\"),\n         (\"llava\", \"LlavaProcessor\"),\n         (\"llava_next\", \"LlavaNextProcessor\"),\n         (\"llava_next_video\", \"LlavaNextVideoProcessor\"),"
        },
        {
            "sha": "eb54d95ab6478b0a407a1d1c160935e96aeb554f",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -292,6 +292,20 @@\n                     \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n                 ),\n             ),\n+            (\n+                \"llama4\",\n+                (\n+                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+                ),\n+            ),\n+            (\n+                \"llama4_text\",\n+                (\n+                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+                ),\n+            ),\n             (\"llava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"llava_next\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"llava_next_video\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),"
        },
        {
            "sha": "59fe1686cec1bbad80233353820eb1517e7bb27b",
            "filename": "src/transformers/models/llama4/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fllama4%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fllama4%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2F__init__.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_llama4 import *\n+    from .image_processing_llama4_fast import *\n+    from .modeling_llama4 import *\n+    from .processing_llama4 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "1c4c00f48fcefaee904ea1ae83a54c51eca66aef",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "added",
            "additions": 432,
            "deletions": 0,
            "changes": 432,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -0,0 +1,432 @@\n+# coding=utf-8\n+# Copyright 2025 The LLAMA4 and HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Llama4VisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Llama4VisionModel`]. It is used to instantiate a\n+    Llama4 vision model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the Llama4 109B.\n+\n+    e.g. [meta-llama/Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        num_hidden_layers (`int`, *optional*, defaults to 34):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of channels in the input image.\n+        intermediate_size (`int`, *optional*, defaults to 5632):\n+            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n+        vision_output_dim (`int`, *optional*, defaults to 7680):\n+            Dimensionality of the vision model output. Includes output of transformer\n+            encoder with intermediate layers and global transformer encoder.\n+        image_size (`int`, *optional*, defaults to 448):\n+            The size (resolution) of each image *tile*.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The size (resolution) of each patch.\n+        norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        vision_feature_layer (``, *optional*, defaults to -1): TODO\n+        vision_feature_select_strategy (`int`, *optional*, defaults to `\"default\"`): TODO\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        pixel_shuffle_ratio (`int`, *optional*, defaults to 0.5): TODO\n+        projector_input_dim (`int`, *optional*, defaults to 4096): TODO\n+        projector_output_dim (`int`, *optional*, defaults to 4096): TODO\n+        multi_modal_projector_bias (`int`, *optional*, defaults to `False`): TODO\n+        projector_dropout (`int`, *optional*, defaults to 0.0): TODO\n+        attention_dropout (`int`, *optional*, defaults to 0.0): TODO\n+        rope_theta (`int`, *optional*, defaults to 10000): TODO\n+    \"\"\"\n+\n+    base_model_tp_plan = {\n+        \"model.layers.*.self_attn.q_proj\": \"colwise\",\n+        \"model.layers.*.self_attn.k_proj\": \"colwise\",\n+        \"model.layers.*.self_attn.v_proj\": \"colwise\",\n+        \"model.layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"vision_adapter.mlp.fc1\": \"colwise\",\n+        \"vision_adapter.mlp.fc2\": \"rowwise\",\n+        \"patch_embedding.linear\": \"colwise_rep\",\n+    }\n+    model_type = \"llama4_vision_model\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size: int = 768,\n+        hidden_act: str = \"gelu\",\n+        num_hidden_layers: int = 34,\n+        num_attention_heads: int = 16,\n+        num_channels: int = 3,\n+        intermediate_size: int = 5632,\n+        vision_output_dim: int = 7680,\n+        image_size: int = 448,\n+        patch_size: int = 14,\n+        norm_eps: float = 1e-5,\n+        vision_feature_layer=-1,\n+        vision_feature_select_strategy=\"default\",\n+        initializer_range: float = 0.02,\n+        pixel_shuffle_ratio=0.5,\n+        projector_input_dim=4096,\n+        projector_output_dim=4096,\n+        multi_modal_projector_bias=False,\n+        projector_dropout=0.0,\n+        attention_dropout=0.0,\n+        rope_theta=10000,\n+        **kwargs,\n+    ):\n+        self.hidden_size = hidden_size\n+        self.hidden_act = hidden_act\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_channels = num_channels\n+        self.intermediate_size = intermediate_size\n+        self.image_size = image_size\n+        self.vision_output_dim = vision_output_dim\n+        self.patch_size = patch_size\n+        self.norm_eps = norm_eps\n+        self.num_attention_heads = num_attention_heads\n+        self.initializer_range = initializer_range\n+        self.pixel_shuffle_ratio = pixel_shuffle_ratio\n+        self.projector_input_dim = projector_input_dim\n+        self.projector_output_dim = projector_output_dim\n+        self.multi_modal_projector_bias = multi_modal_projector_bias\n+        self.projector_dropout = projector_dropout\n+        self.attention_dropout = attention_dropout\n+        self.vision_feature_layer = vision_feature_layer\n+        self.vision_feature_select_strategy = vision_feature_select_strategy\n+        self.rope_theta = rope_theta\n+        super().__init__(**kwargs)\n+\n+\n+class Llama4TextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Llama4TextModel`]. It is used to instantiate a\n+    Llama4 text model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the Llama4 109B.\n+\n+    e.g. [meta-llama/Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 202048):\n+            Vocabulary size of the Llama4 text model. Defines the maximum number of different tokens that can be represented\n+            by the `inputs_ids` passed when calling [`Llama4TextModel`].\n+        hidden_size (`int`, *optional*, defaults to 5120):\n+            Dimensionality of the embeddings and hidden states.\n+        intermediate_size (`int`, *optional*, defaults to 8192):\n+            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n+        intermediate_size_mlp (`int`, *optional*, defaults to 16384): TODO\n+        num_hidden_layers (`int`, *optional*, defaults to 48):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 40):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 8):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If not\n+            specified, will default to `num_attention_heads`.\n+        head_dim (`int`, *optional*, defaults to 128): TODO\n+        hidden_act (`str` or `Callable`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler.\n+        max_position_embeddings (`int`, *optional*, defaults to 131072):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions.\n+        pad_token_id (`int`, *optional*, defaults to 128004):\n+            The id of the padding token.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            The id of the beginning of sentence token.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            The id of the end of sentence token.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to `500000.0`):\n+            The base period of the RoPE embeddings.\n+        attention_dropout (`int`, *optional*, defaults to 0.0): TODO\n+        num_experts_per_tok (`int`, *optional*, defaults to 1): TODO\n+        num_local_experts (`int`, *optional*, defaults to 16): TODO\n+        moe_layers (`int`, *optional*): TODO\n+        interleave_moe_layer_step (`int`, *optional*, defaults to 1): TODO\n+        use_qk_norm (`int`, *optional*, defaults to `True`): TODO\n+        output_router_logits (`int`, *optional*, defaults to `False`): TODO\n+        router_aux_loss_coef (`int`, *optional*, defaults to 0.001): TODO\n+        router_jitter_noise (`int`, *optional*, defaults to 0.0): TODO\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+            <TODO>\n+            <TODO>\n+        no_rope_layers (`int`, *optional*): TODO\n+        no_rope_layer_interval (`int`, *optional*, defaults to 4): TODO\n+        attention_chunk_size (`int`, *optional*, defaults to 8192):\n+            <TODO>\n+        attn_temperature_tuning (`int`, *optional*, defaults to 4): TODO\n+        floor_scale (`int`, *optional*, defaults to 8192): TODO\n+        attn_scale (`int`, *optional*, defaults to 0.1): TODO\n+\n+    Example:\n+    \"\"\"\n+\n+    model_type = \"llama4_text\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.input_layernorm.weight\": \"sequence_parallel\",\n+        \"layers.*.post_attention_layernorm.weight\": \"sequence_parallel\",\n+        \"norm.weight\": \"sequence_parallel\",\n+        \"layers.*.feed_forward.shared_expert.gate_proj\": \"local_colwise\",\n+        \"layers.*.feed_forward.shared_expert.up_proj\": \"local_colwise\",\n+        \"layers.*.feed_forward.shared_expert.down_proj\": \"local_rowwise\",\n+        \"layers.*.feed_forward.experts.gate_up_proj\": \"local_packed_rowwise\",  # row because not linear\n+        \"layers.*.feed_forward.experts.down_proj\": \"local_colwise\",  # col because not linear\n+        \"layers.*.feed_forward.experts\": \"local\",\n+        \"layers.*.feed_forward.gate_proj\": \"local_colwise\",\n+        \"layers.*.feed_forward.up_proj\": \"local_colwise\",\n+        \"layers.*.feed_forward.down_proj\": \"local_rowwise\",\n+        \"layers.*.feed_forward\": \"gather\",\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=202048,\n+        hidden_size=5120,\n+        intermediate_size=8192,\n+        intermediate_size_mlp=16384,\n+        num_hidden_layers=48,\n+        num_attention_heads=40,\n+        num_key_value_heads=8,\n+        head_dim=128,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=4096 * 32,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-5,\n+        use_cache=True,\n+        pad_token_id=None,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        tie_word_embeddings=False,\n+        rope_theta=500000,\n+        attention_dropout=0.0,\n+        num_experts_per_tok=1,\n+        num_local_experts=16,\n+        moe_layers=None,\n+        interleave_moe_layer_step=1,\n+        use_qk_norm=True,\n+        output_router_logits=False,\n+        router_aux_loss_coef=0.001,\n+        router_jitter_noise=0.0,\n+        rope_scaling=None,\n+        no_rope_layers=None,\n+        no_rope_layer_interval=4,\n+        attention_chunk_size=8192,\n+        attn_temperature_tuning=4,\n+        floor_scale=8192,\n+        attn_scale=0.1,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+        self.attn_temperature_tuning = attn_temperature_tuning\n+        self.attn_scale = attn_scale\n+        self.floor_scale = floor_scale\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.intermediate_size_mlp = intermediate_size_mlp\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = False\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.attention_dropout = attention_dropout\n+        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        self.use_qk_norm = use_qk_norm\n+\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_local_experts = num_local_experts\n+\n+        self.output_router_logits = output_router_logits\n+        self.router_aux_loss_coef = router_aux_loss_coef\n+        self.router_jitter_noise = router_jitter_noise\n+        default_no_rope_layers = [\n+            int((layer_idx + 1) % no_rope_layer_interval != 0) for layer_idx in range(self.num_hidden_layers)\n+        ]\n+\n+        # no_rope_layers == [] is invalid as we cannot have 0 layers\n+        self.no_rope_layers = no_rope_layers if no_rope_layers else default_no_rope_layers\n+\n+        self.interleave_moe_layer_step = interleave_moe_layer_step\n+        self.moe_layers = (\n+            moe_layers\n+            if moe_layers is not None\n+            else list(range(interleave_moe_layer_step - 1, num_hidden_layers, interleave_moe_layer_step))\n+        )\n+        self.attention_chunk_size = attention_chunk_size\n+\n+\n+class Llama4Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Llama4Model`]. It is used to instantiate an\n+    Llama4 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the Llama4 109B.\n+\n+    e.g. [meta-llama/Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vision_config (`Llama4VisionConfig`, *optional*):\n+            The Llama4 Vision config.\n+        text_config (`Llama4TextConfig`, *optional*):\n+            The Llama4 Text config.\n+        boi_token_index (`int`, *optional*, defaults to 200080):\n+            The begin-of-image token index to wrap the image prompt.\n+        eoi_token_index (`int`, *optional*, defaults to 200081):\n+            The end-of-image token index to wrap the image prompt.\n+        image_token_index (`int`, *optional*, defaults to 200092):\n+            The image token index to encode the image prompt.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+\n+    ```python\n+    >>> from transformers import Llama4Model, Llama4Config\n+\n+    >>> # Initializing a Llama4 7B style configuration\n+    >>> configuration = Llama4Config()\n+\n+    >>> # Initializing a model from the Llama4 7B style configuration\n+    >>> model = Llama4Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"llama4\"\n+    sub_configs = {\"text_config\": Llama4TextConfig, \"vision_config\": Llama4VisionConfig}\n+    base_model_tp_plan = {\n+        \"multi_modal_projector.linear_1\": \"colwise_rep\",\n+    }\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        text_config=None,\n+        boi_token_index=200080,\n+        eoi_token_index=200081,\n+        image_token_index=200092,\n+        tie_word_embeddings=False,\n+        **kwargs,\n+    ):\n+        if vision_config is None:\n+            self.vision_config = Llama4VisionConfig()\n+            logger.info(\"vision_config is None, using default llama4 vision config\")\n+        elif isinstance(vision_config, dict):\n+            self.vision_config = Llama4VisionConfig(**vision_config)\n+        elif isinstance(vision_config, Llama4VisionConfig):\n+            self.vision_config = vision_config\n+\n+        self.boi_token_index = boi_token_index\n+        self.eoi_token_index = eoi_token_index\n+        self.image_token_index = image_token_index\n+\n+        if text_config is None:\n+            self.text_config = Llama4TextConfig()\n+            logger.info(\"text_config is None, using default llama4 text config\")\n+        elif isinstance(text_config, dict):\n+            self.text_config = Llama4TextConfig(**text_config)\n+        elif isinstance(text_config, Llama4TextConfig):\n+            self.text_config = text_config\n+\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+\n+\n+__all__ = [\"Llama4Config\", \"Llama4TextConfig\", \"Llama4VisionConfig\"]"
        },
        {
            "sha": "d1cd6b19933afc211d21f139ff4a196e743a1682",
            "filename": "src/transformers/models/llama4/convert_llama4_weights_to_hf.py",
            "status": "added",
            "additions": 736,
            "deletions": 0,
            "changes": 736,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -0,0 +1,736 @@\n+import argparse\n+import gc\n+import io\n+import json\n+import os\n+import re\n+from typing import List, Optional\n+\n+import torch\n+from tokenizers import AddedToken, processors\n+from tqdm import tqdm\n+\n+from transformers import (\n+    GenerationConfig,\n+    Llama4Config,\n+    Llama4ForConditionalGeneration,\n+    Llama4ImageProcessorFast,\n+    Llama4Processor,\n+    Llama4TextConfig,\n+    Llama4VisionConfig,\n+    PreTrainedTokenizerFast,\n+)\n+from transformers.integrations.tiktoken import TikTokenConverter\n+\n+\n+_OFFLINE_QUANT_COMPATIBLE = os.environ.get(\"OFFLINE_QUANT_COMPATIBLE\", \"0\") == \"1\"\n+\n+torch.serialization.add_safe_globals([io.BytesIO])\n+# fmt: off\n+# `None` means we drop the key\n+\n+\n+weight_postfix = \".weight\" if _OFFLINE_QUANT_COMPATIBLE else \"\"\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    # CausalLM keys\n+    r\"output.weight\":                                        r\"language_model.lm_head.weight\",\n+    r\"\\nnorm.weight\":                                        r\"\\nlanguage_model.model.norm.weight\",\n+    # Model keys\n+    r\"tok_embeddings.weight\":                                r\"language_model.model.embed_tokens.weight\",\n+    r\"freq_cis\":                                             None,\n+    r\"rope.freqs\":                                           None,\n+    r\"layers.(\\d+).attention_norm.weight\":                   r\"language_model.model.layers.\\1.input_layernorm.weight\",\n+    r\"layers.(\\d+).attention.wqkv.layer_norm_weight\":        r\"language_model.model.layers.\\1.input_layernorm.weight\",\n+    r\"layers.(\\d+).feed_forward.norm.weight\":                r\"language_model.model.layers.\\1.post_attention_layernorm.weight\",\n+    r\"layers.(\\d+).attention.wo.weight\":                     r\"language_model.model.layers.\\1.self_attn.o_proj.weight\",\n+    r\"layers.(\\d+).attention.wqkv.weight\":                   r\"language_model.model.layers.\\1.self_attn.qkv_proj.weight\",\n+\n+    # MoE keys: no simple MLPmodel.\n+    r\"layers.(\\d+).feed_forward.experts.moe_w_in_eD_F\":      r\"language_model.model.layers.\\1.feed_forward.experts.gate_proj\" + weight_postfix,       # will be fused with up\n+    r\"layers.(\\d+).feed_forward.experts.moe_w_out_eF_D\":     r\"language_model.model.layers.\\1.feed_forward.experts.down_proj\" + weight_postfix,       # expert win\n+    r\"layers.(\\d+).feed_forward.experts.moe_w_swiglu_eD_F\":  r\"language_model.model.layers.\\1.feed_forward.experts.up_proj\" + weight_postfix,         # fused with up\n+    r\"layers.(\\d+).feed_forward.router_DE\":                  r\"language_model.model.layers.\\1.feed_forward.router.weight\",           # used for top\n+    r\"layers.(\\d+).feed_forward.w_in_shared_FD\":             r\"language_model.model.layers.\\1.feed_forward.shared_expert.gate_proj\", # might need to be fused for efficiency?\n+    r\"layers.(\\d+).feed_forward.w_out_shared_DF\":            r\"language_model.model.layers.\\1.feed_forward.shared_expert.down_proj\", # might need to be fused for efficiency?\n+    r\"layers.(\\d+).feed_forward.w_swiglu_FD\":                r\"language_model.model.layers.\\1.feed_forward.shared_expert.up_proj\",   # might need to be fused for efficiency?\n+    r\"layers.(\\d+).feed_forward.global_gate_stats_3E\":       None,\n+    # Unused keys in load hooks (explicitly removed)\n+    r'layers.(\\d+).attention.wqkv._extra_state':             None,\n+    r'layers.(\\d+).attention.wo._extra_state':               None,\n+    # Key apparently unused in base models\n+    r'layers.(\\d+).feed_forward.expert_activation_DE':       None,\n+\n+    # MLP layer variant\n+    r\"layers.(\\d+).feed_forward.w1.weight\":                  r\"language_model.model.layers.\\1.feed_forward.gate_proj.weight\",               # might need to be fused for efficiency?\n+    r\"layers.(\\d+).feed_forward.w3.weight\":                  r\"language_model.model.layers.\\1.feed_forward.up_proj.weight\",                 # might need to be fused for efficiency?\n+    # r\"layers.(\\d+).feed_forward.mlp.fc1_weight\":             r\"language_model.model.layers.\\1.feed_forward.gate_up_proj.weight\",\n+    r\"layers.(\\d+).feed_forward.mlp.fc2_weight\":             r\"language_model.model.layers.\\1.feed_forward.down_proj.weight\",\n+    r\"layers.(\\d+).feed_forward.mlp.layer_norm.weight\":      r\"language_model.model.layers.\\1.post_attention_layernorm.weight\",\n+\n+    # Vision encoder mapping\n+    r\"vision_embeddings.vision_encoder.conv1._linear\":                                            r\"vision_model.patch_embedding.linear\",\n+    r'vision_embeddings.vision_adapter.mlp.c_fc':                                                 r\"vision_model.vision_adapter.mlp.fc1\",\n+    r'vision_embeddings.vision_adapter.mlp.c_proj':                                               r\"vision_model.vision_adapter.mlp.fc2\",\n+    r\"vision_embeddings.vision_encoder.transformer.resblocks.(\\d+).attn.wq.(weight|bias)\":        r\"vision_model.model.layers.\\1.self_attn.q_proj.\\2\",\n+    r\"vision_embeddings.vision_encoder.transformer.resblocks.(\\d+).attn.wk.(weight|bias)\":        r\"vision_model.model.layers.\\1.self_attn.k_proj.\\2\",\n+    r\"vision_embeddings.vision_encoder.transformer.resblocks.(\\d+).attn.wv.(weight|bias)\":        r\"vision_model.model.layers.\\1.self_attn.v_proj.\\2\",\n+    r\"vision_embeddings.vision_encoder.transformer.resblocks.(\\d+).attn.wo.(weight|bias)\":        r\"vision_model.model.layers.\\1.self_attn.o_proj.\\2\",\n+    r\"vision_embeddings.vision_encoder.transformer.resblocks.(\\d+).mlp.c_fc\":                     r\"vision_model.model.layers.\\1.mlp.fc1\",\n+    r\"vision_embeddings.vision_encoder.transformer.resblocks.(\\d+).mlp.c_proj\":                   r\"vision_model.model.layers.\\1.mlp.fc2\",\n+    r\"vision_embeddings.vision_encoder.transformer.resblocks.(\\d+).ln_1.(weight|bias)\":           r\"vision_model.model.layers.\\1.input_layernorm.\\2\",\n+    r\"vision_embeddings.vision_encoder.transformer.resblocks.(\\d+).ln_2.(weight|bias)\":           r\"vision_model.model.layers.\\1.post_attention_layernorm.\\2\",\n+    # r'vision_embeddings.vision_encoder.ln_(1|2).(weight|bias)':                                   r'vision_model.transformer.vision_encoder.layernorm_\\1.\\2',\n+    r'vision_embeddings.vision_encoder.ln_post':                                                  r'vision_model.layernorm_post',\n+    r'vision_embeddings.vision_encoder.ln_pre':                                                   r'vision_model.layernorm_pre',\n+    r'vision_embeddings.vision_encoder.class_embedding':                                          r'vision_model.class_embedding',\n+    r\"vision_embeddings.vision_encoder.positional_embedding_vlm\":                                 r\"vision_model.positional_embedding_vlm\",\n+    r\"vision_embeddings.vision_encoder.(?=\\w)\":                                                   r\"vision_model.model.\",\n+    r\"vision_projection.weight\":                                                                  r\"multi_modal_projector.linear_1.weight\",\n+}\n+# fmt: on\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+def permute_for_rope(input_tensor, n_heads, dim1, dim2):\n+    \"\"\"\n+    When you go from the complex ROPE formulation to sin and cos one, you need\n+    to permute the query and key weights (to avoid doing it on the fly)\n+    \"\"\"\n+    input_tensor = input_tensor.view(n_heads, dim1 // n_heads // 2, 2, dim2)\n+    input_tensor = input_tensor.transpose(1, 2).reshape(dim1, dim2)\n+    return input_tensor\n+\n+\n+def is_param_same_across_shards(key):\n+    \"\"\"\n+    Return `False` if the parameter is different across checkpoint shards\n+    and needs to be concatenated.\n+    \"\"\"\n+    patterns = [\n+        r\"language_model.layers.(\\d+).(.*)layernorm.weight\",\n+        r\"language_model.norm.weight\",\n+        r\"router.weight\",\n+        r\"feed_forward.global_gate_stats\",\n+        # not all vision weights are sharded, some are repeated\n+        r\"vision_model.class_embedding\",\n+        r\"vision_model.positional_embedding_vlm\",\n+        r\"vision_embeddings.vision_encoder.positional_embedding_vlm\",\n+        r\"vision_model.model.layers.(\\d+).self_attn.o_proj.bias\",\n+        r\"vision_model.model.layers.(\\d+).input_layernorm\",\n+        r\"vision_model.model.layers.(\\d+).post_attention_layernorm\",\n+        r\"vision_model.layernorm_pre\",\n+        r\"vision_model.layernorm_post\",\n+        r\"vision_model.model.layers.(\\d+).mlp.fc2.bias\",\n+        r\"norm.weight\",\n+        ]  # fmt: skip\n+    return any(re.search(pattern, key) for pattern in patterns)\n+\n+\n+def get_concat_dim(key):\n+    \"\"\"\n+    Return the dimension to concatenate the weights on.\n+    \"\"\"\n+    concat_dim_1 = [\n+        # language dim 1 sharded weights\n+        \"feed_forward.router.weight\",\n+        \"self_attn.o_proj\",\n+        \"experts.gate_proj\",\n+        \"experts.up_proj\",\n+        \"expert.down_proj\",\n+        # \"feed_forward.up_proj\",\n+        # \"feed_forward.gate_proj\",\n+        \"feed_forward.down_proj\",\n+        \"global_gate_stats\",\n+        # vision dim1 sharded stuff\n+        \"mlp.fc2.weight\", # covers all rowparallels across vis\n+        ]  # fmt: off\n+    if any(re.search(pattern, key) for pattern in concat_dim_1):\n+        return 1\n+    return 0\n+\n+\n+def compute_intermediate_size(hidden_dim, multiple_of=1024, ffn_dim_multiplier=1.3):\n+    hidden_dim = 4 * int(2 * hidden_dim / 3)\n+    hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n+    hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n+    return hidden_dim\n+\n+\n+# Ignore extra info - h/t Aritra\n+def safe_load(filename):\n+    # Can use weights_only because io.BytesIO was registered, but we still need to skip those objects\n+    shard = torch.load(filename, weights_only=True, map_location=\"cpu\", mmap=True)\n+    shard = {k: v for k, v in shard.items() if not isinstance(v, io.BytesIO)}\n+    return shard\n+\n+\n+# Unpack mlp projections - possibly to be removed when they are fused\n+def preprocess_keys(state_dict):\n+    new_state_dict = {}\n+    for key, value in state_dict.items():\n+        if \"mlp.fc1_weight\" in key:\n+            prefix = key.split(\"mlp.fc1_weight\")[0]\n+            w1, w3 = value.chunk(2, dim=0)\n+            new_state_dict[prefix + \"w1.weight\"] = w1\n+            new_state_dict[prefix + \"w3.weight\"] = w3\n+        else:\n+            new_state_dict[key] = value\n+    return new_state_dict\n+\n+\n+def max_context_length(model_path, instruct=False):\n+    \"\"\"256K for base, 1M for 128E instruct, 10M for 16E instruct.\"\"\"\n+    if not instruct:\n+        return 256 * 1024\n+\n+    with open(os.path.join(model_path, \"params.json\"), \"r\") as f:\n+        params = json.load(f)\n+    params = params.get(\"model\", params)\n+    num_experts = params[\"moe_args\"][\"num_experts\"]\n+    return 10485760 if num_experts == 16 else 1048576\n+\n+\n+def write_model(\n+    model_path,\n+    input_base_path,\n+    num_shards,\n+    convert_checkpoints,\n+    safe_serialization=True,\n+    instruct=False,\n+):\n+    os.makedirs(model_path, exist_ok=True)\n+\n+    with open(os.path.join(input_base_path, \"params.json\"), \"r\") as f:\n+        params = json.load(f)\n+\n+    params = params.get(\"model\", params)\n+    torch_dtype = \"bfloat16\"\n+\n+    # ------------------------------------------------------------\n+    # Text model params and config\n+    # ------------------------------------------------------------\n+\n+    # params from config\n+    vocab_size = 202048  # params[\"vocab_size\"] # seems like the lm head is 25256 so padded instead of 202048\n+    num_layers = params[\"n_layers\"]\n+    dim = params[\"dim\"]\n+    num_heads = params[\"n_heads\"]\n+    rms_norm_eps = params[\"norm_eps\"]\n+    rope_theta = params[\"rope_theta\"]\n+    no_rope_layer_interval = params[\"nope_layer_interval\"]\n+    attention_chunk_size = params[\"attention_chunk_size\"]\n+\n+    config_kwargs = {}\n+    if params[\"use_scaled_rope\"]:\n+        # some constans from original code\n+        rope_scaling = {\n+            \"rope_type\": \"llama3\",\n+            \"factor\": 8.0,\n+            \"low_freq_factor\": 1.0,\n+            \"high_freq_factor\": 4.0,\n+            \"original_max_position_embeddings\": 8192,\n+        }\n+        config_kwargs.update({\"rope_scaling\": rope_scaling})\n+\n+    # compute additional params for weight conversion\n+    num_heads_per_shard = num_heads // num_shards\n+    dim_per_head = dim // num_heads\n+    # intermediate_size = compute_intermediate_size(dim, multiple_of=params[\"multiple_of\"])\n+\n+    num_key_value_heads = params[\"n_kv_heads\"]  # for GQA / MQA\n+\n+    num_experts = params[\"moe_args\"][\"num_experts\"]\n+    interleave_moe_layer_step = params[\"moe_args\"].get(\"interleave_moe_layer_step\", 1)\n+\n+    no_rope_layer_interval = params[\"nope_layer_interval\"]\n+\n+    bos_token_id = 200000\n+    eos_token_id = [200001, 200007, 200008] if instruct else 200001\n+    pad_token_id = 200018\n+\n+    text_config = Llama4TextConfig(\n+        num_attention_heads=num_heads,\n+        vocab_size=vocab_size,\n+        hidden_size=dim,\n+        rms_norm_eps=rms_norm_eps,\n+        rope_theta=rope_theta,\n+        num_hidden_layers=num_layers,\n+        intermediate_size=8192,\n+        intermediate_size_mlp=16384,\n+        max_position_embeddings=max_context_length(input_base_path, instruct),\n+        num_local_experts=num_experts,\n+        interleave_moe_layer_step=interleave_moe_layer_step,\n+        use_qk_norm=params[\"use_qk_norm\"],\n+        no_rope_layer_interval=no_rope_layer_interval,\n+        attention_chunk_size=attention_chunk_size,\n+        bos_token_id=bos_token_id,\n+        eos_token_id=eos_token_id,\n+        pad_token_id=pad_token_id,\n+        tie_word_embeddings=False,  # Constant set to False\n+        torch_dtype=torch_dtype,\n+        for_llm_compressor=_OFFLINE_QUANT_COMPATIBLE,\n+        no_rope_layers=no_rope_layer_interval,\n+        **config_kwargs,\n+    )\n+    # default vision config frmo params\n+\n+    vision_params = params[\"vision_args\"]\n+    vision_dim = vision_params[\"dim\"]\n+    vision_num_layers = vision_params[\"n_layers\"]\n+    image_size = vision_params[\"image_size\"][\"height\"]  # siglip config is outdated\n+    vision_num_heads = vision_params[\"n_heads\"]\n+\n+    vision_output_dim = vision_params[\"output_dim\"]\n+\n+    vision_config = Llama4VisionConfig(\n+        hidden_act=\"gelu\",\n+        num_hidden_layers=vision_num_layers,\n+        image_size=image_size,\n+        num_attention_heads=vision_num_heads,\n+        hidden_size=vision_dim,\n+        vision_output_dim=vision_output_dim,\n+    )\n+\n+    config = Llama4Config(text_config=text_config, vision_config=vision_config)\n+    config.save_pretrained(model_path)\n+\n+    print(\"Model config saved successfully...\")\n+\n+    # ------------------------------------------------------------\n+    # Convert weights\n+    # ------------------------------------------------------------\n+\n+    if convert_checkpoints:\n+        print(f\"Fetching all parameters from the checkpoint at {input_base_path}...\")\n+        if num_shards == 1:\n+            if os.path.exists(os.path.join(input_base_path, \"consolidated.00.pth\")):\n+                path = os.path.join(input_base_path, \"consolidated.00.pth\")\n+            else:\n+                path = os.path.join(input_base_path, \"consolidated.pth\")\n+            loaded = [safe_load(path)]\n+        else:\n+            loaded = [\n+                safe_load(os.path.join(input_base_path, f\"consolidated.{i:02d}.pth\"))\n+                for i in tqdm(range(num_shards), desc=\"Loading shards\", unit=\"shard\")\n+            ]\n+        loaded = [preprocess_keys(d) for d in loaded]\n+\n+        all_keys_raw = list(loaded[0].keys())\n+        repeated_keys = []\n+        sharded_keys = []\n+        for _key in all_keys_raw:\n+            try:\n+                if (loaded[0][_key] == loaded[1][_key]).all():\n+                    repeated_keys.append(_key)\n+                else:\n+                    sharded_keys.append(_key)\n+            except Exception as e:\n+                print(f\"Encountered exception {e} for {_key}\")\n+        print(\"Initializing an empty model\")\n+        with torch.device(\"meta\"):\n+            model = Llama4ForConditionalGeneration(config)\n+\n+        print(\"Converting model...\")\n+        all_keys = list(loaded[0].keys())\n+        new_keys = convert_old_keys_to_new_keys(all_keys)\n+        state_dict = {}\n+        replicated_params = []  # To keep track of replicated weights.\n+        for key in tqdm(all_keys, desc=\"Renaming and processing all keys\", unit=\"key\"):\n+            new_key = new_keys[key]\n+            print(key, new_key)\n+            if not is_param_same_across_shards(new_key):\n+                current_parameter = [chunk.pop(key) for chunk in loaded if not isinstance(chunk[key], io.BytesIO)]\n+            else:\n+                print(f\"{key} (now {new_key}) is the same across all shards.\")\n+                replicated_params.append((key, new_key))\n+                current_parameter = [loaded[0].pop(key)] if not isinstance(loaded[0][key], io.BytesIO) else []\n+\n+            if \"running_gate_stats_3E\" in key:\n+                new_keys.pop(new_key)\n+                continue\n+\n+            concat_dim = get_concat_dim(new_key)\n+\n+            # Post-process the current_parameter.\n+            if \"qkv_proj\" in new_key:\n+                queries = []\n+                keys = []\n+                values = []\n+                for param in current_parameter:\n+                    query, key_, value = param.split(\n+                        [\n+                            num_heads * dim_per_head // num_shards,\n+                            num_key_value_heads * dim_per_head // num_shards,\n+                            num_key_value_heads * dim_per_head // num_shards,\n+                        ]\n+                    )\n+                    queries.append(query.reshape(num_heads_per_shard, -1, dim))\n+                    keys.append(key_.reshape(num_key_value_heads // num_shards, -1, dim))\n+                    values.append(value.reshape(num_key_value_heads // num_shards, -1, dim))\n+\n+                queries = torch.cat(queries, dim=0).reshape(dim, dim)\n+                keys = torch.cat(keys, dim=0).reshape(num_key_value_heads * dim_per_head, dim)\n+                values = torch.cat(values, dim=0).reshape(num_key_value_heads * dim_per_head, dim)\n+                # queries = permute_for_rope(queries, num_heads, dim, dim)\n+                # keys = permute_for_rope(keys, num_key_value_heads, num_key_value_heads*dim_per_head, dim)\n+\n+                q = new_key.replace(\"qkv\", \"q\")\n+                tqdm.write(f\"Processing: {key.ljust(50)}  ->\\t {q}, {queries.shape}\")\n+                state_dict[q] = queries\n+\n+                k = new_key.replace(\"qkv\", \"k\")\n+                tqdm.write(f\"Processing: {key.ljust(50)}  ->\\t {k}, {keys.shape}\")\n+                state_dict[k] = keys\n+\n+                v = new_key.replace(\"qkv\", \"v\")\n+                tqdm.write(f\"Processing: {key.ljust(50)}  ->\\t {v}, {values.shape}\")\n+                state_dict[v] = values\n+            elif _OFFLINE_QUANT_COMPATIBLE and \"feed_forward.experts.\" in new_key:\n+                # for experts, we need to split expert for offline quantiation purpose and don't need to fuse\n+                expert_lists = []\n+                for k in current_parameter:\n+                    expert_lists.append(\n+                        list(k.reshape(num_experts, -1, k.shape[-1]).unbind(0))\n+                    )  # [#expert * IN, OUT] -> #experts * [IN, OUT]\n+                for i in range(num_experts):\n+                    expert = torch.cat([expert_list[i] for expert_list in expert_lists], dim=concat_dim)\n+                    expert_key = new_key.replace(\"experts.\", f\"experts.{i}.\")\n+                    state_dict[expert_key] = expert.transpose(0, 1).contiguous()  # [OUT, IN]\n+                    tqdm.write(f\"Processing: {key.ljust(50)}  ->\\t {expert_key}, {state_dict[expert_key].shape}\")\n+            elif re.search(r\"(gate|up)_proj\", new_key):\n+                path = new_key.split(\".\")\n+                gate_key = re.sub(r\"(gate|up)_proj\", lambda m: \"gate_proj\", new_key)\n+                up_key = re.sub(r\"(gate|up)_proj\", lambda m: \"up_proj\", new_key)\n+                if gate_key == new_key:\n+                    state_dict[new_key] = torch.cat(current_parameter, dim=concat_dim)\n+                elif new_key == up_key:\n+                    if \"experts\" not in new_key:\n+                        state_dict[new_key] = torch.cat(current_parameter, dim=concat_dim)\n+                    else:\n+                        gate_proj = state_dict.pop(gate_key)\n+                        gate_proj = [\n+                            gate_proj.reshape(num_experts, -1, 8, 1024)[:, :, k, :].reshape(num_experts, -1, 1024)\n+                            for k in range(8)\n+                        ]\n+                        gate_proj = torch.cat(gate_proj, dim=-1)\n+\n+                        up_proj = [\n+                            k.reshape(num_experts, -1, 8, 1024).reshape(num_experts, -1, 1024)\n+                            for k in current_parameter\n+                        ]\n+                        up_proj = torch.cat(up_proj, dim=-1)\n+\n+                        gate_up_proj = torch.cat((gate_proj, up_proj), dim=-1)\n+                        new_key = new_key.replace(\"up_proj\", \"gate_up_proj\")\n+                        state_dict[new_key] = gate_up_proj.contiguous()\n+\n+                    tqdm.write(f\"Processing: {key.ljust(50)}  ->\\t {new_key}, {state_dict[new_key].shape}\")\n+            elif \"down_proj\" in new_key:\n+                current_parameter = torch.cat(current_parameter, dim=concat_dim)\n+                if \"experts\" in new_key:\n+                    p = []\n+                    for i in range(8):\n+                        p += [current_parameter.reshape(8, -1, 5120)[i, :, :].view(num_experts, -1, 5120)]\n+                    current_parameter = torch.cat(p, dim=1)\n+                state_dict[new_key] = current_parameter.contiguous()\n+                tqdm.write(f\"Processing: {key.ljust(50)}  ->\\t {new_key}, {state_dict[new_key].shape}\")\n+            elif \"router\" in new_key:\n+                current_parameter = torch.cat(current_parameter, dim=concat_dim)\n+                state_dict[new_key] = current_parameter.transpose(0, 1)\n+            elif \"lm_head\" in new_key:\n+                current_parameter = torch.cat(current_parameter, dim=concat_dim).clone()\n+                # TODO we need to do better than mean, works for now\n+                # if (vocab_size - current_parameter.shape[0]) > 0:\n+                #     mean_embedding = torch.mean(current_parameter, dim=0)[:, None].repeat(vocab_size-current_parameter.shape[0],1)\n+                #     print(mean_embedding.shape)\n+                #     current_parameter = torch.cat((current_parameter, mean_embedding), dim=0)\n+                state_dict[new_key] = current_parameter\n+                tqdm.write(\n+                    f\"Processing: {key.ljust(50)}  ->\\t {new_key}, {state_dict[new_key].shape}, concat dim = {concat_dim}\"\n+                )\n+            elif new_key == \"vision_model.patch_embedding.linear.weight\":\n+                current_parameter = torch.cat(current_parameter, dim=concat_dim).clone()\n+                # We don't reshape the patch embedding as we're using unfolded convolution as well\n+                state_dict[new_key] = current_parameter  # .reshape(-1, 3, vision_patch_size, vision_patch_size)\n+            # generic concat for weights/select one for biases\n+            elif isinstance(current_parameter, list) and len(current_parameter) > 0:\n+                if not is_param_same_across_shards(new_key):\n+                    current_parameter = torch.cat(current_parameter, dim=concat_dim)\n+                    state_dict[new_key] = current_parameter\n+                    tqdm.write(\n+                        f\"Processing: {key.ljust(50)}  ->\\t {new_key}, {state_dict[new_key].shape}, concat dim = {concat_dim}\"\n+                    )\n+                elif is_param_same_across_shards(new_key):\n+                    state_dict[new_key] = current_parameter[0]\n+                    tqdm.write(\n+                        f\"Processing: {key.ljust(50)}  ->\\t {new_key}, {state_dict[new_key].shape}, concat dim = {concat_dim}\"\n+                    )\n+\n+            elif new_key == \"\":\n+                # skip empty keys\n+                continue\n+            else:\n+                # just load the parameter\n+                state_dict[new_key] = current_parameter\n+                tqdm.write(\n+                    f\"Processing: {key.ljust(50)}  ->\\t {new_key}, {state_dict[new_key].shape}, concat dim = {concat_dim}\"\n+                )\n+        del loaded\n+        gc.collect()\n+\n+        print(\"Loading the checkpoint in a Llama4 model.\")\n+        state_dict.pop(\"\")\n+        model.load_state_dict(state_dict, strict=True, assign=True)\n+        print(\"Model reloaded successfully.\")\n+        print(\"Saving the model.\")\n+        model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+        del state_dict, model\n+\n+        # Safety check: reload the converted model\n+        gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    with torch.no_grad():\n+        # TODO test if we can do `tp_plan=\"auto\"``\n+        model = Llama4ForConditionalGeneration.from_pretrained(\n+            model_path, torch_dtype=torch.bfloat16, device_map=\"auto\", attn_implementation=\"eager\"\n+        )\n+\n+        model.generation_config.top_p = 0.9\n+        model.generation_config.temperature = 0.6\n+        print(\"Model reloaded successfully.\")\n+\n+        from transformers import AutoTokenizer\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_path)\n+        inputs = tokenizer([\"Roses are red,\"], return_tensors=\"pt\").to(model.device)\n+        out = model.generate(**inputs, max_new_tokens=4)\n+        print(tokenizer.batch_decode(out))\n+    # generation config\n+    if instruct:\n+        print(\"Saving generation config...\")\n+        generation_config = GenerationConfig(\n+            do_sample=True,\n+            temperature=0.6,\n+            top_p=0.9,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            pad_token_id=pad_token_id,\n+        )\n+        generation_config.save_pretrained(model_path)\n+\n+\n+BOS_ADDED_TOKEN = AddedToken(\n+    \"<|begin_of_text|>\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True\n+)\n+EOS_ADDED_TOKEN = AddedToken(\n+    \"<|end_of_text|>\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True\n+)\n+EOT_ADDED_TOKEN = AddedToken(\"<|eot|>\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True)\n+\n+\n+def get_reserved_special_tokens(name, count, start_index=0):\n+    return [f\"<|{name}_reserved_special_token_{i}|>\" for i in range(start_index, start_index + count)]\n+\n+\n+# 200005, ..., 200079\n+LLAMA4_TEXT_POST_TRAIN_SPECIAL_TOKENS = [\n+    \"<|header_start|>\",\n+    \"<|header_end|>\",\n+    \"<|eom|>\",\n+    \"<|eot|>\",\n+    \"<|step|>\",\n+    \"<|text_post_train_reserved_special_token_0|>\",\n+    \"<|text_post_train_reserved_special_token_1|>\",\n+    \"<|text_post_train_reserved_special_token_2|>\",\n+    \"<|text_post_train_reserved_special_token_3|>\",\n+    \"<|text_post_train_reserved_special_token_4|>\",\n+    \"<|text_post_train_reserved_special_token_5|>\",\n+    \"<|python_start|>\",\n+    \"<|python_end|>\",\n+    \"<|finetune_right_pad|>\",\n+] + get_reserved_special_tokens(\n+    \"text_post_train\", 61, 6\n+)  # <|text_post_train_reserved_special_token_6|>, ..., <|text_post_train_reserved_special_token_66|>\n+\n+# 200080, ..., 201133\n+LLAMA4_VISION_SPECIAL_TOKENS = [\n+    \"<|image_start|>\",\n+    \"<|image_end|>\",\n+    \"<|vision_reserved_special_token_0|>\",\n+    \"<|vision_reserved_special_token_1|>\",\n+    \"<|tile_x_separator|>\",\n+    \"<|tile_y_separator|>\",\n+    \"<|vision_reserved_special_token_2|>\",\n+    \"<|vision_reserved_special_token_3|>\",\n+    \"<|vision_reserved_special_token_4|>\",\n+    \"<|vision_reserved_special_token_5|>\",\n+    \"<|image|>\",\n+    \"<|vision_reserved_special_token_6|>\",\n+    \"<|patch|>\",\n+] + get_reserved_special_tokens(\n+    \"vision\", 1041, 7\n+)  # <|vision_reserved_special_token_7|>, ..., <|vision_reserved_special_token_1047|>\n+\n+LLAMA4_SPECIAL_TOKENS = LLAMA4_TEXT_POST_TRAIN_SPECIAL_TOKENS + LLAMA4_VISION_SPECIAL_TOKENS\n+\n+BASIC_SPECIAL_TOKENS = [\n+    \"<|begin_of_text|>\",\n+    \"<|end_of_text|>\",\n+    \"<|fim_prefix|>\",\n+    \"<|fim_middle|>\",\n+    \"<|fim_suffix|>\",\n+]\n+\n+\n+class Llama4Converter(TikTokenConverter):\n+    def __init__(\n+        self,\n+        vocab_file,\n+        special_tokens: List[str],\n+        pattern: str,\n+        model_max_length: int = 0,\n+        chat_template: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        super().__init__(vocab_file, pattern=pattern)\n+        self.additional_special_tokens = special_tokens\n+        tokenizer = self.converted()\n+        if chat_template is not None:\n+            kwargs[\"chat_template\"] = chat_template\n+\n+        self.converted_tokenizer = PreTrainedTokenizerFast(\n+            tokenizer_object=tokenizer,\n+            model_input_names=[\"input_ids\", \"attention_mask\"],\n+            model_max_length=model_max_length,\n+            **kwargs,\n+        )\n+\n+        # to check\n+        # import tiktoken\n+        # model = tiktoken.Encoding(\n+        #     name=Path(model_path).name,\n+        #     pat_str=self.O200K_PATTERN,\n+        #     mergeable_ranks=mergeable_ranks,\n+        #     special_tokens=self.special_tokens,\n+        # )\n+\n+        instruct = chat_template is not None\n+        self.update_post_processor(self.converted_tokenizer)\n+        # finer special_tokens_map.json\n+        self.converted_tokenizer._bos_token = BOS_ADDED_TOKEN\n+        self.converted_tokenizer._eos_token = EOT_ADDED_TOKEN if instruct else EOS_ADDED_TOKEN\n+\n+    # We can't do this while building the tokenizer because we have no easy access to the bos token id\n+    def update_post_processor(self, tokenizer):\n+        tokenizer._tokenizer.post_processor = processors.Sequence(\n+            [\n+                processors.ByteLevel(trim_offsets=False),\n+                processors.TemplateProcessing(\n+                    single=\"<|begin_of_text|> $A\",\n+                    pair=\"<|begin_of_text|>:0 $A:0 <|begin_of_text|>:1 $B:1\",\n+                    special_tokens=[\n+                        (\"<|begin_of_text|>\", tokenizer.convert_tokens_to_ids(\"<|begin_of_text|>\")),\n+                    ],\n+                ),\n+            ]\n+        )\n+\n+\n+O200K_PATTERN = r\"\"\"[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"  # noqa: E501\n+\n+\n+def write_tokenizer(args):\n+    tokenizer_path = os.path.join(args.input_dir, \"tokenizer.model\")\n+    chat_template = \"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\\\"%d %b %Y\\\") %}\\n    {%- else %}\\n        {%- set date_string = \\\"26 Jul 2024\\\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}    \\n    {%- if messages[0]['content'] is string %}\\n        {%- set system_message = messages[0]['content']|trim %}\\n    {%- else %}\\n        {#- FIXME: The processor requires an array, always. #}\\n        {%- set system_message = messages[0]['content'][0]['text']|trim %}\\n    {%- endif %}\\n    {%- set messages = messages[1:] %}\\n    {%- set user_supplied_system_message = true %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n    {%- set user_supplied_system_message = false %}\\n{%- endif %}\\n\\n{#- System message if the user supplied one #}\\n{%- if user_supplied_system_message %}\\n    {{- \\\"<|header_start|>system<|header_end|>\\n\\n\\\" }}\\n    {%- if tools is not none %}\\n        {{- \\\"Environment: ipython\\n\\\" }}\\n    {%- endif %}\\n    {%- if tools is not none and not tools_in_user_message %}\\n        {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n        {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n        {{- \\\"Do not use variables.\\n\\n\\\" }}\\n        {%- for t in tools %}\\n            {{- t | tojson(indent=4) }}\\n            {{- \\\"\\n\\n\\\" }}\\n        {%- endfor %}\\n    {%- endif %}\\n    {{- system_message }}\\n    {{- \\\"<|eot|>\\\" }}\\n{%- endif %}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|header_start|>user<|header_end|>\\n\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\n\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\n\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\n\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n    {{- '<|header_start|>' + message['role'] + '<|header_end|>\\n\\n' }}\\n        {%- if message['content'] is string %}\\n            {{- message['content'] }}\\n        {%- else %}\\n            {%- for content in message['content'] %}\\n                {%- if content['type'] == 'image' %}\\n                    {{- '<|image|>' }}\\n                {%- elif content['type'] == 'text' %}\\n                    {{- content['text'] }}\\n                {%- endif %}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\\"<|eot|>\\\" }}\\n    {%- elif 'tool_calls' in message and message.tool_calls|length > 0 %}\\n       {{- '<|header_start|>assistant<|header_end|>\\n\\n' -}}\\n       {{- '<|python_start|>' }}\\n        {%- if message['content'] is string %}\\n            {{- message['content'] }}\\n        {%- else %}\\n            {%- for content in message['content'] %}\\n                {%- if content['type'] == 'image' %}\\n                    {{- '<|image|>' }}\\n                {%- elif content['type'] == 'text' %}\\n                    {{- content['text'] }}\\n                {%- endif %}\\n            {%- endfor %}\\n        {%- endif %}\\n       {{- '<|python_end|>' }}\\n        {%- for tool_call in message.tool_calls %}\\n           {{- '{\\\"name\\\": \\\"' + tool_call.function.name + '\\\", ' }}\\n           {{- '\\\"parameters\\\": ' }}\\n           {{- tool_call.function.arguments | tojson }}\\n           {{- \\\"}\\\" }}\\n        {%- endfor %}\\n       {{- \\\"<|eot|>\\\" }}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|header_start|>ipython<|header_end|>\\n\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|header_start|>assistant<|header_end|>\\n\\n' }}\\n{%- endif %}\\n\"\n+\n+    special_tokens = BASIC_SPECIAL_TOKENS + LLAMA4_SPECIAL_TOKENS\n+    converter = Llama4Converter(\n+        vocab_file=tokenizer_path,\n+        pattern=O200K_PATTERN,\n+        special_tokens=special_tokens,\n+        chat_template=chat_template if args.instruct else None,\n+        bos_token=\"<|begin_of_text|>\",\n+        eos_token=\"<|end_of_text|>\" if not args.instruct else \"<|eot|>\",\n+        pad_token=\"<|finetune_right_pad_id|>\",\n+        model_max_length=max_context_length(args.input_dir, args.instruct),\n+    )\n+    tokenizer = converter.converted_tokenizer\n+\n+    image_processor = Llama4ImageProcessorFast()\n+    processor = Llama4Processor(\n+        image_processor=image_processor,\n+        tokenizer=tokenizer,\n+        chat_template=tokenizer.chat_template,\n+    )\n+    processor.save_pretrained(args.output_dir)\n+    del processor\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--input_dir\",\n+        type=str,\n+        default=\"/fsx/arthur/Llama-4-17B-Omni-Instruct-Original\",\n+        help=\"Location of the local folder copied from the Hub.\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        default=\"llama4_hf_vision\",\n+        type=str,\n+        help=\"Location to write HF model and tokenizer\",\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n+    )\n+    parser.add_argument(\n+        \"--special_tokens\",\n+        default=None,\n+        type=List[str],\n+        help=\"The list of special tokens that should be added to the model.\",\n+    )\n+    parser.add_argument(\n+        \"--num_shards\",\n+        default=8,\n+        type=int,\n+        help=\"The number of individual shards used for the model. Does not have to be the same as the number of consolidated_xx.pth\",\n+    )\n+    parser.add_argument(\n+        \"--instruct\",\n+        action=\"store_true\",\n+        help=\"Whether the model is an instruct model\",\n+    )\n+    parser.add_argument(\n+        \"--convert_checkpoints\",\n+        action=\"store_true\",\n+        help=\"Whether to convert the original weights (or skip if previously converted)\",\n+    )\n+\n+    args = parser.parse_args()\n+    write_tokenizer(args)\n+\n+    write_model(\n+        model_path=args.output_dir,\n+        input_base_path=args.input_dir,\n+        safe_serialization=args.safe_serialization,\n+        num_shards=args.num_shards,\n+        instruct=args.instruct,\n+        convert_checkpoints=args.convert_checkpoints,\n+    )"
        },
        {
            "sha": "6935ba798f79479ef11db1f8a26eaa50506de23f",
            "filename": "src/transformers/models/llama4/image_processing_llama4_fast.py",
            "status": "added",
            "additions": 480,
            "deletions": 0,
            "changes": 480,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -0,0 +1,480 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Got-OCR-2.\"\"\"\n+\n+import math\n+from collections import defaultdict\n+from functools import lru_cache\n+from typing import List, Optional, Set, Tuple, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+def get_factors(dividend: int) -> Set[int]:\n+    \"\"\"\n+    Calculate all factors of a given number, i.e. a dividor that leaves\n+    no remainder. For example, if dividend=12, it will return {1, 2, 3, 4, 6, 12}.\n+\n+    Args:\n+        dividend (int): The number to find factors for.\n+\n+    Returns:\n+        set: A set containing all factors of the number.\n+    \"\"\"\n+    factors_set = set()\n+\n+    for i in range(1, int(dividend**0.5) + 1):\n+        if dividend % i == 0:\n+            factors_set.add(i)\n+            factors_set.add(dividend // i)\n+    return factors_set\n+\n+\n+def get_max_res_without_distortion(\n+    image_size: Tuple[int, int],\n+    target_size: Tuple[int, int],\n+) -> Tuple[int, int]:\n+    \"\"\"\n+    Determines the maximum resolution to which an image can be resized to without distorting its\n+    aspect ratio, based on the target resolution.\n+\n+    Args:\n+        image_size (Tuple[int, int]): The original resolution of the image (height, width).\n+        target_resolution (Tuple[int, int]): The desired resolution to fit the image into (height, width).\n+    Returns:\n+        Tuple[int, int]: The optimal dimensions (height, width) to which the image should be resized.\n+    Example:\n+        >>> _get_max_res_without_distortion([200, 300], target_size = [450, 200])\n+        (134, 200)\n+        >>> _get_max_res_without_distortion([800, 600], target_size = [450, 1300])\n+        (450, 338)\n+    \"\"\"\n+\n+    original_height, original_width = image_size\n+    target_height, target_width = target_size\n+\n+    scale_w = target_width / original_width\n+    scale_h = target_height / original_height\n+\n+    if scale_w < scale_h:\n+        new_width = target_width\n+        new_height = min(math.floor(original_height * scale_w), target_height)\n+    else:\n+        new_height = target_height\n+        new_width = min(math.floor(original_width * scale_h), target_width)\n+\n+    return new_height, new_width\n+\n+\n+class Llama4ImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    max_patches: Optional[int]\n+    resize_to_max_canvas: Optional[bool]\n+\n+\n+def split_to_tiles(images: torch.Tensor, num_tiles_height: int, num_tiles_width: int) -> torch.Tensor:\n+    # Split image into number of required tiles (width x height)\n+    batch_size, num_channels, height, width = images.size()\n+    images = images.view(\n+        batch_size,\n+        num_channels,\n+        num_tiles_height,\n+        height // num_tiles_height,\n+        num_tiles_width,\n+        width // num_tiles_width,\n+    )\n+    # Permute dimensions to reorder the axes\n+    image = images.permute(0, 2, 4, 1, 3, 5).contiguous()\n+    # Reshape into the desired output shape (batch_size * 4, num_channels, width/2, height/2)\n+    image = image.view(\n+        batch_size,\n+        num_tiles_width * num_tiles_height,\n+        num_channels,\n+        height // num_tiles_height,\n+        width // num_tiles_width,\n+    )\n+    return image\n+\n+\n+@lru_cache(maxsize=1)\n+def find_supported_resolutions(max_num_chunks: int, patch_size: SizeDict) -> torch.Tensor:\n+    \"\"\"\n+    Computes all of the allowed resolutions for a fixed number of chunks\n+    and patch_size. Useful for when dividing an image into chunks.\n+\n+    Args:\n+        max_num_chunks (int): Maximum number of chunks for processing.\n+        patch_size (int): Size of the side of the patch.\n+\n+    Returns:\n+        torch.Tensor: List of possible resolutions as tuples (height, width).\n+\n+    Example:\n+        >>> max_num_chunks = 5\n+        >>> patch_size = 224\n+        >>> find_supported_resolutions(max_num_chunks, patch_size)\n+        tensor([(224, 896), (448, 448), (224, 224), (896, 224), (224, 672),\n+        (672, 224), (224, 448), (448, 224)])\n+\n+        Given max_num_chunks=4, patch_size=224, it will create a dictionary:\n+        {\n+        0.25: [(1, 4)],\n+        1.0: [(2, 2), (1, 1)],\n+        4.0: [(4, 1)],\n+        0.33: [(1, 3)],\n+        3.0: [(3, 1)],\n+        0.5: [(1, 2)],\n+        2.0: [(2, 1)]\n+        }\n+\n+        and return the resolutions multiplied by the patch_size:\n+        [(1*224, 4*224), (2*224, 2*224), ..., (2*224, 1*224)]\n+    \"\"\"\n+    height, width = patch_size.height, patch_size.width\n+    if height != width:\n+        raise ValueError(\"`size` must be square.\")\n+\n+    patch_size = height\n+\n+    asp_dict = defaultdict(list)\n+    for chunk_size in range(max_num_chunks, 0, -1):\n+        _factors = sorted(get_factors(chunk_size))\n+        _asp_ratios = [(factor, chunk_size // factor) for factor in _factors]\n+        for height, width in _asp_ratios:\n+            ratio_float = height / width\n+            asp_dict[ratio_float].append((height, width))\n+\n+    # get the resolutions multiplied by the patch_size\n+    possible_resolutions = []\n+    for key, value in asp_dict.items():\n+        for height, depth in value:\n+            possible_resolutions.append((height * patch_size, depth * patch_size))\n+\n+    return possible_resolutions\n+\n+\n+def pad_to_best_fit(\n+    images: \"torch.Tensor\",\n+    target_size: Tuple[int, int],\n+    background_color: Union[int, Tuple[int, int, int]] = 0,\n+) -> \"torch.Tensor\":\n+    \"\"\"\n+    Pads an image to fit the target size.\n+\n+    Args:\n+        images (`np.ndarray`):\n+            The images to pad.\n+        background_color (`int` or `Tuple[int, int, int]`, *optional*, defaults to 0):\n+            The color to use for the padding. Can be an integer for single channel or a\n+            tuple of integers representing for multi-channel images. If passed as integer\n+            in mutli-channel mode, it will default to `0` in subsequent channels.\n+    Returns:\n+        `torch.Tensor`: The padded images.\n+    \"\"\"\n+\n+    num_channels = images.shape[1] if len(images.shape) == 4 else images.shape[0]\n+    if isinstance(background_color, int):\n+        background_color = [background_color] + [0] * (num_channels - 1)\n+    elif len(background_color) != num_channels:\n+        raise ValueError(\n+            f\"background_color must have no more than {num_channels} elements to match the number of channels\"\n+        )\n+\n+    height, width = images.shape[-2:]\n+    target_height, target_width = target_size\n+    paste_x_right = target_width - width\n+    paste_y_right = target_height - height\n+    padded_images = F.pad(images, padding=[0, 0, paste_x_right, paste_y_right], fill=background_color)\n+\n+    return padded_images\n+\n+\n+def get_best_fit(\n+    image_size: Tuple[int, int],\n+    possible_resolutions: torch.Tensor,\n+    resize_to_max_canvas: bool = False,\n+) -> Tuple[int, int]:\n+    \"\"\"\n+    Determines the best canvas possible from a list of possible resolutions to, without distortion,\n+    resize an image to.\n+\n+    For each possible resolution, calculates the scaling factors for\n+    width and height, and selects the smallest one, which is the limiting side.\n+    E.g. to match the canvas you can upscale height by 2x, and width by 1.5x,\n+    therefore, the maximum upscaling you can do is min(2, 1.5) = 1.5.\n+\n+    If upscaling is possible (any of the scaling factors is greater than 1),\n+    then picks the smallest upscaling factor > 1, unless resize_to_max_canvas is True.\n+\n+    If upscaling is not possible, then picks the largest scaling factor <= 1, i.e.\n+    reduce downscaling as much as possible.\n+\n+    If there are multiple resolutions with the same max scale, we pick the one with the lowest area,\n+    to minimize padding. E.g., the same image can be upscaled to 224x224 and 224x448, but the latter\n+    has more padding.\n+\n+    Args:\n+        image_size (Tuple[int, int]): A tuple containing the height and width of the image.\n+        possible_resolutions (torch.Tensor): A tensor of shape (N, 2) where each\n+            row represents a possible resolution (height, width).\n+        resize_to_max_canvas (bool): If True, will return the largest upscaling resolution.\n+\n+    Returns:\n+        List[int]: The best resolution [height, width] for the given image.\n+\n+    Example:\n+        >>> image_size = (200, 300)\n+        >>> possible_resolutions = torch.tensor([[224, 672],\n+        ...                                     [672, 224],\n+        ...                                     [224, 448],\n+        ...                                     [448, 224],\n+        ...                                     [224, 224]])\n+        >>> get_best_fit(image_size, possible_resolutions)\n+        [224, 448]\n+\n+        We have:\n+            scale_w = tensor([2.2400, 0.7467, 1.4933, 0.7467, 0.7467])\n+            scale_h = tensor([1.1200, 3.3600, 1.1200, 2.2400, 1.1200])\n+            scales = tensor([1.1200, 0.7467, 1.1200, 0.7467, 0.7467])\n+        Only one of the scales > 1:\n+            upscaling_possible = tensor([1.1200, 1.1200])\n+            smallest_rescale = tensor(1.1200)\n+        So we pick the resolution with the smallest smallest area:\n+            areas = tensor([150528, 100352]) # [672, 224], [224, 448]\n+            optimal_canvas = tensor([224, 448])\n+    \"\"\"\n+\n+    original_height, original_width = image_size\n+\n+    # get all possible resolutions heights/widths\n+    target_heights, target_widths = (\n+        possible_resolutions[:, 0],\n+        possible_resolutions[:, 1],\n+    )\n+\n+    # get scaling factors to resize the image without distortion\n+    scale_w = target_widths / original_width\n+    scale_h = target_heights / original_height\n+\n+    # get the min scale between width and height (limiting side -> no distortion)\n+    scales = torch.where(scale_h > scale_w, scale_w, scale_h)\n+\n+    # filter only scales that allow upscaling\n+    upscaling_options = scales[scales >= 1]\n+    if len(upscaling_options) > 0:\n+        if resize_to_max_canvas:\n+            selected_scale = torch.max(upscaling_options)\n+        else:\n+            selected_scale = torch.min(upscaling_options)\n+    else:\n+        # no upscaling possible,\n+        # get the minimum downscaling (max scale for scales<1)\n+        downscaling_options = scales[scales < 1]\n+        selected_scale = torch.max(downscaling_options)\n+\n+    # get all resolutions that support this scaling factor,\n+    # e.g. you can upscale to 224x224, 224x448, 224x672 without distortion\n+    chosen_canvas = possible_resolutions[scales == selected_scale]\n+\n+    # if there are multiple resolutions,\n+    # get the one with minimum area to reduce padding\n+    if len(chosen_canvas) > 1:\n+        areas = chosen_canvas[:, 0] * chosen_canvas[:, 1]\n+        optimal_idx = torch.argmin(areas)\n+        optimal_canvas = chosen_canvas[optimal_idx]\n+    else:\n+        optimal_canvas = chosen_canvas[0]\n+\n+    return tuple(optimal_canvas.tolist())\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast Llama4 image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n+        max_patches (`int`, *optional*, defaults to 16):\n+            The maximum number of patches to be extracted from the image.\n+            Can be overridden by the `max_patches` parameter in the `preprocess` method.\n+        resize_to_max_canvas (`bool`, *optional*, defaults to False):\n+            Whether to resize the image to the maximum canvas size.\n+            If True, picks the canvas the allows the largest resizing without distortion.\n+            If False, downsample as little as possible, including no resizing at all,\n+            but never upsample, unless the image is smaller than the patch size.\n+    \"\"\",\n+)\n+class Llama4ImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = [0.5, 0.5, 0.5]\n+    image_std = [0.5, 0.5, 0.5]\n+    size = {\"height\": 336, \"width\": 336}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+    max_patches = 16\n+    resize_to_max_canvas = False\n+    valid_kwargs = Llama4ImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[Llama4ImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    def rescale_and_normalize(\n+        self,\n+        images: \"torch.Tensor\",\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Union[float, List[float]],\n+        image_std: Union[float, List[float]],\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Rescale and normalize images.\n+        Override to rescale and normalize the images in torch.bfloat16 as in the original implementation\n+        \"\"\"\n+        if do_rescale and do_normalize:\n+            images = images.to(dtype=torch.bfloat16) * rescale_factor\n+            images = self.normalize(images, image_mean, image_std)\n+        elif do_rescale:\n+            images = images * rescale_factor\n+        elif do_normalize:\n+            images = self.normalize(images, image_mean, image_std)\n+\n+        return images\n+\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+        max_patches (`int`, *optional*, defaults to 16):\n+            The maximum number of patches to be extracted from the image.\n+            Can be overridden by the `max_patches` parameter in the `preprocess` method.\n+        resize_to_max_canvas (`bool`, *optional*, defaults to False):\n+            Whether to resize the image to the maximum canvas size.\n+            If True, picks the canvas the allows the largest resizing without distortion.\n+            If False, downsample as little as possible, including no resizing at all,\n+            but never upsample, unless the image is smaller than the patch size.\n+        \"\"\",\n+    )\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[Llama4ImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: List[\"torch.Tensor\"],\n+        size: SizeDict,\n+        max_patches: int,\n+        resize_to_max_canvas: bool,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, List[float]]],\n+        image_std: Optional[Union[float, List[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        possible_resolutions = find_supported_resolutions(max_num_chunks=max_patches, patch_size=size)\n+        possible_resolutions = torch.tensor(possible_resolutions)\n+        # process images by batch, grouped by shape\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_processed_images = {}\n+        grouped_aspect_ratios = {}\n+        for shape, stacked_images in grouped_images.items():\n+            image_size = stacked_images.shape[-2:]\n+            target_size = get_best_fit(image_size, possible_resolutions, resize_to_max_canvas=resize_to_max_canvas)\n+            # If target_size requires upscaling, we might want to limit the upscaling to max_upscaling_size\n+            max_upscaling_size = None if resize_to_max_canvas else size.height\n+            if max_upscaling_size is not None:\n+                new_target_height = min(max(image_size[0], max_upscaling_size), target_size[0])\n+                new_target_width = min(max(image_size[1], max_upscaling_size), target_size[1])\n+                target_size_without_distortion = (new_target_height, new_target_width)\n+\n+            # resize to target_size while preserving aspect ratio\n+            new_size_without_distortion = get_max_res_without_distortion(image_size, target_size_without_distortion)\n+            new_size_without_distortion = SizeDict(\n+                height=max(new_size_without_distortion[0], 1), width=max(new_size_without_distortion[1], 1)\n+            )\n+            processed_images = self.resize(\n+                stacked_images,\n+                new_size_without_distortion,\n+                interpolation=interpolation,\n+            )\n+\n+            # pad to target_size to be able to split into tiles\n+            processed_images = pad_to_best_fit(processed_images, target_size)\n+            processed_images = self.rescale_and_normalize(\n+                processed_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+\n+            ratio_h, ratio_w = (\n+                target_size[0] // size.height,\n+                target_size[1] // size.height,\n+            )\n+            # split into tiles\n+            processed_images = split_to_tiles(processed_images, ratio_h, ratio_w)\n+            grouped_processed_images[shape] = processed_images\n+            grouped_aspect_ratios[shape] = torch.tensor([[ratio_h, ratio_w]] * stacked_images.shape[0])\n+\n+            # add a global tile to the processed tile if there are more than one tile\n+            if ratio_h * ratio_w > 1:\n+                global_tiles = self.resize(\n+                    stacked_images,\n+                    size,\n+                    interpolation=interpolation,\n+                )\n+                global_tiles = self.rescale_and_normalize(\n+                    global_tiles, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+                )\n+                grouped_processed_images[shape] = torch.cat([processed_images, global_tiles.unsqueeze(1)], dim=1)\n+        processed_images = reorder_images(grouped_processed_images, grouped_images_index)\n+        aspect_ratios_list = reorder_images(grouped_aspect_ratios, grouped_images_index)\n+\n+        processed_images = torch.cat(processed_images, dim=0) if return_tensors else processed_images\n+        aspect_ratios = torch.stack(aspect_ratios_list, dim=0) if return_tensors else aspect_ratios_list\n+        return BatchFeature(\n+            data={\"pixel_values\": processed_images, \"aspect_ratios\": aspect_ratios}, tensor_type=return_tensors\n+        )\n+\n+\n+__all__ = [\"Llama4ImageProcessorFast\"]"
        },
        {
            "sha": "f511674dc0ef3dc27e2d8a1bb6f4ff4d364043db",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "added",
            "additions": 1903,
            "deletions": 0,
            "changes": 1903,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -0,0 +1,1903 @@\n+# coding=utf-8\n+# Copyright 2025 The LLAMA4 and HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from dataclasses import dataclass\n+from typing import Callable, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+\n+from transformers.models.llama4.configuration_llama4 import Llama4VisionConfig\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    BaseModelOutputWithPast,\n+    CausalLMOutputWithPast,\n+    ModelOutput,\n+)\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from .configuration_llama4 import Llama4Config, Llama4TextConfig\n+\n+\n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+logger = logging.get_logger(__name__)\n+_CHECKPOINT_FOR_DOC = \"meta-ai/Llama-4-17B\"\n+_CONFIG_FOR_DOC = \"Llama4Config\"\n+\n+\n+class Llama4TextExperts(nn.Module):\n+    def __init__(self, config: Llama4Config):\n+        super().__init__()\n+        self.num_experts = config.num_local_experts\n+        self.intermediate_size = config.intermediate_size\n+        self.hidden_size = config.hidden_size\n+        self.expert_dim = self.intermediate_size\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n+        self.down_proj = nn.Parameter(torch.empty((self.num_experts, self.expert_dim, self.hidden_size)))\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        This should really not be run on a single machine, as we are reaching compute bound:\n+        - the inputs are expected to be \"sorted\" per expert already.\n+        - the weights are viewed with another dim, to match num_expert, 1, shape * num_tokens, shape\n+\n+        Args:\n+            hidden_states (torch.Tensor): (batch_size * token_num, hidden_size)\n+            selected_experts (torch.Tensor): (batch_size * token_num, top_k)\n+            routing_weights (torch.Tensor): (batch_size * token_num, top_k)\n+        Returns:\n+            torch.Tensor\n+        \"\"\"\n+        hidden_states = hidden_states.view(self.num_experts, -1, self.hidden_size)\n+        gate_up = torch.bmm(hidden_states, self.gate_up_proj)\n+        gate, up = gate_up.chunk(2, dim=-1)  # not supported for DTensors\n+        next_states = torch.bmm((up * self.act_fn(gate)), self.down_proj)\n+        next_states = next_states.view(-1, self.hidden_size)\n+        return next_states\n+\n+\n+# Phi3MLP\n+class Llama4TextMLP(nn.Module):\n+    def __init__(self, config, intermediate_size=None):\n+        super().__init__()\n+\n+        if intermediate_size is None:\n+            intermediate_size = config.intermediate_size\n+\n+        self.config = config\n+        self.gate_proj = nn.Linear(config.hidden_size, intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(config.hidden_size, intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(intermediate_size, config.hidden_size, bias=False)\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.activation_fn(self.gate_proj(x)) * self.up_proj(x)\n+        return self.down_proj(down_proj)\n+\n+\n+class Llama4TextL2Norm(torch.nn.Module):\n+    def __init__(self, dim: int = None, eps: float = 1e-6):\n+        super().__init__()\n+        self.eps = eps\n+\n+    def _norm(self, x):\n+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+\n+    def forward(self, x):\n+        return self._norm(x.float()).type_as(x)\n+\n+    def extra_repr(self):\n+        return f\"eps={self.eps}\"\n+\n+\n+class Llama4TextRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-5):\n+        \"\"\"\n+        Llama4RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.eps = eps\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+\n+    def _norm(self, x):\n+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+\n+    def forward(self, x):\n+        output = self._norm(x.float()).type_as(x)\n+        return output * self.weight\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n+\n+\n+class Llama4TextMoe(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n+        self.hidden_dim = config.hidden_size\n+        self.num_experts = config.num_local_experts\n+        self.experts = Llama4TextExperts(config)\n+        self.router = nn.Linear(config.hidden_size, config.num_local_experts, bias=False)\n+        self.shared_expert = Llama4TextMLP(config)\n+\n+    def forward(self, hidden_states):\n+        batch, seq_len, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, self.hidden_dim)\n+        router_logits = self.router(hidden_states).transpose(0, 1)\n+        tokens_per_expert = batch * seq_len\n+\n+        router_top_value, router_indices = torch.topk(router_logits.transpose(0, 1), self.top_k, dim=1)\n+        router_scores = (\n+            torch.full_like(router_logits.transpose(0, 1), float(\"-inf\"))\n+            .scatter_(1, router_indices, router_top_value)\n+            .transpose(0, 1)\n+        )\n+        # We do this to make sure we have -inf for non topK tokens before going through the !\n+        # Here we are just creating a tensor to index each and every single one of the hidden states. Let s maybe register a buffer for this!\n+        router_indices = (\n+            torch.arange(tokens_per_expert, device=hidden_states.device).view(1, -1).expand(router_scores.size(0), -1)\n+        )\n+        router_scores = torch.sigmoid(router_scores.float()).to(hidden_states.dtype)\n+\n+        router_indices = router_indices.reshape(-1, 1).expand(-1, hidden_dim)\n+        routed_in = torch.gather(\n+            input=hidden_states,\n+            dim=0,\n+            index=router_indices,\n+        ).to(hidden_states.device)\n+        # we gather inputs corresponding to each expert based on the router indices\n+        routed_in = routed_in * router_scores.reshape(-1, 1)\n+        routed_out = self.experts(routed_in)\n+        out = self.shared_expert(hidden_states)\n+        # now that we finished expert computation -> we scatter add because we gathered previously\n+        # we have to do this because we used all experts on all tokens. This is faster than the for loop, tho you are compute bound\n+        # this scales a lot better if you do EP!\n+        out.scatter_add_(dim=0, index=router_indices, src=routed_out.view(-1, hidden_dim))\n+        return out, router_scores\n+\n+\n+class Llama4TextRotaryEmbedding(nn.Module):\n+    def __init__(self, config: Llama4TextConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        self.rope_type = \"llama3\" if config.rope_scaling is not None else \"default\"\n+\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            # This .to() is needed if the model has been moved to a device after being initialized (because\n+            # the buffer is automatically moved, but not the original copy)\n+            self.original_inv_freq = self.original_inv_freq.to(device)\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.to(x.device) @ position_ids_expanded).transpose(1, 2)\n+            freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # Convert to complex representation\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        freqs_cis = freqs_cis * self.attention_scaling\n+        return freqs_cis\n+\n+\n+def apply_rotary_emb(\n+    xq: torch.Tensor,\n+    xk: torch.Tensor,\n+    freqs_cis: torch.Tensor,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n+    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n+    xq_out = torch.view_as_real(xq_ * freqs_cis[:, :, None, :]).flatten(3)\n+    xk_out = torch.view_as_real(xk_ * freqs_cis[:, :, None, :]).flatten(3)\n+    return xq_out.type_as(xq), xk_out.type_as(xk)\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) / math.sqrt(module.head_dim)\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights.float(), dim=-1).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Llama4TextAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Llama4TextConfig, layer_idx):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_attention_heads = config.num_attention_heads\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attn_scale = config.attn_scale\n+        self.floor_scale = config.floor_scale\n+        self.attn_temperature_tuning = config.attn_temperature_tuning\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+        self.use_rope = int((layer_idx + 1) % 4 != 0)  # rope unused for dense layers\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        if self.config.use_qk_norm and self.use_rope:\n+            self.qk_norm = Llama4TextL2Norm()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape)\n+        key_states = self.k_proj(hidden_states).view(*input_shape, -1, self.head_dim)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        if self.use_rope:  # the 16E model skips rope for long context on certain layers\n+            query_states, key_states = apply_rotary_emb(\n+                query_states, key_states, position_embeddings.to(query_states.device)\n+            )\n+\n+        if hasattr(self, \"qk_norm\"):  # the 128E model does not use qk_norm\n+            query_states = self.qk_norm(query_states)\n+            key_states = self.qk_norm(key_states)\n+\n+        # Use temperature tuning from https://arxiv.org/abs/2501.19399) to NoROPE layers\n+        if self.attn_temperature_tuning and not self.use_rope:\n+            attn_scales = (\n+                torch.log(torch.floor((cache_position.float() + 1.0) / self.floor_scale) + 1.0) * self.attn_scale + 1.0\n+            )\n+            attn_scales = attn_scales.view((*input_shape, 1, 1))\n+            query_states = (query_states * attn_scales).to(query_states.dtype)\n+\n+        query_states = query_states.transpose(1, 2)\n+        key_states = key_states.transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Llama4TextDecoderLayer(nn.Module):\n+    def __init__(self, config, layer_idx):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.self_attn = Llama4TextAttention(config, layer_idx)\n+        self.use_chunked_attention = int((layer_idx + 1) % 4 != 0)  # <=> use rope\n+        self.is_moe_layer = layer_idx in config.moe_layers\n+        if self.is_moe_layer:  # the 128E model interleaves dense / sparse\n+            self.feed_forward = Llama4TextMoe(config)\n+        else:\n+            self.feed_forward = Llama4TextMLP(config, intermediate_size=config.intermediate_size_mlp)\n+\n+        self.input_layernorm = Llama4TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Llama4TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        self.layer_idx = layer_idx\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        chunk_causal_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        output_attentions: Optional[bool] = False,\n+        output_router_logits: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # use local attention mask for ROPE layers\n+        if self.use_chunked_attention and chunk_causal_mask is not None:\n+            attention_mask = chunk_causal_mask\n+\n+        # Self Attention\n+        attention_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = residual + attention_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.feed_forward(hidden_states)\n+        if self.is_moe_layer:\n+            hidden_states, router_logits = hidden_states\n+        else:\n+            router_logits = None\n+        hidden_states = residual + hidden_states.view(residual.shape)\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        if output_router_logits:\n+            outputs += (router_logits,)\n+\n+        return outputs\n+\n+\n+LLAMA4_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`Llama4Config`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Llama4 Model outputting raw hidden-states without any specific head on top.\",\n+    LLAMA4_START_DOCSTRING,\n+)\n+class Llama4PreTrainedModel(PreTrainedModel):\n+    config_class = Llama4Config\n+    supports_gradient_checkpointing = True\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+    _supports_attention_backend = True\n+\n+    def _init_weights(self, module):\n+        std = (\n+            self.config.initializer_range\n+            if hasattr(self.config, \"initializer_range\")\n+            else self.config.text_config.initializer_range\n+        )\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+LLAMA4_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Llama4 Model outputting raw hidden-states without any specific head on top.\",\n+    LLAMA4_START_DOCSTRING,\n+)\n+class Llama4TextModel(Llama4PreTrainedModel):\n+    _no_split_modules = [\"Llama4TextDecoderLayer\"]\n+    base_model_prefix = \"model\"\n+    config_class = Llama4TextConfig\n+\n+    def __init__(self, config: Llama4TextConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Llama4TextDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Llama4TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Llama4TextRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(LLAMA4_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids.to(self.embed_tokens.weight.device))\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask, chunk_causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        freq_cis = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    chunk_causal_mask,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                    freq_cis,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    chunk_causal_mask=chunk_causal_mask,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                    position_embeddings=freq_cis,\n+                    **flash_attn_kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        output = BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+        chunked_attention_mask=None,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask, attention_mask  # flash does not support chunked attn TODO support flash\n+            return None, None\n+\n+        if self.config._attn_implementation not in [\"sdpa\", \"flex_attention\", \"eager\"]:\n+            return None, None\n+\n+        sequence_length = input_tensor.shape[1]\n+        cache_position = cache_position.to(self.device)\n+        attention_chunk_size = self.config.attention_chunk_size\n+\n+        first_cache_position = cache_position[0]\n+        last_cache_position = cache_position[-1]\n+\n+        # to avoid graph break, we introduce this hack\n+        cond1 = first_cache_position >= attention_chunk_size\n+        cond2 = (first_cache_position < attention_chunk_size) & (\n+            first_cache_position + sequence_length > attention_chunk_size\n+        )\n+\n+        key_length = torch.where(\n+            cond1,\n+            attention_chunk_size + sequence_length - 1,\n+            torch.where(cond2, first_cache_position + sequence_length, attention_chunk_size),\n+        )\n+\n+        if past_key_values is not None and past_key_values.is_compileable:\n+            target_length = past_key_values.get_max_cache_shape\n+        else:\n+            target_length = attention_mask.shape[-1] if attention_mask is not None else sequence_length\n+\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                offsets = (first_cache_position, max(last_cache_position - key_length, 0))\n+                chunked_attention_mask = make_flex_block_causal_mask(\n+                    attention_mask, self.config.attention_chunk_size, sequence_length, key_length, offsets=offsets\n+                )\n+                attention_mask = make_flex_block_causal_mask(\n+                    attention_mask,\n+                    query_length=sequence_length,\n+                    key_length=past_key_values.get_max_cache_shape(),\n+                    offsets=None if sequence_length != 1 else (first_cache_position, 0),\n+                )\n+                return attention_mask, chunked_attention_mask\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask, chunked_attention_mask\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+        if target_length > self.config.attention_chunk_size:\n+            chunked_attention_mask = self.create_chunked_attention_mask(\n+                self.config.attention_chunk_size,\n+                start=first_cache_position,\n+                end=first_cache_position + key_length,\n+                device=device,\n+            )\n+            chunked_attention_mask = chunked_attention_mask & attention_mask\n+            if sequence_length == 1:\n+                chunked_attention_mask = chunked_attention_mask[-1:]\n+            if self.config._attn_implementation == \"eager\":\n+                chunked_attention_mask = (\n+                    chunked_attention_mask[None, None, :, :]\n+                    .to(dtype)\n+                    .masked_fill(chunked_attention_mask, torch.finfo(dtype).min)\n+                )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.ndim == 4\n+            and not output_attentions  # Only unmask for 4d masks\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+            # chunked_attention_mask = AttentionMaskConverter._unmask_unattended(chunked_attention_mask, min_dtype)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and chunked_attention_mask is not None:\n+            chunked_attention_mask = chunked_attention_mask.bool()\n+            causal_mask = causal_mask.bool()\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=first_cache_position,\n+                is_training=self.training,\n+            ):\n+                causal_mask = None\n+        return causal_mask, chunked_attention_mask\n+\n+    def create_chunked_attention_mask(\n+        self, attention_chunk_size: int, start: int, end: int, device: torch.device\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Generate the following:\n+\n+        'What'      :  0 ■ ⬚ ⬚ ⬚ ⬚ ⬚    |\n+        '▁is'       :  1 ■ ■ ⬚ ⬚ ⬚ ⬚     |\n+        '▁ch'       :  2 ■ ■ ■ ⬚ ⬚ ⬚     |\n+        'unked'     :  3 ⬚ ⬚ ⬚ ■ ⬚ ⬚    |\n+        '▁attention':  4 ⬚ ⬚ ⬚ ■ ■ ⬚    |\n+        '?'         :  5 ⬚ ⬚ ⬚ ■ ■ ■     |\n+\n+        If the chunk size is 3.\n+        This can just be appplied over the already created attention mask\n+        \"\"\"\n+        block_pos = torch.abs(\n+            (torch.arange(start, end).unsqueeze(0) // attention_chunk_size)\n+            - (torch.arange(start, end).unsqueeze(1) // attention_chunk_size)\n+        )\n+        token_pos = torch.arange(start, end).unsqueeze(0) - torch.arange(start, end).unsqueeze(1)\n+        mask = (block_pos == 0) & (token_pos <= 0)\n+        return mask.to(device)\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.to(device).reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(device)\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+\n+class Llama4ForCausalLM(Llama4PreTrainedModel, GenerationMixin):\n+    base_model_prefix = \"language_model\"\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    config_class = Llama4TextConfig\n+\n+    def __init__(self, config: Llama4TextConfig):\n+        super().__init__(config)\n+        self.model = Llama4TextModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @add_start_docstrings_to_model_forward(LLAMA4_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Llama4ForCausalLM\n+\n+        >>> model = Llama4ForCausalLM.from_pretrained(\"meta-llama4/Llama4-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama4/Llama4-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@dataclass\n+class Llama4CausalLMOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for Llava causal language model (or autoregressive) outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: torch.FloatTensor = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+class Llama4VisionMLP2(torch.nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.fc1 = nn.Linear(self.intermediate_size, config.projector_input_dim, bias=False)\n+        self.fc2 = nn.Linear(config.projector_output_dim, config.projector_output_dim, bias=False)\n+        self.activation_fn = nn.GELU()  # ACT2FN[config.hidden_act]\n+        self.dropout = config.projector_dropout\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n+        return self.activation_fn(self.fc2(hidden_states))\n+\n+\n+class Llama4MultiModalProjector(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.vision_output_dim,\n+            config.text_config.hidden_size,\n+            bias=False,\n+        )\n+\n+    def forward(self, image_features):\n+        hidden_states = self.linear_1(image_features)\n+        return hidden_states\n+\n+\n+def pixel_shuffle(input_tensor, shuffle_ratio):\n+    # input_tensor: [batch_size, num_patches, channels]\n+    batch_size, num_patches, channels = input_tensor.shape\n+    patch_size = int(math.sqrt(num_patches))\n+\n+    input_tensor = input_tensor.view(batch_size, patch_size, patch_size, -1)\n+    batch_size, height, width, channels = input_tensor.size()\n+\n+    reshaped_tensor = input_tensor.view(batch_size, height, int(width * shuffle_ratio), int(channels / shuffle_ratio))\n+    reshaped_tensor = reshaped_tensor.permute(0, 2, 1, 3).contiguous()\n+\n+    reshaped_tensor = reshaped_tensor.view(\n+        batch_size, int(height * shuffle_ratio), int(width * shuffle_ratio), int(channels / (shuffle_ratio**2))\n+    )\n+    reshaped_tensor = reshaped_tensor.permute(0, 2, 1, 3).contiguous()\n+\n+    output_tensor = reshaped_tensor.view(batch_size, -1, reshaped_tensor.shape[-1])\n+    return output_tensor\n+\n+\n+class Llama4VisionPixelShuffleMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.pixel_shuffle_ratio = config.pixel_shuffle_ratio\n+        self.inner_dim = int(config.projector_input_dim // (self.pixel_shuffle_ratio**2))\n+        self.output_dim = config.projector_output_dim\n+        self.mlp = Llama4VisionMLP2(config)\n+\n+    def forward(self, encoded_patches: torch.Tensor) -> torch.Tensor:\n+        encoded_patches = pixel_shuffle(encoded_patches, self.pixel_shuffle_ratio)\n+        return self.mlp(encoded_patches)\n+\n+\n+LLAVA_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`LlavaConfig`] or [`LlavaVisionConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+# TODO there is a different RoPE for vision encoder, defined as below\n+def reshape_for_broadcast(freqs_ci: torch.Tensor, query: torch.Tensor):\n+    ndim = query.ndim\n+    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(query.shape)]\n+    return freqs_ci.view(*shape)\n+\n+\n+def vision_apply_rotary_emb(\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    freqs_ci: torch.Tensor,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    query_ = torch.view_as_complex(query.float().reshape(*query.shape[:-1], -1, 2))\n+    key_ = torch.view_as_complex(key.float().reshape(*key.shape[:-1], -1, 2))\n+    freqs_ci = reshape_for_broadcast(freqs_ci=freqs_ci, query=query_)  # freqs_ci[:,:,None,:]\n+    freqs_ci = freqs_ci.to(query_.device)\n+    query_out = torch.view_as_real(query_ * freqs_ci).flatten(3)\n+    key_out = torch.view_as_real(key_ * freqs_ci).flatten(3)\n+    return query_out.type_as(query), key_out.type_as(key)  # but this drops to 8e-3\n+\n+\n+class Llama4VisionAttention(nn.Module):\n+    def __init__(self, config: Llama4VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = config.hidden_size // config.num_attention_heads\n+        self.num_key_value_groups = 1\n+        self.attention_dropout = config.attention_dropout\n+\n+        self.q_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=True)\n+        self.k_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=True)\n+        self.v_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=True)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.embed_dim, bias=True)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        freqs_ci: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape)\n+\n+        query_states, key_states = vision_apply_rotary_emb(query_states, key_states, freqs_ci=freqs_ci)\n+\n+        query_states = query_states.transpose(1, 2)\n+        key_states = key_states.transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        # flex disable because breaks on TP 8, embed is 88 not power of 2\n+        if self.config._attn_implementation not in [\"eager\", \"flex_attention\"]:\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            None,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=None,\n+            is_causal=False,  # HAS TO BE ENFORCED\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Llama4VisionMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = nn.GELU()  # ACT2FN[config.hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size, bias=True)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size, bias=True)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+class Llama4VisionEncoderLayer(nn.Module):\n+    def __init__(self, config: Llama4VisionConfig):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = Llama4VisionAttention(config)\n+        self.mlp = Llama4VisionMLP(config)\n+\n+        self.input_layernorm = nn.LayerNorm(config.hidden_size)\n+        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_state: torch.Tensor,\n+        freqs_ci: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = None,\n+    ):\n+        # Self Attention\n+        residual = hidden_state\n+\n+        hidden_state = self.input_layernorm(hidden_state)\n+\n+        hidden_state, attn_weights = self.self_attn(\n+            hidden_state,\n+            freqs_ci=freqs_ci,\n+            attention_mask=attention_mask,\n+        )\n+        hidden_state = residual + hidden_state\n+\n+        # Feed forward\n+        residual = hidden_state\n+        hidden_state = self.post_attention_layernorm(hidden_state)\n+        hidden_state = self.mlp(hidden_state)\n+        hidden_state = residual + hidden_state\n+\n+        outputs = (hidden_state,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+class Llama4VisionEncoder(nn.Module):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`Llama4VisionEncoderLayer`].\n+\n+    Args:\n+        config: Llama4VisionConfig\n+    \"\"\"\n+\n+    def __init__(self, config: Llama4VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList([Llama4VisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+        self.config = config\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        freqs_ci: torch.Tensor,  # TODO move this to an attribute instead of keeping it around\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutput]:\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n+                than the model's internal embedding lookup matrix.\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        for encoder_layer in self.layers:\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    encoder_layer.__call__,\n+                    hidden_states,\n+                    attention_mask,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = encoder_layer(\n+                    hidden_state=hidden_states,\n+                    attention_mask=attention_mask,\n+                    output_attentions=output_attentions,\n+                    freqs_ci=freqs_ci,\n+                )\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+            hidden_states = layer_outputs[0]\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n+        )\n+\n+\n+class Llama4UnfoldConvolution(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        kernel_size = config.patch_size\n+        if isinstance(kernel_size, int):\n+            kernel_size = (kernel_size, kernel_size)\n+        self.unfold = torch.nn.Unfold(kernel_size=kernel_size, stride=config.patch_size)\n+        self.linear = nn.Linear(\n+            config.num_channels * kernel_size[0] * kernel_size[1],\n+            config.hidden_size,\n+            bias=False,\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.unfold(hidden_states)\n+        hidden_states = hidden_states.permute(0, 2, 1)\n+        hidden_states = self.linear(hidden_states)\n+        return hidden_states\n+\n+\n+class Llama4VisionRotaryEmbedding(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        idx = config.image_size // config.patch_size\n+        img_idx = torch.arange(idx**2, dtype=torch.int32).reshape(idx**2, 1)\n+        img_idx = torch.cat([img_idx, img_idx[:1]], dim=0)\n+        img_idx[-1, -1] = -2  # ID_CLS_TOKEN\n+        frequencies_x = img_idx % idx  # get the coordinates of the 2d matrix along x\n+        frequencies_y = img_idx // idx  # get the coordinates of the 2d matrix along y\n+        freq_dim = config.hidden_size // config.num_attention_heads // 2\n+        rope_freq = 1.0 / (config.rope_theta ** (torch.arange(0, freq_dim, 2)[: (freq_dim // 2)].float() / freq_dim))\n+        freqs_x = ((frequencies_x + 1)[..., None] * rope_freq[None, None, :]).repeat_interleave(2, dim=-1)\n+        freqs_y = ((frequencies_y + 1)[..., None] * rope_freq[None, None, :]).repeat_interleave(2, dim=-1)\n+        freqs = torch.cat([freqs_x, freqs_y], dim=-1).float().contiguous()[..., ::2]\n+        freqs = freqs.masked_fill(img_idx.reshape(-1, 1, 1) < 0, 0)\n+        freq_cis = torch.view_as_complex(torch.stack([torch.cos(freqs), torch.sin(freqs)], dim=-1))\n+        self.freqs_ci = freq_cis  # idx**2, idx**2, idx * 2\n+\n+    def forward(self, hidden_states):\n+        return self.freqs_ci.to(hidden_states.device)\n+\n+\n+class Llama4VisionModel(Llama4PreTrainedModel):\n+    base_model_prefix = \"vision_model\"\n+    _no_split_modules = [\"Llama4VisionAttention\"]\n+    config_class = Llama4VisionConfig\n+\n+    def __init__(self, config: Llama4VisionConfig):\n+        super().__init__(config)\n+        self.image_size = config.image_size\n+        self.patch_size = config.patch_size\n+        self.hidden_size = config.hidden_size\n+        self.num_channels = config.num_channels\n+\n+        self.num_patches = (self.image_size // self.patch_size) ** 2 + 1\n+        self.scale = config.hidden_size**-0.5\n+\n+        self.patch_embedding = Llama4UnfoldConvolution(config)\n+\n+        self.class_embedding = nn.Parameter(self.scale * torch.randn(self.hidden_size))\n+        self.positional_embedding_vlm = nn.Parameter(self.scale * torch.randn(self.num_patches, self.hidden_size))\n+        self.rotary_embedding = Llama4VisionRotaryEmbedding(config)\n+\n+        # layer norms\n+        self.layernorm_pre = nn.LayerNorm(self.hidden_size)\n+        self.layernorm_post = nn.LayerNorm(self.hidden_size)\n+\n+        # encoders\n+        self.model = Llama4VisionEncoder(config)\n+        self.vision_adapter = Llama4VisionPixelShuffleMLP(config)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        \"\"\"\n+        This function is used to fetch the first embedding layer to activate grads on inputs.\n+        \"\"\"\n+        return self.patch_embedding\n+\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[BaseModelOutput, Tuple[torch.Tensor, ...]]:\n+        r\"\"\"\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, MllamaVisionModel\n+\n+        >>> checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n+        >>> model = MllamaVisionModel.from_pretrained(checkpoint)\n+        >>> processor = AutoProcessor.from_pretrained(checkpoint)\n+\n+        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> output = model(**inputs)\n+\n+        >>> print(output.last_hidden_state.shape)\n+        torch.Size([1, 1, 4, 1025, 7680])\n+        ```\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # num_concurrent_media and num_chunks are both currently 1\n+        batch_size_times_num_tiles, num_channels, height, width = pixel_values.shape\n+        num_concurrent_media = 1\n+        num_chunks = 1\n+        hidden_state = self.patch_embedding(pixel_values)\n+        _, num_patches, hidden_dim = hidden_state.shape\n+\n+        # Add cls token\n+        hidden_state = hidden_state.reshape(\n+            batch_size_times_num_tiles * num_concurrent_media * num_chunks, num_patches, hidden_dim\n+        )\n+        class_embedding = self.class_embedding.expand(hidden_state.shape[0], 1, hidden_state.shape[-1])\n+        hidden_state = torch.cat([hidden_state, class_embedding], dim=1)\n+        num_patches += 1\n+\n+        # Position embeddings\n+        hidden_state = hidden_state.reshape(\n+            batch_size_times_num_tiles * num_concurrent_media, num_chunks, num_patches, hidden_dim\n+        )\n+        positional_embedding = self.positional_embedding_vlm.to(dtype=hidden_state.dtype, device=hidden_state.device)\n+        hidden_state = hidden_state + positional_embedding\n+\n+        hidden_state = self.layernorm_pre(hidden_state)\n+\n+        hidden_state = hidden_state.view(batch_size_times_num_tiles, -1, hidden_dim)\n+        freqs_ci = self.rotary_embedding(pixel_values)\n+\n+        output = self.model(\n+            hidden_state,\n+            attention_mask=None,\n+            output_hidden_states=output_hidden_states,\n+            output_attentions=output_attentions,\n+            freqs_ci=freqs_ci,\n+        )\n+\n+        hidden_state = output.last_hidden_state\n+\n+        hidden_state = self.layernorm_post(hidden_state)\n+\n+        hidden_state = hidden_state[:, :-1, :]\n+\n+        # now, we use Llama4VisionPixelShuffle + mlp to project embeddings\n+        hidden_state = self.vision_adapter(hidden_state)\n+\n+        hidden_states = output.hidden_states if output_hidden_states else None\n+\n+        if output_attentions:\n+            attentions = output[2]\n+        else:\n+            attentions = None\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_state, hidden_states, attentions] if v is not None)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_state,\n+            hidden_states=hidden_states,\n+            attentions=attentions,\n+        )\n+\n+\n+class Llama4ForConditionalGeneration(Llama4PreTrainedModel, GenerationMixin):\n+    _tp_plan = {}\n+    base_model_prefix = \"\"\n+    config_class = Llama4Config\n+    _supports_flex_attn = True\n+\n+    def __init__(self, config: Llama4Config):\n+        super().__init__(config)\n+        self.vision_model = Llama4VisionModel(config.vision_config)\n+\n+        self.multi_modal_projector = Llama4MultiModalProjector(config)\n+        self.language_model = Llama4ForCausalLM(config.text_config)\n+        self.vocab_size = config.text_config.vocab_size\n+        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.language_model.get_output_embeddings()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.language_model.set_output_embeddings(new_embeddings)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Union[int, List[int]],\n+        vision_feature_select_strategy: str,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply al projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n+            vision_feature_select_strategy (`str`):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        if vision_feature_select_strategy not in [\"default\", \"full\"]:\n+            raise ValueError(f\"Unexpected select feature strategy: {self.vision_feature_select_strategy}\")\n+        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n+        image_outputs = self.vision_model(pixel_values, output_hidden_states=False, **kwargs)\n+        hidden_state = image_outputs.last_hidden_state\n+        return hidden_state\n+\n+    @replace_return_docstrings(output_type=Llama4CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        image_sizes: torch.Tensor = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, Llama4CausalLMOutputWithPast]:\n+        r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, LlavaForConditionalGeneration\n+\n+        >>> model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n+        >>> processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n+\n+        >>> prompt = \"USER: <image>\\nWhat's the content of the image? ASSISTANT:\"\n+        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(**inputs, max_new_tokens=15)\n+        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"USER:  \\nWhat's the content of the image? ASSISTANT: The image features a busy city street with a stop sign prominently displayed\"\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer\n+            if vision_feature_layer is not None\n+            else self.config.vision_config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_config.vision_feature_select_strategy\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+                image_sizes=image_sizes,\n+            )\n+            original_inputs_embeds_shape = inputs_embeds.shape\n+\n+            vision_flat = image_features.view(-1, image_features.size(-1))\n+            projected_vision_flat = self.multi_modal_projector(vision_flat)\n+\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            final_mask = special_image_mask.to(inputs_embeds.device)\n+            inputs_embeds = inputs_embeds.view(-1, inputs_embeds.size(-1))\n+\n+            final_mask_1d = final_mask[..., 0].reshape(-1)\n+            num_tokens_to_fill = final_mask_1d.sum()\n+\n+            if num_tokens_to_fill != projected_vision_flat.size(0):\n+                raise ValueError(\n+                    f\"Mismatch: final_mask wants {num_tokens_to_fill} embeddings, \"\n+                    f\"but multi_modal_projector returned {projected_vision_flat.size(0)}\"\n+                )\n+\n+            expanded_mask = final_mask_1d.unsqueeze(-1).expand(-1, inputs_embeds.size(-1))\n+            inputs_embeds.masked_scatter_(expanded_mask, projected_vision_flat)\n+\n+            inputs_embeds = inputs_embeds.view(original_inputs_embeds_shape)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n+        )\n+\n+        logits = outputs[0]\n+\n+        loss = None\n+        if labels is not None:\n+            # Shift so that tokens < n predict n\n+            if attention_mask is not None:\n+                # we use the input attention mask to shift the logits and labels, because it is 2D.\n+                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n+                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n+                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n+                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n+            else:\n+                shift_logits = logits[..., :-1, :].contiguous()\n+                shift_labels = labels[..., 1:].contiguous()\n+            # Flatten the tokens\n+            loss_fct = nn.CrossEntropyLoss()\n+            loss = loss_fct(\n+                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n+            )\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return Llama4CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = self.language_model.prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n+        return model_inputs\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to place the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+\n+__all__ = [\n+    \"Llama4PreTrainedModel\",\n+    \"Llama4TextModel\",\n+    \"Llama4VisionModel\",\n+    \"Llama4ForCausalLM\",\n+    \"Llama4ForConditionalGeneration\",\n+]"
        },
        {
            "sha": "0ca4a44c5e9032430c98a65ababceefa824657e5",
            "filename": "src/transformers/models/llama4/processing_llama4.py",
            "status": "added",
            "additions": 275,
            "deletions": 0,
            "changes": 275,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -0,0 +1,275 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from typing import List, Optional, Union\n+\n+from transformers.processing_utils import (\n+    ImagesKwargs,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    Unpack,\n+)\n+from transformers.tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import (\n+    ImageInput,\n+    make_flat_list_of_images,\n+)\n+\n+\n+class Llama4ImagesKwargs(ImagesKwargs, total=False):\n+    max_patches: Optional[int]\n+    resize_to_max_canvas: Optional[bool]\n+\n+\n+class Llama4ProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: Llama4ImagesKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding_side\": \"left\",\n+        },\n+    }\n+\n+\n+chat_template = \"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\\\"%d %b %Y\\\") %}\\n    {%- else %}\\n        {%- set date_string = \\\"26 Jul 2024\\\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}    \\n    {%- if messages[0]['content'] is string %}\\n        {%- set system_message = messages[0]['content']|trim %}\\n    {%- else %}\\n        {#- FIXME: The processor requires an array, always. #}\\n        {%- set system_message = messages[0]['content'][0]['text']|trim %}\\n    {%- endif %}\\n    {%- set messages = messages[1:] %}\\n    {%- set user_supplied_system_message = true %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n    {%- set user_supplied_system_message = false %}\\n{%- endif %}\\n\\n{#- System message if the user supplied one #}\\n{%- if user_supplied_system_message %}\\n    {{- \\\"<|header_start|>system<|header_end|>\\n\\n\\\" }}\\n    {%- if tools is not none %}\\n        {{- \\\"Environment: ipython\\n\\\" }}\\n    {%- endif %}\\n    {%- if tools is not none and not tools_in_user_message %}\\n        {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n        {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n        {{- \\\"Do not use variables.\\n\\n\\\" }}\\n        {%- for t in tools %}\\n            {{- t | tojson(indent=4) }}\\n            {{- \\\"\\n\\n\\\" }}\\n        {%- endfor %}\\n    {%- endif %}\\n    {{- system_message }}\\n    {{- \\\"<|eot|>\\\" }}\\n{%- endif %}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|header_start|>user<|header_end|>\\n\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\n\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\n\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\n\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n    {{- '<|header_start|>' + message['role'] + '<|header_end|>\\n\\n' }}\\n        {%- if message['content'] is string %}\\n            {{- message['content'] }}\\n        {%- else %}\\n            {%- for content in message['content'] %}\\n                {%- if content['type'] == 'image' %}\\n                    {{- '<|image|>' }}\\n                {%- elif content['type'] == 'text' %}\\n                    {{- content['text'] }}\\n                {%- endif %}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\\"<|eot|>\\\" }}\\n    {%- elif 'tool_calls' in message and message.tool_calls|length > 0 %}\\n       {{- '<|header_start|>assistant<|header_end|>\\n\\n' -}}\\n       {{- '<|python_start|>' }}\\n        {%- if message['content'] is string %}\\n            {{- message['content'] }}\\n        {%- else %}\\n            {%- for content in message['content'] %}\\n                {%- if content['type'] == 'image' %}\\n                    {{- '<|image|>' }}\\n                {%- elif content['type'] == 'text' %}\\n                    {{- content['text'] }}\\n                {%- endif %}\\n            {%- endfor %}\\n        {%- endif %}\\n       {{- '<|python_end|>' }}\\n        {%- for tool_call in message.tool_calls %}\\n           {{- '{\\\"name\\\": \\\"' + tool_call.function.name + '\\\", ' }}\\n           {{- '\\\"parameters\\\": ' }}\\n           {{- tool_call.function.arguments | tojson }}\\n           {{- \\\"}\\\" }}\\n        {%- endfor %}\\n       {{- \\\"<|eot|>\\\" }}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|header_start|>ipython<|header_end|>\\n\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|header_start|>assistant<|header_end|>\\n\\n' }}\\n{%- endif %}\\n\"\n+\n+\n+class Llama4Processor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Llama4 processor which wraps a [`AutoImageProcessor`] and\n+    [`PretrainedTokenizerFast`] tokenizer into a single processor that inherits both the image processor and\n+    tokenizer functionalities. See the [`~Llama4Processor.__call__`] and [`~Llama4Processor.decode`] for more information.\n+    Args:\n+        image_processor ([`AutoImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`], *optional*):\n+            The tokenizer is a required input.\n+        patch_size (`int`, *optional*, defaults to 28):\n+            The size of image patches for tokenization.\n+        img_size (`int`, *optional*, defaults to 364):\n+            The size of the image to be tokenized. This should correspond to the size given to the image processor.\n+        image_token (`str`, *optional*, defaults to `\"<|image|>\"`):\n+            The token to be used to represent an image in the text.\n+        downsample_factor (`int`, *optional*, defaults to 1):\n+            The factor by which to scale the patch size.\n+        start_of_img_token (`str`, *optional*, defaults to `\"<|START_OF_IMG|>\"`):\n+            The token to be used to represent the start of an image in the text.\n+        end_of_img_token (`str`, *optional*, defaults to `\"<|END_OF_IMG|>\"`):\n+            The token to be used to represent the end of an image in the text.\n+        img_patch_token (`str`, *optional*, defaults to `\"<|IMG_PATCH|>\"`):\n+            The token to be used to represent an image patch in the text.\n+        img_line_break_token (`str`, *optional*, defaults to `\"<|IMG_LINE_BREAK|>\"`):\n+            The token to be used to represent a line break in the text.\n+        tile_token (`str`, *optional*, defaults to `\"TILE\"`):\n+            The token to be used to represent an image patch in the text.\n+        tile_global_token (`str`, *optional*, defaults to `\"TILE_GLOBAL\"`):\n+            The token to be used to represent the cover image in the text.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\n+        \"chat_template\",\n+        \"image_token\",\n+        \"patch_size\",\n+        \"img_size\",\n+        \"downsample_factor\",\n+        \"start_of_img_token\",\n+        \"end_of_img_token\",\n+        \"img_patch_token\",\n+        \"img_line_break_token\",\n+        \"tile_token\",\n+        \"tile_global_token\",\n+    ]\n+    image_processor_class = \"AutoImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer=None,\n+        patch_size: int = 14,\n+        pixel_shuffle_ratio: float = 0.5,\n+        fake_image_token=\"<|image|>\",\n+        image_token=\"<|image|>\",\n+        start_of_image_token=\"<|image_start|>\",\n+        end_of_image_token=\"<|image_end|>\",\n+        patch_token=\"<|patch|>\",\n+        tile_x_separator_token=\"<|tile_x_separator|>\",\n+        tile_y_separator_token=\"<|tile_y_separator|>\",\n+        chat_template=chat_template,\n+        **kwargs,\n+    ):\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+\n+        self.downsample_ratio = int(round(1.0 / (pixel_shuffle_ratio**2)))\n+        self.patch_size = patch_size\n+\n+        self.fake_image_token = fake_image_token\n+        self.image_token = image_token\n+        self.start_of_img_token = start_of_image_token\n+        self.end_of_img_token = end_of_image_token\n+        self.img_patch_token = patch_token\n+        self.tile_token = tile_x_separator_token\n+        self.tile_global_token = tile_y_separator_token\n+\n+    def _prompt_split_image(self, aspect_ratio, num_patches_per_chunk):\n+        \"\"\"\n+        Create a structured string representation of image tokens\n+\n+        Args:\n+           num_patches: Number of patches in the image\n+\n+        Returns:\n+            String with appropriate image tokens\n+        \"\"\"\n+        img_string = \"<|image_start|>\"\n+        ratio_h, ratio_w = aspect_ratio\n+        if ratio_h * ratio_w > 1:\n+            for yy in range(ratio_h):\n+                for xx in range(ratio_w):\n+                    img_string += \"<|patch|>\" * num_patches_per_chunk\n+                    if xx < ratio_w - 1:\n+                        img_string += \"<|tile_x_separator|>\"\n+\n+                img_string += \"<|tile_y_separator|>\"\n+        img_string += \"<|image|>\"\n+        img_string += \"<|patch|>\" * num_patches_per_chunk\n+        img_string += \"<|image_end|>\"\n+\n+        return img_string\n+\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[Llama4ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode the text.\n+        To prepare the vision inputs, this method forwards the `images` and `kwargs` arguments to\n+        Llama4ImageProcessor's [`~Llama4ImageProcessor.__call__`] if `images` is not `None`.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        if text is None:\n+            raise ValueError(\"You have to specify text.\")\n+\n+        output_kwargs = self._merge_kwargs(\n+            Llama4ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        if not isinstance(text, (list, tuple)):\n+            text = [text]\n+\n+        # Process images\n+        image_inputs = {}\n+        if images is not None:\n+            images = make_flat_list_of_images(images)\n+            image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+            image_height, image_width = image_inputs[\"pixel_values\"][0].shape[-2:]\n+            num_patches_per_chunk = int(\n+                (image_height // self.patch_size) * (image_width // self.patch_size) // self.downsample_ratio\n+            )\n+            aspect_ratios = image_inputs.pop(\"aspect_ratios\")\n+\n+            total_placeholders = sum(prompt.count(self.fake_image_token) for prompt in text)\n+            if total_placeholders != len(images):\n+                raise ValueError(\n+                    f\"Found {total_placeholders} placeholders across the batch, \"\n+                    f\"but have {len(images)} flattened images.\"\n+                )\n+\n+            image_index = 0\n+            processed_text = []\n+            for prompt in text:\n+                placeholder_count = prompt.count(self.fake_image_token)\n+                if placeholder_count == 0:\n+                    # do nothing if there is no image\n+                    processed_text.append(prompt)\n+                    continue\n+                prompt_splits = prompt.split(self.fake_image_token)\n+                new_prompt = []\n+                for local_image_index, split_part in enumerate(prompt_splits):\n+                    new_prompt.append(split_part)\n+                    if local_image_index < placeholder_count:\n+                        tokens_for_this_image = self._prompt_split_image(\n+                            aspect_ratios[image_index], num_patches_per_chunk\n+                        )\n+                        image_index += 1\n+                        new_prompt.append(tokens_for_this_image)\n+                processed_text.append(\"\".join(new_prompt))\n+\n+            if image_index != len(images):\n+                raise ValueError(\"Number of image placeholders in the prompt does not match the number of images.\")\n+\n+            text = processed_text\n+\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs})\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(tokenizer_input_names) + list(image_processor_input_names)\n+\n+\n+__all__ = [\"Llama4Processor\"]"
        },
        {
            "sha": "1df663a26e9f9647a8f27f931bdd35187b42ddee",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -981,6 +981,8 @@ def __init__(\n         else:\n             self.device = device if device is not None else -1\n \n+        if torch.distributed.is_initialized():\n+            self.device = self.model.device\n         logger.warning(f\"Device set to use {self.device}\")\n \n         self.binary_output = binary_output"
        },
        {
            "sha": "b1c40e7ff2d7c08e8b8e741a59f933f58c13fb30",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -1178,10 +1178,6 @@ def validate_init_kwargs(processor_config, valid_kwargs):\n         unused_kwargs = {}\n         unused_keys = set(kwargs_from_config) - set(valid_kwargs)\n         if unused_keys:\n-            unused_key_str = \", \".join(unused_keys)\n-            logger.warning(\n-                f\"Some kwargs in processor config are unused and will not have any effect: {unused_key_str}. \"\n-            )\n             unused_kwargs = {k: processor_config[k] for k in unused_keys}\n         return unused_kwargs\n "
        },
        {
            "sha": "ff3856c9d4573d9b44edeb9a359e8958bf31811b",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 110,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -43,8 +43,7 @@\n _torch_distributed_available = torch.distributed.is_available()\n \n if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n-    from torch.distributed.tensor import Replicate\n-    from torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel\n+    pass\n \n \n def softmax_backward_data(parent, grad_output, output, dim, self):\n@@ -335,29 +334,6 @@ def isin_mps_friendly(elements: torch.Tensor, test_elements: torch.Tensor | int)\n         return torch.isin(elements, test_elements)\n \n \n-# TODO need to add the __repr__ that shows that it is a colwise parallel\n-# See https://github.com/pytorch/pytorch/issues/145726\n-def translate_to_torch_parallel_style(style: str):\n-    \"\"\"\n-    In model configurations, we use a neutral type (string) to specify parallel\n-    styles, here we translate them into torch.distributed tensor-parallel\n-    types.\n-    \"\"\"\n-    if not isinstance(style, str):\n-        raise ValueError(f\"Unsupported parallel style type {type(style)}, expected str\")\n-\n-    if style == \"colwise\":\n-        return ColwiseParallel()\n-    elif style == \"rowwise\":\n-        return RowwiseParallel()\n-    elif style == \"colwise_rep\":\n-        return ColwiseParallel(output_layouts=Replicate())\n-    elif style == \"rowwise_rep\":\n-        return RowwiseParallel(input_layouts=Replicate())\n-    else:\n-        raise ValueError(f\"Unsupported parallel style value: {style}\")\n-\n-\n def compile_compatible_method_lru_cache(*lru_args, **lru_kwargs):\n     \"\"\"\n     LRU cache decorator from standard functools library, but with a workaround to disable\n@@ -382,88 +358,3 @@ def wrapper(self, *args, **kwargs):\n         return wrapper\n \n     return decorator\n-\n-\n-def distribute_module(\n-    module: nn.Module,\n-    device_mesh=None,\n-    partition_fn=None,\n-    input_fn=None,\n-    output_fn=None,\n-) -> nn.Module:\n-    \"\"\"\n-    This function expose three functions to control the parameters/inputs/outputs of the module:\n-\n-    1. To perform sharding on the module before runtime execution by specifying the\n-    ``partition_fn`` (i.e. allow user to convert Module parameters to :class:`DTensor`\n-    parameters according to the `partition_fn` specified).\n-    2. To control the inputs or outputs of the module during runtime execution by\n-    specifying the ``input_fn`` and ``output_fn``. (i.e. convert the input to\n-    :class:`DTensor`, convert the output back to ``torch.Tensor``)\n-\n-    Args:\n-        module (:class:`nn.Module`): user module to be partitioned.\n-        device_mesh (:class:`DeviceMesh`): the device mesh to place the module.\n-        partition_fn (Callable): the function to partition parameters (i.e. shard certain\n-            parameters across the ``device_mesh``). If ``partition_fn`` is not specified,\n-            by default we replicate all module parameters of ``module`` across the mesh.\n-        input_fn (Callable): specify the input distribution, i.e. could control how the\n-            input of the module is sharded. ``input_fn`` will be installed as a module\n-            ``forward_pre_hook`` (pre forward hook).\n-        output_fn (Callable): specify the output distribution, i.e. could control how the\n-            output is sharded, or convert it back to torch.Tensor. ``output_fn`` will be\n-            installed as a module ``forward_hook`` (post forward hook).\n-\n-    Returns:\n-        A module that contains parameters/buffers that are all ``DTensor`` s.\n-\n-    .. note::\n-        When initialize the DeviceMesh with the ``xla`` device_type, ``distribute_module``\n-        return nn.Module with PyTorch/XLA SPMD annotated parameters. See\n-        `this issue <https://github.com/pytorch/pytorch/issues/92909>`__\n-        for more details. The XLA integration is experimental and subject to change.\n-\n-    \"\"\"\n-\n-    torch._C._log_api_usage_once(\"torch.dtensor.distribute_module\")\n-\n-    device_mesh = device_mesh\n-\n-    # register input_fn as module forward pre hook\n-    if input_fn is not None:\n-        # check the input_fn signature\n-        num_args = len(inspect.signature(input_fn).parameters)\n-        if num_args == 2:\n-            # input_fn only takes in inputs and device mesh\n-            logger.warning(\n-                \"Deprecating input_fn that takes two arguments (inputs, device_mesh), \"\n-                \"please use input_fn that takes in (module, inputs, device_mesh) instead!\",\n-                FutureWarning,\n-                stacklevel=2,\n-            )\n-            module.register_forward_pre_hook(lambda _, inputs: input_fn(inputs, device_mesh))  # type: ignore[call-arg]\n-        elif num_args == 3:\n-            # input_fn takes in module, inputs, device mesh\n-            module.register_forward_pre_hook(lambda mod, inputs: input_fn(mod, inputs, device_mesh))\n-        else:\n-            raise ValueError(f\"input_fn should take in 3 arguments, but got {num_args} arguments!\")\n-    # register output_fn as module forward hook\n-    if output_fn is not None:\n-        num_args = len(inspect.signature(output_fn).parameters)\n-        if num_args == 2:\n-            # output_fn only takes in outputs and device mesh\n-            logger.warning(\n-                \"Deprecating output_fn that takes two arguments (inputs, device_mesh), \"\n-                \"please use output_fn that takes in (module, inputs, device_mesh) instead!\",\n-                FutureWarning,\n-                stacklevel=2,\n-            )\n-            module.register_forward_hook(\n-                lambda mod, inputs, outputs: output_fn(outputs, device_mesh)  # type: ignore[call-arg]\n-            )\n-        elif num_args == 3:\n-            module.register_forward_hook(lambda mod, inputs, outputs: output_fn(mod, outputs, device_mesh))\n-        else:\n-            raise ValueError(f\"output_fn should take in 3 arguments, but got {num_args} arguments!\")\n-\n-    return module"
        },
        {
            "sha": "a780dca7548f49ae3dac3fee57cec79748287187",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 51,
            "deletions": 1,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -15,14 +15,18 @@\n from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n \n from ..utils import is_torch_available\n-from ..utils.quantization_config import QuantizationConfigMixin\n+from ..utils.quantization_config import QuantizationConfigMixin, QuantizationMethod\n+from .quantizers_utils import get_module_from_name\n \n \n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n \n if is_torch_available():\n     import torch\n+    from torch.nn import ModuleList\n+else:\n+    ModuleList = str\n \n \n class HfQuantizer(ABC):\n@@ -198,6 +202,10 @@ def validate_environment(self, *args, **kwargs):\n         \"\"\"\n         return\n \n+    def update_tp_plan(self, config):\n+        \"updates the tp plan for the scales\"\n+        return config\n+\n     def preprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n         \"\"\"\n         Setting model attributes and/or converting model before weights loading. At this point\n@@ -212,6 +220,7 @@ def preprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n         \"\"\"\n         model.is_quantized = True\n         model.quantization_method = self.quantization_config.quant_method\n+        self._convert_model_for_quantization(model)\n         return self._process_model_before_weight_loading(model, **kwargs)\n \n     def postprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n@@ -288,3 +297,44 @@ def is_serializable(self, safe_serialization=None): ...\n     @property\n     @abstractmethod\n     def is_trainable(self): ...\n+\n+    def _convert_model_for_quantization(self, model):\n+        from accelerate import init_empty_weights\n+\n+        for name, module in model.named_modules():\n+            module_class_name = module.__class__.__name__\n+            if (\n+                module_class_name in MODULES_TO_PATCH_FOR_QUANTIZATION.keys()\n+                and self.quantization_config.quant_method == QuantizationMethod.COMPRESSED_TENSORS\n+            ):\n+                with init_empty_weights():\n+                    parent_module, name = get_module_from_name(model, name)\n+                    parent_module._modules[name] = MODULES_TO_PATCH_FOR_QUANTIZATION[module_class_name](\n+                        model.config.get_text_config()\n+                    )\n+\n+\n+class SequentialLlama4TextExperts(ModuleList):\n+    \"\"\"\n+    A module that implements a compressed version of a list of expert modules.\n+    This is specifically designed to work with Llama4TextExperts in MoE layers.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        from transformers.models.llama4.modeling_llama4 import Llama4TextMLP\n+\n+        super().__init__([Llama4TextMLP(config) for _ in range(config.num_local_experts)])\n+        self.num_experts = config.num_local_experts\n+\n+    def forward(\n+        self,\n+        hidden_states: \"torch.Tensor\",\n+    ) -> \"torch.Tensor\":\n+        hidden_states = hidden_states.reshape(self.num_experts, -1, hidden_states.shape[-1])\n+        routed_out = torch.zeros_like(hidden_states)\n+        for expert_idx in range(self.num_experts):\n+            routed_out[expert_idx] = self[expert_idx](hidden_states[expert_idx])\n+        return routed_out\n+\n+\n+MODULES_TO_PATCH_FOR_QUANTIZATION = {\"Llama4TextExperts\": SequentialLlama4TextExperts}"
        },
        {
            "sha": "4e45abf95340b51138983a3a7132ab2f98b295de",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -146,6 +146,19 @@ def _process_model_after_weight_loading(self, model, **kwargs):\n                 self.compressor.quantization_config.quantization_status = QuantizationStatus.FROZEN\n             self.compressor.decompress(model_path=cache_path, model=model)\n \n+    def update_tp_plan(self, config):\n+        additional_plan = {\n+            \"layers.*.feed_forward.experts.*.gate_proj.weight\": \"local_colwise\",\n+            \"layers.*.feed_forward.experts.*.gate_proj.weight_scale\": \"local_colwise\",\n+            \"layers.*.feed_forward.experts.*.up_proj.weight\": \"local_colwise\",\n+            \"layers.*.feed_forward.experts.*.up_proj.weight_scale\": \"local_colwise\",\n+            \"layers.*.feed_forward.experts.*.down_proj.weight\": \"local_rowwise\",\n+        }\n+        if config.get_text_config() is not None and config.get_text_config().base_model_tp_plan is not None:\n+            config.get_text_config().base_model_tp_plan.update(additional_plan)\n+\n+        return config\n+\n     @property\n     def is_trainable(self):\n         return True"
        },
        {
            "sha": "f0fa8f063bb452c2c49fc042a27b507e89ba5d9a",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 58,
            "deletions": 7,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -116,7 +116,7 @@ def check_quantized_param(\n         state_dict: Dict[str, Any],\n         **kwargs,\n     ):\n-        from ..integrations import FbgemmFp8Linear\n+        from ..integrations import FbgemmFp8Linear, FbgemmFp8Llama4TextExperts\n \n         module, tensor_name = get_module_from_name(model, param_name)\n \n@@ -129,6 +129,13 @@ def check_quantized_param(\n                 if tensor_name == \"weight_scale\":\n                     raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n                 return True\n+        if isinstance(module, FbgemmFp8Llama4TextExperts):\n+            if self.pre_quantized or tensor_name == \"bias\":\n+                return False\n+            else:\n+                if tensor_name == \"gate_up_proj_scale\" or tensor_name == \"down_proj_scale\":\n+                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n+                return True\n         return False\n \n     def create_quantized_param(\n@@ -143,12 +150,52 @@ def create_quantized_param(\n         \"\"\"\n         Quantizes weights into weight and weight_scale\n         \"\"\"\n-        new_value, weight_scale = torch.ops.fbgemm.quantize_fp8_per_row(param_value)\n+\n+        from ..integrations import FbgemmFp8Llama4TextExperts\n \n         module, tensor_name = get_module_from_name(model, param_name)\n-        module._buffers[tensor_name] = new_value.to(target_device)\n-        # to have the right output shape -> (out_features, 1)\n-        module._buffers[\"weight_scale\"] = weight_scale.view(weight_scale.shape[0], 1).to(target_device)\n+        if isinstance(module, FbgemmFp8Llama4TextExperts):\n+            if tensor_name == \"gate_up_proj\":\n+                # Process each expert separately\n+                # Transpose the second and third dimension\n+                transposed_param = param_value.transpose(1, 2)\n+\n+                # Reshape to 2D for quantization\n+                original_shape = transposed_param.shape\n+                flattened_param = transposed_param.reshape(-1, original_shape[-1])\n+\n+                # Quantize using per row instead of per column\n+                new_value_flat, weight_scale_flat = torch.ops.fbgemm.quantize_fp8_per_row(flattened_param)\n+\n+                # Reshape back to original dimensions\n+                new_value = new_value_flat.reshape(original_shape)\n+                new_value = new_value.transpose(1, 2)\n+                weight_scale = weight_scale_flat.reshape(original_shape[0], 1, original_shape[1])\n+            elif tensor_name == \"down_proj\":\n+                # Process each expert separately\n+                # Transpose the weights for proper quantization\n+                transposed_param = param_value.transpose(1, 2)\n+\n+                # Reshape to 2D for quantization\n+                original_shape = transposed_param.shape\n+                flattened_param = transposed_param.reshape(-1, original_shape[-1])\n+\n+                # Quantize using per column\n+                new_value_flat, weight_scale_flat = torch.ops.fbgemm.quantize_fp8_per_row(flattened_param)\n+\n+                # Reshape back to original dimensions\n+                new_value = new_value_flat.reshape(original_shape)\n+                new_value = new_value.transpose(1, 2)\n+                weight_scale = weight_scale_flat.reshape(original_shape[0], original_shape[1], 1)\n+\n+            module._parameters[f\"{tensor_name}_scale\"] = torch.nn.Parameter(weight_scale.to(target_device))\n+        else:\n+            new_value, weight_scale = torch.ops.fbgemm.quantize_fp8_per_row(param_value)\n+            module._parameters[f\"{tensor_name}_scale\"] = torch.nn.Parameter(\n+                weight_scale.view(weight_scale.shape[0], 1).to(target_device)\n+            )\n+\n+        module._parameters[tensor_name] = torch.nn.Parameter(new_value.to(target_device))\n \n         if unexpected_keys is not None and param_name in unexpected_keys:\n             unexpected_keys.remove(param_name)\n@@ -165,25 +212,29 @@ def _process_model_before_weight_loading(\n     ):\n         from ..integrations import replace_with_fbgemm_fp8_linear\n \n+        tp_plan = model._tp_plan\n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n             model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n         )\n \n+        config = model.config\n         model = replace_with_fbgemm_fp8_linear(\n             model,\n             modules_to_not_convert=self.modules_to_not_convert,\n             quantization_config=self.quantization_config,\n             pre_quantized=self.pre_quantized,\n+            config=config,\n+            tp_plan=tp_plan,\n         )\n \n         model.config.quantization_config = self.quantization_config\n \n     def update_missing_keys(self, model, missing_keys: List[str], prefix: str) -> List[str]:\n-        from ..integrations import FbgemmFp8Linear\n+        from ..integrations import FbgemmFp8Linear, FbgemmFp8Llama4TextExperts\n \n         not_missing_keys = []\n         for name, module in model.named_modules():\n-            if isinstance(module, FbgemmFp8Linear):\n+            if isinstance(module, FbgemmFp8Linear) or isinstance(module, FbgemmFp8Llama4TextExperts):\n                 for missing in missing_keys:\n                     if (\n                         (name in missing or name in f\"{prefix}.{missing}\")"
        },
        {
            "sha": "8e047569e7ce56573949b4e03aa9f1057802bf90",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -3950,7 +3950,7 @@ def _eventual_warn_about_too_long_sequence(self, ids: List[int], max_length: Opt\n             verbose (`bool`): Whether or not to print more information and warnings.\n \n         \"\"\"\n-        if max_length is None and len(ids) > self.model_max_length and verbose:\n+        if max_length is None and len(ids) > self.model_max_length and verbose and self.model_max_length != 0:\n             if not self.deprecation_warnings.get(\"sequence-length-is-longer-than-the-specified-maximum\", False):\n                 logger.warning(\n                     \"Token indices sequence length is longer than the specified maximum sequence length \""
        },
        {
            "sha": "b03455c89e6ec30a8a6efc2fa883436559196f8f",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -5823,6 +5823,41 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class Llama4ForCausalLM(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Llama4ForConditionalGeneration(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Llama4PreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Llama4TextModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Llama4VisionModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class LlavaForConditionalGeneration(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "c59c9b4bdd55f77c8215b788f6626ea5b5073563",
            "filename": "src/transformers/utils/dummy_torchvision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -72,6 +72,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torchvision\"])\n \n \n+class Llama4ImageProcessorFast(metaclass=DummyObject):\n+    _backends = [\"torchvision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torchvision\"])\n+\n+\n class LlavaImageProcessorFast(metaclass=DummyObject):\n     _backends = [\"torchvision\"]\n "
        },
        {
            "sha": "4e7293d50ae99e8343bee3dbc2e4ddc301198b57",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -408,6 +408,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class Llama4ImageProcessor(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class LlavaImageProcessor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/llama4/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/tests%2Fmodels%2Fllama4%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/tests%2Fmodels%2Fllama4%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama4%2F__init__.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89"
        },
        {
            "sha": "bf84b3550db7a979e158608fcea863d792466c1b",
            "filename": "tests/models/llama4/test_image_processing_llama4.py",
            "status": "added",
            "additions": 128,
            "deletions": 0,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/tests%2Fmodels%2Fllama4%2Ftest_image_processing_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/tests%2Fmodels%2Fllama4%2Ftest_image_processing_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama4%2Ftest_image_processing_llama4.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -0,0 +1,128 @@\n+# coding=utf-8\n+# Copyright 2022 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    pass\n+\n+if is_vision_available() and is_torchvision_available():\n+    from transformers import Llama4ImageProcessorFast\n+\n+\n+class Llama4ImageProcessingTester(unittest.TestCase):\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        max_patches=1,\n+        do_resize=True,\n+        size=None,\n+        do_normalize=True,\n+        do_pad=False,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+        do_convert_rgb=True,\n+    ):\n+        super().__init__()\n+        size = size if size is not None else {\"height\": 20, \"width\": 20}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.max_patches = max_patches\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_pad = do_pad\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"max_patches\": self.max_patches,\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+            \"do_pad\": self.do_pad,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class Llama4ImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    test_slow_image_processor = False\n+    fast_image_processing_class = Llama4ImageProcessorFast if is_torchvision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = Llama4ImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processor, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processor, \"size\"))\n+            self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processor, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processor, \"image_std\"))\n+            self.assertTrue(hasattr(image_processor, \"do_convert_rgb\"))\n+\n+    def test_split_tiles(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)[0]\n+            processed_images = image_processor(\n+                image,\n+                max_patches=16,\n+            )\n+            self.assertEqual(len(processed_images.pixel_values), 1)\n+            self.assertEqual(processed_images.pixel_values[0].shape[0], 17)\n+            self.assertEqual(processed_images.pixel_values[0].shape[-2:], (20, 20))"
        },
        {
            "sha": "65672993a09f2a24ed378f2322538ce952eccbcc",
            "filename": "tests/models/llama4/test_modeling_llama4.py",
            "status": "added",
            "additions": 121,
            "deletions": 0,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -0,0 +1,121 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Llama4 model.\"\"\"\n+\n+import unittest\n+\n+from transformers import is_torch_available\n+from transformers.testing_utils import (\n+    require_read_token,\n+    require_torch_large_gpu,\n+    slow,\n+    torch_device,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        Llama4ForConditionalGeneration,\n+        Llama4Processor,\n+    )\n+\n+\n+@slow\n+@require_torch_large_gpu\n+@require_read_token\n+class Llama4IntegrationTest(unittest.TestCase):\n+    model_id = \"ll-re/Llama-4-17B-Omni-Instruct\"\n+    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n+    # Depending on the hardware we get different logits / generations\n+    cuda_compute_capability_major_version = None\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        if is_torch_available() and torch.cuda.is_available():\n+            # 8 is for A100 / A10 and 7 for T4\n+            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n+        cls.model = Llama4ForConditionalGeneration.from_pretrained(\n+            \"ll-re/Llama-4-17B-Omni-Instruct\", device_map=\"auto\", torch_dtype=torch.float32\n+        )\n+\n+    def setUp(self):\n+        self.processor = Llama4Processor.from_pretrained(\"ll-re/Llama-4-17B-Omni-Instruct\", padding_side=\"left\")\n+\n+        url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\"\n+        self.messages = [\n+            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": url},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+\n+    def test_model_17b_16e_fp16(self):\n+        EXPECTED_TEXT = [\n+            \"The capital of France is Paris, which is located in the north-central part of the country. Paris is known for its iconic landmarks such as the\",\n+            \"Roses are red, violets are blue, and this poem is about you. Roses are red, violets are blue, and I love\",\n+        ]\n+\n+        messages = [\n+            {\"role\": \"user\", \"content\": \"Who are you?\"},\n+        ]\n+        inputs = self.processor.apply_chat_template(\n+            messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True\n+        ).to(torch_device)\n+\n+        output = self.model.generate(**inputs, max_new_tokens=100)\n+        output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+\n+        print(output_text)\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n+\n+    def test_model_17b_16e_batch(self):\n+        messages_2 = [\n+            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\",\n+                    },\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"Are these images identical?\"},\n+                ],\n+            },\n+        ]\n+\n+        inputs = self.processor.apply_chat_template(\n+            [self.messages, messages_2],\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+            add_generation_prompt=True,\n+        ).to(torch_device)\n+\n+        output = self.model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+\n+        EXPECTED_TEXTS = [\n+            'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear turquoise water and a blue sky in the background. It looks like',\n+            \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n*   **Image 1:** Shows a cow\"\n+        ]  # fmt: skip\n+        self.assertEqual(output_text, EXPECTED_TEXTS)"
        },
        {
            "sha": "4ec01fa49703f6f16320292debe91ad2ac40ff50",
            "filename": "tests/models/llama4/test_processor_llama4.py",
            "status": "added",
            "additions": 65,
            "deletions": 0,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/tests%2Fmodels%2Fllama4%2Ftest_processor_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/tests%2Fmodels%2Fllama4%2Ftest_processor_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama4%2Ftest_processor_llama4.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -0,0 +1,65 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import shutil\n+import tempfile\n+import unittest\n+from typing import Optional\n+\n+from transformers import AutoProcessor, Llama4Processor, PreTrainedTokenizerFast\n+from transformers.testing_utils import require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import Llama4ImageProcessorFast\n+\n+\n+@require_vision\n+class Llama4ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Llama4Processor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+\n+        image_processor = Llama4ImageProcessorFast(max_patches=1, size={\"height\": 20, \"width\": 20})\n+        tokenizer = PreTrainedTokenizerFast.from_pretrained(\"unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit\")\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = Llama4Processor(image_processor, tokenizer, **processor_kwargs)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    # Override as Llama4ProcessorProcessor needs image tokens in prompts\n+    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n+        if batch_size is None:\n+            return \"lower newer <image>\"\n+\n+        if batch_size < 1:\n+            raise ValueError(\"batch_size must be greater than 0\")\n+\n+        if batch_size == 1:\n+            return [\"lower newer <image>\"]\n+        return [\"lower newer <image>\", \"<image> upper older longer string\"] + [\"<image> lower newer\"] * (\n+            batch_size - 2\n+        )"
        },
        {
            "sha": "76fc2cc428f859c7540e5e083c94e89d74db2499",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -236,6 +236,16 @@\n         \"text_config\",\n         \"vision_config\",\n     ],\n+    \"Llama4Config\": [\"boi_token_index\", \"eoi_token_index\"],\n+    \"Llama4TextConfig\": [\n+        \"interleave_moe_layer_step\",\n+        \"no_rope_layer_interval\",\n+        \"no_rope_layers\",\n+        \"output_router_logits\",\n+        \"router_aux_loss_coef\",\n+        \"router_jitter_noise\",\n+    ],\n+    \"Llama4VisionConfig\": [\"multi_modal_projector_bias\", \"norm_eps\"],\n }\n \n \n@@ -358,6 +368,8 @@ def check_attribute_being_used(config_class, attributes, default_value, source_s\n         \"rope_theta\",\n         \"partial_rotary_factor\",\n         \"pretraining_tp\",\n+        \"boi_token_index\",\n+        \"eoi_token_index\",\n     ]\n     attributes_used_in_generation = [\"encoder_no_repeat_ngram_size\"]\n "
        },
        {
            "sha": "b9f6645638f5587ae88cafd9166726535d002d5b",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -67,6 +67,7 @@\n # docstrings instead. If formatting should be ignored for the docstring, you can put a comment # no-format on the\n # line before the docstring.\n OBJECTS_TO_IGNORE = [\n+    \"Llama4Processor\",\n     # Deprecated\n     \"InputExample\",\n     \"InputFeatures\","
        },
        {
            "sha": "48a6b2fa71858e90488ff2623ab8cecaec923c55",
            "filename": "utils/check_dummies.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/utils%2Fcheck_dummies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/utils%2Fcheck_dummies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_dummies.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -223,13 +223,20 @@ def check_dummies(overwrite: bool = False):\n                     f.write(dummy_files[backend])\n             else:\n                 # Temporary fix to help people identify which objects introduced are not correctly protected.\n+                found = False\n                 for _actual, _dummy in zip(\n                     actual_dummies[\"torch\"].split(\"class\"), dummy_files[\"torch\"].split(\"class\")\n                 ):\n                     if _actual != _dummy:\n                         actual_broken = _actual\n                         dummy_broken = _dummy\n+                        found = True\n                         break\n+\n+                if not found:\n+                    print(\"A transient error was found with the dummies, please investigate.\")\n+                    continue\n+\n                 raise ValueError(\n                     \"The main __init__ has objects that are not present in \"\n                     f\"transformers.utils.dummy_{short_names.get(backend, backend)}_objects.py.\\n\""
        },
        {
            "sha": "42beda83e6a7bbf9021e41d7084397da91e9b941",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/25b7f272347a93d6fb73cad126f6f6dc88e8ce89/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=25b7f272347a93d6fb73cad126f6f6dc88e8ce89",
            "patch": "@@ -144,6 +144,8 @@\n         \"Qwen2_5_VLModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5_VLForConditionalGeneration.\n         \"MllamaTextModel\",  # Building part of bigger (tested) model. # TODO: add tests\n         \"MllamaVisionModel\",  # Building part of bigger (tested) model. # TODO: add tests\n+        \"Llama4TextModel\",  # Building part of bigger (tested) model. # TODO: add tests\n+        \"Llama4VisionModel\",  # Building part of bigger (tested) model. # TODO: add tests\n         \"Emu3VQVAE\",  # Building part of bigger (tested) model\n         \"Emu3TextModel\",  # Building part of bigger (tested) model\n     ]\n@@ -170,6 +172,7 @@\n     \"models/decision_transformer/test_modeling_decision_transformer.py\",\n     \"models/bark/test_modeling_bark.py\",\n     \"models/shieldgemma2/test_modeling_shieldgemma2.py\",\n+    \"models/llama4/test_modeling_llama4.py\",\n ]\n \n # Update this list for models that are not in any of the auto MODEL_XXX_MAPPING. Being in this list is an exception and"
        }
    ],
    "stats": {
        "total": 5749,
        "additions": 5527,
        "deletions": 222
    }
}