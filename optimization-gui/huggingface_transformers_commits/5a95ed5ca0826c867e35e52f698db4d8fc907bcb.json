{
    "author": "Cyrilvallez",
    "message": "ðŸš¨ðŸš¨ Fix initialization of Mask2Former (#38864)\n\n* Correctly fix init\n\nCo-authored-by: BUI Van Tuan <buivantuan07@gmail.com>\n\n* add back the block, breaking BC but this is correct author's code\n\n* override the test for params needing it\n\n---------\n\nCo-authored-by: BUI Van Tuan <buivantuan07@gmail.com>",
    "sha": "5a95ed5ca0826c867e35e52f698db4d8fc907bcb",
    "files": [
        {
            "sha": "db942a3ac6eed38e6a56742e6a0aec5df9a8f7c8",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 5,
            "deletions": 15,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/5a95ed5ca0826c867e35e52f698db4d8fc907bcb/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5a95ed5ca0826c867e35e52f698db4d8fc907bcb/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=5a95ed5ca0826c867e35e52f698db4d8fc907bcb",
            "patch": "@@ -2127,30 +2127,20 @@ def _init_weights(self, module: nn.Module):\n             for p in module.parameters():\n                 if p.dim() > 1:\n                     nn.init.xavier_uniform_(p, gain=xavier_std)\n-\n-        elif isinstance(module, Mask2FormerPixelLevelModule):\n-            for submodule in module.modules():\n-                if isinstance(submodule, (nn.Conv2d, nn.Linear)):\n-                    submodule.weight.data.normal_(mean=0.0, std=std)\n-                    if submodule.bias is not None:\n-                        submodule.bias.data.zero_()\n+            module.cross_attn.in_proj_bias.data.zero_()\n \n         elif isinstance(module, Mask2FormerPixelDecoder):\n-            for p in module.parameters():\n-                if p.dim() > 1:\n-                    nn.init.xavier_uniform_(p)\n             nn.init.normal_(module.level_embed, std=0)\n \n-        elif isinstance(module, Mask2FormerPixelDecoderEncoderOnly):\n-            for p in module.parameters():\n-                if p.dim() > 1:\n-                    nn.init.xavier_uniform_(p)\n-\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n \n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:"
        },
        {
            "sha": "29588f9fe07cd5dd50b67ce928c085254ff8533e",
            "filename": "src/transformers/utils/backbone_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/5a95ed5ca0826c867e35e52f698db4d8fc907bcb/src%2Ftransformers%2Futils%2Fbackbone_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5a95ed5ca0826c867e35e52f698db4d8fc907bcb/src%2Ftransformers%2Futils%2Fbackbone_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fbackbone_utils.py?ref=5a95ed5ca0826c867e35e52f698db4d8fc907bcb",
            "patch": "@@ -324,12 +324,7 @@ def load_backbone(config):\n         raise ValueError(\"Cannot specify both config.backbone_config and config.backbone\")\n \n     # If any of thhe following are set, then the config passed in is from a model which contains a backbone.\n-    if (\n-        backbone_config is None\n-        and use_timm_backbone is None\n-        and backbone_checkpoint is None\n-        and backbone_checkpoint is None\n-    ):\n+    if backbone_config is None and use_timm_backbone is None and backbone_checkpoint is None:\n         return AutoBackbone.from_config(config=config, **backbone_kwargs)\n \n     # config from the parent model that has a backbone"
        },
        {
            "sha": "7052b74957d70f10da3a353adacd8d6496e1fd51",
            "filename": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5a95ed5ca0826c867e35e52f698db4d8fc907bcb/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5a95ed5ca0826c867e35e52f698db4d8fc907bcb/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py?ref=5a95ed5ca0826c867e35e52f698db4d8fc907bcb",
            "patch": "@@ -590,15 +590,14 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    if param.requires_grad:\n-                        if (\n-                            \"level_embed\" in name\n-                            or \"sampling_offsets.bias\" in name\n-                            or \"value_proj\" in name\n-                            or \"output_proj\" in name\n-                            or \"reference_points\" in name\n-                        ):\n-                            continue\n+                    if (\n+                        \"level_embed\" in name\n+                        or \"sampling_offsets.bias\" in name\n+                        or \"value_proj\" in name\n+                        or \"output_proj\" in name\n+                        or \"reference_points\" in name\n+                    ):\n+                        continue\n                     self.assertIn(\n                         ((param.data.mean() * 1e9).round() / 1e9).item(),\n                         [0.0, 1.0],"
        },
        {
            "sha": "5762a1f6ffc423da8efc47a195d6bf633340ad6f",
            "filename": "tests/models/mask2former/test_modeling_mask2former.py",
            "status": "modified",
            "additions": 54,
            "deletions": 2,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/5a95ed5ca0826c867e35e52f698db4d8fc907bcb/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5a95ed5ca0826c867e35e52f698db4d8fc907bcb/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py?ref=5a95ed5ca0826c867e35e52f698db4d8fc907bcb",
            "patch": "@@ -18,7 +18,7 @@\n import numpy as np\n \n from tests.test_modeling_common import floats_tensor\n-from transformers import Mask2FormerConfig, is_torch_available, is_vision_available\n+from transformers import AutoModelForImageClassification, Mask2FormerConfig, is_torch_available, is_vision_available\n from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4\n from transformers.testing_utils import (\n     require_timm,\n@@ -33,7 +33,7 @@\n from transformers.utils import cached_property\n \n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -350,6 +350,58 @@ def test_backbone_selection(self):\n             elif model.__class__.__name__ == \"Mask2FormerForUniversalSegmentation\":\n                 self.assertEqual(model.model.pixel_level_module.encoder.out_indices, [1, 2, 3])\n \n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    if (\n+                        \"self_attn.sampling_offsets.bias\" in name\n+                        or \"self_attn.value_proj.weight\" in name\n+                        or \"self_attn.output_proj.weight\" in name\n+                    ):\n+                        continue\n+                    self.assertIn(\n+                        ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    def test_initialization_pretrained_backbone(self):\n+        backbone_name = \"microsoft/resnet-18\"\n+\n+        # load Mask2Former config with a pretrained backbone\n+        config = Mask2FormerConfig(\n+            backbone=backbone_name,\n+            use_pretrained_backbone=True,\n+        )\n+\n+        # load pretrained backbone\n+        backbone_model = AutoModelForImageClassification.from_pretrained(backbone_name, device_map=torch_device)\n+\n+        def params_match(params1, params2):\n+            return all((p1 == p2).all() for p1, p2 in zip(params1, params2))\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device).eval()\n+            if model.__class__.__name__ == \"Mask2FormerModel\":\n+                self.assertTrue(\n+                    params_match(\n+                        backbone_model.base_model.encoder.parameters(),\n+                        model.pixel_level_module.encoder.encoder.parameters(),\n+                    )\n+                )\n+            elif model.__class__.__name__ == \"Mask2FormerForUniversalSegmentation\":\n+                self.assertTrue(\n+                    params_match(\n+                        backbone_model.base_model.encoder.parameters(),\n+                        model.model.pixel_level_module.encoder.encoder.parameters(),\n+                    )\n+                )\n+\n \n TOLERANCE = 1e-4\n "
        }
    ],
    "stats": {
        "total": 100,
        "additions": 68,
        "deletions": 32
    }
}