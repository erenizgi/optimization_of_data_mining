{
    "author": "LawJarp-A",
    "message": "Add EfficientLoFTRImageProcessorFast for GPU-accelerated image processing (#40215)\n\n* Add EfficientLoFTRImageProcessorFast for GPU-accelerated image processing\n\n* Fix fast processor output format and add comprehensive tests\n\n* Fix trailing whitespace in test file\n\n* Apply ruff formatting to test file\n\n* simplify pair validation logic\n\n* add superglue tests to fast image processor\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "a871f6f58d49f3a05ae9dae519caa8aa9d919a07",
    "files": [
        {
            "sha": "2994ae83262dc9ac7d38b6699af4080165e008ec",
            "filename": "docs/source/en/model_doc/efficientloftr.md",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/a871f6f58d49f3a05ae9dae519caa8aa9d919a07/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a871f6f58d49f3a05ae9dae519caa8aa9d919a07/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md?ref=a871f6f58d49f3a05ae9dae519caa8aa9d919a07",
            "patch": "@@ -148,6 +148,14 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n - post_process_keypoint_matching\n - visualize_keypoint_matching\n \n+## EfficientLoFTRImageProcessorFast\n+\n+[[autodoc]] EfficientLoFTRImageProcessorFast\n+\n+- preprocess\n+- post_process_keypoint_matching\n+- visualize_keypoint_matching\n+\n <frameworkcontent>\n <pt>\n ## EfficientLoFTRModel"
        },
        {
            "sha": "7d07ca6dc7d647645ded9956dffd4d82d71990d0",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a871f6f58d49f3a05ae9dae519caa8aa9d919a07/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a871f6f58d49f3a05ae9dae519caa8aa9d919a07/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=a871f6f58d49f3a05ae9dae519caa8aa9d919a07",
            "patch": "@@ -92,7 +92,7 @@\n             (\"donut-swin\", (\"DonutImageProcessor\", \"DonutImageProcessorFast\")),\n             (\"dpt\", (\"DPTImageProcessor\", \"DPTImageProcessorFast\")),\n             (\"efficientformer\", (\"EfficientFormerImageProcessor\", None)),\n-            (\"efficientloftr\", (\"EfficientLoFTRImageProcessor\", None)),\n+            (\"efficientloftr\", (\"EfficientLoFTRImageProcessor\", \"EfficientLoFTRImageProcessorFast\")),\n             (\"efficientnet\", (\"EfficientNetImageProcessor\", \"EfficientNetImageProcessorFast\")),\n             (\"eomt\", (\"EomtImageProcessor\", \"EomtImageProcessorFast\")),\n             (\"flava\", (\"FlavaImageProcessor\", \"FlavaImageProcessorFast\")),"
        },
        {
            "sha": "53e51c91c17b4ff51c672f369e43deb35faa0fea",
            "filename": "src/transformers/models/efficientloftr/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a871f6f58d49f3a05ae9dae519caa8aa9d919a07/src%2Ftransformers%2Fmodels%2Fefficientloftr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a871f6f58d49f3a05ae9dae519caa8aa9d919a07/src%2Ftransformers%2Fmodels%2Fefficientloftr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2F__init__.py?ref=a871f6f58d49f3a05ae9dae519caa8aa9d919a07",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_efficientloftr import *\n     from .image_processing_efficientloftr import *\n+    from .image_processing_efficientloftr_fast import *\n     from .modeling_efficientloftr import *\n else:\n     import sys"
        },
        {
            "sha": "5eb6e6589058e43e2de992d62ff123adeb88139d",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr_fast.py",
            "status": "added",
            "additions": 326,
            "deletions": 0,
            "changes": 326,
            "blob_url": "https://github.com/huggingface/transformers/blob/a871f6f58d49f3a05ae9dae519caa8aa9d919a07/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a871f6f58d49f3a05ae9dae519caa8aa9d919a07/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py?ref=a871f6f58d49f3a05ae9dae519caa8aa9d919a07",
            "patch": "@@ -0,0 +1,326 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for EfficientLoFTR.\"\"\"\n+\n+from typing import TYPE_CHECKING, Optional, Union\n+\n+import torch\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    SizeDict,\n+    get_image_type,\n+    is_pil_image,\n+    is_valid_image,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    is_vision_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if TYPE_CHECKING:\n+    from .modeling_efficientloftr import KeypointMatchingOutput\n+\n+if is_torchvision_v2_available():\n+    import torchvision.transforms.v2.functional as F\n+elif is_torchvision_available():\n+    import torchvision.transforms.functional as F\n+\n+if is_vision_available():\n+    from PIL import Image, ImageDraw\n+\n+\n+def _is_valid_image(image):\n+    return is_pil_image(image) or (\n+        is_valid_image(image) and get_image_type(image) != ImageType.PIL and len(image.shape) == 3\n+    )\n+\n+\n+def flatten_pair_images(images):\n+    # Handle the pair validation and flattening similar to slow processor\n+    if isinstance(images, list):\n+        if len(images) == 2 and all((_is_valid_image(image) or isinstance(image, torch.Tensor)) for image in images):\n+            # Single pair of images - keep as is, they'll be processed by the base class\n+            return images\n+        elif all(\n+            isinstance(image_pair, list)\n+            and len(image_pair) == 2\n+            and all(_is_valid_image(image) or isinstance(image, torch.Tensor) for image in image_pair)\n+            for image_pair in images\n+        ):\n+            # Multiple pairs - flatten them\n+            images = [image for image_pair in images for image in image_pair]\n+            return images\n+    raise ValueError(\n+        \"Input images must be a one of the following :\",\n+        \" - A pair of PIL images.\",\n+        \" - A pair of 3D arrays.\",\n+        \" - A list of pairs of PIL images.\",\n+        \" - A list of pairs of 3D arrays.\",\n+    )\n+\n+\n+def is_grayscale(\n+    image: \"torch.Tensor\",\n+):\n+    \"\"\"Checks if an image is grayscale (all RGB channels are identical).\"\"\"\n+    if image.ndim < 3 or image.shape[0 if image.ndim == 3 else 1] == 1:\n+        return True\n+    return torch.all(image[..., 0, :, :] == image[..., 1, :, :]) and torch.all(\n+        image[..., 1, :, :] == image[..., 2, :, :]\n+    )\n+\n+\n+def convert_to_grayscale(\n+    image: \"torch.Tensor\",\n+) -> \"torch.Tensor\":\n+    \"\"\"\n+    Converts an image to grayscale format using the NTSC formula. Only support torch.Tensor.\n+\n+    This function is supposed to return a 1-channel image, but it returns a 3-channel image with the same value in each\n+    channel, because of an issue that is discussed in :\n+    https://github.com/huggingface/transformers/pull/25786#issuecomment-1730176446\n+\n+    Args:\n+        image (torch.Tensor):\n+            The image to convert.\n+    \"\"\"\n+    if is_grayscale(image):\n+        return image\n+    return F.rgb_to_grayscale(image, num_output_channels=3)\n+\n+\n+class EfficientLoFTRFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    do_grayscale (`bool`, *optional*, defaults to `True`):\n+        Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n+    \"\"\"\n+\n+    do_grayscale: Optional[bool] = True\n+\n+\n+@auto_docstring\n+class EfficientLoFTRImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    size = {\"height\": 480, \"width\": 640}\n+    default_to_square = False\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = None\n+    valid_kwargs = EfficientLoFTRFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[EfficientLoFTRFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[EfficientLoFTRFastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+        **kwargs,\n+    ) -> ImageInput:\n+        # we need to handle image pairs validation and flattening\n+        return flatten_pair_images(images)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        size: Union[dict[str, int], SizeDict],\n+        rescale_factor: float,\n+        do_rescale: bool,\n+        do_resize: bool,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_grayscale: bool,\n+        disable_grouping: bool,\n+        return_tensors: Union[str, TensorType],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size=size, interpolation=interpolation)\n+            processed_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_rescale:\n+                stacked_images = self.rescale(stacked_images, rescale_factor)\n+            if do_grayscale:\n+                stacked_images = convert_to_grayscale(stacked_images)\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # Convert back to pairs format\n+        image_pairs = [processed_images[i : i + 2] for i in range(0, len(processed_images), 2)]\n+\n+        # Stack each pair into a single tensor to match slow processor format\n+        stacked_pairs = [torch.stack(pair, dim=0) for pair in image_pairs]\n+\n+        # Return in same format as slow processor\n+        image_pairs = torch.stack(stacked_pairs, dim=0) if return_tensors else stacked_pairs\n+\n+        return BatchFeature(data={\"pixel_values\": image_pairs})\n+\n+    def post_process_keypoint_matching(\n+        self,\n+        outputs: \"KeypointMatchingOutput\",\n+        target_sizes: Union[TensorType, list[tuple]],\n+        threshold: float = 0.0,\n+    ) -> list[dict[str, torch.Tensor]]:\n+        \"\"\"\n+        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        with coordinates absolute to the original image sizes.\n+        Args:\n+            outputs ([`KeypointMatchingOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n+                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the\n+                target size `(height, width)` of each image in the batch. This must be the original image size (before\n+                any processing).\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold to filter out the matches with low scores.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n+            of the pair, the matching scores and the matching indices.\n+        \"\"\"\n+        if outputs.matches.shape[0] != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n+        if not all(len(target_size) == 2 for target_size in target_sizes):\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        if isinstance(target_sizes, list):\n+            image_pair_sizes = torch.tensor(target_sizes, device=outputs.matches.device)\n+        else:\n+            if target_sizes.shape[1] != 2 or target_sizes.shape[2] != 2:\n+                raise ValueError(\n+                    \"Each element of target_sizes must contain the size (h, w) of each image of the batch\"\n+                )\n+            image_pair_sizes = target_sizes\n+\n+        keypoints = outputs.keypoints.clone()\n+        keypoints = keypoints * image_pair_sizes.flip(-1).reshape(-1, 2, 1, 2)\n+        keypoints = keypoints.to(torch.int32)\n+\n+        results = []\n+        for keypoints_pair, matches, scores in zip(keypoints, outputs.matches, outputs.matching_scores):\n+            # Filter out matches with low scores\n+            valid_matches = torch.logical_and(scores > threshold, matches > -1)\n+\n+            matched_keypoints0 = keypoints_pair[0][valid_matches[0]]\n+            matched_keypoints1 = keypoints_pair[1][valid_matches[1]]\n+            matching_scores = scores[0][valid_matches[0]]\n+\n+            results.append(\n+                {\n+                    \"keypoints0\": matched_keypoints0,\n+                    \"keypoints1\": matched_keypoints1,\n+                    \"matching_scores\": matching_scores,\n+                }\n+            )\n+\n+        return results\n+\n+    def visualize_keypoint_matching(\n+        self,\n+        images,\n+        keypoint_matching_output: list[dict[str, torch.Tensor]],\n+    ) -> list[\"Image.Image\"]:\n+        \"\"\"\n+        Plots the image pairs side by side with the detected keypoints as well as the matching between them.\n+\n+        Args:\n+            images:\n+                Image pairs to plot. Same as `EfficientLoFTRImageProcessor.preprocess`. Expects either a list of 2\n+                images or a list of list of 2 images list with pixel values ranging from 0 to 255.\n+            keypoint_matching_output (List[Dict[str, torch.Tensor]]]):\n+                A post processed keypoint matching output\n+\n+        Returns:\n+            `List[PIL.Image.Image]`: A list of PIL images, each containing the image pairs side by side with the detected\n+            keypoints as well as the matching between them.\n+        \"\"\"\n+        from ...image_utils import to_numpy_array\n+        from .image_processing_efficientloftr import validate_and_format_image_pairs\n+\n+        images = validate_and_format_image_pairs(images)\n+        images = [to_numpy_array(image) for image in images]\n+        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n+\n+        results = []\n+        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n+            height0, width0 = image_pair[0].shape[:2]\n+            height1, width1 = image_pair[1].shape[:2]\n+            plot_image = torch.zeros((max(height0, height1), width0 + width1, 3), dtype=torch.uint8)\n+            plot_image[:height0, :width0] = torch.from_numpy(image_pair[0])\n+            plot_image[:height1, width0:] = torch.from_numpy(image_pair[1])\n+\n+            plot_image_pil = Image.fromarray(plot_image.numpy())\n+            draw = ImageDraw.Draw(plot_image_pil)\n+\n+            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n+            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n+            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n+                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n+            ):\n+                color = self._get_color(matching_score)\n+                draw.line(\n+                    (keypoint0_x, keypoint0_y, keypoint1_x + width0, keypoint1_y),\n+                    fill=color,\n+                    width=3,\n+                )\n+                draw.ellipse((keypoint0_x - 2, keypoint0_y - 2, keypoint0_x + 2, keypoint0_y + 2), fill=\"black\")\n+                draw.ellipse(\n+                    (keypoint1_x + width0 - 2, keypoint1_y - 2, keypoint1_x + width0 + 2, keypoint1_y + 2),\n+                    fill=\"black\",\n+                )\n+\n+            results.append(plot_image_pil)\n+        return results\n+\n+    def _get_color(self, score):\n+        \"\"\"Maps a score to a color.\"\"\"\n+        r = int(255 * (1 - score))\n+        g = int(255 * score)\n+        b = 0\n+        return (r, g, b)\n+\n+\n+__all__ = [\"EfficientLoFTRImageProcessorFast\"]"
        },
        {
            "sha": "d4d978428ff068796046dbcf9d3b454257faaffd",
            "filename": "tests/models/efficientloftr/test_image_processing_efficientloftr.py",
            "status": "modified",
            "additions": 109,
            "deletions": 3,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/a871f6f58d49f3a05ae9dae519caa8aa9d919a07/tests%2Fmodels%2Fefficientloftr%2Ftest_image_processing_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a871f6f58d49f3a05ae9dae519caa8aa9d919a07/tests%2Fmodels%2Fefficientloftr%2Ftest_image_processing_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientloftr%2Ftest_image_processing_efficientloftr.py?ref=a871f6f58d49f3a05ae9dae519caa8aa9d919a07",
            "patch": "@@ -11,25 +11,38 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import time\n import unittest\n \n+import numpy as np\n+import pytest\n+from packaging import version\n+\n from tests.models.superglue.test_image_processing_superglue import (\n     SuperGlueImageProcessingTest,\n     SuperGlueImageProcessingTester,\n )\n-from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n \n if is_torch_available():\n-    import numpy as np\n     import torch\n \n     from transformers.models.efficientloftr.modeling_efficientloftr import KeypointMatchingOutput\n \n if is_vision_available():\n     from transformers import EfficientLoFTRImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import EfficientLoFTRImageProcessorFast\n+\n \n def random_array(size):\n     return np.random.randint(255, size=size)\n@@ -84,7 +97,100 @@ def prepare_keypoint_matching_output(self, pixel_values):\n @require_vision\n class EfficientLoFTRImageProcessingTest(SuperGlueImageProcessingTest, unittest.TestCase):\n     image_processing_class = EfficientLoFTRImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = EfficientLoFTRImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self) -> None:\n         super().setUp()\n         self.image_processor_tester = EfficientLoFTRImageProcessingTester(self)\n+\n+    def test_slow_fast_equivalence(self):\n+        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        # Create image pairs instead of single images\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=False)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        # Create image pairs instead of single images\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+    def test_fast_is_faster_than_slow(self):\n+        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast speed test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast speed test as one of the image processors is not defined\")\n+\n+        # Create image pairs for speed test\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=False)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        # Time slow processor\n+        start_time = time.time()\n+        for _ in range(10):\n+            _ = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        slow_time = time.time() - start_time\n+\n+        # Time fast processor\n+        start_time = time.time()\n+        for _ in range(10):\n+            _ = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+        fast_time = time.time() - start_time\n+\n+        # Fast should be faster (or at least not significantly slower)\n+        self.assertLessEqual(\n+            fast_time, slow_time * 1.2, \"Fast processor should not be significantly slower than slow processor\"\n+        )\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_vision\n+    @pytest.mark.torch_compile_test\n+    def test_can_compile_fast_image_processor(self):\n+        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=False)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.pixel_values, output_compiled.pixel_values, atol=1e-4, rtol=1e-4, mean_atol=1e-5\n+        )"
        },
        {
            "sha": "9355b5d9538d011264f724d64d377b4d43dad355",
            "filename": "tests/models/superglue/test_image_processing_superglue.py",
            "status": "modified",
            "additions": 183,
            "deletions": 168,
            "changes": 351,
            "blob_url": "https://github.com/huggingface/transformers/blob/a871f6f58d49f3a05ae9dae519caa8aa9d919a07/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a871f6f58d49f3a05ae9dae519caa8aa9d919a07/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py?ref=a871f6f58d49f3a05ae9dae519caa8aa9d919a07",
            "patch": "@@ -129,64 +129,67 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processing(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_grayscale\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_grayscale\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 480, \"width\": 640})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 480, \"width\": 640})\n \n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size={\"height\": 42, \"width\": 42}\n-        )\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size={\"height\": 42, \"width\": 42}\n+            )\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n \n     @unittest.skip(reason=\"SuperPointImageProcessor is always supposed to return a grayscaled image\")\n     def test_call_numpy_4_channels(self):\n         pass\n \n     def test_number_and_format_of_images_in_input(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-\n-        # Cases where the number of images and the format of lists in the input is correct\n-        image_input = self.image_processor_tester.prepare_image_inputs(pairs=False, batch_size=2)\n-        image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n-        self.assertEqual((1, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n-\n-        image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=2)\n-        image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n-        self.assertEqual((1, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n-\n-        image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=4)\n-        image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n-        self.assertEqual((2, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n-\n-        image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=6)\n-        image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n-        self.assertEqual((3, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n-\n-        # Cases where the number of images or the format of lists in the input is incorrect\n-        ## List of 4 images\n-        image_input = self.image_processor_tester.prepare_image_inputs(pairs=False, batch_size=4)\n-        with self.assertRaises(ValueError) as cm:\n-            image_processor.preprocess(image_input, return_tensors=\"pt\")\n-        self.assertEqual(ValueError, cm.exception.__class__)\n-\n-        ## List of 3 images\n-        image_input = self.image_processor_tester.prepare_image_inputs(pairs=False, batch_size=3)\n-        with self.assertRaises(ValueError) as cm:\n-            image_processor.preprocess(image_input, return_tensors=\"pt\")\n-        self.assertEqual(ValueError, cm.exception.__class__)\n-\n-        ## List of 2 pairs and 1 image\n-        image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=3)\n-        with self.assertRaises(ValueError) as cm:\n-            image_processor.preprocess(image_input, return_tensors=\"pt\")\n-        self.assertEqual(ValueError, cm.exception.__class__)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+\n+            # Cases where the number of images and the format of lists in the input is correct\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=False, batch_size=2)\n+            image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual((1, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n+\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=2)\n+            image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual((1, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n+\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=4)\n+            image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual((2, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n+\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=6)\n+            image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual((3, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n+\n+            # Cases where the number of images or the format of lists in the input is incorrect\n+            ## List of 4 images\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=False, batch_size=4)\n+            with self.assertRaises(ValueError) as cm:\n+                image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual(ValueError, cm.exception.__class__)\n+\n+            ## List of 3 images\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=False, batch_size=3)\n+            with self.assertRaises(ValueError) as cm:\n+                image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual(ValueError, cm.exception.__class__)\n+\n+            ## List of 2 pairs and 1 image\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=3)\n+            with self.assertRaises(ValueError) as cm:\n+                image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual(ValueError, cm.exception.__class__)\n \n     @parameterized.expand(\n         [\n@@ -197,9 +200,10 @@ def test_number_and_format_of_images_in_input(self):\n         ],\n     )\n     def test_valid_image_shape_in_input(self, image_input, output):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n-        self.assertEqual(output, tuple(image_processed[\"pixel_values\"].shape))\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual(output, tuple(image_processed[\"pixel_values\"].shape))\n \n     @parameterized.expand(\n         [\n@@ -213,139 +217,144 @@ def test_valid_image_shape_in_input(self, image_input, output):\n         ],\n     )\n     def test_invalid_image_shape_in_input(self, image_input):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        with self.assertRaises(ValueError) as cm:\n-            image_processor.preprocess(image_input, return_tensors=\"pt\")\n-        self.assertEqual(ValueError, cm.exception.__class__)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            with self.assertRaises(ValueError) as cm:\n+                image_processor(image_input, return_tensors=\"pt\")\n+            self.assertEqual(ValueError, cm.exception.__class__)\n \n     def test_input_images_properly_paired(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs()\n-        pre_processed_images = image_processor.preprocess(image_inputs, return_tensors=\"np\")\n-        self.assertEqual(len(pre_processed_images[\"pixel_values\"].shape), 5)\n-        self.assertEqual(pre_processed_images[\"pixel_values\"].shape[1], 2)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs()\n+            pre_processed_images = image_processor(image_inputs, return_tensors=\"pt\")\n+            self.assertEqual(len(pre_processed_images[\"pixel_values\"].shape), 5)\n+            self.assertEqual(pre_processed_images[\"pixel_values\"].shape[1], 2)\n \n     def test_input_not_paired_images_raises_error(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(pairs=False)\n-        with self.assertRaises(ValueError):\n-            image_processor.preprocess(image_inputs[0])\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(pairs=False)\n+            with self.assertRaises(ValueError):\n+                image_processor(image_inputs[0])\n \n     def test_input_image_properly_converted_to_grayscale(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs()\n-        pre_processed_images = image_processor.preprocess(image_inputs)\n-        for image_pair in pre_processed_images[\"pixel_values\"]:\n-            for image in image_pair:\n-                self.assertTrue(np.all(image[0, ...] == image[1, ...]) and np.all(image[1, ...] == image[2, ...]))\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs()\n+            pre_processed_images = image_processor(image_inputs, return_tensors=\"pt\")\n+            for image_pair in pre_processed_images[\"pixel_values\"]:\n+                for image in image_pair:\n+                    self.assertTrue(\n+                        torch.all(image[0, ...] == image[1, ...]) and torch.all(image[1, ...] == image[2, ...])\n+                    )\n \n     def test_call_numpy(self):\n         # Test overwritten because SuperGlueImageProcessor combines images by pair to feed it into SuperGlue\n \n         # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random numpy tensors\n-        image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n-        for image_pair in image_pairs:\n-            self.assertEqual(len(image_pair), 2)\n-\n-        expected_batch_size = int(self.image_processor_tester.batch_size / 2)\n-\n-        # Test with 2 images\n-        encoded_images = image_processing(image_pairs[0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n-        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n-\n-        # Test with list of pairs\n-        encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs)\n-        self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n-\n-        # Test without paired images\n-        image_pairs = self.image_processor_tester.prepare_image_inputs(\n-            equal_resolution=False, numpify=True, pairs=False\n-        )\n-        with self.assertRaises(ValueError):\n-            image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image_pair in image_pairs:\n+                self.assertEqual(len(image_pair), 2)\n+\n+            expected_batch_size = int(self.image_processor_tester.batch_size / 2)\n+\n+            # Test with 2 images\n+            encoded_images = image_processing(image_pairs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test with list of pairs\n+            encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs)\n+            self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n+\n+            # Test without paired images\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(\n+                equal_resolution=False, numpify=True, pairs=False\n+            )\n+            with self.assertRaises(ValueError):\n+                image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n \n     def test_call_pil(self):\n         # Test overwritten because SuperGlueImageProcessor combines images by pair to feed it into SuperGlue\n \n         # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n-        for image_pair in image_pairs:\n-            self.assertEqual(len(image_pair), 2)\n-\n-        expected_batch_size = int(self.image_processor_tester.batch_size / 2)\n-\n-        # Test with 2 images\n-        encoded_images = image_processing(image_pairs[0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n-        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n-\n-        # Test with list of pairs\n-        encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs)\n-        self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n-\n-        # Test without paired images\n-        image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, pairs=False)\n-        with self.assertRaises(ValueError):\n-            image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image_pair in image_pairs:\n+                self.assertEqual(len(image_pair), 2)\n+\n+            expected_batch_size = int(self.image_processor_tester.batch_size / 2)\n+\n+            # Test with 2 images\n+            encoded_images = image_processing(image_pairs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test with list of pairs\n+            encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs)\n+            self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n+\n+            # Test without paired images\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, pairs=False)\n+            with self.assertRaises(ValueError):\n+                image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n \n     def test_call_pytorch(self):\n         # Test overwritten because SuperGlueImageProcessor combines images by pair to feed it into SuperGlue\n \n         # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-        for image_pair in image_pairs:\n-            self.assertEqual(len(image_pair), 2)\n-\n-        expected_batch_size = int(self.image_processor_tester.batch_size / 2)\n-\n-        # Test with 2 images\n-        encoded_images = image_processing(image_pairs[0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n-        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n-\n-        # Test with list of pairs\n-        encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs)\n-        self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n-\n-        # Test without paired images\n-        image_pairs = self.image_processor_tester.prepare_image_inputs(\n-            equal_resolution=False, torchify=True, pairs=False\n-        )\n-        with self.assertRaises(ValueError):\n-            image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+            for image_pair in image_pairs:\n+                self.assertEqual(len(image_pair), 2)\n+\n+            expected_batch_size = int(self.image_processor_tester.batch_size / 2)\n+\n+            # Test with 2 images\n+            encoded_images = image_processing(image_pairs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test with list of pairs\n+            encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs)\n+            self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n+\n+            # Test without paired images\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(\n+                equal_resolution=False, torchify=True, pairs=False\n+            )\n+            with self.assertRaises(ValueError):\n+                image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n \n     def test_image_processor_with_list_of_two_images(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n \n-        image_pairs = self.image_processor_tester.prepare_image_inputs(\n-            equal_resolution=False, numpify=True, batch_size=2, pairs=False\n-        )\n-        self.assertEqual(len(image_pairs), 2)\n-        self.assertTrue(isinstance(image_pairs[0], np.ndarray))\n-        self.assertTrue(isinstance(image_pairs[1], np.ndarray))\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(\n+                equal_resolution=False, numpify=True, batch_size=2, pairs=False\n+            )\n+            self.assertEqual(len(image_pairs), 2)\n+            self.assertTrue(isinstance(image_pairs[0], np.ndarray))\n+            self.assertTrue(isinstance(image_pairs[1], np.ndarray))\n \n-        expected_batch_size = 1\n-        encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n-        self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n+            expected_batch_size = 1\n+            encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n+            self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n \n     @require_torch\n     def test_post_processing_keypoint_matching(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs()\n-        pre_processed_images = image_processor.preprocess(image_inputs, return_tensors=\"pt\")\n-        outputs = self.image_processor_tester.prepare_keypoint_matching_output(**pre_processed_images)\n-\n         def check_post_processed_output(post_processed_output, image_pair_size):\n             for post_processed_output, (image_size0, image_size1) in zip(post_processed_output, image_pair_size):\n                 self.assertTrue(\"keypoints0\" in post_processed_output)\n@@ -368,17 +377,23 @@ def check_post_processed_output(post_processed_output, image_pair_size):\n                 all_scores_different_from_minus_one = torch.all(post_processed_output[\"matching_scores\"] != -1)\n                 self.assertTrue(all_scores_different_from_minus_one)\n \n-        tuple_image_sizes = [\n-            ((image_pair[0].size[0], image_pair[0].size[1]), (image_pair[1].size[0], image_pair[1].size[1]))\n-            for image_pair in image_inputs\n-        ]\n-        tuple_post_processed_outputs = image_processor.post_process_keypoint_matching(outputs, tuple_image_sizes)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs()\n+            pre_processed_images = image_processor.preprocess(image_inputs, return_tensors=\"pt\")\n+            outputs = self.image_processor_tester.prepare_keypoint_matching_output(**pre_processed_images)\n+\n+            tuple_image_sizes = [\n+                ((image_pair[0].size[0], image_pair[0].size[1]), (image_pair[1].size[0], image_pair[1].size[1]))\n+                for image_pair in image_inputs\n+            ]\n+            tuple_post_processed_outputs = image_processor.post_process_keypoint_matching(outputs, tuple_image_sizes)\n \n-        check_post_processed_output(tuple_post_processed_outputs, tuple_image_sizes)\n+            check_post_processed_output(tuple_post_processed_outputs, tuple_image_sizes)\n \n-        tensor_image_sizes = torch.tensor(\n-            [(image_pair[0].size, image_pair[1].size) for image_pair in image_inputs]\n-        ).flip(2)\n-        tensor_post_processed_outputs = image_processor.post_process_keypoint_matching(outputs, tensor_image_sizes)\n+            tensor_image_sizes = torch.tensor(\n+                [(image_pair[0].size, image_pair[1].size) for image_pair in image_inputs]\n+            ).flip(2)\n+            tensor_post_processed_outputs = image_processor.post_process_keypoint_matching(outputs, tensor_image_sizes)\n \n-        check_post_processed_output(tensor_post_processed_outputs, tensor_image_sizes)\n+            check_post_processed_output(tensor_post_processed_outputs, tensor_image_sizes)"
        }
    ],
    "stats": {
        "total": 800,
        "additions": 628,
        "deletions": 172
    }
}