{
    "author": "alex-jw-brooks",
    "message": "Allow Exclusion of Input IDs from RepetitionPenaltyLogitsProcessor (#37625)\n\n* Allow exclusion of input IDs for repetition penalty\n\n* Add logit proc tests for rep penalty exclusion\n\n* Expose rep pen flag through generate\n\n* Only slice if needed\n\n* keep current rep pen default behavior\n\n* Revert exposing reppen changes through generate\n\n* Fix test arg\n\n* Update src/transformers/generation/logits_process.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Rename to rep penalty kwarg\n\n* Add custom repetition penalty processor example\n\n* Validate prompt_ignore_length\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "a42ba80fa520c784c8f11a973ca9034e5f859b79",
    "files": [
        {
            "sha": "352fff9e637f9f2d53a901a9ba00ac28122efaa8",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 25,
            "deletions": 3,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/a42ba80fa520c784c8f11a973ca9034e5f859b79/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a42ba80fa520c784c8f11a973ca9034e5f859b79/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=a42ba80fa520c784c8f11a973ca9034e5f859b79",
            "patch": "@@ -292,7 +292,8 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n class RepetitionPenaltyLogitsProcessor(LogitsProcessor):\n     r\"\"\"\n     [`LogitsProcessor`] that prevents the repetition of previous tokens through a penalty. This penalty is applied at\n-    most once per token. Note that, for decoder-only models like most LLMs, the considered tokens include the prompt.\n+    most once per token. Note that, for decoder-only models like most LLMs, the considered tokens include the prompt\n+    by default.\n \n     In the original [paper](https://arxiv.org/pdf/1909.05858.pdf), the authors suggest the use of a penalty of around\n     1.2 to achieve a good balance between truthful generation and lack of repetition. To penalize and reduce\n@@ -303,11 +304,13 @@ class RepetitionPenaltyLogitsProcessor(LogitsProcessor):\n         penalty (`float`):\n             The parameter for repetition penalty. 1.0 means no penalty. Above 1.0 penalizes previously generated\n             tokens. Between 0.0 and 1.0 rewards previously generated tokens.\n+        prompt_ignore_length (`int`, *optional*):\n+            The original input ids sequence length, which if provided, will not be used in the penalty calculation.\n \n     Examples:\n \n     ```py\n-    >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n+    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, RepetitionPenaltyLogitsProcessor\n \n     >>> # Initializing the model and tokenizer for it\n     >>> model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n@@ -323,17 +326,36 @@ class RepetitionPenaltyLogitsProcessor(LogitsProcessor):\n     >>> penalized_ids = model.generate(**inputs, repetition_penalty=1.1)\n     >>> print(tokenizer.batch_decode(penalized_ids, skip_special_tokens=True)[0])\n     I'm not going to be able to do that. I'll just have to go out and play\n+\n+    >>> # We can also exclude the input prompt by creating an instance of this class\n+    >>> # with a `prompt_ignore_length` and passing it as a custom logit processor\n+    >>> rep_pen_processor = RepetitionPenaltyLogitsProcessor(\n+    ...     penalty=1.1,\n+    ...     prompt_ignore_length=inputs[\"input_ids\"].shape[-1]\n+    ... )\n+    >>> penalized_ids = model.generate(**inputs, logits_processor=[rep_pen_processor])\n+    >>> print(tokenizer.batch_decode(penalized_ids, skip_special_tokens=True)[0])\n+    I'm not going to be able to do that. I'm going to have to go through a lot of things, and\n     ```\n     \"\"\"\n \n-    def __init__(self, penalty: float):\n+    def __init__(self, penalty: float, prompt_ignore_length: Optional[int] = None):\n         if not isinstance(penalty, float) or not (penalty > 0):\n             raise ValueError(f\"`penalty` has to be a strictly positive float, but is {penalty}\")\n \n+        if prompt_ignore_length is not None and (\n+            not isinstance(prompt_ignore_length, int) or prompt_ignore_length < 0\n+        ):\n+            raise ValueError(f\"`prompt_ignore_length` has to be a positive integer, but is {prompt_ignore_length}\")\n+\n         self.penalty = penalty\n+        self.prompt_ignore_length = prompt_ignore_length\n \n     @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n+        if self.prompt_ignore_length:\n+            input_ids = input_ids[:, self.prompt_ignore_length :]\n+\n         score = torch.gather(scores, 1, input_ids)\n \n         # if score < 0 then repetition penalty has to be multiplied to reduce the token probabilities"
        },
        {
            "sha": "ea0a7581e5c7bd359f0efbdfb99a66d5be6ac300",
            "filename": "tests/generation/test_logits_process.py",
            "status": "modified",
            "additions": 50,
            "deletions": 0,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/a42ba80fa520c784c8f11a973ca9034e5f859b79/tests%2Fgeneration%2Ftest_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a42ba80fa520c784c8f11a973ca9034e5f859b79/tests%2Fgeneration%2Ftest_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_logits_process.py?ref=a42ba80fa520c784c8f11a973ca9034e5f859b79",
            "patch": "@@ -203,6 +203,56 @@ def test_repetition_penalty_dist_process(self):\n         # processor should not change logits in-place\n         self.assertFalse(torch.all(scores == processed_scores))\n \n+    def test_repetition_penalty_dist_process_exclusion_no_new_input_ids(self):\n+        input_ids = torch.tensor([[0, 1], [5, 0]], device=torch_device, dtype=torch.long)\n+        vocab_size = 10\n+\n+        scores = self._get_uniform_logits(batch_size=2, length=vocab_size)\n+\n+        # give values special values\n+        scores[0, 0] = -(1 / vocab_size)\n+        scores[1, 5] = 4 / vocab_size\n+\n+        rep_penalty_proc = RepetitionPenaltyLogitsProcessor(\n+            penalty=2.0,\n+            prompt_ignore_length=input_ids.shape[-1],\n+        )\n+\n+        processed_scores = rep_penalty_proc(input_ids, scores)\n+\n+        # Because input IDs were provided & we call with the same input\n+        # IDs that we initialize with, it should be the same as calling\n+        # with no input IDs, so no scores should be penalized.\n+        self.assertTrue(torch.all(scores == processed_scores))\n+\n+    def test_repetition_penalty_dist_process_exclusion_with_new_input_ids(self):\n+        orig_input_ids = torch.tensor([[0, 1], [5, 0]], device=torch_device, dtype=torch.long)\n+        curr_input_ids = torch.tensor([[0, 1, 0, 1], [5, 0, 5, 0]], device=torch_device, dtype=torch.long)\n+        vocab_size = 10\n+\n+        scores = self._get_uniform_logits(batch_size=2, length=vocab_size)\n+\n+        # give values special values\n+        scores[0, 0] = -(1 / vocab_size)\n+        scores[1, 5] = 4 / vocab_size\n+\n+        rep_penalty_proc = RepetitionPenaltyLogitsProcessor(\n+            penalty=2.0,\n+            prompt_ignore_length=orig_input_ids.shape[-1],\n+        )\n+\n+        processed_scores = rep_penalty_proc(curr_input_ids, scores)\n+\n+        # check that values were correctly changed\n+        self.assertAlmostEqual(processed_scores[0, 0].item(), -(1 / vocab_size) * 2)\n+        self.assertAlmostEqual(processed_scores[0, 1].item(), (1 / vocab_size) / 2)\n+\n+        self.assertAlmostEqual(processed_scores[1, 0].item(), (1 / vocab_size) / 2)\n+        self.assertAlmostEqual(processed_scores[1, 5].item(), (4 / vocab_size) / 2)\n+\n+        # processor should not change logits in-place\n+        self.assertFalse(torch.all(scores == processed_scores))\n+\n     def test_encoder_repetition_penalty_dist_process(self):\n         input_ids = torch.tensor([[0, 1], [5, 0]], device=torch_device, dtype=torch.long)\n         vocab_size = 10"
        }
    ],
    "stats": {
        "total": 78,
        "additions": 75,
        "deletions": 3
    }
}