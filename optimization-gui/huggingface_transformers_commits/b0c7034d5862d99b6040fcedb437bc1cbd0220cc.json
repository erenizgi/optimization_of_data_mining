{
    "author": "cyyever",
    "message": "Remove self-assignment (#41062)\n\n* Remove self-assignment\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Update src/transformers/integrations/flash_paged.py\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n* Clear pass\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Clear pass\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Clear pass\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>",
    "sha": "b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
    "files": [
        {
            "sha": "e2137bb1d1b01e301fe5558ccd4c9f77ec31b442",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -81,9 +81,7 @@ def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None)\n             audio = load_audio_torchcodec(audio, sampling_rate=sampling_rate)\n         else:\n             audio = load_audio_librosa(audio, sampling_rate=sampling_rate, timeout=timeout)\n-    elif isinstance(audio, np.ndarray):\n-        audio = audio\n-    else:\n+    elif not isinstance(audio, np.ndarray):\n         raise TypeError(\n             \"Incorrect format used for `audio`. Should be an url linking to an audio, a local path, or numpy array.\"\n         )"
        },
        {
            "sha": "ac3258da67bc92a1c48babbd39468543e533c7d8",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -477,9 +477,7 @@ def load_image(image: Union[str, \"PIL.Image.Image\"], timeout: Optional[float] =\n                 raise ValueError(\n                     f\"Incorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got {image}. Failed with {e}\"\n                 )\n-    elif isinstance(image, PIL.Image.Image):\n-        image = image\n-    else:\n+    elif not isinstance(image, PIL.Image.Image):\n         raise TypeError(\n             \"Incorrect format used for image. Should be an url linking to an image, a base64 string, a local path, or a PIL image.\"\n         )"
        },
        {
            "sha": "2f11f452c1bb830c1e4cde52e832511c22e0adad",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -58,11 +58,8 @@ def paged_attention_forward(\n \n     # Retrieve the cumulative sequence lengths for the current layer\n     if isinstance(cu_seq_lens_k, dict):\n-        cu_seq_lens_k = cu_seq_lens_k[layer_type].clone()\n+        cu_seq_lens_k = cu_seq_lens_k[layer_type]\n         max_seqlen_k = max_seqlen_k[layer_type]\n-    else:\n-        cu_seq_lens_k = cu_seq_lens_k.clone()\n-        max_seqlen_k = max_seqlen_k\n \n     if implementation is not None and hasattr(implementation, \"flash_attn_varlen_func\"):\n         flash_attn_varlen_func = implementation.flash_attn_varlen_func"
        },
        {
            "sha": "6658235c2e03820308075e0dca4c0eb21d6a2860",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -2710,7 +2710,6 @@ def forward(\n             logits_mask = self.prepare_question_mask(question_lengths, seqlen)\n             if token_type_ids is None:\n                 token_type_ids = torch.ones(logits_mask.size(), dtype=int, device=logits_mask.device) - logits_mask\n-            logits_mask = logits_mask\n             logits_mask[:, 0] = False\n             logits_mask.unsqueeze_(2)\n "
        },
        {
            "sha": "be7eaf47b428ace15aa9923406450cca1892d66b",
            "filename": "src/transformers/models/colpali/configuration_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconfiguration_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconfiguration_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconfiguration_colpali.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -83,9 +83,7 @@ def __init__(\n                     f\"The model type `{vlm_config['model_type']}` is not supported. Please provide a valid model type.\"\n                 )\n             vlm_config = CONFIG_MAPPING[vlm_config[\"model_type\"]](**vlm_config)\n-        elif isinstance(vlm_config, PretrainedConfig):\n-            vlm_config = vlm_config\n-        else:\n+        elif not isinstance(vlm_config, PretrainedConfig):\n             raise TypeError(\n                 f\"Invalid type for `vlm_config`. Expected `PretrainedConfig`, `dict`, or `None`, but got {type(vlm_config)}.\"\n             )"
        },
        {
            "sha": "21f6e46f1f0037a58503b7a0432f6c3f16657f58",
            "filename": "src/transformers/models/colqwen2/configuration_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -75,9 +75,7 @@ def __init__(\n                     \"The `model_type` key is missing in the `vlm_config` dictionary. Please provide the model type.\"\n                 )\n             vlm_config = CONFIG_MAPPING[vlm_config[\"model_type\"]](**vlm_config)\n-        elif isinstance(vlm_config, PretrainedConfig):\n-            vlm_config = vlm_config\n-        else:\n+        elif not isinstance(vlm_config, PretrainedConfig):\n             raise TypeError(\n                 f\"Invalid type for `vlm_config`. Expected `PretrainedConfig`, `dict`, or `None`, but got {type(vlm_config)}.\"\n             )"
        },
        {
            "sha": "cbf7e44aa8d3d84d44a29335249a50da12a6f045",
            "filename": "src/transformers/models/csm/processing_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -152,7 +152,6 @@ def _get_encoded_length(audio_length, kernel_sizes=None, strides=None, dilations\n                 padding_left = padding_total\n                 padding_right = extra_padding\n             else:\n-                padding_left = padding_left\n                 padding_right = padding_right + extra_padding\n \n             cur_length = cur_length + padding_left + padding_right"
        },
        {
            "sha": "5752c1fb7aa9e8f3f043737437562f1e617f10de",
            "filename": "src/transformers/models/cvt/convert_cvt_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fcvt%2Fconvert_cvt_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fcvt%2Fconvert_cvt_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fconvert_cvt_original_pytorch_checkpoint_to_pytorch.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -283,11 +283,9 @@ def convert_cvt_checkpoint(cvt_model, image_size, cvt_file_name, pytorch_dump_fo\n     num_labels = 1000\n \n     repo_id = \"huggingface/label-files\"\n-    num_labels = num_labels\n     id2label = json.loads(Path(hf_hub_download(repo_id, img_labels_file, repo_type=\"dataset\")).read_text())\n     id2label = {int(k): v for k, v in id2label.items()}\n \n-    id2label = id2label\n     label2id = {v: k for k, v in id2label.items()}\n \n     config = CvtConfig(num_labels=num_labels, id2label=id2label, label2id=label2id)"
        },
        {
            "sha": "cdc008e3c7bbfc6f9f6c14be256c782c11846feb",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -1843,8 +1843,6 @@ def __init__(\n         self, config: DFineConfig, in_channels: int, out_channels: int, num_blocks: int, expansion: float = 1.0\n     ):\n         super().__init__()\n-        in_channels = in_channels\n-        out_channels = out_channels\n         activation = config.activation_function\n \n         hidden_channels = int(out_channels * expansion)"
        },
        {
            "sha": "9a41fb23308eefb77878b3a3f6bee4a25992eb10",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -1110,8 +1110,6 @@ def __init__(\n         self, config: DFineConfig, in_channels: int, out_channels: int, num_blocks: int, expansion: float = 1.0\n     ):\n         super().__init__()\n-        in_channels = in_channels\n-        out_channels = out_channels\n         activation = config.activation_function\n \n         hidden_channels = int(out_channels * expansion)"
        },
        {
            "sha": "c13ff3a1ae430eb9151c0496978234cd6f4a1ba7",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -253,7 +253,6 @@ def forward(\n \n         if rel_att is not None:\n             attention_scores = attention_scores + rel_att\n-        attention_scores = attention_scores\n         attention_scores = attention_scores.view(\n             -1, self.num_attention_heads, attention_scores.size(-2), attention_scores.size(-1)\n         )"
        },
        {
            "sha": "9f97d1c0c2967aebb8cd22377c2d3ac8f2529555",
            "filename": "src/transformers/models/deprecated/van/convert_van_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -168,11 +168,9 @@ def convert_weights_and_push(save_directory: Path, model_name: Optional[str] = N\n     num_labels = 1000\n \n     repo_id = \"huggingface/label-files\"\n-    num_labels = num_labels\n     id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n     id2label = {int(k): v for k, v in id2label.items()}\n \n-    id2label = id2label\n     label2id = {v: k for k, v in id2label.items()}\n \n     ImageNetPreTrainedConfig = partial(VanConfig, num_labels=num_labels, id2label=id2label, label2id=label2id)"
        },
        {
            "sha": "69bfffeb93f1d32f23bc95e2327ce3de9bf4d88d",
            "filename": "src/transformers/models/depth_pro/configuration_depth_pro.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -188,7 +188,6 @@ def __init__(\n                     sub_config.update({\"image_size\": patch_size})\n                 sub_config = CONFIG_MAPPING[sub_config[\"model_type\"]](**sub_config)\n             elif isinstance(sub_config, PretrainedConfig):\n-                sub_config = sub_config\n                 image_size = getattr(sub_config, \"image_size\", None)\n                 if image_size != patch_size:\n                     raise ValueError("
        },
        {
            "sha": "86cf0206c8c9bf2eae325bf9493daa2613b5982c",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -299,7 +299,6 @@ def forward(\n         scaled_images_features = []\n         for i in range(self.n_scaled_images):\n             hidden_state = scaled_images_last_hidden_state[i]\n-            batch_size = batch_size\n             padding = torch_int(self.merge_padding_value * (1 / self.scaled_images_ratios[i]))\n             output_height = base_height * 2**i\n             output_width = base_width * 2**i"
        },
        {
            "sha": "37bfa25ff6c82229c055a1d6c31f32366cfa498c",
            "filename": "src/transformers/models/dpt/configuration_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -200,9 +200,7 @@ def __init__(\n             if isinstance(backbone_config, dict):\n                 logger.info(\"Initializing the config with a `BiT` backbone.\")\n                 backbone_config = BitConfig(**backbone_config)\n-            elif isinstance(backbone_config, PretrainedConfig):\n-                backbone_config = backbone_config\n-            else:\n+            elif not isinstance(backbone_config, PretrainedConfig):\n                 raise ValueError(\n                     f\"backbone_config must be a dictionary or a `PretrainedConfig`, got {backbone_config.__class__}.\"\n                 )"
        },
        {
            "sha": "3524b221a0ec8421376edabc65baaffeb25c2797",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -90,7 +90,6 @@ def __init__(self, dim: int):\n         super().__init__()\n         # Generate and save the inverse frequency buffer (non trainable)\n         inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n-        inv_freq = inv_freq\n         self.register_buffer(\"inv_freq\", inv_freq)\n \n         self._seq_len_cached = None"
        },
        {
            "sha": "8bb5713d1764e4d2a22e638a61d3e6bc364b5ddd",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -188,7 +188,6 @@ def __init__(self, dim: int):\n         super().__init__()\n         # Generate and save the inverse frequency buffer (non trainable)\n         inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n-        inv_freq = inv_freq\n         self.register_buffer(\"inv_freq\", inv_freq)\n \n         self._seq_len_cached = None"
        },
        {
            "sha": "e2db43a7d7879a98fd2be04898c108af67204c50",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -94,7 +94,6 @@ def __init__(self, dim: int):\n         super().__init__()\n         # Generate and save the inverse frequency buffer (non trainable)\n         inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n-        inv_freq = inv_freq\n         self.register_buffer(\"inv_freq\", inv_freq)\n \n         self._seq_len_cached = None"
        },
        {
            "sha": "a4930ef9b906a04f8bc050467f48f9b401b83712",
            "filename": "src/transformers/models/hubert/convert_distilhubert_original_s3prl_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fhubert%2Fconvert_distilhubert_original_s3prl_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fhubert%2Fconvert_distilhubert_original_s3prl_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fconvert_distilhubert_original_s3prl_checkpoint_to_pytorch.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -88,8 +88,6 @@ def recursively_load_weights(fairseq_model, hf_model):\n             is_used = True\n         else:\n             for key, mapped_key in MAPPING.items():\n-                mapped_key = mapped_key\n-\n                 if key in name:\n                     is_used = True\n                     if \"*\" in mapped_key:"
        },
        {
            "sha": "b7adeb2c86c2ec8788c64f5a9d20cd71aa9f4e98",
            "filename": "src/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -45,7 +45,6 @@ def torch_extract_patches(image_tensor, patch_height, patch_width):\n         patch_width (int):\n             The width of the patches to extract.\n     \"\"\"\n-    image_tensor = image_tensor\n     patches = torch.nn.functional.unfold(image_tensor, (patch_height, patch_width), stride=(patch_height, patch_width))\n     patches = patches.reshape(image_tensor.size(0), image_tensor.size(1), patch_height, patch_width, -1)\n     patches = patches.permute(0, 4, 2, 3, 1).reshape("
        },
        {
            "sha": "5d198ee9e552f2557fb5cf580f822e781a4d9eed",
            "filename": "src/transformers/models/levit/convert_levit_timm_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Flevit%2Fconvert_levit_timm_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Flevit%2Fconvert_levit_timm_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fconvert_levit_timm_to_pytorch.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -86,11 +86,9 @@ def convert_weights_and_push(save_directory: Path, model_name: Optional[str] = N\n     expected_shape = (1, num_labels)\n \n     repo_id = \"huggingface/label-files\"\n-    num_labels = num_labels\n     id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n     id2label = {int(k): v for k, v in id2label.items()}\n \n-    id2label = id2label\n     label2id = {v: k for k, v in id2label.items()}\n \n     ImageNetPreTrainedConfig = partial(LevitConfig, num_labels=num_labels, id2label=id2label, label2id=label2id)"
        },
        {
            "sha": "a75b4b7981078cd690ae7d063e5715dce8bf6696",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -409,8 +409,6 @@ def get_image_features(\n \n         if vision_feature_select_strategy == \"default\":\n             selected_image_feature = selected_image_feature[:, 1:]\n-        elif vision_feature_select_strategy == \"full\":\n-            selected_image_feature = selected_image_feature\n \n         image_features = self.multi_modal_projector(selected_image_feature)\n         image_features = torch.split(image_features, image_num_patches, dim=0)"
        },
        {
            "sha": "9e3b15cea54826648d4b51bd02e2d8e9ac664b8a",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -461,8 +461,6 @@ def get_image_features(\n \n         if vision_feature_select_strategy == \"default\":\n             selected_image_feature = selected_image_feature[:, 1:]\n-        elif vision_feature_select_strategy == \"full\":\n-            selected_image_feature = selected_image_feature\n         image_features = self.multi_modal_projector(selected_image_feature)\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n \n@@ -659,8 +657,6 @@ def get_video_features(\n \n         if vision_feature_select_strategy == \"default\":\n             selected_video_features = selected_video_features[:, 1:]\n-        elif vision_feature_select_strategy == \"full\":\n-            selected_video_features = selected_video_features\n \n         # Same as image features except that video has pooling layer\n         video_features = self.vision_resampler(selected_video_features)"
        },
        {
            "sha": "7eda08ffa0bdf66fb7a2ca4a51b970bacafb7b17",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -327,8 +327,6 @@ def get_image_features(\n \n         if vision_feature_select_strategy == \"default\":\n             selected_image_feature = selected_image_feature[:, 1:]\n-        elif vision_feature_select_strategy == \"full\":\n-            selected_image_feature = selected_image_feature\n         image_features = self.multi_modal_projector(selected_image_feature)\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n \n@@ -386,8 +384,6 @@ def get_video_features(\n \n         if vision_feature_select_strategy == \"default\":\n             selected_video_features = selected_video_features[:, 1:]\n-        elif vision_feature_select_strategy == \"full\":\n-            selected_video_features = selected_video_features\n \n         # Same as image features except that video has pooling layer\n         video_features = self.vision_resampler(selected_video_features)"
        },
        {
            "sha": "727655374574c62ef2c351358dd28cd44b2ed32a",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -432,8 +432,6 @@ def get_image_features(\n \n         if vision_feature_select_strategy == \"default\":\n             selected_image_feature = selected_image_feature[:, 1:]\n-        elif vision_feature_select_strategy == \"full\":\n-            selected_image_feature = selected_image_feature\n         image_features = self.multi_modal_projector(selected_image_feature)\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n \n@@ -633,8 +631,6 @@ def get_video_features(\n \n         if vision_feature_select_strategy == \"default\":\n             selected_video_feature = selected_video_feature[:, 1:]\n-        elif vision_feature_select_strategy == \"full\":\n-            selected_video_feature = selected_video_feature\n         video_features = self.multi_modal_projector(selected_video_feature)\n \n         video_features = self.apply_pooling(video_features)"
        },
        {
            "sha": "ec2304e09dd147544b88aebee9395c113f975fed",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -409,8 +409,6 @@ def get_image_features(\n \n         if vision_feature_select_strategy == \"default\":\n             selected_image_feature = selected_image_feature[:, 1:]\n-        elif vision_feature_select_strategy == \"full\":\n-            selected_image_feature = selected_image_feature\n         image_features = self.multi_modal_projector(selected_image_feature)\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n \n@@ -459,8 +457,6 @@ def get_video_features(\n \n         if vision_feature_select_strategy == \"default\":\n             selected_video_feature = selected_video_feature[:, 1:]\n-        elif vision_feature_select_strategy == \"full\":\n-            selected_video_feature = selected_video_feature\n         video_features = self.multi_modal_projector(selected_video_feature)\n \n         video_features = self.apply_pooling(video_features)"
        },
        {
            "sha": "51c041d7b698924a02a65b7c7529584a8caa7afc",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -2572,9 +2572,6 @@ def __init__(\n     ):\n         super().__init__()\n         self.activation_fn = ACT2FN[\"quick_gelu\"]\n-        hidden_size = hidden_size\n-        intermediate_size = intermediate_size\n-        output_size = output_size\n         self.fc1 = nn.Linear(hidden_size, intermediate_size)\n         self.fc2 = nn.Linear(intermediate_size, output_size)\n "
        },
        {
            "sha": "08c084065ff86a519007afc9d6a1d97eacf6d381",
            "filename": "src/transformers/models/perception_lm/configuration_perception_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconfiguration_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconfiguration_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconfiguration_perception_lm.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -68,7 +68,7 @@ def __init__(\n         if isinstance(vision_config, dict):\n             vision_config = TimmWrapperConfig(**vision_config)\n         elif isinstance(vision_config, TimmWrapperConfig):\n-            vision_config = vision_config\n+            pass\n         elif vision_config is None:\n             vision_config = TimmWrapperConfig()\n         self.vision_config = vision_config"
        },
        {
            "sha": "5a1fe6bfac6f9b6f65974bbf08acf664a0761e8c",
            "filename": "src/transformers/models/prophetnet/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fconvert_prophetnet_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fconvert_prophetnet_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fconvert_prophetnet_original_pytorch_checkpoint_to_pytorch.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -132,9 +132,7 @@ def convert_prophetnet_checkpoint_to_pytorch(prophetnet_checkpoint_path: str, py\n             else:\n                 model = getattr(model, attribute)\n \n-                if old_attribute == \"\":\n-                    old_model = old_model\n-                else:\n+                if old_attribute:\n                     if not hasattr(old_model, old_attribute):\n                         raise ValueError(f\"{old_model} does not have {old_attribute}\")\n                     old_model = getattr(old_model, old_attribute)"
        },
        {
            "sha": "a27296dae8e4c240125d4a158a39abf664396c32",
            "filename": "src/transformers/models/regnet/convert_regnet_seer_10b_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -164,11 +164,9 @@ def convert_weights_and_push(save_directory: Path, model_name: Optional[str] = N\n     num_labels = 1000\n \n     repo_id = \"huggingface/label-files\"\n-    num_labels = num_labels\n     id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type=\"dataset\")).read_text())\n     id2label = {int(k): v for k, v in id2label.items()}\n \n-    id2label = id2label\n     label2id = {v: k for k, v in id2label.items()}\n \n     ImageNetPreTrainedConfig = partial(RegNetConfig, num_labels=num_labels, id2label=id2label, label2id=label2id)"
        },
        {
            "sha": "d74e6ad263f0fe5cefb4318d112a32e61b3871a9",
            "filename": "src/transformers/models/regnet/convert_regnet_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -224,11 +224,9 @@ def convert_weights_and_push(save_directory: Path, model_name: Optional[str] = N\n     expected_shape = (1, num_labels)\n \n     repo_id = \"huggingface/label-files\"\n-    num_labels = num_labels\n     id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type=\"dataset\")).read_text())\n     id2label = {int(k): v for k, v in id2label.items()}\n \n-    id2label = id2label\n     label2id = {v: k for k, v in id2label.items()}\n \n     ImageNetPreTrainedConfig = partial(RegNetConfig, num_labels=num_labels, id2label=id2label, label2id=label2id)"
        },
        {
            "sha": "1e02a3e8b6c0bb75ad37afa0527789fea7780038",
            "filename": "src/transformers/models/resnet/convert_resnet_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -128,11 +128,9 @@ def convert_weights_and_push(save_directory: Path, model_name: Optional[str] = N\n     expected_shape = (1, num_labels)\n \n     repo_id = \"huggingface/label-files\"\n-    num_labels = num_labels\n     id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n     id2label = {int(k): v for k, v in id2label.items()}\n \n-    id2label = id2label\n     label2id = {v: k for k, v in id2label.items()}\n \n     ImageNetPreTrainedConfig = partial(ResNetConfig, num_labels=num_labels, id2label=id2label, label2id=label2id)"
        },
        {
            "sha": "8a93f28d5a20b15613e855c3346c6c89f17ae72c",
            "filename": "src/transformers/models/sam2/configuration_sam2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fsam2%2Fconfiguration_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fsam2%2Fconfiguration_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fconfiguration_sam2.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -214,7 +214,7 @@ def __init__(\n             backbone_config[\"model_type\"] = backbone_config.get(\"model_type\", \"sam2_hiera_det_model\")\n             backbone_config = CONFIG_MAPPING[backbone_config[\"model_type\"]](**backbone_config)\n         elif isinstance(backbone_config, Sam2HieraDetConfig):\n-            backbone_config = backbone_config\n+            pass\n         elif backbone_config is None:\n             backbone_config = Sam2HieraDetConfig()\n \n@@ -434,8 +434,6 @@ def __init__(\n         if isinstance(vision_config, dict):\n             vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"sam2_vision_model\")\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n-        elif isinstance(vision_config, PretrainedConfig):\n-            vision_config = vision_config\n         if isinstance(prompt_encoder_config, Sam2PromptEncoderConfig):\n             prompt_encoder_config = prompt_encoder_config.to_dict()\n         if isinstance(mask_decoder_config, Sam2MaskDecoderConfig):"
        },
        {
            "sha": "2712165b44c538191ad2d66d956cc2a97d778f66",
            "filename": "src/transformers/models/sam2_video/configuration_sam2_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fconfiguration_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fconfiguration_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fconfiguration_sam2_video.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -335,8 +335,6 @@ def __init__(\n         if isinstance(vision_config, dict):\n             vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"sam2_vision_model\")\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n-        elif isinstance(vision_config, PretrainedConfig):\n-            vision_config = vision_config\n         if isinstance(prompt_encoder_config, Sam2VideoPromptEncoderConfig):\n             prompt_encoder_config = prompt_encoder_config.to_dict()\n         if isinstance(mask_decoder_config, Sam2VideoMaskDecoderConfig):"
        },
        {
            "sha": "53e10998b2a7575a0de210426222fb4a9a881bc1",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -264,8 +264,6 @@ def __init__(\n         if isinstance(vision_config, dict):\n             vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"sam2_vision_model\")\n             vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n-        elif isinstance(vision_config, PretrainedConfig):\n-            vision_config = vision_config\n         if isinstance(prompt_encoder_config, Sam2VideoPromptEncoderConfig):\n             prompt_encoder_config = prompt_encoder_config.to_dict()\n         if isinstance(mask_decoder_config, Sam2VideoMaskDecoderConfig):"
        },
        {
            "sha": "9332e18856a281d1a4b3b796961c879726fade45",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -631,8 +631,6 @@ def forward(\n         output_attentions: bool = False,\n         conv_attention_mask: Optional[torch.Tensor] = None,\n     ):\n-        hidden_states = hidden_states\n-\n         # 1. Feed-Forward 1 layer\n         residual = hidden_states\n         hidden_states = self.ffn1_layer_norm(hidden_states)"
        },
        {
            "sha": "4836416bced6533a82dead3fc74295afa5dc74e5",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -476,8 +476,6 @@ def forward(\n         output_attentions: bool = False,\n         conv_attention_mask: Optional[torch.Tensor] = None,\n     ):\n-        hidden_states = hidden_states\n-\n         # 1. Feed-Forward 1 layer\n         residual = hidden_states\n         hidden_states = self.ffn1_layer_norm(hidden_states)\n@@ -540,7 +538,6 @@ def _apply_chunk_attention(self, attention_mask, hidden_states):\n         if self.config.speech_encoder_left_chunk_num >= 0:\n             start_indices = (chunk_indices - self.config.speech_encoder_left_chunk_num).clamp_(min=0)\n             start_indices = start_indices * self.config.speech_encoder_chunk_size\n-            start_indices = start_indices\n         start_indices = start_indices.unsqueeze(1).expand(-1, sequence_len)\n \n         end_indices = ((chunk_indices + 1) * self.config.speech_encoder_chunk_size).clamp_(max=sequence_len)"
        },
        {
            "sha": "99253578db5f44d470ffaa18892194be40a0b957",
            "filename": "src/transformers/models/sew_d/modeling_sew_d.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -755,7 +755,6 @@ def forward(\n \n         if rel_att is not None:\n             attention_scores = attention_scores + rel_att\n-        attention_scores = attention_scores\n         attention_scores = attention_scores.view(\n             -1, self.num_attention_heads, attention_scores.size(-2), attention_scores.size(-1)\n         )"
        },
        {
            "sha": "3448089c632bbc5515f0b1c75da424ed1cc23173",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -428,8 +428,6 @@ def forward(\n         output_attentions: bool = False,\n         conv_attention_mask: Optional[torch.Tensor] = None,\n     ):\n-        hidden_states = hidden_states\n-\n         # 1. Feed-Forward 1 layer\n         residual = hidden_states\n         hidden_states = self.ffn1_layer_norm(hidden_states)"
        },
        {
            "sha": "79f70da7cb840ebd4c7db0171c80460b58ca4108",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -326,8 +326,6 @@ def forward(\n         output_attentions: bool = False,\n         conv_attention_mask: Optional[torch.Tensor] = None,\n     ):\n-        hidden_states = hidden_states\n-\n         # 1. Feed-Forward 1 layer\n         residual = hidden_states\n         hidden_states = self.ffn1_layer_norm(hidden_states)"
        },
        {
            "sha": "9a3a79e05d86c48cc19d1e46097b5583861aea8d",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -602,8 +602,6 @@ def forward(\n         relative_position_embeddings: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ):\n-        hidden_states = hidden_states\n-\n         # 1. Feed-Forward 1 layer\n         residual = hidden_states\n         hidden_states = self.ffn1_layer_norm(hidden_states)"
        },
        {
            "sha": "bfa6c20737d8060f8ceca29926f113c2fc176ac9",
            "filename": "src/transformers/models/wav2vec2_conformer/modular_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -410,8 +410,6 @@ def forward(\n         relative_position_embeddings: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ):\n-        hidden_states = hidden_states\n-\n         # 1. Feed-Forward 1 layer\n         residual = hidden_states\n         hidden_states = self.ffn1_layer_norm(hidden_states)"
        },
        {
            "sha": "fd577c0c0bac80bd6718129abc83914963508594",
            "filename": "src/transformers/models/xlstm/modeling_xlstm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c7034d5862d99b6040fcedb437bc1cbd0220cc/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py?ref=b0c7034d5862d99b6040fcedb437bc1cbd0220cc",
            "patch": "@@ -169,7 +169,7 @@ def mlstm_chunkwise_parallel_fw_H(\n         eps: float = 1e-6,\n     ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         _device = matQ.device\n-        nc, chunk_size = num_chunks, chunk_size\n+        nc = num_chunks\n         batch_size, nh, dqk, dhv = matC_states.shape\n         matC_k_states = matC_states.view(batch_size, nh, nc, dqk // nc, dhv)\n         vecN_k_states = vecN_states.view(batch_size, nh, nc, dqk // nc)"
        }
    ],
    "stats": {
        "total": 103,
        "additions": 10,
        "deletions": 93
    }
}