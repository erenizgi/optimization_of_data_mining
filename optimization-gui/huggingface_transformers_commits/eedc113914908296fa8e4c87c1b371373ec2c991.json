{
    "author": "yonigozlan",
    "message": "Add Image Processor Fast Deformable DETR (#34353)\n\n* add deformable detr image processor fast\r\n\r\n* add fast processor to doc\r\n\r\n* fix copies\r\n\r\n* nit docstring\r\n\r\n* Add tests gpu/cpu and fix docstrings\r\n\r\n* fix docstring\r\n\r\n* import changes from detr\r\n\r\n* fix imports\r\n\r\n* rebase and fix\r\n\r\n* fix input data format change in detr and rtdetr fast",
    "sha": "eedc113914908296fa8e4c87c1b371373ec2c991",
    "files": [
        {
            "sha": "5ed99dfe81d1c053b53af99bd858f98893e462c9",
            "filename": "docs/source/en/model_doc/deformable_detr.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/eedc113914908296fa8e4c87c1b371373ec2c991/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/eedc113914908296fa8e4c87c1b371373ec2c991/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md?ref=eedc113914908296fa8e4c87c1b371373ec2c991",
            "patch": "@@ -54,6 +54,12 @@ If you're interested in submitting a resource to be included here, please feel f\n     - preprocess\n     - post_process_object_detection\n \n+## DeformableDetrImageProcessorFast\n+\n+[[autodoc]] DeformableDetrImageProcessorFast\n+    - preprocess\n+    - post_process_object_detection\n+\n ## DeformableDetrFeatureExtractor\n \n [[autodoc]] DeformableDetrFeatureExtractor"
        },
        {
            "sha": "e56959928b4f41aa85e9009564a6e134cab5ce03",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=eedc113914908296fa8e4c87c1b371373ec2c991",
            "patch": "@@ -1186,7 +1186,7 @@\n     )\n     _import_structure[\"models.convnext\"].extend([\"ConvNextFeatureExtractor\", \"ConvNextImageProcessor\"])\n     _import_structure[\"models.deformable_detr\"].extend(\n-        [\"DeformableDetrFeatureExtractor\", \"DeformableDetrImageProcessor\"]\n+        [\"DeformableDetrFeatureExtractor\", \"DeformableDetrImageProcessor\", \"DeformableDetrImageProcessorFast\"]\n     )\n     _import_structure[\"models.deit\"].extend([\"DeiTFeatureExtractor\", \"DeiTImageProcessor\"])\n     _import_structure[\"models.deprecated.deta\"].append(\"DetaImageProcessor\")\n@@ -6100,6 +6100,7 @@\n         from .models.deformable_detr import (\n             DeformableDetrFeatureExtractor,\n             DeformableDetrImageProcessor,\n+            DeformableDetrImageProcessorFast,\n         )\n         from .models.deit import DeiTFeatureExtractor, DeiTImageProcessor\n         from .models.deprecated.deta import DetaImageProcessor"
        },
        {
            "sha": "0b180272bdb085f92ea6d4ff71d8f09c2b194726",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=eedc113914908296fa8e4c87c1b371373ec2c991",
            "patch": "@@ -68,7 +68,7 @@\n             (\"convnextv2\", (\"ConvNextImageProcessor\",)),\n             (\"cvt\", (\"ConvNextImageProcessor\",)),\n             (\"data2vec-vision\", (\"BeitImageProcessor\",)),\n-            (\"deformable_detr\", (\"DeformableDetrImageProcessor\",)),\n+            (\"deformable_detr\", (\"DeformableDetrImageProcessor\", \"DeformableDetrImageProcessorFast\")),\n             (\"deit\", (\"DeiTImageProcessor\",)),\n             (\"depth_anything\", (\"DPTImageProcessor\",)),\n             (\"deta\", (\"DetaImageProcessor\",)),"
        },
        {
            "sha": "7c756c4bdffd7f63519f96141780304eba120add",
            "filename": "src/transformers/models/deformable_detr/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2F__init__.py?ref=eedc113914908296fa8e4c87c1b371373ec2c991",
            "patch": "@@ -29,6 +29,7 @@\n else:\n     _import_structure[\"feature_extraction_deformable_detr\"] = [\"DeformableDetrFeatureExtractor\"]\n     _import_structure[\"image_processing_deformable_detr\"] = [\"DeformableDetrImageProcessor\"]\n+    _import_structure[\"image_processing_deformable_detr_fast\"] = [\"DeformableDetrImageProcessorFast\"]\n \n try:\n     if not is_torch_available():\n@@ -54,6 +55,7 @@\n     else:\n         from .feature_extraction_deformable_detr import DeformableDetrFeatureExtractor\n         from .image_processing_deformable_detr import DeformableDetrImageProcessor\n+        from .image_processing_deformable_detr_fast import DeformableDetrImageProcessorFast\n \n     try:\n         if not is_torch_available():"
        },
        {
            "sha": "fde0540c5d494f1dbfe507163cb792c09eb3bd63",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "added",
            "additions": 1057,
            "deletions": 0,
            "changes": 1057,
            "blob_url": "https://github.com/huggingface/transformers/blob/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=eedc113914908296fa8e4c87c1b371373ec2c991",
            "patch": "@@ -0,0 +1,1057 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Deformable DETR.\"\"\"\n+\n+import functools\n+import pathlib\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    SizeDict,\n+    get_image_size_for_max_height_width,\n+    get_max_height_width,\n+    safe_squeeze,\n+)\n+from ...image_transforms import (\n+    center_to_corners_format,\n+    corners_to_center_format,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    AnnotationFormat,\n+    AnnotationType,\n+    ChannelDimension,\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    get_image_size,\n+    get_image_type,\n+    infer_channel_dimension_format,\n+    make_list_of_images,\n+    pil_torch_interpolation_mapping,\n+    validate_annotations,\n+    validate_kwargs,\n+)\n+from ...utils import (\n+    TensorType,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    is_vision_available,\n+    logging,\n+)\n+from .image_processing_deformable_detr import (\n+    get_size_with_aspect_ratio,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    from torchvision.io import read_image\n+\n+    if is_vision_available():\n+        from ...image_utils import pil_torch_interpolation_mapping\n+\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n+\n+\n+# Copied from transformers.models.detr.image_processing_detr_fast.convert_coco_poly_to_mask\n+def convert_coco_poly_to_mask(segmentations, height: int, width: int, device: torch.device) -> torch.Tensor:\n+    \"\"\"\n+    Convert a COCO polygon annotation to a mask.\n+\n+    Args:\n+        segmentations (`List[List[float]]`):\n+            List of polygons, each polygon represented by a list of x-y coordinates.\n+        height (`int`):\n+            Height of the mask.\n+        width (`int`):\n+            Width of the mask.\n+    \"\"\"\n+    try:\n+        from pycocotools import mask as coco_mask\n+    except ImportError:\n+        raise ImportError(\"Pycocotools is not installed in your environment.\")\n+\n+    masks = []\n+    for polygons in segmentations:\n+        rles = coco_mask.frPyObjects(polygons, height, width)\n+        mask = coco_mask.decode(rles)\n+        if len(mask.shape) < 3:\n+            mask = mask[..., None]\n+        mask = torch.as_tensor(mask, dtype=torch.uint8, device=device)\n+        mask = torch.any(mask, axis=2)\n+        masks.append(mask)\n+    if masks:\n+        masks = torch.stack(masks, axis=0)\n+    else:\n+        masks = torch.zeros((0, height, width), dtype=torch.uint8, device=device)\n+\n+    return masks\n+\n+\n+# Copied from transformers.models.detr.image_processing_detr_fast.prepare_coco_detection_annotation with DETR->DeformableDetr\n+def prepare_coco_detection_annotation(\n+    image,\n+    target,\n+    return_segmentation_masks: bool = False,\n+    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+):\n+    \"\"\"\n+    Convert the target in COCO format into the format expected by DeformableDetr.\n+    \"\"\"\n+    image_height, image_width = image.size()[-2:]\n+\n+    image_id = target[\"image_id\"]\n+    image_id = torch.as_tensor([image_id], dtype=torch.int64, device=image.device)\n+\n+    # Get all COCO annotations for the given image.\n+    annotations = target[\"annotations\"]\n+    classes = []\n+    area = []\n+    boxes = []\n+    keypoints = []\n+    for obj in annotations:\n+        if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0:\n+            classes.append(obj[\"category_id\"])\n+            area.append(obj[\"area\"])\n+            boxes.append(obj[\"bbox\"])\n+            if \"keypoints\" in obj:\n+                keypoints.append(obj[\"keypoints\"])\n+\n+    classes = torch.as_tensor(classes, dtype=torch.int64, device=image.device)\n+    area = torch.as_tensor(area, dtype=torch.float32, device=image.device)\n+    iscrowd = torch.zeros_like(classes, dtype=torch.int64, device=image.device)\n+    # guard against no boxes via resizing\n+    boxes = torch.as_tensor(boxes, dtype=torch.float32, device=image.device).reshape(-1, 4)\n+    boxes[:, 2:] += boxes[:, :2]\n+    boxes[:, 0::2] = boxes[:, 0::2].clip(min=0, max=image_width)\n+    boxes[:, 1::2] = boxes[:, 1::2].clip(min=0, max=image_height)\n+\n+    keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n+\n+    new_target = {\n+        \"image_id\": image_id,\n+        \"class_labels\": classes[keep],\n+        \"boxes\": boxes[keep],\n+        \"area\": area[keep],\n+        \"iscrowd\": iscrowd[keep],\n+        \"orig_size\": torch.as_tensor([int(image_height), int(image_width)], dtype=torch.int64, device=image.device),\n+    }\n+\n+    if keypoints:\n+        keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=image.device)\n+        # Apply the keep mask here to filter the relevant annotations\n+        keypoints = keypoints[keep]\n+        num_keypoints = keypoints.shape[0]\n+        keypoints = keypoints.reshape((-1, 3)) if num_keypoints else keypoints\n+        new_target[\"keypoints\"] = keypoints\n+\n+    if return_segmentation_masks:\n+        segmentation_masks = [obj[\"segmentation\"] for obj in annotations]\n+        masks = convert_coco_poly_to_mask(segmentation_masks, image_height, image_width, device=image.device)\n+        new_target[\"masks\"] = masks[keep]\n+\n+    return new_target\n+\n+\n+# Copied from transformers.models.detr.image_processing_detr_fast.masks_to_boxes\n+def masks_to_boxes(masks: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    Compute the bounding boxes around the provided panoptic segmentation masks.\n+\n+    Args:\n+        masks: masks in format `[number_masks, height, width]` where N is the number of masks\n+\n+    Returns:\n+        boxes: bounding boxes in format `[number_masks, 4]` in xyxy format\n+    \"\"\"\n+    if masks.numel() == 0:\n+        return torch.zeros((0, 4), device=masks.device)\n+\n+    h, w = masks.shape[-2:]\n+    y = torch.arange(0, h, dtype=torch.float32, device=masks.device)\n+    x = torch.arange(0, w, dtype=torch.float32, device=masks.device)\n+    # see https://github.com/pytorch/pytorch/issues/50276\n+    y, x = torch.meshgrid(y, x, indexing=\"ij\")\n+\n+    x_mask = masks * torch.unsqueeze(x, 0)\n+    x_max = x_mask.view(x_mask.shape[0], -1).max(-1)[0]\n+    x_min = (\n+        torch.where(masks, x.unsqueeze(0), torch.tensor(1e8, device=masks.device)).view(masks.shape[0], -1).min(-1)[0]\n+    )\n+\n+    y_mask = masks * torch.unsqueeze(y, 0)\n+    y_max = y_mask.view(y_mask.shape[0], -1).max(-1)[0]\n+    y_min = (\n+        torch.where(masks, y.unsqueeze(0), torch.tensor(1e8, device=masks.device)).view(masks.shape[0], -1).min(-1)[0]\n+    )\n+\n+    return torch.stack([x_min, y_min, x_max, y_max], 1)\n+\n+\n+# Copied from transformers.models.detr.image_processing_detr_fast.rgb_to_id\n+def rgb_to_id(color):\n+    \"\"\"\n+    Converts RGB color to unique ID.\n+    \"\"\"\n+    if isinstance(color, torch.Tensor) and len(color.shape) == 3:\n+        if color.dtype == torch.uint8:\n+            color = color.to(torch.int32)\n+        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n+    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\n+\n+\n+# Copied from transformers.models.detr.image_processing_detr_fast.prepare_coco_panoptic_annotation with DETR->DeformableDetr\n+def prepare_coco_panoptic_annotation(\n+    image: torch.Tensor,\n+    target: Dict,\n+    masks_path: Union[str, pathlib.Path],\n+    return_masks: bool = True,\n+    input_data_format: Union[ChannelDimension, str] = None,\n+) -> Dict:\n+    \"\"\"\n+    Prepare a coco panoptic annotation for DeformableDetr.\n+    \"\"\"\n+    image_height, image_width = get_image_size(image, channel_dim=input_data_format)\n+    annotation_path = pathlib.Path(masks_path) / target[\"file_name\"]\n+\n+    new_target = {}\n+    new_target[\"image_id\"] = torch.as_tensor(\n+        [target[\"image_id\"] if \"image_id\" in target else target[\"id\"]], dtype=torch.int64, device=image.device\n+    )\n+    new_target[\"size\"] = torch.as_tensor([image_height, image_width], dtype=torch.int64, device=image.device)\n+    new_target[\"orig_size\"] = torch.as_tensor([image_height, image_width], dtype=torch.int64, device=image.device)\n+\n+    if \"segments_info\" in target:\n+        masks = read_image(annotation_path).permute(1, 2, 0).to(torch.int32).to(image.device)\n+        masks = rgb_to_id(masks)\n+\n+        ids = torch.as_tensor([segment_info[\"id\"] for segment_info in target[\"segments_info\"]], device=image.device)\n+        masks = masks == ids[:, None, None]\n+        masks = masks.to(torch.bool)\n+        if return_masks:\n+            new_target[\"masks\"] = masks\n+        new_target[\"boxes\"] = masks_to_boxes(masks)\n+        new_target[\"class_labels\"] = torch.as_tensor(\n+            [segment_info[\"category_id\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.int64,\n+            device=image.device,\n+        )\n+        new_target[\"iscrowd\"] = torch.as_tensor(\n+            [segment_info[\"iscrowd\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.int64,\n+            device=image.device,\n+        )\n+        new_target[\"area\"] = torch.as_tensor(\n+            [segment_info[\"area\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.float32,\n+            device=image.device,\n+        )\n+\n+    return new_target\n+\n+\n+class DeformableDetrImageProcessorFast(BaseImageProcessorFast):\n+    r\"\"\"\n+    Constructs a fast Deformable DETR image processor.\n+\n+    Args:\n+        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be\n+            overridden by the `do_resize` parameter in the `preprocess` method.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 800, \"longest_edge\": 1333}`):\n+            Size of the image's `(height, width)` dimensions after resizing. Can be overridden by the `size` parameter\n+            in the `preprocess` method. Available options are:\n+                - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                    Do NOT keep the aspect ratio.\n+                - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                    the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                    less or equal to `longest_edge`.\n+                - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                    aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                    `max_width`.\n+        resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n+            Resampling filter to use if resizing the image.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Controls whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n+            `do_rescale` parameter in the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by the `rescale_factor` parameter in the\n+            `preprocess` method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Controls whether to normalize the image. Can be overridden by the `do_normalize` parameter in the\n+            `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_MEAN`):\n+            Mean values to use when normalizing the image. Can be a single value or a list of values, one for each\n+            channel. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_STD`):\n+            Standard deviation values to use when normalizing the image. Can be a single value or a list of values, one\n+            for each channel. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+            Controls whether to convert the annotations to the format expected by the DETR model. Converts the\n+            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+            If `pad_size` is provided, the image will be padded to the specified dimensions.\n+            Otherwise, the image will be padded to the maximum height and width of the batch.\n+        pad_size (`Dict[str, int]`, *optional*):\n+            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+            height and width in the batch.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.__init__\n+    def __init__(\n+        self,\n+        format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        resample: Union[PILImageResampling, \"F.InterpolationMode\"] = PILImageResampling.BILINEAR,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Union[float, List[float]] = None,\n+        image_std: Union[float, List[float]] = None,\n+        do_convert_annotations: Optional[bool] = None,\n+        do_pad: bool = True,\n+        pad_size: Optional[Dict[str, int]] = None,\n+        **kwargs,\n+    ) -> None:\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n+\n+        if \"max_size\" in kwargs:\n+            logger.warning_once(\n+                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n+                \"Please specify in `size['longest_edge'] instead`.\",\n+            )\n+            max_size = kwargs.pop(\"max_size\")\n+        else:\n+            max_size = None if size is None else 1333\n+\n+        size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n+        size = get_size_dict(size, max_size=max_size, default_to_square=False)\n+\n+        # Backwards compatibility\n+        if do_convert_annotations is None:\n+            do_convert_annotations = do_normalize\n+\n+        super().__init__(**kwargs)\n+        self.format = format\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.do_convert_annotations = do_convert_annotations\n+        self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n+        self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n+        self.do_pad = do_pad\n+        self.pad_size = pad_size\n+        self._valid_processor_keys = [\n+            \"images\",\n+            \"annotations\",\n+            \"return_segmentation_masks\",\n+            \"masks_path\",\n+            \"do_resize\",\n+            \"size\",\n+            \"resample\",\n+            \"do_rescale\",\n+            \"rescale_factor\",\n+            \"do_normalize\",\n+            \"do_convert_annotations\",\n+            \"image_mean\",\n+            \"image_std\",\n+            \"do_pad\",\n+            \"pad_size\",\n+            \"format\",\n+            \"return_tensors\",\n+            \"data_format\",\n+            \"input_data_format\",\n+        ]\n+\n+    @classmethod\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.from_dict with Detr->DeformableDetr\n+    def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n+        \"\"\"\n+        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n+        created using from_dict and kwargs e.g. `DeformableDetrImageProcessorFast.from_pretrained(checkpoint, size=600,\n+        max_size=800)`\n+        \"\"\"\n+        image_processor_dict = image_processor_dict.copy()\n+        if \"max_size\" in kwargs:\n+            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+        return super().from_dict(image_processor_dict, **kwargs)\n+\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.prepare_annotation with DETR->DeformableDetr\n+    def prepare_annotation(\n+        self,\n+        image: torch.Tensor,\n+        target: Dict,\n+        format: Optional[AnnotationFormat] = None,\n+        return_segmentation_masks: bool = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> Dict:\n+        \"\"\"\n+        Prepare an annotation for feeding into DeformableDetr model.\n+        \"\"\"\n+        format = format if format is not None else self.format\n+\n+        if format == AnnotationFormat.COCO_DETECTION:\n+            return_segmentation_masks = False if return_segmentation_masks is None else return_segmentation_masks\n+            target = prepare_coco_detection_annotation(\n+                image, target, return_segmentation_masks, input_data_format=input_data_format\n+            )\n+        elif format == AnnotationFormat.COCO_PANOPTIC:\n+            return_segmentation_masks = True if return_segmentation_masks is None else return_segmentation_masks\n+            target = prepare_coco_panoptic_annotation(\n+                image,\n+                target,\n+                masks_path=masks_path,\n+                return_masks=return_segmentation_masks,\n+                input_data_format=input_data_format,\n+            )\n+        else:\n+            raise ValueError(f\"Format {format} is not supported.\")\n+        return target\n+\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.resize\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize the image to the given size. Size can be `min_size` (scalar) or `(height, width)` tuple. If size is an\n+        int, smaller edge of the image will be matched to this number.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n+                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                        Do NOT keep the aspect ratio.\n+                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                        less or equal to `longest_edge`.\n+                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                        `max_width`.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                Resampling filter to use if resizing the image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        if size.shortest_edge and size.longest_edge:\n+            # Resize the image so that the shortest edge or the longest edge is of the given size\n+            # while maintaining the aspect ratio of the original image.\n+            new_size = get_size_with_aspect_ratio(\n+                image.size()[-2:],\n+                size[\"shortest_edge\"],\n+                size[\"longest_edge\"],\n+            )\n+        elif size.max_height and size.max_width:\n+            new_size = get_image_size_for_max_height_width(image.size()[-2:], size[\"max_height\"], size[\"max_width\"])\n+        elif size.height and size.width:\n+            new_size = (size[\"height\"], size[\"width\"])\n+        else:\n+            raise ValueError(\n+                \"Size must contain 'height' and 'width' keys or 'shortest_edge' and 'longest_edge' keys. Got\"\n+                f\" {size.keys()}.\"\n+            )\n+\n+        image = F.resize(\n+            image,\n+            size=new_size,\n+            interpolation=interpolation,\n+            **kwargs,\n+        )\n+        return image\n+\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.resize_annotation\n+    def resize_annotation(\n+        self,\n+        annotation: Dict[str, Any],\n+        orig_size: Tuple[int, int],\n+        target_size: Tuple[int, int],\n+        threshold: float = 0.5,\n+        interpolation: \"F.InterpolationMode\" = None,\n+    ):\n+        \"\"\"\n+        Resizes an annotation to a target size.\n+\n+        Args:\n+            annotation (`Dict[str, Any]`):\n+                The annotation dictionary.\n+            orig_size (`Tuple[int, int]`):\n+                The original size of the input image.\n+            target_size (`Tuple[int, int]`):\n+                The target size of the image, as returned by the preprocessing `resize` step.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The threshold used to binarize the segmentation masks.\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+                The resampling filter to use when resizing the masks.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST\n+        ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n+\n+        new_annotation = {}\n+        new_annotation[\"size\"] = target_size\n+\n+        for key, value in annotation.items():\n+            if key == \"boxes\":\n+                boxes = value\n+                scaled_boxes = boxes * torch.as_tensor(\n+                    [ratio_width, ratio_height, ratio_width, ratio_height], dtype=torch.float32, device=boxes.device\n+                )\n+                new_annotation[\"boxes\"] = scaled_boxes\n+            elif key == \"area\":\n+                area = value\n+                scaled_area = area * (ratio_width * ratio_height)\n+                new_annotation[\"area\"] = scaled_area\n+            elif key == \"masks\":\n+                masks = value[:, None]\n+                masks = [F.resize(mask, target_size, interpolation=interpolation) for mask in masks]\n+                masks = torch.stack(masks).to(torch.float32)\n+                masks = masks[:, 0] > threshold\n+                new_annotation[\"masks\"] = masks\n+            elif key == \"size\":\n+                new_annotation[\"size\"] = target_size\n+            else:\n+                new_annotation[key] = value\n+\n+        return new_annotation\n+\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.normalize_annotation\n+    def normalize_annotation(self, annotation: Dict, image_size: Tuple[int, int]) -> Dict:\n+        image_height, image_width = image_size\n+        norm_annotation = {}\n+        for key, value in annotation.items():\n+            if key == \"boxes\":\n+                boxes = value\n+                boxes = corners_to_center_format(boxes)\n+                boxes /= torch.as_tensor(\n+                    [image_width, image_height, image_width, image_height], dtype=torch.float32, device=boxes.device\n+                )\n+                norm_annotation[key] = boxes\n+            else:\n+                norm_annotation[key] = value\n+        return norm_annotation\n+\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast._update_annotation_for_padded_image\n+    def _update_annotation_for_padded_image(\n+        self,\n+        annotation: Dict,\n+        input_image_size: Tuple[int, int],\n+        output_image_size: Tuple[int, int],\n+        padding,\n+        update_bboxes,\n+    ) -> Dict:\n+        \"\"\"\n+        Update the annotation for a padded image.\n+        \"\"\"\n+        new_annotation = {}\n+        new_annotation[\"size\"] = output_image_size\n+        ratio_height, ratio_width = (input / output for output, input in zip(output_image_size, input_image_size))\n+\n+        for key, value in annotation.items():\n+            if key == \"masks\":\n+                masks = value\n+                masks = F.pad(\n+                    masks,\n+                    padding,\n+                    fill=0,\n+                )\n+                masks = safe_squeeze(masks, 1)\n+                new_annotation[\"masks\"] = masks\n+            elif key == \"boxes\" and update_bboxes:\n+                boxes = value\n+                boxes *= torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height], device=boxes.device)\n+                new_annotation[\"boxes\"] = boxes\n+            elif key == \"size\":\n+                new_annotation[\"size\"] = output_image_size\n+            else:\n+                new_annotation[key] = value\n+        return new_annotation\n+\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.pad\n+    def pad(\n+        self,\n+        image: torch.Tensor,\n+        padded_size: Tuple[int, int],\n+        annotation: Optional[Dict[str, Any]] = None,\n+        update_bboxes: bool = True,\n+        fill: int = 0,\n+    ):\n+        original_size = image.size()[-2:]\n+        padding_bottom = padded_size[0] - original_size[0]\n+        padding_right = padded_size[1] - original_size[1]\n+        if padding_bottom < 0 or padding_right < 0:\n+            raise ValueError(\n+                f\"Padding dimensions are negative. Please make sure that the padded size is larger than the \"\n+                f\"original size. Got padded size: {padded_size}, original size: {original_size}.\"\n+            )\n+        if original_size != padded_size:\n+            padding = [0, 0, padding_right, padding_bottom]\n+            image = F.pad(image, padding, fill=fill)\n+            if annotation is not None:\n+                annotation = self._update_annotation_for_padded_image(\n+                    annotation, original_size, padded_size, padding, update_bboxes\n+                )\n+\n+        # Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+        pixel_mask = torch.zeros(padded_size, dtype=torch.int64, device=image.device)\n+        pixel_mask[: original_size[0], : original_size[1]] = 1\n+\n+        return image, pixel_mask, annotation\n+\n+    @functools.lru_cache(maxsize=1)\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast._validate_input_arguments\n+    def _validate_input_arguments(\n+        self,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Union[float, List[float]],\n+        image_std: Union[float, List[float]],\n+        do_resize: bool,\n+        size: Dict[str, int],\n+        resample: \"PILImageResampling\",\n+        data_format: Union[str, ChannelDimension],\n+        return_tensors: Union[TensorType, str],\n+    ):\n+        if return_tensors != \"pt\":\n+            raise ValueError(\"Only returning PyTorch tensors is currently supported.\")\n+\n+        if data_format != ChannelDimension.FIRST:\n+            raise ValueError(\"Only channel first data format is currently supported.\")\n+\n+        if do_resize and None in (size, resample):\n+            raise ValueError(\"Size and resample must be specified if do_resize is True.\")\n+\n+        if do_rescale and rescale_factor is None:\n+            raise ValueError(\"Rescale factor must be specified if do_rescale is True.\")\n+\n+        if do_normalize and None in (image_mean, image_std):\n+            raise ValueError(\"Image mean and standard deviation must be specified if do_normalize is True.\")\n+\n+    # Copied from transformers.models.detr.image_processing_detr_fast.DetrImageProcessorFast.preprocess\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        return_segmentation_masks: bool = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[Dict[str, int]] = None,\n+        resample: Optional[Union[PILImageResampling, \"F.InterpolationMode\"]] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[Union[int, float]] = None,\n+        do_normalize: Optional[bool] = None,\n+        do_convert_annotations: Optional[bool] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_pad: Optional[bool] = None,\n+        format: Optional[Union[str, AnnotationFormat]] = None,\n+        return_tensors: Optional[Union[TensorType, str]] = None,\n+        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        pad_size: Optional[Dict[str, int]] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess an image or a batch of images so that it can be used by the model.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image or batch of images to preprocess. Expects a single or batch of images with pixel values ranging\n+                from 0 to 255. If passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n+                List of annotations associated with the image or batch of images. If annotation is for object\n+                detection, the annotations should be a dictionary with the following keys:\n+                - \"image_id\" (`int`): The image id.\n+                - \"annotations\" (`List[Dict]`): List of annotations for an image. Each annotation should be a\n+                  dictionary. An image can have no annotations, in which case the list should be empty.\n+                If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n+                - \"image_id\" (`int`): The image id.\n+                - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n+                  An image can have no segments, in which case the list should be empty.\n+                - \"file_name\" (`str`): The file name of the image.\n+            return_segmentation_masks (`bool`, *optional*, defaults to self.return_segmentation_masks):\n+                Whether to return segmentation masks.\n+            masks_path (`str` or `pathlib.Path`, *optional*):\n+                Path to the directory containing the segmentation masks.\n+            do_resize (`bool`, *optional*, defaults to self.do_resize):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to self.size):\n+                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n+                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                        Do NOT keep the aspect ratio.\n+                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                        less or equal to `longest_edge`.\n+                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                        `max_width`.\n+            resample (`PILImageResampling` or `InterpolationMode`, *optional*, defaults to self.resample):\n+                Resampling filter to use when resizing the image.\n+            do_rescale (`bool`, *optional*, defaults to self.do_rescale):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to self.rescale_factor):\n+                Rescale factor to use when rescaling the image.\n+            do_normalize (`bool`, *optional*, defaults to self.do_normalize):\n+                Whether to normalize the image.\n+            do_convert_annotations (`bool`, *optional*, defaults to self.do_convert_annotations):\n+                Whether to convert the annotations to the format expected by the model. Converts the bounding\n+                boxes from the format `(top_left_x, top_left_y, width, height)` to `(center_x, center_y, width, height)`\n+                and in relative coordinates.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to self.image_mean):\n+                Mean to use when normalizing the image.\n+            image_std (`float` or `List[float]`, *optional*, defaults to self.image_std):\n+                Standard deviation to use when normalizing the image.\n+            do_pad (`bool`, *optional*, defaults to self.do_pad):\n+                Whether to pad the image. If `True`, padding will be applied to the bottom and right of\n+                the image with zeros. If `pad_size` is provided, the image will be padded to the specified\n+                dimensions. Otherwise, the image will be padded to the maximum height and width of the batch.\n+            format (`str` or `AnnotationFormat`, *optional*, defaults to self.format):\n+                Format of the annotations.\n+            return_tensors (`str` or `TensorType`, *optional*, defaults to self.return_tensors):\n+                Type of tensors to return. If `None`, will return the list of images.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            pad_size (`Dict[str, int]`, *optional*):\n+                The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+                provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+                height and width in the batch.\n+        \"\"\"\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            logger.warning_once(\n+                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n+                \"use `do_pad` instead.\"\n+            )\n+            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n+\n+        if \"max_size\" in kwargs:\n+            logger.warning_once(\n+                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n+                \" `size['longest_edge']` instead.\"\n+            )\n+            size = kwargs.pop(\"max_size\")\n+        do_resize = self.do_resize if do_resize is None else do_resize\n+        size = self.size if size is None else size\n+        size = get_size_dict(size=size, default_to_square=False)\n+        resample = self.resample if resample is None else resample\n+        do_rescale = self.do_rescale if do_rescale is None else do_rescale\n+        rescale_factor = self.rescale_factor if rescale_factor is None else rescale_factor\n+        do_normalize = self.do_normalize if do_normalize is None else do_normalize\n+        image_mean = self.image_mean if image_mean is None else image_mean\n+        image_std = self.image_std if image_std is None else image_std\n+        do_convert_annotations = (\n+            self.do_convert_annotations if do_convert_annotations is None else do_convert_annotations\n+        )\n+        do_pad = self.do_pad if do_pad is None else do_pad\n+        pad_size = self.pad_size if pad_size is None else pad_size\n+        format = self.format if format is None else format\n+        device = kwargs.pop(\"device\", None)\n+\n+        # Make hashable for cache\n+        size = SizeDict(**size)\n+        image_mean = tuple(image_mean) if isinstance(image_mean, list) else image_mean\n+        image_std = tuple(image_std) if isinstance(image_std, list) else image_std\n+\n+        images = make_list_of_images(images)\n+        image_type = get_image_type(images[0])\n+\n+        if image_type not in [ImageType.PIL, ImageType.TORCH, ImageType.NUMPY]:\n+            raise ValueError(f\"Unsupported input image type {image_type}\")\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n+\n+        self._validate_input_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            return_tensors=return_tensors,\n+            data_format=data_format,\n+        )\n+\n+        if annotations is not None and isinstance(annotations, dict):\n+            annotations = [annotations]\n+\n+        if annotations is not None and len(images) != len(annotations):\n+            raise ValueError(\n+                f\"The number of images ({len(images)}) and annotations ({len(annotations)}) do not match.\"\n+            )\n+\n+        format = AnnotationFormat(format)\n+        if annotations is not None:\n+            validate_annotations(format, SUPPORTED_ANNOTATION_FORMATS, annotations)\n+\n+        if (\n+            masks_path is not None\n+            and format == AnnotationFormat.COCO_PANOPTIC\n+            and not isinstance(masks_path, (pathlib.Path, str))\n+        ):\n+            raise ValueError(\n+                \"The path to the directory containing the mask PNG files should be provided as a\"\n+                f\" `pathlib.Path` or string object, but is {type(masks_path)} instead.\"\n+            )\n+\n+        data = {}\n+        if image_type == ImageType.PIL:\n+            images = [F.pil_to_tensor(image) for image in images]\n+        elif image_type == ImageType.NUMPY:\n+            # not using F.to_tensor as it doesn't handle (C, H, W) numpy arrays\n+            images = [torch.from_numpy(image).contiguous() for image in images]\n+\n+        if device is not None:\n+            images = [image.to(device) for image in images]\n+\n+        # We assume that all images have the same channel dimension format.\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(images[0])\n+        if input_data_format == ChannelDimension.LAST:\n+            images = [image.permute(2, 0, 1).contiguous() for image in images]\n+            input_data_format = ChannelDimension.FIRST\n+\n+        if do_rescale and do_normalize:\n+            # fused rescale and normalize\n+            new_mean = torch.tensor(image_mean, device=images[0].device) * (1.0 / rescale_factor)\n+            new_std = torch.tensor(image_std, device=images[0].device) * (1.0 / rescale_factor)\n+\n+        processed_images = []\n+        processed_annotations = []\n+        pixel_masks = []  # Initialize pixel_masks here\n+        for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+            # prepare (COCO annotations as a list of Dict -> DETR target as a single Dict per image)\n+            if annotations is not None:\n+                annotation = self.prepare_annotation(\n+                    image,\n+                    annotation,\n+                    format,\n+                    return_segmentation_masks=return_segmentation_masks,\n+                    masks_path=masks_path,\n+                    input_data_format=input_data_format,\n+                )\n+\n+            if do_resize:\n+                interpolation = (\n+                    pil_torch_interpolation_mapping[resample]\n+                    if isinstance(resample, (PILImageResampling, int))\n+                    else resample\n+                )\n+                resized_image = self.resize(image, size=size, interpolation=interpolation)\n+                if annotations is not None:\n+                    annotation = self.resize_annotation(\n+                        annotation,\n+                        orig_size=image.size()[-2:],\n+                        target_size=resized_image.size()[-2:],\n+                    )\n+                image = resized_image\n+\n+            if do_rescale and do_normalize:\n+                # fused rescale and normalize\n+                image = F.normalize(image.to(dtype=torch.float32), new_mean, new_std)\n+            elif do_rescale:\n+                image = image * rescale_factor\n+            elif do_normalize:\n+                image = F.normalize(image, image_mean, image_std)\n+\n+            if do_convert_annotations and annotations is not None:\n+                annotation = self.normalize_annotation(annotation, get_image_size(image, input_data_format))\n+\n+            processed_images.append(image)\n+            processed_annotations.append(annotation)\n+        images = processed_images\n+        annotations = processed_annotations if annotations is not None else None\n+\n+        if do_pad:\n+            # depends on all resized image shapes so we need another loop\n+            if pad_size is not None:\n+                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+            else:\n+                padded_size = get_max_height_width(images)\n+\n+            padded_images = []\n+            padded_annotations = []\n+            for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+                # Pads images and returns their mask: {'pixel_values': ..., 'pixel_mask': ...}\n+                if padded_size == image.size()[-2:]:\n+                    padded_images.append(image)\n+                    pixel_masks.append(torch.ones(padded_size, dtype=torch.int64, device=image.device))\n+                    padded_annotations.append(annotation)\n+                    continue\n+                image, pixel_mask, annotation = self.pad(\n+                    image, padded_size, annotation=annotation, update_bboxes=do_convert_annotations\n+                )\n+                padded_images.append(image)\n+                padded_annotations.append(annotation)\n+                pixel_masks.append(pixel_mask)\n+            images = padded_images\n+            annotations = padded_annotations if annotations is not None else None\n+            data.update({\"pixel_mask\": torch.stack(pixel_masks, dim=0)})\n+\n+        data.update({\"pixel_values\": torch.stack(images, dim=0)})\n+        encoded_inputs = BatchFeature(data, tensor_type=return_tensors)\n+        if annotations is not None:\n+            encoded_inputs[\"labels\"] = [\n+                BatchFeature(annotation, tensor_type=return_tensors) for annotation in annotations\n+            ]\n+        return encoded_inputs\n+\n+    # Copied from transformers.models.deformable_detr.image_processing_deformable_detr.DeformableDetrImageProcessor.post_process\n+    def post_process(self, outputs, target_sizes):\n+        \"\"\"\n+        Converts the raw output of [`DeformableDetrForObjectDetection`] into final bounding boxes in (top_left_x,\n+        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DeformableDetrObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n+                Tensor containing the size (height, width) of each image of the batch. For evaluation, this must be the\n+                original image size (before any data augmentation). For visualization, this should be the image size\n+                after data augment, but before padding.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        logger.warning_once(\n+            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n+            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n+        )\n+\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if len(out_logits) != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n+        if target_sizes.shape[1] != 2:\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        prob = out_logits.sigmoid()\n+        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 100, dim=1)\n+        scores = topk_values\n+        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n+        labels = topk_indexes % out_logits.shape[2]\n+        boxes = center_to_corners_format(out_bbox)\n+        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n+\n+        # and from relative [0, 1] to absolute [0, height] coordinates\n+        img_h, img_w = target_sizes.unbind(1)\n+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n+        boxes = boxes * scale_fct[:, None, :]\n+\n+        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n+\n+        return results\n+\n+    # Copied from transformers.models.deformable_detr.image_processing_deformable_detr.DeformableDetrImageProcessor.post_process_object_detection\n+    def post_process_object_detection(\n+        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, List[Tuple]] = None, top_k: int = 100\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`DeformableDetrForObjectDetection`] into final bounding boxes in (top_left_x,\n+        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DetrObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                (height, width) of each image in the batch. If left to None, predictions will not be resized.\n+            top_k (`int`, *optional*, defaults to 100):\n+                Keep only top k bounding boxes before filtering by thresholding.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if target_sizes is not None:\n+            if len(out_logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+        prob = out_logits.sigmoid()\n+        prob = prob.view(out_logits.shape[0], -1)\n+        k_value = min(top_k, prob.size(1))\n+        topk_values, topk_indexes = torch.topk(prob, k_value, dim=1)\n+        scores = topk_values\n+        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n+        labels = topk_indexes % out_logits.shape[2]\n+        boxes = center_to_corners_format(out_bbox)\n+        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n+\n+        # and from relative [0, 1] to absolute [0, height] coordinates\n+        if target_sizes is not None:\n+            if isinstance(target_sizes, List):\n+                img_h = torch.Tensor([i[0] for i in target_sizes])\n+                img_w = torch.Tensor([i[1] for i in target_sizes])\n+            else:\n+                img_h, img_w = target_sizes.unbind(1)\n+            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n+            boxes = boxes * scale_fct[:, None, :]\n+\n+        results = []\n+        for s, l, b in zip(scores, labels, boxes):\n+            score = s[s > threshold]\n+            label = l[s > threshold]\n+            box = b[s > threshold]\n+            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n+\n+        return results"
        },
        {
            "sha": "0d28d7df7a647aa04fa9ca5166240f02ec0ca8cc",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=eedc113914908296fa8e4c87c1b371373ec2c991",
            "patch": "@@ -416,7 +416,7 @@ def __init__(\n     def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n         \"\"\"\n         Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n-        created using from_dict and kwargs e.g. `DetrImageProcessor.from_pretrained(checkpoint, size=600,\n+        created using from_dict and kwargs e.g. `DetrImageProcessorFast.from_pretrained(checkpoint, size=600,\n         max_size=800)`\n         \"\"\"\n         image_processor_dict = image_processor_dict.copy()\n@@ -863,6 +863,7 @@ def preprocess(\n             input_data_format = infer_channel_dimension_format(images[0])\n         if input_data_format == ChannelDimension.LAST:\n             images = [image.permute(2, 0, 1).contiguous() for image in images]\n+            input_data_format = ChannelDimension.FIRST\n \n         if do_rescale and do_normalize:\n             # fused rescale and normalize"
        },
        {
            "sha": "d447ee8c22ae8be6c29cd1717958232a56885085",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=eedc113914908296fa8e4c87c1b371373ec2c991",
            "patch": "@@ -639,6 +639,7 @@ def preprocess(\n             input_data_format = infer_channel_dimension_format(images[0])\n         if input_data_format == ChannelDimension.LAST:\n             images = [image.permute(2, 0, 1).contiguous() for image in images]\n+            input_data_format = ChannelDimension.FIRST\n \n         if do_rescale and do_normalize:\n             # fused rescale and normalize"
        },
        {
            "sha": "189fbd25baf012b4a759d7c97be185ec4e7c66c5",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eedc113914908296fa8e4c87c1b371373ec2c991/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=eedc113914908296fa8e4c87c1b371373ec2c991",
            "patch": "@@ -135,6 +135,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class DeformableDetrImageProcessorFast(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class DeiTFeatureExtractor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "4a65f1b8d17864b6ec60db99d54e9775425f87c0",
            "filename": "tests/models/deformable_detr/test_image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 265,
            "deletions": 130,
            "changes": 395,
            "blob_url": "https://github.com/huggingface/transformers/blob/eedc113914908296fa8e4c87c1b371373ec2c991/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eedc113914908296fa8e4c87c1b371373ec2c991/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py?ref=eedc113914908296fa8e4c87c1b371373ec2c991",
            "patch": "@@ -20,8 +20,8 @@\n \n import numpy as np\n \n-from transformers.testing_utils import require_torch, require_vision, slow\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import require_torch, require_torch_gpu, require_vision, slow\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import AnnotationFormatTestMixin, ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -32,7 +32,7 @@\n if is_vision_available():\n     from PIL import Image\n \n-    from transformers import DeformableDetrImageProcessor\n+    from transformers import DeformableDetrImageProcessor, DeformableDetrImageProcessorFast\n \n \n class DeformableDetrImageProcessingTester(unittest.TestCase):\n@@ -52,6 +52,7 @@ def __init__(\n         rescale_factor=1 / 255,\n         do_pad=True,\n     ):\n+        super().__init__()\n         # by setting size[\"longest_edge\"] > max_resolution we're effectively not testing this :p\n         size = size if size is not None else {\"shortest_edge\": 18, \"longest_edge\": 1333}\n         self.parent = parent\n@@ -133,6 +134,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class DeformableDetrImageProcessingTest(AnnotationFormatTestMixin, ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = DeformableDetrImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = DeformableDetrImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -143,25 +145,27 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"do_pad\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n-        self.assertEqual(image_processor.do_pad, True)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n+            self.assertEqual(image_processor.do_pad, True)\n \n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n-        )\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-        self.assertEqual(image_processor.do_pad, False)\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n+            )\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n+            self.assertEqual(image_processor.do_pad, False)\n \n     @slow\n     def test_call_pytorch_with_coco_detection_annotations(self):\n@@ -172,40 +176,41 @@ def test_call_pytorch_with_coco_detection_annotations(self):\n \n         target = {\"image_id\": 39769, \"annotations\": target}\n \n-        # encode them\n-        image_processing = DeformableDetrImageProcessor()\n-        encoding = image_processing(images=image, annotations=target, return_tensors=\"pt\")\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class()\n+            encoding = image_processing(images=image, annotations=target, return_tensors=\"pt\")\n \n-        # verify pixel values\n-        expected_shape = torch.Size([1, 3, 800, 1066])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+            # verify pixel values\n+            expected_shape = torch.Size([1, 3, 800, 1066])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n-        expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n-\n-        # verify area\n-        expected_area = torch.tensor([5887.9600, 11250.2061, 489353.8438, 837122.7500, 147967.5156, 165732.3438])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n-        # verify boxes\n-        expected_boxes_shape = torch.Size([6, 4])\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n-        expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n-        # verify image_id\n-        expected_image_id = torch.tensor([39769])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n-        # verify is_crowd\n-        expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n-        # verify class_labels\n-        expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n-        # verify orig_size\n-        expected_orig_size = torch.tensor([480, 640])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n-        # verify size\n-        expected_size = torch.tensor([800, 1066])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+            expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n+            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+\n+            # verify area\n+            expected_area = torch.tensor([5887.9600, 11250.2061, 489353.8438, 837122.7500, 147967.5156, 165732.3438])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            # verify boxes\n+            expected_boxes_shape = torch.Size([6, 4])\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n+            expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            # verify image_id\n+            expected_image_id = torch.tensor([39769])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            # verify is_crowd\n+            expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            # verify class_labels\n+            expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            # verify orig_size\n+            expected_orig_size = torch.tensor([480, 640])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            # verify size\n+            expected_size = torch.tensor([800, 1066])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n \n     @slow\n     def test_call_pytorch_with_coco_panoptic_annotations(self):\n@@ -218,43 +223,45 @@ def test_call_pytorch_with_coco_panoptic_annotations(self):\n \n         masks_path = pathlib.Path(\"./tests/fixtures/tests_samples/COCO/coco_panoptic\")\n \n-        # encode them\n-        image_processing = DeformableDetrImageProcessor(format=\"coco_panoptic\")\n-        encoding = image_processing(images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\")\n-\n-        # verify pixel values\n-        expected_shape = torch.Size([1, 3, 800, 1066])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class(format=\"coco_panoptic\")\n+            encoding = image_processing(images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\")\n \n-        expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+            # verify pixel values\n+            expected_shape = torch.Size([1, 3, 800, 1066])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n-        # verify area\n-        expected_area = torch.tensor([147979.6875, 165527.0469, 484638.5938, 11292.9375, 5879.6562, 7634.1147])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n-        # verify boxes\n-        expected_boxes_shape = torch.Size([6, 4])\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n-        expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n-        # verify image_id\n-        expected_image_id = torch.tensor([39769])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n-        # verify is_crowd\n-        expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n-        # verify class_labels\n-        expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n-        # verify masks\n-        expected_masks_sum = 822873\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].sum().item(), expected_masks_sum)\n-        # verify orig_size\n-        expected_orig_size = torch.tensor([480, 640])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n-        # verify size\n-        expected_size = torch.tensor([800, 1066])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+            expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n+            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+\n+            # verify area\n+            expected_area = torch.tensor([147979.6875, 165527.0469, 484638.5938, 11292.9375, 5879.6562, 7634.1147])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            # verify boxes\n+            expected_boxes_shape = torch.Size([6, 4])\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n+            expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            # verify image_id\n+            expected_image_id = torch.tensor([39769])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            # verify is_crowd\n+            expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            # verify class_labels\n+            expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            # verify masks\n+            expected_masks_sum = 822873\n+            relative_error = torch.abs(encoding[\"labels\"][0][\"masks\"].sum() - expected_masks_sum) / expected_masks_sum\n+            self.assertTrue(relative_error < 1e-3)\n+            # verify orig_size\n+            expected_orig_size = torch.tensor([480, 640])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            # verify size\n+            expected_size = torch.tensor([800, 1066])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n \n     @slow\n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_batched_coco_detection_annotations with Detr->DeformableDetr\n@@ -549,53 +556,181 @@ def test_max_width_max_height_resizing_and_pad_strategy(self):\n             self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([2, 3, 150, 100]))\n \n     def test_longest_edge_shortest_edge_resizing_strategy(self):\n-        image_1 = torch.ones([958, 653, 3], dtype=torch.uint8)\n+        for image_processing_class in self.image_processor_list:\n+            image_1 = torch.ones([958, 653, 3], dtype=torch.uint8)\n+\n+            # max size is set; width < height;\n+            # do_pad=False, longest_edge=640, shortest_edge=640, image=958x653 -> 640x436\n+            image_processor = image_processing_class(\n+                size={\"longest_edge\": 640, \"shortest_edge\": 640},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 640, 436]))\n+\n+            image_2 = torch.ones([653, 958, 3], dtype=torch.uint8)\n+            # max size is set; height < width;\n+            # do_pad=False, longest_edge=640, shortest_edge=640, image=653x958 -> 436x640\n+            image_processor = image_processing_class(\n+                size={\"longest_edge\": 640, \"shortest_edge\": 640},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_2], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 436, 640]))\n+\n+            image_3 = torch.ones([100, 120, 3], dtype=torch.uint8)\n+            # max size is set; width == size; height > max_size;\n+            # do_pad=False, longest_edge=118, shortest_edge=100, image=120x100 -> 118x98\n+            image_processor = image_processing_class(\n+                size={\"longest_edge\": 118, \"shortest_edge\": 100},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_3], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 98, 118]))\n+\n+            image_4 = torch.ones([128, 50, 3], dtype=torch.uint8)\n+            # max size is set; height == size; width < max_size;\n+            # do_pad=False, longest_edge=256, shortest_edge=50, image=50x128 -> 50x128\n+            image_processor = image_processing_class(\n+                size={\"longest_edge\": 256, \"shortest_edge\": 50},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_4], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 128, 50]))\n+\n+            image_5 = torch.ones([50, 50, 3], dtype=torch.uint8)\n+            # max size is set; height == width; width < max_size;\n+            # do_pad=False, longest_edge=117, shortest_edge=50, image=50x50 -> 50x50\n+            image_processor = image_processing_class(\n+                size={\"longest_edge\": 117, \"shortest_edge\": 50},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_5], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 50, 50]))\n+\n+    @slow\n+    @require_torch_gpu\n+    # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations\n+    def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n+        # prepare image and target\n+        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+        with open(\"./tests/fixtures/tests_samples/COCO/coco_annotations.txt\", \"r\") as f:\n+            target = json.loads(f.read())\n+\n+        target = {\"image_id\": 39769, \"annotations\": target}\n+\n+        # Ignore copy\n+        processor = self.image_processor_list[1]()\n+\n+        # 1. run processor on CPU\n+        encoding_cpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=\"cpu\")\n+        # 2. run processor on GPU\n+        encoding_gpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=\"cuda\")\n+\n+        # verify pixel values\n+        self.assertEqual(encoding_cpu[\"pixel_values\"].shape, encoding_gpu[\"pixel_values\"].shape)\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"pixel_values\"][0, 0, 0, :3],\n+                encoding_gpu[\"pixel_values\"][0, 0, 0, :3].to(\"cpu\"),\n+                atol=1e-4,\n+            )\n+        )\n+        # verify area\n+        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\")))\n+        # verify boxes\n+        self.assertEqual(encoding_cpu[\"labels\"][0][\"boxes\"].shape, encoding_gpu[\"labels\"][0][\"boxes\"].shape)\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"labels\"][0][\"boxes\"][0], encoding_gpu[\"labels\"][0][\"boxes\"][0].to(\"cpu\"), atol=1e-3\n+            )\n+        )\n+        # verify image_id\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\"))\n+        )\n+        # verify is_crowd\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\"))\n+        )\n+        # verify class_labels\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"labels\"][0][\"class_labels\"], encoding_gpu[\"labels\"][0][\"class_labels\"].to(\"cpu\")\n+            )\n+        )\n+        # verify orig_size\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\"))\n+        )\n+        # verify size\n+        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\")))\n \n-        # max size is set; width < height;\n-        # do_pad=False, longest_edge=640, shortest_edge=640, image=958x653 -> 640x436\n-        image_processor = DeformableDetrImageProcessor(\n-            size={\"longest_edge\": 640, \"shortest_edge\": 640},\n-            do_pad=False,\n+    @slow\n+    @require_torch_gpu\n+    # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations\n+    def test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations(self):\n+        # prepare image, target and masks_path\n+        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+        with open(\"./tests/fixtures/tests_samples/COCO/coco_panoptic_annotations.txt\", \"r\") as f:\n+            target = json.loads(f.read())\n+\n+        target = {\"file_name\": \"000000039769.png\", \"image_id\": 39769, \"segments_info\": target}\n+\n+        masks_path = pathlib.Path(\"./tests/fixtures/tests_samples/COCO/coco_panoptic\")\n+\n+        # Ignore copy\n+        processor = self.image_processor_list[1](format=\"coco_panoptic\")\n+\n+        # 1. run processor on CPU\n+        encoding_cpu = processor(\n+            images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\", device=\"cpu\"\n         )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 640, 436]))\n-\n-        image_2 = torch.ones([653, 958, 3], dtype=torch.uint8)\n-        # max size is set; height < width;\n-        # do_pad=False, longest_edge=640, shortest_edge=640, image=653x958 -> 436x640\n-        image_processor = DeformableDetrImageProcessor(\n-            size={\"longest_edge\": 640, \"shortest_edge\": 640},\n-            do_pad=False,\n+        # 2. run processor on GPU\n+        encoding_gpu = processor(\n+            images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\", device=\"cuda\"\n         )\n-        inputs = image_processor(images=[image_2], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 436, 640]))\n-\n-        image_3 = torch.ones([100, 120, 3], dtype=torch.uint8)\n-        # max size is set; width == size; height > max_size;\n-        # do_pad=False, longest_edge=118, shortest_edge=100, image=120x100 -> 118x98\n-        image_processor = DeformableDetrImageProcessor(\n-            size={\"longest_edge\": 118, \"shortest_edge\": 100},\n-            do_pad=False,\n+\n+        # verify pixel values\n+        self.assertEqual(encoding_cpu[\"pixel_values\"].shape, encoding_gpu[\"pixel_values\"].shape)\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"pixel_values\"][0, 0, 0, :3],\n+                encoding_gpu[\"pixel_values\"][0, 0, 0, :3].to(\"cpu\"),\n+                atol=1e-4,\n+            )\n         )\n-        inputs = image_processor(images=[image_3], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 98, 118]))\n-\n-        image_4 = torch.ones([128, 50, 3], dtype=torch.uint8)\n-        # max size is set; height == size; width < max_size;\n-        # do_pad=False, longest_edge=256, shortest_edge=50, image=50x128 -> 50x128\n-        image_processor = DeformableDetrImageProcessor(\n-            size={\"longest_edge\": 256, \"shortest_edge\": 50},\n-            do_pad=False,\n+        # verify area\n+        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\")))\n+        # verify boxes\n+        self.assertEqual(encoding_cpu[\"labels\"][0][\"boxes\"].shape, encoding_gpu[\"labels\"][0][\"boxes\"].shape)\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"labels\"][0][\"boxes\"][0], encoding_gpu[\"labels\"][0][\"boxes\"][0].to(\"cpu\"), atol=1e-3\n+            )\n         )\n-        inputs = image_processor(images=[image_4], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 128, 50]))\n-\n-        image_5 = torch.ones([50, 50, 3], dtype=torch.uint8)\n-        # max size is set; height == width; width < max_size;\n-        # do_pad=False, longest_edge=117, shortest_edge=50, image=50x50 -> 50x50\n-        image_processor = DeformableDetrImageProcessor(\n-            size={\"longest_edge\": 117, \"shortest_edge\": 50},\n-            do_pad=False,\n+        # verify image_id\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\"))\n         )\n-        inputs = image_processor(images=[image_5], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 50, 50]))\n+        # verify is_crowd\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\"))\n+        )\n+        # verify class_labels\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"labels\"][0][\"class_labels\"], encoding_gpu[\"labels\"][0][\"class_labels\"].to(\"cpu\")\n+            )\n+        )\n+        # verify masks\n+        masks_sum_cpu = encoding_cpu[\"labels\"][0][\"masks\"].sum()\n+        masks_sum_gpu = encoding_gpu[\"labels\"][0][\"masks\"].sum()\n+        relative_error = torch.abs(masks_sum_cpu - masks_sum_gpu) / masks_sum_cpu\n+        self.assertTrue(relative_error < 1e-3)\n+        # verify orig_size\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\"))\n+        )\n+        # verify size\n+        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\")))"
        },
        {
            "sha": "bb8b9272efc95217615940cfc2074ddbb57f8cbf",
            "filename": "tests/models/grounding_dino/test_image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 92,
            "deletions": 87,
            "changes": 179,
            "blob_url": "https://github.com/huggingface/transformers/blob/eedc113914908296fa8e4c87c1b371373ec2c991/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eedc113914908296fa8e4c87c1b371373ec2c991/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py?ref=eedc113914908296fa8e4c87c1b371373ec2c991",
            "patch": "@@ -159,26 +159,28 @@ def image_processor_dict(self):\n \n     # Copied from tests.models.deformable_detr.test_image_processing_deformable_detr.DeformableDetrImageProcessingTest.test_image_processor_properties with DeformableDetr->GroundingDino\n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"do_pad\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n \n     # Copied from tests.models.deformable_detr.test_image_processing_deformable_detr.DeformableDetrImageProcessingTest.test_image_processor_from_dict_with_kwargs with DeformableDetr->GroundingDino\n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n-        self.assertEqual(image_processor.do_pad, True)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n+            self.assertEqual(image_processor.do_pad, True)\n \n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n-        )\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-        self.assertEqual(image_processor.do_pad, False)\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n+            )\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n+            self.assertEqual(image_processor.do_pad, False)\n \n     def test_post_process_object_detection(self):\n         image_processor = self.image_processing_class(**self.image_processor_dict)\n@@ -206,40 +208,41 @@ def test_call_pytorch_with_coco_detection_annotations(self):\n \n         target = {\"image_id\": 39769, \"annotations\": target}\n \n-        # encode them\n-        image_processing = GroundingDinoImageProcessor()\n-        encoding = image_processing(images=image, annotations=target, return_tensors=\"pt\")\n-\n-        # verify pixel values\n-        expected_shape = torch.Size([1, 3, 800, 1066])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n-\n-        # verify area\n-        expected_area = torch.tensor([5887.9600, 11250.2061, 489353.8438, 837122.7500, 147967.5156, 165732.3438])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n-        # verify boxes\n-        expected_boxes_shape = torch.Size([6, 4])\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n-        expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n-        # verify image_id\n-        expected_image_id = torch.tensor([39769])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n-        # verify is_crowd\n-        expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n-        # verify class_labels\n-        expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n-        # verify orig_size\n-        expected_orig_size = torch.tensor([480, 640])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n-        # verify size\n-        expected_size = torch.tensor([800, 1066])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class()\n+            encoding = image_processing(images=image, annotations=target, return_tensors=\"pt\")\n+\n+            # verify pixel values\n+            expected_shape = torch.Size([1, 3, 800, 1066])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n+            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+\n+            # verify area\n+            expected_area = torch.tensor([5887.9600, 11250.2061, 489353.8438, 837122.7500, 147967.5156, 165732.3438])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            # verify boxes\n+            expected_boxes_shape = torch.Size([6, 4])\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n+            expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            # verify image_id\n+            expected_image_id = torch.tensor([39769])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            # verify is_crowd\n+            expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            # verify class_labels\n+            expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            # verify orig_size\n+            expected_orig_size = torch.tensor([480, 640])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            # verify size\n+            expected_size = torch.tensor([800, 1066])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n \n     @slow\n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_batched_coco_detection_annotations with Detr->GroundingDino\n@@ -373,43 +376,45 @@ def test_call_pytorch_with_coco_panoptic_annotations(self):\n \n         masks_path = pathlib.Path(\"./tests/fixtures/tests_samples/COCO/coco_panoptic\")\n \n-        # encode them\n-        image_processing = GroundingDinoImageProcessor(format=\"coco_panoptic\")\n-        encoding = image_processing(images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\")\n-\n-        # verify pixel values\n-        expected_shape = torch.Size([1, 3, 800, 1066])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n-\n-        # verify area\n-        expected_area = torch.tensor([147979.6875, 165527.0469, 484638.5938, 11292.9375, 5879.6562, 7634.1147])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n-        # verify boxes\n-        expected_boxes_shape = torch.Size([6, 4])\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n-        expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n-        # verify image_id\n-        expected_image_id = torch.tensor([39769])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n-        # verify is_crowd\n-        expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n-        # verify class_labels\n-        expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n-        # verify masks\n-        expected_masks_sum = 822873\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].sum().item(), expected_masks_sum)\n-        # verify orig_size\n-        expected_orig_size = torch.tensor([480, 640])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n-        # verify size\n-        expected_size = torch.tensor([800, 1066])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class(format=\"coco_panoptic\")\n+            encoding = image_processing(images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\")\n+\n+            # verify pixel values\n+            expected_shape = torch.Size([1, 3, 800, 1066])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n+            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+\n+            # verify area\n+            expected_area = torch.tensor([147979.6875, 165527.0469, 484638.5938, 11292.9375, 5879.6562, 7634.1147])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            # verify boxes\n+            expected_boxes_shape = torch.Size([6, 4])\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n+            expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            # verify image_id\n+            expected_image_id = torch.tensor([39769])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            # verify is_crowd\n+            expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            # verify class_labels\n+            expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            # verify masks\n+            expected_masks_sum = 822873\n+            relative_error = torch.abs(encoding[\"labels\"][0][\"masks\"].sum() - expected_masks_sum) / expected_masks_sum\n+            self.assertTrue(relative_error < 1e-3)\n+            # verify orig_size\n+            expected_orig_size = torch.tensor([480, 640])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            # verify size\n+            expected_size = torch.tensor([800, 1066])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n \n     @slow\n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_batched_coco_panoptic_annotations with Detr->GroundingDino"
        }
    ],
    "stats": {
        "total": 1655,
        "additions": 1435,
        "deletions": 220
    }
}