{
    "author": "manueldeprada",
    "message": "Fix QuantoQuantizedCache import issues (#40109)\n\n* fix quantoquantized",
    "sha": "34a1fc6426fa86efd6eea3e3da5acdba32c28085",
    "files": [
        {
            "sha": "9ef198a767036fc084fa053373339f74700f75de",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/34a1fc6426fa86efd6eea3e3da5acdba32c28085/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34a1fc6426fa86efd6eea3e3da5acdba32c28085/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=34a1fc6426fa86efd6eea3e3da5acdba32c28085",
            "patch": "@@ -16,9 +16,6 @@\n )\n \n \n-if _is_quanto_greater_than_0_2_5 := is_quanto_greater(\"0.2.5\", accept_dev=True):\n-    from optimum.quanto import MaxOptimizer, qint2, qint4, quantize_weight\n-\n if is_hqq_available():\n     from hqq.core.quantize import Quantizer as HQQQuantizer\n \n@@ -558,7 +555,7 @@ def __init__(\n         q_group_size: int = 64,\n         residual_length: int = 128,\n     ):\n-        super().__init__(self)\n+        super().__init__()\n         self.nbits = nbits\n         self.axis_key = axis_key\n         self.axis_value = axis_value\n@@ -635,10 +632,12 @@ def __init__(\n             residual_length=residual_length,\n         )\n \n-        if not _is_quanto_greater_than_0_2_5:\n+        # We need to import quanto here to avoid circular imports due to optimum/quanto/models/transformers_models.py\n+        if is_quanto_greater(\"0.2.5\", accept_dev=True):\n+            from optimum.quanto import MaxOptimizer, qint2, qint4\n+        else:\n             raise ImportError(\n                 \"You need optimum-quanto package version to be greater or equal than 0.2.5 to use `QuantoQuantizedCache`. \"\n-                \"Detected version {optimum_quanto_version}.\"\n             )\n \n         if self.nbits not in [2, 4]:\n@@ -656,6 +655,8 @@ def __init__(\n         self.optimizer = MaxOptimizer()  # hardcode as it's the only one for per-channel quantization\n \n     def _quantize(self, tensor, axis):\n+        from optimum.quanto import quantize_weight\n+\n         scale, zeropoint = self.optimizer(tensor, self.qtype, axis, self.q_group_size)\n         qtensor = quantize_weight(tensor, self.qtype, axis, scale, zeropoint, self.q_group_size)\n         return qtensor"
        },
        {
            "sha": "1ae4d9d711597fa14e2632adcc3f9a00c7995025",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/34a1fc6426fa86efd6eea3e3da5acdba32c28085/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34a1fc6426fa86efd6eea3e3da5acdba32c28085/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=34a1fc6426fa86efd6eea3e3da5acdba32c28085",
            "patch": "@@ -1286,7 +1286,7 @@ def is_quanto_greater(library_version: str, accept_dev: bool = False):\n     given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n     2.7.0).\n     \"\"\"\n-    if not _is_package_available(\"optimum-quanto\"):\n+    if not _is_package_available(\"optimum.quanto\"):\n         return False\n \n     if accept_dev:"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 8,
        "deletions": 7
    }
}