{
    "author": "pankajbaid567",
    "message": "fix: Restore explicit .keys() calls for TensorDict compatibility (#42373)\n\n* fix: Restore explicit .keys() calls for TensorDict compatibility\n\nFixes issue where TensorDict objects cause RuntimeError: generator raised StopIteration\nwhen used with data collators and tokenization utilities.\n\nProblem:\n- TensorDict.__iter__() iterates over batch dimensions instead of dictionary keys\n- PR #37283 removed explicit .keys() calls, breaking TensorDict compatibility\n- Affected DataCollatorWithPadding, DataCollatorForLanguageModeling, and other collators\n\nSolution:\n- Restored explicit .keys() calls in 5 critical locations where dict-list conversion happens\n- Added len() check to handle empty batch edge case\n- Changes are backward compatible and generalize to all Mapping objects\n\nFiles modified:\n- src/transformers/tokenization_utils_base.py: Fixed pad() method\n- src/transformers/tokenization_mistral_common.py: Fixed pad() method\n- src/transformers/feature_extraction_sequence_utils.py: Fixed pad() method\n- src/transformers/models/mluke/tokenization_mluke.py: Fixed pad() method\n- src/transformers/models/luke/tokenization_luke.py: Fixed pad() method\n\nTesting:\n- Added comprehensive test suite: tests/trainer/test_tensordict_compatibility.py\n- 7 test cases covering basic padding, variable lengths, mixed inputs, additional fields\n- Added @require_tensordict decorator and is_tensordict_available() in testing_utils.py\n- All existing tests pass (54/54 data collator tests, 2/2 padding tests)\n\nImpact:\n- Zero performance regression for standard dict usage\n- Restores functionality for TensorDict and other Mapping implementations\n- Fully backward compatible\n\n* style: Apply ruff formatting to fix CI checks\n\n* refactor: Address reviewer feedback on TensorDict fix\n\n- Move TensorDict tests from standalone file to test_data_collator.py\n- Simplify comments from verbose explanation to short reference: 'Call .keys() explicitly to avoid issue #42370'\n- Delete tests/trainer/test_tensordict_compatibility.py (tests now in test_data_collator.py)\n\nAddresses feedback from @ligz08 in PR review\n\n* docs: Update comments to be self-explanatory about TensorDict compatibility\n\nChanged from 'avoid issue #42370' to 'for compatibility with TensorDict and other Mapping subclasses'\nso users don't need to look up the issue on GitHub to understand why .keys() is needed.\n\nAddresses maintainer feedback.\n\n* Remove TensorDict tests and utilities as requested\n\n- Removed TensorDictCompatibilityTest class from test_data_collator.py\n- Removed is_tensordict_available() and require_tensordict() from testing_utils.py\n- TensorDict is not a CI dependency, so these tests would be skipped anyway\n- The .keys() fix for TensorDict compatibility remains in place\n\n---------\n\nCo-authored-by: Pankaj Baid <baidpankaj567@gmail.com>",
    "sha": "1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da",
    "files": [
        {
            "sha": "fdd98aa6730b3330778677b02bf1cbd05019c38c",
            "filename": "src/transformers/feature_extraction_sequence_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py?ref=1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da",
            "patch": "@@ -123,8 +123,9 @@ def pad(\n         # If we have a list of dicts, let's convert it in a dict of lists\n         # We do this to allow using this method as a collate_fn function in PyTorch Dataloader\n         if isinstance(processed_features, (list, tuple)) and isinstance(processed_features[0], (dict, BatchFeature)):\n+            # Call .keys() explicitly for compatibility with TensorDict and other Mapping subclasses\n             processed_features = {\n-                key: [example[key] for example in processed_features] for key in processed_features[0]\n+                key: [example[key] for example in processed_features] for key in processed_features[0].keys()\n             }\n \n         # The model's main input name, usually `input_values`, has be passed for padding"
        },
        {
            "sha": "99d3fdff9b326d2f051ccaef6677b20b650a3d9d",
            "filename": "src/transformers/models/luke/tokenization_luke.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py?ref=1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da",
            "patch": "@@ -1449,7 +1449,8 @@ def pad(\n         # If we have a list of dicts, let's convert it in a dict of lists\n         # We do this to allow using this method as a collate_fn function in PyTorch Dataloader\n         if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n-            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0]}\n+            # Call .keys() explicitly for compatibility with TensorDict and other Mapping subclasses\n+            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}\n \n         # The model's main input name, usually `input_ids`, has be passed for padding\n         if self.model_input_names[0] not in encoded_inputs:"
        },
        {
            "sha": "b66f86a8808b0a1a2f07964c12d91ee276f881c9",
            "filename": "src/transformers/models/mluke/tokenization_mluke.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py?ref=1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da",
            "patch": "@@ -1287,7 +1287,8 @@ def pad(\n         # If we have a list of dicts, let's convert it in a dict of lists\n         # We do this to allow using this method as a collate_fn function in PyTorch Dataloader\n         if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n-            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0]}\n+            # Call .keys() explicitly for compatibility with TensorDict and other Mapping subclasses\n+            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}\n \n         # The model's main input name, usually `input_ids`, has be passed for padding\n         if self.model_input_names[0] not in encoded_inputs:"
        },
        {
            "sha": "75414f01f591cfc151f9885e65240d125c0e5757",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da",
            "patch": "@@ -1198,7 +1198,8 @@ def pad(\n         # If we have a list of dicts, let's convert it in a dict of lists\n         # We do this to allow using this method as a collate_fn function in PyTorch Dataloader\n         if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n-            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0]}\n+            # Call .keys() explicitly for compatibility with TensorDict and other Mapping subclasses\n+            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}\n \n         # The model's main input name, usually `input_ids`, has been passed for padding\n         if self.model_input_names[0] not in encoded_inputs:"
        },
        {
            "sha": "392bee97987db034faf492f9d4382b0886c96d16",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=1bca9bac88b98b8809cc2d9c145c93ef1ec8a7da",
            "patch": "@@ -3506,8 +3506,13 @@ def pad(\n \n         # If we have a list of dicts, let's convert it in a dict of lists\n         # We do this to allow using this method as a collate_fn function in PyTorch Dataloader\n-        if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n-            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0]}\n+        if (\n+            isinstance(encoded_inputs, (list, tuple))\n+            and len(encoded_inputs) > 0\n+            and isinstance(encoded_inputs[0], Mapping)\n+        ):\n+            # Call .keys() explicitly for compatibility with TensorDict and other Mapping subclasses\n+            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}\n \n         # The model's main input name, usually `input_ids`, has been passed for padding\n         if self.model_input_names[0] not in encoded_inputs:"
        }
    ],
    "stats": {
        "total": 21,
        "additions": 15,
        "deletions": 6
    }
}