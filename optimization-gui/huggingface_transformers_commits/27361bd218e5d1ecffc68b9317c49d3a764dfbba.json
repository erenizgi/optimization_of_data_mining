{
    "author": "jiqing-feng",
    "message": "fix xpu tests (#36656)\n\n* fix awq xpu tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* update\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix llava next video bnb tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "27361bd218e5d1ecffc68b9317c49d3a764dfbba",
    "files": [
        {
            "sha": "851249b270beec82152b04ae320f9b0c624712f8",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/27361bd218e5d1ecffc68b9317c49d3a764dfbba/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27361bd218e5d1ecffc68b9317c49d3a764dfbba/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=27361bd218e5d1ecffc68b9317c49d3a764dfbba",
            "patch": "@@ -902,6 +902,9 @@ def post_init(self):\n             )\n \n         if self.backend == AwqBackendPackingMethod.LLMAWQ:\n+            # Only cuda device can run this function\n+            if not torch.cuda.is_available():\n+                raise ValueError(\"LLM-AWQ backend is only supported on CUDA\")\n             compute_capability = torch.cuda.get_device_capability()\n             major, minor = compute_capability\n             if major < 8:"
        },
        {
            "sha": "ba7323a075519e072f9d88e3310cf847e9f68d15",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/27361bd218e5d1ecffc68b9317c49d3a764dfbba/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27361bd218e5d1ecffc68b9317c49d3a764dfbba/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=27361bd218e5d1ecffc68b9317c49d3a764dfbba",
            "patch": "@@ -430,13 +430,13 @@ def test_small_model_integration_test(self):\n \n         # verify generation\n         output = model.generate(**inputs, do_sample=False, max_new_tokens=40)\n-        EXPECTED_DECODED_TEXT = 'USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat comical situation of a young child reading a book while another child is attempting to read the same book. The child who is reading the book seems'  # fmt: skip\n-\n-        self.assertEqual(\n-            self.processor.decode(output[0], skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n+        EXPECTED_DECODED_TEXT = (\n+            \"USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat comical situation of a young child reading a book while another child is attempting to read the same book. The child who is reading the book seems\",  # cuda output\n+            \"USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat comical situation of a young child reading a book while wearing a pair of glasses that are too large for them. The glasses are\",  # xpu output\n         )\n \n+        self.assertTrue(self.processor.decode(output[0], skip_special_tokens=True) in EXPECTED_DECODED_TEXT)\n+\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_batch(self):"
        },
        {
            "sha": "d597f8de71a4e2bea7199af0753b45ef3b3de21f",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 12,
            "deletions": 7,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/27361bd218e5d1ecffc68b9317c49d3a764dfbba/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27361bd218e5d1ecffc68b9317c49d3a764dfbba/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=27361bd218e5d1ecffc68b9317c49d3a764dfbba",
            "patch": "@@ -59,16 +59,21 @@ def test_wrong_backend(self):\n         with self.assertRaises(ValueError):\n             AwqConfig(bits=4, backend=\"unexisting-backend\")\n \n-        compute_capability = torch.cuda.get_device_capability()\n-        major, minor = compute_capability\n-\n-        if major < 8:\n+        # Only cuda device can run this function\n+        support_llm_awq = False\n+        if torch.cuda.is_available():\n+            compute_capability = torch.cuda.get_device_capability()\n+            major, minor = compute_capability\n+            if major >= 8:\n+                support_llm_awq = True\n+\n+        if support_llm_awq:\n+            # LLMAWQ should work on an A100\n+            AwqConfig(bits=4, backend=\"llm-awq\")\n+        else:\n             # LLMAWQ does not work on a T4\n             with self.assertRaises(ValueError):\n                 AwqConfig(bits=4, backend=\"llm-awq\")\n-        else:\n-            # LLMAWQ should work on an A100\n-            AwqConfig(bits=4, backend=\"llm-awq\")\n \n     def test_to_dict(self):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 32,
        "additions": 20,
        "deletions": 12
    }
}