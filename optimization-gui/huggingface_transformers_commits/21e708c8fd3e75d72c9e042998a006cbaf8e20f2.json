{
    "author": "remi-or",
    "message": "Fix for missing default values in encoder decoder  (#40517)\n\n* Added default_value for is_updated and type check\n\n* Forgot one\n\n* Repo consistency",
    "sha": "21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
    "files": [
        {
            "sha": "b490ba96f6cdc111cff21f7ff2cbd37848b40299",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -136,6 +136,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -170,7 +171,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n@@ -266,6 +267,7 @@ def forward(\n             self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if past_key_values is not None:\n@@ -303,7 +305,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom"
        },
        {
            "sha": "dfa8fefea6ab7b69f8584e120fc8ed6c5ac19782",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -139,6 +139,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -173,7 +174,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n@@ -269,6 +270,7 @@ def forward(\n             self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if past_key_values is not None:\n@@ -306,7 +308,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom"
        },
        {
            "sha": "f3a62be4b1d04463d78861e7291237c75a7f8275",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -473,6 +473,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -502,7 +503,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)"
        },
        {
            "sha": "a52e3488f0b081bcc6c7cfc6c1637f25c666475d",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -217,6 +217,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -246,7 +247,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "f32fa309073f98e1546879c848d4ae1faf1b7713",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -234,6 +234,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -268,7 +269,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n@@ -363,6 +364,7 @@ def forward(\n             self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if past_key_values is not None:\n@@ -400,7 +402,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment"
        },
        {
            "sha": "fbfd43a1a16f8b13700f65e02a0e120adce2637d",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -97,6 +97,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -131,7 +132,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores."
        },
        {
            "sha": "175474e33d890d85852c1f45a43e4d63fcb1e4a4",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -1276,6 +1276,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -1305,7 +1306,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "542d7c0a0e1e6a059e6a0cf1a84bdf1b0f7b8f80",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -195,6 +195,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -224,7 +225,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "044ce18460c1f6854614d316e5ed3d93e8cc4569",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -216,6 +216,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -245,7 +246,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "496613ab3255e35e21a2999dfe5149c0d4bc35bc",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -200,6 +200,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -229,7 +230,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "acde2b26912fd01a51264b81ab97cae0449f1840",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -160,6 +160,7 @@ def forward(\n         is_cross_attention = encoder_hidden_states is not None\n         attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -195,7 +196,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores."
        },
        {
            "sha": "35853d7f1c8ac9e1aca42492f6f35023c31b10f2",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -447,6 +447,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -481,7 +482,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores."
        },
        {
            "sha": "f91ea45d622e3afd83d221b08bdd641bff9a7bd7",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -184,6 +184,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -218,7 +219,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n@@ -314,6 +315,7 @@ def forward(\n             self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if past_key_values is not None:\n@@ -351,7 +353,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment"
        },
        {
            "sha": "844d329c7e19b7d67942474ce96ed477e53e7172",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -185,6 +185,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -219,7 +220,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores."
        },
        {
            "sha": "c5366f1e50e53ddf1572a508a3742582a197b998",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -242,6 +242,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -276,7 +277,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores."
        },
        {
            "sha": "9bbc84b0a048fea7ed73eb512a1226b4ca541f21",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -171,6 +171,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -205,7 +206,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores."
        },
        {
            "sha": "79e2ca8e0bd4430ca6610d250f49b2160a439e0b",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -464,6 +464,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -493,7 +494,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n@@ -582,6 +583,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -611,7 +613,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)"
        },
        {
            "sha": "d69623b5647372648e352b85bece969ca92a20a1",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -270,6 +270,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -299,7 +300,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)"
        },
        {
            "sha": "bbbf31f8b43804b048242bebedf14d383f17730a",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -732,6 +732,7 @@ def forward(\n         query_states = self.q_proj(hidden_states)\n         query_states = query_states.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -761,7 +762,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "9354a7a7c5c05e64eb60f4cb6c8f7456102a124f",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -812,6 +812,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -841,7 +842,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)"
        },
        {
            "sha": "4e84a1550349fdce471529cf5d177bb98f35578a",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -470,7 +470,8 @@ def forward(\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n-        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n+        is_updated = False\n+        if isinstance(past_key_values, EncoderDecoderCache):\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n@@ -498,7 +499,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9"
        },
        {
            "sha": "e87893691c8bf7dcc656f8dc2c98de658fd691d1",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -282,6 +282,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -311,7 +312,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "1b2f0b9e85bff21a76fe28c6fa1f4fd652bcf642",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -217,6 +217,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -246,7 +247,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "20998929f9f9486c25231923e069983902e0a65c",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -226,6 +226,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -255,7 +256,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "0c4cb0f93f8ebd00c038359bbdbe8d29094c00e8",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -224,6 +224,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -258,7 +259,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores."
        },
        {
            "sha": "4e57d0aadda2a6793bc955e730732deb97c35a11",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -368,7 +368,8 @@ def forward(\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n-        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n+        is_updated = False\n+        if isinstance(past_key_values, EncoderDecoderCache):\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n@@ -396,7 +397,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9"
        },
        {
            "sha": "d16c32bd5cdfb3d6e16a7cc86182616a36e7b61a",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -250,6 +250,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -277,7 +278,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "bd103e36c034b10b3b1d02a636b563ce4f27068b",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -258,6 +258,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -285,7 +286,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "a0a0aa7af18cafb89bc46e6c40978d920521aac9",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -146,6 +146,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -175,7 +176,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         if attn_prompt is not None:"
        },
        {
            "sha": "5969229adc4a3087dc6cd70ba4d04d3aa866e46e",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -572,6 +572,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -599,7 +600,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "c32dc54f44c067fc0c32b57c057def8cf2232465",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -216,6 +216,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -245,7 +246,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "8f7472bb40528f3b35308d831ff41c3f9b43ca47",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -237,6 +237,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -266,7 +267,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "cb29bfd0cec1634b03135352558c069192572a64",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -401,6 +401,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -430,7 +431,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "ea6d3a5eea9e3a6f69f155c6e4a09b58d2fdd822",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -312,7 +312,8 @@ def forward(\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n-        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n+        is_updated = False\n+        if isinstance(past_key_values, EncoderDecoderCache):\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n@@ -340,7 +341,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9"
        },
        {
            "sha": "a79c803e77b94202ca3ab601d167d9dbd7ecccdc",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -463,6 +463,7 @@ def forward(\n         # previous time steps are cached - no need to recompute key and value if they are static\n         query_states = self.query_proj(hidden_states) / (self.head_dim**0.5)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -492,7 +493,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         query_states = query_states.view(batch_size, tgt_len, self.num_attn_heads, self.head_dim).transpose(1, 2)"
        },
        {
            "sha": "5c27186096debcf38890a9cd86daba9f6161a786",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -240,6 +240,7 @@ def forward(\n             .transpose(1, 2)\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -276,7 +277,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores."
        },
        {
            "sha": "105152a8f8dcdd2842fd6c9e1a7756e44e8cf796",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -183,6 +183,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -217,7 +218,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n@@ -313,6 +314,7 @@ def forward(\n             self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if past_key_values is not None:\n@@ -350,7 +352,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment"
        },
        {
            "sha": "3faebd368bb4582540584efee5306e452b3e632b",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -183,6 +183,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -217,7 +218,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores."
        },
        {
            "sha": "76c198ed0b82af0ea9480c45c9717a6279ac03dc",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -298,6 +298,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -332,7 +333,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores."
        },
        {
            "sha": "aa10b27d0f055c62d007be2a2a9604d00b6ef686",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -235,6 +235,7 @@ def forward(\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -281,7 +282,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores."
        },
        {
            "sha": "edff9d3121dfd7490bb2149312faceabdd634810",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -1050,6 +1050,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -1079,7 +1080,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)"
        },
        {
            "sha": "529b86a43d7a8c54a94602f816f1a607d2de2fcc",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -916,6 +916,7 @@ def forward(\n         is_cross_attention = encoder_hidden_states is not None\n         batch_size, seq_length = hidden_states.shape[:2]\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -945,7 +946,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         query_states = self.q_proj(hidden_states)"
        },
        {
            "sha": "b1a380bc99b6875ed1a2075f73d039b824ab0980",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -274,6 +274,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -301,7 +302,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "95489c3071bc7a089d4e34375032760db9618a57",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -900,6 +900,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -929,7 +930,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)"
        },
        {
            "sha": "0c28442509685d4490fd0931464ee58099c4e807",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -505,7 +505,8 @@ def forward(\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n-        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n+        is_updated = False\n+        if isinstance(past_key_values, EncoderDecoderCache):\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n@@ -533,7 +534,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9"
        },
        {
            "sha": "f3c6e3fb1a2a2a4d9c38ad936a8ee53f44ec5224",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -493,7 +493,8 @@ def forward(\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n-        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n+        is_updated = False\n+        if isinstance(past_key_values, EncoderDecoderCache):\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n@@ -521,7 +522,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9"
        },
        {
            "sha": "3f4bf53d1acca339baeb16adab681cabafd44257",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -319,6 +319,7 @@ def forward(\n             .transpose(1, 2)\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -355,7 +356,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores."
        },
        {
            "sha": "25abedadfe764c62e952f1f3ec74c95055d8efac",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -382,6 +382,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -411,7 +412,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward"
        },
        {
            "sha": "6bf7bfaf533c292e74eed91621e9ff1820edf974",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -200,6 +200,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -229,7 +230,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)"
        },
        {
            "sha": "3debe9c62cb4cc06eeed402e65a4011fd62060f3",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -591,7 +591,8 @@ def forward(\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n-        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n+        is_updated = False\n+        if isinstance(past_key_values, EncoderDecoderCache):\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n@@ -619,7 +620,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9"
        },
        {
            "sha": "bcf877e1da4ca78f021f4023df0e19f80b4fa3fa",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -277,6 +277,7 @@ def forward(\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n         # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n+        is_updated = False\n         if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n             is_updated = past_key_values.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n@@ -305,7 +306,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9"
        },
        {
            "sha": "b900c08316d8ebfeaaff3a1d44e39fbfc26e7847",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -157,6 +157,7 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n \n+        is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n                 is_updated = past_key_values.is_updated.get(self.layer_idx)\n@@ -186,7 +187,7 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)"
        },
        {
            "sha": "40def52d645e05ec4b743ece22a68e1c51369a60",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -184,6 +184,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -218,7 +219,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n@@ -314,6 +315,7 @@ def forward(\n             self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if past_key_values is not None:\n@@ -351,7 +353,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment"
        },
        {
            "sha": "c625ce7b53ea8769b12cbb921c998a647b006290",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -181,6 +181,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -215,7 +216,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n@@ -311,6 +312,7 @@ def forward(\n             self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if past_key_values is not None:\n@@ -348,7 +350,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment"
        },
        {
            "sha": "06cb898f09c31ff64b2a5e76328b84c53bcd7e15",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21e708c8fd3e75d72c9e042998a006cbaf8e20f2/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=21e708c8fd3e75d72c9e042998a006cbaf8e20f2",
            "patch": "@@ -182,6 +182,7 @@ def forward(\n             1, 2\n         )\n \n+        is_updated = False\n         is_cross_attention = encoder_hidden_states is not None\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -216,7 +217,7 @@ def forward(\n                     key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention:\n+                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n                     past_key_values.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores."
        }
    ],
    "stats": {
        "total": 201,
        "additions": 132,
        "deletions": 69
    }
}