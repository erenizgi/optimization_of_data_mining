{
    "author": "S1ro1",
    "message": "Allow `device_mesh` have multiple dim  (#38949)\n\n* Feat: something\n\n* Feat: initial changes\n\n* tmp changes to unblock\n\n* Refactor\n\n* remove todo\n\n* Feat: docstring\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "82603b6cc284dbdf2b7a7cf070feb6a2c3bb53cf",
    "files": [
        {
            "sha": "1577e7db584df5a25e15a1c45ed5c687dd40aaeb",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/82603b6cc284dbdf2b7a7cf070feb6a2c3bb53cf/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82603b6cc284dbdf2b7a7cf070feb6a2c3bb53cf/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=82603b6cc284dbdf2b7a7cf070feb6a2c3bb53cf",
            "patch": "@@ -4581,6 +4581,7 @@ def from_pretrained(\n                 A torch tensor parallel degree. If not provided would default to world size.\n             device_mesh (`torch.distributed.DeviceMesh`, *optional*):\n                 A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.\n+                If provided, it has to contain dimension named `\"tp\"` which will be used for tensor parallelism\n             offload_folder (`str` or `os.PathLike`, *optional*):\n                 If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n             offload_state_dict (`bool`, *optional*):\n@@ -4718,13 +4719,17 @@ def from_pretrained(\n         # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple\n         # `device_map` pointing to the correct device\n         if tp_plan is not None:\n-            if device_mesh is None and tp_plan is not None:\n+            if device_mesh is None:\n                 tp_plan, device_map, device_mesh = initialize_tensor_parallelism(tp_plan, tp_size=None)\n             else:\n-                # TODO: make device_mesh support multiple dimensions\n-                if device_mesh.ndim != 1:\n-                    raise ValueError(\"device_mesh must be 1 dimensional and will be used for TP\")\n-                device_map = torch.device(device_mesh.device_type, int(os.environ[\"LOCAL_RANK\"]))\n+                if \"tp\" not in device_mesh.mesh_dim_names:\n+                    raise ValueError(\n+                        \"When using `tp_plan`, the `device_mesh` must contain a 'tp' dimension. \"\n+                        \"Please provide a valid `device_mesh`.\"\n+                    )\n+                device_mesh = device_mesh[\"tp\"]\n+                tp_size = device_mesh[\"tp\"].size()\n+                device_map = torch.device(f\"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}\")\n \n             if tp_size is None:\n                 tp_size = torch.distributed.get_world_size()"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 10,
        "deletions": 5
    }
}