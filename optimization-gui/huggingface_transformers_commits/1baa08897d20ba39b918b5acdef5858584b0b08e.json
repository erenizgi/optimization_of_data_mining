{
    "author": "amyeroberts",
    "message": "Repo consistency fix after #33339 (#33873)\n\n* Repo consistency fix after #33339\r\n\r\n* [run-slow] omdet_turbo",
    "sha": "1baa08897d20ba39b918b5acdef5858584b0b08e",
    "files": [
        {
            "sha": "8387edf3c1f977f28c194c8fb575c502202089fb",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/1baa08897d20ba39b918b5acdef5858584b0b08e/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1baa08897d20ba39b918b5acdef5858584b0b08e/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=1baa08897d20ba39b918b5acdef5858584b0b08e",
            "patch": "@@ -418,29 +418,6 @@ def __init__(self, config: OmDetTurboConfig, num_heads: int, n_points: int):\n \n         self.disable_custom_kernels = config.disable_custom_kernels\n \n-        self._reset_parameters()\n-\n-    def _reset_parameters(self):\n-        nn.init.constant_(self.sampling_offsets.weight.data, 0.0)\n-        default_dtype = torch.get_default_dtype()\n-        thetas = torch.arange(self.n_heads, dtype=torch.int64).to(default_dtype) * (2.0 * math.pi / self.n_heads)\n-        grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n-        grid_init = (\n-            (grid_init / grid_init.abs().max(-1, keepdim=True)[0])\n-            .view(self.n_heads, 1, 1, 2)\n-            .repeat(1, self.n_levels, self.n_points, 1)\n-        )\n-        for i in range(self.n_points):\n-            grid_init[:, :, i, :] *= i + 1\n-        with torch.no_grad():\n-            self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n-        nn.init.constant_(self.attention_weights.weight.data, 0.0)\n-        nn.init.constant_(self.attention_weights.bias.data, 0.0)\n-        nn.init.xavier_uniform_(self.value_proj.weight.data)\n-        nn.init.constant_(self.value_proj.bias.data, 0.0)\n-        nn.init.xavier_uniform_(self.output_proj.weight.data)\n-        nn.init.constant_(self.output_proj.bias.data, 0.0)\n-\n     def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n         return tensor if position_embeddings is None else tensor + position_embeddings\n "
        }
    ],
    "stats": {
        "total": 23,
        "additions": 0,
        "deletions": 23
    }
}