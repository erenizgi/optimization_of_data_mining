{
    "author": "eljandoubi",
    "message": "Pass num_items_in_batch directly to loss computation (#36753)\n\n* Pass num_items_in_batch directly to loss computation\n\n* use self loss instead\n\n* fix loss kwrgs\n\n* fix vocab size",
    "sha": "e7337ee7be682c9601cc1bb2b189759e9bfb104e",
    "files": [
        {
            "sha": "6084c00bd12e115641dc8053bace823ac884ad22",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/e7337ee7be682c9601cc1bb2b189759e9bfb104e/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e7337ee7be682c9601cc1bb2b189759e9bfb104e/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=e7337ee7be682c9601cc1bb2b189759e9bfb104e",
            "patch": "@@ -21,7 +21,6 @@\n \n import torch\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationMixin\n@@ -582,6 +581,9 @@ def forward(\n         ```\"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        # num_items_in_batch is only needed for loss computation\n+        num_items_in_batch = kwargs.pop(\"num_items_in_batch\", None)\n+\n         kwargs_encoder = {argument: value for argument, value in kwargs.items() if not argument.startswith(\"decoder_\")}\n \n         kwargs_decoder = {\n@@ -638,8 +640,13 @@ def forward(\n         loss = None\n         if labels is not None:\n             logits = decoder_outputs.logits if return_dict else decoder_outputs[0]\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.reshape(-1, self.decoder.config.vocab_size), labels.reshape(-1))\n+\n+            loss = self.loss_function(\n+                logits=logits,\n+                labels=labels,\n+                vocab_size=self.decoder.config.vocab_size,\n+                num_items_in_batch=num_items_in_batch,\n+            )\n \n         if not return_dict:\n             if loss is not None:"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 10,
        "deletions": 3
    }
}