{
    "author": "zucchini-nlp",
    "message": "Pixtral: vectorize patch embeddings and enable tests (#35122)\n\n* initial POC\r\n\r\n* - batch mix feature\r\n\r\n* fix tests\r\n\r\n* fix tests\r\n\r\n* make style\r\n\r\n* do not skip and instead fix tests\r\n\r\n* update\r\n\r\n* return back the test\r\n\r\n* correct text with the correct ckpt",
    "sha": "9725e5be2fb17780ce1f645e5ba768aa78533526",
    "files": [
        {
            "sha": "67313c8f55d34dea2aee0942c2599d6ee806dd7f",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9725e5be2fb17780ce1f645e5ba768aa78533526/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9725e5be2fb17780ce1f645e5ba768aa78533526/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=9725e5be2fb17780ce1f645e5ba768aa78533526",
            "patch": "@@ -280,6 +280,7 @@ def get_image_features(\n         pixel_values: torch.FloatTensor,\n         vision_feature_layer: Union[int, List[int]],\n         vision_feature_select_strategy: str,\n+        **kwargs,\n     ):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n@@ -300,8 +301,9 @@ def get_image_features(\n         if vision_feature_select_strategy not in [\"default\", \"full\"]:\n             raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n \n+        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n         # this is not memory efficient at all (output_hidden_states=True) will save all the hidden states.\n-        image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n+        image_outputs = self.vision_tower(pixel_values, output_hidden_states=True, **kwargs)\n \n         # If we have one vision feature layer, return the corresponding hidden states,\n         # otherwise, select the hidden states of each feature layer and concatenate them\n@@ -422,6 +424,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        image_sizes: torch.Tensor = None,\n     ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -492,6 +495,7 @@ def forward(\n                 pixel_values=pixel_values,\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n+                image_sizes=image_sizes,\n             )\n \n             n_image_tokens = (input_ids == self.config.image_token_index).sum().item()"
        },
        {
            "sha": "969575d2e49ab899495b5fba60a569fb7ce07a9c",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 85,
            "deletions": 139,
            "changes": 224,
            "blob_url": "https://github.com/huggingface/transformers/blob/9725e5be2fb17780ce1f645e5ba768aa78533526/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9725e5be2fb17780ce1f645e5ba768aa78533526/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=9725e5be2fb17780ce1f645e5ba768aa78533526",
            "patch": "@@ -15,12 +15,13 @@\n \"\"\"Image processor class for Pixtral.\"\"\"\n \n import math\n-from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+from typing import Dict, List, Optional, Tuple, Union\n \n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n from ...image_transforms import (\n+    pad,\n     resize,\n     to_channel_dimension_format,\n )\n@@ -31,13 +32,13 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    is_valid_image,\n+    make_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_kwargs,\n     validate_preprocess_arguments,\n )\n-from ...utils import TensorType, is_torch_device, is_torch_dtype, is_vision_available, logging\n+from ...utils import TensorType, is_vision_available, logging\n from ...utils.import_utils import requires_backends\n \n \n@@ -48,91 +49,6 @@\n     import PIL\n \n \n-class BatchMixFeature(BatchFeature):\n-    def to(self, *args, **kwargs) -> \"BatchMixFeature\":\n-        \"\"\"\n-        Send all values to device by calling `v.to(*args, **kwargs)` (PyTorch only). This should support casting in\n-        different `dtypes` and sending the `BatchFeature` to a different `device`.\n-\n-        Args:\n-            args (`Tuple`):\n-                Will be passed to the `to(...)` function of the tensors.\n-            kwargs (`Dict`, *optional*):\n-                Will be passed to the `to(...)` function of the tensors.\n-\n-        Returns:\n-            [`BatchFeature`]: The same instance after modification.\n-        \"\"\"\n-\n-        def _recursive_to(obj, device, *args, **kwargs):\n-            # Lists can be nested, so keep digging until we hit tensors\n-            if isinstance(obj, list):\n-                return [_recursive_to(o, device, *args, **kwargs) for o in obj]\n-            # We cast only floating point tensors to avoid issues with tokenizers casting `LongTensor` to `FloatTensor`\n-            elif isinstance(obj, torch.Tensor) and torch.is_floating_point(obj):\n-                # cast and send to device\n-                return obj.to(*args, **kwargs)\n-            elif isinstance(obj, torch.Tensor) and device is not None:\n-                # only send to device, don't cast\n-                return obj.to(device=device)\n-            else:\n-                return obj\n-\n-        requires_backends(self, [\"torch\"])\n-        import torch  # noqa\n-\n-        device = kwargs.get(\"device\")\n-        # Check if the args are a device or a dtype\n-        if device is None and len(args) > 0:\n-            # device should be always the first argument\n-            arg = args[0]\n-            if is_torch_dtype(arg):\n-                # The first argument is a dtype\n-                pass\n-            elif isinstance(arg, str) or is_torch_device(arg) or isinstance(arg, int):\n-                device = arg\n-            else:\n-                # it's something else\n-                raise ValueError(f\"Attempting to cast a BatchFeature to type {str(arg)}. This is not supported.\")\n-\n-        self.data = {k: _recursive_to(v, device, *args, **kwargs) for k, v in self.data.items()}\n-        return self\n-\n-\n-# Copied from transformers.models.idefics2.image_processing_idefics2.make_list_of_images\n-def make_list_of_images(images: ImageInput) -> List[List[np.ndarray]]:\n-    \"\"\"\n-    Convert a single image or a list of images to a list of numpy arrays.\n-\n-    Args:\n-        images (`ImageInput`):\n-            A single image or a list of images.\n-\n-    Returns:\n-        A list of numpy arrays.\n-    \"\"\"\n-    # If it's a single image, convert it to a list of lists\n-    if is_valid_image(images):\n-        images = [[images]]\n-    # If it's a list of images, it's a single batch, so convert it to a list of lists\n-    elif isinstance(images, (list, tuple)) and len(images) > 0 and is_valid_image(images[0]):\n-        images = [images]\n-    # If it's a list of batches, it's already in the right format\n-    elif (\n-        isinstance(images, (list, tuple))\n-        and len(images) > 0\n-        and isinstance(images[0], (list, tuple))\n-        and len(images[0]) > 0\n-        and is_valid_image(images[0][0])\n-    ):\n-        pass\n-    else:\n-        raise ValueError(\n-            \"Invalid input type. Must be a single image, a list of images, or a list of batches of images.\"\n-        )\n-    return images\n-\n-\n # Adapted from function in image_transforms.py to ensure any transparent pixels are converted to white.\n def convert_to_rgb(image: ImageInput) -> ImageInput:\n     \"\"\"\n@@ -219,18 +135,6 @@ def get_resize_output_image_size(\n     return num_height_tokens * patch_height, num_width_tokens * patch_width\n \n \n-# Hack to get tensor conversion used in BatchFeature without batching the images\n-def _get_is_as_tensor_fns(tensor_type: Union[str, TensorType]) -> Tuple[Callable, Callable]:\n-    return BatchFeature()._get_is_as_tensor_fns(tensor_type)\n-\n-\n-def convert_to_tensor(array, tensor_type: Union[str, TensorType]) -> Any:\n-    is_tensor, as_tensor = _get_is_as_tensor_fns(tensor_type)\n-    if is_tensor(array):\n-        return array\n-    return as_tensor(array)\n-\n-\n class PixtralImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Pixtral image processor.\n@@ -368,6 +272,49 @@ def resize(\n             **kwargs,\n         )\n \n+    def _pad_for_batching(\n+        self,\n+        pixel_values: List[np.ndarray],\n+        image_sizes: List[List[int]],\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Pads images on the `num_of_patches` dimension with zeros to form a batch of same number of patches.\n+        Args:\n+            pixel_values (`List[np.ndarray]`):\n+                An array of pixel values of each images of shape (`batch_size`, `height`, `width`, `channels`)\n+            image_sizes (`List[List[int]]`):\n+                A list of sizes for each image in `pixel_values` in (height, width) format.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use the inferred format of the input image.\n+        Returns:\n+            List[`np.ndarray`]: The padded images.\n+        \"\"\"\n+\n+        max_shape = (\n+            max([size[0] for size in image_sizes]),\n+            max([size[1] for size in image_sizes]),\n+        )\n+        pixel_values = [\n+            pad(\n+                image,\n+                padding=((0, max_shape[0] - size[0]), (0, max_shape[1] - size[1])),\n+                data_format=data_format,\n+                input_data_format=input_data_format,\n+            )\n+            for image, size in zip(pixel_values, image_sizes)\n+        ]\n+        return pixel_values\n+\n     def preprocess(\n         self,\n         images: ImageInput,\n@@ -449,9 +396,9 @@ def preprocess(\n \n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n \n-        images_list = make_list_of_images(images)\n+        images = make_list_of_images(images)\n \n-        if not valid_images(images_list[0]):\n+        if not valid_images(images[0]):\n             raise ValueError(\n                 \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n                 \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n@@ -469,57 +416,56 @@ def preprocess(\n         )\n \n         if do_convert_rgb:\n-            images_list = [[convert_to_rgb(image) for image in images] for images in images_list]\n+            images = [convert_to_rgb(image) for image in images]\n \n         # All transformations expect numpy arrays.\n-        images_list = [[to_numpy_array(image) for image in images] for images in images_list]\n+        images = [to_numpy_array(image) for image in images]\n \n-        if do_rescale and is_scaled_image(images_list[0][0]):\n+        if do_rescale and is_scaled_image(images[0]):\n             logger.warning_once(\n                 \"It looks like you are trying to rescale already rescaled images. If the input\"\n                 \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n             )\n \n         if input_data_format is None:\n             # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(images_list[0][0])\n+            input_data_format = infer_channel_dimension_format(images[0])\n \n         batch_images = []\n         batch_image_sizes = []\n-        for sample_images in images_list:\n-            images = []\n-            image_sizes = []\n-            for image in sample_images:\n-                if do_resize:\n-                    image = self.resize(\n-                        image=image,\n-                        size=size,\n-                        patch_size=patch_size,\n-                        resample=resample,\n-                        input_data_format=input_data_format,\n-                    )\n-\n-                if do_rescale:\n-                    image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-\n-                if do_normalize:\n-                    image = self.normalize(\n-                        image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n-                    )\n-\n-                images.append(image)\n-                image_sizes.append(get_image_size(image, input_data_format))\n-            batch_images.append(images)\n-            batch_image_sizes.append(image_sizes)\n-\n-        images_list = [\n-            [to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images]\n-            for images in batch_images\n-        ]\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(\n+                    image=image,\n+                    size=size,\n+                    patch_size=patch_size,\n+                    resample=resample,\n+                    input_data_format=input_data_format,\n+                )\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+\n+            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+\n+            batch_images.append(image)\n+            batch_image_sizes.append(get_image_size(image, data_format))\n+\n+        pixel_values = self._pad_for_batching(\n+            pixel_values=batch_images,\n+            image_sizes=batch_image_sizes,\n+            input_data_format=data_format,\n+            data_format=data_format,\n+        )\n \n-        # Convert to tensor type outside of BatchFeature to avoid batching the images of different sizes\n-        images_list = [[convert_to_tensor(image, return_tensors) for image in images] for images in images_list]\n-        return BatchMixFeature(data={\"pixel_values\": images_list, \"image_sizes\": batch_image_sizes}, tensor_type=None)\n+        return BatchFeature(\n+            data={\"pixel_values\": pixel_values, \"image_sizes\": batch_image_sizes}, tensor_type=return_tensors\n+        )\n \n \n __all__ = [\"PixtralImageProcessor\"]"
        },
        {
            "sha": "1013c69176712a4f7466aea93e5c15b0959aaf26",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral_fast.py",
            "status": "modified",
            "additions": 87,
            "deletions": 60,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/9725e5be2fb17780ce1f645e5ba768aa78533526/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9725e5be2fb17780ce1f645e5ba768aa78533526/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py?ref=9725e5be2fb17780ce1f645e5ba768aa78533526",
            "patch": "@@ -16,7 +16,7 @@\n \n from typing import Dict, List, Optional, Union\n \n-from ...image_processing_utils import get_size_dict\n+from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import BaseImageProcessorFast\n from ...image_utils import (\n     ChannelDimension,\n@@ -26,6 +26,7 @@\n     get_image_size,\n     get_image_type,\n     infer_channel_dimension_format,\n+    make_list_of_images,\n     validate_fast_preprocess_arguments,\n     validate_kwargs,\n )\n@@ -38,10 +39,8 @@\n     logging,\n )\n from .image_processing_pixtral import (\n-    BatchMixFeature,\n     convert_to_rgb,\n     get_resize_output_image_size,\n-    make_list_of_images,\n )\n \n \n@@ -189,6 +188,36 @@ def resize(\n             **kwargs,\n         )\n \n+    # Adapted from transformers.models.pixtral.image_processing_pixtral.PixtralImageProcessor._pad_for_batching\n+    def _pad_for_batching(\n+        self,\n+        pixel_values: List[torch.Tensor],\n+        image_sizes: List[List[int]],\n+    ):\n+        \"\"\"\n+        Pads images on the `num_of_patches` dimension with zeros to form a batch of same number of patches.\n+        Args:\n+            pixel_values (`List[torch.Tensor]`):\n+                An array of pixel values of each images of shape (`batch_size`, `channels`, `height`, `width`)\n+            image_sizes (`List[List[int]]`):\n+                A list of sizes for each image in `pixel_values` in (height, width) format.\n+        Returns:\n+            List[`torch.Tensor`]: The padded images.\n+        \"\"\"\n+\n+        max_shape = (\n+            max([size[0] for size in image_sizes]),\n+            max([size[1] for size in image_sizes]),\n+        )\n+        pixel_values = [\n+            torch.nn.functional.pad(\n+                image,\n+                pad=(0, max_shape[1] - size[1], 0, max_shape[0] - size[0]),\n+            )\n+            for image, size in zip(pixel_values, image_sizes)\n+        ]\n+        return torch.stack(pixel_values)\n+\n     def preprocess(\n         self,\n         images: ImageInput,\n@@ -206,7 +235,7 @@ def preprocess(\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n         **kwargs,\n-    ) -> BatchMixFeature:\n+    ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or batch of images.\n \n@@ -271,8 +300,8 @@ def preprocess(\n \n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n \n-        images_list = make_list_of_images(images)\n-        image_type = get_image_type(images_list[0][0])\n+        images = make_list_of_images(images)\n+        image_type = get_image_type(images[0])\n \n         if image_type not in [ImageType.PIL, ImageType.TORCH, ImageType.NUMPY]:\n             raise ValueError(f\"Unsupported input image type {image_type}\")\n@@ -290,65 +319,63 @@ def preprocess(\n             data_format=data_format,\n         )\n \n-        if do_convert_rgb:\n-            images_list = [[convert_to_rgb(image) for image in images] for images in images_list]\n-\n-        if image_type == ImageType.PIL:\n-            images_list = [[F.pil_to_tensor(image) for image in images] for images in images_list]\n-        elif image_type == ImageType.NUMPY:\n-            # not using F.to_tensor as it doesn't handle (C, H, W) numpy arrays\n-            images_list = [[torch.from_numpy(image).contiguous() for image in images] for images in images_list]\n-\n-        if device is not None:\n-            images_list = [[image.to(device) for image in images] for images in images_list]\n-\n-        # We assume that all images have the same channel dimension format.\n-        if input_data_format is None:\n-            input_data_format = infer_channel_dimension_format(images_list[0][0])\n-        if input_data_format == ChannelDimension.LAST:\n-            images_list = [[image.permute(2, 0, 1).contiguous() for image in images] for images in images_list]\n-            input_data_format = ChannelDimension.FIRST\n-\n         if do_rescale and do_normalize:\n             # fused rescale and normalize\n-            new_mean = torch.tensor(image_mean, device=images_list[0][0].device) * (1.0 / rescale_factor)\n-            new_std = torch.tensor(image_std, device=images_list[0][0].device) * (1.0 / rescale_factor)\n+            new_mean = torch.tensor(image_mean, device=device) * (1.0 / rescale_factor)\n+            new_std = torch.tensor(image_std, device=device) * (1.0 / rescale_factor)\n \n         batch_images = []\n         batch_image_sizes = []\n-        for sample_images in images_list:\n-            images = []\n-            image_sizes = []\n-            for image in sample_images:\n-                if do_resize:\n-                    interpolation = (\n-                        pil_torch_interpolation_mapping[resample]\n-                        if isinstance(resample, (PILImageResampling, int))\n-                        else resample\n-                    )\n-                    image = self.resize(\n-                        image=image,\n-                        size=size,\n-                        patch_size=patch_size,\n-                        interpolation=interpolation,\n-                    )\n-\n-                if do_rescale and do_normalize:\n-                    # fused rescale and normalize\n-                    image = F.normalize(image.to(dtype=torch.float32), new_mean, new_std)\n-                elif do_rescale:\n-                    image = image * rescale_factor\n-                elif do_normalize:\n-                    image = F.normalize(image, image_mean, image_std)\n-\n-                images.append(image)\n-                image_sizes.append(get_image_size(image, input_data_format))\n-            batch_images.append(images)\n-            batch_image_sizes.append(image_sizes)\n-\n-        return BatchMixFeature(\n-            data={\"pixel_values\": batch_images, \"image_sizes\": batch_image_sizes},\n-            tensor_type=None,\n+        for image in images:\n+            if do_convert_rgb:\n+                image = convert_to_rgb(image)\n+\n+            if image_type == ImageType.PIL:\n+                image = F.pil_to_tensor(image)\n+            elif image_type == ImageType.NUMPY:\n+                # not using F.to_tensor as it doesn't handle (C, H, W) numpy arrays\n+                image = torch.from_numpy(image).contiguous()\n+\n+            # We assume that all images have the same channel dimension format.\n+            if input_data_format is None:\n+                input_data_format = infer_channel_dimension_format(image)\n+\n+            if input_data_format == ChannelDimension.LAST:\n+                image = image.permute(2, 0, 1).contiguous()\n+\n+            image = image.to(device)\n+\n+            if do_resize:\n+                interpolation = (\n+                    pil_torch_interpolation_mapping[resample]\n+                    if isinstance(resample, (PILImageResampling, int))\n+                    else resample\n+                )\n+                image = self.resize(\n+                    image=image,\n+                    size=size,\n+                    patch_size=patch_size,\n+                    interpolation=interpolation,\n+                )\n+\n+            if do_rescale and do_normalize:\n+                # fused rescale and normalize\n+                image = F.normalize(image.to(dtype=torch.float32), new_mean, new_std)\n+            elif do_rescale:\n+                image = image * rescale_factor\n+            elif do_normalize:\n+                image = F.normalize(image, image_mean, image_std)\n+\n+            batch_images.append(image)\n+            batch_image_sizes.append(get_image_size(image, ChannelDimension.FIRST))\n+\n+        pixel_values = self._pad_for_batching(\n+            pixel_values=batch_images,\n+            image_sizes=batch_image_sizes,\n+        )\n+\n+        return BatchFeature(\n+            data={\"pixel_values\": pixel_values, \"image_sizes\": batch_image_sizes}, tensor_type=return_tensors\n         )\n \n "
        },
        {
            "sha": "af41bab842594cb675f5b62a21f1705791a6035b",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 40,
            "deletions": 21,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/9725e5be2fb17780ce1f645e5ba768aa78533526/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9725e5be2fb17780ce1f645e5ba768aa78533526/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=9725e5be2fb17780ce1f645e5ba768aa78533526",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"PyTorch Pixtral model.\"\"\"\n \n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -57,7 +57,7 @@ class PixtralRotaryEmbedding(nn.Module):\n     a corresponding positional embedding, based on its index in the grid.\n     \"\"\"\n \n-    def __init__(self, config, device):\n+    def __init__(self, config, device=None):\n         super().__init__()\n         self.rope_type = \"default\"\n         self.dim = config.head_dim\n@@ -89,7 +89,6 @@ def forward(self, x, position_ids):\n \n         # Core RoPE block\n         freqs = self.inv_freq[position_ids]\n-        # position_ids_expanded = position_ids[:, None, :].float()\n         # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n@@ -175,7 +174,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        position_embeddings: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -261,8 +260,8 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        position_embeddings: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        output_attentions: Optional[bool] = None,\n     ) -> Tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n@@ -310,7 +309,7 @@ def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        position_embeddings: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -375,7 +374,7 @@ def forward(\n         if not return_dict:\n             return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=[hidden_states], attentions=all_attentions\n+            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n         )\n \n \n@@ -399,10 +398,9 @@ def forward(\n class PixtralPreTrainedModel(PreTrainedModel):\n     config_class = PixtralVisionConfig\n     base_model_prefix = \"model\"\n+    main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"PixtralVisionAttention\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n+    _no_split_modules = [\"PixtralAttentionLayer\"]\n \n     def _init_weights(self, module):\n         std = (\n@@ -426,6 +424,8 @@ def _init_weights(self, module):\n         pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n             Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See [`AutoImageProcessor.__call__`]\n             for details.\n+        image_sizes (`torch.LongTensor` of shape `(batch_size, 2)`, *optional*):\n+            The sizes of the images in the batch, being (height, width) for each image.\n         output_attentions (`bool`, *optional*):\n             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n             tensors for more detail.\n@@ -470,15 +470,22 @@ def __init__(self, config):\n             stride=config.patch_size,\n             bias=False,\n         )\n+        self.patch_size = config.patch_size\n         self.ln_pre = PixtralRMSNorm(config.hidden_size, eps=1e-5)\n         self.transformer = PixtralTransformer(config)\n-        self.patch_positional_embedding = PixtralRotaryEmbedding(config, device=self.device)\n+        self.patch_positional_embedding = PixtralRotaryEmbedding(config)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.patch_conv\n \n     @add_start_docstrings_to_model_forward(PIXTRAL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        pixel_values: List[torch.Tensor],\n-        output_hidden_states: Optional[bool] = False,\n+        pixel_values: torch.Tensor,\n+        image_sizes: torch.Tensor,\n+        output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         *args,\n@@ -490,24 +497,36 @@ def forward(\n                 all tokens of all images of shape (N_toks, D)\n         \"\"\"\n         # pass images through initial convolution independently\n-        if len(pixel_values) > 1:\n-            raise ValueError(\"Batching/padding not supported yet!\")\n-        patch_embeds_list = [self.patch_conv(img.to(self.dtype)) for sample in pixel_values for img in sample]\n+        patch_embeds = self.patch_conv(pixel_values)\n+        patch_embeds_list = [\n+            embed[..., : (size[0] // self.patch_size), : (size[1] // self.patch_size)]\n+            for embed, size in zip(patch_embeds, image_sizes)\n+        ]\n \n         # flatten to a single sequence\n         patch_embeds = torch.cat([p.flatten(1).T for p in patch_embeds_list], dim=0).unsqueeze(0)\n         patch_embeds = self.ln_pre(patch_embeds)\n+\n         # positional embeddings\n         position_ids = position_ids_in_meshgrid(\n             patch_embeds_list, max_width=self.config.image_size // self.config.patch_size\n-        ).to(self.device)\n-\n-        position_embedding = self.patch_positional_embedding(patch_embeds, position_ids)\n+        )\n+        position_embeddings = self.patch_positional_embedding(patch_embeds, position_ids)\n \n         attention_mask = generate_block_attention_mask(\n             [p.shape[-2] * p.shape[-1] for p in patch_embeds_list], patch_embeds\n         )\n-        return self.transformer(patch_embeds, attention_mask, position_embedding)\n+\n+        out = self.transformer(\n+            patch_embeds,\n+            attention_mask=attention_mask,\n+            position_embeddings=position_embeddings,\n+            output_hidden_states=output_hidden_states,\n+            output_attentions=output_attentions,\n+            return_dict=return_dict,\n+        )\n+\n+        return out\n \n \n __all__ = [\"PixtralVisionModel\", \"PixtralPreTrainedModel\"]"
        },
        {
            "sha": "aea6375f78bcb22a6c73c3d4d1b4e6e99ba6ca1f",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 14,
            "deletions": 78,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/9725e5be2fb17780ce1f645e5ba768aa78533526/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9725e5be2fb17780ce1f645e5ba768aa78533526/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=9725e5be2fb17780ce1f645e5ba768aa78533526",
            "patch": "@@ -22,7 +22,7 @@\n from ...image_utils import ImageInput, is_valid_image, load_image\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import is_torch_device, is_torch_dtype, logging, requires_backends\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n@@ -50,58 +50,6 @@ def is_image_or_image_url(elem):\n     return is_url(elem) or is_valid_image(elem)\n \n \n-# Copied from transformers.models.pixtral.image_processing_pixtral.BatchMixFeature\n-class BatchMixFeature(BatchFeature):\n-    def to(self, *args, **kwargs) -> \"BatchMixFeature\":\n-        \"\"\"\n-        Send all values to device by calling `v.to(*args, **kwargs)` (PyTorch only). This should support casting in\n-        different `dtypes` and sending the `BatchFeature` to a different `device`.\n-\n-        Args:\n-            args (`Tuple`):\n-                Will be passed to the `to(...)` function of the tensors.\n-            kwargs (`Dict`, *optional*):\n-                Will be passed to the `to(...)` function of the tensors.\n-\n-        Returns:\n-            [`BatchFeature`]: The same instance after modification.\n-        \"\"\"\n-\n-        def _recursive_to(obj, device, *args, **kwargs):\n-            # Lists can be nested, so keep digging until we hit tensors\n-            if isinstance(obj, list):\n-                return [_recursive_to(o, device, *args, **kwargs) for o in obj]\n-            # We cast only floating point tensors to avoid issues with tokenizers casting `LongTensor` to `FloatTensor`\n-            elif isinstance(obj, torch.Tensor) and torch.is_floating_point(obj):\n-                # cast and send to device\n-                return obj.to(*args, **kwargs)\n-            elif isinstance(obj, torch.Tensor) and device is not None:\n-                # only send to device, don't cast\n-                return obj.to(device=device)\n-            else:\n-                return obj\n-\n-        requires_backends(self, [\"torch\"])\n-        import torch  # noqa\n-\n-        device = kwargs.get(\"device\")\n-        # Check if the args are a device or a dtype\n-        if device is None and len(args) > 0:\n-            # device should be always the first argument\n-            arg = args[0]\n-            if is_torch_dtype(arg):\n-                # The first argument is a dtype\n-                pass\n-            elif isinstance(arg, str) or is_torch_device(arg) or isinstance(arg, int):\n-                device = arg\n-            else:\n-                # it's something else\n-                raise ValueError(f\"Attempting to cast a BatchFeature to type {str(arg)}. This is not supported.\")\n-\n-        self.data = {k: _recursive_to(v, device, *args, **kwargs) for k, v in self.data.items()}\n-        return self\n-\n-\n class PixtralProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a Pixtral processor which wraps a Pixtral image processor and a Pixtral tokenizer into a single processor.\n@@ -161,7 +109,7 @@ def __call__(\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[PixtralProcessorKwargs],\n-    ) -> BatchMixFeature:\n+    ) -> BatchFeature:\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n@@ -205,28 +153,16 @@ def __call__(\n \n         if images is not None:\n             if is_image_or_image_url(images):\n-                if isinstance(text, str) or isinstance(text, list) and len(text) == 1:\n-                    # If there's a single sample, the image must belong to it\n-                    images = [[images]]\n-                else:\n-                    raise ValueError(\n-                        \"You have supplied multiple text samples, but `images` is not a nested list. When processing multiple samples, `images` should be a list of lists of images, one list per sample.\"\n-                    )\n+                images = [images]\n             elif isinstance(images, list) and is_image_or_image_url(images[0]):\n-                if isinstance(text, str) or isinstance(text, list) and len(text) == 1:\n-                    # If there's a single sample, all images must belong to it\n-                    images = [images]\n-                else:\n-                    raise ValueError(\n-                        \"You have supplied multiple text samples, but `images` is not a nested list. When processing multiple samples, `images` should be a list of lists of images, one list per sample.\"\n-                    )\n-            elif isinstance(images, list) and isinstance(images[0], list) and is_image_or_image_url(images[0][0]):\n                 pass\n+            elif isinstance(images, list) and isinstance(images[0], list) and is_image_or_image_url(images[0][0]):\n+                images = [image for sublist in images for image in sublist]\n             else:\n                 raise ValueError(\n                     \"Invalid input images. Please provide a single image, a list of images, or a list of lists of images.\"\n                 )\n-            images = [[load_image(im) for im in sample] for sample in images]\n+            images = [load_image(im) if isinstance(im, str) else im for im in images]\n             image_inputs = self.image_processor(images, patch_size=self.patch_size, **output_kwargs[\"images_kwargs\"])\n         else:\n             image_inputs = {}\n@@ -240,15 +176,13 @@ def __call__(\n         prompt_strings = text\n         if image_inputs.get(\"pixel_values\") is not None:\n             # Replace the image token with the expanded image token sequence\n-            images = image_inputs[\"pixel_values\"]\n-            image_sizes = image_inputs.pop(\"image_sizes\")\n+            image_sizes = iter(image_inputs[\"image_sizes\"])\n             prompt_strings = []\n+            replace_strings = []\n \n-            for sample_images, sample_image_sizes, sample in zip(images, image_sizes, text):\n-                replace_strings = []\n-                # First calculate the number of tokens needed for each image and put in a placeholder\n-                for image, image_size in zip(sample_images, sample_image_sizes):\n-                    height, width = image_size\n+            for sample in text:\n+                while self.image_token in sample:\n+                    height, width = next(image_sizes)\n                     num_height_tokens = height // self.patch_size\n                     num_width_tokens = width // self.patch_size\n                     replace_tokens = [\n@@ -267,7 +201,9 @@ def __call__(\n                 prompt_strings.append(sample)\n \n         text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n-        return BatchMixFeature(data={**text_inputs, **image_inputs})\n+        return BatchFeature(\n+            data={**text_inputs, **image_inputs}, tensor_type=output_kwargs[\"common_kwargs\"][\"return_tensors\"]\n+        )\n \n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "5e19a74f221ad95edfe42edae98303345d154a6b",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 59,
            "deletions": 18,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/9725e5be2fb17780ce1f645e5ba768aa78533526/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9725e5be2fb17780ce1f645e5ba768aa78533526/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=9725e5be2fb17780ce1f645e5ba768aa78533526",
            "patch": "@@ -564,9 +564,8 @@ def test_generation_siglip_backbone(self):\n         self.assertTrue(processor.batch_decode(output, skip_special_tokens=True)[0] == EXPECTED_DECODED_TEXT)\n \n     @slow\n-    @require_bitsandbytes\n     def test_pixtral(self):\n-        model_id = \"hf-internal-testing/pixtral-12b\"\n+        model_id = \"mistral-community/pixtral-12b\"\n         model = LlavaForConditionalGeneration.from_pretrained(model_id)\n         processor = AutoProcessor.from_pretrained(model_id)\n \n@@ -579,33 +578,75 @@ def test_pixtral(self):\n         PROMPT = \"<s>[INST]Describe the images.\\n[IMG][IMG][IMG][IMG][/INST]\"\n \n         # image = Image.open(requests.get(url, stream=True).raw)\n-        inputs = processor(text=PROMPT, images=IMG_URLS, return_tensors=\"pt\").to(\"cuda\")\n+        inputs = processor(text=PROMPT, images=IMG_URLS, return_tensors=\"pt\").to(model.device)\n         generate_ids = model.generate(**inputs, max_new_tokens=500)\n         ouptut = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        print(ouptut)\n \n         # fmt: off\n         EXPECTED_GENERATION = \"\"\"\n Describe the images.\n-Sure, let's break down each image description:\n+Certainly! Here are the descriptions of the images:\n \n-1. **Image 1:**\n-   - **Description:** A black dog with a glossy coat is sitting on a wooden floor. The dog has a focused expression and is looking directly at the camera.\n-   - **Details:** The wooden floor has a rustic appearance with visible wood grain patterns. The dog's eyes are a striking color, possibly brown or amber, which contrasts with its black fur.\n+1. **Image 1**: This image features a black dog with a glossy coat sitting on a wooden surface. The dog has a calm and attentive expression, looking directly at the camera. The wooden background has a rustic appearance with visible grain and texture.\n \n-2. **Image 2:**\n-   - **Description:** A scenic view of a mountainous landscape with a winding road cutting through it. The road is surrounded by lush green vegetation and leads to a distant valley.\n-   - **Details:** The mountains are rugged with steep slopes, and the sky is clear, indicating good weather. The winding road adds a sense of depth and perspective to the image.\n+2. **Image 2**: This image captures a breathtaking view of a mountainous landscape. The mountains are rugged and covered with patches of green vegetation. The sky above is clear, and the scene conveys a sense of tranquility and natural beauty.\n \n-3. **Image 3:**\n-   - **Description:** A beach scene with waves crashing against the shore. There are several people in the water and on the beach, enjoying the waves and the sunset.\n-   - **Details:** The waves are powerful, creating a dynamic and lively atmosphere. The sky is painted with hues of orange and pink from the setting sun, adding a warm glow to the scene.\n+3. **Image 3**: This image shows a beach scene during sunset. The waves are gently rolling onto the shore, and several people can be seen in the water, possibly surfing or swimming. The sky is painted with warm hues of orange and yellow, creating a serene and picturesque atmosphere.\n \n-4. **Image 4:**\n-   - **Description:** A garden path leading to a large tree with a bench underneath it. The path is bordered by well-maintained grass and flowers.\n-   - **Details:** The path is made of small stones or gravel, and the tree provides a shaded area with the bench invitingly placed beneath it. The surrounding area is lush and green, suggesting a well-kept garden.\n+4. **Image 4**: This image depicts a narrow, winding path that cuts through a lush, green landscape. On either side of the path, there is dense grass and various trees, including a prominent tree with white blossoms. The sky is clear and blue, adding to the peaceful and inviting ambiance of the scene.\n \n-Each image captures a different scene, from a close-up of a dog to expansive natural landscapes, showcasing various elements of nature and human interaction with it.\n+These descriptions provide a detailed overview of the content and atmosphere of each image.\n \"\"\"\n         # fmt: on\n         # check that both inputs are handled correctly and generate the same output\n-        self.assertListEqual(ouptut, EXPECTED_GENERATION)\n+        self.assertEqual(ouptut, EXPECTED_GENERATION)\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_pixtral_4bit(self):\n+        model_id = \"mistral-community/pixtral-12b\"\n+        model = LlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        IMG_URLS = [\n+            Image.open(requests.get(\"https://picsum.photos/id/237/400/300\", stream=True).raw),\n+            Image.open(requests.get(\"https://picsum.photos/id/231/200/300\", stream=True).raw),\n+        ]\n+        PROMPT = \"<s>[INST][IMG][IMG]Describe the images.[/INST]\"\n+\n+        inputs = processor(text=PROMPT, images=IMG_URLS, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        generate_ids = model.generate(**inputs, max_new_tokens=50)\n+        output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+\n+        EXPECTED_GENERATION = \"Describe the images.The image showcases a dog, which is prominently positioned in the center, taking up a significant portion of the frame. The dog is situated against a backdrop of a wooden surface, which spans the entire image. The dog appears to be a black Labrador\"  # fmt: skip\n+        self.assertEqual(output, EXPECTED_GENERATION)\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_pixtral_batched(self):\n+        model_id = \"mistral-community/pixtral-12b\"\n+        model = LlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        processor = AutoProcessor.from_pretrained(model_id)\n+        processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n+\n+        IMG_URLS = [\n+            Image.open(requests.get(\"https://picsum.photos/id/237/400/300\", stream=True).raw),\n+            Image.open(requests.get(\"https://picsum.photos/id/17/150/500\", stream=True).raw),\n+        ]\n+        PROMPT = [\n+            \"<s>[INST][IMG]What breed is the dog?[/INST]\",\n+            \"<s>[INST][IMG]What is shown in this image?[/INST]\",\n+        ]\n+\n+        inputs = processor(text=PROMPT, images=IMG_URLS, padding=True, return_tensors=\"pt\").to(\n+            torch_device, torch.float16\n+        )\n+        generate_ids = model.generate(**inputs, max_new_tokens=50)\n+        output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n+\n+        EXPECTED_GENERATION = [\n+            'What breed is the dog?The dog in the image is a black Labrador Retriever.',\n+            'What is shown in this image?The image depicts a narrow, winding dirt path surrounded by lush greenery. The path is flanked by grass and shrubs on both sides. On the left side, there are tall trees and dense foliage, while on the right side, there'\n+        ]  # fmt: skip\n+        self.assertEqual(output, EXPECTED_GENERATION)"
        },
        {
            "sha": "cc3fbba3d275cc9fd90dcae4588feeba73e1b5b5",
            "filename": "tests/models/pixtral/test_image_processing_pixtral.py",
            "status": "modified",
            "additions": 56,
            "deletions": 74,
            "changes": 130,
            "blob_url": "https://github.com/huggingface/transformers/blob/9725e5be2fb17780ce1f645e5ba768aa78533526/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9725e5be2fb17780ce1f645e5ba768aa78533526/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py?ref=9725e5be2fb17780ce1f645e5ba768aa78533526",
            "patch": "@@ -13,7 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import random\n import time\n import unittest\n \n@@ -92,49 +91,47 @@ def prepare_image_processor_dict(self):\n             \"do_convert_rgb\": self.do_convert_rgb,\n         }\n \n-    def expected_output_image_shape(self, image):\n-        if isinstance(image, Image.Image):\n-            width, height = image.size\n-        elif isinstance(image, np.ndarray):\n-            height, width = image.shape[:2]\n-        elif isinstance(image, torch.Tensor):\n-            height, width = image.shape[-2:]\n+    def expected_output_image_shape(self, images):\n+        if not isinstance(images, (list, tuple)):\n+            images = [images]\n \n-        max_height = max_width = self.size.get(\"longest_edge\")\n+        batch_size = len(images)\n+        return_height, return_width = 0, 0\n+        for image in images:\n+            if isinstance(image, Image.Image):\n+                width, height = image.size\n+            elif isinstance(image, np.ndarray):\n+                height, width = image.shape[:2]\n+            elif isinstance(image, torch.Tensor):\n+                height, width = image.shape[-2:]\n \n-        ratio = max(height / max_height, width / max_width)\n-        if ratio > 1:\n-            height = int(np.ceil(height / ratio))\n-            width = int(np.ceil(width / ratio))\n+            max_height = max_width = self.size.get(\"longest_edge\")\n \n-        patch_height, patch_width = self.patch_size[\"height\"], self.patch_size[\"width\"]\n-        num_height_tokens = (height - 1) // patch_height + 1\n-        num_width_tokens = (width - 1) // patch_width + 1\n+            ratio = max(height / max_height, width / max_width)\n+            if ratio > 1:\n+                height = int(np.ceil(height / ratio))\n+                width = int(np.ceil(width / ratio))\n \n-        height = num_height_tokens * patch_height\n-        width = num_width_tokens * patch_width\n+            patch_height, patch_width = self.patch_size[\"height\"], self.patch_size[\"width\"]\n+            num_height_tokens = (height - 1) // patch_height + 1\n+            num_width_tokens = (width - 1) // patch_width + 1\n \n-        return self.num_channels, height, width\n+            return_height = max(num_height_tokens * patch_height, return_height)\n+            return_width = max(num_width_tokens * patch_width, return_width)\n+\n+        return batch_size, self.num_channels, return_height, return_width\n \n     def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n-        # Use prepare_image_inputs to make a list of list of single images\n-\n-        images_list = []\n-        for _ in range(self.batch_size):\n-            images = []\n-            for _ in range(random.randint(1, self.max_num_images_per_sample)):\n-                img = prepare_image_inputs(\n-                    batch_size=1,\n-                    num_channels=self.num_channels,\n-                    min_resolution=self.min_resolution,\n-                    max_resolution=self.max_resolution,\n-                    equal_resolution=equal_resolution,\n-                    numpify=numpify,\n-                    torchify=torchify,\n-                )[0]\n-                images.append(img)\n-            images_list.append(images)\n-        return images_list\n+        images = prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+        return images\n \n \n @require_torch\n@@ -173,71 +170,56 @@ def test_call_pil(self):\n             image_processing = image_processing_class(**self.image_processor_dict)\n             # create random PIL images\n             image_inputs_list = self.image_processor_tester.prepare_image_inputs()\n-            for image_inputs in image_inputs_list:\n-                for image in image_inputs:\n-                    self.assertIsInstance(image, Image.Image)\n+            for image in image_inputs_list:\n+                self.assertIsInstance(image, Image.Image)\n \n             # Test not batched input\n-            encoded_images = image_processing(image_inputs_list[0][0], return_tensors=\"pt\").pixel_values\n-            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(\n-                image_inputs_list[0][0]\n-            )\n-            self.assertEqual(tuple(encoded_images[0][0].shape), expected_output_image_shape)\n+            encoded_images = image_processing(image_inputs_list[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs_list[0])\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n             # Test batched\n-            batch_encoded_images = image_processing(image_inputs_list, return_tensors=\"pt\").pixel_values\n-            for encoded_images, images in zip(batch_encoded_images, image_inputs_list):\n-                for encoded_image, image in zip(encoded_images, images):\n-                    expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image)\n-                    self.assertEqual(tuple(encoded_image.shape), expected_output_image_shape)\n+            encoded_images = image_processing(image_inputs_list, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs_list)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n     def test_call_numpy(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n             image_processing = image_processing_class(**self.image_processor_dict)\n             # create random numpy tensors\n             image_inputs_list = self.image_processor_tester.prepare_image_inputs(numpify=True)\n-            for image_inputs in image_inputs_list:\n-                for image in image_inputs:\n-                    self.assertIsInstance(image, np.ndarray)\n+            for image in image_inputs_list:\n+                self.assertIsInstance(image, np.ndarray)\n \n             # Test not batched input\n-            encoded_images = image_processing(image_inputs_list[0][0], return_tensors=\"pt\").pixel_values\n-            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(\n-                image_inputs_list[0][0]\n-            )\n-            self.assertEqual(tuple(encoded_images[0][0].shape), expected_output_image_shape)\n+            encoded_images = image_processing(image_inputs_list[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs_list[0])\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n             # Test batched\n             batch_encoded_images = image_processing(image_inputs_list, return_tensors=\"pt\").pixel_values\n-            for encoded_images, images in zip(batch_encoded_images, image_inputs_list):\n-                for encoded_image, image in zip(encoded_images, images):\n-                    expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image)\n-                    self.assertEqual(tuple(encoded_image.shape), expected_output_image_shape)\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs_list)\n+            self.assertEqual(tuple(batch_encoded_images.shape), expected_output_image_shape)\n \n     def test_call_pytorch(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n             image_processing = image_processing_class(**self.image_processor_dict)\n             # create random PyTorch tensors\n             image_inputs_list = self.image_processor_tester.prepare_image_inputs(torchify=True)\n-            for image_inputs in image_inputs_list:\n-                for image in image_inputs:\n-                    self.assertIsInstance(image, torch.Tensor)\n+            for image in image_inputs_list:\n+                self.assertIsInstance(image, torch.Tensor)\n \n             # Test not batched input\n-            encoded_images = image_processing(image_inputs_list[0][0], return_tensors=\"pt\").pixel_values\n-            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(\n-                image_inputs_list[0][0]\n-            )\n-            self.assertEqual(tuple(encoded_images[0][0].shape), expected_output_image_shape)\n+            encoded_images = image_processing(image_inputs_list[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs_list[0])\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n             # Test batched\n             batch_encoded_images = image_processing(image_inputs_list, return_tensors=\"pt\").pixel_values\n-            for encoded_images, images in zip(batch_encoded_images, image_inputs_list):\n-                for encoded_image, image in zip(encoded_images, images):\n-                    expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image)\n-                    self.assertEqual(tuple(encoded_image.shape), expected_output_image_shape)\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs_list)\n+            self.assertEqual(tuple(batch_encoded_images.shape), expected_output_image_shape)\n \n     @require_vision\n     @require_torch"
        },
        {
            "sha": "f254d9eecd04cff7db2394bd5f56f5ff1a82ce30",
            "filename": "tests/models/pixtral/test_modeling_pixtral.py",
            "status": "modified",
            "additions": 15,
            "deletions": 109,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/9725e5be2fb17780ce1f645e5ba768aa78533526/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9725e5be2fb17780ce1f645e5ba768aa78533526/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py?ref=9725e5be2fb17780ce1f645e5ba768aa78533526",
            "patch": "@@ -74,15 +74,17 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.scope = scope\n \n-        # in ViT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n-        num_patches = (image_size // patch_size) ** 2\n-        self.seq_length = num_patches + 1\n+        # in Pixtral, the seq length equals the number of patches * batch_size because the patches are flattened\n+        self.seq_length = (image_size // patch_size) ** 2 * batch_size\n \n     def prepare_config_and_inputs(self):\n         pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        image_sizes = torch.tensor(\n+            [[self.image_size, self.image_size]] * self.batch_size, dtype=torch.long, device=torch_device\n+        )\n         config = self.get_config()\n \n-        return config, pixel_values\n+        return config, pixel_values, image_sizes\n \n     def get_config(self):\n         return PixtralVisionConfig(\n@@ -127,8 +129,8 @@ def create_and_check_model_with_projection(self, config, pixel_values):\n \n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n-        config, pixel_values = config_and_inputs\n-        inputs_dict = {\"pixel_values\": pixel_values}\n+        config, pixel_values, image_sizes = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values, \"image_sizes\": image_sizes}\n         return config, inputs_dict\n \n \n@@ -142,113 +144,17 @@ class PixtralVisionModelModelTest(ModelTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_head_masking = False\n     test_torchscript = False\n+    test_resize_embeddings = False\n \n     def setUp(self):\n         self.model_tester = PixtralVisionModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=PixtralVisionConfig, has_text_modality=False)\n \n-    @unittest.skip(\"model does not support input embeds\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    @unittest.skip(\"model does not support input embeds\")\n-    def test_inputs_embeds_matches_input_ids(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing_use_reentrant(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing_use_reentrant_false(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Compile not yet supported because in Pixtral models\")\n-    def test_sdpa_can_compile_dynamic(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Compile not yet supported because in Pixtral models\")\n-    def test_sdpa_can_dispatch_on_flash(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_attention_outputs(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_cpu_offload(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_batching_equivalence(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_disk_offload_bin(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_retain_grad_hidden_states_attentions(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_multi_gpu_data_parallel_forward(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_model_parallelism(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_model_outputs_equivalence(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_save_load(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n     def test_model_get_set_embeddings(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_resize_tokens_embeddings(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_model_main_input_name(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_initialization(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_hidden_states_output(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_gradient_checkpointing_backward_compatibility(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_feed_forward_chunking(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_disk_offload_safetensors(self):\n-        pass\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n-    @unittest.skip(reason=\"Not supported yet\")\n-    def test_determinism(self):\n-        pass\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (torch.nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, torch.nn.Linear))"
        },
        {
            "sha": "a678e7c0102c1d4f35e0068244b63b788bc20f5d",
            "filename": "tests/models/pixtral/test_processor_pixtral.py",
            "status": "modified",
            "additions": 64,
            "deletions": 52,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/9725e5be2fb17780ce1f645e5ba768aa78533526/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9725e5be2fb17780ce1f645e5ba768aa78533526/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py?ref=9725e5be2fb17780ce1f645e5ba768aa78533526",
            "patch": "@@ -14,7 +14,6 @@\n import shutil\n import tempfile\n import unittest\n-from typing import Optional\n \n import requests\n import torch\n@@ -28,7 +27,7 @@\n if is_vision_available():\n     from PIL import Image\n \n-    from transformers import AutoTokenizer, PixtralImageProcessor, PixtralProcessor\n+    from transformers import PixtralProcessor\n \n \n @require_vision\n@@ -46,20 +45,15 @@ def setUpClass(cls):\n \n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n-\n-        # FIXME - just load the processor directly from the checkpoint\n-        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/pixtral-12b\")\n-        image_processor = PixtralImageProcessor()\n-        processor = PixtralProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+        processor = PixtralProcessor.from_pretrained(\"mistral-community/pixtral-12b\")\n         processor.save_pretrained(self.tmpdirname)\n \n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    @unittest.skip(\"No chat template was set for this model (yet)\")\n     def test_chat_template(self):\n         processor = self.processor_class.from_pretrained(self.tmpdirname)\n-        expected_prompt = \"USER: [IMG]\\nWhat is shown in this image? ASSISTANT:\"\n+        expected_prompt = \"<s>[INST][IMG]What is shown in this image?[/INST]\"\n \n         messages = [\n             {\n@@ -73,13 +67,12 @@ def test_chat_template(self):\n         formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n         self.assertEqual(expected_prompt, formatted_prompt)\n \n-    @unittest.skip(\"No chat template was set for this model (yet)\")\n     def test_image_token_filling(self):\n         processor = self.processor_class.from_pretrained(self.tmpdirname)\n         # Important to check with non square image\n         image = torch.randint(0, 2, (3, 500, 316))\n-        expected_image_tokens = 1526\n-        image_token_index = 32000\n+        expected_image_tokens = 640\n+        image_token_index = 10\n \n         messages = [\n             {\n@@ -111,11 +104,8 @@ def test_processor_with_single_image(self):\n         self.assertIn(\"input_ids\", inputs_image)\n         self.assertTrue(len(inputs_image[\"input_ids\"]) == 1)\n         self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n-        self.assertIsInstance(inputs_image[\"pixel_values\"], list)\n-        self.assertTrue(len(inputs_image[\"pixel_values\"]) == 1)\n-        self.assertIsInstance(inputs_image[\"pixel_values\"][0], list)\n-        self.assertTrue(len(inputs_image[\"pixel_values\"][0]) == 1)\n-        self.assertIsInstance(inputs_image[\"pixel_values\"][0][0], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 32, 32]))\n \n         # fmt: off\n         input_ids = inputs_image[\"input_ids\"]\n@@ -131,11 +121,8 @@ def test_processor_with_single_image(self):\n         self.assertIn(\"input_ids\", inputs_url)\n         self.assertTrue(len(inputs_url[\"input_ids\"]) == 1)\n         self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n-        self.assertIsInstance(inputs_url[\"pixel_values\"], list)\n-        self.assertTrue(len(inputs_url[\"pixel_values\"]) == 1)\n-        self.assertIsInstance(inputs_url[\"pixel_values\"][0], list)\n-        self.assertTrue(len(inputs_url[\"pixel_values\"][0]) == 1)\n-        self.assertIsInstance(inputs_url[\"pixel_values\"][0][0], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 32, 32]))\n \n         # fmt: off\n         input_ids = inputs_url[\"input_ids\"]\n@@ -146,6 +133,28 @@ def test_processor_with_single_image(self):\n         )\n         # fmt: on\n \n+        # Test passing inputs as a single list\n+        inputs_image = processor(text=prompt_string, images=[self.image_0], return_tensors=\"pt\")\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 32, 32]))\n+\n+        # fmt: off\n+        self.assertEqual(\n+            inputs_image[\"input_ids\"][0].tolist(),\n+            [21510,  1058,  1032,    10,    10,    12,    10,    10,    13,  1010, 7493,  1681,  1278,  4701,  1307,  1278,  3937,  1063,  1349,  4290, 16002, 41150,  1058]\n+        )\n+        # fmt: on\n+\n+        # Test as nested single list\n+        inputs_image = processor(text=prompt_string, images=[[self.image_0]], return_tensors=\"pt\")\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 32, 32]))\n+\n+        # fmt: off\n+        self.assertEqual(\n+            inputs_image[\"input_ids\"][0].tolist(),\n+            [21510,  1058,  1032,    10,    10,    12,    10,    10,    13,  1010, 7493,  1681,  1278,  4701,  1307,  1278,  3937,  1063,  1349,  4290, 16002, 41150,  1058]\n+        )\n+        # fmt: on\n+\n     def test_processor_with_multiple_images_single_list(self):\n         processor = self.processor_class.from_pretrained(self.tmpdirname)\n         prompt_string = \"USER: [IMG][IMG]\\nWhat's the difference between these two images? ASSISTANT:\"\n@@ -159,11 +168,8 @@ def test_processor_with_multiple_images_single_list(self):\n         self.assertIn(\"input_ids\", inputs_image)\n         self.assertTrue(len(inputs_image[\"input_ids\"]) == 1)\n         self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n-        self.assertIsInstance(inputs_image[\"pixel_values\"], list)\n-        self.assertTrue(len(inputs_image[\"pixel_values\"]) == 1)\n-        self.assertIsInstance(inputs_image[\"pixel_values\"][0], list)\n-        self.assertTrue(len(inputs_image[\"pixel_values\"][0]) == 2)\n-        self.assertIsInstance(inputs_image[\"pixel_values\"][0][0], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([2, 3, 32, 32]))\n \n         # fmt: off\n         input_ids = inputs_image[\"input_ids\"]\n@@ -179,11 +185,9 @@ def test_processor_with_multiple_images_single_list(self):\n         self.assertIn(\"input_ids\", inputs_url)\n         self.assertTrue(len(inputs_url[\"input_ids\"]) == 1)\n         self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n-        self.assertIsInstance(inputs_url[\"pixel_values\"], list)\n-        self.assertTrue(len(inputs_url[\"pixel_values\"]) == 1)\n-        self.assertIsInstance(inputs_url[\"pixel_values\"][0], list)\n-        self.assertTrue(len(inputs_url[\"pixel_values\"][0]) == 2)\n-        self.assertIsInstance(inputs_url[\"pixel_values\"][0][0], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([2, 3, 32, 32]))\n+\n         # fmt: off\n         input_ids = inputs_url[\"input_ids\"]\n         self.assertEqual(\n@@ -193,6 +197,17 @@ def test_processor_with_multiple_images_single_list(self):\n         )\n         # fmt: on\n \n+        # Test passing in as a nested list\n+        inputs_url = processor(text=prompt_string, images=[[self.image_0, self.image_1]], return_tensors=\"pt\")\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([2, 3, 32, 32]))\n+\n+        # fmt: off\n+        self.assertEqual(\n+            inputs_url[\"input_ids\"][0].tolist(),\n+            [21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+        )\n+        # fmt: on\n+\n     def test_processor_with_multiple_images_multiple_lists(self):\n         processor = self.processor_class.from_pretrained(self.tmpdirname)\n         prompt_string = [\n@@ -211,11 +226,8 @@ def test_processor_with_multiple_images_multiple_lists(self):\n         self.assertIn(\"input_ids\", inputs_image)\n         self.assertTrue(len(inputs_image[\"input_ids\"]) == 2)\n         self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n-        self.assertIsInstance(inputs_image[\"pixel_values\"], list)\n-        self.assertTrue(len(inputs_image[\"pixel_values\"]) == 2)\n-        self.assertIsInstance(inputs_image[\"pixel_values\"][0], list)\n-        self.assertTrue(len(inputs_image[\"pixel_values\"][0]) == 2)\n-        self.assertIsInstance(inputs_image[\"pixel_values\"][0][0], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([3, 3, 32, 32]))\n \n         # fmt: off\n         input_ids = inputs_image[\"input_ids\"]\n@@ -231,11 +243,8 @@ def test_processor_with_multiple_images_multiple_lists(self):\n         self.assertIn(\"input_ids\", inputs_url)\n         self.assertTrue(len(inputs_url[\"input_ids\"]) == 2)\n         self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n-        self.assertIsInstance(inputs_url[\"pixel_values\"], list)\n-        self.assertTrue(len(inputs_url[\"pixel_values\"]) == 2)\n-        self.assertIsInstance(inputs_url[\"pixel_values\"][0], list)\n-        self.assertTrue(len(inputs_url[\"pixel_values\"][0]) == 2)\n-        self.assertIsInstance(inputs_url[\"pixel_values\"][0][0], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([3, 3, 32, 32]))\n \n         # fmt: off\n         input_ids = inputs_url[\"input_ids\"]\n@@ -246,6 +255,19 @@ def test_processor_with_multiple_images_multiple_lists(self):\n         )\n         # fmt: on\n \n+        # Test passing as a single flat list\n+        inputs_image = processor(\n+            text=prompt_string, images=[self.image_0, self.image_1, self.image_2], return_tensors=\"pt\", padding=True\n+        )\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([3, 3, 32, 32]))\n+\n+        # fmt: off\n+        self.assertEqual(\n+            inputs_image[\"input_ids\"][0].tolist(),\n+            [21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+        )\n+        # fmt: on\n+\n     def test_processor_returns_full_length_batches(self):\n         # to avoid https://github.com/huggingface/transformers/issues/34204\n         processor = self.processor_class.from_pretrained(self.tmpdirname)\n@@ -264,13 +286,3 @@ def test_processor_returns_full_length_batches(self):\n         self.assertIn(\"input_ids\", inputs_image)\n         self.assertTrue(len(inputs_image[\"input_ids\"]) == 5)\n         self.assertTrue(len(inputs_image[\"pixel_values\"]) == 5)\n-\n-    # Override as PixtralProcessor needs nested images to work properly with batched inputs\n-    @require_vision\n-    def prepare_image_inputs(self, batch_size: Optional[int] = None):\n-        \"\"\"This function prepares a list of PIL images for testing\"\"\"\n-        if batch_size is None:\n-            return super().prepare_image_inputs()\n-        if batch_size < 1:\n-            raise ValueError(\"batch_size must be greater than 0\")\n-        return [[super().prepare_image_inputs()]] * batch_size"
        },
        {
            "sha": "ba996a966cc8d50477ad73a59dd6c2cfd5093732",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9725e5be2fb17780ce1f645e5ba768aa78533526/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9725e5be2fb17780ce1f645e5ba768aa78533526/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=9725e5be2fb17780ce1f645e5ba768aa78533526",
            "patch": "@@ -2991,6 +2991,10 @@ def test_inputs_embeds(self):\n             model.to(torch_device)\n             model.eval()\n \n+            model_forward_args = inspect.signature(model.forward).parameters\n+            if \"inputs_embeds\" not in model_forward_args:\n+                self.skipTest(reason=\"This model doesn't use `inputs_embeds`\")\n+\n             inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n \n             if not self.is_encoder_decoder:"
        }
    ],
    "stats": {
        "total": 981,
        "additions": 429,
        "deletions": 552
    }
}