{
    "author": "rishub-tamirisa",
    "message": "Add back `_tp_plan` attribute (#39944)\n\n* Update modeling_utils.py\n\n* make sure we update with the module's plan\n\n* use public api\n\n* oups\n\n* update\n\n* fix failing test\n\n* Update src/transformers/integrations/tensor_parallel.py\n\n* Update src/transformers/integrations/tensor_parallel.py\n\n* fix\n\n* make the API more friendly!\n\n* fix tests\n\n* fix styling\n\n---------\n\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "c50f140be277642b2c1777455e71c633c253bfae",
    "files": [
        {
            "sha": "196d9871623949654573a75edbf4a7a40366a2b5",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c50f140be277642b2c1777455e71c633c253bfae/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c50f140be277642b2c1777455e71c633c253bfae/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=c50f140be277642b2c1777455e71c633c253bfae",
            "patch": "@@ -23,7 +23,12 @@\n import _pytest\n import pytest\n \n-from transformers.testing_utils import HfDoctestModule, HfDocTestParser, is_torch_available, patch_torch_compile_force_graph\n+from transformers.testing_utils import (\n+    HfDoctestModule,\n+    HfDocTestParser,\n+    is_torch_available,\n+    patch_torch_compile_force_graph,\n+)\n \n \n NOT_DEVICE_TESTS = {"
        },
        {
            "sha": "30b1f3ca261162aca29c04d49b6c1153db803d0b",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c50f140be277642b2c1777455e71c633c253bfae/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c50f140be277642b2c1777455e71c633c253bfae/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=c50f140be277642b2c1777455e71c633c253bfae",
            "patch": "@@ -198,6 +198,7 @@ class PretrainedConfig(PushToHubMixin):\n     attribute_map: dict[str, str] = {}\n     base_model_tp_plan: Optional[dict[str, Any]] = None\n     base_model_pp_plan: Optional[dict[str, tuple[list[str]]]] = None\n+    base_model_ep_plan: Optional[dict[str, tuple[list[str]]]] = None\n     _auto_class: Optional[str] = None\n \n     def __setattr__(self, key, value):"
        },
        {
            "sha": "4aadd35999f47ccffcdbc4ae67de38f202fe5193",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 7,
            "deletions": 24,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/c50f140be277642b2c1777455e71c633c253bfae/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c50f140be277642b2c1777455e71c633c253bfae/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=c50f140be277642b2c1777455e71c633c253bfae",
            "patch": "@@ -1013,8 +1013,7 @@ def shard_and_distribute_module(\n \n     \"\"\"\n     param_name, param_type = parameter_name.rsplit(\".\", 1) if \".\" in parameter_name else parameter_name\n-    tp_plan = model._tp_plan or {}\n-    tp_plan.update(getattr(type(model), \"_tp_plan\", None) or {})\n+    tp_plan = model.tp_plan or {}\n     module_to_tp = model.get_submodule(param_name)  # TODO: can i loop over modules?\n     rank = int(rank)\n     current_shard_plan = _get_parameter_tp_plan(parameter_name, tp_plan)\n@@ -1079,42 +1078,26 @@ def verify_tp_plan(expected_keys: list[str], tp_plan: dict[str, str] | None):\n \n \n def distribute_model(model, distributed_config, device_mesh, tp_size):\n-    _plan = \"_tp_plan\"\n-    tp_plan = (getattr(model, \"_tp_plan\", None) or {}).copy()\n-    model._tp_plan = getattr(model.config, \"base_model_tp_plan\").copy()\n-    model._tp_plan.update(tp_plan)\n     model._tp_size = tp_size\n     model._device_mesh = device_mesh\n     if distributed_config is not None:\n         if isinstance(distributed_config, dict):\n             distributed_config = DistributedConfig.from_dict(distributed_config)\n-        if distributed_config.enable_expert_parallel:\n-            _plan = \"_ep_plan\"\n-            model._tp_plan = getattr(model.config, \"base_model_ep_plan\", model._tp_plan).copy()\n-\n-    # now fetch my childrens\n-    for name, module in model.named_children():\n-        if plan := getattr(module, _plan, getattr(module, \"tp_plan\", None)):\n-            model._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n-        if hasattr(module, \"config\"):\n-            plan = getattr(module.config, f\"base_model{_plan}\", {})\n-            if plan == {}:\n-                plan = getattr(module.config, \"base_model_tp_plan\", {})\n-            model._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n-\n-    if model._tp_plan is not None and is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n-        for v in model._tp_plan.values():\n+        model.config.distributed_config = distributed_config\n+    model_plan = model.tp_plan\n+    if model_plan is not None and is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n+        for v in model_plan.values():\n             if v not in ALL_PARALLEL_STYLES:\n                 raise ValueError(f\"Unsupported tensor parallel style {v}. Supported styles are {ALL_PARALLEL_STYLES}\")\n         for name, module in model.named_modules():\n             if not getattr(module, \"_is_hooked\", False):\n                 from transformers.integrations.tensor_parallel import add_tensor_parallel_hooks_to_module\n \n-                plan = _get_parameter_tp_plan(parameter_name=name, tp_plan=model._tp_plan, is_weight=False)\n+                plan = _get_parameter_tp_plan(parameter_name=name, tp_plan=model_plan, is_weight=False)\n                 add_tensor_parallel_hooks_to_module(\n                     model=model,\n                     module=module,\n-                    tp_plan=model._tp_plan,\n+                    tp_plan=model_plan,\n                     layer_name=\"\",\n                     current_module_plan=plan,\n                     device_mesh=device_mesh,"
        },
        {
            "sha": "9d253ce99b43e12ca008ec46957eeb1c77d2c541",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 81,
            "deletions": 1,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/c50f140be277642b2c1777455e71c633c253bfae/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c50f140be277642b2c1777455e71c633c253bfae/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=c50f140be277642b2c1777455e71c633c253bfae",
            "patch": "@@ -2276,7 +2276,87 @@ def post_init(self):\n                         )\n \n         # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n-        self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else None\n+        self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else {}\n+        self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n+        self._ep_plan = self.config.base_model_ep_plan.copy() if self.config.base_model_ep_plan is not None else {}\n+        for name, module in self.named_children():\n+            if plan := getattr(module, \"_ep_plan\", None):\n+                self._ep_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n+            if plan := getattr(module, \"_tp_plan\", None):\n+                self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n+            if plan := getattr(module, \"_pp_plan\", None):\n+                self._pp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n+\n+    @property\n+    def tp_plan(self) -> dict[str, str]:\n+        \"\"\"\n+        The full tp plan for the model's modules\n+        \"\"\"\n+        if hasattr(self.config, \"distributed_config\") and self.config.distributed_config.enable_expert_parallel:\n+            return self._ep_plan\n+        return self._tp_plan\n+\n+    @property\n+    def pp_plan(self) -> dict[str, tuple[str, str]]:\n+        return self._pp_plan\n+\n+    @tp_plan.setter\n+    def tp_plan(self, plan: dict[str, str]):\n+        if plan is not None:\n+            # Validate that all parallel styles in the plan are supported\n+            from .integrations.tensor_parallel import ALL_PARALLEL_STYLES\n+\n+            for layer_pattern, parallel_style in plan.items():\n+                if parallel_style not in ALL_PARALLEL_STYLES:\n+                    raise ValueError(\n+                        f\"Unsupported tensor parallel style '{parallel_style}' for layer '{layer_pattern}'. \"\n+                        f\"Supported styles are {list(ALL_PARALLEL_STYLES.keys())}\"\n+                    )\n+\n+            # Validate that the layer patterns match existing model structure\n+            # We check this by getting all parameter names and seeing if any match the patterns\n+            if hasattr(self, \"named_parameters\"):\n+                model_param_names = [name for name, _ in self.named_parameters()]\n+                if model_param_names:  # Only validate if model has parameters\n+                    import re\n+\n+                    for layer_pattern in plan.keys():\n+                        # Convert pattern to regex (replace * with .*)\n+                        regex_pattern = layer_pattern.replace(\"*\", r\"\\d+\")\n+                        pattern_matched = False\n+                        for param_name in model_param_names:\n+                            if re.match(regex_pattern, param_name):\n+                                pattern_matched = True\n+                                break\n+                        if not pattern_matched:\n+                            # Try more flexible matching - check if pattern components exist\n+                            pattern_parts = layer_pattern.split(\".\")\n+                            flexible_matched = False\n+                            for param_name in model_param_names:\n+                                param_parts = param_name.split(\".\")\n+                                if len(pattern_parts) <= len(param_parts):\n+                                    match_count = 0\n+                                    for i, pattern_part in enumerate(pattern_parts):\n+                                        if pattern_part == \"*\":\n+                                            match_count += 1\n+                                        elif i < len(param_parts) and pattern_part == param_parts[i]:\n+                                            match_count += 1\n+                                    if match_count == len(pattern_parts):\n+                                        flexible_matched = True\n+                                        break\n+                            if not flexible_matched:\n+                                import warnings\n+\n+                                warnings.warn(\n+                                    f\"Layer pattern '{layer_pattern}' does not match any parameters in the model. \"\n+                                    f\"This rule may not be applied during tensor parallelization.\"\n+                                )\n+\n+        self._tp_plan = plan if plan is not None else {}\n+\n+    @pp_plan.setter\n+    def pp_plan(self, plan: dict[str, tuple[str, str]]):\n+        self._pp_plan = plan\n \n     def dequantize(self):\n         \"\"\""
        },
        {
            "sha": "592c9a7b5a270d17deca0b08880ea52fc8967fe6",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "modified",
            "additions": 118,
            "deletions": 0,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/c50f140be277642b2c1777455e71c633c253bfae/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c50f140be277642b2c1777455e71c633c253bfae/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=c50f140be277642b2c1777455e71c633c253bfae",
            "patch": "@@ -215,6 +215,124 @@ def test_model_save(self):\n                     del non_tp_tensor, tp_tensor\n \n \n+class TestTensorParallelProperties(TestCasePlus):\n+    def test_tp_plan_property_setter_getter(self):\n+        \"\"\"Test that tp_plan property can be set and retrieved correctly.\"\"\"\n+        from transformers import AutoModelForCausalLM\n+\n+        model_id = \"JackFram/llama-68m\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\")\n+\n+        # Test setting empty plan\n+        model.tp_plan = {}\n+        self.assertEqual(model.tp_plan, {})\n+\n+        # Test setting a valid plan\n+        valid_plan = {\"model.layers.*.self_attn.q_proj\": \"colwise\"}\n+        model.tp_plan = valid_plan\n+        self.assertEqual(model.tp_plan, valid_plan)\n+\n+        # Test updating the plan\n+        model.tp_plan.update({\"model.layers.*.self_attn.k_proj\": \"colwise\"})\n+        expected_plan = {\"model.layers.*.self_attn.q_proj\": \"colwise\", \"model.layers.*.self_attn.k_proj\": \"colwise\"}\n+        self.assertEqual(model.tp_plan, expected_plan)\n+\n+        # Test overriding existing entry\n+        model.tp_plan.update({\"model.layers.*.self_attn.q_proj\": \"colwise_rep\"})\n+        expected_plan = {\n+            \"model.layers.*.self_attn.q_proj\": \"colwise_rep\",\n+            \"model.layers.*.self_attn.k_proj\": \"colwise\",\n+        }\n+        self.assertEqual(model.tp_plan, expected_plan)\n+\n+    def test_tp_plan_validation_invalid_style(self):\n+        \"\"\"Test that invalid parallel styles are rejected.\"\"\"\n+        from transformers import AutoModelForCausalLM\n+\n+        model_id = \"JackFram/llama-68m\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\")\n+\n+        # Test invalid parallel style\n+        with self.assertRaises(ValueError) as context:\n+            model.tp_plan = {\"layers.*.self_attn.q_proj\": \"invalid_style\"}\n+\n+        self.assertIn(\"Unsupported tensor parallel style 'invalid_style'\", str(context.exception))\n+        self.assertIn(\"Supported styles are\", str(context.exception))\n+\n+    def test_tp_plan_validation_nonexistent_layer_warning(self):\n+        \"\"\"Test that warnings are issued for non-existent layer patterns.\"\"\"\n+        import warnings\n+\n+        from transformers import AutoModelForCausalLM\n+\n+        model_id = \"JackFram/llama-68m\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\")\n+\n+        # Test warning for non-existent layer pattern\n+        with warnings.catch_warnings(record=True) as w:\n+            warnings.simplefilter(\"always\")\n+            model.tp_plan = {\"nonexistent.*.layer\": \"colwise\"}\n+\n+            # Check that a warning was issued\n+            self.assertTrue(len(w) > 0)\n+            warning_message = str(w[0].message)\n+            self.assertIn(\"Layer pattern 'nonexistent.*.layer' does not match any parameters\", warning_message)\n+\n+    def test_tp_plan_valid_layer_patterns(self):\n+        \"\"\"Test that valid layer patterns are accepted without warnings.\"\"\"\n+        import warnings\n+\n+        from transformers import AutoModelForCausalLM\n+\n+        model_id = \"JackFram/llama-68m\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\")\n+\n+        # Test valid layer patterns that should match the model structure\n+        valid_plans = [\n+            {\"model.layers.*.self_attn.q_proj\": \"colwise\"},\n+            {\"model.layers.*.self_attn.k_proj\": \"rowwise\"},\n+            {\"model.layers.*.mlp.gate_proj\": \"colwise_rep\"},\n+        ]\n+\n+        for plan in valid_plans:\n+            with warnings.catch_warnings(record=True) as w:\n+                warnings.simplefilter(\"always\")\n+                model.tp_plan = plan\n+\n+                # Filter out any warnings that are not about layer patterns\n+                layer_warnings = [\n+                    warning\n+                    for warning in w\n+                    if \"Layer pattern\" in str(warning.message)\n+                    and \"does not match any parameters\" in str(warning.message)\n+                ]\n+\n+                # Should not have layer pattern warnings for valid patterns\n+                self.assertEqual(\n+                    len(layer_warnings),\n+                    0,\n+                    f\"Unexpected warning for valid pattern {plan}: {[str(w.message) for w in layer_warnings]}\",\n+                )\n+\n+        # Verify the final plan was set correctly\n+        self.assertEqual(model.tp_plan, valid_plans[-1])\n+\n+    def test_tp_plan_none_handling(self):\n+        \"\"\"Test that None values are handled correctly.\"\"\"\n+        from transformers import AutoModelForCausalLM\n+\n+        model_id = \"JackFram/llama-68m\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\")\n+\n+        # Test setting None\n+        model.tp_plan = None\n+        self.assertEqual(model.tp_plan, {})\n+\n+        # Test setting a plan after None\n+        model.tp_plan = {\"model.layers.*.self_attn.q_proj\": \"colwise\"}\n+        self.assertEqual(model.tp_plan, {\"model.layers.*.self_attn.q_proj\": \"colwise\"})\n+\n+\n @require_torch_multi_accelerator\n class TestTensorParallelAccelerator(TestTensorParallel):\n     nproc_per_node = backend_device_count(torch_device)"
        }
    ],
    "stats": {
        "total": 239,
        "additions": 213,
        "deletions": 26
    }
}