{
    "author": "Mihonarium",
    "message": "Fix grammatical error in MoE variable name: expert_hitted → expert_hit, hitted_experts → hit_experts (#39959)\n\n* Fix grammatical error: expert_hitted -> expert_hit in MoE implementations\n\n* Fix grammatical error: hitted_experts -> hit_experts in MoE implementation",
    "sha": "43b3f58875e93ddaa66ac0610cb6da3a6fc37768",
    "files": [
        {
            "sha": "393894c243bc18a71494ae9a6c9f248ab1350f63",
            "filename": "src/transformers/integrations/mxfp4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmxfp4.py?ref=43b3f58875e93ddaa66ac0610cb6da3a6fc37768",
            "patch": "@@ -264,8 +264,8 @@ def topk(vals, k):\n \n         expt_data = compute_expt_data_torch(hist, n_local_experts, n_gates_pad)\n \n-        hitted_experts = n_expts_act\n-    return RoutingData(gate_scal, hist, n_local_experts, hitted_experts, expt_data), gather_indx, scatter_indx\n+        hit_experts = n_expts_act\n+    return RoutingData(gate_scal, hist, n_local_experts, hit_experts, expt_data), gather_indx, scatter_indx\n \n \n def mlp_forward(self, hidden_states):"
        },
        {
            "sha": "9e40950ad8e0d714d794b6ccf0a3e1e6f9efb856",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=43b3f58875e93ddaa66ac0610cb6da3a6fc37768",
            "patch": "@@ -356,8 +356,8 @@ def forward(\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n         # Loop over all available experts in the model and perform the computation on each expert\n-        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hitted:\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n             expert_layer = self.experts[expert_idx]\n             idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n "
        },
        {
            "sha": "964153295710bc409d25df7a97bfccfe20340f0a",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=43b3f58875e93ddaa66ac0610cb6da3a6fc37768",
            "patch": "@@ -167,8 +167,8 @@ def forward(\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n         # Loop over all available experts in the model and perform the computation on each expert\n-        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hitted:\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n             expert_layer = self.experts[expert_idx]\n             idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n "
        },
        {
            "sha": "f0fa21a48d031426d6ec4971227e601755f86da3",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=43b3f58875e93ddaa66ac0610cb6da3a6fc37768",
            "patch": "@@ -97,8 +97,8 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n                 expert_mask = expert_mask.permute(2, 1, 0)\n                 # we sum on the top_k and on the sequence lenght to get which experts\n                 # are hit this time around\n-                expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-            for expert_idx in expert_hitted[:]:\n+                expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+            for expert_idx in expert_hit[:]:\n                 with torch.no_grad():\n                     _, token_idx = torch.where(expert_mask[expert_idx[0]])\n                 current_state = hidden_states[token_idx]"
        },
        {
            "sha": "07e42d43d30af1b4d0d93ebf6759ac03d3dd37ab",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=43b3f58875e93ddaa66ac0610cb6da3a6fc37768",
            "patch": "@@ -95,8 +95,8 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n                 expert_mask = expert_mask.permute(2, 1, 0)\n                 # we sum on the top_k and on the sequence lenght to get which experts\n                 # are hit this time around\n-                expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-            for expert_idx in expert_hitted[:]:\n+                expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+            for expert_idx in expert_hit[:]:\n                 with torch.no_grad():\n                     _, token_idx = torch.where(expert_mask[expert_idx[0]])\n                 current_state = hidden_states[token_idx]"
        },
        {
            "sha": "46b14f7126028f4ed095aa87b07dbe7327e22c79",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=43b3f58875e93ddaa66ac0610cb6da3a6fc37768",
            "patch": "@@ -465,8 +465,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         # this will be used to easily index which expert is going to be sollicitated\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hitted:\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n             expert_layer = self.experts[expert_idx]\n             idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n             # Index the correct hidden states and compute the expert hidden state for"
        },
        {
            "sha": "7c793ccbd36e0371591285dd24bdf40dddd1de1d",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=43b3f58875e93ddaa66ac0610cb6da3a6fc37768",
            "patch": "@@ -121,8 +121,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         # this will be used to easily index which expert is going to be sollicitated\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hitted:\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n             expert_layer = self.experts[expert_idx]\n             idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n             # Index the correct hidden states and compute the expert hidden state for"
        },
        {
            "sha": "8dc8c772e74aa082535055b728caf1d005b98262",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=43b3f58875e93ddaa66ac0610cb6da3a6fc37768",
            "patch": "@@ -201,8 +201,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         # this will be used to easily index which expert is going to be sollicitated\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hitted:\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n             expert_layer = self.experts[expert_idx]\n             idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n             # Index the correct hidden states and compute the expert hidden state for"
        },
        {
            "sha": "125b1911c89bbecc2075aa665398fb328809245a",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=43b3f58875e93ddaa66ac0610cb6da3a6fc37768",
            "patch": "@@ -621,8 +621,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n         # Loop over all available experts in the model and perform the computation on each expert\n-        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hitted:\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n             expert_layer = self.experts[expert_idx]\n             idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n "
        },
        {
            "sha": "16695cb9b6fc016ede92d6ae7c73d079e35e9095",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=43b3f58875e93ddaa66ac0610cb6da3a6fc37768",
            "patch": "@@ -244,8 +244,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n         # Loop over all available experts in the model and perform the computation on each expert\n-        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hitted:\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n             expert_layer = self.experts[expert_idx]\n             idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n "
        },
        {
            "sha": "8cf50cdce2faab3c0851da4b7850932fa7113475",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43b3f58875e93ddaa66ac0610cb6da3a6fc37768/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=43b3f58875e93ddaa66ac0610cb6da3a6fc37768",
            "patch": "@@ -100,8 +100,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n         # Loop over all available experts in the model and perform the computation on each expert\n-        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hitted:\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n             expert_layer = self.experts[expert_idx]\n             idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n "
        }
    ],
    "stats": {
        "total": 44,
        "additions": 22,
        "deletions": 22
    }
}