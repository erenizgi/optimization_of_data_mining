{
    "author": "LysandreJik",
    "message": "Fix loading logic flaw with regards to unexpected and missing keys (#40850)\n\n* Unexpected keys should be ignored at load with device map\n\n* remove them all\n\n* fix logic flaw\n\n* fix\n\n* simplify\n\n* style\n\n* fix\n\n* revert caching allocator change\n\n* add other test\n\n* add nice doc\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "7258ea44bc0c0a425a468f66f8559d1de8c4126d",
    "files": [
        {
            "sha": "a64085c4e931aa6d490418753ceba99e29c9147d",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 24,
            "deletions": 27,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/7258ea44bc0c0a425a468f66f8559d1de8c4126d/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7258ea44bc0c0a425a468f66f8559d1de8c4126d/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=7258ea44bc0c0a425a468f66f8559d1de8c4126d",
            "patch": "@@ -738,8 +738,6 @@ def _load_state_dict_into_meta_model(\n         file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device)\n \n     for param_name, empty_param in state_dict.items():\n-        if param_name not in expected_keys:  # when loading from ckpt, we skip param if doesnt exist in modeling\n-            continue\n         # we need to use serialized_param_name as file pointer is untouched\n         if is_meta_state_dict:\n             # This is the name of the parameter as it appears on disk file\n@@ -1414,7 +1412,6 @@ def _get_device_map(\n \n \n def _find_missing_and_unexpected_keys(\n-    cls,\n     model: \"PreTrainedModel\",\n     original_checkpoint_keys: list[str],\n     checkpoint_keys: list[str],\n@@ -1444,12 +1441,6 @@ def _find_missing_and_unexpected_keys(\n     model_buffers = {n for n, _ in model.named_buffers()}\n     unexpected_keys = sorted(unexpected_keys - model_buffers)\n \n-    # Old checkpoints may have keys for rotary_emb.inv_freq for each layer, however we moved this buffer to the main model\n-    # (so the buffer name has changed). Remove them in such a case\n-    has_inv_freq_buffers = any(buffer.endswith(\"rotary_emb.inv_freq\") for buffer in model_buffers)\n-    if has_inv_freq_buffers:\n-        unexpected_keys = [k for k in unexpected_keys if \"rotary_emb.inv_freq\" not in k]\n-\n     tied_params = find_tied_parameters(model)\n     for group in tied_params:\n         missing_in_group = [k for k in missing_keys if k in group]\n@@ -1460,15 +1451,6 @@ def _find_missing_and_unexpected_keys(\n         missing_keys = hf_quantizer.update_missing_keys(model, missing_keys, prefix)\n         unexpected_keys = hf_quantizer.update_unexpected_keys(model, unexpected_keys, prefix)\n \n-    # Model-specific exceptions for missing and unexpected keys (e.g. if the modeling change over time, or any other reason...)\n-    if cls._keys_to_ignore_on_load_missing is not None:\n-        for pattern in cls._keys_to_ignore_on_load_missing:\n-            missing_keys = [k for k in missing_keys if re.search(pattern, k) is None]\n-\n-    if cls._keys_to_ignore_on_load_unexpected is not None:\n-        for pattern in cls._keys_to_ignore_on_load_unexpected:\n-            unexpected_keys = [k for k in unexpected_keys if re.search(pattern, k) is None]\n-\n     return missing_keys, unexpected_keys\n \n \n@@ -5320,12 +5302,7 @@ def _load_pretrained_model(\n \n         # Find missing and unexpected keys from the state dict\n         missing_keys, unexpected_keys = _find_missing_and_unexpected_keys(\n-            cls,\n-            model,\n-            original_checkpoint_keys,\n-            checkpoint_keys,\n-            loading_base_model_from_task_state_dict,\n-            hf_quantizer,\n+            model, original_checkpoint_keys, checkpoint_keys, loading_base_model_from_task_state_dict, hf_quantizer\n         )\n         # Find all the keys with shape mismatch (if we ignore the mismatch, the weights need to be newly initialized the\n         # same way as missing keys)\n@@ -5339,8 +5316,10 @@ def _load_pretrained_model(\n             weights_only,\n         )\n \n-        # We need to update both the mapping and the list of checkpoint keys to remove the mismatched ones\n-        key_renaming_mapping = {k: v for k, v in key_renaming_mapping.items() if v not in mismatched_keys}\n+        # We need to update both the mapping and the list of checkpoint keys to remove the mismatched and unexpected ones\n+        key_renaming_mapping = {\n+            k: v for k, v in key_renaming_mapping.items() if v not in mismatched_keys and v not in unexpected_keys\n+        }\n         checkpoint_keys = list(key_renaming_mapping.values())\n \n         # Move missing (and potentially mismatched) keys back to cpu from meta device (because they won't be moved when\n@@ -5366,14 +5345,15 @@ def _load_pretrained_model(\n             # in the submodule\n             key_renaming_mapping = {k: v[len(_prefix) :] for k, v in key_renaming_mapping.items()}\n             checkpoint_keys = list(key_renaming_mapping.values())\n+            unexpected_keys = [k[len(_prefix) :] if k.startswith(_prefix) else k for k in unexpected_keys]\n             # We need to update the device map as well\n             if device_map is not None:\n                 device_map = {k[len(_prefix) :] if k.startswith(_prefix) else k: v for k, v in device_map.items()}\n             # small sanity check: the base model should not contain task-specific head keys\n             task_specific_expected_keys = [s for s in model.state_dict() if not s.startswith(_prefix)]\n             base_model_expected_keys = list(model_to_load.state_dict().keys())\n             if any(\n-                key in task_specific_expected_keys and key not in base_model_expected_keys for key in checkpoint_keys\n+                key in task_specific_expected_keys and key not in base_model_expected_keys for key in unexpected_keys\n             ):\n                 raise ValueError(\n                     \"The state dictionary of the model you are trying to load is corrupted. Are you sure it was \"\n@@ -5555,6 +5535,23 @@ def _load_pretrained_model(\n                         device_mesh,\n                     )\n \n+        # Model-specific exceptions for missing and unexpected keys (e.g. if the modeling change over time, or any other reason...)\n+        # We should remove them here to avoid raising warnings if they are present in the lists\n+        if cls._keys_to_ignore_on_load_missing is not None:\n+            for pattern in cls._keys_to_ignore_on_load_missing:\n+                missing_keys = [k for k in missing_keys if re.search(pattern, k) is None]\n+\n+        if cls._keys_to_ignore_on_load_unexpected is not None:\n+            for pattern in cls._keys_to_ignore_on_load_unexpected:\n+                unexpected_keys = [k for k in unexpected_keys if re.search(pattern, k) is None]\n+\n+        # Old checkpoints may have keys for rotary_emb.inv_freq for each layer, however we moved this buffer to the main model\n+        # (so the buffer name has changed). Remove them in such a case. This is another exception that was not added to\n+        # `_keys_to_ignore_on_load_unexpected` as it touches many models\n+        has_inv_freq_buffers = any(buffer.endswith(\"rotary_emb.inv_freq\") for buffer, _ in model.named_buffers())\n+        if has_inv_freq_buffers:\n+            unexpected_keys = [k for k in unexpected_keys if \"rotary_emb.inv_freq\" not in k]\n+\n         # All potential warnings/infos\n         if len(error_msgs) > 0:\n             error_msg = \"\\n\\t\".join(error_msgs)"
        },
        {
            "sha": "fc2bbb60c452cf956aa8dec77ac01bfae9ee4186",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 113,
            "deletions": 1,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/7258ea44bc0c0a425a468f66f8559d1de8c4126d/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7258ea44bc0c0a425a468f66f8559d1de8c4126d/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=7258ea44bc0c0a425a468f66f8559d1de8c4126d",
            "patch": "@@ -29,7 +29,7 @@\n \n import pytest\n import requests\n-from huggingface_hub import HfApi, HfFolder\n+from huggingface_hub import HfApi, HfFolder, split_torch_state_dict_into_shards\n from parameterized import parameterized\n from pytest import mark\n from requests.exceptions import HTTPError\n@@ -139,6 +139,32 @@ def __init__(self, config):\n         def forward(self, x):\n             return self.linear_2(self.linear(x))\n \n+    class BaseModelWithUnexpectedKeys(PreTrainedModel):\n+        base_model_prefix = \"base\"\n+        config_class = PretrainedConfig\n+        _keys_to_ignore_on_load_unexpected = [r\"^mtp.*\"]\n+\n+        def __init__(self, config):\n+            super().__init__(config)\n+            self.linear = nn.Linear(50, 50)\n+            self.linear_2 = nn.Linear(50, 50)\n+\n+        def forward(self, x):\n+            return self.linear_2(self.linear(x))\n+\n+    class BaseModelWithMissingKeys(PreTrainedModel):\n+        base_model_prefix = \"base\"\n+        config_class = PretrainedConfig\n+        _keys_to_ignore_on_load_missing = [r\"^linear\"]\n+\n+        def __init__(self, config):\n+            super().__init__(config)\n+            self.linear = nn.Linear(50, 50)\n+            self.linear_2 = nn.Linear(50, 50)\n+\n+        def forward(self, x):\n+            return self.linear_2(self.linear(x))\n+\n     class BaseModelWithTiedWeights(PreTrainedModel):\n         config_class = PretrainedConfig\n \n@@ -2028,6 +2054,92 @@ class MyModelD(MyModelA):\n         self.assertIs(MyModelC.config_class, MyConfigC)\n         self.assertIs(MyModelD.config_class, MyConfigA)\n \n+    def test_ignore_missing_key_works(self):\n+        \"\"\"Test that if a parameter (not buffer) is specified in `_keys_to_ignore_on_load_missing` and is actually\n+        missing from the checkpoint, it will still be moved to cpu and initialized\"\"\"\n+        temp = tempfile.TemporaryDirectory()\n+        # Create dummy model\n+        model = BaseModelWithMissingKeys(PretrainedConfig())\n+\n+        # Save the config\n+        model.config.save_pretrained(temp.name)\n+        # Get the state dict to save\n+        state_dict = model.state_dict()\n+        # Remove the layer that we should ignore if missing\n+        del state_dict[\"linear.weight\"], state_dict[\"linear.bias\"]\n+        # Save the state dict as a single shard\n+        safe_save_file(state_dict, Path(temp.name) / \"model.safetensors\", metadata={\"format\": \"pt\"})\n+\n+        # Try loading back, with the missing key not present in the state_dict\n+        model = BaseModelWithMissingKeys.from_pretrained(temp.name)\n+\n+        # Make sure the skipped missing key is not still on meta device!\n+        for k, v in model.state_dict().items():\n+            self.assertTrue(v.device.type == \"cpu\", f\"{k} is not on cpu!\")\n+\n+    def test_device_map_works_with_unexpected_keys(self):\n+        \"\"\"Test that if a parameter is specified in `_keys_to_ignore_on_load_unexpected` and is actually\n+        present in the checkpoint, it will correctly be removed from the weights we load, especially those\n+        we use if the device map has offloading\"\"\"\n+        temp = tempfile.TemporaryDirectory()\n+\n+        # Create dummy model\n+        model = BaseModelWithUnexpectedKeys(PretrainedConfig())\n+\n+        # Save the config\n+        model.config.save_pretrained(temp.name)\n+\n+        # Get the state dict to save\n+        state_dict = model.state_dict()\n+        # Add a layer that is in the \"_keys_to_ignore_on_load_unexpected\" list to ignore\n+        state_dict[\"mtp\"] = torch.randn(12, 12)\n+        # Save the state dict as a single shard\n+        safe_save_file(state_dict, Path(temp.name) / \"model.safetensors\", metadata={\"format\": \"pt\"})\n+\n+        # Load the model with entire shards placed on disk in order to trigger `get_disk_only_shard_files`.\n+        # Unexpected keys (mtp) should be removed from the state dict, therefore this should not error out.\n+        BaseModelWithUnexpectedKeys.from_pretrained(temp.name, device_map={\"linear\": \"cpu\", \"linear_2\": \"disk\"})\n+\n+    def test_device_map_works_with_unexpected_keys_sharded(self):\n+        \"\"\"Test that if a parameter is specified in `_keys_to_ignore_on_load_unexpected` and is actually\n+        present in the checkpoint, it will correctly be removed from the weights we load, especially those\n+        we use if the device map has offloading\"\"\"\n+        temp = tempfile.TemporaryDirectory()\n+\n+        # Create dummy model\n+        model = BaseModelWithUnexpectedKeys(PretrainedConfig())\n+\n+        # Save the config\n+        model.config.save_pretrained(temp.name)\n+\n+        # Get the state dict to save\n+        state_dict = model.state_dict()\n+\n+        # Add a layer that is in the \"_keys_to_ignore_on_load_unexpected\" list to ignore\n+        state_dict[\"mtp\"] = torch.randn(50, 50)\n+\n+        # Split the state dict in shards, save the index and the shards\n+        shards = split_torch_state_dict_into_shards(state_dict, max_shard_size=\"1kb\")\n+        index = {\n+            \"metadata\": {\"total_parameters\": model.num_parameters(), **shards.metadata},\n+            \"weight_map\": shards.tensor_to_filename,\n+        }\n+        with open(Path(temp.name) / SAFE_WEIGHTS_INDEX_NAME, \"w\", encoding=\"utf-8\") as f:\n+            content = json.dumps(index, indent=2, sort_keys=True) + \"\\n\"\n+            f.write(content)\n+\n+        # Save each shard\n+        filename_to_tensors = shards.filename_to_tensors.items()\n+        for shard_file, tensors in filename_to_tensors:\n+            shard = {}\n+            for tensor in tensors:\n+                shard[tensor] = state_dict[tensor].contiguous()\n+            safe_save_file(shard, Path(temp.name) / shard_file, metadata={\"format\": \"pt\"})\n+\n+        # Load the model with entire shards placed on disk in order to trigger `get_disk_only_shard_files`.\n+        # Unexpected keys (mtp) should be removed from the state dict, therefore this should not error out.\n+        BaseModelWithUnexpectedKeys.from_pretrained(temp.name, device_map={\"linear\": \"cpu\", \"linear_2\": \"disk\"})\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 165,
        "additions": 137,
        "deletions": 28
    }
}