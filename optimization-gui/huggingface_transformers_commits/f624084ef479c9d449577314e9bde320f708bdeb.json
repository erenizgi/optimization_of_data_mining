{
    "author": "Wauplin",
    "message": "[cleanup] Offline mode and cache dir from `huggingface_hub` constants + cleanup in `PushToHubMixin` (#42391)\n\n* Offline mode directly from huggingface_hub constant\n\n* remove deprecated cache dir env variables\n\n* push_to_hub changes\n\n* always use tmp dir in push_to_hub + keyword args\n\n* code quality",
    "sha": "f624084ef479c9d449577314e9bde320f708bdeb",
    "files": [
        {
            "sha": "87d5c18d07f97b197cede257085c73cdea20b074",
            "filename": "docs/source/ar/installation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Far%2Finstallation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Far%2Finstallation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Finstallation.md?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -142,17 +142,12 @@ conda install conda-forge::transformers\n \n ## Ø¥Ø¹Ø¯Ø§Ø¯ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª\n \n-ØªÙØ­Ù…Ù‘Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ÙØ³Ø¨Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØªÙØ®Ø²Ù‘Ù† Ù…Ø¤Ù‚ØªÙ‹Ø§ ÙÙŠ: `~/.cache/huggingface/hub`. Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø§Ù„Ø°ÙŠ ÙŠÙØ­Ø¯Ø¯Ù‡ Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© `TRANSFORMERS_CACHE`. Ø¹Ù„Ù‰ WindowsØŒ ÙŠÙƒÙˆÙ† Ø¯Ù„ÙŠÙ„ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù‡Ùˆ `C:\\Users\\username\\.cache\\huggingface\\hub`. ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ± Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© shell Ø§Ù„Ù…ÙˆØ¶Ø­Ø© Ø£Ø¯Ù†Ø§Ù‡ - Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© - Ù„ØªØ­Ø¯ÙŠØ¯ Ø¯Ù„ÙŠÙ„ Ø°Ø§ÙƒØ±Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù…Ø®ØªÙ„Ù:\n+ØªÙØ­Ù…Ù‘Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ÙØ³Ø¨Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØªÙØ®Ø²Ù‘Ù† Ù…Ø¤Ù‚ØªÙ‹Ø§ ÙÙŠ: `~/.cache/huggingface/hub`. Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø§Ù„Ø°ÙŠ ÙŠÙØ­Ø¯Ø¯Ù‡ Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© `HF_HUB_CACHE`. Ø¹Ù„Ù‰ WindowsØŒ ÙŠÙƒÙˆÙ† Ø¯Ù„ÙŠÙ„ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù‡Ùˆ `C:\\Users\\username\\.cache\\huggingface\\hub`. ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ± Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© shell Ø§Ù„Ù…ÙˆØ¶Ø­Ø© Ø£Ø¯Ù†Ø§Ù‡ - Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© - Ù„ØªØ­Ø¯ÙŠØ¯ Ø¯Ù„ÙŠÙ„ Ø°Ø§ÙƒØ±Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù…Ø®ØªÙ„Ù:\n \n-1. Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© (Ø§ÙØªØ±Ø§Ø¶ÙŠ): `HF_HUB_CACHE` Ø£Ùˆ `TRANSFORMERS_CACHE`.\n+1. Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© (Ø§ÙØªØ±Ø§Ø¶ÙŠ): `HF_HUB_CACHE`.\n 2. Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø©: `HF_HOME`.\n 3. Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø©: `XDG_CACHE_HOME` + `/huggingface`.\n \n-<Tip>\n-\n-Ø³ÙŠØ³ØªØ®Ø¯Ù… ğŸ¤— Transformers Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© `PYTORCH_TRANSFORMERS_CACHE` Ø£Ùˆ `PYTORCH_PRETRAINED_BERT_CACHE` Ø¥Ø°Ø§ ÙƒÙ†Øª Ù‚Ø§Ø¯Ù…Ù‹Ø§ Ù…Ù† Ø¥ØµØ¯Ø§Ø± Ø³Ø§Ø¨Ù‚ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙƒØªØ¨Ø© ÙˆÙ‚Ù…Øª Ø¨ØªØ¹ÙŠÙŠÙ† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ù‡Ø°Ù‡ØŒ Ù…Ø§ Ù„Ù… ØªØ­Ø¯Ø¯ Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© `TRANSFORMERS_CACHE`.\n-\n-</Tip>\n \n ## Ø§Ù„ÙˆØ¶Ø¹ Ø¯ÙˆÙ† Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª\n "
        },
        {
            "sha": "9a8b5400baf6164c77c6434c687444d7490cf4f0",
            "filename": "docs/source/de/installation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fde%2Finstallation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fde%2Finstallation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Finstallation.md?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -147,19 +147,13 @@ conda install conda-forge::transformers\n \n ## Cache Einrichtung\n \n-Vorgefertigte Modelle werden heruntergeladen und lokal zwischengespeichert unter: `~/.cache/huggingface/hub`. Dies ist das Standardverzeichnis, das durch die Shell-Umgebungsvariable \"TRANSFORMERS_CACHE\" vorgegeben ist. Unter Windows wird das Standardverzeichnis durch `C:\\Benutzer\\Benutzername\\.cache\\huggingface\\hub` angegeben. Sie kÃ¶nnen die unten aufgefÃ¼hrten Shell-Umgebungsvariablen - in der Reihenfolge ihrer PrioritÃ¤t - Ã¤ndern, um ein anderes Cache-Verzeichnis anzugeben:\n+Vorgefertigte Modelle werden heruntergeladen und lokal zwischengespeichert unter: `~/.cache/huggingface/hub`. Dies ist das Standardverzeichnis, das durch die Shell-Umgebungsvariable \"HF_HUB_CACHE\" vorgegeben ist. Unter Windows wird das Standardverzeichnis durch `C:\\Benutzer\\Benutzername\\.cache\\huggingface\\hub` angegeben. Sie kÃ¶nnen die unten aufgefÃ¼hrten Shell-Umgebungsvariablen - in der Reihenfolge ihrer PrioritÃ¤t - Ã¤ndern, um ein anderes Cache-Verzeichnis anzugeben:\n \n-1. Shell-Umgebungsvariable (Standard): `HF_HUB_CACHE` oder `TRANSFORMERS_CACHE`.\n+1. Shell-Umgebungsvariable (Standard): `HF_HUB_CACHE`.\n 2. Shell-Umgebungsvariable: `HF_HOME`.\n 3. Shell-Umgebungsvariable: `XDG_CACHE_HOME` + `/huggingface`.\n \n \n-<Tip>\n-\n-Transformers verwendet die Shell-Umgebungsvariablen `PYTORCH_TRANSFORMERS_CACHE` oder `PYTORCH_PRETRAINED_BERT_CACHE`, wenn Sie von einer frÃ¼heren Iteration dieser Bibliothek kommen und diese Umgebungsvariablen gesetzt haben, sofern Sie nicht die Shell-Umgebungsvariable `TRANSFORMERS_CACHE` angeben.\n-\n-</Tip>\n-\n ## Offline Modus\n \n Transformers ist in der Lage, in einer Firewall- oder Offline-Umgebung zu laufen, indem es nur lokale Dateien verwendet. Setzen Sie die Umgebungsvariable `HF_HUB_OFFLINE=1`, um dieses Verhalten zu aktivieren."
        },
        {
            "sha": "d154bd133a1b2dbbcd34ed84b2d5e499f6c5e547",
            "filename": "docs/source/en/installation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fen%2Finstallation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fen%2Finstallation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finstallation.md?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -127,16 +127,14 @@ When you load a pretrained model with [`~PreTrainedModel.from_pretrained`], the\n \n Every time you load a model, it checks whether the cached model is up-to-date. If it's the same, then the local model is loaded. If it's not the same, the newer model is downloaded and cached.\n \n-The default directory given by the shell environment variable `TRANSFORMERS_CACHE` is `~/.cache/huggingface/hub`. On Windows, the default directory is `C:\\Users\\username\\.cache\\huggingface\\hub`.\n+The default directory given by the shell environment variable `HF_HUB_CACHE` is `~/.cache/huggingface/hub`. On Windows, the default directory is `C:\\Users\\username\\.cache\\huggingface\\hub`.\n \n Cache a model in a different directory by changing the path in the following shell environment variables (listed by priority).\n \n-1. [HF_HUB_CACHE](https://hf.co/docs/huggingface_hub/package_reference/environment_variables#hfhubcache) or `TRANSFORMERS_CACHE` (default)\n+1. [HF_HUB_CACHE](https://hf.co/docs/huggingface_hub/package_reference/environment_variables#hfhubcache) (default)\n 2. [HF_HOME](https://hf.co/docs/huggingface_hub/package_reference/environment_variables#hfhome)\n 3. [XDG_CACHE_HOME](https://hf.co/docs/huggingface_hub/package_reference/environment_variables#xdgcachehome) + `/huggingface` (only if `HF_HOME` is not set)\n \n-Older versions of Transformers uses the shell environment variables `PYTORCH_TRANSFORMERS_CACHE` or `PYTORCH_PRETRAINED_BERT_CACHE`. You should keep these unless you specify the newer shell environment variable `TRANSFORMERS_CACHE`.\n-\n ### Offline mode\n \n To use Transformers in an offline or firewalled environment requires the downloaded and cached files ahead of time. Download a model repository from the Hub with the [`~huggingface_hub.snapshot_download`] method."
        },
        {
            "sha": "772fcb7ad83b2d192fc2428d269b38743e64f452",
            "filename": "docs/source/en/perf_train_cpu_many.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -203,8 +203,8 @@ spec:\n               env:\n               - name: LD_PRELOAD\n                 value: \"/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4.5.9:/usr/local/lib/libiomp5.so\"\n-              - name: TRANSFORMERS_CACHE\n-                value: \"/tmp/pvc-mount/transformers_cache\"\n+              - name: HF_HUB_CACHE\n+                value: \"/tmp/pvc-mount/hub_cache\"\n               - name: HF_DATASETS_CACHE\n                 value: \"/tmp/pvc-mount/hf_datasets_cache\"\n               - name: LOGLEVEL"
        },
        {
            "sha": "2c657187a63a2a0b8fb71e6e4eb1c16735bb44c9",
            "filename": "docs/source/es/installation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fes%2Finstallation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fes%2Finstallation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Finstallation.md?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -139,18 +139,12 @@ conda install conda-forge::transformers\n \n ## ConfiguraciÃ³n de CachÃ©\n \n-Los modelos preentrenados se descargan y almacenan en cachÃ© localmente en: `~/.cache/huggingface/transformers/`. Este es el directorio predeterminado proporcionado por la variable de entorno de shell `TRANSFORMERS_CACHE`. En Windows, el directorio predeterminado es dado por `C:\\Users\\username\\.cache\\huggingface\\transformers`. Puedes cambiar las variables de entorno de shell que se muestran a continuaciÃ³n, en orden de prioridad, para especificar un directorio de cachÃ© diferente:\n+Los modelos preentrenados se descargan y almacenan en cachÃ© localmente en: `~/.cache/huggingface/transformers/`. Este es el directorio predeterminado proporcionado por la variable de entorno de shell `HF_HUB_CACHE`. En Windows, el directorio predeterminado es dado por `C:\\Users\\username\\.cache\\huggingface\\transformers`. Puedes cambiar las variables de entorno de shell que se muestran a continuaciÃ³n, en orden de prioridad, para especificar un directorio de cachÃ© diferente:\n \n-1. Variable de entorno del shell (por defecto): `TRANSFORMERS_CACHE`.\n+1. Variable de entorno del shell (por defecto): `HF_HUB_CACHE`.\n 2. Variable de entorno del shell:`HF_HOME` + `transformers/`.\n 3. Variable de entorno del shell: `XDG_CACHE_HOME` + `/huggingface/transformers`.\n \n-<Tip>\n-\n-ğŸ¤— Transformers usarÃ¡ las variables de entorno de shell `PYTORCH_TRANSFORMERS_CACHE` o `PYTORCH_PRETRAINED_BERT_CACHE` si viene de una iteraciÃ³n anterior de la biblioteca y ha configurado esas variables de entorno, a menos que especifiques la variable de entorno de shell `TRANSFORMERS_CACHE`.\n-\n-</Tip>\n-\n \n ## Modo Offline\n "
        },
        {
            "sha": "9d7f0f6c66e44ff669af7143752a2d3103140553",
            "filename": "docs/source/fr/installation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Ffr%2Finstallation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Ffr%2Finstallation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Finstallation.md?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -157,18 +157,12 @@ conda install conda-forge::transformers\n \n ## Configuration du cache\n \n-Les modÃ¨les prÃ©-entraÃ®nÃ©s sont tÃ©lÃ©chargÃ©s et mis en cache localement dans le dossier suivant : `~/.cache/huggingface/hub`. C'est le dossier par dÃ©faut donnÃ© par la variable d'environnement `TRANSFORMERS_CACHE`. Sur Windows, le dossier par dÃ©faut est `C:\\Users\\nom_utilisateur\\.cache\\huggingface\\hub`. Vous pouvez modifier les variables d'environnement indiquÃ©es ci-dessous - par ordre de prioritÃ© - pour spÃ©cifier un dossier de cache diffÃ©rent :\n+Les modÃ¨les prÃ©-entraÃ®nÃ©s sont tÃ©lÃ©chargÃ©s et mis en cache localement dans le dossier suivant : `~/.cache/huggingface/hub`. C'est le dossier par dÃ©faut donnÃ© par la variable d'environnement `HF_HUB_CACHE`. Sur Windows, le dossier par dÃ©faut est `C:\\Users\\nom_utilisateur\\.cache\\huggingface\\hub`. Vous pouvez modifier les variables d'environnement indiquÃ©es ci-dessous - par ordre de prioritÃ© - pour spÃ©cifier un dossier de cache diffÃ©rent :\n \n-1. Variable d'environnement (par dÃ©faut) : `HF_HUB_CACHE` ou `TRANSFORMERS_CACHE`.\n+1. Variable d'environnement (par dÃ©faut) : `HF_HUB_CACHE`.\n 2. Variable d'environnement : `HF_HOME`.\n 3. Variable d'environnement : `XDG_CACHE_HOME` + `/huggingface`.\n \n-<Tip>\n-\n-ğŸ¤— Transformers utilisera les variables d'environnement `PYTORCH_TRANSFORMERS_CACHE` ou `PYTORCH_PRETRAINED_BERT_CACHE` si vous utilisez une version prÃ©cÃ©dente de cette librairie et avez dÃ©fini ces variables d'environnement, sauf si vous spÃ©cifiez la variable d'environnement `TRANSFORMERS_CACHE`.\n-\n-</Tip>\n-\n ## Mode hors ligne\n \n ğŸ¤— Transformers peut fonctionner dans un environnement cloisonnÃ© ou hors ligne en n'utilisant que des fichiers locaux. DÃ©finissez la variable d'environnement `HF_HUB_OFFLINE=1` pour activer ce mode."
        },
        {
            "sha": "3023ae0b7c9f8d0061347ee2073243177b904b20",
            "filename": "docs/source/it/add_new_model.md",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fit%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fit%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fadd_new_model.md?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -746,11 +746,9 @@ Il metodo `push_to_hub`, presente in tutti i modelli `transformers`, Ã© una mani\n \n ```python\n brand_new_bert.push_to_hub(\n-    repo_path_or_name=\"brand_new_bert\",\n+    repo_id=\"brand_new_bert\"\n     # Uncomment the following line to push to an organization\n-    # organization=\"<ORGANIZATION>\",\n-    commit_message=\"Add model\",\n-    use_temp_dir=True,\n+    # repo_id=\"<ORGANIZATION>/brand_new_bert\"\n )\n ```\n "
        },
        {
            "sha": "748c6ee591b0f7542a2a94bdf2e77caaf50b6e86",
            "filename": "docs/source/it/installation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fit%2Finstallation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fit%2Finstallation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Finstallation.md?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -138,18 +138,12 @@ conda install conda-forge::transformers\n \n ## Impostazione della cache\n \n-I modelli pre-allenati sono scaricati e memorizzati localmente nella cache in: `~/.cache/huggingface/transformers/`. Questa Ã¨ la directory di default data dalla variabile d'ambiente della shell `TRANSFORMERS_CACHE`. Su Windows, la directory di default Ã¨ data da `C:\\Users\\username\\.cache\\huggingface\\transformers`. Puoi cambiare le variabili d'ambiente della shell indicate in seguito, in ordine di prioritÃ , per specificare una directory differente per la cache:\n+I modelli pre-allenati sono scaricati e memorizzati localmente nella cache in: `~/.cache/huggingface/transformers/`. Questa Ã¨ la directory di default data dalla variabile d'ambiente della shell `HF_HUB_CACHE`. Su Windows, la directory di default Ã¨ data da `C:\\Users\\username\\.cache\\huggingface\\transformers`. Puoi cambiare le variabili d'ambiente della shell indicate in seguito, in ordine di prioritÃ , per specificare una directory differente per la cache:\n \n-1. Variabile d'ambiente della shell (default): `TRANSFORMERS_CACHE`.\n+1. Variabile d'ambiente della shell (default): `HF_HUB_CACHE`.\n 2. Variabile d'ambiente della shell: `HF_HOME` + `transformers/`.\n 3. Variabile d'ambiente della shell: `XDG_CACHE_HOME` + `/huggingface/transformers`.\n \n-<Tip>\n-\n-ğŸ¤— Transformers utilizzerÃ  le variabili d'ambiente della shell `PYTORCH_TRANSFORMERS_CACHE` o `PYTORCH_PRETRAINED_BERT_CACHE` se si proviene da un'iterazione precedente di questa libreria e sono state impostate queste variabili d'ambiente, a meno che non si specifichi la variabile d'ambiente della shell `TRANSFORMERS_CACHE`.\n-\n-</Tip>\n-\n ## ModalitÃ  Offline\n \n ğŸ¤— Transformers puÃ² essere eseguita in un ambiente firewalled o offline utilizzando solo file locali. Imposta la variabile d'ambiente `HF_HUB_OFFLINE=1` per abilitare questo comportamento."
        },
        {
            "sha": "2763fcb7a2f19e2c0b9e48e426287b7b94357031",
            "filename": "docs/source/ja/installation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fja%2Finstallation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fja%2Finstallation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Finstallation.md?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -143,18 +143,12 @@ conda install conda-forge::transformers\n \n ## ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š\n \n-å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™: `~/.cache/huggingface/hub`. ã“ã‚Œã¯ã‚·ã‚§ãƒ«ç’°å¢ƒå¤‰æ•°`TRANSFORMERS_CACHE`ã§æŒ‡å®šã•ã‚Œã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã™ã€‚Windowsã§ã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯`C:\\Users\\username\\.cache\\huggingface\\hub`ã«ãªã£ã¦ã„ã¾ã™ã€‚ç•°ãªã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã™ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®ã‚·ã‚§ãƒ«ç’°å¢ƒå¤‰æ•°ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚å„ªå…ˆåº¦ã¯ä»¥ä¸‹ã®é †ç•ªã«å¯¾å¿œã—ã¾ã™:\n+å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™: `~/.cache/huggingface/hub`. ã“ã‚Œã¯ã‚·ã‚§ãƒ«ç’°å¢ƒå¤‰æ•°`HF_HUB_CACHE`ã§æŒ‡å®šã•ã‚Œã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã™ã€‚Windowsã§ã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯`C:\\Users\\username\\.cache\\huggingface\\hub`ã«ãªã£ã¦ã„ã¾ã™ã€‚ç•°ãªã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã™ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®ã‚·ã‚§ãƒ«ç’°å¢ƒå¤‰æ•°ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚å„ªå…ˆåº¦ã¯ä»¥ä¸‹ã®é †ç•ªã«å¯¾å¿œã—ã¾ã™:\n \n-1. ã‚·ã‚§ãƒ«ç’°å¢ƒå¤‰æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ): `HF_HUB_CACHE` ã¾ãŸã¯ `TRANSFORMERS_CACHE`.\n+1. ã‚·ã‚§ãƒ«ç’°å¢ƒå¤‰æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ): `HF_HUB_CACHE`.\n 2. ã‚·ã‚§ãƒ«ç’°å¢ƒå¤‰æ•°: `HF_HOME`.\n 3. ã‚·ã‚§ãƒ«ç’°å¢ƒå¤‰æ•°: `XDG_CACHE_HOME` + `/huggingface`.\n \n-<Tip>\n-\n-ã‚‚ã—ã€ä»¥å‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã„ãŸäººã§ã€`PYTORCH_TRANSFORMERS_CACHE`ã¾ãŸã¯`PYTORCH_PRETRAINED_BERT_CACHE`ã‚’è¨­å®šã—ã¦ã„ãŸå ´åˆã€ã‚·ã‚§ãƒ«ç’°å¢ƒå¤‰æ•°`TRANSFORMERS_CACHE`ã‚’æŒ‡å®šã—ãªã„é™ã‚ŠğŸ¤— Transformersã¯ã“ã‚Œã‚‰ã®ã‚·ã‚§ãƒ«ç’°å¢ƒå¤‰æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n-\n-</Tip>\n-\n ## ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰\n \n ğŸ¤— Transformersã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ã‚„ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã®ç’°å¢ƒã§ã‚‚å‹•ä½œã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®å‹•ä½œã‚’æœ‰åŠ¹ã«ã™ã‚‹ãŸã‚ã«ã¯ã€ç’°å¢ƒå¤‰æ•°`HF_HUB_OFFLINE=1`ã‚’è¨­å®šã—ã¾ã™ã€‚"
        },
        {
            "sha": "db0f7b13a5ee23a34fd4dd2074b5ac362356f9cf",
            "filename": "docs/source/ko/installation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fko%2Finstallation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fko%2Finstallation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finstallation.md?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -143,18 +143,12 @@ conda install conda-forge::transformers\n \n ## ìºì‹œ êµ¬ì„±í•˜ê¸°[[cache-setup]]\n \n-ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ì€ ë‹¤ìš´ë¡œë“œëœ í›„ ë¡œì»¬ ê²½ë¡œ `~/.cache/huggingface/hub`ì— ìºì‹œë©ë‹ˆë‹¤. ì…¸ í™˜ê²½ ë³€ìˆ˜ `TRANSFORMERS_CACHE`ì˜ ê¸°ë³¸ ë””ë ‰í„°ë¦¬ì…ë‹ˆë‹¤. Windowsì˜ ê²½ìš° ê¸°ë³¸ ë””ë ‰í„°ë¦¬ëŠ” `C:\\Users\\username\\.cache\\huggingface\\hub`ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ì…¸ í™˜ê²½ ë³€ìˆ˜ë¥¼ (ìš°ì„  ìˆœìœ„) ìˆœì„œëŒ€ë¡œ ë³€ê²½í•˜ì—¬ ë‹¤ë¥¸ ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ì€ ë‹¤ìš´ë¡œë“œëœ í›„ ë¡œì»¬ ê²½ë¡œ `~/.cache/huggingface/hub`ì— ìºì‹œë©ë‹ˆë‹¤. ì…¸ í™˜ê²½ ë³€ìˆ˜ `HF_HUB_CACHE`ì˜ ê¸°ë³¸ ë””ë ‰í„°ë¦¬ì…ë‹ˆë‹¤. Windowsì˜ ê²½ìš° ê¸°ë³¸ ë””ë ‰í„°ë¦¬ëŠ” `C:\\Users\\username\\.cache\\huggingface\\hub`ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ì…¸ í™˜ê²½ ë³€ìˆ˜ë¥¼ (ìš°ì„  ìˆœìœ„) ìˆœì„œëŒ€ë¡œ ë³€ê²½í•˜ì—¬ ë‹¤ë¥¸ ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n \n-1. ì…¸ í™˜ê²½ ë³€ìˆ˜ (ê¸°ë³¸): `HF_HUB_CACHE` ë˜ëŠ” `TRANSFORMERS_CACHE`\n+1. ì…¸ í™˜ê²½ ë³€ìˆ˜ (ê¸°ë³¸): `HF_HUB_CACHE`\n 2. ì…¸ í™˜ê²½ ë³€ìˆ˜: `HF_HOME`\n 3. ì…¸ í™˜ê²½ ë³€ìˆ˜: `XDG_CACHE_HOME` + `/huggingface`\n \n-<Tip>\n-\n-ê³¼ê±° ğŸ¤— Transformersì—ì„œ ì“°ì˜€ë˜ ì…¸ í™˜ê²½ ë³€ìˆ˜ `PYTORCH_TRANSFORMERS_CACHE` ë˜ëŠ” `PYTORCH_PRETRAINED_BERT_CACHE`ì´ ì„¤ì •ë˜ìˆë‹¤ë©´, ì…¸ í™˜ê²½ ë³€ìˆ˜ `TRANSFORMERS_CACHE`ì„ ì§€ì •í•˜ì§€ ì•ŠëŠ” í•œ ìš°ì„  ì‚¬ìš©ë©ë‹ˆë‹¤.\n-\n-</Tip>\n-\n ## ì˜¤í”„ë¼ì¸ ëª¨ë“œ[[offline-mode]]\n \n ğŸ¤— Transformersë¥¼ ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©í•˜ë„ë¡ í•´ì„œ ë°©í™”ë²½ ë˜ëŠ” ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í™œì„±í™”í•˜ë ¤ë©´ `HF_HUB_OFFLINE=1` í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
        },
        {
            "sha": "133b4897d9c317f9ec6c5e04d61779f2d200ebfe",
            "filename": "docs/source/pt/installation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fpt%2Finstallation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fpt%2Finstallation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Finstallation.md?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -153,22 +153,14 @@ conda install conda-forge::transformers\n ## ConfiguraÃ§Ã£o do CachÃª\n \n Os modelos prÃ©-treinados sÃ£o baixados e armazenados no cachÃª local, encontrado em `~/.cache/huggingface/transformers/`.\n-Este Ã© o diretÃ³rio padrÃ£o determinado pela variÃ¡vel `TRANSFORMERS_CACHE` dentro do shell.\n+Este Ã© o diretÃ³rio padrÃ£o determinado pela variÃ¡vel `HF_HUB_CACHE` dentro do shell.\n No Windows, este diretÃ³rio prÃ©-definido Ã© dado por `C:\\Users\\username\\.cache\\huggingface\\transformers`.\n Ã‰ possÃ­vel mudar as variÃ¡veis dentro do shell em ordem de prioridade para especificar um diretÃ³rio de cachÃª diferente:\n \n-1. VariÃ¡vel de ambiente do shell (por padrÃ£o): `TRANSFORMERS_CACHE`.\n+1. VariÃ¡vel de ambiente do shell (por padrÃ£o): `HF_HUB_CACHE`.\n 2. VariÃ¡vel de ambiente do shell:`HF_HOME` + `transformers/`.\n 3. VariÃ¡vel de ambiente do shell: `XDG_CACHE_HOME` + `/huggingface/transformers`.\n \n-<Tip>\n-\n-    O ğŸ¤— Transformers usarÃ¡ as variÃ¡veis de ambiente do shell `PYTORCH_TRANSFORMERS_CACHE` ou `PYTORCH_PRETRAINED_BERT_CACHE`\n-    se estiver vindo de uma versÃ£o anterior da biblioteca que tenha configurado essas variÃ¡veis de ambiente, a menos que\n-    vocÃª especifique a variÃ¡vel de ambiente do shell `TRANSFORMERS_CACHE`.\n-\n-</Tip>\n-\n \n ## Modo Offline\n "
        },
        {
            "sha": "cc0412c83b5c268c0abfee79e2b2a3bd83730784",
            "filename": "docs/source/zh/installation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fzh%2Finstallation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/docs%2Fsource%2Fzh%2Finstallation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Finstallation.md?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -129,18 +129,12 @@ conda install conda-forge::transformers\n \n ## ç¼“å­˜è®¾ç½®\n \n-é¢„è®­ç»ƒæ¨¡å‹ä¼šè¢«ä¸‹è½½å¹¶æœ¬åœ°ç¼“å­˜åˆ° `~/.cache/huggingface/hub`ã€‚è¿™æ˜¯ç”±ç¯å¢ƒå˜é‡ `TRANSFORMERS_CACHE` æŒ‡å®šçš„é»˜è®¤ç›®å½•ã€‚åœ¨ Windows ä¸Šï¼Œé»˜è®¤ç›®å½•ä¸º `C:\\Users\\username\\.cache\\huggingface\\hub`ã€‚ä½ å¯ä»¥æŒ‰ç…§ä¸åŒä¼˜å…ˆçº§æ”¹å˜ä¸‹è¿°ç¯å¢ƒå˜é‡ï¼Œä»¥æŒ‡å®šä¸åŒçš„ç¼“å­˜ç›®å½•ã€‚\n+é¢„è®­ç»ƒæ¨¡å‹ä¼šè¢«ä¸‹è½½å¹¶æœ¬åœ°ç¼“å­˜åˆ° `~/.cache/huggingface/hub`ã€‚è¿™æ˜¯ç”±ç¯å¢ƒå˜é‡ `HF_HUB_CACHE` æŒ‡å®šçš„é»˜è®¤ç›®å½•ã€‚åœ¨ Windows ä¸Šï¼Œé»˜è®¤ç›®å½•ä¸º `C:\\Users\\username\\.cache\\huggingface\\hub`ã€‚ä½ å¯ä»¥æŒ‰ç…§ä¸åŒä¼˜å…ˆçº§æ”¹å˜ä¸‹è¿°ç¯å¢ƒå˜é‡ï¼Œä»¥æŒ‡å®šä¸åŒçš„ç¼“å­˜ç›®å½•ã€‚\n \n-1. ç¯å¢ƒå˜é‡ï¼ˆé»˜è®¤ï¼‰: `HF_HUB_CACHE` æˆ– `TRANSFORMERS_CACHE`ã€‚\n+1. ç¯å¢ƒå˜é‡ï¼ˆé»˜è®¤ï¼‰: `HF_HUB_CACHE`ã€‚\n 2. ç¯å¢ƒå˜é‡ `HF_HOME`ã€‚\n 3. ç¯å¢ƒå˜é‡ `XDG_CACHE_HOME` + `/huggingface`ã€‚\n \n-<Tip>\n-\n-é™¤éä½ æ˜ç¡®æŒ‡å®šäº†ç¯å¢ƒå˜é‡ `TRANSFORMERS_CACHE`ï¼ŒğŸ¤— Transformers å°†å¯èƒ½ä¼šä½¿ç”¨è¾ƒæ—©ç‰ˆæœ¬è®¾ç½®çš„ç¯å¢ƒå˜é‡ `PYTORCH_TRANSFORMERS_CACHE` æˆ– `PYTORCH_PRETRAINED_BERT_CACHE`ã€‚\n-\n-</Tip>\n-\n ## ç¦»çº¿æ¨¡å¼\n \n ğŸ¤— Transformers å¯ä»¥ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶åœ¨é˜²ç«å¢™æˆ–ç¦»çº¿ç¯å¢ƒä¸­è¿è¡Œã€‚è®¾ç½®ç¯å¢ƒå˜é‡ `HF_HUB_OFFLINE=1` ä»¥å¯ç”¨è¯¥è¡Œä¸ºã€‚"
        },
        {
            "sha": "07c616beb9a2eb6c01a17fe4b838f918e50b6930",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -202,10 +202,7 @@\n     \"utils\": [\n         \"CONFIG_NAME\",\n         \"MODEL_CARD_NAME\",\n-        \"PYTORCH_PRETRAINED_BERT_CACHE\",\n-        \"PYTORCH_TRANSFORMERS_CACHE\",\n         \"SPIECE_UNDERLINE\",\n-        \"TRANSFORMERS_CACHE\",\n         \"WEIGHTS_NAME\",\n         \"TensorType\",\n         \"add_end_docstrings\",\n@@ -711,10 +708,7 @@\n     # Files and general utilities\n     from .utils import CONFIG_NAME as CONFIG_NAME\n     from .utils import MODEL_CARD_NAME as MODEL_CARD_NAME\n-    from .utils import PYTORCH_PRETRAINED_BERT_CACHE as PYTORCH_PRETRAINED_BERT_CACHE\n-    from .utils import PYTORCH_TRANSFORMERS_CACHE as PYTORCH_TRANSFORMERS_CACHE\n     from .utils import SPIECE_UNDERLINE as SPIECE_UNDERLINE\n-    from .utils import TRANSFORMERS_CACHE as TRANSFORMERS_CACHE\n     from .utils import WEIGHTS_NAME as WEIGHTS_NAME\n     from .utils import TensorType as TensorType\n     from .utils import add_end_docstrings as add_end_docstrings"
        },
        {
            "sha": "53d6a8a900a8088ebc01c57230b63f22cf88efc9",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -20,6 +20,7 @@\n import warnings\n from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union\n \n+from huggingface_hub import create_repo\n from packaging import version\n \n from . import __version__\n@@ -454,7 +455,7 @@ def save_pretrained(self, save_directory: str | os.PathLike, push_to_hub: bool =\n         if push_to_hub:\n             commit_message = kwargs.pop(\"commit_message\", None)\n             repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n-            repo_id = self._create_repo(repo_id, **kwargs)\n+            repo_id = create_repo(repo_id, exist_ok=True, **kwargs).repo_id\n             files_timestamps = self._get_files_timestamps(save_directory)\n \n         # This attribute is important to know on load, but should not be serialized on save."
        },
        {
            "sha": "0bdf7921cb710de071f4b4c84441f86b01eac025",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -22,6 +22,7 @@\n from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union\n \n import numpy as np\n+from huggingface_hub import create_repo\n \n from .dynamic_module_utils import custom_object_save\n from .utils import (\n@@ -361,7 +362,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n         if push_to_hub:\n             commit_message = kwargs.pop(\"commit_message\", None)\n             repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n-            repo_id = self._create_repo(repo_id, **kwargs)\n+            repo_id = create_repo(repo_id, exist_ok=True, **kwargs).repo_id\n             files_timestamps = self._get_files_timestamps(save_directory)\n \n         # If we have a custom config, we copy the file defining it in the folder and set the attributes so it can be"
        },
        {
            "sha": "423cb699b804ea2c07df0819fa632147cd6f5b55",
            "filename": "src/transformers/file_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Ffile_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Ffile_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffile_utils.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -29,16 +29,11 @@\n     ENV_VARS_TRUE_VALUES,\n     FEATURE_EXTRACTOR_NAME,\n     HF_MODULES_CACHE,\n-    HUGGINGFACE_CO_PREFIX,\n-    HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n     MODEL_CARD_NAME,\n     MULTIPLE_CHOICE_DUMMY_INPUTS,\n-    PYTORCH_PRETRAINED_BERT_CACHE,\n-    PYTORCH_TRANSFORMERS_CACHE,\n     S3_BUCKET_PREFIX,\n     SENTENCEPIECE_UNDERLINE,\n     SPIECE_UNDERLINE,\n-    TRANSFORMERS_CACHE,\n     TRANSFORMERS_DYNAMIC_MODULE_NAME,\n     WEIGHTS_INDEX_NAME,\n     WEIGHTS_NAME,\n@@ -58,7 +53,6 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     copy_func,\n-    default_cache_path,\n     define_sagemaker_information,\n     get_torch_version,\n     has_file,"
        },
        {
            "sha": "1c904f6adec3af2256abebb1253b642cce19d157",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -22,6 +22,8 @@\n from dataclasses import dataclass, is_dataclass\n from typing import TYPE_CHECKING, Any, Optional\n \n+from huggingface_hub import create_repo\n+\n from .. import __version__\n from ..configuration_utils import PreTrainedConfig\n from ..utils import (\n@@ -748,7 +750,7 @@ def save_pretrained(\n         if push_to_hub:\n             commit_message = kwargs.pop(\"commit_message\", None)\n             repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n-            repo_id = self._create_repo(repo_id, **kwargs)\n+            repo_id = create_repo(repo_id, exist_ok=True, **kwargs).repo_id\n             files_timestamps = self._get_files_timestamps(save_directory)\n \n         output_config_file = os.path.join(save_directory, config_file_name)"
        },
        {
            "sha": "2a75c96e72b477c30b4f0c4976834eabf3c094fa",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -12,13 +12,13 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\n import copy\n import json\n import os\n from typing import Any, Optional, TypeVar, Union\n \n import numpy as np\n+from huggingface_hub import create_repo\n \n from .dynamic_module_utils import custom_object_save\n from .feature_extraction_utils import BatchFeature as BaseBatchFeature\n@@ -208,7 +208,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n         if push_to_hub:\n             commit_message = kwargs.pop(\"commit_message\", None)\n             repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n-            repo_id = self._create_repo(repo_id, **kwargs)\n+            repo_id = create_repo(repo_id, exist_ok=True, **kwargs).repo_id\n             files_timestamps = self._get_files_timestamps(save_directory)\n \n         # If we have a custom config, we copy the file defining it in the folder and set the attributes so it can be"
        },
        {
            "sha": "403f5f6eb29530d43657a106d569babca676999a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -36,7 +36,7 @@\n from zipfile import is_zipfile\n \n import torch\n-from huggingface_hub import split_torch_state_dict_into_shards\n+from huggingface_hub import create_repo, split_torch_state_dict_into_shards\n from packaging import version\n from safetensors import safe_open\n from safetensors.torch import save_file as safe_save_file\n@@ -3013,8 +3013,6 @@ def save_pretrained(\n             kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n-        ignore_metadata_errors = kwargs.pop(\"ignore_metadata_errors\", False)\n-\n         if token is not None:\n             kwargs[\"token\"] = token\n \n@@ -3055,7 +3053,7 @@ def save_pretrained(\n             commit_message = kwargs.pop(\"commit_message\", None)\n             repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n             create_pr = kwargs.pop(\"create_pr\", False)\n-            repo_id = self._create_repo(repo_id, **kwargs)\n+            repo_id = create_repo(repo_id, exist_ok=True, **kwargs).repo_id\n             files_timestamps = self._get_files_timestamps(save_directory)\n \n         metadata = {}\n@@ -3340,9 +3338,7 @@ def save_pretrained(\n \n         if push_to_hub:\n             # Eventually create an empty model card\n-            model_card = create_and_tag_model_card(\n-                repo_id, self.model_tags, token=token, ignore_metadata_errors=ignore_metadata_errors\n-            )\n+            model_card = create_and_tag_model_card(repo_id, self.model_tags, token=token)\n \n             # Update model card if needed:\n             model_card.save(os.path.join(save_directory, \"README.md\"))"
        },
        {
            "sha": "1688c936b1229fd02d4cd68d6fe24a54f0ba6eac",
            "filename": "src/transformers/models/conditional_detr/convert_conditional_detr_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconvert_conditional_detr_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconvert_conditional_detr_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconvert_conditional_detr_original_pytorch_checkpoint_to_pytorch.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -292,7 +292,7 @@ def convert_conditional_detr_checkpoint(model_name, pytorch_dump_folder_path):\n     model = ConditionalDetrForSegmentation(config) if is_panoptic else ConditionalDetrForObjectDetection(config)\n     model.load_state_dict(state_dict)\n     model.eval()\n-    model.push_to_hub(repo_id=model_name, organization=\"DepuMeng\", commit_message=\"Add model\")\n+    model.push_to_hub(repo_id=f\"DepuMeng/{model_name}\", commit_message=\"Add model\")\n     # verify our conversion\n     original_outputs = conditional_detr(pixel_values)\n     outputs = model(pixel_values)"
        },
        {
            "sha": "c7fffee14bd35f9e833cc4a64ea4843d53ccaac7",
            "filename": "src/transformers/models/convnext/convert_convnext_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconvert_convnext_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconvert_convnext_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconvert_convnext_to_pytorch.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -214,11 +214,7 @@ def convert_convnext_checkpoint(checkpoint_url, pytorch_dump_folder_path):\n     if \"22k\" in checkpoint_url and \"1k\" in checkpoint_url:\n         model_name += \"-22k-1k\"\n \n-    model.push_to_hub(\n-        repo_path_or_name=Path(pytorch_dump_folder_path, model_name),\n-        organization=\"nielsr\",\n-        commit_message=\"Add model\",\n-    )\n+    model.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "ecb8b0784e0573a4a421db89f5487ac2e1cfe5e2",
            "filename": "src/transformers/models/deformable_detr/convert_deformable_detr_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconvert_deformable_detr_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconvert_deformable_detr_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconvert_deformable_detr_to_pytorch.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -198,7 +198,7 @@ def convert_deformable_detr_checkpoint(\n         model_name += \"-with-box-refine\" if with_box_refine else \"\"\n         model_name += \"-two-stage\" if two_stage else \"\"\n         print(\"Pushing model to hub...\")\n-        model.push_to_hub(repo_path_or_name=model_name, organization=\"nielsr\", commit_message=\"Add model\")\n+        model.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "924aeaba5082f344bdd44578b1b2d0f00e9454a4",
            "filename": "src/transformers/models/dit/convert_dit_unilm_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fdit%2Fconvert_dit_unilm_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fdit%2Fconvert_dit_unilm_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdit%2Fconvert_dit_unilm_to_pytorch.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -196,18 +196,8 @@ def convert_dit_checkpoint(checkpoint_url, pytorch_dump_folder_path, push_to_hub\n             model_name = \"dit-base\" if \"base\" in checkpoint_url else \"dit-large\"\n         else:\n             model_name = \"dit-base-finetuned-rvlcdip\" if \"dit-b\" in checkpoint_url else \"dit-large-finetuned-rvlcdip\"\n-        image_processor.push_to_hub(\n-            repo_path_or_name=Path(pytorch_dump_folder_path, model_name),\n-            organization=\"nielsr\",\n-            commit_message=\"Add image processor\",\n-            use_temp_dir=True,\n-        )\n-        model.push_to_hub(\n-            repo_path_or_name=Path(pytorch_dump_folder_path, model_name),\n-            organization=\"nielsr\",\n-            commit_message=\"Add model\",\n-            use_temp_dir=True,\n-        )\n+        image_processor.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n+        model.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "42d647d25662963ba82d41a5ebc408e36fad5b62",
            "filename": "src/transformers/models/dpt/convert_dpt_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_to_pytorch.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -239,18 +239,8 @@ def convert_dpt_checkpoint(checkpoint_url, pytorch_dump_folder_path, push_to_hub\n \n     if push_to_hub:\n         print(\"Pushing model to hub...\")\n-        model.push_to_hub(\n-            repo_path_or_name=Path(pytorch_dump_folder_path, model_name),\n-            organization=\"nielsr\",\n-            commit_message=\"Add model\",\n-            use_temp_dir=True,\n-        )\n-        image_processor.push_to_hub(\n-            repo_path_or_name=Path(pytorch_dump_folder_path, model_name),\n-            organization=\"nielsr\",\n-            commit_message=\"Add image processor\",\n-            use_temp_dir=True,\n-        )\n+        model.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n+        image_processor.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "82290776e0b4e6586f0563940e58c730a6ff5fac",
            "filename": "src/transformers/models/glpn/convert_glpn_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fglpn%2Fconvert_glpn_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fglpn%2Fconvert_glpn_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fconvert_glpn_to_pytorch.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -16,7 +16,6 @@\n \n import argparse\n from collections import OrderedDict\n-from pathlib import Path\n \n import requests\n import torch\n@@ -179,18 +178,8 @@ def convert_glpn_checkpoint(checkpoint_path, pytorch_dump_folder_path, push_to_h\n     # finally, push to hub if required\n     if push_to_hub:\n         logger.info(\"Pushing model and image processor to the hub...\")\n-        model.push_to_hub(\n-            repo_path_or_name=Path(pytorch_dump_folder_path, model_name),\n-            organization=\"nielsr\",\n-            commit_message=\"Add model\",\n-            use_temp_dir=True,\n-        )\n-        image_processor.push_to_hub(\n-            repo_path_or_name=Path(pytorch_dump_folder_path, model_name),\n-            organization=\"nielsr\",\n-            commit_message=\"Add image processor\",\n-            use_temp_dir=True,\n-        )\n+        model.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n+        image_processor.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "1556780c59f99ebb9be05ccdc3940076f5c4c75d",
            "filename": "src/transformers/models/got_ocr2/convert_got_ocr2_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -135,7 +135,7 @@ def write_model(\n     print(\"Saving the model.\")\n     model.save_pretrained(model_path)\n     if push_to_hub:\n-        model.push_to_hub(\"stepfun-ai/GOT-OCR-2.0-hf\", use_temp_dir=True)\n+        model.push_to_hub(\"stepfun-ai/GOT-OCR-2.0-hf\")\n     del state_dict, model\n \n     # Safety check: reload the converted model\n@@ -217,7 +217,7 @@ def write_tokenizer(tokenizer_path: str, save_dir: str, push_to_hub: bool = Fals\n     tokenizer.save_pretrained(save_dir)\n \n     if push_to_hub:\n-        tokenizer.push_to_hub(\"stepfun-ai/GOT-OCR-2.0-hf\", use_temp_dir=True)\n+        tokenizer.push_to_hub(\"stepfun-ai/GOT-OCR-2.0-hf\")\n \n \n def write_image_processor(save_dir: str, push_to_hub: bool = False):\n@@ -233,7 +233,7 @@ def write_image_processor(save_dir: str, push_to_hub: bool = False):\n \n     image_processor.save_pretrained(save_dir)\n     if push_to_hub:\n-        image_processor.push_to_hub(\"stepfun-ai/GOT-OCR-2.0-hf\", use_temp_dir=True)\n+        image_processor.push_to_hub(\"stepfun-ai/GOT-OCR-2.0-hf\")\n \n \n def main():"
        },
        {
            "sha": "1c4ccbd62b8ed0b2b30630707ae1cbb097101801",
            "filename": "src/transformers/models/groupvit/convert_groupvit_nvlab_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconvert_groupvit_nvlab_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconvert_groupvit_nvlab_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconvert_groupvit_nvlab_to_hf.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -191,8 +191,8 @@ def convert_groupvit_checkpoint(\n \n     if push_to_hub:\n         print(\"Pushing to the hub...\")\n-        processor.push_to_hub(model_name, organization=\"nielsr\")\n-        model.push_to_hub(model_name, organization=\"nielsr\")\n+        processor.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n+        model.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "4570342006d32cb724e566f629f4486e17de8c4a",
            "filename": "src/transformers/models/internvl/convert_internvl_weights_to_hf.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -244,7 +244,8 @@ def write_model(\n     config.architectures = [\"InternVLForConditionalGeneration\"]\n     config.save_pretrained(model_path)\n     if push_to_hub:\n-        config.push_to_hub(hub_dir, use_temp_dir=True)\n+        model_name = (hub_dir or model_path).split(os.path.sep)[-1]\n+        config.push_to_hub(model_name)\n     print(\"Model config saved successfully...\")\n \n     # ------------------------------------------------------------\n@@ -303,9 +304,10 @@ def write_model(\n     print(\"Unexpected keys:\", unexpected_keys)\n \n     print(\"Saving the model.\")\n+    model_name = model_path.split(os.path.sep)[-1]\n     model.save_pretrained(model_path)\n     if push_to_hub:\n-        model.push_to_hub(hub_dir, use_temp_dir=True)\n+        model.push_to_hub(model_name)\n \n     image_processor = GotOcr2ImageProcessorFast.from_pretrained(model_path)\n     video_processor = InternVLVideoProcessor.from_pretrained(model_path)\n@@ -318,7 +320,7 @@ def write_model(\n     )\n     processor.save_pretrained(model_path)\n     if push_to_hub:\n-        processor.push_to_hub(hub_dir, use_temp_dir=True)\n+        processor.push_to_hub(model_name)\n \n     # generation config\n     if get_lm_type(input_base_path) == \"llama\":\n@@ -330,7 +332,7 @@ def write_model(\n         )\n         generation_config.save_pretrained(model_path)\n         if push_to_hub:\n-            generation_config.push_to_hub(hub_dir, use_temp_dir=True)\n+            generation_config.push_to_hub(model_name)\n \n     # del state_dict, model\n \n@@ -393,8 +395,9 @@ def write_tokenizer(\n \n     tokenizer.chat_template = chat_template\n     tokenizer.save_pretrained(save_dir)\n+    model_name = (hub_dir or save_dir).split(os.path.sep)[-1]\n     if push_to_hub:\n-        tokenizer.push_to_hub(hub_dir, use_temp_dir=True)\n+        tokenizer.push_to_hub(model_name)\n \n \n def write_image_processor(save_dir: str, push_to_hub: bool = False, hub_dir: Optional[str] = None):\n@@ -410,8 +413,9 @@ def write_image_processor(save_dir: str, push_to_hub: bool = False, hub_dir: Opt\n     )\n \n     image_processor.save_pretrained(save_dir)\n+    model_name = (hub_dir or save_dir).split(os.path.sep)[-1]\n     if push_to_hub:\n-        image_processor.push_to_hub(hub_dir, use_temp_dir=True)\n+        image_processor.push_to_hub(model_name)\n \n \n def main():"
        },
        {
            "sha": "41d4f538588a9aeff322f457e5467dc855493567",
            "filename": "src/transformers/models/llama/convert_llama_weights_to_hf.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -427,7 +427,8 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n         print(\"Saving in the Transformers format.\")\n         if push_to_hub:\n             print(\"Pushing to the hub.\")\n-            model.push_to_hub(model_path, safe_serialization=safe_serialization, private=True, use_temp_dir=True)\n+            model_name = model_path.split(os.path.sep)[-1]\n+            model.push_to_hub(model_name, safe_serialization=safe_serialization, private=True)\n         else:\n             print(\"Saving to disk.\")\n             model.save_pretrained(model_path, safe_serialization=safe_serialization)\n@@ -511,7 +512,8 @@ def write_tokenizer(\n \n     if push_to_hub:\n         print(f\"Pushing a {tokenizer_class.__name__} to the Hub repo - {tokenizer_path}.\")\n-        tokenizer.push_to_hub(tokenizer_path, private=True, use_temp_dir=True)\n+        model_name = tokenizer_path.split(os.path.sep)[-1]\n+        tokenizer.push_to_hub(model_name, private=True)\n     else:\n         print(f\"Saving a {tokenizer_class.__name__} to {tokenizer_path}.\")\n         tokenizer.save_pretrained(tokenizer_path)"
        },
        {
            "sha": "2594f50fe835c025d20a0fc3d6cbc2c7757f9a5b",
            "filename": "src/transformers/models/maskformer/convert_maskformer_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_original_pytorch_checkpoint_to_pytorch.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -720,13 +720,5 @@ def get_name(checkpoint_file: Path):\n         image_processor.save_pretrained(save_directory / model_name)\n         mask_former_for_instance_segmentation.save_pretrained(save_directory / model_name)\n \n-        image_processor.push_to_hub(\n-            repo_path_or_name=save_directory / model_name,\n-            commit_message=\"Add model\",\n-            use_temp_dir=True,\n-        )\n-        mask_former_for_instance_segmentation.push_to_hub(\n-            repo_path_or_name=save_directory / model_name,\n-            commit_message=\"Add model\",\n-            use_temp_dir=True,\n-        )\n+        image_processor.push_to_hub(repo_id=model_name)\n+        mask_former_for_instance_segmentation.push_to_hub(repo_id=model_name)"
        },
        {
            "sha": "f321017c34d74e83d78f5cf5449308565f03a293",
            "filename": "src/transformers/models/oneformer/convert_to_hf_oneformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Foneformer%2Fconvert_to_hf_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Foneformer%2Fconvert_to_hf_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fconvert_to_hf_oneformer.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -1180,13 +1180,6 @@ def get_name(checkpoint_file: Path):\n         processor.save_pretrained(save_directory / model_name)\n         oneformer_for_universal_segmentation.save_pretrained(save_directory / model_name)\n \n-        processor.push_to_hub(\n-            repo_id=os.path.join(\"shi-labs\", config_file.stem),\n-            commit_message=\"Add configs\",\n-            use_temp_dir=True,\n-        )\n-        oneformer_for_universal_segmentation.push_to_hub(\n-            repo_id=os.path.join(\"shi-labs\", config_file.stem),\n-            commit_message=\"Add model\",\n-            use_temp_dir=True,\n-        )\n+        model_id = f\"shi-labs/{model_name}\"\n+        processor.push_to_hub(repo_id=model_id)\n+        oneformer_for_universal_segmentation.push_to_hub(repo_id=model_id)"
        },
        {
            "sha": "a8520489d00294e90a5d8d926e013c5da9ddc500",
            "filename": "src/transformers/models/ovis2/convert_ovis2_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fovis2%2Fconvert_ovis2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fovis2%2Fconvert_ovis2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fconvert_ovis2_weights_to_hf.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -358,8 +358,9 @@ def main():\n \n     # Push to hub if requested\n     if args.push_to_hub:\n-        processor.push_to_hub(args.hub_dir, use_temp_dir=True)\n-        model.push_to_hub(args.hub_dir, use_temp_dir=True)\n+        model_name = args.hub_dir.split(\"/\")[-1]\n+        processor.push_to_hub(model_name)\n+        model.push_to_hub(model_name)\n \n     model = (\n         AutoModelForImageTextToText.from_pretrained("
        },
        {
            "sha": "8622fc56f08b0c10466a71b772f92c8ce6e26f1a",
            "filename": "src/transformers/models/perception_lm/convert_perception_lm_weights_to_hf.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -423,17 +423,13 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n         model.config.dtype = torch.bfloat16\n \n         print(\"Saving in the Transformers format.\")\n+        model_name = model_path.split(os.path.sep)[-1]\n         if push_to_hub:\n             print(\"Pushing to the hub.\")\n-            model.push_to_hub(\n-                model_path,\n-                safe_serialization=safe_serialization,\n-                private=True,\n-                use_temp_dir=True,\n-            )\n+            model.push_to_hub(model_name, safe_serialization=safe_serialization, private=True)\n         else:\n             print(\"Saving to disk.\")\n-            model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+            model.save_pretrained(model_name, safe_serialization=safe_serialization)\n \n \n class Llama3Converter(TikTokenConverter):\n@@ -543,7 +539,8 @@ def write_tokenizer(\n \n     if push_to_hub:\n         print(f\"Pushing a {tokenizer_class.__name__} to the Hub repo - {tokenizer_path}.\")\n-        processor.push_to_hub(tokenizer_path, private=True, use_temp_dir=True)\n+        model_name = tokenizer_path.split(os.path.sep)[-1]\n+        processor.push_to_hub(model_name, private=True)\n     else:\n         print(f\"Saving a {tokenizer_class.__name__} to {tokenizer_path}.\")\n         processor.save_pretrained(tokenizer_path)"
        },
        {
            "sha": "30993059a9c048dcfc0b6d2a11ad08f19f8eb996",
            "filename": "src/transformers/models/regnet/convert_regnet_seer_10b_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -257,18 +257,12 @@ def load_using_classy_vision(checkpoint_url: str) -> tuple[dict, dict]:\n         )\n         logger.info(\"Finally, pushing!\")\n         # push it to hub\n-        our_model.push_to_hub(\n-            repo_path_or_name=save_directory / model_name,\n-            commit_message=\"Add model\",\n-            output_dir=save_directory / model_name,\n-        )\n+        our_model.push_to_hub(repo_id=model_name, commit_message=\"Add model\", output_dir=save_directory / model_name)\n         size = 384\n         # we can use the convnext one\n         image_processor = AutoImageProcessor.from_pretrained(\"facebook/convnext-base-224-22k-1k\", size=size)\n         image_processor.push_to_hub(\n-            repo_path_or_name=save_directory / model_name,\n-            commit_message=\"Add image processor\",\n-            output_dir=save_directory / model_name,\n+            repo_id=model_name, commit_message=\"Add image processor\", output_dir=save_directory / model_name\n         )\n \n "
        },
        {
            "sha": "274fa214b7676ed8c7ead5959cc5017a0c08b253",
            "filename": "src/transformers/models/regnet/convert_regnet_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -201,20 +201,12 @@ def convert_weight_and_push(\n     assert torch.allclose(from_output, our_output), \"The model logits don't match the original one.\"\n \n     if push_to_hub:\n-        our_model.push_to_hub(\n-            repo_path_or_name=save_directory / name,\n-            commit_message=\"Add model\",\n-            use_temp_dir=True,\n-        )\n+        our_model.push_to_hub(repo_id=name)\n \n         size = 224 if \"seer\" not in name else 384\n         # we can use the convnext one\n         image_processor = AutoImageProcessor.from_pretrained(\"facebook/convnext-base-224-22k-1k\", size=size)\n-        image_processor.push_to_hub(\n-            repo_path_or_name=save_directory / name,\n-            commit_message=\"Add image processor\",\n-            use_temp_dir=True,\n-        )\n+        image_processor.push_to_hub(repo_id=name)\n \n         print(f\"Pushed {name}\")\n "
        },
        {
            "sha": "7d33b343f531569a50e10bcf544ce10431a1b200",
            "filename": "src/transformers/models/resnet/convert_resnet_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -105,19 +105,11 @@ def convert_weight_and_push(name: str, config: ResNetConfig, save_directory: Pat\n     print(checkpoint_name)\n \n     if push_to_hub:\n-        our_model.push_to_hub(\n-            repo_path_or_name=save_directory / checkpoint_name,\n-            commit_message=\"Add model\",\n-            use_temp_dir=True,\n-        )\n+        our_model.push_to_hub(repo_id=checkpoint_name)\n \n         # we can use the convnext one\n         image_processor = AutoImageProcessor.from_pretrained(\"facebook/convnext-base-224-22k-1k\")\n-        image_processor.push_to_hub(\n-            repo_path_or_name=save_directory / checkpoint_name,\n-            commit_message=\"Add image processor\",\n-            use_temp_dir=True,\n-        )\n+        image_processor.push_to_hub(repo_id=checkpoint_name)\n \n         print(f\"Pushed {checkpoint_name}\")\n "
        },
        {
            "sha": "56f0e5819d1087021ed9c38ff82b1ee43d7976b5",
            "filename": "src/transformers/models/sam3/convert_sam3_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fsam3%2Fconvert_sam3_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fsam3%2Fconvert_sam3_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fconvert_sam3_to_hf.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -397,8 +397,8 @@ def convert_sam3_checkpoint(\n         if repo_id is None:\n             raise ValueError(\"repo_id must be provided when push_to_hub=True\")\n         print(f\"Pushing model to Hub: {repo_id}\")\n-        model.push_to_hub(repo_id, use_temp_dir=True)\n-        processor.push_to_hub(repo_id, use_temp_dir=True)\n+        model.push_to_hub(repo_id)\n+        processor.push_to_hub(repo_id)\n \n     print(\"Conversion complete!\")\n     print(f\"Model saved successfully to: {output_path}\")"
        },
        {
            "sha": "b6667c771c8782f79cc0a8949f5bf9060606012d",
            "filename": "src/transformers/models/sam3_video/convert_sam3_video_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconvert_sam3_video_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconvert_sam3_video_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconvert_sam3_video_to_hf.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -683,8 +683,8 @@ def convert_sam3_checkpoint(\n         if repo_id is None:\n             raise ValueError(\"repo_id must be provided when push_to_hub=True\")\n         print(f\"Pushing model to Hub: {repo_id}\")\n-        model.push_to_hub(repo_id, use_temp_dir=True, private=True)\n-        processor.push_to_hub(repo_id, use_temp_dir=True, private=True)\n+        model.push_to_hub(repo_id, private=True)\n+        processor.push_to_hub(repo_id, private=True)\n \n     print(\"Conversion complete!\")\n     print(f\"Model saved successfully to: {output_path}\")"
        },
        {
            "sha": "c4cffe3b64fc7d7ea5e85ed96fbf4c90f218c876",
            "filename": "src/transformers/models/swinv2/convert_swinv2_timm_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fswinv2%2Fconvert_swinv2_timm_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fswinv2%2Fconvert_swinv2_timm_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fconvert_swinv2_timm_to_pytorch.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -16,7 +16,6 @@\n \n import argparse\n import json\n-from pathlib import Path\n \n import requests\n import timm\n@@ -195,11 +194,7 @@ def convert_swinv2_checkpoint(swinv2_name, pytorch_dump_folder_path):\n     print(f\"Saving image processor to {pytorch_dump_folder_path}\")\n     image_processor.save_pretrained(pytorch_dump_folder_path)\n \n-    model.push_to_hub(\n-        repo_path_or_name=Path(pytorch_dump_folder_path, swinv2_name),\n-        organization=\"nandwalritik\",\n-        commit_message=\"Add model\",\n-    )\n+    model.push_to_hub(repo_id=f\"nandwalritik/{swinv2_name}\", commit_message=\"Add model\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "686376d381d1334cb023673354da4b90666c7398",
            "filename": "src/transformers/models/videomae/convert_videomae_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fvideomae%2Fconvert_videomae_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fvideomae%2Fconvert_videomae_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fconvert_videomae_to_pytorch.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -294,7 +294,7 @@ def convert_videomae_checkpoint(checkpoint_url, pytorch_dump_folder_path, model_\n \n     if push_to_hub:\n         print(\"Pushing to the hub...\")\n-        model.push_to_hub(model_name, organization=\"nielsr\")\n+        model.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "1dfb4e3aa6a5776ea56a1ff5a41cc1d7fd672dc3",
            "filename": "src/transformers/models/x_clip/convert_x_clip_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconvert_x_clip_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconvert_x_clip_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconvert_x_clip_original_pytorch_to_hf.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -361,9 +361,9 @@ def convert_xclip_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_\n \n     if push_to_hub:\n         print(\"Pushing model, processor and slow tokenizer files to the hub...\")\n-        model.push_to_hub(model_name, organization=\"nielsr\")\n-        processor.push_to_hub(model_name, organization=\"nielsr\")\n-        slow_tokenizer.push_to_hub(model_name, organization=\"nielsr\")\n+        model.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n+        processor.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n+        slow_tokenizer.push_to_hub(repo_id=f\"nielsr/{model_name}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "d475c296412ac2857bf6385e493701cee5846aac",
            "filename": "src/transformers/models/yolos/convert_yolos_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fyolos%2Fconvert_yolos_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fmodels%2Fyolos%2Fconvert_yolos_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fconvert_yolos_to_pytorch.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -237,8 +237,8 @@ def convert_yolos_checkpoint(\n \n         print(\"Pushing to the hub...\")\n         model_name = model_mapping[yolos_name]\n-        image_processor.push_to_hub(model_name, organization=\"hustvl\")\n-        model.push_to_hub(model_name, organization=\"hustvl\")\n+        image_processor.push_to_hub(repo_id=f\"hustvl/{model_name}\")\n+        model.push_to_hub(repo_id=f\"hustvl/{model_name}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "4e804b83605d718961d0d9d1e98e282793f2f6dc",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -34,7 +34,6 @@\n from ..tokenization_utils import PreTrainedTokenizer\n from ..utils import (\n     CONFIG_NAME,\n-    HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n     cached_file,\n     extract_commit_hash,\n     find_adapter_config_file,\n@@ -832,8 +831,7 @@ def pipeline(\n         model, default_revision = get_default_model_and_revision(targeted_task, task_options)\n         revision = revision if revision is not None else default_revision\n         logger.warning(\n-            f\"No model was supplied, defaulted to {model} and revision\"\n-            f\" {revision} ({HUGGINGFACE_CO_RESOLVE_ENDPOINT}/{model}).\\n\"\n+            f\"No model was supplied, defaulted to {model} and revision {revision}.\\n\"\n             \"Using a pipeline without specifying a model name and revision in production is not recommended.\"\n         )\n         hub_kwargs[\"revision\"] = revision"
        },
        {
            "sha": "ff24b9df710c38723e7bb58efe00320b3d07fe3d",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -28,6 +28,7 @@\n \n import numpy as np\n import typing_extensions\n+from huggingface_hub import create_repo\n from huggingface_hub.dataclasses import validate_typed_dict\n from huggingface_hub.errors import EntryNotFoundError\n \n@@ -808,7 +809,7 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n         if push_to_hub:\n             commit_message = kwargs.pop(\"commit_message\", None)\n             repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n-            repo_id = self._create_repo(repo_id, **kwargs)\n+            repo_id = create_repo(repo_id, exist_ok=True, **kwargs).repo_id\n             files_timestamps = self._get_files_timestamps(save_directory)\n         # If we have a custom config, we copy the file defining it in the folder and set the attributes so it can be\n         # loaded from the Hub."
        },
        {
            "sha": "c79c42bb52b9db78cec7af1d42c48913776b1b40",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -11,7 +11,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n import os\n import re\n import shutil\n@@ -22,6 +21,7 @@\n from typing import Any, Union, overload\n \n import numpy as np\n+from huggingface_hub import create_repo\n \n from transformers.audio_utils import load_audio_as\n from transformers.tokenization_utils_base import (\n@@ -1825,8 +1825,6 @@ def save_pretrained(\n         commit_message: str | None = None,\n         repo_id: str | None = None,\n         private: bool | None = None,\n-        repo_url: str | None = None,\n-        organization: str | None = None,\n         **kwargs,\n     ) -> tuple[str, ...]:\n         \"\"\"\n@@ -1848,8 +1846,6 @@ def save_pretrained(\n             commit_message (`str`, *optional*): The commit message to use when pushing to the hub.\n             repo_id (`str`, *optional*): The name of the repository to which push to the Hub.\n             private (`bool`, *optional*): Whether the model repository is private or not.\n-            repo_url (`str`, *optional*): The URL to the Git repository to which push to the Hub.\n-            organization (`str`, *optional*): The name of the organization in which you would like to push your model.\n             kwargs (`Dict[str, Any]`, *optional*):\n                 Not supported by `MistralCommonTokenizer.save_pretrained`.\n                 Will raise an error if used.\n@@ -1869,9 +1865,7 @@ def save_pretrained(\n \n         if push_to_hub:\n             repo_id = repo_id or str(save_directory).split(os.path.sep)[-1]\n-            repo_id = self._create_repo(\n-                repo_id, token=token, private=private, repo_url=repo_url, organization=organization\n-            )\n+            repo_id = create_repo(repo_id, token=token, private=private, exist_ok=True).repo_id\n             files_timestamps = self._get_files_timestamps(save_directory)\n \n             self._upload_modified_files("
        },
        {
            "sha": "23d502b8ca8cc337cce93eaedcea06e5482e7c9d",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -33,7 +33,7 @@\n from typing import TYPE_CHECKING, Any, Literal, NamedTuple, Optional, Union, overload\n \n import numpy as np\n-from huggingface_hub import list_repo_files\n+from huggingface_hub import create_repo, list_repo_files\n from packaging import version\n \n from . import __version__\n@@ -2659,7 +2659,7 @@ def save_pretrained(\n         if push_to_hub:\n             commit_message = kwargs.pop(\"commit_message\", None)\n             repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n-            repo_id = self._create_repo(repo_id, **kwargs)\n+            repo_id = create_repo(repo_id, exist_ok=True, **kwargs).repo_id\n             files_timestamps = self._get_files_timestamps(save_directory)\n \n         special_tokens_map_file = os.path.join("
        },
        {
            "sha": "7faa8bc3b577e252066a7914887d2bf42350f01b",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -72,28 +72,21 @@\n     torch_float,\n     torch_int,\n     transpose,\n-    working_or_temp_dir,\n )\n from .hub import (\n     CHAT_TEMPLATE_DIR,\n     CHAT_TEMPLATE_FILE,\n     CLOUDFRONT_DISTRIB_PREFIX,\n     HF_MODULES_CACHE,\n-    HUGGINGFACE_CO_PREFIX,\n-    HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n     LEGACY_PROCESSOR_CHAT_TEMPLATE_FILE,\n-    PYTORCH_PRETRAINED_BERT_CACHE,\n-    PYTORCH_TRANSFORMERS_CACHE,\n     S3_BUCKET_PREFIX,\n-    TRANSFORMERS_CACHE,\n     TRANSFORMERS_DYNAMIC_MODULE_NAME,\n     EntryNotFoundError,\n     PushInProgress,\n     PushToHubMixin,\n     RepositoryNotFoundError,\n     RevisionNotFoundError,\n     cached_file,\n-    default_cache_path,\n     define_sagemaker_information,\n     extract_commit_hash,\n     has_file,"
        },
        {
            "sha": "7772e70ed14825fcbf9ba03969a60d0710d3beb2",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -18,11 +18,10 @@\n import inspect\n import json\n import os\n-import tempfile\n import warnings\n from collections import OrderedDict, UserDict, defaultdict\n from collections.abc import Callable, Iterable, MutableMapping\n-from contextlib import AbstractContextManager, ExitStack, contextmanager\n+from contextlib import AbstractContextManager, ExitStack\n from dataclasses import dataclass, fields, is_dataclass\n from enum import Enum\n from functools import partial, wraps\n@@ -501,15 +500,6 @@ def _flatten_dict(d, parent_key=\"\", delimiter=\".\"):\n     return dict(_flatten_dict(d, parent_key, delimiter))\n \n \n-@contextmanager\n-def working_or_temp_dir(working_dir, use_temp_dir: bool = False):\n-    if use_temp_dir:\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            yield tmp_dir\n-    else:\n-        yield working_dir\n-\n-\n def transpose(array, axes=None):\n     \"\"\"\n     Framework-agnostic version of transpose operation."
        },
        {
            "sha": "ba1e35a7d14c2bf3833175689aada7fdb06f5279",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 39,
            "deletions": 160,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -19,14 +19,13 @@\n import os\n import re\n import sys\n-import warnings\n+import tempfile\n from concurrent import futures\n from pathlib import Path\n from typing import TypedDict\n from uuid import uuid4\n \n import httpx\n-import huggingface_hub\n from huggingface_hub import (\n     _CACHED_NO_EXIST,\n     CommitOperationAdd,\n@@ -57,8 +56,12 @@\n )\n \n from . import __version__, logging\n-from .generic import working_or_temp_dir\n-from .import_utils import ENV_VARS_TRUE_VALUES, get_torch_version, is_torch_available, is_training_run_on_sagemaker\n+from .import_utils import (\n+    ENV_VARS_TRUE_VALUES,\n+    get_torch_version,\n+    is_torch_available,\n+    is_training_run_on_sagemaker,\n+)\n \n \n LEGACY_PROCESSOR_CHAT_TEMPLATE_FILE = \"chat_template.json\"\n@@ -80,59 +83,21 @@ class DownloadKwargs(TypedDict, total=False):\n     commit_hash: str | None\n \n \n-_is_offline_mode = huggingface_hub.constants.HF_HUB_OFFLINE\n-\n-\n def is_offline_mode():\n-    return _is_offline_mode\n-\n+    return constants.HF_HUB_OFFLINE\n \n-torch_cache_home = os.getenv(\"TORCH_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"torch\"))\n-default_cache_path = constants.default_cache_path\n \n-# Determine default cache directory. Lots of legacy environment variables to ensure backward compatibility.\n+# Determine default cache directory.\n # The best way to set the cache path is with the environment variable HF_HOME. For more details, check out this\n # documentation page: https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables.\n-#\n-# In code, use `HF_HUB_CACHE` as the default cache path. This variable is set by the library and is guaranteed\n-# to be set to the right value.\n-#\n-# TODO: clean this for v5?\n-PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\"PYTORCH_PRETRAINED_BERT_CACHE\", constants.HF_HUB_CACHE)\n-PYTORCH_TRANSFORMERS_CACHE = os.getenv(\"PYTORCH_TRANSFORMERS_CACHE\", PYTORCH_PRETRAINED_BERT_CACHE)\n-TRANSFORMERS_CACHE = os.getenv(\"TRANSFORMERS_CACHE\", PYTORCH_TRANSFORMERS_CACHE)\n \n HF_MODULES_CACHE = os.getenv(\"HF_MODULES_CACHE\", os.path.join(constants.HF_HOME, \"modules\"))\n TRANSFORMERS_DYNAMIC_MODULE_NAME = \"transformers_modules\"\n SESSION_ID = uuid4().hex\n \n-# Add deprecation warning for old environment variables.\n-for key in (\"PYTORCH_PRETRAINED_BERT_CACHE\", \"PYTORCH_TRANSFORMERS_CACHE\", \"TRANSFORMERS_CACHE\"):\n-    if os.getenv(key) is not None:\n-        warnings.warn(\n-            f\"Using `{key}` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\",\n-            FutureWarning,\n-        )\n-\n-\n S3_BUCKET_PREFIX = \"https://s3.amazonaws.com/models.huggingface.co/bert\"\n CLOUDFRONT_DISTRIB_PREFIX = \"https://cdn.huggingface.co\"\n \n-_staging_mode = os.environ.get(\"HUGGINGFACE_CO_STAGING\", \"NO\").upper() in ENV_VARS_TRUE_VALUES\n-_default_endpoint = \"https://hub-ci.huggingface.co\" if _staging_mode else \"https://huggingface.co\"\n-\n-HUGGINGFACE_CO_RESOLVE_ENDPOINT = _default_endpoint\n-if os.environ.get(\"HUGGINGFACE_CO_RESOLVE_ENDPOINT\", None) is not None:\n-    warnings.warn(\n-        \"Using the environment variable `HUGGINGFACE_CO_RESOLVE_ENDPOINT` is deprecated and will be removed in \"\n-        \"Transformers v5. Use `HF_ENDPOINT` instead.\",\n-        FutureWarning,\n-    )\n-    HUGGINGFACE_CO_RESOLVE_ENDPOINT = os.environ.get(\"HUGGINGFACE_CO_RESOLVE_ENDPOINT\", None)\n-HUGGINGFACE_CO_RESOLVE_ENDPOINT = os.environ.get(\"HF_ENDPOINT\", HUGGINGFACE_CO_RESOLVE_ENDPOINT)\n-HUGGINGFACE_CO_PREFIX = HUGGINGFACE_CO_RESOLVE_ENDPOINT + \"/{model_id}/resolve/{revision}/{filename}\"\n-HUGGINGFACE_CO_EXAMPLES_TELEMETRY = HUGGINGFACE_CO_RESOLVE_ENDPOINT + \"/api/telemetry/examples\"\n-\n \n def _get_cache_file_to_return(\n     path_or_repo_id: str,\n@@ -424,7 +389,7 @@ def cached_files(\n         return existing_files if existing_files else None\n \n     if cache_dir is None:\n-        cache_dir = TRANSFORMERS_CACHE\n+        cache_dir = constants.HF_HUB_CACHE\n     if isinstance(cache_dir, Path):\n         cache_dir = str(cache_dir)\n \n@@ -529,7 +494,7 @@ def cached_files(\n             # even when `local_files_only` is True, in which case raising for connections errors only would not make sense)\n             elif _raise_exceptions_for_missing_entries:\n                 raise OSError(\n-                    f\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load the files, and couldn't find them in the\"\n+                    f\"We couldn't connect to '{constants.ENDPOINT}' to load the files, and couldn't find them in the\"\n                     f\" cached files.\\nCheck your internet connection or see how to run the library in offline mode at\"\n                     \" 'https://huggingface.co/docs/transformers/installation#offline-mode'.\"\n                 ) from e\n@@ -661,41 +626,6 @@ class PushToHubMixin:\n     A Mixin containing the functionality to push a model or tokenizer to the hub.\n     \"\"\"\n \n-    def _create_repo(\n-        self,\n-        repo_id: str,\n-        private: bool | None = None,\n-        token: bool | str | None = None,\n-        repo_url: str | None = None,\n-        organization: str | None = None,\n-    ) -> str:\n-        \"\"\"\n-        Create the repo if needed, cleans up repo_id with deprecated kwargs `repo_url` and `organization`, retrieves\n-        the token.\n-        \"\"\"\n-        if repo_url is not None:\n-            warnings.warn(\n-                \"The `repo_url` argument is deprecated and will be removed in v5 of Transformers. Use `repo_id` \"\n-                \"instead.\"\n-            )\n-            if repo_id is not None:\n-                raise ValueError(\n-                    \"`repo_id` and `repo_url` are both specified. Please set only the argument `repo_id`.\"\n-                )\n-            repo_id = repo_url.replace(f\"{HUGGINGFACE_CO_RESOLVE_ENDPOINT}/\", \"\")\n-        if organization is not None:\n-            warnings.warn(\n-                \"The `organization` argument is deprecated and will be removed in v5 of Transformers. Set your \"\n-                \"organization directly in the `repo_id` passed instead (`repo_id={organization}/{model_id}`).\"\n-            )\n-            if not repo_id.startswith(organization):\n-                if \"/\" in repo_id:\n-                    repo_id = repo_id.split(\"/\")[-1]\n-                repo_id = f\"{organization}/{repo_id}\"\n-\n-        url = create_repo(repo_id=repo_id, token=token, private=private, exist_ok=True)\n-        return url.repo_id\n-\n     def _get_files_timestamps(self, working_dir: str | os.PathLike):\n         \"\"\"\n         Returns the list of files with their last modification timestamp.\n@@ -784,17 +714,19 @@ def _upload_modified_files(\n     def push_to_hub(\n         self,\n         repo_id: str,\n-        use_temp_dir: bool | None = None,\n+        *,\n+        # Commit details\n         commit_message: str | None = None,\n+        commit_description: str | None = None,\n+        # Repo / upload details\n         private: bool | None = None,\n         token: bool | str | None = None,\n-        max_shard_size: int | str | None = \"5GB\",\n+        revision: str | None = None,\n         create_pr: bool = False,\n+        # Serialization details\n+        max_shard_size: int | str | None = \"5GB\",\n         safe_serialization: bool = True,\n-        revision: str | None = None,\n-        commit_description: str | None = None,\n         tags: list[str] | None = None,\n-        **deprecated_kwargs,\n     ) -> str:\n         \"\"\"\n         Upload the {object_files} to the ğŸ¤— Model Hub.\n@@ -803,30 +735,26 @@ def push_to_hub(\n             repo_id (`str`):\n                 The name of the repository you want to push your {object} to. It should contain your organization name\n                 when pushing to a given organization.\n-            use_temp_dir (`bool`, *optional*):\n-                Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n-                Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n             commit_message (`str`, *optional*):\n                 Message to commit while pushing. Will default to `\"Upload {object}\"`.\n+            commit_description (`str`, *optional*):\n+                The description of the commit that will be created\n             private (`bool`, *optional*):\n                 Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n             token (`bool` or `str`, *optional*):\n-                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n-                when running `hf auth login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n-                is not specified.\n+                The token to use as HTTP bearer authorization for remote files. If `True` (default), will use the token generated\n+                when running `hf auth login` (stored in `~/.huggingface`).\n+            revision (`str`, *optional*):\n+                Branch to push the uploaded files to.\n+            create_pr (`bool`, *optional*, defaults to `False`):\n+                Whether or not to create a PR with the uploaded files or directly commit.\n             max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n                 Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n                 will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n                 by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n                 Google Colab instances without any CPU OOM issues.\n-            create_pr (`bool`, *optional*, defaults to `False`):\n-                Whether or not to create a PR with the uploaded files or directly commit.\n             safe_serialization (`bool`, *optional*, defaults to `True`):\n                 Whether or not to convert the model weights in safetensors format for safer serialization.\n-            revision (`str`, *optional*):\n-                Branch to push the uploaded files to.\n-            commit_description (`str`, *optional*):\n-                The description of the commit that will be created\n             tags (`list[str]`, *optional*):\n                 List of tags to push on the Hub.\n \n@@ -844,65 +772,24 @@ def push_to_hub(\n         {object}.push_to_hub(\"huggingface/my-finetuned-bert\")\n         ```\n         \"\"\"\n-        ignore_metadata_errors = deprecated_kwargs.pop(\"ignore_metadata_errors\", False)\n-        repo_path_or_name = deprecated_kwargs.pop(\"repo_path_or_name\", None)\n-        if repo_path_or_name is not None:\n-            # Should use `repo_id` instead of `repo_path_or_name`. When using `repo_path_or_name`, we try to infer\n-            # repo_id from the folder path, if it exists.\n-            warnings.warn(\n-                \"The `repo_path_or_name` argument is deprecated and will be removed in v5 of Transformers. Use \"\n-                \"`repo_id` instead.\",\n-                FutureWarning,\n-            )\n-            if repo_id is not None:\n-                raise ValueError(\n-                    \"`repo_id` and `repo_path_or_name` are both specified. Please set only the argument `repo_id`.\"\n-                )\n-            if os.path.isdir(repo_path_or_name):\n-                # repo_path: infer repo_id from the path\n-                repo_id = repo_path_or_name.split(os.path.sep)[-1]\n-                working_dir = repo_id\n-            else:\n-                # repo_name: use it as repo_id\n-                repo_id = repo_path_or_name\n-                working_dir = repo_id.split(\"/\")[-1]\n-        else:\n-            # Repo_id is passed correctly: infer working_dir from it\n-            working_dir = repo_id.split(\"/\")[-1]\n+        # Create repo if it doesn't exist yet\n+        repo_id = create_repo(repo_id, private=private, token=token, exist_ok=True).repo_id\n \n-        # Deprecation warning will be sent after for repo_url and organization\n-        repo_url = deprecated_kwargs.pop(\"repo_url\", None)\n-        organization = deprecated_kwargs.pop(\"organization\", None)\n-\n-        repo_id = self._create_repo(\n-            repo_id, private=private, token=token, repo_url=repo_url, organization=organization\n-        )\n-\n-        # Create a new empty model card and eventually tag it\n-        model_card = create_and_tag_model_card(\n-            repo_id, tags, token=token, ignore_metadata_errors=ignore_metadata_errors\n-        )\n-\n-        if use_temp_dir is None:\n-            use_temp_dir = not os.path.isdir(working_dir)\n-\n-        with working_or_temp_dir(working_dir=working_dir, use_temp_dir=use_temp_dir) as work_dir:\n-            files_timestamps = self._get_files_timestamps(work_dir)\n+        # Load model card or create a new one + eventually tag it\n+        model_card = create_and_tag_model_card(repo_id, tags, token=token)\n \n+        with tempfile.TemporaryDirectory() as tmp_dir:\n             # Save all files.\n-            self.save_pretrained(\n-                work_dir,\n-                max_shard_size=max_shard_size,\n-                safe_serialization=safe_serialization,\n-            )\n+            self.save_pretrained(tmp_dir, max_shard_size=max_shard_size, safe_serialization=safe_serialization)\n \n-            # Update model card if needed:\n-            model_card.save(os.path.join(work_dir, \"README.md\"))\n+            # Update model card\n+            model_card.save(os.path.join(tmp_dir, \"README.md\"))\n \n+            # Upload\n             return self._upload_modified_files(\n-                work_dir,\n+                tmp_dir,\n                 repo_id,\n-                files_timestamps,\n+                files_timestamps={},\n                 commit_message=commit_message,\n                 token=token,\n                 create_pr=create_pr,\n@@ -1003,12 +890,7 @@ def get_checkpoint_shard_files(\n     return cached_filenames, sharded_metadata\n \n \n-def create_and_tag_model_card(\n-    repo_id: str,\n-    tags: list[str] | None = None,\n-    token: str | None = None,\n-    ignore_metadata_errors: bool = False,\n-):\n+def create_and_tag_model_card(repo_id: str, tags: list[str] | None = None, token: str | None = None) -> ModelCard:\n     \"\"\"\n     Creates or loads an existing model card and tags it.\n \n@@ -1019,13 +901,10 @@ def create_and_tag_model_card(\n             The list of tags to add in the model card\n         token (`str`, *optional*):\n             Authentication token, obtained with `huggingface_hub.HfApi.login` method. Will default to the stored token.\n-        ignore_metadata_errors (`bool`, *optional*, defaults to `False`):\n-            If True, errors while parsing the metadata section will be ignored. Some information might be lost during\n-            the process. Use it at your own risk.\n     \"\"\"\n     try:\n         # Check if the model card is present on the remote repo\n-        model_card = ModelCard.load(repo_id, token=token, ignore_metadata_errors=ignore_metadata_errors)\n+        model_card = ModelCard.load(repo_id, token=token)\n     except EntryNotFoundError:\n         # Otherwise create a simple model card from template\n         model_description = \"This is the model card of a ğŸ¤— transformers model that has been pushed on the Hub. This model card has been automatically generated.\""
        },
        {
            "sha": "6c98439356e24a6bce93760396a580bc0a283da9",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -22,6 +22,7 @@\n from typing import Any, Optional, Union\n \n import numpy as np\n+from huggingface_hub import create_repo\n from huggingface_hub.dataclasses import validate_typed_dict\n \n from .dynamic_module_utils import custom_object_save\n@@ -568,7 +569,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n         if push_to_hub:\n             commit_message = kwargs.pop(\"commit_message\", None)\n             repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n-            repo_id = self._create_repo(repo_id, **kwargs)\n+            repo_id = create_repo(repo_id, exist_ok=True, **kwargs).repo_id\n             files_timestamps = self._get_files_timestamps(save_directory)\n \n         # If we have a custom config, we copy the file defining it in the folder and set the attributes so it can be"
        },
        {
            "sha": "3f75a6f1da6dc0b4a905ece8c57c601b1f0ec391",
            "filename": "tests/utils/test_hub_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/tests%2Futils%2Ftest_hub_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/tests%2Futils%2Ftest_hub_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_hub_utils.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -18,22 +18,15 @@\n import unittest.mock as mock\n from pathlib import Path\n \n-from huggingface_hub import hf_hub_download\n+from huggingface_hub import constants, hf_hub_download\n from huggingface_hub.errors import HfHubHTTPError, LocalEntryNotFoundError, OfflineModeIsEnabled\n \n-from transformers.utils import (\n-    CONFIG_NAME,\n-    TRANSFORMERS_CACHE,\n-    WEIGHTS_NAME,\n-    cached_file,\n-    has_file,\n-    list_repo_templates,\n-)\n+from transformers.utils import CONFIG_NAME, WEIGHTS_NAME, cached_file, has_file, list_repo_templates\n \n \n RANDOM_BERT = \"hf-internal-testing/tiny-random-bert\"\n TINY_BERT_PT_ONLY = \"hf-internal-testing/tiny-bert-pt-only\"\n-CACHE_DIR = os.path.join(TRANSFORMERS_CACHE, \"models--hf-internal-testing--tiny-random-bert\")\n+CACHE_DIR = os.path.join(constants.HF_HUB_CACHE, \"models--hf-internal-testing--tiny-random-bert\")\n FULL_COMMIT_HASH = \"9b8c223d42b2188cb49d29af482996f9d0f3e5a6\"\n \n GATED_REPO = \"hf-internal-testing/dummy-gated-model\""
        },
        {
            "sha": "eaf986db28a465b34990c8c172df97ea046c5c66",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 27,
            "deletions": 79,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -26,10 +26,11 @@\n import uuid\n import warnings\n from pathlib import Path\n+from unittest.mock import patch\n \n import httpx\n import pytest\n-from huggingface_hub import HfApi, split_torch_state_dict_into_shards\n+from huggingface_hub import HfApi, snapshot_download, split_torch_state_dict_into_shards\n from parameterized import parameterized\n from pytest import mark\n \n@@ -289,91 +290,38 @@ def forward(self, mask, inputs_embeds):\n             return attention_mask\n \n     class TestOffline(unittest.TestCase):\n-        def test_offline(self):\n-            # Ugly setup with monkeypatches, amending env vars here is too late as libs have already been imported\n-            from huggingface_hub import constants\n+        @patch(\"huggingface_hub.constants\")\n+        def test_offline(self, mock_hf_hub_constants):\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                mock_hf_hub_constants.HF_HUB_OFFLINE = True\n \n-            from transformers.utils import hub\n+                # First offline load should fail\n+                with pytest.raises(OSError):\n+                    AutoModelForImageClassification.from_pretrained(TINY_IMAGE_CLASSIF, cache_dir=tmpdir)\n \n-            offlfine_env = hub._is_offline_mode\n-            hub_cache_env = constants.HF_HUB_CACHE\n-            hub_cache_env1 = constants.HUGGINGFACE_HUB_CACHE\n-            default_cache = constants.default_cache_path\n-            transformers_cache = hub.TRANSFORMERS_CACHE\n+                # Download model from Hub\n+                mock_hf_hub_constants.HF_HUB_OFFLINE = False\n+                snapshot_download(TINY_IMAGE_CLASSIF, cache_dir=tmpdir)\n \n-            try:\n-                hub._is_offline_mode = True\n-                with tempfile.TemporaryDirectory() as tmpdir:\n-                    LOG.info(\"Temporary cache dir %s\", tmpdir)\n-                    constants.HF_HUB_CACHE = tmpdir\n-                    constants.HUGGINGFACE_HUB_CACHE = tmpdir\n-                    constants.default_cache_path = tmpdir\n-                    hub.TRANSFORMERS_CACHE = tmpdir\n-                    # First offline load should fail\n-                    try:\n-                        AutoModelForImageClassification.from_pretrained(TINY_IMAGE_CLASSIF, revision=\"main\")\n-                    except OSError:\n-                        LOG.info(\"Loading model %s in offline mode failed as expected\", TINY_IMAGE_CLASSIF)\n-                    else:\n-                        self.fail(f\"Loading model {TINY_IMAGE_CLASSIF} in offline mode should fail\")\n-\n-                    # Download model -> Huggingface Hub not concerned by our offline mode\n-                    LOG.info(\"Downloading %s for offline tests\", TINY_IMAGE_CLASSIF)\n-                    hub_api = HfApi()\n-                    local_dir = hub_api.snapshot_download(TINY_IMAGE_CLASSIF, cache_dir=tmpdir)\n-\n-                    LOG.info(\"Model %s downloaded in %s\", TINY_IMAGE_CLASSIF, local_dir)\n-\n-                    AutoModelForImageClassification.from_pretrained(TINY_IMAGE_CLASSIF, revision=\"main\")\n-            finally:\n-                # Tear down: reset env as it was before calling this test\n-                hub._is_offline_mode = offlfine_env\n-                constants.HF_HUB_CACHE = hub_cache_env\n-                constants.HUGGINGFACE_HUB_CACHE = hub_cache_env1\n-                constants.default_cache_path = default_cache\n-                hub.TRANSFORMERS_CACHE = transformers_cache\n+                # Load again in offline mode - should work now\n+                mock_hf_hub_constants.HF_HUB_OFFLINE = True\n+                AutoModelForImageClassification.from_pretrained(TINY_IMAGE_CLASSIF, cache_dir=tmpdir)\n \n         def test_local_files_only(self):\n-            # Ugly setup with monkeypatches, amending env vars here is too late as libs have already been imported\n-            from huggingface_hub import constants\n-\n-            from transformers.utils import hub\n-\n-            hub_cache_env = constants.HF_HUB_CACHE\n-            hub_cache_env1 = constants.HUGGINGFACE_HUB_CACHE\n-            default_cache = constants.default_cache_path\n-            transformers_cache = hub.TRANSFORMERS_CACHE\n-            try:\n-                with tempfile.TemporaryDirectory() as tmpdir:\n-                    LOG.info(\"Temporary cache dir %s\", tmpdir)\n-                    constants.HF_HUB_CACHE = tmpdir\n-                    constants.HUGGINGFACE_HUB_CACHE = tmpdir\n-                    constants.default_cache_path = tmpdir\n-                    hub.TRANSFORMERS_CACHE = tmpdir\n-                    try:\n-                        AutoModelForImageClassification.from_pretrained(\n-                            TINY_IMAGE_CLASSIF, revision=\"main\", local_files_only=True\n-                        )\n-                    except OSError:\n-                        LOG.info(\"Loading model %s in offline mode failed as expected\", TINY_IMAGE_CLASSIF)\n-                    else:\n-                        self.fail(f\"Loading model {TINY_IMAGE_CLASSIF} in offline mode should fail\")\n-\n-                    LOG.info(\"Downloading %s for offline tests\", TINY_IMAGE_CLASSIF)\n-                    hub_api = HfApi()\n-                    local_dir = hub_api.snapshot_download(TINY_IMAGE_CLASSIF, cache_dir=tmpdir)\n-\n-                    LOG.info(\"Model %s downloaded in %s\", TINY_IMAGE_CLASSIF, local_dir)\n-\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                # Empty cache => fail to load from cache\n+                with pytest.raises(OSError):\n                     AutoModelForImageClassification.from_pretrained(\n-                        TINY_IMAGE_CLASSIF, revision=\"main\", local_files_only=True\n+                        TINY_IMAGE_CLASSIF, cache_dir=tmpdir, local_files_only=True\n                     )\n-            finally:\n-                # Tear down: reset env as it was before calling this test\n-                constants.HF_HUB_CACHE = hub_cache_env\n-                constants.HUGGINGFACE_HUB_CACHE = hub_cache_env1\n-                constants.default_cache_path = default_cache\n-                hub.TRANSFORMERS_CACHE = transformers_cache\n+\n+                # Populate cache\n+                snapshot_download(TINY_IMAGE_CLASSIF, cache_dir=tmpdir)\n+\n+                # Load again from cache => success\n+                AutoModelForImageClassification.from_pretrained(\n+                    TINY_IMAGE_CLASSIF, cache_dir=tmpdir, local_files_only=True\n+                )\n \n \n # Need to be serializable, which means they cannot be in a test class method"
        },
        {
            "sha": "771dd44d0b5cd7bf4e11179d62d7ceda9acb5eed",
            "filename": "tests/utils/test_offline.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f624084ef479c9d449577314e9bde320f708bdeb/tests%2Futils%2Ftest_offline.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f624084ef479c9d449577314e9bde320f708bdeb/tests%2Futils%2Ftest_offline.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_offline.py?ref=f624084ef479c9d449577314e9bde320f708bdeb",
            "patch": "@@ -180,7 +180,7 @@ def test_offline_model_dynamic_model(self):\n \n     def test_is_offline_mode(self):\n         \"\"\"\n-        Test `_is_offline_mode` helper (should respect both HF_HUB_OFFLINE and legacy TRANSFORMERS_OFFLINE env vars)\n+        Test `is_offline_mode` helper (should respect both HF_HUB_OFFLINE and legacy TRANSFORMERS_OFFLINE env vars)\n         \"\"\"\n         load = \"from transformers.utils import is_offline_mode\"\n         run = \"print(is_offline_mode())\""
        }
    ],
    "stats": {
        "total": 695,
        "additions": 174,
        "deletions": 521
    }
}