{
    "author": "zucchini-nlp",
    "message": "[BLIP] remove cache from Qformer (#39335)\n\n* remove cache from Qformer\n\n* fix\n\n* this was never correct...",
    "sha": "c30af65521e806eac11552d1fcd83d8e1461c07f",
    "files": [
        {
            "sha": "f1412467019897bbba50ecd22b9b4b8960ba6dbd",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 34,
            "deletions": 57,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/c30af65521e806eac11552d1fcd83d8e1461c07f/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c30af65521e806eac11552d1fcd83d8e1461c07f/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=c30af65521e806eac11552d1fcd83d8e1461c07f",
            "patch": "@@ -37,6 +37,7 @@\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging, torch_int\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_blip_2 import Blip2Config, Blip2QFormerConfig, Blip2VisionConfig\n \n@@ -642,6 +643,7 @@ def transpose_for_scores(self, x):\n         x = x.view(*new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n@@ -661,11 +663,6 @@ def forward(\n             key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n             value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n             attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n         else:\n             key_layer = self.transpose_for_scores(self.key(hidden_states))\n             value_layer = self.transpose_for_scores(self.value(hidden_states))\n@@ -674,8 +671,6 @@ def forward(\n \n         query_layer = self.transpose_for_scores(mixed_query_layer)\n \n-        past_key_value = (key_layer, value_layer)\n-\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n@@ -722,9 +717,14 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(*new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        outputs = outputs + (past_key_value,)\n+        outputs = (\n+            (\n+                context_layer,\n+                attention_probs,\n+            )\n+            if output_attentions\n+            else (context_layer,)\n+        )\n         return outputs\n \n \n@@ -768,6 +768,7 @@ def prune_heads(self, heads):\n         self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -779,13 +780,12 @@ def forward(\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.attention(\n-            hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            output_attentions=output_attentions,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -845,6 +845,7 @@ def __init__(self, config, layer_idx):\n         self.intermediate_query = Blip2QFormerIntermediate(config)\n         self.output_query = Blip2QFormerOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n@@ -856,19 +857,14 @@ def forward(\n         output_attentions=False,\n         query_length=0,\n     ):\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n-            hidden_states,\n-            attention_mask,\n-            head_mask,\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n         )\n         attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:-1]\n-\n-        present_key_value = self_attention_outputs[-1]\n+        outputs = self_attention_outputs[1:]\n \n         if query_length > 0:\n             query_attention_output = attention_output[:, :query_length, :]\n@@ -877,16 +873,16 @@ def forward(\n                 if encoder_hidden_states is None:\n                     raise ValueError(\"encoder_hidden_states must be given for cross-attention layers\")\n                 cross_attention_outputs = self.crossattention(\n-                    query_attention_output,\n-                    attention_mask,\n-                    head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n+                    hidden_states=query_attention_output,\n+                    attention_mask=attention_mask,\n+                    head_mask=head_mask,\n+                    encoder_hidden_states=encoder_hidden_states,\n+                    encoder_attention_mask=encoder_attention_mask,\n                     output_attentions=output_attentions,\n                 )\n                 query_attention_output = cross_attention_outputs[0]\n                 # add cross attentions if we output attention weights\n-                outputs = outputs + cross_attention_outputs[1:-1]\n+                outputs = outputs + cross_attention_outputs[1:]\n \n             layer_output = apply_chunking_to_forward(\n                 self.feed_forward_chunk_query,\n@@ -912,8 +908,6 @@ def forward(\n             )\n         outputs = (layer_output,) + outputs\n \n-        outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -936,6 +930,8 @@ def __init__(self, config):\n         )\n         self.gradient_checkpointing = False\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n+    @deprecate_kwarg(\"use_cache\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n@@ -954,39 +950,27 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions else None\n \n-        next_decoder_cache = () if use_cache else None\n-\n         for i in range(self.config.num_hidden_layers):\n             layer_module = self.layer[i]\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n-\n-            if getattr(self.config, \"gradient_checkpointing\", False) and self.training and use_cache:\n-                logger.warning(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n                 query_length=query_length,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if layer_module.has_cross_attention:\n+                if query_length > 0 and layer_module.has_cross_attention:\n                     all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n \n         if output_hidden_states:\n@@ -997,7 +981,6 @@ def forward(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -1006,7 +989,6 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -1138,6 +1120,8 @@ def get_extended_attention_mask(\n         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n         return extended_attention_mask\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n+    @deprecate_kwarg(\"use_cache\", version=\"4.55.0\")\n     @auto_docstring\n     def forward(\n         self,\n@@ -1167,11 +1151,6 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # past_key_values_length\n-        past_key_values_length = (\n-            past_key_values[0][0].shape[2] - self.config.query_length if past_key_values is not None else 0\n-        )\n-\n         query_length = (\n             query_length if query_length is not None else query_embeds.shape[1] if query_embeds is not None else 0\n         )\n@@ -1186,7 +1165,7 @@ def forward(\n         device = embedding_output.device\n \n         if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n+            attention_mask = torch.ones(((batch_size, seq_length)), device=device)\n \n         # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n         # ourselves in which case we just need to make it broadcastable to all heads.\n@@ -1228,8 +1207,6 @@ def forward(\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n-            past_key_values=past_key_values,\n-            use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,"
        },
        {
            "sha": "de541f3716c81791f2061b45cdc20b7b891ca9d8",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 24,
            "deletions": 54,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/c30af65521e806eac11552d1fcd83d8e1461c07f/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c30af65521e806eac11552d1fcd83d8e1461c07f/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=c30af65521e806eac11552d1fcd83d8e1461c07f",
            "patch": "@@ -37,6 +37,7 @@\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_instructblip import InstructBlipConfig, InstructBlipQFormerConfig, InstructBlipVisionConfig\n \n@@ -559,6 +560,7 @@ def transpose_for_scores(self, x):\n         x = x.view(*new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n@@ -578,11 +580,6 @@ def forward(\n             key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n             value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n             attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n         else:\n             key_layer = self.transpose_for_scores(self.key(hidden_states))\n             value_layer = self.transpose_for_scores(self.value(hidden_states))\n@@ -591,8 +588,6 @@ def forward(\n \n         query_layer = self.transpose_for_scores(mixed_query_layer)\n \n-        past_key_value = (key_layer, value_layer)\n-\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n@@ -642,7 +637,6 @@ def forward(\n \n         outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n \n-        outputs = outputs + (past_key_value,)\n         return outputs\n \n \n@@ -687,6 +681,7 @@ def prune_heads(self, heads):\n         self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -698,13 +693,12 @@ def forward(\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.attention(\n-            hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            output_attentions=output_attentions,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -763,6 +757,7 @@ def __init__(self, config, layer_idx):\n         self.intermediate_query = InstructBlipQFormerIntermediate(config)\n         self.output_query = InstructBlipQFormerOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n@@ -774,19 +769,14 @@ def forward(\n         output_attentions=False,\n         query_length=0,\n     ):\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n         )\n         attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:-1]\n-\n-        present_key_value = self_attention_outputs[-1]\n+        outputs = self_attention_outputs[1:]\n \n         if query_length > 0:\n             query_attention_output = attention_output[:, :query_length, :]\n@@ -796,15 +786,15 @@ def forward(\n                     raise ValueError(\"encoder_hidden_states must be given for cross-attention layers\")\n                 cross_attention_outputs = self.crossattention(\n                     query_attention_output,\n-                    attention_mask,\n-                    head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n+                    attention_mask=attention_mask,\n+                    head_mask=head_mask,\n+                    encoder_hidden_states=encoder_hidden_states,\n+                    encoder_attention_mask=encoder_attention_mask,\n                     output_attentions=output_attentions,\n                 )\n                 query_attention_output = cross_attention_outputs[0]\n                 # add cross attentions if we output attention weights\n-                outputs = outputs + cross_attention_outputs[1:-1]\n+                outputs = outputs + cross_attention_outputs[1:]\n \n             layer_output = apply_chunking_to_forward(\n                 self.feed_forward_chunk_query,\n@@ -830,8 +820,6 @@ def forward(\n             )\n         outputs = (layer_output,) + outputs\n \n-        outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -855,6 +843,8 @@ def __init__(self, config):\n         )\n         self.gradient_checkpointing = False\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n+    @deprecate_kwarg(\"use_cache\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n@@ -873,39 +863,27 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions else None\n \n-        next_decoder_cache = () if use_cache else None\n-\n         for i in range(self.config.num_hidden_layers):\n             layer_module = self.layer[i]\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n-\n-            if getattr(self.config, \"gradient_checkpointing\", False) and self.training and use_cache:\n-                logger.warning(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n                 query_length=query_length,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if layer_module.has_cross_attention:\n+                if query_length > 0 and layer_module.has_cross_attention:\n                     all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n \n         if output_hidden_states:\n@@ -916,7 +894,6 @@ def forward(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -925,7 +902,6 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -1061,6 +1037,8 @@ def get_extended_attention_mask(\n         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n         return extended_attention_mask\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n+    @deprecate_kwarg(\"use_cache\", version=\"4.55.0\")\n     def forward(\n         self,\n         input_ids: torch.LongTensor,\n@@ -1104,26 +1082,20 @@ def forward(\n         if input_ids is None and query_embeds is None:\n             raise ValueError(\"You have to specify query_embeds when input_ids is None\")\n \n-        # past_key_values_length\n-        past_key_values_length = (\n-            past_key_values[0][0].shape[2] - self.config.query_length if past_key_values is not None else 0\n-        )\n-\n         query_length = query_embeds.shape[1] if query_embeds is not None else 0\n \n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n             position_ids=position_ids,\n             query_embeds=query_embeds,\n-            past_key_values_length=past_key_values_length,\n         )\n \n         input_shape = embedding_output.size()[:-1]\n         batch_size, seq_length = input_shape\n         device = embedding_output.device\n \n         if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n+            attention_mask = torch.ones(((batch_size, seq_length)), device=device)\n \n         # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n         # ourselves in which case we just need to make it broadcastable to all heads.\n@@ -1161,8 +1133,6 @@ def forward(\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n-            past_key_values=past_key_values,\n-            use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,"
        },
        {
            "sha": "c0051d502c8cc677744599d05682f0e9d1248bc0",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 24,
            "deletions": 54,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/c30af65521e806eac11552d1fcd83d8e1461c07f/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c30af65521e806eac11552d1fcd83d8e1461c07f/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=c30af65521e806eac11552d1fcd83d8e1461c07f",
            "patch": "@@ -41,6 +41,7 @@\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_instructblipvideo import (\n     InstructBlipVideoConfig,\n@@ -422,6 +423,7 @@ def transpose_for_scores(self, x):\n         x = x.view(*new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n@@ -441,11 +443,6 @@ def forward(\n             key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n             value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n             attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n         else:\n             key_layer = self.transpose_for_scores(self.key(hidden_states))\n             value_layer = self.transpose_for_scores(self.value(hidden_states))\n@@ -454,8 +451,6 @@ def forward(\n \n         query_layer = self.transpose_for_scores(mixed_query_layer)\n \n-        past_key_value = (key_layer, value_layer)\n-\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n@@ -505,7 +500,6 @@ def forward(\n \n         outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n \n-        outputs = outputs + (past_key_value,)\n         return outputs\n \n \n@@ -548,6 +542,7 @@ def prune_heads(self, heads):\n         self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -559,13 +554,12 @@ def forward(\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.attention(\n-            hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            output_attentions=output_attentions,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -622,6 +616,7 @@ def __init__(self, config, layer_idx):\n         self.intermediate_query = InstructBlipVideoQFormerIntermediate(config)\n         self.output_query = InstructBlipVideoQFormerOutput(config)\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n@@ -633,19 +628,14 @@ def forward(\n         output_attentions=False,\n         query_length=0,\n     ):\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n         )\n         attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:-1]\n-\n-        present_key_value = self_attention_outputs[-1]\n+        outputs = self_attention_outputs[1:]\n \n         if query_length > 0:\n             query_attention_output = attention_output[:, :query_length, :]\n@@ -655,15 +645,15 @@ def forward(\n                     raise ValueError(\"encoder_hidden_states must be given for cross-attention layers\")\n                 cross_attention_outputs = self.crossattention(\n                     query_attention_output,\n-                    attention_mask,\n-                    head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n+                    attention_mask=attention_mask,\n+                    head_mask=head_mask,\n+                    encoder_hidden_states=encoder_hidden_states,\n+                    encoder_attention_mask=encoder_attention_mask,\n                     output_attentions=output_attentions,\n                 )\n                 query_attention_output = cross_attention_outputs[0]\n                 # add cross attentions if we output attention weights\n-                outputs = outputs + cross_attention_outputs[1:-1]\n+                outputs = outputs + cross_attention_outputs[1:]\n \n             layer_output = apply_chunking_to_forward(\n                 self.feed_forward_chunk_query,\n@@ -689,8 +679,6 @@ def forward(\n             )\n         outputs = (layer_output,) + outputs\n \n-        outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -713,6 +701,8 @@ def __init__(self, config):\n         )\n         self.gradient_checkpointing = False\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n+    @deprecate_kwarg(\"use_cache\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states,\n@@ -731,39 +721,27 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions else None\n \n-        next_decoder_cache = () if use_cache else None\n-\n         for i in range(self.config.num_hidden_layers):\n             layer_module = self.layer[i]\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n-\n-            if getattr(self.config, \"gradient_checkpointing\", False) and self.training and use_cache:\n-                logger.warning(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n                 query_length=query_length,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if layer_module.has_cross_attention:\n+                if query_length > 0 and layer_module.has_cross_attention:\n                     all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n \n         if output_hidden_states:\n@@ -774,7 +752,6 @@ def forward(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -783,7 +760,6 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -1022,6 +998,8 @@ def get_extended_attention_mask(\n         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n         return extended_attention_mask\n \n+    @deprecate_kwarg(\"past_key_value\", version=\"4.55.0\")\n+    @deprecate_kwarg(\"use_cache\", version=\"4.55.0\")\n     def forward(\n         self,\n         input_ids: torch.LongTensor,\n@@ -1065,26 +1043,20 @@ def forward(\n         if input_ids is None and query_embeds is None:\n             raise ValueError(\"You have to specify query_embeds when input_ids is None\")\n \n-        # past_key_values_length\n-        past_key_values_length = (\n-            past_key_values[0][0].shape[2] - self.config.query_length if past_key_values is not None else 0\n-        )\n-\n         query_length = query_embeds.shape[1] if query_embeds is not None else 0\n \n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n             position_ids=position_ids,\n             query_embeds=query_embeds,\n-            past_key_values_length=past_key_values_length,\n         )\n \n         input_shape = embedding_output.size()[:-1]\n         batch_size, seq_length = input_shape\n         device = embedding_output.device\n \n         if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n+            attention_mask = torch.ones(((batch_size, seq_length)), device=device)\n \n         # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n         # ourselves in which case we just need to make it broadcastable to all heads.\n@@ -1122,8 +1094,6 @@ def forward(\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n-            past_key_values=past_key_values,\n-            use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,"
        }
    ],
    "stats": {
        "total": 247,
        "additions": 82,
        "deletions": 165
    }
}