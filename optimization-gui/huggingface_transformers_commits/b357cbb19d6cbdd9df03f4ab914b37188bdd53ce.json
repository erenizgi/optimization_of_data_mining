{
    "author": "qgallouedec",
    "message": "[Trackio] Allow single-gpu training and monitor power (#39595)\n\nAllow not distributed and monitor power",
    "sha": "b357cbb19d6cbdd9df03f4ab914b37188bdd53ce",
    "files": [
        {
            "sha": "41a32b63acbb93f11bebd9a165b68f13275e74a0",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b357cbb19d6cbdd9df03f4ab914b37188bdd53ce/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b357cbb19d6cbdd9df03f4ab914b37188bdd53ce/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=b357cbb19d6cbdd9df03f4ab914b37188bdd53ce",
            "patch": "@@ -1115,19 +1115,22 @@ def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n             \"total_flos\",\n         ]\n \n-        if is_torch_available() and torch.cuda.is_available() and dist.is_available() and dist.is_initialized():\n+        if is_torch_available() and torch.cuda.is_available():\n             device_idx = torch.cuda.current_device()\n             total_memory = torch.cuda.get_device_properties(device_idx).total_memory\n             memory_allocated = torch.cuda.memory_allocated(device_idx)\n-\n+            power = torch.cuda.power_draw(device_idx)\n             gpu_memory_logs = {\n                 f\"gpu/{device_idx}/allocated_memory\": memory_allocated / (1024**3),  # GB\n                 f\"gpu/{device_idx}/memory_usage\": memory_allocated / total_memory,  # ratio\n+                f\"gpu/{device_idx}/power\": power / 1000,  # Watts\n             }\n-\n-            gathered_logs = [None] * dist.get_world_size()\n-            dist.all_gather_object(gathered_logs, gpu_memory_logs)\n-            gpu_memory_logs = {k: v for d in gathered_logs for k, v in d.items()}\n+            if dist.is_available() and dist.is_initialized():\n+                gathered_logs = [None] * dist.get_world_size()\n+                dist.all_gather_object(gathered_logs, gpu_memory_logs)\n+                gpu_memory_logs = {k: v for d in gathered_logs for k, v in d.items()}\n+        else:\n+            gpu_memory_logs = {}\n \n         if not self._initialized:\n             self.setup(args, state, model)"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 9,
        "deletions": 6
    }
}