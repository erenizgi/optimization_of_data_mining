{
    "author": "cyyever",
    "message": "Fix various Pylint warnings (#40107)\n\nTidy code\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
    "files": [
        {
            "sha": "e63096d05244a1cc888a82c4f3ee4c6efc54b592",
            "filename": "examples/legacy/run_chinese_ref.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/examples%2Flegacy%2Frun_chinese_ref.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/examples%2Flegacy%2Frun_chinese_ref.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_chinese_ref.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -19,14 +19,14 @@ def _is_chinese_char(cp):\n     # like the all of the other languages.\n     if (\n         (cp >= 0x4E00 and cp <= 0x9FFF)\n-        or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-        or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-        or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-        or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-        or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+        or (cp >= 0x3400 and cp <= 0x4DBF)\n+        or (cp >= 0x20000 and cp <= 0x2A6DF)\n+        or (cp >= 0x2A700 and cp <= 0x2B73F)\n+        or (cp >= 0x2B740 and cp <= 0x2B81F)\n+        or (cp >= 0x2B820 and cp <= 0x2CEAF)\n         or (cp >= 0xF900 and cp <= 0xFAFF)\n-        or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-    ):  #\n+        or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+    ):\n         return True\n \n     return False"
        },
        {
            "sha": "c90a55b44bf85cfaccf06107d492b202e1a9db45",
            "filename": "src/transformers/commands/add_new_model_like.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -91,7 +91,7 @@ def visit_SimpleStatementLine(self, node: cst.SimpleStatementLine):\n \"\"\".lstrip()\n \n \n-class ModelInfos(object):\n+class ModelInfos:\n     \"\"\"\n     Retrieve the basic informations about an existing model classes.\n     \"\"\""
        },
        {
            "sha": "89bac4fec212544b6c891033a38ff18ba1c0fcfe",
            "filename": "src/transformers/commands/chat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fcommands%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fcommands%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fchat.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -22,9 +22,10 @@\n import string\n import time\n from argparse import ArgumentParser, Namespace\n+from collections.abc import AsyncIterator\n from dataclasses import dataclass, field\n from threading import Thread\n-from typing import AsyncIterator, Optional\n+from typing import Optional\n \n import yaml\n from huggingface_hub import AsyncInferenceClient, ChatCompletionStreamOutput"
        },
        {
            "sha": "a715568878473a170becded1662e42e337e9c595",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -24,10 +24,11 @@\n import threading\n import time\n from argparse import ArgumentParser, Namespace\n+from collections.abc import Generator, Iterable\n from dataclasses import dataclass, field\n from io import BytesIO\n from threading import Thread\n-from typing import Generator, Iterable, Optional, Union\n+from typing import Optional, Union\n \n from huggingface_hub import model_info\n from huggingface_hub.constants import HF_HUB_OFFLINE"
        },
        {
            "sha": "fa3fb3d176ac0b75064d62d3d1199ae3cdfbe90b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -1615,9 +1615,7 @@ def _find_mismatched_keys(\n                 # This skips size mismatches for 4-bit weights. Two 4-bit values share an 8-bit container, causing size differences.\n                 # Without matching with module type or parameter type it seems like a practical way to detect valid 4bit weights.\n                 if not (\n-                    is_quantized\n-                    and new_state_dict[key].shape[-1] == 1\n-                    and new_state_dict[key].numel() * 2 == model_state_dict[key].numel()\n+                    is_quantized and tensor.shape[-1] == 1 and tensor.numel() * 2 == model_state_dict[key].numel()\n                 ):\n                     mismatched_keys.append(key)\n                     mismatched_shapes.append((tensor.shape, model_state_dict[key].shape))"
        },
        {
            "sha": "7306877466d9b793682d01d90645f08420930b59",
            "filename": "src/transformers/models/convnext/modeling_tf_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -391,7 +391,7 @@ def call(\n \n         # Change the other hidden state outputs to NCHW as well\n         if output_hidden_states:\n-            hidden_states = tuple([tf.transpose(h, perm=(0, 3, 1, 2)) for h in encoder_outputs[1]])\n+            hidden_states = tuple(tf.transpose(h, perm=(0, 3, 1, 2)) for h in encoder_outputs[1])\n \n         if not return_dict:\n             hidden_states = hidden_states if output_hidden_states else ()"
        },
        {
            "sha": "d370c3008d4701cdd4c3c78572a29218cc55693b",
            "filename": "src/transformers/models/convnextv2/modeling_tf_convnextv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -439,7 +439,7 @@ def call(\n \n         # Change the other hidden state outputs to NCHW as well\n         if output_hidden_states:\n-            hidden_states = tuple([tf.transpose(h, perm=(0, 3, 1, 2)) for h in encoder_outputs[1]])\n+            hidden_states = tuple(tf.transpose(h, perm=(0, 3, 1, 2)) for h in encoder_outputs[1])\n \n         if not return_dict:\n             hidden_states = hidden_states if output_hidden_states else ()"
        },
        {
            "sha": "9239e1918eeca7588e0a978af4ef524380cd3f5f",
            "filename": "src/transformers/models/cvt/modeling_tf_cvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -784,7 +784,7 @@ def call(\n         # Change back to (batch_size, num_channels, height, width) format to have uniformity in the modules\n         hidden_state = tf.transpose(hidden_state, perm=(0, 3, 1, 2))\n         if output_hidden_states:\n-            all_hidden_states = tuple([tf.transpose(hs, perm=(0, 3, 1, 2)) for hs in all_hidden_states])\n+            all_hidden_states = tuple(tf.transpose(hs, perm=(0, 3, 1, 2)) for hs in all_hidden_states)\n \n         if not return_dict:\n             return tuple(v for v in [hidden_state, cls_token, all_hidden_states] if v is not None)"
        },
        {
            "sha": "24b162fa9f9247adf17ce2dfb2d63a0f71a82489",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -125,7 +125,7 @@ def __init__(\n         if image_mean is None:\n             self.background_color = (127, 127, 127)\n         else:\n-            self.background_color = tuple([int(x * 255) for x in image_mean])\n+            self.background_color = tuple(int(x * 255) for x in image_mean)\n \n     def resize(\n         self,"
        },
        {
            "sha": "9837acad03124965a0b834c1a7e220280c681902",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -69,7 +69,7 @@ def __init__(self, **kwargs: Unpack[DeepseekVLFastImageProcessorKwargs]):\n         if kwargs.get(\"image_mean\") is None:\n             background_color = (127, 127, 127)\n         else:\n-            background_color = tuple([int(x * 255) for x in kwargs.get(\"image_mean\")])\n+            background_color = tuple(int(x * 255) for x in kwargs.get(\"image_mean\"))\n         self.background_color = tuple(background_color)\n \n     def resize("
        },
        {
            "sha": "769d47f24199364bc77c43e701667a4771ac2362",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -151,12 +151,12 @@ def __init__(\n         if image_mean is None:\n             self.background_color = (127, 127, 127)\n         else:\n-            self.background_color = tuple([int(x * 255) for x in image_mean])\n+            self.background_color = tuple(int(x * 255) for x in image_mean)\n \n         if high_res_image_mean is None:\n             self.high_res_background_color = (127, 127, 127)\n         else:\n-            self.high_res_background_color = tuple([int(x * 255) for x in high_res_image_mean])\n+            self.high_res_background_color = tuple(int(x * 255) for x in high_res_image_mean)\n \n     def resize(\n         self,"
        },
        {
            "sha": "e619283e1be14d1b70e99f438397e91a65faf40d",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -100,7 +100,7 @@ def __init__(self, **kwargs: Unpack[DeepseekVLHybridFastImageProcessorKwargs]):\n         if kwargs.get(\"high_res_image_mean\") is None:\n             high_res_background_color = (127, 127, 127)\n         else:\n-            high_res_background_color = tuple([int(x * 255) for x in kwargs.get(\"high_res_image_mean\")])\n+            high_res_background_color = tuple(int(x * 255) for x in kwargs.get(\"high_res_image_mean\"))\n         super().__init__(**kwargs)\n         self.background_color = tuple(background_color)\n         self.high_res_background_color = tuple(high_res_background_color)"
        },
        {
            "sha": "80292d6b7d6e6fb3cb49c05c3a5ee0dbbcb13f6c",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -535,7 +535,7 @@ def __init__(\n         if high_res_image_mean is None:\n             self.high_res_background_color = (127, 127, 127)\n         else:\n-            self.high_res_background_color = tuple([int(x * 255) for x in high_res_image_mean])\n+            self.high_res_background_color = tuple(int(x * 255) for x in high_res_image_mean)\n \n     @filter_out_non_signature_kwargs()\n     def preprocess(\n@@ -756,7 +756,7 @@ def __init__(self, **kwargs: Unpack[DeepseekVLHybridFastImageProcessorKwargs]):\n         if kwargs.get(\"high_res_image_mean\") is None:\n             high_res_background_color = (127, 127, 127)\n         else:\n-            high_res_background_color = tuple([int(x * 255) for x in kwargs.get(\"high_res_image_mean\")])\n+            high_res_background_color = tuple(int(x * 255) for x in kwargs.get(\"high_res_image_mean\"))\n         DeepseekVLImageProcessorFast().__init__(**kwargs)\n         self.background_color = tuple(background_color)\n         self.high_res_background_color = tuple(high_res_background_color)"
        },
        {
            "sha": "643097e79c3eab47b62f7e7c4781b2b276418176",
            "filename": "src/transformers/models/deprecated/efficientformer/modeling_tf_efficientformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_tf_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_tf_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_tf_efficientformer.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -867,7 +867,7 @@ def call(\n         # The hidden states are in (batch_size, height, width, num_channels)\n         # shape after all stages except the MB3D blocks.\n         if output_hidden_states:\n-            hidden_states = tuple([tf.transpose(h, perm=(0, 3, 1, 2)) for h in encoder_outputs[1][:-1]]) + (\n+            hidden_states = tuple(tf.transpose(h, perm=(0, 3, 1, 2)) for h in encoder_outputs[1][:-1]) + (\n                 encoder_outputs[1][-1],\n             )\n "
        },
        {
            "sha": "aeae3f2a2f7c8c3d1df0603a9f54c7f2c8acc17f",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -967,7 +967,7 @@ def forward(\n             pasts_or_spout_value = torch.split(pasts_or_spout_value, [1] * self.config.num_layers, dim=1)\n             # make same shape as past_key_values\n             pasts_or_spout_value = tuple(\n-                tuple([b.squeeze(1) for b in torch.split(a.squeeze(1), [1, 1], dim=1)]) for a in pasts_or_spout_value\n+                tuple(b.squeeze(1) for b in torch.split(a.squeeze(1), [1, 1], dim=1)) for a in pasts_or_spout_value\n             )\n         else:\n             pasts_or_spout_value = [None] * self.config.num_layers"
        },
        {
            "sha": "dd82cbca17824face2ea67dd60e106ac3cb54149",
            "filename": "src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -597,7 +597,7 @@ def call(\n             if not isinstance(encoder_outputs, tuple):\n                 encoder_outputs = encoder_outputs.to_tuple()\n             output = (loss, logits, past_key_values) + decoder_outputs[start_index:] + encoder_outputs\n-            output = tuple([x for x in output if x is not None])\n+            output = tuple(x for x in output if x is not None)\n             return output\n \n         return TFSeq2SeqLMOutput("
        },
        {
            "sha": "14703ba7d605dd9dbdbb55789d0ea960588048da",
            "filename": "src/transformers/models/esm/openfold_utils/chunk_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fchunk_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fchunk_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fchunk_utils.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -229,7 +229,7 @@ def chunk_layer(\n         raise ValueError(\"Must provide at least one input\")\n \n     initial_dims = [shape[:no_batch_dims] for shape in _fetch_dims(inputs)]\n-    orig_batch_dims = tuple([max(s) for s in zip(*initial_dims)])\n+    orig_batch_dims = tuple(max(s) for s in zip(*initial_dims))\n \n     def _prep_inputs(t: torch.Tensor) -> torch.Tensor:\n         if not low_mem:"
        },
        {
            "sha": "0535f38a33c5dccf592a366c010c1fcae897e85c",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -128,7 +128,7 @@ def __init__(\n         if image_mean is None:\n             self.background_color = (127, 127, 127)\n         else:\n-            self.background_color = tuple([int(x * 255) for x in image_mean])\n+            self.background_color = tuple(int(x * 255) for x in image_mean)\n \n     def resize(\n         self,"
        },
        {
            "sha": "12f0d0f394fe4ce4251c7cfdb215a50a187e8867",
            "filename": "src/transformers/models/janus/image_processing_janus_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -74,7 +74,7 @@ def __init__(self, **kwargs: Unpack[JanusFastImageProcessorKwargs]):\n         if kwargs.get(\"image_mean\") is None:\n             background_color = (127, 127, 127)\n         else:\n-            background_color = tuple([int(x * 255) for x in kwargs.get(\"image_mean\")])\n+            background_color = tuple(int(x * 255) for x in kwargs.get(\"image_mean\"))\n         super().__init__(**kwargs)\n         self.background_color = tuple(background_color)\n "
        },
        {
            "sha": "b5341ac33b4a79be8b60f5a72adaad5fa8ad5154",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -1356,7 +1356,7 @@ def __init__(\n         if image_mean is None:\n             self.background_color = (127, 127, 127)\n         else:\n-            self.background_color = tuple([int(x * 255) for x in image_mean])\n+            self.background_color = tuple(int(x * 255) for x in image_mean)\n \n     def pad_to_square(\n         self,"
        },
        {
            "sha": "1d3beefd7d59d98d6569f15ff90fd3d1a6f49658",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -1592,10 +1592,10 @@ def forward(\n             # unpad `hidden_states` because the calling function is expecting a length == input_ids.size(1)\n             hidden_states = hidden_states[:, :-padding_len]\n             if output_hidden_states:\n-                encoder_states = tuple([state[:, :-padding_len] for state in encoder_states])\n+                encoder_states = tuple(state[:, :-padding_len] for state in encoder_states)\n \n             if output_attentions:\n-                all_attentions = tuple([state[:, :, :-padding_len, :] for state in all_attentions])\n+                all_attentions = tuple(state[:, :, :-padding_len, :] for state in all_attentions)\n \n         if not return_dict:\n             return tuple("
        },
        {
            "sha": "f499ffac30c9f8324a3d16c1bb4a0ad9638337bc",
            "filename": "src/transformers/models/led/modeling_tf_led.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_tf_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_tf_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_tf_led.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -1907,9 +1907,7 @@ def call(\n         # undo padding\n         if output_attentions:\n             all_attentions = (\n-                tuple([state[:, :, :-padding_len, :] for state in all_attentions])\n-                if padding_len > 0\n-                else all_attentions\n+                tuple(state[:, :, :-padding_len, :] for state in all_attentions) if padding_len > 0 else all_attentions\n             )\n \n         if output_hidden_states:"
        },
        {
            "sha": "f181217cd1017a932c7d88f1e4a7817909a42626",
            "filename": "src/transformers/models/longformer/modeling_longformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -1283,10 +1283,10 @@ def forward(\n         # unpad `hidden_states` because the calling function is expecting a length == input_ids.size(1)\n         hidden_states = hidden_states[:, : hidden_states.shape[1] - padding_len]\n         if output_hidden_states:\n-            all_hidden_states = tuple([state[:, : state.shape[1] - padding_len] for state in all_hidden_states])\n+            all_hidden_states = tuple(state[:, : state.shape[1] - padding_len] for state in all_hidden_states)\n \n         if output_attentions:\n-            all_attentions = tuple([state[:, :, : state.shape[2] - padding_len, :] for state in all_attentions])\n+            all_attentions = tuple(state[:, :, : state.shape[2] - padding_len, :] for state in all_attentions)\n \n         if not return_dict:\n             return tuple("
        },
        {
            "sha": "891f5d76c95ce1f572ea33baf6db51cf21278a80",
            "filename": "src/transformers/models/longformer/modeling_tf_longformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_tf_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_tf_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_tf_longformer.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -1706,9 +1706,7 @@ def call(\n         hidden_states = hidden_states[:, :-padding_len] if padding_len > 0 else hidden_states\n         if output_attentions:\n             all_attentions = (\n-                tuple([state[:, :, :-padding_len, :] for state in all_attentions])\n-                if padding_len > 0\n-                else all_attentions\n+                tuple(state[:, :, :-padding_len, :] for state in all_attentions) if padding_len > 0 else all_attentions\n             )\n \n         if not return_dict:"
        },
        {
            "sha": "dcad0f302a8ebfdc0679f140eb6dd7e139f215c2",
            "filename": "src/transformers/models/mobilevit/modeling_tf_mobilevit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_tf_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_tf_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_tf_mobilevit.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -869,7 +869,7 @@ def call(\n             if not self.expand_output:\n                 remaining_encoder_outputs = encoder_outputs[1:]\n                 remaining_encoder_outputs = tuple(\n-                    [tf.transpose(h, perm=(0, 3, 1, 2)) for h in remaining_encoder_outputs[0]]\n+                    tf.transpose(h, perm=(0, 3, 1, 2)) for h in remaining_encoder_outputs[0]\n                 )\n                 remaining_encoder_outputs = (remaining_encoder_outputs,)\n                 return output + remaining_encoder_outputs\n@@ -878,7 +878,7 @@ def call(\n \n         # Change the other hidden state outputs to NCHW as well\n         if output_hidden_states:\n-            hidden_states = tuple([tf.transpose(h, perm=(0, 3, 1, 2)) for h in encoder_outputs[1]])\n+            hidden_states = tuple(tf.transpose(h, perm=(0, 3, 1, 2)) for h in encoder_outputs[1])\n \n         return TFBaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,"
        },
        {
            "sha": "b830a516804e7aa0d3f5fcfd2800f5e95cce7354",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -2044,7 +2044,7 @@ def forward(\n                     raise Exception(\"target_values cannot be negative for negative_binomial distribution.\")\n                 distribution = self.distribution_output.distribution(y_hat)\n                 # y_hat should be a 2-tuple, each with dimension [bs, num_targets]\n-                y_hat = tuple([item.view(-1, self.config.num_targets) for item in y_hat])\n+                y_hat = tuple(item.view(-1, self.config.num_targets) for item in y_hat)\n                 loss_val = loss(distribution, target_values)\n                 # take average of the loss\n                 loss_val = weighted_average(loss_val)"
        },
        {
            "sha": "1912e318f8fdb9f5e4e1daf72a2fbb6c907911b4",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -1884,7 +1884,7 @@ def forward(\n             if self.distribution_output:\n                 distribution = self.distribution_output.distribution(y_hat)\n                 # y_hat should be a 2-tuple, each with dimension [bs, num_targets]\n-                y_hat = tuple([item.view(-1, self.config.num_targets) for item in y_hat])\n+                y_hat = tuple(item.view(-1, self.config.num_targets) for item in y_hat)\n                 loss = nll(distribution, target_values)\n                 # take average of the loss\n                 loss = weighted_average(loss)"
        },
        {
            "sha": "1fac50013c3910c44ccfdcd63ca7b48ed940121b",
            "filename": "src/transformers/models/perception_lm/processing_perception_lm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -15,7 +15,8 @@\n Processor class for PerceptionLM.\n \"\"\"\n \n-from typing import Iterable, Union\n+from collections.abc import Iterable\n+from typing import Union\n \n import numpy as np\n "
        },
        {
            "sha": "760edd48453a141bc41825b5fdc823be8c868508",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -17,10 +17,11 @@\n \n import sys\n from collections import namedtuple\n+from collections.abc import Iterable\n from dataclasses import dataclass\n from functools import reduce\n from operator import mul\n-from typing import Any, Iterable, Optional, Union\n+from typing import Any, Optional, Union\n \n import numpy as np\n import torch"
        },
        {
            "sha": "13714b4e69aa6d788779a08e42dfe4ea8418367c",
            "filename": "src/transformers/models/regnet/modeling_tf_regnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_tf_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_tf_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_tf_regnet.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -415,7 +415,7 @@ def call(\n \n         # Change the other hidden state outputs to NCHW as well\n         if output_hidden_states:\n-            hidden_states = tuple([tf.transpose(h, perm=(0, 3, 1, 2)) for h in encoder_outputs[1]])\n+            hidden_states = tuple(tf.transpose(h, perm=(0, 3, 1, 2)) for h in encoder_outputs[1])\n \n         if not return_dict:\n             return (last_hidden_state, pooled_output) + encoder_outputs[1:]"
        },
        {
            "sha": "2f8e68b957487a172101d8b32241a572c71ddf02",
            "filename": "src/transformers/models/segformer/modeling_tf_segformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_tf_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_tf_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_tf_segformer.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -608,7 +608,7 @@ def call(\n \n         # Change the other hidden state outputs to NCHW as well\n         if output_hidden_states:\n-            hidden_states = tuple([tf.transpose(h, perm=(0, 3, 1, 2)) for h in encoder_outputs[1]])\n+            hidden_states = tuple(tf.transpose(h, perm=(0, 3, 1, 2)) for h in encoder_outputs[1])\n \n         if not return_dict:\n             if tf.greater(len(encoder_outputs[1:]), 0):"
        },
        {
            "sha": "61c52d06e6053dfef81533557a0fde3434519aee",
            "filename": "src/transformers/models/superglue/modeling_superglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -44,7 +44,7 @@ def concat_pairs(tensor_tuple0: tuple[torch.Tensor], tensor_tuple1: tuple[torch.\n     Returns:\n         (`tuple[torch.Tensor]`): Tuple of concatenated tensors.\n     \"\"\"\n-    return tuple([torch.cat([tensor0, tensor1]) for tensor0, tensor1 in zip(tensor_tuple0, tensor_tuple1)])\n+    return tuple(torch.cat([tensor0, tensor1]) for tensor0, tensor1 in zip(tensor_tuple0, tensor_tuple1))\n \n \n def normalize_keypoints(keypoints: torch.Tensor, height: int, width: int) -> torch.Tensor:"
        },
        {
            "sha": "1b2535424c268e2e028c759891c04d0c88d055db",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -612,7 +612,7 @@ def call(\n             if not isinstance(encoder_outputs, tuple):\n                 encoder_outputs = encoder_outputs.to_tuple()\n             output = (loss, logits, past_key_values) + decoder_outputs[start_index:] + encoder_outputs\n-            output = tuple([x for x in output if x is not None])\n+            output = tuple(x for x in output if x is not None)\n             return output\n \n         return TFSeq2SeqLMOutput("
        },
        {
            "sha": "19d294c37ae5d81318ddb72b1320b79db0e10051",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -726,13 +726,13 @@ def torch_forward(self, input_states, cache_params: Optional[Zamba2HybridDynamic\n         dtype = input_states.dtype\n         # Gated MLP's linear projection\n         if cache_params is not None and cache_params.has_previous_state:\n-            projected_states =  self.in_proj(input_states.squeeze(1))\n+            projected_states = self.in_proj(input_states.squeeze(1))\n         else:\n             if attention_mask is not None and not torch.all(attention_mask==1):\n-                    # tune out hidden states for pad tokens, see https://github.com/state-spaces/mamba/issues/66\n-                    input_states = (input_states * attention_mask[:, :, None]).to(dtype)\n-            projected_states =  self.in_proj(input_states)\n-        d_mlp = (projected_states.shape[-1] - 2 * self.intermediate_size -  2 * self.n_groups * self.ssm_state_size- self.num_heads) // 2\n+                # tune out hidden states for pad tokens, see https://github.com/state-spaces/mamba/issues/66\n+                input_states = (input_states * attention_mask[:, :, None]).to(dtype)\n+            projected_states = self.in_proj(input_states)\n+        d_mlp = (projected_states.shape[-1] - 2 * self.intermediate_size - 2 * self.n_groups * self.ssm_state_size- self.num_heads) // 2\n         _, _, gate, hidden_states, dt = projected_states.split(\n                 [d_mlp, d_mlp, self.intermediate_size,  self.conv_dim, self.num_heads], dim=-1\n         )"
        },
        {
            "sha": "acf552a2abb10e4672d2ff5958edcc3998168a8e",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -510,13 +510,13 @@ def torch_forward(self, input_states, cache_params: Optional[Zamba2HybridDynamic\n         dtype = input_states.dtype\n         # Gated MLP's linear projection\n         if cache_params is not None and cache_params.has_previous_state:\n-            projected_states =  self.in_proj(input_states.squeeze(1))\n+            projected_states = self.in_proj(input_states.squeeze(1))\n         else:\n             if attention_mask is not None and not torch.all(attention_mask==1):\n-                    # tune out hidden states for pad tokens, see https://github.com/state-spaces/mamba/issues/66\n-                    input_states = (input_states * attention_mask[:, :, None]).to(dtype)\n-            projected_states =  self.in_proj(input_states)\n-        d_mlp = (projected_states.shape[-1] - 2 * self.intermediate_size -  2 * self.n_groups * self.ssm_state_size- self.num_heads) // 2\n+                # tune out hidden states for pad tokens, see https://github.com/state-spaces/mamba/issues/66\n+                input_states = (input_states * attention_mask[:, :, None]).to(dtype)\n+            projected_states = self.in_proj(input_states)\n+        d_mlp = (projected_states.shape[-1] - 2 * self.intermediate_size - 2 * self.n_groups * self.ssm_state_size- self.num_heads) // 2\n         _, _, gate, hidden_states, dt = projected_states.split(\n                 [d_mlp, d_mlp, self.intermediate_size,  self.conv_dim, self.num_heads], dim=-1\n         )"
        },
        {
            "sha": "a43f57705cdda2f442f62066908d93142840eaf0",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -1269,7 +1269,7 @@ def _ensure_tensor_on_device(self, inputs, device):\n         elif isinstance(inputs, list):\n             return [self._ensure_tensor_on_device(item, device) for item in inputs]\n         elif isinstance(inputs, tuple):\n-            return tuple([self._ensure_tensor_on_device(item, device) for item in inputs])\n+            return tuple(self._ensure_tensor_on_device(item, device) for item in inputs)\n         elif isinstance(inputs, torch.Tensor):\n             return inputs.to(device)\n         else:"
        },
        {
            "sha": "aede13e8eff53f1fb6e3055d240c064a400dde08",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -2632,7 +2632,7 @@ def nested_simplify(obj, decimals=3):\n     if isinstance(obj, list):\n         return [nested_simplify(item, decimals) for item in obj]\n     if isinstance(obj, tuple):\n-        return tuple([nested_simplify(item, decimals) for item in obj])\n+        return tuple(nested_simplify(item, decimals) for item in obj)\n     elif isinstance(obj, np.ndarray):\n         return nested_simplify(obj.tolist())\n     elif isinstance(obj, Mapping):"
        },
        {
            "sha": "396df693211d0bb449aefd3016565e94971b6c77",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -22,11 +22,11 @@\n import warnings\n from collections import OrderedDict, UserDict, defaultdict\n from collections.abc import Iterable, MutableMapping\n-from contextlib import ExitStack, contextmanager\n+from contextlib import AbstractContextManager, ExitStack, contextmanager\n from dataclasses import dataclass, fields, is_dataclass\n from enum import Enum\n from functools import partial, wraps\n-from typing import Any, Callable, ContextManager, Optional, TypedDict\n+from typing import Any, Callable, Optional, TypedDict\n \n import numpy as np\n from packaging import version\n@@ -551,7 +551,7 @@ class ContextManagers:\n     in the `fastcore` library.\n     \"\"\"\n \n-    def __init__(self, context_managers: list[ContextManager]):\n+    def __init__(self, context_managers: list[AbstractContextManager]):\n         self.context_managers = context_managers\n         self.stack = ExitStack()\n "
        },
        {
            "sha": "45b12309c367db3a10939994a01c521f08f8f553",
            "filename": "tests/models/bloom/test_modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -739,7 +739,7 @@ def test_embeddings(self):\n         tensor_ids = torch.LongTensor([EXAMPLE_IDS])\n         with torch.no_grad():\n             embeddings = model.transformer.word_embeddings(tensor_ids)\n-            embeddings_ln = model.transformer.word_embeddings_layernorm(embeddings)  #\n+            embeddings_ln = model.transformer.word_embeddings_layernorm(embeddings)\n         # first check the embeddings before LN\n         output_dict = {\"min\": {}, \"max\": {}, \"mean\": {}, \"sum\": {\"value\": embeddings.sum().item()}}\n         for i, idx in enumerate(EXAMPLE_IDS):"
        },
        {
            "sha": "e1adfb996b616f7c945696fac241f8f16e895f35",
            "filename": "tests/models/evolla/test_processing_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/tests%2Fmodels%2Fevolla%2Ftest_processing_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/tests%2Fmodels%2Fevolla%2Ftest_processing_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fevolla%2Ftest_processing_evolla.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -161,7 +161,7 @@ def test_processor(self):\n         for key, value in expected_output.items():\n             self.assertTrue(\n                 torch.equal(inputs[key], value),\n-                f\"inputs[key] is {inputs[key]} and expected_output[key] is {expected_output[key]}\",\n+                f\"inputs[key] is {inputs[key]} and expected_output[key] is {value}\",\n             )\n \n     def get_tokenizer(self, **kwargs):"
        },
        {
            "sha": "29dba741c3dacb1aca0d810adee8c1cc447afbe1",
            "filename": "tests/models/gemma3n/test_feature_extraction_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/tests%2Fmodels%2Fgemma3n%2Ftest_feature_extraction_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/tests%2Fmodels%2Fgemma3n%2Ftest_feature_extraction_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_feature_extraction_gemma3n.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -18,7 +18,8 @@\n import random\n import tempfile\n import unittest\n-from typing import Optional, Sequence\n+from collections.abc import Sequence\n+from typing import Optional\n \n import numpy as np\n from parameterized import parameterized"
        },
        {
            "sha": "35bf08fce3a11197d941e29f66d191baec0c27a2",
            "filename": "tests/models/imagegpt/test_image_processing_imagegpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/tests%2Fmodels%2Fimagegpt%2Ftest_image_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4/tests%2Fmodels%2Fimagegpt%2Ftest_image_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fimagegpt%2Ftest_image_processing_imagegpt.py?ref=28a03fb78a46a27d875b8b2b06dcfbd12ee20ae4",
            "patch": "@@ -139,7 +139,7 @@ def test_image_processor_to_json_file(self):\n             if key == \"clusters\":\n                 self.assertTrue(np.array_equal(value, image_processor_second[key]))\n             else:\n-                self.assertEqual(image_processor_first[key], value)\n+                self.assertEqual(value, value)\n \n     def test_image_processor_from_and_save_pretrained(self):\n         for image_processing_class in self.image_processor_list:\n@@ -154,7 +154,7 @@ def test_image_processor_from_and_save_pretrained(self):\n                 if key == \"clusters\":\n                     self.assertTrue(np.array_equal(value, image_processor_second[key]))\n                 else:\n-                    self.assertEqual(image_processor_first[key], value)\n+                    self.assertEqual(value, value)\n \n     def test_image_processor_save_load_with_autoimageprocessor(self):\n         for image_processing_class in self.image_processor_list:\n@@ -173,7 +173,7 @@ def test_image_processor_save_load_with_autoimageprocessor(self):\n                 if key == \"clusters\":\n                     self.assertTrue(np.array_equal(value, image_processor_second[key]))\n                 else:\n-                    self.assertEqual(image_processor_first[key], value)\n+                    self.assertEqual(value, value)\n \n     @unittest.skip(reason=\"ImageGPT requires clusters at initialization\")\n     def test_init_without_params(self):"
        }
    ],
    "stats": {
        "total": 141,
        "additions": 70,
        "deletions": 71
    }
}