{
    "author": "vasqu",
    "message": "[`Flex Attn`] Fix torch 2.5.1 incompatibilities (#37406)\n\n* remove compile on mask creation, ensure kv blocks do not explode on indices\n\n* trigger ci\n\n* switch dynamic compilation to false\n\n* patch new masking functions as well\n\n* add len check\n\n* i was wrong\n\n* last comment",
    "sha": "58c768922618cf11ce769fb8368c26c6db54c535",
    "files": [
        {
            "sha": "9abff30e3961e83137959b131b944a9921366e0c",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/58c768922618cf11ce769fb8368c26c6db54c535/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58c768922618cf11ce769fb8368c26c6db54c535/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=58c768922618cf11ce769fb8368c26c6db54c535",
            "patch": "@@ -32,10 +32,11 @@\n from packaging import version\n \n from ..utils import is_torch_flex_attn_available, logging\n-from ..utils.import_utils import _torch_version, is_torchdynamo_compiling\n+from ..utils.import_utils import _torch_version, is_torch_less_or_equal, is_torchdynamo_compiling\n \n \n if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import _DEFAULT_SPARSE_BLOCK_SIZE as flex_default_block_size  # noqa: N811\n     from torch.nn.attention.flex_attention import BlockMask, create_block_mask, flex_attention\n \n \n@@ -63,16 +64,20 @@ def __init__(self, training):\n         Initialize or update the singleton instance.\n         \"\"\"\n         if not self._is_flex_compiled or training != self.training:\n+            self.training = training\n+            if is_torch_less_or_equal(\"2.5.1\"):\n+                self._compiled_flex_attention = torch.compile(flex_attention, dynamic=False)\n             # In PyTorch 2.6.0, there's a known issue with flex attention compilation which may\n             # cause errors. The suggested fix is to compile with \"max-autotune-no-cudagraphs\"\n             # see https://github.com/pytorch/pytorch/issues/146260 for training\n-            self.training = training\n-            if version.parse(_torch_version).base_version == \"2.6.0\" and training:\n+            elif version.parse(_torch_version).base_version == \"2.6.0\" and training:\n                 self._compiled_flex_attention = torch.compile(\n                     flex_attention, dynamic=False, mode=\"max-autotune-no-cudagraphs\"\n                 )\n+            # Fallback, usually the most recent torch 2.7.x+ versions\n             else:\n                 self._compiled_flex_attention = torch.compile(flex_attention)\n+\n             self._is_flex_compiled = True\n \n     def __call__(self):\n@@ -140,7 +145,9 @@ def make_flex_block_causal_mask(\n         key_length = total_seq_len\n     if not query_length:\n         query_length = total_seq_len\n-    attention_mask_2d = torch.nn.functional.pad(attention_mask_2d, value=0, pad=(0, key_length))\n+    # older torch (2.5.x) cannot handle sequences not in multiples of 128 (default block size)\n+    pad_len = ((key_length // flex_default_block_size) + 1) * flex_default_block_size\n+    attention_mask_2d = torch.nn.functional.pad(attention_mask_2d, value=0, pad=(0, pad_len - key_length))\n     device = attention_mask_2d.device\n     document_ids = attention_mask_2d.clone()\n \n@@ -208,7 +215,8 @@ def mask_mod(batch_idx, head_idx, q_idx, kv_idx):\n         Q_LEN=query_length,\n         KV_LEN=key_length,\n         device=device,\n-        _compile=True,\n+        # compiling the mask is not BC with older torch\n+        _compile=not is_torch_less_or_equal(\"2.5.1\"),\n     )\n \n "
        },
        {
            "sha": "128abd56ffac840c99b371c04264c9c9e8b53a36",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/58c768922618cf11ce769fb8368c26c6db54c535/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58c768922618cf11ce769fb8368c26c6db54c535/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=58c768922618cf11ce769fb8368c26c6db54c535",
            "patch": "@@ -25,6 +25,7 @@\n \n \n if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import _DEFAULT_SPARSE_BLOCK_SIZE as flex_default_block_size  # noqa: N811\n     from torch.nn.attention.flex_attention import BlockMask, create_block_mask\n else:\n     # Register a fake type to avoid crashing for annotations and `isinstance` checks\n@@ -550,6 +551,13 @@ def flex_attention_mask(\n \n     # Potentially add the padding 2D mask\n     if attention_mask is not None:\n+        # Older torch (2.5.x) cannot handle sequences not in multiples of 128 (default block size)\n+        # Hence we pad to multiples of this as a minimum to ensure this\n+        pad_len = ((attention_mask.shape[1] // flex_default_block_size) + 1) * flex_default_block_size\n+        pad_len = pad_len - attention_mask.shape[1]\n+        if not _is_torch_greater_or_equal_than_2_6 and pad_len > 0:\n+            attention_mask = torch.nn.functional.pad(attention_mask, value=0, pad=(0, pad_len))\n+\n         padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n         mask_function = and_masks(mask_function, padding_mask_function(padding_mask))\n \n@@ -564,7 +572,7 @@ def flex_attention_mask(\n         Q_LEN=q_length,\n         KV_LEN=kv_length,\n         device=cache_position.device,\n-        _compile=True,\n+        _compile=_is_torch_greater_or_equal_than_2_6,\n     )\n     return block_mask\n "
        },
        {
            "sha": "88226e3c7cd821d71cea5561ade8f96c4d9bd159",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/58c768922618cf11ce769fb8368c26c6db54c535/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58c768922618cf11ce769fb8368c26c6db54c535/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=58c768922618cf11ce769fb8368c26c6db54c535",
            "patch": "@@ -1173,6 +1173,24 @@ def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False):\n         return version.parse(importlib.metadata.version(\"torch\")) >= version.parse(library_version)\n \n \n+@lru_cache\n+def is_torch_less_or_equal(library_version: str, accept_dev: bool = False):\n+    \"\"\"\n+    Accepts a library version and returns True if the current version of the library is less than or equal to the\n+    given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n+    2.7.0).\n+    \"\"\"\n+    if not _is_package_available(\"torch\"):\n+        return False\n+\n+    if accept_dev:\n+        return version.parse(version.parse(importlib.metadata.version(\"torch\")).base_version) <= version.parse(\n+            library_version\n+        )\n+    else:\n+        return version.parse(importlib.metadata.version(\"torch\")) <= version.parse(library_version)\n+\n+\n @lru_cache\n def is_huggingface_hub_greater_or_equal(library_version: str, accept_dev: bool = False):\n     if not _is_package_available(\"huggingface_hub\"):"
        }
    ],
    "stats": {
        "total": 46,
        "additions": 40,
        "deletions": 6
    }
}