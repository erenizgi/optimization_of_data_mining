{
    "author": "zucchini-nlp",
    "message": "Fix: dtype cannot be str (#36262)\n\n* fix\n\n* this wan't supposed to be here, revert\n\n* refine tests a bit more",
    "sha": "523f6e743c74ecea90d0c37a172c9819b5691a19",
    "files": [
        {
            "sha": "2c28ec8f1e5e02b3ea3cf98d56c1d4f47ffbbf83",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/523f6e743c74ecea90d0c37a172c9819b5691a19/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/523f6e743c74ecea90d0c37a172c9819b5691a19/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=523f6e743c74ecea90d0c37a172c9819b5691a19",
            "patch": "@@ -1252,13 +1252,13 @@ def _get_torch_dtype(\n             for key, curr_dtype in torch_dtype.items():\n                 if hasattr(config, key):\n                     value = getattr(config, key)\n+                    curr_dtype = curr_dtype if not isinstance(curr_dtype, str) else getattr(torch, curr_dtype)\n                     value.torch_dtype = curr_dtype\n             # main torch dtype for modules that aren't part of any sub-config\n             torch_dtype = torch_dtype.get(\"\")\n+            torch_dtype = torch_dtype if not isinstance(torch_dtype, str) else getattr(torch, torch_dtype)\n             config.torch_dtype = torch_dtype\n-            if isinstance(torch_dtype, str) and hasattr(torch, torch_dtype):\n-                torch_dtype = getattr(torch, torch_dtype)\n-            elif torch_dtype is None:\n+            if torch_dtype is None:\n                 torch_dtype = torch.float32\n         else:\n             raise ValueError(\n@@ -1269,7 +1269,7 @@ def _get_torch_dtype(\n         dtype_orig = cls._set_default_torch_dtype(torch_dtype)\n     else:\n         # set fp32 as the default dtype for BC\n-        default_dtype = str(torch.get_default_dtype()).split(\".\")[-1]\n+        default_dtype = torch.get_default_dtype()\n         config.torch_dtype = default_dtype\n         for key in config.sub_configs.keys():\n             value = getattr(config, key)"
        },
        {
            "sha": "96cbd4f77bb4a5feca808af0bfd731f7a8afbff1",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/523f6e743c74ecea90d0c37a172c9819b5691a19/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/523f6e743c74ecea90d0c37a172c9819b5691a19/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=523f6e743c74ecea90d0c37a172c9819b5691a19",
            "patch": "@@ -482,9 +482,11 @@ def test_model_from_config_torch_dtype_str(self):\n         # test that from_pretrained works with torch_dtype being strings like \"float32\" for PyTorch backend\n         model = AutoModel.from_pretrained(TINY_T5, torch_dtype=\"float32\")\n         self.assertEqual(model.dtype, torch.float32)\n+        self.assertIsInstance(model.config.torch_dtype, torch.dtype)\n \n         model = AutoModel.from_pretrained(TINY_T5, torch_dtype=\"float16\")\n         self.assertEqual(model.dtype, torch.float16)\n+        self.assertIsInstance(model.config.torch_dtype, torch.dtype)\n \n         # torch.set_default_dtype() supports only float dtypes, so will fail with non-float type\n         with self.assertRaises(ValueError):\n@@ -495,14 +497,22 @@ def test_model_from_config_torch_dtype_composite(self):\n         Test that from_pretrained works with torch_dtype being as a dict per each sub-config in composite config\n         Tiny-Llava has saved auto dtype as `torch.float32` for all modules.\n         \"\"\"\n+        # Load without dtype specified\n+        model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA)\n+        self.assertEqual(model.language_model.dtype, torch.float32)\n+        self.assertEqual(model.vision_tower.dtype, torch.float32)\n+        self.assertIsInstance(model.config.torch_dtype, torch.dtype)\n+\n         # should be able to set torch_dtype as a simple string and the model loads it correctly\n         model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, torch_dtype=\"float32\")\n         self.assertEqual(model.language_model.dtype, torch.float32)\n         self.assertEqual(model.vision_tower.dtype, torch.float32)\n+        self.assertIsInstance(model.config.torch_dtype, torch.dtype)\n \n         model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, torch_dtype=torch.float16)\n         self.assertEqual(model.language_model.dtype, torch.float16)\n         self.assertEqual(model.vision_tower.dtype, torch.float16)\n+        self.assertIsInstance(model.config.torch_dtype, torch.dtype)\n \n         # should be able to set torch_dtype as a dict for each sub-config\n         model = LlavaForConditionalGeneration.from_pretrained(\n@@ -511,6 +521,7 @@ def test_model_from_config_torch_dtype_composite(self):\n         self.assertEqual(model.language_model.dtype, torch.float32)\n         self.assertEqual(model.vision_tower.dtype, torch.float16)\n         self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.bfloat16)\n+        self.assertIsInstance(model.config.torch_dtype, torch.dtype)\n \n         # should be able to set the values as torch.dtype (not str)\n         model = LlavaForConditionalGeneration.from_pretrained(\n@@ -519,6 +530,7 @@ def test_model_from_config_torch_dtype_composite(self):\n         self.assertEqual(model.language_model.dtype, torch.float32)\n         self.assertEqual(model.vision_tower.dtype, torch.float16)\n         self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.bfloat16)\n+        self.assertIsInstance(model.config.torch_dtype, torch.dtype)\n \n         # should be able to set the values in configs directly and pass it to `from_pretrained`\n         config = copy.deepcopy(model.config)\n@@ -529,13 +541,15 @@ def test_model_from_config_torch_dtype_composite(self):\n         self.assertEqual(model.language_model.dtype, torch.float32)\n         self.assertEqual(model.vision_tower.dtype, torch.bfloat16)\n         self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.float16)\n+        self.assertIsInstance(model.config.torch_dtype, torch.dtype)\n \n         # but if the model has `_keep_in_fp32_modules` then those modules should be in fp32 no matter what\n         LlavaForConditionalGeneration._keep_in_fp32_modules = [\"multi_modal_projector\"]\n         model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, config=config, torch_dtype=\"auto\")\n         self.assertEqual(model.language_model.dtype, torch.float32)\n         self.assertEqual(model.vision_tower.dtype, torch.bfloat16)\n         self.assertEqual(model.multi_modal_projector.linear_1.weight.dtype, torch.float32)\n+        self.assertIsInstance(model.config.torch_dtype, torch.dtype)\n \n         # torch.set_default_dtype() supports only float dtypes, so will fail with non-float type\n         with self.assertRaises(ValueError):"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 18,
        "deletions": 4
    }
}