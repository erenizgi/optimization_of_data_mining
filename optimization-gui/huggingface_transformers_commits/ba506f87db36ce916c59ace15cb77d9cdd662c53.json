{
    "author": "winglian",
    "message": "make the loss context manager easier to extend (#39321)",
    "sha": "ba506f87db36ce916c59ace15cb77d9cdd662c53",
    "files": [
        {
            "sha": "1f20ca0c77464dcc6d7e962d470df163009950d9",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba506f87db36ce916c59ace15cb77d9cdd662c53/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba506f87db36ce916c59ace15cb77d9cdd662c53/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=ba506f87db36ce916c59ace15cb77d9cdd662c53",
            "patch": "@@ -3721,7 +3721,13 @@ def compute_loss_context_manager(self):\n         \"\"\"\n         A helper wrapper to group together context managers.\n         \"\"\"\n-        return self.autocast_smart_context_manager()\n+        ctx_stack = contextlib.ExitStack()\n+\n+        autocast_ctx = self.autocast_smart_context_manager()\n+        if not isinstance(autocast_ctx, contextlib.nullcontext):\n+            ctx_stack.enter_context(autocast_ctx)\n+\n+        return ctx_stack\n \n     def autocast_smart_context_manager(self, cache_enabled: Optional[bool] = True):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 8,
        "additions": 7,
        "deletions": 1
    }
}