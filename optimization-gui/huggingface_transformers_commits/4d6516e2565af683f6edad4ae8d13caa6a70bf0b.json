{
    "author": "Cyrilvallez",
    "message": "Simplify tie weights logic (#42895)\n\n* fix\n\n* let's not use source backup, clearer to use original name imo\n\n* fix\n\n* use a set\n\n* simplify\n\n* style\n\n* add comment",
    "sha": "4d6516e2565af683f6edad4ae8d13caa6a70bf0b",
    "files": [
        {
            "sha": "507c4e119e4f50aef2937b14a13edab195a94fb4",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 21,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d6516e2565af683f6edad4ae8d13caa6a70bf0b/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d6516e2565af683f6edad4ae8d13caa6a70bf0b/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4d6516e2565af683f6edad4ae8d13caa6a70bf0b",
            "patch": "@@ -2387,11 +2387,6 @@ def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping\n \n         tied_keys = list(tied_keys.items())\n         for i, (target_param_name, source_param_name) in enumerate(tied_keys):\n-            # Usually we tie a single target to a single source, but when both are missing we may later tie\n-            # both the source and target to a third \"backup\" parameter that is present in the checkpoint, so we use\n-            # a list here\n-            target_param_names = [target_param_name]\n-\n             # This is `from_pretrained` -> let's check symmetrically in case the source key is not present\n             if missing_keys is not None:\n                 remove_from_missing = True\n@@ -2412,7 +2407,6 @@ def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping\n                 # We're missing the source but we have the target -> we swap them, tying the parameter that exists\n                 elif not source_is_there and target_is_there:\n                     target_param_name, source_param_name = source_param_name, target_param_name\n-                    target_param_names = [target_param_name]\n                 # Both are missing -> check other keys in case more than 2 keys are tied to the same weight\n                 elif not source_is_there and not target_is_there:\n                     for target_backup, source_backup in tied_keys[i + 1 :]:\n@@ -2421,10 +2415,10 @@ def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping\n                         if source_backup == source_param_name:\n                             target_backup_is_there = target_backup not in missing_keys\n                             # If the target is present, we found the correct weight to tie into (we know the source is missing)\n+                            # Note here that we do not tie the missing source right now as well, as it will be done anyway when\n+                            # the pair (target_backup, source_backup) becomes the main pair (target_param_name, source_param_name)\n                             if target_backup_is_there:\n                                 source_param_name = target_backup\n-                                # Append the source as well, since both are missing we'll tie both\n-                                target_param_names.append(source_param_name)\n                                 break\n                     # If we did not break from the loop, it was impossible to find a source key -> let's raise\n                     else:\n@@ -2440,19 +2434,18 @@ def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping\n \n             # Perform the actual tying\n             source_param = self.get_parameter_or_buffer(source_param_name)\n-            for target_param_name in target_param_names:\n-                if \".\" in target_param_name:\n-                    parent_name, name = target_param_name.rsplit(\".\", 1)\n-                    parent = self.get_submodule(parent_name)\n-                else:\n-                    name = target_param_name\n-                    parent = self\n-                # Tie the weights\n-                setattr(parent, name, source_param)\n-                self._adjust_bias(parent, source_param)\n-                # Remove from missing if necesary\n-                if missing_keys is not None and remove_from_missing:\n-                    missing_keys.discard(target_param_name)\n+            if \".\" in target_param_name:\n+                parent_name, name = target_param_name.rsplit(\".\", 1)\n+                parent = self.get_submodule(parent_name)\n+            else:\n+                name = target_param_name\n+                parent = self\n+            # Tie the weights\n+            setattr(parent, name, source_param)\n+            self._adjust_bias(parent, source_param)\n+            # Remove from missing if necesary\n+            if missing_keys is not None and remove_from_missing:\n+                missing_keys.discard(target_param_name)\n \n     def _adjust_bias(self, output_embeddings, input_embeddings):\n         if getattr(output_embeddings, \"bias\", None) is not None and hasattr(output_embeddings, \"weight\"):"
        }
    ],
    "stats": {
        "total": 35,
        "additions": 14,
        "deletions": 21
    }
}