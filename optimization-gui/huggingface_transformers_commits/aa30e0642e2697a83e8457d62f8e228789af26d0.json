{
    "author": "SunMarc",
    "message": "Update quantization CI (#41068)\n\n* fix\n\n* new everything\n\n* fix",
    "sha": "aa30e0642e2697a83e8457d62f8e228789af26d0",
    "files": [
        {
            "sha": "f78614d59a6e638a435c444af3a6143bc65ea661",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 32,
            "deletions": 28,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa30e0642e2697a83e8457d62f8e228789af26d0/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa30e0642e2697a83e8457d62f8e228789af26d0/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=aa30e0642e2697a83e8457d62f8e228789af26d0",
            "patch": "@@ -1,4 +1,4 @@\n-FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04\n+FROM nvidia/cuda:12.6.0-cudnn-devel-ubuntu22.04\n LABEL maintainer=\"Hugging Face\"\n \n ARG DEBIAN_FRONTEND=noninteractive\n@@ -9,9 +9,9 @@ SHELL [\"sh\", \"-lc\"]\n # The following `ARG` are mainly used to specify the versions explicitly & directly in this docker file, and not meant\n # to be used as arguments for docker build (so far).\n \n-ARG PYTORCH='2.6.0'\n+ARG PYTORCH='2.8.0'\n # Example: `cu102`, `cu113`, etc.\n-ARG CUDA='cu121'\n+ARG CUDA='cu126'\n # Disable kernel mapping for quantization tests\n ENV DISABLE_KERNEL_MAPPING=1\n \n@@ -46,50 +46,29 @@ RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/opt\n # Add PEFT\n RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/peft@main#egg=peft\n \n-# Add aqlm for quantization testing\n-RUN python3 -m pip install --no-cache-dir aqlm[gpu]==1.0.2\n-\n-# Add vptq for quantization testing\n-RUN pip install vptq\n-\n-# Add spqr for quantization testing\n-# Commented for now as No matching distribution found we need to reach out to the authors\n-# RUN python3 -m pip install --no-cache-dir spqr_quant[gpu]\n-\n # Add hqq for quantization testing\n RUN python3 -m pip install --no-cache-dir hqq\n \n # For GGUF tests\n RUN python3 -m pip install --no-cache-dir gguf\n \n # Add autoawq for quantization testing\n-# New release v0.2.8\n RUN python3 -m pip install --no-cache-dir autoawq[kernels]\n \n # Add quanto for quantization testing\n RUN python3 -m pip install --no-cache-dir optimum-quanto\n \n-# Add eetq for quantization testing\n-RUN git clone https://github.com/NetEase-FuXi/EETQ.git && cd EETQ/ && git submodule update --init --recursive && pip install .\n-\n-# # Add flute-kernel and fast_hadamard_transform for quantization testing\n-# # Commented for now as they cause issues with the build\n-# # TODO: create a new workflow to test them\n-# RUN python3 -m pip install --no-cache-dir flute-kernel==0.4.1\n-# RUN python3 -m pip install --no-cache-dir git+https://github.com/Dao-AILab/fast-hadamard-transform.git\n-\n-# Add fp-quant for quantization testing\n-# Requires py3.11 but our CI runs on 3.9\n-# RUN python3 -m pip install --no-cache-dir \"fp-quant>=0.1.6\"\n-\n # Add compressed-tensors for quantization testing\n RUN python3 -m pip install --no-cache-dir compressed-tensors\n \n # Add AMD Quark for quantization testing\n RUN python3 -m pip install --no-cache-dir amd-quark\n \n # Add AutoRound for quantization testing\n-RUN python3 -m pip install --no-cache-dir \"auto-round>=0.5.0\"\n+RUN python3 -m pip install --no-cache-dir auto-round\n+\n+# Add torchao for quantization testing\n+RUN python3 -m pip install --no-cache-dir torchao\n \n # Add transformers in editable mode\n RUN python3 -m pip install --no-cache-dir -e ./transformers[dev-torch]\n@@ -103,3 +82,28 @@ RUN python3 -m pip uninstall -y flash-attn\n # When installing in editable mode, `transformers` is not recognized as a package.\n # this line must be added in order for python to be aware of transformers.\n RUN cd transformers && python3 setup.py develop\n+\n+# Low usage or incompatible lib, will enable later on\n+\n+# # Add aqlm for quantization testing\n+# RUN python3 -m pip install --no-cache-dir aqlm[gpu]==1.0.2\n+\n+# # Add vptq for quantization testing\n+# RUN pip install vptq\n+\n+# Add spqr for quantization testing\n+# Commented for now as No matching distribution found we need to reach out to the authors\n+# RUN python3 -m pip install --no-cache-dir spqr_quant[gpu]\n+\n+# # Add eetq for quantization testing\n+# RUN git clone https://github.com/NetEase-FuXi/EETQ.git && cd EETQ/ && git submodule update --init --recursive && pip install .\n+\n+# # Add flute-kernel and fast_hadamard_transform for quantization testing\n+# # Commented for now as they cause issues with the build\n+# # TODO: create a new workflow to test them\n+# RUN python3 -m pip install --no-cache-dir flute-kernel==0.4.1\n+# RUN python3 -m pip install --no-cache-dir git+https://github.com/Dao-AILab/fast-hadamard-transform.git\n+\n+# Add fp-quant for quantization testing\n+# Requires py3.11 but our CI runs on 3.9\n+# RUN python3 -m pip install --no-cache-dir \"fp-quant>=0.1.6\"\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 60,
        "additions": 32,
        "deletions": 28
    }
}