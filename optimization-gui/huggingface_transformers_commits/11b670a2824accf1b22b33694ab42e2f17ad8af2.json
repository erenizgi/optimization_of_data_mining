{
    "author": "cyyever",
    "message": "Fix run_slow (#38314)\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "11b670a2824accf1b22b33694ab42e2f17ad8af2",
    "files": [
        {
            "sha": "566a4ed00784e87480ed40e09fc14b00606b7f98",
            "filename": "tests/generation/test_paged_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/11b670a2824accf1b22b33694ab42e2f17ad8af2/tests%2Fgeneration%2Ftest_paged_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11b670a2824accf1b22b33694ab42e2f17ad8af2/tests%2Fgeneration%2Ftest_paged_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_paged_attention.py?ref=11b670a2824accf1b22b33694ab42e2f17ad8af2",
            "patch": "@@ -4,7 +4,7 @@\n from parameterized import parameterized\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n-from transformers.testing_utils import require_flash_attn, require_torch_gpu, run_slow\n+from transformers.testing_utils import require_flash_attn, require_torch_gpu, slow\n \n \n _TEST_PROMPTS = [\n@@ -24,7 +24,7 @@\n ]\n \n \n-@run_slow\n+@slow\n @require_torch_gpu\n @require_flash_attn\n class TestBatchGeneration(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}