{
    "author": "itazap",
    "message": "refactor create_token_type_ids_from_sequences (#37681)\n\n* rm build_input.. from old file\n\n* refactor create_token_type_ids_from_sequences\n\n* handle when cls_token_id is None\n\n* updated fix\n\n* markuplm\n\n* refactoring rest of models\n\n* copies\n\n* revert funnel\n\n* rm incorrect file\n\n* ruff\n\n* ruff",
    "sha": "324cc77dc39566d69539ee026942cd8fc840a03f",
    "files": [
        {
            "sha": "8b34a266f6148550d0826150cc355c39e9e7c426",
            "filename": "src/transformers/models/albert/tokenization_albert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -299,36 +299,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. An ALBERT\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")"
        },
        {
            "sha": "606c614be671645a928479240d90b2e2c7ee4cfb",
            "filename": "src/transformers/models/albert/tokenization_albert_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -155,36 +155,6 @@ def build_inputs_with_special_tokens(\n             return cls + token_ids_0 + sep\n         return cls + token_ids_0 + sep + token_ids_1 + sep\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An ALBERT\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        if token_ids_1 is None, only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of ids.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not self.can_save_slow_tokenizer:\n             raise ValueError("
        },
        {
            "sha": "7789b6cc5a51e3268f3dfb7e8a040b11716cd60b",
            "filename": "src/transformers/models/bert/tokenization_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -236,35 +236,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):"
        },
        {
            "sha": "52278ba8ccddef217538a2b05598c8ab3ce1fae2",
            "filename": "src/transformers/models/bert/tokenization_bert_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Fast Tokenization classes for Bert.\"\"\"\n \n import json\n-from typing import List, Optional, Tuple\n+from typing import Optional, Tuple\n \n from tokenizers import normalizers\n \n@@ -138,35 +138,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)"
        },
        {
            "sha": "e22a3fff84b093c66e4df9362c55e4ad4d266cb1",
            "filename": "src/transformers/models/bert_japanese/tokenization_bert_japanese.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -309,36 +309,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.create_token_type_ids_from_sequences\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if os.path.isdir(save_directory):\n             if self.subword_tokenizer_type == \"sentencepiece\":"
        },
        {
            "sha": "b498f57563b6feab3fd37e98e3288261e003f016",
            "filename": "src/transformers/models/big_bird/tokenization_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -299,28 +299,5 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n-        pair mask has the following format: :: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 | first sequence | second\n-        sequence | If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n \n __all__ = [\"BigBirdTokenizer\"]"
        },
        {
            "sha": "1af9be12b347e1e02dc674882b644d9bcbaf146a",
            "filename": "src/transformers/models/big_bird/tokenization_big_bird_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -175,36 +175,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An ALBERT\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        if token_ids_1 is None, only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of ids.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not self.can_save_slow_tokenizer:\n             raise ValueError("
        },
        {
            "sha": "b1ca1d45f1c2376eb34dc04299ae28b23574863d",
            "filename": "src/transformers/models/biogpt/tokenization_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fbiogpt%2Ftokenization_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fbiogpt%2Ftokenization_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Ftokenization_biogpt.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -281,36 +281,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1))\n         return [1] + ([0] * len(token_ids_0))\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A FAIRSEQ\n-        Transformer sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-\n-        # no bos used in fairseq\n-        if token_ids_1 is None:\n-            return len(token_ids_0 + sep) * [0]\n-        return len(token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")"
        },
        {
            "sha": "c32410b5fd4405945da740c2caea3c3c0ec3dde7",
            "filename": "src/transformers/models/canine/tokenization_canine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -205,37 +205,6 @@ def get_special_tokens_mask(\n             result += ([0] * len(token_ids_1)) + [1]\n         return result\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A CANINE\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        result = len(cls + token_ids_0 + sep) * [0]\n-        if token_ids_1 is not None:\n-            result += len(token_ids_1 + sep) * [1]\n-        return result\n-\n     # CanineTokenizer has no vocab file\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None):\n         return ()"
        },
        {
            "sha": "b6fdda509acb3b730c951798ab3fc4e6e5220d86",
            "filename": "src/transformers/models/codegen/tokenization_codegen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -276,35 +276,6 @@ def convert_tokens_to_string(self, tokens):\n         text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n         return text\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id] if self.sep_token_id is not None else []\n-        cls = [self.cls_token_id] if self.sep_token_id is not None else []\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")"
        },
        {
            "sha": "0d14c82773c4c211eb5175cc4f66e459cf740f1c",
            "filename": "src/transformers/models/codegen/tokenization_codegen_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -154,36 +154,6 @@ def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n \n         return super()._encode_plus(*args, **kwargs)\n \n-    # Copied from transformers.models.codegen.tokenization_codegen.CodeGenTokenizer.create_token_type_ids_from_sequences\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id] if self.sep_token_id is not None else []\n-        cls = [self.cls_token_id] if self.sep_token_id is not None else []\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)"
        },
        {
            "sha": "814743617418f473d7483812ccafa313f0d5bcd6",
            "filename": "src/transformers/models/convbert/tokenization_convbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -239,35 +239,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ConvBERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):"
        },
        {
            "sha": "854ceaa09ba9fb359a6cf7fb3374519697cd37b6",
            "filename": "src/transformers/models/convbert/tokenization_convbert_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Tokenization classes for ConvBERT.\"\"\"\n \n import json\n-from typing import List, Optional, Tuple\n+from typing import Optional, Tuple\n \n from tokenizers import normalizers\n \n@@ -139,35 +139,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ConvBERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)"
        },
        {
            "sha": "af807b425f8d1af709fee62b7a18f5eda42b42e5",
            "filename": "src/transformers/models/deberta/tokenization_deberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -298,36 +298,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A DeBERTa\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._tokenize\n     def _tokenize(self, text):\n         \"\"\"Tokenize a string.\"\"\""
        },
        {
            "sha": "d91e2434458f83301607eb9dd477b2a8f5a10b46",
            "filename": "src/transformers/models/deberta/tokenization_deberta_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -179,36 +179,6 @@ def build_inputs_with_special_tokens(\n         sep = [self.sep_token_id]\n         return cls + token_ids_0 + sep + token_ids_1 + sep\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A DeBERTa\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     # Copied from transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast._batch_encode_plus\n     def _batch_encode_plus(self, *args, **kwargs) -> BatchEncoding:\n         is_split_into_words = kwargs.get(\"is_split_into_words\", False)"
        },
        {
            "sha": "1bf6e95659087c25fdc8acb3fb5581a49b94d375",
            "filename": "src/transformers/models/deberta_v2/tokenization_deberta_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -208,33 +208,6 @@ def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_spe\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A DeBERTa\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n         add_prefix_space = kwargs.pop(\"add_prefix_space\", False)\n         if is_split_into_words or add_prefix_space:"
        },
        {
            "sha": "44aa71c68562a6c5b63fe250a68f0790dbee6d5f",
            "filename": "src/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -169,33 +169,6 @@ def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_spe\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A DeBERTa\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not self.can_save_slow_tokenizer:\n             raise ValueError("
        },
        {
            "sha": "df152b64c1316f0b4a432da83d3ad8aa142144d0",
            "filename": "src/transformers/models/deprecated/realm/tokenization_realm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -304,35 +304,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A REALM sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):"
        },
        {
            "sha": "64c8400729f0ddacabc246315aa500cae1a3f789",
            "filename": "src/transformers/models/deprecated/realm/tokenization_realm_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Fast Tokenization classes for REALM.\"\"\"\n \n import json\n-from typing import List, Optional, Tuple\n+from typing import Optional, Tuple\n \n from tokenizers import normalizers\n \n@@ -215,35 +215,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A REALM sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)"
        },
        {
            "sha": "1e094a5a1a36622b0be7763ef185ce5fce626b92",
            "filename": "src/transformers/models/deprecated/retribert/tokenization_retribert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -233,35 +233,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):"
        },
        {
            "sha": "395d7cd55e8f737f5f5636ba17a689294aecf161",
            "filename": "src/transformers/models/deprecated/retribert/tokenization_retribert_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Tokenization classes for RetriBERT.\"\"\"\n \n import json\n-from typing import List, Optional, Tuple\n+from typing import Optional, Tuple\n \n from tokenizers import normalizers\n \n@@ -142,35 +142,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)"
        },
        {
            "sha": "9cc632f6856805146522c58c4575ea2bb45bd08a",
            "filename": "src/transformers/models/distilbert/tokenization_distilbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -247,36 +247,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.create_token_type_ids_from_sequences\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.save_vocabulary\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         index = 0"
        },
        {
            "sha": "4c19e0f534eef1d2dd19bd5a8d4ae4dccc5c98f8",
            "filename": "src/transformers/models/distilbert/tokenization_distilbert_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 31,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Tokenization classes for DistilBERT.\"\"\"\n \n import json\n-from typing import List, Optional, Tuple\n+from typing import Optional, Tuple\n \n from tokenizers import normalizers\n \n@@ -140,36 +140,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    # Copied from transformers.models.bert.tokenization_bert_fast.BertTokenizerFast.create_token_type_ids_from_sequences\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     # Copied from transformers.models.bert.tokenization_bert_fast.BertTokenizerFast.save_vocabulary\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)"
        },
        {
            "sha": "f9c0aea392dfaaa56b7d5658e65f38eff9a0cc7f",
            "filename": "src/transformers/models/electra/tokenization_electra.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -238,35 +238,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Electra sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):"
        },
        {
            "sha": "6767e3e3517870b15fd219d0015f90f120151b1f",
            "filename": "src/transformers/models/electra/tokenization_electra_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \n import json\n-from typing import List, Optional, Tuple\n+from typing import Optional, Tuple\n \n from tokenizers import normalizers\n \n@@ -135,35 +135,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ELECTRA sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)"
        },
        {
            "sha": "cfcd3defa8852ad12d786a7c8f27f2b616d4d9e8",
            "filename": "src/transformers/models/flaubert/tokenization_flaubert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fflaubert%2Ftokenization_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fflaubert%2Ftokenization_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Ftokenization_flaubert.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -485,36 +485,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.create_token_type_ids_from_sequences\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. An XLM sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.save_vocabulary\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not os.path.isdir(save_directory):"
        },
        {
            "sha": "3218cbff1eb99cbf3b8509e19e92f5ba245461cc",
            "filename": "src/transformers/models/fnet/tokenization_fnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -293,35 +293,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. An FNet sequence\n-        pair mask has the following format: :\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 | first sequence | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")"
        },
        {
            "sha": "ca2c3ca3907541cce5020ce493f9880a64114ed7",
            "filename": "src/transformers/models/fnet/tokenization_fnet_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -138,36 +138,6 @@ def build_inputs_with_special_tokens(\n             return cls + token_ids_0 + sep\n         return cls + token_ids_0 + sep + token_ids_1 + sep\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An FNet\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        if token_ids_1 is None, only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of ids.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")"
        },
        {
            "sha": "d47efffd03ed2c4a66699954c4d49015059598ac",
            "filename": "src/transformers/models/fsmt/tokenization_fsmt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Ffsmt%2Ftokenization_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Ffsmt%2Ftokenization_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Ftokenization_fsmt.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -430,39 +430,6 @@ def get_special_tokens_mask(\n             return ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A FAIRSEQ\n-        Transformer sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-\n-        Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An\n-        FAIRSEQ_TRANSFORMER sequence pair mask has the following format:\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-\n-        # no bos used in fairseq\n-        if token_ids_1 is None:\n-            return len(token_ids_0 + sep) * [0]\n-        return len(token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")"
        },
        {
            "sha": "9ce5caa9f04c4d47b3b184022f795941e2aa93b2",
            "filename": "src/transformers/models/herbert/tokenization_herbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -564,36 +564,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.create_token_type_ids_from_sequences\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. An XLM sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.save_vocabulary\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not os.path.isdir(save_directory):"
        },
        {
            "sha": "fd13353146b7e1bbeb01b2cd2d25a3dc96fba095",
            "filename": "src/transformers/models/herbert/tokenization_herbert_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -125,34 +125,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. HerBERT, like\n-        BERT sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)"
        },
        {
            "sha": "f7ca50ec4d70f55a94981052ae5fa5999138a82a",
            "filename": "src/transformers/models/layoutlm/tokenization_layoutlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -239,35 +239,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A LayoutLM sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):"
        },
        {
            "sha": "09bcc18e3b85ce607cb8cf1f298b3965fdb875f8",
            "filename": "src/transformers/models/layoutlm/tokenization_layoutlm_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Tokenization class for model LayoutLM.\"\"\"\n \n import json\n-from typing import List, Optional, Tuple\n+from typing import Optional, Tuple\n \n from tokenizers import normalizers\n \n@@ -139,35 +139,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A LayoutLM sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)"
        },
        {
            "sha": "a607777d494ace752a14a472a28bf5f94e052b9d",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -358,29 +358,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n-        pair mask has the following format: :: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 | first sequence | second\n-        sequence | If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):"
        },
        {
            "sha": "f6c9d3a1d261c70b417ba52eac9742b5968fa5ee",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -781,29 +781,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n-        pair mask has the following format: :: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 | first sequence | second\n-        sequence | If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)"
        },
        {
            "sha": "5f6d39a0ca177256dc2f9760bf2595d5f2450308",
            "filename": "src/transformers/models/lxmert/tokenization_lxmert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -238,35 +238,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Lxmert sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):"
        },
        {
            "sha": "eadcff6d418c8eeadca17cc2ffb58dafb72ec58d",
            "filename": "src/transformers/models/lxmert/tokenization_lxmert_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \n import json\n-from typing import List, Optional, Tuple\n+from typing import Optional, Tuple\n \n from tokenizers import normalizers\n \n@@ -135,35 +135,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Lxmert sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)"
        },
        {
            "sha": "b45f6916bce7453aaa00a6c178dd7ab64e23185e",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -474,7 +474,6 @@ def create_token_type_ids_from_sequences(\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task. RoBERTa does not\n         make use of token type ids, therefore a list of zeros is returned.\n-\n         Args:\n             token_ids_0 (`List[int]`):\n                 List of IDs."
        },
        {
            "sha": "6b1db74ffba80f38a047cfe4ded94221588b5d4f",
            "filename": "src/transformers/models/mobilebert/tokenization_mobilebert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -240,35 +240,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A MobileBERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):"
        },
        {
            "sha": "d745868c972f55e5fcd3a4ba4adbb40d412a8611",
            "filename": "src/transformers/models/mobilebert/tokenization_mobilebert_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -16,7 +16,7 @@\n \"\"\"Tokenization classes for MobileBERT.\"\"\"\n \n import json\n-from typing import List, Optional, Tuple\n+from typing import Optional, Tuple\n \n from tokenizers import normalizers\n \n@@ -140,35 +140,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A MobileBERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)"
        },
        {
            "sha": "97e22b6fa2f370dce876d864a02682809e753b0d",
            "filename": "src/transformers/models/prophetnet/tokenization_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -432,34 +432,6 @@ def get_special_tokens_mask(\n             return ([0] * len(token_ids_0)) + [1]\n         return ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ProphetNet\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        if token_ids_1 is None:\n-            return len(token_ids_0 + sep) * [0]\n-        return len(token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):"
        },
        {
            "sha": "a0ba7cf5ea653786bbfb963489b24352be909403",
            "filename": "src/transformers/models/rembert/tokenization_rembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -216,36 +216,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A RemBERT\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))"
        },
        {
            "sha": "0d6dd1411a36eec15ad961415c734a689fe356a8",
            "filename": "src/transformers/models/rembert/tokenization_rembert_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -181,36 +181,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Creates a mask from the two sequences passed to be used in a sequence-pair classification task. A RemBERT\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        if token_ids_1 is None, only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of ids.\n-            token_ids_1 (`List[int]`, *optional*, defaults to `None`):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))"
        },
        {
            "sha": "bea1769d0c6578e24aafe74f980ebd7d63843ebd",
            "filename": "src/transformers/models/roc_bert/tokenization_roc_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -824,36 +824,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.create_token_type_ids_from_sequences\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str, str, str]:\n         index = 0\n         if os.path.isdir(save_directory):"
        },
        {
            "sha": "f1ee807c8d7157e8a2c2a4609601276e1e4474df",
            "filename": "src/transformers/models/roformer/tokenization_roformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -487,35 +487,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A RoFormer\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):"
        },
        {
            "sha": "f3ed8470f2dba1a199a7b4badb43b4b795010f5f",
            "filename": "src/transformers/models/roformer/tokenization_roformer_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Tokenization classes for RoFormer.\"\"\"\n \n import json\n-from typing import List, Optional, Tuple\n+from typing import Optional, Tuple\n \n from tokenizers import normalizers\n from tokenizers.pre_tokenizers import BertPreTokenizer, PreTokenizer\n@@ -132,35 +132,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A RoFormer\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)"
        },
        {
            "sha": "804d5f336f9ee42f1eff5a6521efdfb11f4ed12d",
            "filename": "src/transformers/models/squeezebert/tokenization_squeezebert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -239,35 +239,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A SqueezeBERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):"
        },
        {
            "sha": "c0f96a78a8990b2cba55bf2985e31b89dbbbd863",
            "filename": "src/transformers/models/squeezebert/tokenization_squeezebert_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert_fast.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Tokenization classes for SqueezeBERT.\"\"\"\n \n import json\n-from typing import List, Optional, Tuple\n+from typing import Optional, Tuple\n \n from tokenizers import normalizers\n \n@@ -139,35 +139,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A SqueezeBERT sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)"
        },
        {
            "sha": "a0e6b45e11483f65053afa48d7be34182d79e3f9",
            "filename": "src/transformers/models/xlm/tokenization_xlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fxlm%2Ftokenization_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Fmodels%2Fxlm%2Ftokenization_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Ftokenization_xlm.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -527,35 +527,6 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. An XLM sequence\n-        pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")"
        },
        {
            "sha": "51e5fcb82b1188d38c59a484f877813f5788e7aa",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/324cc77dc39566d69539ee026942cd8fc840a03f/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=324cc77dc39566d69539ee026942cd8fc840a03f",
            "patch": "@@ -3389,9 +3389,13 @@ def create_token_type_ids_from_sequences(\n         Returns:\n             `List[int]`: The token type ids.\n         \"\"\"\n+        cls_len = int(getattr(self, \"cls_token_id\", None) is not None)\n+        sep_len = int(getattr(self, \"sep_token_id\", None) is not None)\n+\n         if token_ids_1 is None:\n-            return len(token_ids_0) * [0]\n-        return [0] * len(token_ids_0) + [1] * len(token_ids_1)\n+            return [0] * (cls_len + len(token_ids_0) + sep_len)\n+\n+        return [0] * (cls_len + len(token_ids_0) + sep_len) + [1] * (len(token_ids_1) + sep_len)\n \n     def build_inputs_with_special_tokens(\n         self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None"
        }
    ],
    "stats": {
        "total": 1421,
        "additions": 17,
        "deletions": 1404
    }
}