{
    "author": "Rocketknight1",
    "message": "Remove redundant test_sdpa_equivalence test (#38436)\n\n* Remove redundant test\n\n* make fixup",
    "sha": "0ed6f7e6b455c6fd3530d8b764996ca771c49ee4",
    "files": [
        {
            "sha": "f41d3ab6e3228ece745d81a5137ead2e8c1a8310",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ed6f7e6b455c6fd3530d8b764996ca771c49ee4/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ed6f7e6b455c6fd3530d8b764996ca771c49ee4/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=0ed6f7e6b455c6fd3530d8b764996ca771c49ee4",
            "patch": "@@ -22,9 +22,7 @@\n from transformers.testing_utils import (\n     is_flaky,\n     require_flash_attn,\n-    require_torch_accelerator,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n )\n \n@@ -410,39 +408,6 @@ def test_model_rope_scaling(self):\n         with self.assertRaises(AssertionError):\n             torch.testing.assert_close(yarn_sin_long, original_sin_long)\n \n-    @require_torch_sdpa\n-    @require_torch_accelerator\n-    @slow\n-    def test_sdpa_equivalence(self):\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(reason=\"Model does not support SDPA\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_sdpa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\"\n-                )\n-                model_sdpa.to(torch_device)\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"eager\"\n-                )\n-                model.to(torch_device)\n-\n-                dummy_input = inputs_dict[model_class.main_input_name]\n-                dummy_input = dummy_input.to(torch_device)\n-                outputs = model(dummy_input, output_hidden_states=True)\n-                outputs_sdpa = model_sdpa(dummy_input, output_hidden_states=True)\n-\n-                logits = outputs.hidden_states[-1]\n-                logits_sdpa = outputs_sdpa.hidden_states[-1]\n-\n-                assert torch.allclose(logits_sdpa, logits, atol=2e-3)\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        }
    ],
    "stats": {
        "total": 35,
        "additions": 0,
        "deletions": 35
    }
}