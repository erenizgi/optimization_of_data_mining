{
    "author": "Abhishek-TAMU",
    "message": "Remove graph breaks for torch.compile() in flash_attention_forward when Lllama Model is padding free tuned (#33932)\n\n* fix: fixes for graph breaks\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* fix: formatting\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* fix: import error\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* fix: Add Fa2Kwargs\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* fix: PR Changes\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* PR changes\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* PR changes\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* PR changes\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* PR changes\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* Revert \"PR changes\"\r\n\r\nThis reverts commit 39d2868e5c93cc5f3f3c7c6ff981b66614c0e0e4.\r\n\r\n* PR changes\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* fix: FlashAttentionKwarg\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* fix: FlashAttentionKwarg\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* PR Changes\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* PR Changes\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* PR Changes\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* PR Changes\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* PR Changes\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* addition of documentation\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* change in _flash_attention_forward\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* make fix-copies\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* revert make fix-copies\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\n\r\n* fix copies\r\n\r\n* style\r\n\r\n* loss kwargs typing\r\n\r\n* style and pull latest changes\r\n\r\n---------\r\n\r\nSigned-off-by: Abhishek <maurya.abhishek@ibm.com>\r\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>",
    "sha": "65753d6065e4d6e79199c923494edbf0d6248fb1",
    "files": [
        {
            "sha": "0a6a7e15bea0816d83c15573638627902a4982f2",
            "filename": "docs/source/en/llm_optims.md",
            "status": "modified",
            "additions": 93,
            "deletions": 0,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/65753d6065e4d6e79199c923494edbf0d6248fb1/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/65753d6065e4d6e79199c923494edbf0d6248fb1/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=65753d6065e4d6e79199c923494edbf0d6248fb1",
            "patch": "@@ -348,6 +348,99 @@ model = AutoModelForCausalLM.from_pretrained(\n )\n ```\n \n+### Fine-Tuning with torch.compile and Padding-Free Data Collation\n+\n+In addition to optimizing inference, you can also enhance the training efficiency of large language models by leveraging torch.compile during fine-tuning and using a padding-free data collator. This approach can significantly speed up training and reduce computational overhead.\n+\n+Here's how you can fine-tune a Llama model using SFTTrainer from the TRL library, with torch_compile enabled and a padding-free data collator:\n+\n+```\n+#################### IMPORTS ###################\n+\n+import math\n+import datasets\n+import dataclasses\n+from transformers import (\n+    AutoModelForCausalLM,\n+    AutoTokenizer,\n+    TrainingArguments\n+)\n+from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n+\n+#################### MODEL LOADING WITH FLASH ATTENTION ###################\n+\n+model_name = \"meta-llama/Llama-3.2-1B\"\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_name,\n+    attn_implementation=\"flash_attention_2\"  # Enables FlashAttention-2\n+)\n+tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n+\n+#################### DATA PREPROCESSING (PADDING-FREE) ###################\n+\n+response_template = \"\\n### Label:\"\n+response_template_ids = tokenizer.encode(\n+    response_template, add_special_tokens=False\n+)[2:]  # Exclude special tokens\n+\n+data_collator = DataCollatorForCompletionOnlyLM(\n+    response_template_ids=response_template_ids,\n+    tokenizer=tokenizer,\n+    ignore_index=-100,\n+    padding_free=True  # Enables padding-free collation\n+)\n+\n+def format_dataset(example):\n+    return {\n+        \"output\": example[\"output\"] + tokenizer.eos_token\n+    }\n+\n+data_files = {\"train\": \"path/to/dataset\"}  # Replace with your dataset path\n+json_dataset = datasets.load_dataset(\"json\", data_files=data_files)\n+formatted_train_dataset = json_dataset[\"train\"].map(format_dataset)\n+\n+################# TRAINING CONFIGURATION ############################\n+\n+train_args = TrainingArguments(\n+    num_train_epochs=5,\n+    per_device_train_batch_size=4,\n+    per_device_eval_batch_size=4,\n+    gradient_accumulation_steps=4,\n+    learning_rate=1e-5,\n+    weight_decay=0.0,\n+    warmup_ratio=0.03,\n+    lr_scheduler_type=\"cosine\",\n+    logging_steps=1,\n+    include_tokens_per_second=True,\n+    save_strategy=\"epoch\",\n+    output_dir=\"output\",\n+    torch_compile=True,  # Enables torch.compile\n+    torch_compile_backend=\"inductor\",\n+    torch_compile_mode=\"default\"\n+)\n+\n+# Convert TrainingArguments to SFTConfig\n+transformer_train_arg_fields = [x.name for x in dataclasses.fields(SFTConfig)]\n+transformer_kwargs = {\n+    k: v\n+    for k, v in train_args.to_dict().items()\n+    if k in transformer_train_arg_fields\n+}\n+training_args = SFTConfig(**transformer_kwargs)\n+\n+####################### FINE-TUNING #####################\n+\n+trainer = SFTTrainer(\n+    model=model,\n+    tokenizer=tokenizer,\n+    train_dataset=formatted_train_dataset,\n+    data_collator=data_collator,\n+    dataset_text_field=\"output\",\n+    args=training_args,\n+)\n+trainer.train()\n+```\n+\n ### PyTorch scaled dot product attention\n \n Scaled dot product attention (SDPA) is automatically enabled in PyTorch 2.0 and it supports FlashAttention, xFormers, and PyTorch's C++ implementation. SDPA chooses the most performant attention algorithm if you're using a CUDA backend. For other backends, SDPA defaults to the PyTorch C++ implementation."
        },
        {
            "sha": "045d2f6d64601000511cd03bffb443e00e912d6a",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 51,
            "deletions": 14,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=65753d6065e4d6e79199c923494edbf0d6248fb1",
            "patch": "@@ -15,7 +15,7 @@\n \n import inspect\n import os\n-from typing import Optional, Tuple\n+from typing import Optional, Tuple, TypedDict\n \n import torch\n import torch.nn.functional as F\n@@ -180,6 +180,10 @@ def prepare_fa2_from_position_ids(query, key, value, position_ids):\n     return (query, key, value, indices_q, (cu_seq_lens, cu_seq_lens), (max_length, max_length))\n \n \n+flash_241 = is_flash_attn_greater_or_equal(\"2.4.1\")\n+deterministic_g = os.environ.get(\"FLASH_ATTENTION_DETERMINISTIC\", \"0\") == \"1\"\n+\n+\n def _flash_attention_forward(\n     query_states: torch.Tensor,\n     key_states: torch.Tensor,\n@@ -194,6 +198,10 @@ def _flash_attention_forward(\n     use_top_left_mask: bool = False,\n     softcap: Optional[float] = None,\n     deterministic: bool = None,\n+    cu_seq_lens_q: Optional[torch.LongTensor] = None,\n+    cu_seq_lens_k: Optional[torch.LongTensor] = None,\n+    max_length_q: Optional[int] = None,\n+    max_length_k: Optional[int] = None,\n ):\n     \"\"\"\n     Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n@@ -232,9 +240,9 @@ def _flash_attention_forward(\n     )\n     flash_kwargs = {\"window_size\": (sliding_window, sliding_window)} if use_sliding_windows else {}\n \n-    if is_flash_attn_greater_or_equal(\"2.4.1\"):\n+    if flash_241:\n         if deterministic is None:\n-            deterministic = os.environ.get(\"FLASH_ATTENTION_DETERMINISTIC\", \"0\") == \"1\"\n+            deterministic = deterministic_g\n         flash_kwargs[\"deterministic\"] = deterministic\n \n     if softcap is not None:\n@@ -267,24 +275,32 @@ def _flash_attention_forward(\n     # If position_ids is provided and check all examples do not contain only 1 sequence, If tensor in increasing\n     # then we probably have one sequence, otherwise it is packed. Additionally check we are in pre-fill/training stage.\n     # Use `flash_attn_varlen_func` to prevent cross-example attention and also allow padding free approach\n-    # Note: the `torch.diff(...)` condition is last to use short-circuit and avoid the cuda synchronization it incurs during inference (query_length == 1 always)\n-    elif position_ids is not None and query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all():\n+    elif position_ids is not None and (\n+        max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())\n+    ):\n         batch_size = query_states.size(0)\n-        query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = prepare_fa2_from_position_ids(\n-            query_states, key_states, value_states, position_ids\n-        )\n \n-        cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n-        max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n+        if cu_seq_lens_q is None or cu_seq_lens_k is None:\n+            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = (\n+                prepare_fa2_from_position_ids(query_states, key_states, value_states, position_ids)\n+            )\n+\n+            cu_seq_lens_q, cu_seq_lens_k = cu_seq_lens\n+            max_length_q, max_length_k = max_seq_lens\n+\n+        else:\n+            query_states = query_states.reshape(-1, query_states.size(-2), query_states.size(-1))\n+            key_states = key_states.reshape(-1, key_states.size(-2), key_states.size(-1))\n+            value_states = value_states.reshape(-1, value_states.size(-2), value_states.size(-1))\n \n         attn_output = flash_attn_varlen_func(\n             query_states,\n             key_states,\n             value_states,\n-            cu_seqlens_q=cu_seqlens_q,\n-            cu_seqlens_k=cu_seqlens_k,\n-            max_seqlen_q=max_seqlen_in_batch_q,\n-            max_seqlen_k=max_seqlen_in_batch_k,\n+            cu_seqlens_q=cu_seq_lens_q,\n+            cu_seqlens_k=cu_seq_lens_k,\n+            max_seqlen_q=max_length_q,\n+            max_seqlen_k=max_length_k,\n             dropout_p=dropout,\n             softmax_scale=softmax_scale,\n             causal=causal,\n@@ -299,3 +315,24 @@ def _flash_attention_forward(\n         )\n \n     return attn_output\n+\n+\n+class FlashAttentionKwargs(TypedDict, total=False):\n+    \"\"\"\n+    Keyword arguments for Flash Attention with Compile.\n+\n+    Attributes:\n+        cu_seq_lens_q (`torch.LongTensor`, *optional*)\n+            Gets cumlative sequence length for query state.\n+        cu_seq_lens_k (`torch.LongTensor`, *optional*)\n+            Gets cumlative sequence length for key state.\n+        max_length_q (`int`, *optional*):\n+            Maximum sequence length for query state.\n+        max_length_k (`int`, *optional*):\n+            Maximum sequence length for key state.\n+    \"\"\"\n+\n+    cu_seq_lens_q: Optional[torch.LongTensor]\n+    cu_seq_lens_k: Optional[torch.LongTensor]\n+    max_length_q: Optional[int]\n+    max_length_k: Optional[int]"
        },
        {
            "sha": "b215fb6561bf810fa2e392c7b7ae1d9591ad4e4f",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=65753d6065e4d6e79199c923494edbf0d6248fb1",
            "patch": "@@ -33,12 +33,14 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n     add_start_docstrings,\n@@ -832,6 +834,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -913,6 +916,7 @@ def forward(\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "6354e20e33fe8ca408edf826dec75d095bfb77ac",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=65753d6065e4d6e79199c923494edbf0d6248fb1",
            "patch": "@@ -38,6 +38,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n+    add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n@@ -51,7 +52,11 @@\n if is_flash_attn_2_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n-from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n+from ...processing_utils import Unpack\n+\n+\n+_CHECKPOINT_FOR_DOC = \"dummy\"\n \n \n class GlmRMSNorm(nn.Module):\n@@ -736,6 +741,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -817,6 +823,7 @@ def forward(\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -1222,6 +1229,11 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(GLM_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "c26477fdc173b1dfda59533764afad16562defaf",
            "filename": "src/transformers/models/glm/modular_glm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py?ref=65753d6065e4d6e79199c923494edbf0d6248fb1",
            "patch": "@@ -46,6 +46,8 @@\n \n logger = logging.get_logger(__name__)\n \n+_CHECKPOINT_FOR_DOC = \"dummy\"\n+\n \n class GlmRMSNorm(Phi3RMSNorm):\n     pass"
        },
        {
            "sha": "4d95f01849d6784893202419c204582976795a1f",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 13,
            "deletions": 3,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=65753d6065e4d6e79199c923494edbf0d6248fb1",
            "patch": "@@ -29,7 +29,7 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -39,8 +39,10 @@\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n+    LossKwargs,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -422,6 +424,7 @@ def forward(\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if isinstance(past_key_value, StaticCache):\n             raise ValueError(\n@@ -506,6 +509,7 @@ def forward(\n             sliding_window=getattr(self, \"sliding_window\", None),\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n             is_causal=self.is_causal,\n+            **kwargs,\n         )\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n@@ -870,6 +874,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -951,6 +956,7 @@ def forward(\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -1102,6 +1108,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class LlamaForCausalLM(LlamaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n@@ -1148,7 +1157,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1198,6 +1207,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1211,7 +1221,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **loss_kwargs)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "4f3187d510fad19d255ae2601bc5e477351a512f",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=65753d6065e4d6e79199c923494edbf0d6248fb1",
            "patch": "@@ -815,7 +815,7 @@ def to(self, device: Union[str, \"torch.device\"]) -> \"BatchEncoding\":\n         # Otherwise it passes the casts down and casts the LongTensor containing the token idxs\n         # into a HalfTensor\n         if isinstance(device, str) or is_torch_device(device) or isinstance(device, int):\n-            self.data = {k: v.to(device=device) for k, v in self.data.items() if isinstance(v, torch.Tensor)}\n+            self.data = {k: v.to(device=device) if isinstance(v, torch.Tensor) else v for k, v in self.data.items()}\n         else:\n             logger.warning(f\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\")\n         return self"
        },
        {
            "sha": "2a10bcaa3c94124926e8e5d50bf21849e0ee0c6b",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=65753d6065e4d6e79199c923494edbf0d6248fb1",
            "patch": "@@ -37,6 +37,7 @@\n from .generic import (\n     ContextManagers,\n     ExplicitEnum,\n+    LossKwargs,\n     ModelOutput,\n     PaddingStrategy,\n     TensorType,"
        },
        {
            "sha": "26ec82b20fd40e9e4261c712b14a86bb8871db4c",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65753d6065e4d6e79199c923494edbf0d6248fb1/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=65753d6065e4d6e79199c923494edbf0d6248fb1",
            "patch": "@@ -24,7 +24,7 @@\n from dataclasses import fields, is_dataclass\n from enum import Enum\n from functools import partial, wraps\n-from typing import Any, ContextManager, Iterable, List, Optional, Tuple\n+from typing import Any, ContextManager, Iterable, List, Optional, Tuple, TypedDict\n \n import numpy as np\n from packaging import version\n@@ -854,3 +854,16 @@ def wrapper(*args, **kwargs):\n         return wrapper\n \n     return decorator\n+\n+\n+class LossKwargs(TypedDict, total=False):\n+    \"\"\"\n+    Keyword arguments to be passed to the loss function\n+\n+    Attributes:\n+        num_items_in_batch (`int`, *optional*):\n+            Number of items in the batch. It is recommended to pass it when\n+            you are doing gradient accumulation.\n+    \"\"\"\n+\n+    num_items_in_batch: Optional[int]"
        }
    ],
    "stats": {
        "total": 212,
        "additions": 192,
        "deletions": 20
    }
}