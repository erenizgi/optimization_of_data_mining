{
    "author": "Rocketknight1",
    "message": "Just import torch AdamW instead (#36177)\n\n* Just import torch AdamW instead\n\n* Update docs too\n\n* Make AdamW undocumented\n\n* make fixup\n\n* Add a basic wrapper class\n\n* Add it back to the docs\n\n* Just remove AdamW entirely\n\n* Remove some AdamW references\n\n* Drop AdamW from the public init\n\n* make fix-copies\n\n* Cleanup some references\n\n* make fixup\n\n* Delete lots of transformers.AdamW references\n\n* Remove extra references to adamw_hf",
    "sha": "9be4728af8bec48073ae841881d7f4e2ac3521d1",
    "files": [
        {
            "sha": "24c978e6fe3ced5eb39e1e3f0e1f956f36c83247",
            "filename": "docs/source/en/main_classes/optimizer_schedules.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/docs%2Fsource%2Fen%2Fmain_classes%2Foptimizer_schedules.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/docs%2Fsource%2Fen%2Fmain_classes%2Foptimizer_schedules.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Foptimizer_schedules.md?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -22,9 +22,6 @@ The `.optimization` module provides:\n - several schedules in the form of schedule objects that inherit from `_LRSchedule`:\n - a gradient accumulation class to accumulate the gradients of multiple batches\n \n-## AdamW (PyTorch)\n-\n-[[autodoc]] AdamW\n \n ## AdaFactor (PyTorch)\n "
        },
        {
            "sha": "cd6dada007cba5c54e2eca1119ec386767cf226e",
            "filename": "docs/source/ja/main_classes/optimizer_schedules.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/docs%2Fsource%2Fja%2Fmain_classes%2Foptimizer_schedules.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/docs%2Fsource%2Fja%2Fmain_classes%2Foptimizer_schedules.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Foptimizer_schedules.md?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -22,10 +22,6 @@ rendered properly in your Markdown viewer.\n - `_LRSchedule` から継承するスケジュール オブジェクトの形式のいくつかのスケジュール:\n - 複数のバッチの勾配を累積するための勾配累積クラス\n \n-## AdamW (PyTorch)\n-\n-[[autodoc]] AdamW\n-\n ## AdaFactor (PyTorch)\n \n [[autodoc]] Adafactor"
        },
        {
            "sha": "4fb45540abb9e09e6e7d14009a1047f92bd2477e",
            "filename": "docs/source/zh/main_classes/optimizer_schedules.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/docs%2Fsource%2Fzh%2Fmain_classes%2Foptimizer_schedules.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/docs%2Fsource%2Fzh%2Fmain_classes%2Foptimizer_schedules.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Foptimizer_schedules.md?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -22,10 +22,6 @@ rendered properly in your Markdown viewer.\n - 继承自 `_LRSchedule` 多个调度器：\n - 一个梯度累积类，用于累积多个批次的梯度\n \n-## AdamW (PyTorch)\n-\n-[[autodoc]] AdamW\n-\n ## AdaFactor (PyTorch)\n \n [[autodoc]] Adafactor"
        },
        {
            "sha": "7eb1e7831f63c0674f3b0c43cd71459010ee3c7e",
            "filename": "examples/legacy/pytorch-lightning/lightning_base.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -8,7 +8,6 @@\n from pytorch_lightning.utilities import rank_zero_info\n \n from transformers import (\n-    AdamW,\n     AutoConfig,\n     AutoModel,\n     AutoModelForPreTraining,\n@@ -20,6 +19,7 @@\n     AutoTokenizer,\n     PretrainedConfig,\n     PreTrainedTokenizer,\n+    is_torch_available,\n )\n from transformers.optimization import (\n     Adafactor,\n@@ -31,6 +31,10 @@\n from transformers.utils.versions import require_version\n \n \n+if is_torch_available():\n+    import torch\n+\n+\n logger = logging.getLogger(__name__)\n \n require_version(\"pytorch_lightning>=1.0.4\")\n@@ -146,7 +150,7 @@ def configure_optimizers(self):\n             )\n \n         else:\n-            optimizer = AdamW(\n+            optimizer = torch.optim.AdamW(\n                 optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon\n             )\n         self.opt = optimizer"
        },
        {
            "sha": "19d3c6f49e7a3e18dd451d28b0ae8ae29062da9c",
            "filename": "examples/legacy/question-answering/run_squad.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -32,7 +32,6 @@\n from transformers import (\n     MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n     WEIGHTS_NAME,\n-    AdamW,\n     AutoConfig,\n     AutoModelForQuestionAnswering,\n     AutoTokenizer,\n@@ -96,7 +95,7 @@ def train(args, train_dataset, model, tokenizer):\n         },\n         {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n     ]\n-    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n+    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n     scheduler = get_linear_schedule_with_warmup(\n         optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n     )"
        },
        {
            "sha": "4193cd1824d6dc6faab2be2265c95c159cdd35fb",
            "filename": "examples/legacy/run_openai_gpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Flegacy%2Frun_openai_gpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Flegacy%2Frun_openai_gpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_openai_gpt.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -43,7 +43,6 @@\n from transformers import (\n     CONFIG_NAME,\n     WEIGHTS_NAME,\n-    AdamW,\n     OpenAIGPTDoubleHeadsModel,\n     OpenAIGPTTokenizer,\n     get_linear_schedule_with_warmup,\n@@ -236,7 +235,7 @@ def tokenize_and_encode(obj):\n             },\n             {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n         ]\n-        optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n+        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n         scheduler = get_linear_schedule_with_warmup(\n             optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n         )"
        },
        {
            "sha": "8fca5507bc578b8e644f5a09c65f1882beaebdae",
            "filename": "examples/legacy/run_swag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Flegacy%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Flegacy%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_swag.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -34,7 +34,6 @@\n import transformers\n from transformers import (\n     WEIGHTS_NAME,\n-    AdamW,\n     AutoConfig,\n     AutoModelForMultipleChoice,\n     AutoTokenizer,\n@@ -298,7 +297,7 @@ def train(args, train_dataset, model, tokenizer):\n         },\n         {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n     ]\n-    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n+    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n     scheduler = get_linear_schedule_with_warmup(\n         optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n     )"
        },
        {
            "sha": "8f056ca8e15db7bf6a123aa0580427b8f62d927c",
            "filename": "examples/legacy/seq2seq/seq2seq_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -22,7 +22,6 @@\n from transformers.models.fsmt.configuration_fsmt import FSMTConfig\n from transformers.optimization import (\n     Adafactor,\n-    AdamW,\n     get_constant_schedule,\n     get_constant_schedule_with_warmup,\n     get_cosine_schedule_with_warmup,\n@@ -102,12 +101,11 @@ def create_optimizer_and_scheduler(self, num_training_steps: int):\n                     \"weight_decay\": 0.0,\n                 },\n             ]\n-            optimizer_cls = Adafactor if self.args.adafactor else AdamW\n             if self.args.adafactor:\n                 optimizer_cls = Adafactor\n                 optimizer_kwargs = {\"scale_parameter\": False, \"relative_step\": False}\n             else:\n-                optimizer_cls = AdamW\n+                optimizer_cls = torch.optim.AdamW\n                 optimizer_kwargs = {\n                     \"betas\": (self.args.adam_beta1, self.args.adam_beta2),\n                     \"eps\": self.args.adam_epsilon,"
        },
        {
            "sha": "f5a3de118e9a91655652343374c92f4ed46c5f1b",
            "filename": "examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search_no_trainer.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -41,7 +41,6 @@\n \n import transformers\n from transformers import (\n-    AdamW,\n     DataCollatorWithPadding,\n     EvalPrediction,\n     SchedulerType,\n@@ -767,7 +766,7 @@ def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n             \"weight_decay\": 0.0,\n         },\n     ]\n-    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n+    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n \n     # Scheduler and math around the number of training steps.\n     overrode_max_train_steps = False"
        },
        {
            "sha": "5c6c3f62c79251cabdf5a1bfe0cc6c2b887e78f7",
            "filename": "examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Fpytorch%2Fspeech-pretraining%2Frun_wav2vec2_pretraining_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/examples%2Fpytorch%2Fspeech-pretraining%2Frun_wav2vec2_pretraining_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-pretraining%2Frun_wav2vec2_pretraining_no_trainer.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -33,7 +33,6 @@\n \n import transformers\n from transformers import (\n-    AdamW,\n     SchedulerType,\n     Wav2Vec2Config,\n     Wav2Vec2FeatureExtractor,\n@@ -583,7 +582,7 @@ def prepare_dataset(batch):\n     )\n \n     # Optimizer\n-    optimizer = AdamW(\n+    optimizer = torch.optim.AdamW(\n         list(model.parameters()),\n         lr=args.learning_rate,\n         betas=[args.adam_beta1, args.adam_beta2],"
        },
        {
            "sha": "1e33af79ab59f8e04ec67ba2a0077dc14def1801",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -4111,7 +4111,6 @@\n     )\n     _import_structure[\"optimization\"] = [\n         \"Adafactor\",\n-        \"AdamW\",\n         \"get_constant_schedule\",\n         \"get_constant_schedule_with_warmup\",\n         \"get_cosine_schedule_with_warmup\",\n@@ -8758,7 +8757,6 @@\n         # Optimization\n         from .optimization import (\n             Adafactor,\n-            AdamW,\n             get_constant_schedule,\n             get_constant_schedule_with_warmup,\n             get_cosine_schedule_with_warmup,"
        },
        {
            "sha": "aa09abeeb461006ee0d6808ecae53a8d29c87b5f",
            "filename": "src/transformers/optimization.py",
            "status": "modified",
            "additions": 1,
            "deletions": 116,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/src%2Ftransformers%2Foptimization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/src%2Ftransformers%2Foptimization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Foptimization.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -17,10 +17,9 @@\n import math\n import warnings\n from functools import partial\n-from typing import Callable, Iterable, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n-from torch import nn\n from torch.optim import Optimizer\n from torch.optim.lr_scheduler import LambdaLR, ReduceLROnPlateau\n \n@@ -604,120 +603,6 @@ def scheduler_hook(param):\n     )\n \n \n-class AdamW(Optimizer):\n-    \"\"\"\n-    Implements Adam algorithm with weight decay fix as introduced in [Decoupled Weight Decay\n-    Regularization](https://arxiv.org/abs/1711.05101).\n-\n-    Parameters:\n-        params (`Iterable[nn.parameter.Parameter]`):\n-            Iterable of parameters to optimize or dictionaries defining parameter groups.\n-        lr (`float`, *optional*, defaults to 0.001):\n-            The learning rate to use.\n-        betas (`Tuple[float,float]`, *optional*, defaults to `(0.9, 0.999)`):\n-            Adam's betas parameters (b1, b2).\n-        eps (`float`, *optional*, defaults to 1e-06):\n-            Adam's epsilon for numerical stability.\n-        weight_decay (`float`, *optional*, defaults to 0.0):\n-            Decoupled weight decay to apply.\n-        correct_bias (`bool`, *optional*, defaults to `True`):\n-            Whether or not to correct bias in Adam (for instance, in Bert TF repository they use `False`).\n-        no_deprecation_warning (`bool`, *optional*, defaults to `False`):\n-            A flag used to disable the deprecation warning (set to `True` to disable the warning).\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        params: Iterable[nn.parameter.Parameter],\n-        lr: float = 1e-3,\n-        betas: Tuple[float, float] = (0.9, 0.999),\n-        eps: float = 1e-6,\n-        weight_decay: float = 0.0,\n-        correct_bias: bool = True,\n-        no_deprecation_warning: bool = False,\n-    ):\n-        if not no_deprecation_warning:\n-            warnings.warn(\n-                \"This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch\"\n-                \" implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this\"\n-                \" warning\",\n-                FutureWarning,\n-            )\n-        require_version(\"torch>=1.5.0\")  # add_ with alpha\n-        if lr < 0.0:\n-            raise ValueError(f\"Invalid learning rate: {lr} - should be >= 0.0\")\n-        if not 0.0 <= betas[0] < 1.0:\n-            raise ValueError(f\"Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)\")\n-        if not 0.0 <= betas[1] < 1.0:\n-            raise ValueError(f\"Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)\")\n-        if not 0.0 <= eps:\n-            raise ValueError(f\"Invalid epsilon value: {eps} - should be >= 0.0\")\n-        defaults = {\"lr\": lr, \"betas\": betas, \"eps\": eps, \"weight_decay\": weight_decay, \"correct_bias\": correct_bias}\n-        super().__init__(params, defaults)\n-\n-    @torch.no_grad()\n-    def step(self, closure: Callable = None):\n-        \"\"\"\n-        Performs a single optimization step.\n-\n-        Arguments:\n-            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n-        \"\"\"\n-        loss = None\n-        if closure is not None:\n-            loss = closure()\n-\n-        for group in self.param_groups:\n-            for p in group[\"params\"]:\n-                if p.grad is None:\n-                    continue\n-                grad = p.grad\n-                if grad.is_sparse:\n-                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n-\n-                state = self.state[p]\n-\n-                # State initialization\n-                if len(state) == 0:\n-                    state[\"step\"] = 0\n-                    # Exponential moving average of gradient values\n-                    state[\"exp_avg\"] = torch.zeros_like(p)\n-                    # Exponential moving average of squared gradient values\n-                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n-\n-                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n-                beta1, beta2 = group[\"betas\"]\n-\n-                state[\"step\"] += 1\n-\n-                # Decay the first and second moment running average coefficient\n-                # In-place operations to update the averages at the same time\n-                exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))\n-                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n-                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n-\n-                step_size = group[\"lr\"]\n-                if group[\"correct_bias\"]:  # No bias correction for Bert\n-                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n-                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n-                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n-\n-                p.addcdiv_(exp_avg, denom, value=-step_size)\n-\n-                # Just adding the square of the weights to the loss function is *not*\n-                # the correct way of using L2 regularization/weight decay with Adam,\n-                # since that will interact with the m and v parameters in strange ways.\n-                #\n-                # Instead we want to decay the weights in a manner that doesn't interact\n-                # with the m/v parameters. This is equivalent to adding the square\n-                # of the weights to the loss with plain (non-momentum) SGD.\n-                # Add weight decay at the end (fixed version)\n-                if group[\"weight_decay\"] > 0.0:\n-                    p.add_(p, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n-\n-        return loss\n-\n-\n class Adafactor(Optimizer):\n     \"\"\"\n     AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:"
        },
        {
            "sha": "03e06333a5882a4a302f5642f280c45bed567dc8",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -1421,11 +1421,6 @@ def optimizer_hook(param):\n         if args.optim == OptimizerNames.ADAFACTOR:\n             optimizer_cls = Adafactor\n             optimizer_kwargs.update({\"scale_parameter\": False, \"relative_step\": False})\n-        elif args.optim == OptimizerNames.ADAMW_HF:\n-            from .optimization import AdamW\n-\n-            optimizer_cls = AdamW\n-            optimizer_kwargs.update(adam_kwargs)\n         elif args.optim in [OptimizerNames.ADAMW_TORCH, OptimizerNames.ADAMW_TORCH_FUSED]:\n             from torch.optim import AdamW\n "
        },
        {
            "sha": "9ad08997d7f1268a755ae70b2a92332deca3780d",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -146,7 +146,6 @@ class OptimizerNames(ExplicitEnum):\n     Stores the acceptable string identifiers for optimizers.\n     \"\"\"\n \n-    ADAMW_HF = \"adamw_hf\"\n     ADAMW_TORCH = \"adamw_torch\"\n     ADAMW_TORCH_FUSED = \"adamw_torch_fused\"\n     ADAMW_TORCH_XLA = \"adamw_torch_xla\"\n@@ -628,7 +627,7 @@ class TrainingArguments:\n \n             The options should be separated by whitespaces.\n         optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"`):\n-            The optimizer to use, such as \"adamw_hf\", \"adamw_torch\", \"adamw_torch_fused\", \"adamw_apex_fused\", \"adamw_anyprecision\",\n+            The optimizer to use, such as \"adamw_torch\", \"adamw_torch_fused\", \"adamw_apex_fused\", \"adamw_anyprecision\",\n             \"adafactor\". See `OptimizerNames` in [training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)\n             for a full list of optimizers.\n         optim_args (`str`, *optional*):\n@@ -2986,7 +2985,7 @@ def set_optimizer(\n \n         Args:\n             name (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"`):\n-                The optimizer to use: `\"adamw_hf\"`, `\"adamw_torch\"`, `\"adamw_torch_fused\"`, `\"adamw_apex_fused\"`,\n+                The optimizer to use: `\"adamw_torch\"`, `\"adamw_torch_fused\"`, `\"adamw_apex_fused\"`,\n                 `\"adamw_anyprecision\"` or `\"adafactor\"`.\n             learning_rate (`float`, *optional*, defaults to 5e-5):\n                 The initial learning rate."
        },
        {
            "sha": "ceca4158f52116db0f4c9614747dadb7e0e67ca4",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -10856,13 +10856,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class AdamW(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n def get_constant_schedule(*args, **kwargs):\n     requires_backends(get_constant_schedule, [\"torch\"])\n "
        },
        {
            "sha": "83916cc58736df837e2b413e9187cf9f19634cae",
            "filename": "templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -535,7 +535,6 @@ def _mp_fn(index):\n from transformers import (\n     CONFIG_MAPPING,\n     MODEL_MAPPING,\n-    AdamW,\n     AutoConfig,\n     {{cookiecutter.model_class}},\n     AutoTokenizer,\n@@ -863,7 +862,7 @@ def tokenize_function(examples):\n             \"weight_decay\": 0.0,\n         },\n     ]\n-    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n+    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n \n     # Prepare everything with our `accelerator`.\n     model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare("
        },
        {
            "sha": "73b09c0627ce87af4427775de40a090e4a176e40",
            "filename": "tests/optimization/test_optimization.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/tests%2Foptimization%2Ftest_optimization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/tests%2Foptimization%2Ftest_optimization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Foptimization%2Ftest_optimization.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -28,7 +28,6 @@\n \n     from transformers import (\n         Adafactor,\n-        AdamW,\n         get_constant_schedule,\n         get_constant_schedule_with_warmup,\n         get_cosine_schedule_with_warmup,\n@@ -76,7 +75,7 @@ def test_adam_w(self):\n         target = torch.tensor([0.4, 0.2, -0.5])\n         criterion = nn.MSELoss()\n         # No warmup, constant schedule, no gradient clipping\n-        optimizer = AdamW(params=[w], lr=2e-1, weight_decay=0.0)\n+        optimizer = torch.optim.AdamW(params=[w], lr=2e-1, weight_decay=0.0)\n         for _ in range(100):\n             loss = criterion(w, target)\n             loss.backward()\n@@ -114,7 +113,7 @@ def test_adafactor(self):\n @require_torch\n class ScheduleInitTest(unittest.TestCase):\n     m = nn.Linear(50, 50) if is_torch_available() else None\n-    optimizer = AdamW(m.parameters(), lr=10.0) if is_torch_available() else None\n+    optimizer = torch.optim.AdamW(m.parameters(), lr=10.0) if is_torch_available() else None\n     num_steps = 10\n \n     def assertListAlmostEqual(self, list1, list2, tol, msg=None):"
        },
        {
            "sha": "e0635048a1bff3289d0eb8dec11f27b6c60c614c",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/9be4728af8bec48073ae841881d7f4e2ac3521d1/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9be4728af8bec48073ae841881d7f4e2ac3521d1/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=9be4728af8bec48073ae841881d7f4e2ac3521d1",
            "patch": "@@ -5375,16 +5375,6 @@ def hp_name(trial):\n     }\n \n     optim_test_params = [\n-        (\n-            OptimizerNames.ADAMW_HF,\n-            transformers.optimization.AdamW,\n-            default_adam_kwargs,\n-        ),\n-        (\n-            OptimizerNames.ADAMW_HF.value,\n-            transformers.optimization.AdamW,\n-            default_adam_kwargs,\n-        ),\n         (\n             OptimizerNames.ADAMW_TORCH,\n             torch.optim.AdamW,"
        }
    ],
    "stats": {
        "total": 192,
        "additions": 18,
        "deletions": 174
    }
}