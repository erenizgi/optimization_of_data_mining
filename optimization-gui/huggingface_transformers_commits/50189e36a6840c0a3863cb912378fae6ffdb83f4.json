{
    "author": "jmtzt",
    "message": "Add I-JEPA (#33125)\n\n* first draft\n\n* add IJepaEmbeddings class\n\n* fix copy-from for IJepa model\n\n* add weight conversion script\n\n* update attention class names in IJepa model\n\n* style changes\n\n* Add push_to_hub option to convert_ijepa_checkpoint function\n\n* add initial tests for I-JEPA\n\n* minor style changes to conversion script\n\n* make fixup related\n\n* rename conversion script\n\n* Add I-JEPA to sdpa docs\n\n* minor fixes\n\n* adjust conversion script\n\n* update conversion script\n\n* adjust sdpa docs\n\n* [run_slow] ijepa\n\n* [run-slow] ijepa\n\n* [run-slow] ijepa\n\n* [run-slow] ijepa\n\n* [run-slow] ijepa\n\n* [run-slow] ijepa\n\n* formatting issues\n\n* adjust modeling to modular code\n\n* add IJepaModel to objects to ignore in docstring checks\n\n* [run-slow] ijepa\n\n* fix formatting issues\n\n* add usage instruction snippet to docs\n\n* change pos encoding, add checkpoint for doc\n\n* add verify logits for all models\n\n* [run-slow] ijepa\n\n* update docs to include image feature extraction instructions\n\n* remove pooling layer from IJepaModel in image classification class\n\n* [run-slow] ijepa\n\n* remove pooling layer from IJepaModel constructor\n\n* update docs\n\n* [run-slow] ijepa\n\n* [run-slow] ijepa\n\n* small changes\n\n* [run-slow] ijepa\n\n* style adjustments\n\n* update copyright in init file\n\n* adjust modular ijepa\n\n* [run-slow] ijepa",
    "sha": "50189e36a6840c0a3863cb912378fae6ffdb83f4",
    "files": [
        {
            "sha": "3521d4ccfed8943159439c8071719749ac918609",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -657,6 +657,8 @@\n         title: GLPN\n       - local: model_doc/hiera\n         title: Hiera\n+      - local: model_doc/ijepa\n+        title: I-JEPA\n       - local: model_doc/imagegpt\n         title: ImageGPT\n       - local: model_doc/levit"
        },
        {
            "sha": "3cad4e663f23fd2c6be7e66cf2585c5454ed8e57",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -168,6 +168,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                         [Hiera](model_doc/hiera)                         |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                        [Hubert](model_doc/hubert)                        |       ‚úÖ        |         ‚úÖ         |      ‚ùå      |\n |                        [I-BERT](model_doc/ibert)                         |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n+|                        [I-JEPA](model_doc/ijepa)                         |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                       [IDEFICS](model_doc/idefics)                       |       ‚úÖ        |         ‚úÖ         |      ‚ùå      |\n |                      [Idefics2](model_doc/idefics2)                      |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                      [Idefics3](model_doc/idefics3)                      |       ‚úÖ        |         ‚ùå         |      ‚ùå      |"
        },
        {
            "sha": "9a0cd368a8188f6c7ea88fca001b5163e14190d9",
            "filename": "docs/source/en/model_doc/ijepa.md",
            "status": "added",
            "additions": 78,
            "deletions": 0,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -0,0 +1,78 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# I-JEPA\n+\n+## Overview\n+\n+The I-JEPA model was proposed in [Image-based Joint-Embedding Predictive Architecture](https://arxiv.org/pdf/2301.08243.pdf) by Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas.\n+I-JEPA is a self-supervised learning method that predicts the representations of one part of an image based on other parts of the same image. This approach focuses on learning semantic features without relying on pre-defined invariances from hand-crafted data transformations, which can bias specific tasks, or on filling in pixel-level details, which often leads to less meaningful representations.\n+\n+The abstract from the paper is the following:\n+\n+This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image- based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample tar- get blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transform- ers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.\n+\n+This model was contributed by [jmtzt](https://huggingface.co/jmtzt).\n+The original code can be found [here](https://github.com/facebookresearch/ijepa).\n+\n+## How to use\n+\n+Here is how to use this model for image feature extraction:\n+\n+```python\n+import requests\n+import torch\n+from PIL import Image\n+from torch.nn.functional import cosine_similarity\n+\n+from transformers import AutoModel, AutoProcessor\n+\n+url_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+url_2 = \"http://images.cocodataset.org/val2017/000000219578.jpg\"\n+image_1 = Image.open(requests.get(url_1, stream=True).raw)\n+image_2 = Image.open(requests.get(url_2, stream=True).raw)\n+\n+model_id = \"jmtzt/ijepa_vith14_1k\"\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = AutoModel.from_pretrained(model_id)\n+\n+@torch.no_grad()\n+def infer(image):\n+    inputs = processor(image, return_tensors=\"pt\")\n+    outputs = model(**inputs)\n+    return outputs.last_hidden_state.mean(dim=1)\n+\n+\n+embed_1 = infer(image_1)\n+embed_2 = infer(image_2)\n+\n+similarity = cosine_similarity(embed_1, embed_2)\n+print(similarity)\n+```\n+\n+## IJepaConfig\n+\n+[[autodoc]] IJepaConfig\n+\n+## IJepaModel\n+\n+[[autodoc]] IJepaModel\n+    - forward\n+\n+## IJepaForImageClassification\n+\n+[[autodoc]] IJepaForImageClassification\n+    - forward"
        },
        {
            "sha": "ec8dea2735b531fa952c3440e55d0882937756a9",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -235,14 +235,15 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)\n * [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)\n * [Gemma2](https://huggingface.co/docs/transformers/model_doc/gemma2#transformers.Gemma2Model)\n+* [Granite](https://huggingface.co/docs/transformers/model_doc/granite#transformers.GraniteModel)\n * [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)\n * [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)\n * [GPTNeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXModel)\n * [Hubert](https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertModel)\n * [Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)\n * [Idefics2](https://huggingface.co/docs/transformers/model_doc/idefics2#transformers.Idefics2Model)\n * [Idefics3](https://huggingface.co/docs/transformers/model_doc/idefics3#transformers.Idefics3Model)\n-* [Granite](https://huggingface.co/docs/transformers/model_doc/granite#transformers.GraniteModel)\n+* [I-JEPA](https://huggingface.co/docs/transformers/model_doc/ijepa#transformers.IJepaModel)\n * [GraniteMoe](https://huggingface.co/docs/transformers/model_doc/granitemoe#transformers.GraniteMoeModel)\n * [JetMoe](https://huggingface.co/docs/transformers/model_doc/jetmoe#transformers.JetMoeModel)\n * [Jamba](https://huggingface.co/docs/transformers/model_doc/jamba#transformers.JambaModel)"
        },
        {
            "sha": "625936a45869c86d52e52c1ac7470eb9029cfd49",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -485,6 +485,7 @@\n     \"models.idefics\": [\"IdeficsConfig\"],\n     \"models.idefics2\": [\"Idefics2Config\"],\n     \"models.idefics3\": [\"Idefics3Config\"],\n+    \"models.ijepa\": [\"IJepaConfig\"],\n     \"models.imagegpt\": [\"ImageGPTConfig\"],\n     \"models.informer\": [\"InformerConfig\"],\n     \"models.instructblip\": [\n@@ -2462,6 +2463,13 @@\n             \"Idefics3Processor\",\n         ]\n     )\n+    _import_structure[\"models.ijepa\"].extend(\n+        [\n+            \"IJepaForImageClassification\",\n+            \"IJepaModel\",\n+            \"IJepaPreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.imagegpt\"].extend(\n         [\n             \"ImageGPTForCausalImageModeling\",\n@@ -5368,6 +5376,7 @@\n     )\n     from .models.idefics2 import Idefics2Config\n     from .models.idefics3 import Idefics3Config\n+    from .models.ijepa import IJepaConfig\n     from .models.imagegpt import ImageGPTConfig\n     from .models.informer import InformerConfig\n     from .models.instructblip import (\n@@ -7181,6 +7190,11 @@\n             Idefics3PreTrainedModel,\n             Idefics3Processor,\n         )\n+        from .models.ijepa import (\n+            IJepaForImageClassification,\n+            IJepaModel,\n+            IJepaPreTrainedModel,\n+        )\n         from .models.imagegpt import (\n             ImageGPTForCausalImageModeling,\n             ImageGPTForImageClassification,"
        },
        {
            "sha": "e957d802d80e712b6067e41943bb01894098aa46",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -117,6 +117,7 @@\n     idefics,\n     idefics2,\n     idefics3,\n+    ijepa,\n     imagegpt,\n     informer,\n     instructblip,"
        },
        {
            "sha": "c1f2d689df7095f941d298a01a2aad85f955c7e4",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -135,6 +135,7 @@\n         (\"idefics\", \"IdeficsConfig\"),\n         (\"idefics2\", \"Idefics2Config\"),\n         (\"idefics3\", \"Idefics3Config\"),\n+        (\"ijepa\", \"IJepaConfig\"),\n         (\"imagegpt\", \"ImageGPTConfig\"),\n         (\"informer\", \"InformerConfig\"),\n         (\"instructblip\", \"InstructBlipConfig\"),\n@@ -440,6 +441,7 @@\n         (\"idefics\", \"IDEFICS\"),\n         (\"idefics2\", \"Idefics2\"),\n         (\"idefics3\", \"Idefics3\"),\n+        (\"ijepa\", \"I-JEPA\"),\n         (\"imagegpt\", \"ImageGPT\"),\n         (\"informer\", \"Informer\"),\n         (\"instructblip\", \"InstructBLIP\"),"
        },
        {
            "sha": "e19c8efd205552b3fadacd3c37a8a19bf9c7ae69",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -90,6 +90,7 @@\n             (\"idefics\", (\"IdeficsImageProcessor\",)),\n             (\"idefics2\", (\"Idefics2ImageProcessor\",)),\n             (\"idefics3\", (\"Idefics3ImageProcessor\",)),\n+            (\"ijepa\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"imagegpt\", (\"ImageGPTImageProcessor\",)),\n             (\"instructblip\", (\"BlipImageProcessor\",)),\n             (\"instructblipvideo\", (\"InstructBlipVideoImageProcessor\",)),\n@@ -433,7 +434,9 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n         if image_processor_class is None and image_processor_auto_map is None:\n             if not isinstance(config, PretrainedConfig):\n                 config = AutoConfig.from_pretrained(\n-                    pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n+                    pretrained_model_name_or_path,\n+                    trust_remote_code=trust_remote_code,\n+                    **kwargs,\n                 )\n             # It could be in `config.image_processor_type``\n             image_processor_class = getattr(config, \"image_processor_type\", None)"
        },
        {
            "sha": "7a7cd9d475884caceec0bc1d9fa67dec096010ee",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -132,6 +132,7 @@\n         (\"idefics\", \"IdeficsModel\"),\n         (\"idefics2\", \"Idefics2Model\"),\n         (\"idefics3\", \"Idefics3Model\"),\n+        (\"ijepa\", \"IJepaModel\"),\n         (\"imagegpt\", \"ImageGPTModel\"),\n         (\"informer\", \"InformerModel\"),\n         (\"jamba\", \"JambaModel\"),\n@@ -578,6 +579,7 @@\n         (\"focalnet\", \"FocalNetModel\"),\n         (\"glpn\", \"GLPNModel\"),\n         (\"hiera\", \"HieraModel\"),\n+        (\"ijepa\", \"IJepaModel\"),\n         (\"imagegpt\", \"ImageGPTModel\"),\n         (\"levit\", \"LevitModel\"),\n         (\"mllama\", \"MllamaVisionModel\"),\n@@ -655,6 +657,7 @@\n         (\"efficientnet\", \"EfficientNetForImageClassification\"),\n         (\"focalnet\", \"FocalNetForImageClassification\"),\n         (\"hiera\", \"HieraForImageClassification\"),\n+        (\"ijepa\", \"IJepaForImageClassification\"),\n         (\"imagegpt\", \"ImageGPTForImageClassification\"),\n         (\n             \"levit\","
        },
        {
            "sha": "efc8c90b17628d1fb421aadeee105254360c60c1",
            "filename": "src/transformers/models/ijepa/__init__.py",
            "status": "added",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fijepa%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fijepa%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2F__init__.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -0,0 +1,55 @@\n+# Copyright 2023 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    is_torch_available,\n+)\n+\n+\n+_import_structure = {\"configuration_ijepa\": [\"IJepaConfig\"]}\n+\n+try:\n+    if not is_torch_available():\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    pass\n+else:\n+    _import_structure[\"modeling_ijepa\"] = [\n+        \"IJepaForImageClassification\",\n+        \"IJepaModel\",\n+        \"IJepaPreTrainedModel\",\n+    ]\n+\n+if TYPE_CHECKING:\n+    from .configuration_ijepa import IJepaConfig\n+\n+    try:\n+        if not is_torch_available():\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        pass\n+    else:\n+        from .modeling_ijepa import (\n+            IJepaForImageClassification,\n+            IJepaModel,\n+            IJepaPreTrainedModel,\n+        )\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)"
        },
        {
            "sha": "26378e6e81d9cef6f0e933c45358470772d15b11",
            "filename": "src/transformers/models/ijepa/configuration_ijepa.py",
            "status": "added",
            "additions": 108,
            "deletions": 0,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fijepa%2Fconfiguration_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fijepa%2Fconfiguration_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fconfiguration_ijepa.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -0,0 +1,108 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"I-JEPA model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+\n+\n+class IJepaConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`IJepaModel`]. It is used to instantiate an IJEPA\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the I-JEPA\n+    [google/ijepa-base-patch16-224](https://huggingface.co/google/ijepa-base-patch16-224) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        intermediate_size (`int`, *optional*, defaults to 3072):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n+        hidden_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n+        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n+            The epsilon used by the layer normalization layers.\n+        image_size (`int`, *optional*, defaults to 224):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        qkv_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the queries, keys and values.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import IJepaConfig, IJepaModel\n+\n+    >>> # Initializing a IJEPA ijepa-base-patch16-224 style configuration\n+    >>> configuration = IJepaConfig()\n+\n+    >>> # Initializing a model (with random weights) from the ijepa-base-patch16-224 style configuration\n+    >>> model = IJepaModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"ijepa\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=768,\n+        num_hidden_layers=12,\n+        num_attention_heads=12,\n+        intermediate_size=3072,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.0,\n+        attention_probs_dropout_prob=0.0,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-12,\n+        image_size=224,\n+        patch_size=16,\n+        num_channels=3,\n+        qkv_bias=True,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.initializer_range = initializer_range\n+        self.layer_norm_eps = layer_norm_eps\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.qkv_bias = qkv_bias"
        },
        {
            "sha": "5c15a72ff88847ab0cc25571ee37a916e180e62b",
            "filename": "src/transformers/models/ijepa/convert_ijepa_to_hf.py",
            "status": "added",
            "additions": 267,
            "deletions": 0,
            "changes": 267,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fijepa%2Fconvert_ijepa_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fijepa%2Fconvert_ijepa_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fconvert_ijepa_to_hf.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -0,0 +1,267 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Convert IJEPA checkpoints from the original repository.\n+\n+URL: https://github.com/facebookresearch/ijepa\n+\"\"\"\n+\n+import argparse\n+import gc\n+import re\n+from pathlib import Path\n+\n+import requests\n+import torch\n+from PIL import Image\n+\n+from transformers import (\n+    IJepaConfig,\n+    IJepaModel,\n+    ViTImageProcessor,\n+)\n+from transformers.utils import logging\n+\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    # Projection layer + position embeddings\n+    r\"pos_embed\":                               r\"embeddings.position_embeddings\",\n+    r\"patch_embed.proj.weight\":                 r\"embeddings.patch_embeddings.projection.weight\",\n+    r\"patch_embed.proj.bias\":                   r\"embeddings.patch_embeddings.projection.bias\",\n+\n+    # Encoder layers: Layernorms, Attention, Feedforward layers\n+    r\"blocks.(\\d+).norm1.weight\":               r\"encoder.layer.\\1.layernorm_before.weight\",\n+    r\"blocks.(\\d+).norm1.bias\":                 r\"encoder.layer.\\1.layernorm_before.bias\",\n+    r\"blocks.(\\d+).attn.proj.weight\":           r\"encoder.layer.\\1.attention.output.dense.weight\",\n+    r\"blocks.(\\d+).attn.proj.bias\":             r\"encoder.layer.\\1.attention.output.dense.bias\",\n+    r\"blocks.(\\d+).norm2.weight\":               r\"encoder.layer.\\1.layernorm_after.weight\",\n+    r\"blocks.(\\d+).norm2.bias\":                 r\"encoder.layer.\\1.layernorm_after.bias\",\n+    r\"blocks.(\\d+).mlp.fc1.weight\":             r\"encoder.layer.\\1.intermediate.dense.weight\",\n+    r\"blocks.(\\d+).mlp.fc1.bias\":               r\"encoder.layer.\\1.intermediate.dense.bias\",\n+    r\"blocks.(\\d+).mlp.fc2.weight\":             r\"encoder.layer.\\1.output.dense.weight\",\n+    r\"blocks.(\\d+).mlp.fc2.bias\":               r\"encoder.layer.\\1.output.dense.bias\",\n+\n+    # Layernorm + pooler\n+    r\"norm.weight\":                             r\"layernorm.weight\",\n+    r\"norm.bias\":                               r\"layernorm.bias\",\n+}\n+# fmt: on\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+    \"\"\"\n+    Converts old keys to new keys using the mapping and dynamically removes the 'ijepa.' prefix if necessary.\n+\n+    Args:\n+        state_dict_keys (dict): The keys from the state_dict to convert.\n+\n+    Returns:\n+        dict: A mapping from old keys to new keys.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+\n+        # Apply regex-based mapping\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # Skip the key\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+\n+    return output_dict\n+\n+\n+# we split up the matrix of each encoder layer into queries, keys and values\n+def read_in_q_k_v(state_dict, config):\n+    for i in range(config.num_hidden_layers):\n+        # read in weights + bias of input projection layer (in timm, this is a single matrix + bias)\n+        in_proj_weight = state_dict.pop(f\"blocks.{i}.attn.qkv.weight\")\n+        in_proj_bias = state_dict.pop(f\"blocks.{i}.attn.qkv.bias\")\n+        # next, add query, keys and values (in that order) to the state dict\n+        state_dict[f\"encoder.layer.{i}.attention.attention.query.weight\"] = in_proj_weight[: config.hidden_size, :]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.query.bias\"] = in_proj_bias[: config.hidden_size]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.key.weight\"] = in_proj_weight[\n+            config.hidden_size : config.hidden_size * 2, :\n+        ]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.key.bias\"] = in_proj_bias[\n+            config.hidden_size : config.hidden_size * 2\n+        ]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.value.weight\"] = in_proj_weight[-config.hidden_size :, :]\n+        state_dict[f\"encoder.layer.{i}.attention.attention.value.bias\"] = in_proj_bias[-config.hidden_size :]\n+\n+\n+def rename_key(dct, old, new):\n+    val = dct.pop(old)\n+    dct[new] = val\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    im = Image.open(requests.get(url, stream=True).raw)\n+    return im\n+\n+\n+def get_ijepa_config(model_name):\n+    patch_size = int(model_name.split(\"_\")[1][4:])\n+    config = IJepaConfig(patch_size=patch_size)\n+    if \"vith\" in model_name:\n+        config.hidden_size = 1280\n+        config.num_hidden_layers = 32\n+        config.num_attention_heads = 16\n+        config.layer_norm_eps = 1e-6\n+        config.mlp_ratio = 4\n+        config.intermediate_size = 5120\n+        if model_name == \"ijepa_vith16_1k\":\n+            config.image_size = 448\n+    elif \"vitg\" in model_name:\n+        config.hidden_size = 1408\n+        config.num_hidden_layers = 40\n+        config.num_attention_heads = 16\n+        config.layer_norm_eps = 1e-6\n+        config.mlp_ratio = 48 / 11\n+        config.intermediate_size = 6144\n+    else:\n+        raise ValueError(\"Model not supported, only supports huge and giant models.\")\n+    return config\n+\n+\n+@torch.no_grad()\n+def write_model(model_name, output_dir, safe_serialization, push_to_hub, verify_logits):\n+    \"\"\"\n+    Copy/paste/tweak model's weights to our IJEPA structure.\n+    \"\"\"\n+\n+    # define default IJEPA configuration\n+    config = get_ijepa_config(model_name)\n+\n+    checkpoint_mapping = {\n+        \"ijepa_vith14_1k\": \"https://dl.fbaipublicfiles.com/ijepa/IN1K-vit.h.14-300e.pth.tar\",\n+        \"ijepa_vith14_22k\": \"https://dl.fbaipublicfiles.com/ijepa/IN22K-vit.h.14-900e.pth.tar\",\n+        \"ijepa_vith16_1k\": \"https://dl.fbaipublicfiles.com/ijepa/IN1K-vit.h.16-448px-300e.pth.tar\",\n+        \"ijepa_vitg16_22k\": \"https://dl.fbaipublicfiles.com/ijepa/IN22K-vit.g.16-600e.pth.tar\",\n+    }\n+\n+    # Load original checkpoint\n+    checkpoint_url = checkpoint_mapping[model_name]\n+    original_state_dict = torch.hub.load_state_dict_from_url(checkpoint_url, map_location=\"cpu\")[\"encoder\"]\n+    original_state_dict = {k.replace(\"module.\", \"\"): v for k, v in original_state_dict.items()}\n+\n+    # Rename keys\n+    state_dict = original_state_dict.copy()\n+    new_keys = convert_old_keys_to_new_keys(state_dict.keys())\n+    for old_key, new_key in new_keys.items():\n+        rename_key(state_dict, old_key, new_key)\n+    read_in_q_k_v(state_dict, config)\n+\n+    # load HuggingFace model\n+    model = IJepaModel(config, add_pooling_layer=False).eval()\n+    model.load_state_dict(state_dict)\n+    size = {\"height\": config.image_size, \"width\": config.image_size}\n+    image_processor = ViTImageProcessor(size=size)\n+\n+    if verify_logits:\n+        # Check outputs on an image, prepared by ViTImageProcessor\n+        encoding = image_processor(images=prepare_img(), return_tensors=\"pt\")\n+        pixel_values = encoding[\"pixel_values\"]\n+        with torch.no_grad():\n+            outputs = model(pixel_values)\n+\n+        expected_slices = {\n+            \"ijepa_vith14_1k\": torch.Tensor(\n+                [[-0.0621, -0.0054, -2.7513], [-0.1952, 0.0909, -3.9536], [0.0942, -0.0331, -1.2833]]\n+            ),\n+            \"ijepa_vith14_22k\": torch.Tensor(\n+                [[0.0358, -0.0045, -0.2154], [0.0418, -0.0246, 0.0108], [0.2529, -0.0345, -0.0246]]\n+            ),\n+            \"ijepa_vith16_1k\": torch.Tensor(\n+                [[0.5145, -0.1259, 0.0615], [0.1132, 0.0028, -0.0496], [1.1586, -0.0056, -0.0387]]\n+            ),\n+            \"ijepa_vitg16_22k\": torch.Tensor(\n+                [[0.0512, -0.0510, -0.0649], [0.1972, 0.0380, -0.0790], [0.1667, -0.0834, -0.1240]]\n+            ),\n+        }\n+\n+        assert torch.allclose(\n+            expected_slices[model_name],\n+            outputs.last_hidden_state[0, :3, :3],\n+            atol=1e-4,\n+        )\n+\n+    if output_dir:\n+        Path(output_dir).mkdir(exist_ok=True)\n+        print(f\"Saving model {model_name} to {output_dir}\")\n+        image_processor.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+        model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+\n+    if push_to_hub:\n+        image_processor.push_to_hub(repo_id=f\"jmtzt/{model_name}\", safe_serialization=safe_serialization)\n+        model.push_to_hub(repo_id=f\"jmtzt/{model_name}\", safe_serialization=safe_serialization)\n+\n+    if output_dir:\n+        del model, state_dict\n+        gc.collect()\n+        print(\"Reloading the model to check if it's saved correctly.\")\n+        IJepaModel.from_pretrained(output_dir, device_map=\"auto\")\n+        print(\"Model reloaded successfully.\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    # Required parameters\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"ijepa_vith14_1k\",\n+        type=str,\n+        choices=[\n+            \"ijepa_vith14_1k\",\n+            \"ijepa_vith14_22k\",\n+            \"ijepa_vith16_1k\",\n+            \"ijepa_vitg16_22k\",\n+        ],\n+        help=\"Name of the model you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        default=None,\n+        type=str,\n+        help=\"Path to the output PyTorch model directory.\",\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether or not to push the model to the ü§ó Hub.\",\n+    )\n+    parser.add_argument(\n+        \"--verify_logits\", action=\"store_false\", help=\"Whether or not to verify logits after conversion.\"\n+    )\n+\n+    parser.set_defaults()\n+    args = parser.parse_args()\n+    write_model(args.model_name, args.output_dir, args.safe_serialization, args.push_to_hub, args.verify_logits)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "df254455bad5ab6f7b87e3c24370d04a94b029b4",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "added",
            "additions": 751,
            "deletions": 0,
            "changes": 751,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -0,0 +1,751 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/ijepa/modular_ijepa.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_ijepa.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+import collections.abc\n+import math\n+from typing import Dict, List, Optional, Set, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...activations import ACT2FN\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    torch_int,\n+)\n+from .configuration_ijepa import IJepaConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+_CHECKPOINT_FOR_DOC = \"facebook/ijepa_vith14_1k\"\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"IJepaConfig\"\n+\n+\n+class IJepaPatchEmbeddings(nn.Module):\n+    \"\"\"\n+    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial\n+    `hidden_states` (patch embeddings) of shape `(batch_size, seq_length, hidden_size)` to be consumed by a\n+    Transformer.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        image_size, patch_size = config.image_size, config.patch_size\n+        num_channels, hidden_size = config.num_channels, config.hidden_size\n+\n+        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n+        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.num_patches = num_patches\n+\n+        self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n+\n+    def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n+        batch_size, num_channels, height, width = pixel_values.shape\n+        if num_channels != self.num_channels:\n+            raise ValueError(\n+                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n+                f\" Expected {self.num_channels} but got {num_channels}.\"\n+            )\n+        if not interpolate_pos_encoding:\n+            if height != self.image_size[0] or width != self.image_size[1]:\n+                raise ValueError(\n+                    f\"Input image size ({height}*{width}) doesn't match model\"\n+                    f\" ({self.image_size[0]}*{self.image_size[1]}).\"\n+                )\n+        embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)\n+        return embeddings\n+\n+\n+class IJepaEmbeddings(nn.Module):\n+    \"\"\"\n+    Construct the CLS token, position and patch embeddings. Optionally, also the mask token.\n+    \"\"\"\n+\n+    def __init__(self, config: IJepaConfig, use_mask_token: bool = False) -> None:\n+        super().__init__()\n+        self.mask_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size)) if use_mask_token else None\n+        self.patch_embeddings = IJepaPatchEmbeddings(config)\n+        num_patches = self.patch_embeddings.num_patches\n+        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches, config.hidden_size))\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.patch_size = config.patch_size\n+        self.config = config\n+\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1]\n+        num_positions = self.position_embeddings.shape[1]\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        patch_pos_embed = self.position_embeddings\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return patch_pos_embed\n+\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        bool_masked_pos: Optional[torch.BoolTensor] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        embeddings = self.patch_embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n+\n+        if bool_masked_pos is not None:\n+            seq_length = embeddings.shape[1]\n+            mask_tokens = self.mask_token.expand(batch_size, seq_length, -1)\n+            # replace the masked visual tokens by mask_tokens\n+            mask = bool_masked_pos.unsqueeze(-1).type_as(mask_tokens)\n+            embeddings = embeddings * (1.0 - mask) + mask_tokens * mask\n+\n+        # add positional encoding to each token\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            embeddings = embeddings + self.position_embeddings\n+\n+        embeddings = self.dropout(embeddings)\n+\n+        return embeddings\n+\n+\n+class IJepaPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = IJepaConfig\n+    base_model_prefix = \"ijepa\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"IJepaEmbeddings\", \"IJepaLayer\"]\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n+            # `trunc_normal_cpu` not implemented in `half` issues\n+            module.weight.data = nn.init.trunc_normal_(\n+                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n+            ).to(module.weight.dtype)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, IJepaEmbeddings):\n+            module.position_embeddings.data = nn.init.trunc_normal_(\n+                module.position_embeddings.data.to(torch.float32),\n+                mean=0.0,\n+                std=self.config.initializer_range,\n+            ).to(module.position_embeddings.dtype)\n+\n+\n+class IJepaSelfAttention(nn.Module):\n+    def __init__(self, config: IJepaConfig) -> None:\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n+                f\"heads {config.num_attention_heads}.\"\n+            )\n+\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n+\n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+\n+    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n+        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n+        x = x.view(new_x_shape)\n+        return x.permute(0, 2, 1, 3)\n+\n+    def forward(\n+        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        mixed_query_layer = self.query(hidden_states)\n+\n+        key_layer = self.transpose_for_scores(self.key(hidden_states))\n+        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        query_layer = self.transpose_for_scores(mixed_query_layer)\n+\n+        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+\n+        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n+\n+        # Normalize the attention scores to probabilities.\n+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+\n+        # This is actually dropping out entire tokens to attend to, which might\n+        # seem a bit unusual, but is taken from the original Transformer paper.\n+        attention_probs = self.dropout(attention_probs)\n+\n+        # Mask heads if we want to\n+        if head_mask is not None:\n+            attention_probs = attention_probs * head_mask\n+\n+        context_layer = torch.matmul(attention_probs, value_layer)\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(new_context_layer_shape)\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        return outputs\n+\n+\n+class IJepaSdpaSelfAttention(IJepaSelfAttention):\n+    def __init__(self, config: IJepaConfig) -> None:\n+        super().__init__(config)\n+        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        if output_attentions or head_mask is not None:\n+            logger.warning_once(\n+                \"`IJepaSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n+                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n+                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                head_mask=head_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n+        mixed_query_layer = self.query(hidden_states)\n+\n+        key_layer = self.transpose_for_scores(self.key(hidden_states))\n+        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        query_layer = self.transpose_for_scores(mixed_query_layer)\n+\n+        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            head_mask,\n+            self.attention_probs_dropout_prob if self.training else 0.0,\n+            is_causal=False,\n+            scale=None,\n+        )\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(new_context_layer_shape)\n+\n+        return context_layer, None\n+\n+\n+class IJepaSelfOutput(nn.Module):\n+    \"\"\"\n+    The residual connection is defined in IJepaLayer instead of here (as is the case with other models), due to the\n+    layernorm applied before each block.\n+    \"\"\"\n+\n+    def __init__(self, config: IJepaConfig) -> None:\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class IJepaAttention(nn.Module):\n+    def __init__(self, config: IJepaConfig) -> None:\n+        super().__init__()\n+        self.attention = IJepaSelfAttention(config)\n+        self.output = IJepaSelfOutput(config)\n+        self.pruned_heads = set()\n+\n+    def prune_heads(self, heads: Set[int]) -> None:\n+        if len(heads) == 0:\n+            return\n+        heads, index = find_pruneable_heads_and_indices(\n+            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n+        )\n+\n+        # Prune linear layers\n+        self.attention.query = prune_linear_layer(self.attention.query, index)\n+        self.attention.key = prune_linear_layer(self.attention.key, index)\n+        self.attention.value = prune_linear_layer(self.attention.value, index)\n+        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n+\n+        # Update hyper params and store pruned heads\n+        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n+        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n+        self.pruned_heads = self.pruned_heads.union(heads)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n+\n+        attention_output = self.output(self_outputs[0], hidden_states)\n+\n+        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n+        return outputs\n+\n+\n+class IJepaSdpaAttention(IJepaAttention):\n+    def __init__(self, config: IJepaConfig) -> None:\n+        super().__init__(config)\n+        self.attention = IJepaSdpaSelfAttention(config)\n+\n+\n+class IJepaIntermediate(nn.Module):\n+    def __init__(self, config: IJepaConfig) -> None:\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n+        if isinstance(config.hidden_act, str):\n+            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n+        else:\n+            self.intermediate_act_fn = config.hidden_act\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.intermediate_act_fn(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class IJepaOutput(nn.Module):\n+    def __init__(self, config: IJepaConfig) -> None:\n+        super().__init__()\n+        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+\n+        hidden_states = hidden_states + input_tensor\n+\n+        return hidden_states\n+\n+\n+IJEPA_ATTENTION_CLASSES = {\n+    \"eager\": IJepaAttention,\n+    \"sdpa\": IJepaSdpaAttention,\n+}\n+\n+\n+class IJepaLayer(nn.Module):\n+    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n+\n+    def __init__(self, config: IJepaConfig) -> None:\n+        super().__init__()\n+        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n+        self.seq_len_dim = 1\n+        self.attention = IJEPA_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.intermediate = IJepaIntermediate(config)\n+        self.output = IJepaOutput(config)\n+        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        self_attention_outputs = self.attention(\n+            self.layernorm_before(hidden_states),  # in IJepa, layernorm is applied before self-attention\n+            head_mask,\n+            output_attentions=output_attentions,\n+        )\n+        attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+\n+        # first residual connection\n+        hidden_states = attention_output + hidden_states\n+\n+        # in IJepa, layernorm is also applied after self-attention\n+        layer_output = self.layernorm_after(hidden_states)\n+        layer_output = self.intermediate(layer_output)\n+\n+        # second residual connection is done here\n+        layer_output = self.output(layer_output, hidden_states)\n+\n+        outputs = (layer_output,) + outputs\n+\n+        return outputs\n+\n+\n+class IJepaEncoder(nn.Module):\n+    def __init__(self, config: IJepaConfig) -> None:\n+        super().__init__()\n+        self.config = config\n+        self.layer = nn.ModuleList([IJepaLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        output_hidden_states: bool = False,\n+        return_dict: bool = True,\n+    ) -> Union[tuple, BaseModelOutput]:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        for i, layer_module in enumerate(self.layer):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            layer_head_mask = head_mask[i] if head_mask is not None else None\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    layer_module.__call__,\n+                    hidden_states,\n+                    layer_head_mask,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+class IJepaPooler(nn.Module):\n+    def __init__(self, config: IJepaConfig):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.activation = nn.Tanh()\n+\n+    def forward(self, hidden_states):\n+        # We \"pool\" the model by simply taking the hidden state corresponding\n+        # to the first token.\n+        first_token_tensor = hidden_states[:, 0]\n+        pooled_output = self.dense(first_token_tensor)\n+        pooled_output = self.activation(pooled_output)\n+        return pooled_output\n+\n+\n+IJEPA_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See [`IJepaImageProcessor.__call__`]\n+            for details.\n+\n+        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n+            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        interpolate_pos_encoding (`bool`, *optional*):\n+            Whether to interpolate the pre-trained position encodings.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+_EXPECTED_OUTPUT_SHAPE = [1, 197, 768]\n+\n+\n+IJEPA_START_DOCSTRING = r\"\"\"\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n+    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`IJepaConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare IJepa Model transformer outputting raw hidden-states without any specific head on top.\",\n+    IJEPA_START_DOCSTRING,\n+)\n+class IJepaModel(IJepaPreTrainedModel):\n+    def __init__(self, config: IJepaConfig, add_pooling_layer: bool = False, use_mask_token: bool = False):\n+        super().__init__(config)\n+        self.config = config\n+        self.embeddings = IJepaEmbeddings(config, use_mask_token=use_mask_token)\n+        self.encoder = IJepaEncoder(config)\n+\n+        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.pooler = IJepaPooler(config) if add_pooling_layer else None\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> IJepaPatchEmbeddings:\n+        return self.embeddings.patch_embeddings\n+\n+    def _prune_heads(self, heads_to_prune: Dict[int, List[int]]) -> None:\n+        \"\"\"\n+        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n+        class PreTrainedModel\n+        \"\"\"\n+        for layer, heads in heads_to_prune.items():\n+            self.encoder.layer[layer].attention.prune_heads(heads)\n+\n+    @add_start_docstrings_to_model_forward(IJEPA_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=BaseModelOutputWithPooling,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"vision\",\n+        expected_output=_EXPECTED_OUTPUT_SHAPE,\n+    )\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        bool_masked_pos: Optional[torch.BoolTensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n+            Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        # Prepare head mask if needed\n+        # 1.0 in head_mask indicate we keep the head\n+        # attention_probs has shape bsz x n_heads x N x N\n+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n+        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+\n+        # TODO: maybe have a cleaner way to cast the input (from `ImageProcessor` side?)\n+        expected_dtype = self.embeddings.patch_embeddings.projection.weight.dtype\n+        if pixel_values.dtype != expected_dtype:\n+            pixel_values = pixel_values.to(expected_dtype)\n+\n+        embedding_output = self.embeddings(\n+            pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding\n+        )\n+\n+        encoder_outputs = self.encoder(\n+            embedding_output,\n+            head_mask=head_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        sequence_output = encoder_outputs[0]\n+        sequence_output = self.layernorm(sequence_output)\n+        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n+\n+        if not return_dict:\n+            head_outputs = (sequence_output, pooled_output) if pooled_output is not None else (sequence_output,)\n+            return head_outputs + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=sequence_output,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+# Image classification docstring\n+_IMAGE_CLASS_CHECKPOINT = \"google/ijepa-base-patch16-224\"\n+_IMAGE_CLASS_EXPECTED_OUTPUT = \"Egyptian cat\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    IJepa Model transformer with an image classification head on top (a linear layer on top of the final hidden states)\n+    e.g. for ImageNet.\n+\n+    <Tip>\n+\n+        Note that it's possible to fine-tune IJepa on higher resolution images than the ones it has been trained on, by\n+        setting `interpolate_pos_encoding` to `True` in the forward of the model. This will interpolate the pre-trained\n+        position embeddings to the higher resolution.\n+\n+    </Tip>\n+    \"\"\",\n+    IJEPA_START_DOCSTRING,\n+)\n+class IJepaForImageClassification(IJepaPreTrainedModel):\n+    def __init__(self, config: IJepaConfig) -> None:\n+        super().__init__(config)\n+\n+        self.num_labels = config.num_labels\n+        self.ijepa = IJepaModel(config, add_pooling_layer=False)\n+\n+        # Classifier head\n+        self.classifier = nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(IJEPA_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n+        output_type=ImageClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n+    )\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[tuple, ImageClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.ijepa(\n+            pixel_values,\n+            head_mask=head_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+            return_dict=return_dict,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        logits = self.classifier(sequence_output.mean(dim=1))\n+\n+        loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(logits.device)\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return ImageClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"IJepaPreTrainedModel\", \"IJepaModel\", \"IJepaForImageClassification\"]"
        },
        {
            "sha": "efbd71d91342fd2d7fcca79b7120249fc3e4390b",
            "filename": "src/transformers/models/ijepa/modular_ijepa.py",
            "status": "added",
            "additions": 255,
            "deletions": 0,
            "changes": 255,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -0,0 +1,255 @@\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from transformers.models.ijepa.configuration_ijepa import IJepaConfig\n+\n+from ...modeling_outputs import ImageClassifierOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_start_docstrings,\n+    torch_int,\n+)\n+from ..vit.modeling_vit import (\n+    ViTEmbeddings,\n+    ViTForImageClassification,\n+    ViTModel,\n+)\n+\n+\n+_CHECKPOINT_FOR_DOC = \"facebook/ijepa_vith14_1k\"\n+\n+\n+class IJepaEmbeddings(ViTEmbeddings):\n+    def __init__(self, config: IJepaConfig, use_mask_token: bool = False) -> None:\n+        super().__init__(config, use_mask_token)\n+        # Remove cls_token from IJepaEmbeddings, as it is not used in the model\n+        del self.cls_token\n+        num_patches = self.patch_embeddings.num_patches\n+        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches, config.hidden_size))\n+\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1]\n+        num_positions = self.position_embeddings.shape[1]\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        patch_pos_embed = self.position_embeddings\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return patch_pos_embed\n+\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        bool_masked_pos: Optional[torch.BoolTensor] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        embeddings = self.patch_embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n+\n+        if bool_masked_pos is not None:\n+            seq_length = embeddings.shape[1]\n+            mask_tokens = self.mask_token.expand(batch_size, seq_length, -1)\n+            # replace the masked visual tokens by mask_tokens\n+            mask = bool_masked_pos.unsqueeze(-1).type_as(mask_tokens)\n+            embeddings = embeddings * (1.0 - mask) + mask_tokens * mask\n+\n+        # add positional encoding to each token\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            embeddings = embeddings + self.position_embeddings\n+\n+        embeddings = self.dropout(embeddings)\n+\n+        return embeddings\n+\n+\n+class IJepaPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = IJepaConfig\n+    base_model_prefix = \"ijepa\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"IJepaEmbeddings\", \"IJepaLayer\"]\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n+            # `trunc_normal_cpu` not implemented in `half` issues\n+            module.weight.data = nn.init.trunc_normal_(\n+                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n+            ).to(module.weight.dtype)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, IJepaEmbeddings):\n+            module.position_embeddings.data = nn.init.trunc_normal_(\n+                module.position_embeddings.data.to(torch.float32),\n+                mean=0.0,\n+                std=self.config.initializer_range,\n+            ).to(module.position_embeddings.dtype)\n+\n+\n+_EXPECTED_OUTPUT_SHAPE = [1, 256, 1280]\n+\n+IJEPA_START_DOCSTRING = r\"\"\"\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n+    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`IJepaConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare IJepa Model transformer outputting raw hidden-states without any specific head on top.\",\n+    IJEPA_START_DOCSTRING,\n+)\n+class IJepaModel(IJepaPreTrainedModel, ViTModel):\n+    def __init__(self, config: IJepaConfig, add_pooling_layer: bool = False, use_mask_token: bool = False):\n+        super().__init__(config)\n+        self.config = config\n+        self.embeddings = IJepaEmbeddings(config, use_mask_token=use_mask_token)\n+\n+\n+_IMAGE_CLASS_CHECKPOINT = \"jmtzt/ijepa_vith14_1k\"\n+_IMAGE_CLASS_EXPECTED_OUTPUT = \"Egyptian cat\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    IJepa Model transformer with an image classification head on top (a linear layer on top of the final hidden states)\n+    e.g. for ImageNet.\n+\n+    <Tip>\n+\n+        Note that it's possible to fine-tune IJepa on higher resolution images than the ones it has been trained on, by\n+        setting `interpolate_pos_encoding` to `True` in the forward of the model. This will interpolate the pre-trained\n+        position embeddings to the higher resolution.\n+\n+    </Tip>\n+    \"\"\",\n+    IJEPA_START_DOCSTRING,\n+)\n+class IJepaForImageClassification(IJepaPreTrainedModel, ViTForImageClassification):\n+    def __init__(self, config: IJepaConfig):\n+        super().__init__(config)\n+        self.ijepa = IJepaModel(config, add_pooling_layer=False)\n+        self.post_init()\n+\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[tuple, ImageClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.ijepa(\n+            pixel_values,\n+            head_mask=head_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+            return_dict=return_dict,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        logits = self.classifier(sequence_output.mean(dim=1))\n+\n+        loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(logits.device)\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return ImageClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"IJepaPreTrainedModel\",\n+    \"IJepaModel\",\n+    \"IJepaForImageClassification\",\n+]"
        },
        {
            "sha": "d770b83df935a526d94d5cf9dc120b2eb7f479ad",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -4978,6 +4978,27 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class IJepaForImageClassification(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class IJepaModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class IJepaPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class ImageGPTForCausalImageModeling(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "101b34182a7309f81f5e789d701238e1b514b708",
            "filename": "src/transformers/utils/fx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Futils%2Ffx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/src%2Ftransformers%2Futils%2Ffx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ffx.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -140,6 +140,7 @@ def _generate_supported_model_class_names(\n     \"gptj\",\n     \"hiera\",\n     \"hubert\",\n+    \"ijepa\",\n     \"layoutlm\",\n     \"llama\",\n     \"cohere\","
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/ijepa/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/tests%2Fmodels%2Fijepa%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/tests%2Fmodels%2Fijepa%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fijepa%2F__init__.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4"
        },
        {
            "sha": "27a79bc6724285453a3f23c7bba51875ec586a97",
            "filename": "tests/models/ijepa/test_modeling_ijepa.py",
            "status": "added",
            "additions": 341,
            "deletions": 0,
            "changes": 341,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -0,0 +1,341 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch IJEPA model.\"\"\"\n+\n+import unittest\n+\n+from transformers import IJepaConfig\n+from transformers.testing_utils import (\n+    require_accelerate,\n+    require_torch,\n+    require_torch_accelerator,\n+    require_torch_fp16,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import (\n+    cached_property,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers import IJepaForImageClassification, IJepaModel\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import ViTImageProcessor\n+\n+\n+class IJepaModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        image_size=30,\n+        patch_size=2,\n+        num_channels=3,\n+        is_training=True,\n+        use_labels=True,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.1,\n+        attention_probs_dropout_prob=0.1,\n+        type_sequence_label_size=10,\n+        initializer_range=0.02,\n+        scope=None,\n+        encoder_stride=2,\n+        mask_ratio=0.5,\n+        attn_implementation=\"eager\",\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.initializer_range = initializer_range\n+        self.scope = scope\n+        self.encoder_stride = encoder_stride\n+        self.attn_implementation = attn_implementation\n+\n+        # in IJEPA, the seq length equals the number of patches (we don't add 1 for the [CLS] token)\n+        num_patches = (image_size // patch_size) ** 2\n+        self.seq_length = num_patches\n+        self.mask_ratio = mask_ratio\n+        self.num_masks = int(mask_ratio * self.seq_length)\n+        self.mask_length = num_patches\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.num_channels,\n+                self.image_size,\n+                self.image_size,\n+            ]\n+        )\n+\n+        labels = None\n+        if self.use_labels:\n+            labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n+\n+        config = self.get_config()\n+\n+        return config, pixel_values, labels\n+\n+    def get_config(self):\n+        return IJepaConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            hidden_act=self.hidden_act,\n+            hidden_dropout_prob=self.hidden_dropout_prob,\n+            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n+            is_decoder=False,\n+            initializer_range=self.initializer_range,\n+            encoder_stride=self.encoder_stride,\n+            attn_implementation=self.attn_implementation,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values, labels):\n+        model = IJepaModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape,\n+            (self.batch_size, self.seq_length, self.hidden_size),\n+        )\n+\n+    def create_and_check_for_image_classification(self, config, pixel_values, labels):\n+        config.num_labels = self.type_sequence_label_size\n+        model = IJepaForImageClassification(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values, labels=labels)\n+        self.parent.assertEqual(\n+            result.logits.shape,\n+            (self.batch_size, self.type_sequence_label_size),\n+        )\n+\n+        # test greyscale images\n+        config.num_channels = 1\n+        model = IJepaForImageClassification(config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        pixel_values = floats_tensor([self.batch_size, 1, self.image_size, self.image_size])\n+        result = model(pixel_values)\n+        self.parent.assertEqual(\n+            result.logits.shape,\n+            (self.batch_size, self.type_sequence_label_size),\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            pixel_values,\n+            labels,\n+        ) = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class IJepaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as IJEPA does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (\n+        (\n+            IJepaModel,\n+            IJepaForImageClassification,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    pipeline_model_mapping = (\n+        {\"image-feature-extraction\": IJepaModel, \"image-classification\": IJepaForImageClassification}\n+        if is_torch_available()\n+        else {}\n+    )\n+    fx_compatible = True\n+\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = IJepaModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=IJepaConfig,\n+            has_text_modality=False,\n+            hidden_size=37,\n+        )\n+\n+    @unittest.skip(\n+        \"Since `torch==2.3+cu121`, although this test passes, many subsequent tests have `CUDA error: misaligned address`.\"\n+        \"If `nvidia-xxx-cu118` are also installed, no failure (even with `torch==2.3+cu121`).\"\n+    )\n+    def test_multi_gpu_data_parallel_forward(self):\n+        super().test_multi_gpu_data_parallel_forward()\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"IJEPA does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_for_image_classification(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_image_classification(*config_and_inputs)\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"jmtzt/ijepa_vith14_1k\"\n+        model = IJepaModel.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+    return image\n+\n+\n+@require_torch\n+@require_vision\n+class IJepaModelIntegrationTest(unittest.TestCase):\n+    @cached_property\n+    def default_image_processor(self):\n+        return ViTImageProcessor.from_pretrained(\"jmtzt/ijepa_vith14_1k\") if is_vision_available() else None\n+\n+    @slow\n+    def test_inference_no_head(self):\n+        model = IJepaModel.from_pretrained(\"jmtzt/ijepa_vith14_1k\").to(torch_device)\n+\n+        image_processor = self.default_image_processor\n+        image = prepare_img()\n+        inputs = image_processor(images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        # verify the last hidden state\n+        expected_shape = torch.Size((1, 256, 1280))\n+        self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n+\n+        expected_slice = torch.Tensor(\n+            [[-0.0621, -0.0054, -2.7513], [-0.1952, 0.0909, -3.9536], [0.0942, -0.0331, -1.2833]]\n+        ).to(torch_device)\n+\n+        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+\n+    @slow\n+    @require_accelerate\n+    @require_torch_accelerator\n+    @require_torch_fp16\n+    def test_inference_fp16(self):\n+        r\"\"\"\n+        A small test to make sure that inference work in half precision without any problem.\n+        \"\"\"\n+        model = IJepaModel.from_pretrained(\n+            \"jmtzt/ijepa_vith14_1k\",\n+            torch_dtype=torch.float16,\n+            device_map=\"auto\",\n+        )\n+        image_processor = self.default_image_processor\n+\n+        image = prepare_img()\n+        inputs = image_processor(images=image, return_tensors=\"pt\")\n+        pixel_values = inputs.pixel_values.to(torch_device)\n+\n+        # forward pass to make sure inference works in fp16\n+        with torch.no_grad():\n+            _ = model(pixel_values)\n+\n+    @slow\n+    def test_inference_interpolate_pos_encoding(self):\n+        # I-JEPA, similar to ViT models have an `interpolate_pos_encoding` argument in their forward method,\n+        # allowing to interpolate the pre-trained position embeddings in order to use\n+        # the model on higher resolutions. The DINO model by Facebook AI leverages this\n+        # to visualize self-attention on higher resolution images.\n+        model = IJepaModel.from_pretrained(\"jmtzt/ijepa_vith14_1k\").to(torch_device)\n+\n+        image_processor = self.default_image_processor\n+        image = prepare_img()\n+        inputs = image_processor(images=image, return_tensors=\"pt\")\n+        pixel_values = inputs.pixel_values.to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(pixel_values, interpolate_pos_encoding=True)\n+\n+        # verify the logits\n+        expected_shape = torch.Size((1, 256, 1280))\n+        self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n+\n+        expected_slice = torch.tensor(\n+            [[-0.0621, -0.0054, -2.7513], [-0.1952, 0.0909, -3.9536], [0.0942, -0.0331, -1.2833]]\n+        ).to(torch_device)\n+\n+        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))"
        },
        {
            "sha": "a2ea05edce8063a207a4088b230da242456169d1",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/50189e36a6840c0a3863cb912378fae6ffdb83f4/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50189e36a6840c0a3863cb912378fae6ffdb83f4/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=50189e36a6840c0a3863cb912378fae6ffdb83f4",
            "patch": "@@ -331,6 +331,7 @@\n     \"IBertModel\",\n     \"IdeficsConfig\",\n     \"IdeficsProcessor\",\n+    \"IJepaModel\",\n     \"ImageClassificationPipeline\",\n     \"ImageFeatureExtractionPipeline\",\n     \"ImageGPTConfig\","
        }
    ],
    "stats": {
        "total": 1909,
        "additions": 1907,
        "deletions": 2
    }
}