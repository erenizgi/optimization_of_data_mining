{
    "author": "LysandreJik",
    "message": "Transformers serve -> list all generative models from the cache  (#42146)\n\n* Scan cache for generative models\nCo-authored-by: Wauplin <lucainp@gmail.com>\n\n* Address test",
    "sha": "2186f316af6c849296873d39d7e9e114249b296e",
    "files": [
        {
            "sha": "ab9661209a69890f425e40c4d92a5584f335bce4",
            "filename": "src/transformers/cli/serve.py",
            "status": "modified",
            "additions": 49,
            "deletions": 48,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/2186f316af6c849296873d39d7e9e114249b296e/src%2Ftransformers%2Fcli%2Fserve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2186f316af6c849296873d39d7e9e114249b296e/src%2Ftransformers%2Fcli%2Fserve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fserve.py?ref=2186f316af6c849296873d39d7e9e114249b296e",
            "patch": "@@ -11,13 +11,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n import asyncio\n import base64\n import copy\n-import datetime\n import enum\n-import functools\n import gc\n import io\n import json\n@@ -28,14 +25,15 @@\n import uuid\n from collections.abc import Generator, Iterable\n from contextlib import asynccontextmanager\n+from functools import lru_cache\n from io import BytesIO\n from threading import Thread\n from typing import TYPE_CHECKING, Annotated, Optional, TypedDict, Union\n \n import typer\n-from huggingface_hub import model_info\n-from huggingface_hub.constants import HF_HUB_OFFLINE\n+from huggingface_hub import scan_cache_dir\n from tokenizers.decoders import DecodeStream\n+from tqdm import tqdm\n \n import transformers\n from transformers import BitsAndBytesConfig, GenerationConfig\n@@ -731,51 +729,54 @@ def chunk_to_sse_element(chunk: ChatCompletionChunk | BaseModel) -> str:\n         \"\"\"\n         return f\"data: {chunk.model_dump_json(exclude_none=True)}\\n\\n\"\n \n-    @functools.cache\n-    def get_gen_models(self) -> list[dict[str, any]]:\n+    @staticmethod\n+    @lru_cache\n+    def get_gen_models(cache_dir: str | None = None) -> list[dict[str, any]]:\n         \"\"\"\n-        This is by no means a limit to which models may be instantiated with `transformers serve`: any chat-based\n-        model working with generate can work.\n-\n-        This is a limited list of models to ensure we have a discoverable /v1/models endpoint for third-party\n-        integrations.\n+        List LLMs and VLMs in the cache.\n         \"\"\"\n-        models = [\n-            \"Menlo/Jan-nano\",\n-            \"Menlo/Jan-nano-128k\",\n-            \"Qwen/Qwen2.5-0.5B-Instruct\",\n-            \"Qwen/Qwen2.5-3B-Instruct\",\n-            \"Qwen/Qwen2.5-7B-Instruct\",\n-            \"Qwen/Qwen2.5-14B-Instruct\",\n-            \"meta-llama/Llama-3.1-8B-Instruct\",\n-            \"meta-llama/Llama-3.2-1B-Instruct\",\n-            \"meta-llama/Llama-3.3-70B-Instruct\",\n-            \"HuggingFaceTB/SmolVLM-Instruct\",\n-            \"ibm-granite/granite-vision-3.2-2b\",\n-            \"Qwen/Qwen2.5-VL-7B-Instruct\",\n-        ]\n-\n-        if HF_HUB_OFFLINE:\n-            return [\n-                {\n-                    \"id\": model,\n-                    \"object\": \"model\",\n-                    \"created\": datetime.datetime.now().timestamp(),\n-                    \"owned_by\": model.split(\"/\")[0],\n-                }\n-                for model in models\n-            ]\n-        else:\n-            model_infos = [model_info(model) for model in models]\n-            return [\n-                {\n-                    \"id\": model.id,\n-                    \"object\": \"model\",\n-                    \"created\": model.created_at.timestamp(),\n-                    \"owned_by\": model.author,\n-                }\n-                for model in model_infos\n-            ]\n+        from transformers.models.auto.modeling_auto import (\n+            MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n+            MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES,\n+        )\n+\n+        generative_models = []\n+\n+        logger.warning(\"Scanning the cache directory for LLMs and VLMs.\")\n+        for repo in tqdm(scan_cache_dir(cache_dir).repos):\n+            if repo.repo_type != \"model\":\n+                continue\n+\n+            refs = repo.refs\n+            for ref, revision_info in refs.items():\n+                files = revision_info.files\n+                config_path = next((f.file_path for f in files if f.file_name == \"config.json\"), None)\n+\n+                if not config_path:\n+                    continue\n+\n+                config = json.loads(config_path.open().read())\n+\n+                if not (isinstance(config, dict) and \"architectures\" in config):\n+                    continue\n+\n+                architectures = config[\"architectures\"]\n+                llms = MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values()\n+                vlms = MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES.values()\n+\n+                if any(arch for arch in architectures if arch in [*llms, *vlms]):\n+                    author = repo.repo_id.split(\"/\") if \"/\" in repo.repo_id else \"\"\n+                    repo_handle = repo.repo_id + (f\"@{ref}\" if ref != \"main\" else \"\")\n+                    generative_models.append(\n+                        {\n+                            \"owned_by\": author,\n+                            \"id\": repo_handle,\n+                            \"object\": \"model\",\n+                            \"created\": repo.last_modified,\n+                        }\n+                    )\n+\n+        return generative_models\n \n     def continuous_batching_chat_completion(self, req: dict, request_id: str) -> StreamingResponse | JSONResponse:\n         \"\"\""
        },
        {
            "sha": "a74b0c6f229975f95ebc3aa26eec6348e6549328",
            "filename": "tests/cli/test_serve.py",
            "status": "modified",
            "additions": 29,
            "deletions": 1,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/2186f316af6c849296873d39d7e9e114249b296e/tests%2Fcli%2Ftest_serve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2186f316af6c849296873d39d7e9e114249b296e/tests%2Fcli%2Ftest_serve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcli%2Ftest_serve.py?ref=2186f316af6c849296873d39d7e9e114249b296e",
            "patch": "@@ -12,13 +12,14 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import os\n+import tempfile\n import time\n import unittest\n from threading import Thread\n from unittest.mock import Mock, patch\n \n import httpx\n-from huggingface_hub import ChatCompletionStreamOutput, InferenceClient\n+from huggingface_hub import ChatCompletionStreamOutput, InferenceClient, hf_hub_download\n from parameterized import parameterized\n \n from transformers import GenerationConfig\n@@ -153,6 +154,33 @@ def test_build_chat_completion_chunk():\n     assert expected_choices_content in chunk\n \n \n+def test_generative_model_list():\n+    with tempfile.TemporaryDirectory() as cache_dir:\n+        # \"download\" a few models, including some non-generative models\n+        hf_hub_download(\"Menlo/Jan-nano\", \"config.json\", cache_dir=cache_dir)\n+        hf_hub_download(\"Menlo/Jan-nano-128k\", \"config.json\", cache_dir=cache_dir)\n+        hf_hub_download(\"Qwen/Qwen2.5-0.5B-Instruct\", \"config.json\", cache_dir=cache_dir)\n+        hf_hub_download(\"HuggingFaceTB/SmolVLM-Instruct\", \"config.json\", cache_dir=cache_dir)\n+        hf_hub_download(\"google-bert/bert-base-cased\", \"config.json\", cache_dir=cache_dir)\n+\n+        expected_results = {\n+            \"HuggingFaceTB/SmolVLM-Instruct\": [\"HuggingFaceTB\", \"SmolVLM-Instruct\"],\n+            \"Qwen/Qwen2.5-0.5B-Instruct\": [\"Qwen\", \"Qwen2.5-0.5B-Instruct\"],\n+            \"Menlo/Jan-nano\": [\"Menlo\", \"Jan-nano\"],\n+            \"Menlo/Jan-nano-128k\": [\"Menlo\", \"Jan-nano-128k\"],\n+        }\n+\n+        # list models\n+        result = Serve.get_gen_models(cache_dir)\n+        assert len(expected_results) == len(result)\n+\n+        local_repos = {repo[\"id\"]: repo[\"owned_by\"] for repo in result}\n+\n+        for key, value in expected_results.items():\n+            assert key in local_repos\n+            assert local_repos[key] == value\n+\n+\n @require_openai\n def test_build_response_event():\n     \"\"\""
        }
    ],
    "stats": {
        "total": 127,
        "additions": 78,
        "deletions": 49
    }
}