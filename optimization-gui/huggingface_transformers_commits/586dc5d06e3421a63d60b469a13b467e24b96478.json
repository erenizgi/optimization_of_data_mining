{
    "author": "zucchini-nlp",
    "message": "[Glm4.5V] fix vLLM support (#40696)\n\n* fix\n\n* add a test case",
    "sha": "586dc5d06e3421a63d60b469a13b467e24b96478",
    "files": [
        {
            "sha": "4e19c2c6594fd58a841cbd6e7e29c5a81b352f90",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/586dc5d06e3421a63d60b469a13b467e24b96478/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/586dc5d06e3421a63d60b469a13b467e24b96478/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py?ref=586dc5d06e3421a63d60b469a13b467e24b96478",
            "patch": "@@ -452,7 +452,7 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         \"\"\"\n         patch_size = images_kwargs.get(\"patch_size\", self.patch_size)\n         merge_size = images_kwargs.get(\"merge_size\", self.merge_size)\n-        size = images_kwargs.get(\"size\", self.size)\n+        size = images_kwargs.get(\"size\", {\"shortest_edge\": 112 * 112, \"longest_edge\": 28 * 28 * 15000})\n \n         factor = patch_size * merge_size\n         resized_height, resized_width = smart_resize("
        },
        {
            "sha": "8afdde571c618634b3a53346ffce883c59775631",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/586dc5d06e3421a63d60b469a13b467e24b96478/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/586dc5d06e3421a63d60b469a13b467e24b96478/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=586dc5d06e3421a63d60b469a13b467e24b96478",
            "patch": "@@ -305,10 +305,14 @@ def _decode_and_sample_videos(\n         # Only sample frames if an array video is passed, otherwise first decode -> then sample\n         if is_valid_video(videos[0]) and do_sample_frames:\n             sampled_videos = []\n+            sampled_metadata = []\n             for video, metadata in zip(videos, video_metadata):\n                 indices = sample_indices_fn(metadata=metadata)\n+                metadata.frames_indices = indices\n                 sampled_videos.append(video[indices])\n+                sampled_metadata.append(metadata)\n             videos = sampled_videos\n+            video_metadata = sampled_metadata\n         elif not is_valid_video(videos[0]):\n             if isinstance(videos[0], list):\n                 # Videos sometimes are passed as a list of image URLs, especially through templates"
        },
        {
            "sha": "305817bb41c92847fa173a63bbae7cbe8c2c54d5",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/586dc5d06e3421a63d60b469a13b467e24b96478/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/586dc5d06e3421a63d60b469a13b467e24b96478/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=586dc5d06e3421a63d60b469a13b467e24b96478",
            "patch": "@@ -15,9 +15,9 @@\n \n import os\n import warnings\n-from collections.abc import Iterable\n+from collections.abc import Iterable, Mapping\n from contextlib import redirect_stdout\n-from dataclasses import dataclass\n+from dataclasses import dataclass, fields\n from io import BytesIO\n from typing import Callable, NewType, Optional, Union\n from urllib.parse import urlparse\n@@ -78,7 +78,7 @@\n \n \n @dataclass\n-class VideoMetadata:\n+class VideoMetadata(Mapping):\n     total_num_frames: int\n     fps: float = None\n     width: int = None\n@@ -87,6 +87,12 @@ class VideoMetadata:\n     video_backend: str = None\n     frames_indices: list[int] = None\n \n+    def __iter__(self):\n+        return (f.name for f in fields(self))\n+\n+    def __len__(self):\n+        return len(fields(self))\n+\n     def __getitem__(self, item):\n         return getattr(self, item)\n \n@@ -96,8 +102,8 @@ def __setitem__(self, key, value):\n     @property\n     def timestamps(self) -> float:\n         \"Timestamps of the sampled frames in seconds.\"\n-        if self.fps is None:\n-            raise ValueError(\"Cannot infer video `timestamps` when `fps` is None.\")\n+        if self.fps is None or self.frames_indices is None:\n+            raise ValueError(\"Cannot infer video `timestamps` when `fps` or `frames_indices` is None.\")\n         return [frame_idx / self.fps for frame_idx in self.frames_indices]\n \n     def update(self, dictionary):"
        },
        {
            "sha": "3d0477ee05d5e94bb083273da3eb31f587cd19f8",
            "filename": "tests/test_video_processing_common.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/586dc5d06e3421a63d60b469a13b467e24b96478/tests%2Ftest_video_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/586dc5d06e3421a63d60b469a13b467e24b96478/tests%2Ftest_video_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_video_processing_common.py?ref=586dc5d06e3421a63d60b469a13b467e24b96478",
            "patch": "@@ -342,6 +342,13 @@ def test_call_sample_frames(self):\n             self.assertEqual(encoded_videos.shape[1], 6)\n             self.assertEqual(encoded_videos_batched.shape[1], 6)\n \n+            # The same as above but uses a `VideoMetadata` object in the input\n+            metadata = [[VideoMetadata(duration=2.0, total_num_frames=8, fps=4)]]\n+            batched_metadata = metadata * len(video_inputs)\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", fps=3, video_metadata=metadata)[\n+                self.input_name\n+            ]\n+\n             # We should raise error when asked to sample more frames than there are in input video\n             with self.assertRaises(ValueError):\n                 encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", num_frames=10)[self.input_name]"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 23,
        "deletions": 6
    }
}