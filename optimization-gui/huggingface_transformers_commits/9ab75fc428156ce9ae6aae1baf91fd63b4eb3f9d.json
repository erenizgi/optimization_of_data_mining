{
    "author": "Tialo",
    "message": "fix typo (#39936)\n\n* fix typo\n\n* fix modular instead\n\n* fix\n\n---------\n\nCo-authored-by: y.korobko <y.korobko@tbank.ru>",
    "sha": "9ab75fc428156ce9ae6aae1baf91fd63b4eb3f9d",
    "files": [
        {
            "sha": "8330ba06b2504a9d6c8e01e7877593eddc2c0062",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ab75fc428156ce9ae6aae1baf91fd63b4eb3f9d/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ab75fc428156ce9ae6aae1baf91fd63b4eb3f9d/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=9ab75fc428156ce9ae6aae1baf91fd63b4eb3f9d",
            "patch": "@@ -75,7 +75,7 @@ def __init__(self, config):\n \n     def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weights=None) -> torch.Tensor:\n         \"\"\"\n-        When training is is more efficient to just loop over the experts and compute the output for each expert\n+        When training it is more efficient to just loop over the experts and compute the output for each expert\n         as otherwise the memory would explode.\n \n         For inference we can sacrifice some memory and compute the output for all experts at once. By repeating the inputs."
        },
        {
            "sha": "33cae9f50874b46a7e76fd50e8e67ce23c91cd69",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ab75fc428156ce9ae6aae1baf91fd63b4eb3f9d/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ab75fc428156ce9ae6aae1baf91fd63b4eb3f9d/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=9ab75fc428156ce9ae6aae1baf91fd63b4eb3f9d",
            "patch": "@@ -73,7 +73,7 @@ def __init__(self, config):\n \n     def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weights=None) -> torch.Tensor:\n         \"\"\"\n-        When training is is more efficient to just loop over the experts and compute the output for each expert\n+        When training it is more efficient to just loop over the experts and compute the output for each expert\n         as otherwise the memory would explode.\n \n         For inference we can sacrifice some memory and compute the output for all experts at once. By repeating the inputs."
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}