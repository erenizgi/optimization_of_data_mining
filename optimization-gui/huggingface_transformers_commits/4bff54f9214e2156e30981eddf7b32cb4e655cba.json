{
    "author": "ArthurZucker",
    "message": "Gemma capping (#34282)\n\n* softcapping\r\n\r\n* soft cap before the mask\r\n\r\n* style\r\n\r\n* ...\r\n\r\n* super nit\r\n\r\n* update\r\n\r\n* fixes\r\n\r\n* update\r\n\r\n* small issue with modular\r\n\r\n* fix modular imports\r\n\r\n* update\r\n\r\n* fixup\r\n\r\n* simplify a hell lot\r\n\r\n* simplify cleaning imports\r\n\r\n* finish fixing\r\n\r\n* update our design\r\n\r\n* nits\r\n\r\n* use a deprecation cycle\r\n\r\n* updates\r\n\r\n* Fix modular (recursive deps need to always be computed after merges!)\r\n\r\n* push\r\n\r\n* fix\r\n\r\n* update\r\n\r\n* fix modular order\r\n\r\n* make fix-copies\r\n\r\n* updates\r\n\r\n* update\r\n\r\n* ?\r\n\r\n* don't compile for now\r\n\r\n* ?\r\n\r\n* fix some stuff\r\n\r\n* donc!\r\n\r\n* fix copies\r\n\r\n* update\r\n\r\n* fixup\r\n\r\n* ?\r\n\r\n* fix two tests\r\n\r\n* fix?\r\n\r\n* for now, don't use head info\r\n\r\n* eager when output attentoin and sdpa or flash as it's the simplest behaviour (for our tests as well :))\r\n\r\n* fix-copies\r\n\r\n* revert sdpa check\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\r\n\r\n* rebase, fix-copies and push\r\n\r\n* add a slow integration test\r\n\r\n* update the test\r\n\r\n* fix left padding issue\r\n\r\n* fix test\r\n\r\n* remove duplicate scaling\r\n\r\n* quality\r\n\r\n* add a small test and make sure it works\r\n\r\n* 2b\r\n\r\n---------\r\n\r\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\r\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "4bff54f9214e2156e30981eddf7b32cb4e655cba",
    "files": [
        {
            "sha": "d68166d5268404b9f59e2a7817d7699500078743",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4bff54f9214e2156e30981eddf7b32cb4e655cba/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4bff54f9214e2156e30981eddf7b32cb4e655cba/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4bff54f9214e2156e30981eddf7b32cb4e655cba",
            "patch": "@@ -1519,6 +1519,7 @@ def _autoset_attn_implementation(\n                 \"eager\",\n                 \"sdpa\",\n                 \"flash_attention_2\",\n+                \"flex_attention\",\n             ]:\n                 message = f'Specified `attn_implementation=\"{config._attn_implementation}\"` is not supported. The only possible arguments are `attn_implementation=\"eager\"` (manual attention implementation)'\n                 if cls._supports_flash_attn_2:"
        },
        {
            "sha": "6111261830b8f05871a72398960419bf65211623",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 151,
            "deletions": 263,
            "changes": 414,
            "blob_url": "https://github.com/huggingface/transformers/blob/4bff54f9214e2156e30981eddf7b32cb4e655cba/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4bff54f9214e2156e30981eddf7b32cb4e655cba/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=4bff54f9214e2156e30981eddf7b32cb4e655cba",
            "patch": "@@ -41,7 +41,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal,\n-    is_flash_attn_greater_or_equal_2_10,\n+    is_torch_greater_or_equal,\n     logging,\n     replace_return_docstrings,\n )\n@@ -51,6 +51,8 @@\n if is_flash_attn_2_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n+if is_torch_greater_or_equal(\"2.5\"):\n+    from torch.nn.attention.flex_attention import flex_attention\n \n logger = logging.get_logger(__name__)\n \n@@ -168,19 +170,134 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(config, query, key, value, mask, **_kwargs):\n+    key_states = repeat_kv(key, config.num_key_value_groups)\n+    value_states = repeat_kv(value, config.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * config.scaling\n+\n+    if config.attn_logit_softcapping is not None:\n+        attn_weights = attn_weights / config.attn_logit_softcapping\n+        attn_weights = torch.tanh(attn_weights)\n+        attn_weights = attn_weights * config.attn_logit_softcapping\n+    if mask is not None:  # no matter the length, we just slice it\n+        causal_mask = mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    # upcast attention to fp32\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=config.attention_dropout, training=config.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n+\n+\n+def flash_attention_forward(config, query, key, value, mask, target_dtype=torch.float16, **_kwargs):\n+    if mask is not None:\n+        seq_len = mask.shape[1]\n+        query = query[:, :, :seq_len]\n+        value = value[:, :, :seq_len]\n+\n+    # TODO: These transpose are quite inefficient but Flash Attention requires the layout\n+    # [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor rotary embedding\n+    query_states = query.transpose(1, 2)\n+    key_states = key.transpose(1, 2)\n+    value_states = value.transpose(1, 2)\n+\n+    dropout_rate = config.attention_dropout if config.training else 0.0\n+\n+    input_dtype = query_states.dtype\n+    if input_dtype == torch.float32:\n+        query_states = query_states.to(target_dtype)\n+        key_states = key_states.to(target_dtype)\n+        value_states = value_states.to(target_dtype)\n+\n+    attn_output = _flash_attention_forward(\n+        query_states,\n+        key_states,\n+        value_states,\n+        mask,\n+        seq_len,\n+        dropout=dropout_rate,\n+        softmax_scale=config.scaling,\n+        is_causal=config.is_causal,\n+        sliding_window=config.sliding_window,\n+        use_top_left_mask=config._flash_attn_uses_top_left_mask,\n+        softcap=config.attn_logit_softcapping if is_flash_attn_greater_or_equal(\"2.6.0\") else None,\n+    )\n+\n+    return attn_output, None\n+\n+\n+def flex_attention_forward(config, query, key, value, mask, output_attentions=False, **_kwargs):\n+    def tanh_softcap(score, b, h, q_idx, kv_idx):\n+        soft_cap = config.attn_logit_softcapping\n+        score = soft_cap * torch.tanh(score / soft_cap)\n+        if mask is not None:\n+            return score + mask[b][0][q_idx][kv_idx]\n+        return score\n+\n+    attn_output = flex_attention(\n+        query,\n+        key,\n+        value,\n+        score_mod=tanh_softcap,\n+        enable_gqa=True,\n+        scale=config.scaling,\n+        return_lse=output_attentions,\n+    )\n+    if not output_attentions:\n+        return attn_output, None\n+    else:\n+        return attn_output[0], attn_output[1]\n+\n+\n+def sdpa_attention_forward(config, query, key, value, mask, **_kwargs):\n+    key = repeat_kv(key, config.num_key_value_groups)\n+    value = repeat_kv(value, config.num_key_value_groups)\n+\n+    causal_mask = mask\n+    if mask is not None:\n+        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n+\n+    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+    if query.device.type == \"cuda\" and causal_mask is not None:\n+        query = query.contiguous()\n+        key = key.contiguous()\n+        value = value.contiguous()\n+\n+    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+    is_causal = True if causal_mask is None and query.shape[1] > 1 else False\n+\n+    attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        query,\n+        key,\n+        value,\n+        attn_mask=causal_mask,\n+        dropout_p=config.attention_dropout if config.training else 0.0,\n+        is_causal=is_causal,\n+        scale=config.scaling,\n+    )\n+    return attn_output, None\n+\n+\n+GEMMA2_ATTENTION_FUNCTION = {\n+    \"flash_attention_2\": flash_attention_forward,\n+    \"flex_attention\": flex_attention_forward,\n+    \"eager\": eager_attention_forward,\n+    \"sdpa\": sdpa_attention_forward,\n+}\n+\n+\n class Gemma2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n \n         self.attention_dropout = config.attention_dropout\n         self.hidden_size = config.hidden_size\n@@ -192,7 +309,8 @@ def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n         self.rope_theta = config.rope_theta\n         self.is_causal = True\n         self.scaling = config.query_pre_attn_scalar**-0.5\n-\n+        self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None\n+        self.attn_logit_softcapping = config.attn_logit_softcapping\n         if self.hidden_size % self.num_heads != 0:\n             raise ValueError(\n                 f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n@@ -208,7 +326,6 @@ def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n             max_position_embeddings=self.max_position_embeddings,\n             base=self.rope_theta,\n         )\n-        self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None\n \n     def forward(\n         self,\n@@ -243,33 +360,17 @@ def forward(\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling\n-\n-        if self.config.attn_logit_softcapping is not None:\n-            attn_weights = attn_weights / self.config.attn_logit_softcapping\n-            attn_weights = torch.tanh(attn_weights)\n-            attn_weights = attn_weights * self.config.attn_logit_softcapping\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n+        if output_attentions and self.config._attn_implementation in [\"sdpa\", \"flash_attention_2\"]:\n+            logger.warning_once(\"Setting `attention_type` to `flex_attention` because `output_attentions=True`\")\n+            attention_type = \"eager\"\n+        else:\n+            attention_type = self.config._attn_implementation\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output, attn_weights = GEMMA2_ATTENTION_FUNCTION[attention_type](\n+            self, query_states, key_states, value_states, attention_mask, output_attentions=output_attentions\n+        )\n \n-        attn_output = attn_output.view(bsz, q_len, -1)\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n \n         if not output_attentions:\n@@ -279,230 +380,36 @@ def forward(\n \n \n class Gemma2FlashAttention2(Gemma2Attention):\n-    \"\"\"\n-    Gemma2 flash attention module. This module inherits from `Gemma2Attention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"sliding_window\": self.sliding_window,\n-                \"cache_position\": cache_position,\n-            }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        if attention_mask is not None:\n-            seq_len = attention_mask.shape[1]\n-            key_states = key_states[:, :, :seq_len]\n-            value_states = value_states[:, :, :seq_len]\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (Gemma2RMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            softmax_scale=self.scaling,\n-            is_causal=self.is_causal,\n-            sliding_window=self.sliding_window,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            softcap=self.config.attn_logit_softcapping if is_flash_attn_greater_or_equal(\"2.6.0\") else None,\n+    def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n+        super().__init__(config, layer_idx)\n+        self.config._attn_implementation = \"flash_attention_2\"\n+        logger.warning_once(\n+            \"The `Gemma2FlashAttention2` class is deprecated in favor of simply modifying the `config._attn_implementation`\"\n+            \"attribute of the `GemmaAttention` class! It will be removed in v4.48\"\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n \n class Gemma2SdpaAttention(Gemma2Attention):\n-    \"\"\"\n-    Gemma2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `Gemma2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from Gemma2Attention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Gemma2Model is using Gemma2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"sliding_window\": self.sliding_window,\n-                \"cache_position\": cache_position,\n-            }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-            scale=self.scaling,\n+    def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n+        super().__init__(config, layer_idx)\n+        self.config._attn_implementation = \"sdpa\"\n+        logger.warning_once(\n+            \"The `Gemma2FlashAttention2` class is deprecated in favor of simply modifying the `config._attn_implementation`\"\n+            \"attribute of the `GemmaAttention` class! It will be removed in v4.48\"\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-GEMMA2_ATTENTION_CLASSES = {\n-    \"eager\": Gemma2Attention,\n-    \"flash_attention_2\": Gemma2FlashAttention2,\n-    \"sdpa\": Gemma2SdpaAttention,\n-}\n-\n \n class Gemma2DecoderLayer(nn.Module):\n     def __init__(self, config: Gemma2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-        self.self_attn = GEMMA2_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.config = config\n+        self.is_sliding = not bool(layer_idx % 2)\n+        self.self_attn = Gemma2Attention(config=config, layer_idx=layer_idx)\n         self.mlp = Gemma2MLP(config)\n         self.input_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.config = config\n-        self.is_sliding = not bool(layer_idx % 2)\n+\n         self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.sliding_window = config.sliding_window\n@@ -517,25 +424,6 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n         if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n             # Flash-attn is a 2D tensor\n             if self.config._attn_implementation == \"flash_attention_2\":"
        },
        {
            "sha": "8d86238632365fbb03a5760ff7e1505bc9603671",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 187,
            "deletions": 231,
            "changes": 418,
            "blob_url": "https://github.com/huggingface/transformers/blob/4bff54f9214e2156e30981eddf7b32cb4e655cba/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4bff54f9214e2156e30981eddf7b32cb4e655cba/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=4bff54f9214e2156e30981eddf7b32cb4e655cba",
            "patch": "@@ -29,18 +29,17 @@\n from ...utils import (\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal,\n-    is_flash_attn_greater_or_equal_2_10,\n+    is_torch_greater_or_equal,\n     logging,\n )\n from ..gemma.modeling_gemma import (\n-    GemmaAttention,\n-    GemmaDecoderLayer,\n     GemmaForCausalLM,\n     GemmaForSequenceClassification,\n     GemmaForTokenClassification,\n     GemmaModel,\n     GemmaPreTrainedModel,\n     GemmaRMSNorm,\n+    GemmaRotaryEmbedding,\n     apply_rotary_pos_emb,\n     repeat_kv,\n )\n@@ -49,6 +48,9 @@\n if is_flash_attn_2_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n+if is_torch_greater_or_equal(\"2.5\"):\n+    from torch.nn.attention.flex_attention import flex_attention\n+\n \n _CHECKPOINT_FOR_DOC = \"google/gemma2-7b\"\n \n@@ -207,118 +209,183 @@ def forward(self, x):\n         return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n \n \n-class Gemma2Attention(GemmaAttention):\n+class Gemma2RotaryEmbedding(GemmaRotaryEmbedding):\n+    pass\n+\n+\n+def eager_attention_forward(config, query, key, value, mask, **_kwargs):\n+    key_states = repeat_kv(key, config.num_key_value_groups)\n+    value_states = repeat_kv(value, config.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * config.scaling\n+\n+    if config.attn_logit_softcapping is not None:\n+        attn_weights = attn_weights / config.attn_logit_softcapping\n+        attn_weights = torch.tanh(attn_weights)\n+        attn_weights = attn_weights * config.attn_logit_softcapping\n+    if mask is not None:  # no matter the length, we just slice it\n+        causal_mask = mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    # upcast attention to fp32\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=config.attention_dropout, training=config.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n+\n+\n+def flash_attention_forward(config, query, key, value, mask, target_dtype=torch.float16, **_kwargs):\n+    if mask is not None:\n+        seq_len = mask.shape[1]\n+        query = query[:, :, :seq_len]\n+        value = value[:, :, :seq_len]\n+\n+    # TODO: These transpose are quite inefficient but Flash Attention requires the layout\n+    # [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor rotary embedding\n+    query_states = query.transpose(1, 2)\n+    key_states = key.transpose(1, 2)\n+    value_states = value.transpose(1, 2)\n+\n+    dropout_rate = config.attention_dropout if config.training else 0.0\n+\n+    input_dtype = query_states.dtype\n+    if input_dtype == torch.float32:\n+        query_states = query_states.to(target_dtype)\n+        key_states = key_states.to(target_dtype)\n+        value_states = value_states.to(target_dtype)\n+\n+    attn_output = _flash_attention_forward(\n+        query_states,\n+        key_states,\n+        value_states,\n+        mask,\n+        seq_len,\n+        dropout=dropout_rate,\n+        softmax_scale=config.scaling,\n+        is_causal=config.is_causal,\n+        sliding_window=config.sliding_window,\n+        use_top_left_mask=config._flash_attn_uses_top_left_mask,\n+        softcap=config.attn_logit_softcapping if is_flash_attn_greater_or_equal(\"2.6.0\") else None,\n+    )\n+\n+    return attn_output, None\n+\n+\n+def flex_attention_forward(config, query, key, value, mask, output_attentions=False, **_kwargs):\n+    def tanh_softcap(score, b, h, q_idx, kv_idx):\n+        soft_cap = config.attn_logit_softcapping\n+        score = soft_cap * torch.tanh(score / soft_cap)\n+        if mask is not None:\n+            return score + mask[b][0][q_idx][kv_idx]\n+        return score\n+\n+    attn_output = flex_attention(\n+        query,\n+        key,\n+        value,\n+        score_mod=tanh_softcap,\n+        enable_gqa=True,\n+        scale=config.scaling,\n+        return_lse=output_attentions,\n+    )\n+    if not output_attentions:\n+        return attn_output, None\n+    else:\n+        return attn_output[0], attn_output[1]\n+\n+\n+def sdpa_attention_forward(config, query, key, value, mask, **_kwargs):\n+    key = repeat_kv(key, config.num_key_value_groups)\n+    value = repeat_kv(value, config.num_key_value_groups)\n+\n+    causal_mask = mask\n+    if mask is not None:\n+        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n+\n+    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+    if query.device.type == \"cuda\" and causal_mask is not None:\n+        query = query.contiguous()\n+        key = key.contiguous()\n+        value = value.contiguous()\n+\n+    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+    is_causal = True if causal_mask is None and query.shape[1] > 1 else False\n+\n+    attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        query,\n+        key,\n+        value,\n+        attn_mask=causal_mask,\n+        dropout_p=config.attention_dropout if config.training else 0.0,\n+        is_causal=is_causal,\n+        scale=config.scaling,\n+    )\n+    return attn_output, None\n+\n+\n+GEMMA2_ATTENTION_FUNCTION = {\n+    \"flash_attention_2\": flash_attention_forward,\n+    \"flex_attention\": flex_attention_forward,\n+    \"eager\": eager_attention_forward,\n+    \"sdpa\": sdpa_attention_forward,\n+}\n+\n+\n+class Gemma2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n-        super().__init__(config, layer_idx)\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+\n+        self.attention_dropout = config.attention_dropout\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = config.head_dim\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.max_position_embeddings = config.max_position_embeddings\n+        self.rope_theta = config.rope_theta\n+        self.is_causal = True\n         self.scaling = config.query_pre_attn_scalar**-0.5\n         self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"sliding_window\": self.sliding_window,\n-                \"cache_position\": cache_position,\n-            }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling\n-\n-        if self.config.attn_logit_softcapping is not None:\n-            attn_weights = attn_weights / self.config.attn_logit_softcapping\n-            attn_weights = torch.tanh(attn_weights)\n-            attn_weights = attn_weights * self.config.attn_logit_softcapping\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n+        self.attn_logit_softcapping = config.attn_logit_softcapping\n+        if self.hidden_size % self.num_heads != 0:\n             raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n+                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n+                f\" and `num_heads`: {self.num_heads}).\"\n             )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Gemma2FlashAttention2(Gemma2Attention):\n-    \"\"\"\n-    Gemma2 flash attention module. This module inherits from `Gemma2Attention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n+        self.rotary_emb = Gemma2RotaryEmbedding(\n+            self.head_dim,\n+            max_position_embeddings=self.max_position_embeddings,\n+            base=self.rope_theta,\n+        )\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        output_attentions = False\n-\n         bsz, q_len, _ = hidden_states.size()\n \n         query_states = self.q_proj(hidden_states)\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n         query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n@@ -336,57 +403,14 @@ def forward(\n             }\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        if attention_mask is not None:\n-            seq_len = attention_mask.shape[1]\n-            key_states = key_states[:, :, :seq_len]\n-            value_states = value_states[:, :, :seq_len]\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (Gemma2RMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+        if output_attentions and self.config._attn_implementation in [\"sdpa\", \"flash_attention_2\"]:\n+            logger.warning_once(\"Setting `attention_type` to `flex_attention` because `output_attentions=True`\")\n+            attention_type = \"eager\"\n+        else:\n+            attention_type = self.config._attn_implementation\n \n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            softmax_scale=self.scaling,\n-            is_causal=self.is_causal,\n-            sliding_window=self.sliding_window,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            softcap=self.config.attn_logit_softcapping if is_flash_attn_greater_or_equal(\"2.6.0\") else None,\n+        attn_output, attn_weights = GEMMA2_ATTENTION_FUNCTION[attention_type](\n+            self, query_states, key_states, value_states, attention_mask, output_attentions=output_attentions\n         )\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n@@ -398,105 +422,37 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class Gemma2SdpaAttention(Gemma2Attention):\n-    \"\"\"\n-    Gemma2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `Gemma2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from Gemma2Attention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Gemma2Model is using Gemma2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"sliding_window\": self.sliding_window,\n-                \"cache_position\": cache_position,\n-            }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-            scale=self.scaling,\n+class Gemma2FlashAttention2(Gemma2Attention):\n+    def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n+        super().__init__(config, layer_idx)\n+        self.config._attn_implementation = \"flash_attention_2\"\n+        logger.warning_once(\n+            \"The `Gemma2FlashAttention2` class is deprecated in favor of simply modifying the `config._attn_implementation`\"\n+            \"attribute of the `GemmaAttention` class! It will be removed in v4.48\"\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+class Gemma2SdpaAttention(Gemma2Attention):\n+    def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n+        super().__init__(config, layer_idx)\n+        self.config._attn_implementation = \"sdpa\"\n+        logger.warning_once(\n+            \"The `Gemma2FlashAttention2` class is deprecated in favor of simply modifying the `config._attn_implementation`\"\n+            \"attribute of the `GemmaAttention` class! It will be removed in v4.48\"\n+        )\n \n \n-class Gemma2DecoderLayer(GemmaDecoderLayer):\n+class Gemma2DecoderLayer(nn.Module):\n     def __init__(self, config: Gemma2Config, layer_idx: int):\n-        super().__init__(config, layer_idx)\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n         self.config = config\n         self.is_sliding = not bool(layer_idx % 2)\n+        self.self_attn = Gemma2Attention(config=config, layer_idx=layer_idx)\n         self.mlp = Gemma2MLP(config)\n+        self.input_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n         self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.sliding_window = config.sliding_window"
        },
        {
            "sha": "492642d61babb5736db860b3413448e7131c21d6",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4bff54f9214e2156e30981eddf7b32cb4e655cba/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4bff54f9214e2156e30981eddf7b32cb4e655cba/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=4bff54f9214e2156e30981eddf7b32cb4e655cba",
            "patch": "@@ -209,6 +209,7 @@\n     is_torch_fp16_available_on_device,\n     is_torch_fx_available,\n     is_torch_fx_proxy,\n+    is_torch_greater_or_equal,\n     is_torch_mlu_available,\n     is_torch_mps_available,\n     is_torch_musa_available,"
        },
        {
            "sha": "6306efa2face1923bbd54dfff1506c06a9056ea4",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4bff54f9214e2156e30981eddf7b32cb4e655cba/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4bff54f9214e2156e30981eddf7b32cb4e655cba/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=4bff54f9214e2156e30981eddf7b32cb4e655cba",
            "patch": "@@ -929,6 +929,14 @@ def is_flash_attn_greater_or_equal(library_version: str):\n     return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(library_version)\n \n \n+@lru_cache()\n+def is_torch_greater_or_equal(library_version: str):\n+    if not _is_package_available(\"torch\"):\n+        return False\n+\n+    return version.parse(importlib.metadata.version(\"torch\")) >= version.parse(library_version)\n+\n+\n def is_torchdistx_available():\n     return _torchdistx_available\n "
        },
        {
            "sha": "b1d0042c65167f97894639c7f032684ce65ab498",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4bff54f9214e2156e30981eddf7b32cb4e655cba/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4bff54f9214e2156e30981eddf7b32cb4e655cba/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=4bff54f9214e2156e30981eddf7b32cb4e655cba",
            "patch": "@@ -1496,7 +1496,7 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n \n             # They should result in very similar logits\n-            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=1e-5))\n+            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, atol=1e-5, rtol=1e-5)\n \n     @pytest.mark.generate\n     def test_past_key_values_format(self):"
        },
        {
            "sha": "06116c4dbafbd42ed9ecf2b706917325c9a0f66b",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 44,
            "deletions": 16,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/4bff54f9214e2156e30981eddf7b32cb4e655cba/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4bff54f9214e2156e30981eddf7b32cb4e655cba/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=4bff54f9214e2156e30981eddf7b32cb4e655cba",
            "patch": "@@ -199,19 +199,6 @@ def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_l\n     def test_sdpa_equivalence(self):\n         pass\n \n-    def test_eager_attention_loaded_by_default(self):\n-        \"\"\"Gemma 2 + SDPA = inferior results, because of the logit softcapping. Eager is the default.\"\"\"\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        # Usually we enable SDPA by default, but not for Gemma2\n-        model = Gemma2Model(config)\n-        self.assertTrue(model.config._attn_implementation == \"eager\")\n-\n-        # We can still force SDPA\n-        config._attn_implementation = \"sdpa\"\n-        model = Gemma2Model(config)\n-        self.assertTrue(model.config._attn_implementation == \"sdpa\")\n-\n \n @slow\n @require_torch_gpu\n@@ -277,9 +264,30 @@ def test_model_9b_pipeline_bf16(self):\n             \"Hi today I'm going to be talking about the history of the United States. The United States of America\",\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\n-            torch_device\n-        )\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n+        ).to(torch_device)\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n+\n+        output = pipe(self.input_text, max_new_tokens=20, do_sample=False, padding=True)\n+\n+        self.assertEqual(output[0][0][\"generated_text\"], EXPECTED_TEXTS[0])\n+        self.assertEqual(output[1][0][\"generated_text\"], EXPECTED_TEXTS[1])\n+\n+    @require_read_token\n+    def test_model_2b_pipeline_bf16_flex_attention(self):\n+        # See https://github.com/huggingface/transformers/pull/31747 -- pipeline was broken for Gemma2 before this PR\n+        model_id = \"google/gemma-2-2b\"\n+        # EXPECTED_TEXTS should match the same non-pipeline test, minus the special tokens\n+        EXPECTED_TEXTS = [\n+            \"Hello I am doing a project on the 1960s and I am trying to find out what the average\",\n+            \"Hi today I'm going to be talking about the 10 best anime of all time.\\n\\n1\",\n+        ]\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n+        ).to(torch_device)\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n \n@@ -365,3 +373,23 @@ def test_export_static_cache(self):\n         )\n         ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)\n+\n+    @require_read_token\n+    def test_model_9b_bf16_flex_attention(self):\n+        model_id = \"google/gemma-2-9b\"\n+        EXPECTED_TEXTS = [\n+            \"<bos>Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many\",\n+            \"<pad><pad><bos>Hi today I'm going to be talking about the history of the United States. The United States of America\",\n+        ]\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n+        ).to(torch_device)\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        output_text = tokenizer.batch_decode(output, skip_special_tokens=False)\n+\n+        self.assertEqual(output_text, EXPECTED_TEXTS)"
        },
        {
            "sha": "e5f6e34ece0e0844d3ff5856ad0e58f5973b7e11",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 33,
            "deletions": 25,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/4bff54f9214e2156e30981eddf7b32cb4e655cba/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4bff54f9214e2156e30981eddf7b32cb4e655cba/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=4bff54f9214e2156e30981eddf7b32cb4e655cba",
            "patch": "@@ -153,37 +153,37 @@ def __init__(self, all_bases: Set[str]):\n     def leave_Attribute(self, original_node: cst.Attribute, updated_node: cst.Attribute) -> cst.CSTNode:\n         # Handle ClassB.call_to_method\n         if (\n-            isinstance(original_node.value, cst.Name)\n+            m.matches(original_node.value, m.Name())\n             and original_node.value.value in self.all_bases\n-            and isinstance(original_node.attr, cst.Name)\n+            and m.matches(original_node.attr, m.Name())\n         ):\n             # Replace with super().call_to_method\n             return updated_node.with_changes(\n                 value=cst.Call(cst.Name(\"super\")),\n             )\n         # Handle ClassB().call_to_method\n         elif (\n-            isinstance(original_node.value, cst.Call)\n-            and isinstance(original_node.value.func, cst.Name)\n+            m.matches(original_node.value, m.Call())\n+            and m.matches(original_node.value.func, m.Name())\n             and original_node.value.func.value in self.all_bases\n-            and isinstance(original_node.attr, cst.Name)\n+            and m.matches(original_node.attr, m.Name())\n         ):\n             # Replace with super().call_to_method\n             return updated_node.with_changes(func=cst.Attribute(value=cst.Call(func=cst.Name(\"super\"))))\n         return updated_node\n \n     def leave_Call(self, original_node: cst.Call, updated_node: cst.Call) -> cst.CSTNode:\n         # Check if the function being called is of the form ClassB().func_a or ClassB.func_a\n-        if isinstance(original_node.func, cst.Attribute) and (\n+        if m.matches(original_node.func, m.Attribute()) and (\n             # Match ClassB().func_a(...)\n             (\n-                isinstance(original_node.func.value, cst.Call)\n-                and isinstance(original_node.func.value.func, cst.Name)\n+                m.matches(original_node.func.value, m.Call())\n+                and m.matches(original_node.func.value.func, m.Name())\n                 and original_node.func.value.func.value in self.all_bases\n             )\n             or\n             # Match ClassB.func_a(...)\n-            (isinstance(original_node.func.value, cst.Name) and original_node.func.value.value in self.all_bases)\n+            (m.matches(original_node.func.value, m.Name()) and original_node.func.value.value in self.all_bases)\n         ):\n             # Check if the first argument is 'self', and remove it\n             if len(original_node.args) > 0 and m.matches(original_node.args[0].value, m.Name(\"self\")):\n@@ -632,8 +632,10 @@ def leave_Module(self, node):\n         for id, node in self.global_nodes.items():\n             self.start_lines[id] = self.get_metadata(cst.metadata.PositionProvider, node).start.line\n \n-        # Since we added every Name as part of `self.object_dependency_mapping`, we now remove those that\n-        # are not part of the recorded objects (i.e. built-in variables, imports, etc)\n+    def _restrict_dependencies_to_known_entities(self):\n+        \"\"\"Since we added every Name as part of `self.object_dependency_mapping`, we need to remove those that\n+        are not part of the recorded objects in `self.global_nodes` (i.e. built-in variables, imports, etc).\n+        This should be called only after all merging operations have been finalized!!\"\"\"\n         global_objects = set(self.global_nodes.keys())\n         for object_name, dependencies in self.object_dependency_mapping.items():\n             self.object_dependency_mapping[object_name] = {dep for dep in dependencies if dep in global_objects}\n@@ -814,6 +816,8 @@ def merge_modular_dependencies(self, classes, functions, assignments, object_map\n         # Correctly re-set the global nodes at this point\n         self.global_nodes.update(self.functions)\n         self.global_nodes.update(self.assignments)\n+        # Restrict the dependency mappings to the know entities to avoid Python's built-ins\n+        self._restrict_dependencies_to_known_entities()\n         # Create the global mapping of recursive dependencies for functions and assignments\n         self.object_recursive_dependency_mapping = self._compute_recursive_object_dependencies()\n \n@@ -1142,22 +1146,20 @@ def visit_SimpleStatementLine(self, node):\n                 if assigned_variable == \"__all__\":\n                     self.all_all_to_add = split_all_assignment(node)\n                 else:\n+                    self.current_assignment = assigned_variable\n                     self.assignments[assigned_variable] = node\n \n     def leave_Module(self, node):\n         \"\"\"When we leave the modular file, we do the following in order:\n-        1. compute the nested (recursive) function and assignment dependencies\n-        2. for each modeling file found in the imports, rename it with the new model name, visit it, and update\n+        1. for each modeling file found in the imports, rename it with the new model name, visit it, and update\n         its dependency graph with the new function and assignment definitions found in the modular\n-        3. update the modular dependency graph with the imported functions and assignments (found when visiting the matching files)\n+        2. update the modular dependency graph with the imported functions and assignments (found when visiting the matching files)\n+        3. compute the nested (recursive) function and assignment dependencies\n         \"\"\"\n         # Takes care of finalizing our visit\n         super().leave_Module(node)\n \n-        # 1. compute the nested (recursive) function and assignment dependencies\n-        self.object_recursive_dependency_mapping = self._compute_recursive_object_dependencies()\n-\n-        # 2. for each modeling file found in the imports, rename it with the new model name, visit it, and update dependencies\n+        # 1. for each modeling file found in the imports, rename it with the new model name, visit it, and update dependencies\n         self.visited_modules = {}\n         self.renamers = {}\n         for file, module in self.model_specific_modules.items():\n@@ -1177,10 +1179,13 @@ def leave_Module(self, node):\n             # We record it so that we can rename classes later the exact same way\n             self.renamers[file] = renamer\n \n-        # 3. in turn, we need to add the imported functions/assignments to the dependencies of the modular mapper, using the\n+        # 2. in turn, we need to add the imported functions/assignments to the dependencies of the modular mapper, using the\n         # definitions found in the visited files\n         self.merge_model_specific_imports(self.visited_modules)\n \n+        # 3. compute the nested (recursive) function and assignment dependencies\n+        self.object_recursive_dependency_mapping = self._compute_recursive_object_dependencies()\n+\n         # We need to keep track of which objects were imported directly into which modeling file to not add them wrongly later\n         # Note that we may visit several of the same file types, thus we save them per file type, not file\n         self.imported_objects_per_file = defaultdict(set)\n@@ -1200,9 +1205,9 @@ def merge_model_specific_imports(self, visited_modules):\n             if object_name in visited_module.functions and object_name not in self.functions:\n                 self.functions[object_name] = visited_module.functions[object_name]\n                 self.added_objects_file_mapping[object_name] = file\n-                dependencies = visited_module.object_recursive_dependency_mapping.get(object_name, None)\n+                dependencies = visited_module.object_dependency_mapping.get(object_name, None)\n                 if dependencies is not None:\n-                    self.object_recursive_dependency_mapping[object_name] = dependencies\n+                    self.object_dependency_mapping[object_name] = dependencies\n                     for dep in dependencies:\n                         if dep not in self.global_nodes:\n                             self.added_objects_file_mapping[dep] = file\n@@ -1212,16 +1217,18 @@ def merge_model_specific_imports(self, visited_modules):\n             elif object_name in visited_module.assignments and object_name not in self.assignments:\n                 self.assignments[object_name] = visited_module.assignments[object_name]\n                 self.added_objects_file_mapping[object_name] = file\n-                dependencies = visited_module.object_recursive_dependency_mapping.get(object_name, None)\n+                dependencies = visited_module.object_dependency_mapping.get(object_name, None)\n                 if dependencies is not None:\n-                    self.object_recursive_dependency_mapping[object_name] = dependencies\n+                    self.object_dependency_mapping[object_name] = dependencies\n                     for dep in dependencies:\n                         if dep not in self.global_nodes:\n                             self.added_objects_file_mapping[dep] = file\n                             self.assignments[dep] = visited_module.global_nodes[dep]\n \n         # Do not forget to re-assign all nodes after the merge\n         self.global_nodes = {**self.assignments, **self.classes, **self.functions}\n+        # And restric dependencies to those nodes only\n+        self._restrict_dependencies_to_known_entities()\n \n     def compute_relative_order(self, missing_dependencies: set) -> dict[str, int]:\n         \"\"\"Compute in which relative order the `missing_dependencies` should appear when the nodes are added to the final file that\n@@ -1239,10 +1246,11 @@ def compute_relative_order(self, missing_dependencies: set) -> dict[str, int]:\n             else:\n                 original_dependencies.append(dep)\n         # Sort all lists according to the order in their respective file\n-        all_dependencies = sorted(original_dependencies, key=lambda x: self.start_lines[x])\n+        all_dependencies = []\n         for file, dependencies in other_files_dependencies.items():\n             sorted_dependencies = sorted(dependencies, key=lambda x: self.start_lines_file_mapping[file][x])\n             all_dependencies += sorted_dependencies\n+        all_dependencies += sorted(original_dependencies, key=lambda x: self.start_lines[x])\n \n         # Add all original node first, then merged ones (one file at a time)\n         for dep in all_dependencies:\n@@ -1485,7 +1493,7 @@ def save_modeling_file(modular_file, converted_file):\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n         \"--files_to_parse\",\n-        default=[\"src/transformers/models/gemma/modular_gemma.py\"],\n+        default=[\"src/transformers/models/gemma2/modular_gemma2.py\"],\n         nargs=\"+\",\n         help=\"A list of `modular_xxxx` files that should be converted to single model file\",\n     )"
        }
    ],
    "stats": {
        "total": 962,
        "additions": 426,
        "deletions": 536
    }
}