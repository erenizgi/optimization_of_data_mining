{
    "author": "stevhliu",
    "message": "[docs] optimization cleanup (#42827)\n\n* optimum\n\n* assisted decoding\n\n* fix\n\n* feedback\n\n* toctree",
    "sha": "5ef16eddf9d9a0d8a50c6b65c2bf352f0c3cace5",
    "files": [
        {
            "sha": "a8bd6ee3e9e8c08ab573b1fa865fa41ccdd6716b",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ef16eddf9d9a0d8a50c6b65c2bf352f0c3cace5/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ef16eddf9d9a0d8a50c6b65c2bf352f0c3cace5/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=5ef16eddf9d9a0d8a50c6b65c2bf352f0c3cace5",
            "patch": "@@ -88,12 +88,8 @@\n       title: Cache strategies\n     - local: cache_explanation\n       title: Caching\n-    - local: perf_infer_gpu_one\n-      title: GPU\n-    - local: perf_infer_cpu\n-      title: CPU\n-    - local: llm_optims\n-      title: Optimizing inference\n+    - local: assisted_decoding\n+      title: Assisted decoding\n     - local: llm_tutorial_optimization\n       title: Getting the most out of LLMs\n     title: Optimization"
        },
        {
            "sha": "f617c2b39f1f74866cb1cc72c075fc8240202efc",
            "filename": "docs/source/en/assisted_decoding.md",
            "status": "added",
            "additions": 167,
            "deletions": 0,
            "changes": 167,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ef16eddf9d9a0d8a50c6b65c2bf352f0c3cace5/docs%2Fsource%2Fen%2Fassisted_decoding.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ef16eddf9d9a0d8a50c6b65c2bf352f0c3cace5/docs%2Fsource%2Fen%2Fassisted_decoding.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fassisted_decoding.md?ref=5ef16eddf9d9a0d8a50c6b65c2bf352f0c3cace5",
            "patch": "@@ -0,0 +1,167 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Assisted decoding\n+\n+Assisted decoding speeds up text generation by allowing a helper propose candidate tokens before the main model commits to them. The main model verifies the candidate tokens in one forward pass. The helper is fast and cheap and can replace dozens of more expensive forward passes by the main model.\n+\n+This guide covers assisted decoding methods in Transformers.\n+\n+## Speculative decoding\n+\n+[Speculative decoding](https://hf.co/papers/2211.17192) uses a smaller assistant model to draft candidate tokens. The main model checks these tokens in one pass. Validated tokens enter the final output and rejected tokens trigger standard sampling. Generation is faster because the main model runs fewer expensive forward passes.\n+\n+The method works best when the assistant model is significantly smaller than the main model and uses the same tokenizer. Speculative decoding supports greedy search and sampling but not batched inputs.\n+\n+Pass `assistant_model` to [`~GenerationMixin.generate`]. Set `do_sample=True` to resample if token validation fails.\n+\n+<hfoptions id=\"spec-decoding\">\n+<hfoption id=\"greedy search\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", dtype=\"auto\")\n+assistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", dtype=\"auto\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\")\n+\n+outputs = model.generate(**inputs, assistant_model=assistant_model)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+'Hugging Face is an open-source company that provides a platform for developers to build and deploy machine'\n+```\n+\n+The `assistant_model` argument is also available in the [`Pipeline`] API.\n+\n+```python\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    \"text-generation\",\n+    model=\"meta-llama/Llama-3.1-8B\",\n+    assistant_model=\"meta-llama/Llama-3.2-1B\",\n+    dtype=\"auto\"\n+)\n+pipeline(\"Hugging Face is an open-source company, \", max_new_tokens=50, do_sample=False)\n+```\n+\n+</hfoption>\n+<hfoption id=\"sampling\">\n+\n+Set `temperature` to control randomness. Lower temperatures often improve latency.\n+\n+```py\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", dtype=\"auto\")\n+assistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", dtype=\"auto\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\")\n+\n+outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+'Hugging Face is an open-source company that is dedicated to creating a better world through technology.'\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## Prompt lookup decoding\n+\n+Prompt lookup decoding doesn't need an assistant model. It finds overlapping n-grams in the prompt to propose candidate tokens. If no match exists, it falls back to normal autoregressive decoding. This suits input-grounded tasks like summarization and translation because candidate tokens often mirror local patterns in the source text.\n+\n+Pass `prompt_lookup_num_tokens` to [`~GenerationMixin.generate`]. This sets how many tokens the algorithm tries to copy from earlier in the prompt when it detects a repeated pattern.\n+\n+<hfoptions id=\"prompt-lookup-decoding\">\n+<hfoption id=\"greedy decoding\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", dtype=\"auto\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\")\n+\n+outputs = model.generate(**inputs, prompt_lookup_num_tokens=5)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+'Hugging Face is an open-source company that provides a platform for developers to build and deploy machine learning models. It offers a variety of tools'\n+```\n+\n+</hfoption>\n+<hfoption id=\"sampling\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n+model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", dtype=\"auto\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\")\n+\n+outputs = model.generate(**inputs, prompt_lookup_num_tokens=5, do_sample=True, temperature=0.5)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+'Hugging Face is an open-source company that provides a platform for developers to build and deploy machine learning models. It offers a variety of tools'\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## Self-speculative decoding\n+\n+Self-speculative decoding uses a model's intermediate layers as the assistant to propose candidate tokens. If the proposal matches, the model exits early and the remaining layers verify or correct the tokens.\n+\n+Because it's all one model, weights and caches are shared, which boosts speed without extra memory overhead. This technique only works for models trained to support early-exit logits from intermediate layers.\n+\n+Pass `assistant_early_exit` to [`~GenerationMixin.generate`] to set the exit layer.\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"facebook/layerskip-llama3.2-1B\")\n+model = AutoModelForCausalLM.from_pretrained(\"facebook/layerskip-llama3.2-1B\", dtype=\"auto\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\")\n+\n+outputs = model.generate(**inputs, assistant_early_exit=4, do_sample=False, max_new_tokens=20)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+```\n+\n+## Universal assisted decoding\n+\n+Universal assisted decoding (UAD) makes speculative decoding possible even when the main and assistant models have different tokenizers. It lets you pair any small assistant model with the main model. Candidate tokens are re-encoded and the algorithm computes the longest common subsequence so the continuation stays aligned.\n+\n+Pass `tokenizer`, `assistant_tokenizer`, and `assistant_model` to [`~GenerationMixin.generate`] to enable UAD.\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+assistant_tokenizer = AutoTokenizer.from_pretrained(\"double7/vicuna-68m\")\n+tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\n+model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\", dtype=\"auto\")\n+assistant_model = AutoModelForCausalLM.from_pretrained(\"double7/vicuna-68m\", dtype=\"auto\")\n+inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\")\n+\n+outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=tokenizer, assistant_tokenizer=assistant_tokenizer)\n+tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+'Hugging Face is an open-source company that is dedicated to creating a better world through technology.'\n+```\n+\n+## Resources\n+\n+- Read the [Assisted Generation: a new direction toward low-latency text generation](https://huggingface.co/blog/assisted-generation) blog post for more context about text generation latency and assisted generation.\n\\ No newline at end of file"
        },
        {
            "sha": "be27501863e3380e8b1fd557fbf21d0ebff0f604",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 0,
            "deletions": 134,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ef16eddf9d9a0d8a50c6b65c2bf352f0c3cace5/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ef16eddf9d9a0d8a50c6b65c2bf352f0c3cace5/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=5ef16eddf9d9a0d8a50c6b65c2bf352f0c3cace5",
            "patch": "@@ -96,140 +96,6 @@ tokenizer.batch_decode(outputs, skip_special_tokens=True)\n \"['Hugging Face is an open-source company that develops and maintains the Hugging Face platform, which is a collection of tools and libraries for building and deploying natural language processing (NLP) models. Hugging Face was founded in 2018 by Thomas Wolf']\"\n ```\n \n-## Advanced decoding methods\n-\n-Advanced decoding methods aim at either tackling specific generation quality issues (e.g. repetition) or at improving the generation throughput in certain situations. These techniques are more complex, and may not work correctly with all models.\n-\n-### Speculative decoding\n-\n-[Speculative](https://hf.co/papers/2211.17192) or assistive decoding isn't a search or sampling strategy. Instead, speculative decoding adds a second smaller model to generate candidate tokens. The main model verifies the candidate tokens in a single `forward` pass, which speeds up the decoding process overall. This method is especially useful for LLMs where it can be more costly and slower to generate tokens. Refer to the [speculative decoding](./llm_optims#speculative-decoding) guide to learn more.\n-\n-Currently, only greedy search and multinomial sampling are supported with speculative decoding. Batched inputs aren't supported either.\n-\n-Enable speculative decoding with the `assistant_model` parameter. You'll notice the fastest speed up with an assistant model that is much smaller than the main model. Add `do_sample=True` to enable token validation with resampling.\n-\n-<hfoptions id=\"spec-decoding\">\n-<hfoption id=\"greedy search\">\n-\n-```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n-model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n-assistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n-inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\")\n-\n-outputs = model.generate(**inputs, assistant_model=assistant_model)\n-tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-'Hugging Face is an open-source company that provides a platform for developers to build and deploy machine'\n-```\n-\n-Speculative decoding is also supported in [`Pipeline`] with the `assistant_model` parameter.\n-\n-```python\n-from transformers import pipeline\n-import torch\n-\n-pipe = pipeline(\n-    \"text-generation\",\n-    model=\"meta-llama/Llama-3.1-8B\",\n-    assistant_model=\"meta-llama/Llama-3.2-1B\",\n-    dtype=torch.bfloat16\n-)\n-pipe_output = pipe(\"Once upon a time, \", max_new_tokens=50, do_sample=False)\n-pipe_output[0][\"generated_text\"]\n-```\n-\n-</hfoption>\n-<hfoption id=\"multinomial sampling\">\n-\n-Add the `temperature` parameter to control sampling randomness. For speculative decoding, a lower temperature may improve latency.\n-\n-```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n-model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n-assistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n-inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\")\n-\n-outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)\n-tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-'Hugging Face is an open-source company that is dedicated to creating a better world through technology.'\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n-#### Prompt lookup decoding\n-\n-[Prompt lookup decoding](./llm_optims#prompt-lookup-decoding) is a variant of speculative decoding that uses overlapping n-grams as the candidate tokens. It works well for input-grounded tasks such as summarization. Refer to the [prompt lookup decoding](./llm_optims#prompt-lookup-decoding) guide to learn more.\n-\n-Enable prompt lookup decoding with the `prompt_lookup_num_tokens` parameter.\n-\n-```py\n-import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-from accelerate import Accelerator\n-\n-device = Accelerator().device\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n-model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", dtype=torch.float16).to(device)\n-assistant_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", dtype=torch.float16).to(device)\n-inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n-\n-outputs = model.generate(**inputs, assistant_model=assistant_model, max_new_tokens=20, prompt_lookup_num_tokens=5)\n-tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-'Hugging Face is an open-source company that provides a platform for developers to build and deploy machine learning models. It offers a variety of tools'\n-```\n-\n-### Self-speculative decoding\n-\n-Early exiting uses the earlier hidden states from the language modeling head as inputs, effectively skipping layers to yield a lower quality output. The lower quality output is used as the assistant output and self-speculation is applied to fix the output using the remaining layers. The final generated result from this self-speculative method is the same (or has the same distribution) as the original models generation.\n-\n-The assistant model is also part of the target model, so the caches and weights can be shared, resulting in lower memory requirements.\n-\n-For a model trained with early exit, pass `assistant_early_exit` to [`~GenerationMixin.generate`].\n-\n-```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-prompt = \"Alice and Bob\"\n-checkpoint = \"facebook/layerskip-llama3.2-1B\"\n-\n-tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n-inputs = tokenizer(prompt, return_tensors=\"pt\")\n-\n-model = AutoModelForCausalLM.from_pretrained(checkpoint)\n-outputs = model.generate(**inputs, assistant_early_exit=4, do_sample=False, max_new_tokens=20)\n-tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-```\n-\n-#### Universal assisted decoding\n-\n-Universal assisted decoding (UAD) enables the main and assistant models to use different tokenizers. The main models input tokens are re-encoded into assistant model tokens. Candidate tokens are generated in the assistant encoding which are re-encoded into the main model candidate tokens. The candidate tokens are verified as explained in [speculative decoding](#speculative-decoding).\n-\n-Re-encoding involves decoding token ids into text and encoding the text with a different tokenizer. To prevent tokenization discrepancies during re-encoding, UAD finds the longest common sub-sequence between the source and target encodings to ensure the new tokens include the correct prompt suffix.\n-\n-Add the `tokenizer` and `assistant_tokenizer` parameters to [`~GenerationMixin.generate`] to enable UAD.\n-\n-```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-prompt = \"Alice and Bob\"\n-\n-assistant_tokenizer = AutoTokenizer.from_pretrained(\"double7/vicuna-68m\")\n-tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\n-inputs = tokenizer(prompt, return_tensors=\"pt\")\n-\n-model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b\")\n-assistant_model = AutoModelForCausalLM.from_pretrained(\"double7/vicuna-68m\")\n-outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=tokenizer, assistant_tokenizer=assistant_tokenizer)\n-tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']\n-```\n-\n ## Custom generation methods\n \n Custom generation methods enable specialized behavior such as:"
        },
        {
            "sha": "2cb50ba2879a4d0b4a9c9323eb7e239645a47c59",
            "filename": "docs/source/en/llm_optims.md",
            "status": "removed",
            "additions": 0,
            "deletions": 401,
            "changes": 401,
            "blob_url": "https://github.com/huggingface/transformers/blob/12fe95f88b8beb31344c905bb4931efcafe0d044/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/12fe95f88b8beb31344c905bb4931efcafe0d044/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=12fe95f88b8beb31344c905bb4931efcafe0d044",
            "patch": "@@ -1,401 +0,0 @@\n-<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-http://www.apache.org/licenses/LICENSE-2.0\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n--->\n-\n-# Optimizing inference\n-\n-Inference with large language models (LLMs) can be challenging because they have to store and handle billions of parameters. To load a 70B parameter [Llama 2](https://hf.co/meta-llama/Llama-2-70b-hf) model, it requires 256GB of memory for full precision weights and 128GB of memory for half-precision weights. The most powerful GPUs today - the A100 and H100 - only have 80GB of memory.\n-\n-On top of the memory requirements, inference is slow because LLMs are called repeatedly to generate the next token. The input sequence increases as generation progresses, which takes longer and longer to process.\n-\n-This guide will show you how to optimize LLM inference to accelerate generation and reduce memory usage.\n-\n-> [!TIP]\n-> Try out [Text Generation Inference (TGI)](https://hf.co/docs/text-generation-inference), a Hugging Face library dedicated to deploying and serving highly optimized LLMs for inference.\n-\n-## Static kv-cache and torch.compile\n-\n-LLMs compute key-value (kv) values for each input token, and it performs the same kv computation each time because the generated output becomes part of the input. However, performing the same kv computation every time is not very efficient.\n-\n-A *kv-cache* stores the past keys and values instead of recomputing them each time. As a result, the kv-cache is dynamic and it grows with each generation step which prevents you from taking advantage of [torch.compile](./perf_torch_compile), a powerful optimization method that fuses PyTorch code into optimized kernels.\n-\n-The *static kv-cache* solves this issue by pre-allocating the kv-cache size to a maximum value, so you can combine it with [torch.compile](./perf_torch_compile) for up to a 4x speed up. Your speed up may vary depending on the model size (larger models have a smaller speed up) and hardware.\n-\n-> [!WARNING]\n-> Follow this [issue](https://github.com/huggingface/transformers/issues/28981) to track which models (Llama, Gemma, Mistral, etc.) support a static kv-cache and torch.compile.\n-\n-Depending on your task, there are several ways you can use the static kv-cache.\n-\n-1. For basic use cases, set [cache_implementation](https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation) to `\"static\"` (recommended).\n-2. For multi-turn generation or a custom generation loop, initialize and handle [`StaticCache`] directly.\n-3. For more unique hardware or use cases, it may be better to compile the entire [`~GenerationMixin.generate`] function into a single graph.\n-\n-> [!TIP]\n-> Regardless of how you use the static kv-cache and torch.compile, left-pad your inputs with [pad_to_multiple_of](https://hf.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.pad_to_multiple_of) to a limited set of values to avoid shape-related recompilations.\n-\n-<hfoptions id=\"static-kv\">\n-<hfoption id=\"1. cache_implementation\">\n-\n-1. Set the [cache_implementation](https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation) to `\"static\"` in a models [`GenerationConfig`].\n-2. Call [torch.compile](./perf_torch_compile) to compile the forward pass with the static kv-cache.\n-\n-```py\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n-import torch\n-import os\n-os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n-model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", dtype=\"auto\", device_map=\"auto\")\n-\n-model.generation_config.cache_implementation = \"static\"\n-\n-model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n-input_text = \"The theory of special relativity states \"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n-\n-outputs = model.generate(**input_ids)\n-print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n-['The theory of special relativity states 1. The speed of light is constant in all inertial reference']\n-```\n-\n-Under the hood, [`~GenerationMixin.generate`] attempts to reuse the same cache object to avoid recompilation at each call, which is critical to get the most out of [torch.compile](./perf_torch_compile). Be aware of the following to avoid triggering recompilation or if generation is slower than expected.\n-\n-1. If the batch size changes or the maximum output length increases between calls, the cache is reinitialized and recompiled.\n-2. The first several calls of the compiled function are slower because it is being compiled.\n-\n-</hfoption>\n-<hfoption id=\"2. StaticCache\">\n-\n-Directly initialize a [`StaticCache`] object and pass it to the `past_key_values` parameter in [`~GenerationMixin.generate`]. The [`StaticCache`] keeps the cache contents, so you can pass it to a new [`~GenerationMixin.generate`] call to continue generation, similar to a dynamic cache.\n-\n-```py\n-from transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache\n-import torch\n-import os\n-os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n-model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", dtype=\"auto\", device_map=\"auto\")\n-\n-model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n-input_text = \"The theory of special relativity states \"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n-prompt_length = input_ids.input_ids.shape[1]\n-model.generation_config.max_new_tokens = 16\n-\n-past_key_values = StaticCache(\n-    config=model.config,\n-    # If you plan to reuse the cache, make sure the cache length is large enough for all cases\n-    max_cache_len=prompt_length+(model.generation_config.max_new_tokens*2),\n-)\n-outputs = model.generate(**input_ids, past_key_values=past_key_values)\n-print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n-['The theory of special relativity states 1. The speed of light is constant in all inertial reference frames. 2']\n-\n-# pass in the generated text and the same cache object to continue generation from where it left off. Optionally, in a\n-# multi-turn conversation, append the new user input to the generated text.\n-new_input_ids = outputs\n-outputs = model.generate(new_input_ids, past_key_values=past_key_values)\n-print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n-['The theory of special relativity states 1. The speed of light is constant in all inertial reference frames. 2. The speed of light is constant in all inertial reference frames. 3.']\n-```\n-\n-> [!TIP]\n-> To reuse [`StaticCache`] on a new prompt, use [`~StaticCache.reset`] to reset the cache contents between calls.\n-\n-Another option for using [`StaticCache`] is to pass it to a models forward pass using the same `past_key_values` argument. This allows you to write your own custom decoding function to decode the next token given the current token, position, and cache position of previously generated tokens.\n-\n-```py\n-from transformers import LlamaTokenizer, LlamaForCausalLM, StaticCache, logging\n-from accelerate import Accelerator\n-from transformers.testing_utils import CaptureLogger\n-import torch\n-\n-prompts = [\n-    \"Simply put, the theory of relativity states that \",\n-    \"My favorite all time favorite condiment is ketchup.\",\n-]\n-\n-NUM_TOKENS_TO_GENERATE = 40\n-torch_device = Accelerator().device\n-\n-tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", pad_token=\"</s>\", padding_side=\"right\")\n-model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"sequential\")\n-inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n-\n-def decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_values):\n-    logits = model(\n-        cur_token,\n-        position_ids=input_pos,\n-        cache_position=cache_position,\n-        past_key_values=past_key_values,\n-        return_dict=False,\n-        use_cache=True\n-    )[0]\n-    new_token = torch.argmax(logits[:, -1], dim=-1)[:, None]\n-    return new_token\n-```\n-\n-To enable static kv-cache and [torch.compile](./perf_torch_compile) with [`StaticCache`], follow the steps below.\n-\n-1. Initialize [`StaticCache`] before using the model for inference to configure parameters like the maximum batch size and sequence length.\n-2. Call [torch.compile](./perf_torch_compile) on the model to compile the forward pass with the static kv-cache.\n-3. se SDPBackend.MATH in the [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) context manager to enable the native PyTorch C++ implementation of scaled dot product attention to speed up inference even more.\n-\n-```py\n-from torch.nn.attention import SDPBackend, sdpa_kernel\n-\n-batch_size, seq_length = inputs[\"input_ids\"].shape\n-with torch.no_grad():\n-    past_key_values = StaticCache(\n-        config=model.config, max_cache_len=4096\n-    )\n-    cache_position = torch.arange(seq_length, device=torch_device)\n-    generated_ids = torch.zeros(\n-        batch_size, seq_length + NUM_TOKENS_TO_GENERATE + 1, dtype=torch.int, device=torch_device\n-    )\n-    generated_ids[:, cache_position] = inputs[\"input_ids\"].to(torch_device).to(torch.int)\n-\n-    logits = model(\n-        **inputs, cache_position=cache_position, past_key_values=past_key_values,return_dict=False, use_cache=True\n-    )[0]\n-    next_token = torch.argmax(logits[:, -1], dim=-1)[:, None]\n-    generated_ids[:, seq_length] = next_token[:, 0]\n-\n-    decode_one_tokens = torch.compile(decode_one_tokens, mode=\"reduce-overhead\", fullgraph=True)\n-    cache_position = torch.tensor([seq_length + 1], device=torch_device)\n-    for _ in range(1, NUM_TOKENS_TO_GENERATE):\n-        with sdpa_kernel(SDPBackend.MATH):\n-            next_token = decode_one_tokens(model, next_token.clone(), None, cache_position, past_key_values)\n-            generated_ids[:, cache_position] = next_token.int()\n-        cache_position += 1\n-\n-text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n-text\n-['Simply put, the theory of relativity states that 1) the speed of light is constant, 2) the speed of light is the same for all observers, and 3) the laws of physics are the same for all observers.',\n- 'My favorite all time favorite condiment is ketchup. I love it on everything. I love it on my eggs, my fries, my chicken, my burgers, my hot dogs, my sandwiches, my salads, my p']\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n-## Decoding strategies\n-\n-Decoding can also be optimized to accelerate generation. You can use a lightweight assistant model to generate candidate tokens faster than the LLM itself or you can use a variant of this decoding strategy that works especially well for input-grounded tasks.\n-\n-### Speculative decoding\n-\n-> [!TIP]\n-> For a more in-depth explanation, take a look at the [Assisted Generation: a new direction toward low-latency text generation](https://hf.co/blog/assisted-generation) blog post!\n-\n-For each input token, the model weights are loaded each time during the forward pass, which is slow and cumbersome when a model has billions of parameters. Speculative decoding alleviates this slowdown by using a second smaller and faster assistant model to generate candidate tokens that are verified by the larger model in a single forward pass. If the verified tokens are correct, the LLM essentially gets them for \"free\" without having to generate them itself. There is no degradation in accuracy because the verification forward pass ensures the same outputs are generated as if the LLM had generated them on its own.\n-\n-To get the largest speed up, the assistant model should be a lot smaller than the LLM so that it can generate tokens quickly. The assistant and LLM model must also share the same tokenizer to avoid re-encoding and decoding tokens.\n-\n-> [!WARNING]\n-> Speculative decoding is only supported for the greedy search and sampling decoding strategies, and it doesn't support batched inputs.\n-\n-Enable speculative decoding by loading an assistant model and passing it to [`~GenerationMixin.generate`].\n-\n-<hfoptions id=\"spec-decoding\">\n-<hfoption id=\"greedy search\">\n-\n-```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-from accelerate import Accelerator\n-import torch\n-\n-device = Accelerator().device\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n-inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n-\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", dtype=\"auto\").to(device)\n-assistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n-outputs = model.generate(**inputs, assistant_model=assistant_model)\n-tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-[\"Einstein's theory of relativity states that the speed of light is constant.    \"]\n-```\n-\n-</hfoption>\n-<hfoption id=\"sampling\">\n-\n-For speculative sampling decoding, add the [do_sample](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.do_sample) and [temperature](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.temperature) parameters to [`~GenerationMixin.generate`].\n-\n-```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-from accelerate import Accelerator\n-import torch\n-\n-device = Accelerator().device\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n-inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n-\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", dtype=\"auto\").to(device)\n-assistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n-outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.7)\n-print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n-[\"Einstein's theory of relativity states that motion in the universe is not a straight line.\\n\"]\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n-### Prompt lookup decoding\n-\n-Prompt lookup decoding is a variant of speculative decoding that is also compatible with greedy search and sampling. Prompt lookup works especially well for input-grounded tasks - such as summarization - where there is often overlapping words between the prompt and output. These overlapping n-grams are used as the LLM candidate tokens.\n-\n-To enable prompt lookup decoding, specify the number of tokens that should be overlapping in the [prompt_lookup_num_tokens](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.prompt_lookup_num_tokens) parameter. Then pass this parameter to [`~GenerationMixin.generate`].\n-\n-<hfoptions id=\"pld\">\n-<hfoption id=\"greedy decoding\">\n-\n-```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-from accelerate import Accelerator\n-import torch\n-\n-device = Accelerator().device\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n-inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)\n-\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", dtype=\"auto\").to(device)\n-assistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n-outputs = model.generate(**inputs, prompt_lookup_num_tokens=3)\n-print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n-['The second law of thermodynamics states that entropy increases with temperature.      ']\n-```\n-\n-</hfoption>\n-<hfoption id=\"sampling\">\n-\n-For prompt lookup decoding with sampling, add the [do_sample](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.do_sample) and [temperature](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.temperature) parameters to [`~GenerationMixin.generate`].\n-\n-```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-from accelerate import Accelerator\n-import torch\n-\n-device = Accelerator().device\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n-inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)\n-\n-model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", dtype=\"auto\").to(device)\n-outputs = model.generate(**inputs, prompt_lookup_num_tokens=3, do_sample=True, temperature=0.7)\n-print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n-[\"The second law of thermodynamics states that energy cannot be created nor destroyed. It's not a\"]\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n-## Attention\n-\n-A known issue with transformer models is that the self-attention mechanism grows quadratically in compute and memory with the number of input tokens. This limitation is only magnified in LLMs which handles much longer sequences. To address this, try FlashAttention2 or PyTorch's scaled dot product attention (SDPA), which are more memory efficient attention implementations.\n-\n-### FlashAttention-2\n-\n-FlashAttention and [FlashAttention-2](./perf_infer_gpu_one#flashattention-2) break up the attention computation into smaller chunks and reduces the number of intermediate read/write operations to the GPU memory to speed up inference. FlashAttention-2 improves on the original FlashAttention algorithm by also parallelizing over sequence length dimension and better partitioning work on the hardware to reduce synchronization and communication overhead.\n-\n-To use FlashAttention-2, set [attn_implementation](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.PreTrainedModel.from_pretrained.attn_implementation) to `\"flash_attention_2\"` in [`~PreTrainedModel.from_pretrained`] or set with `model.set_attention_implementation(\"flash_attention_2\")` to dynamically update the [attention interface](./attention_interface) after the model is loaded.\n-\n-```py\n-from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n-\n-quant_config = BitsAndBytesConfig(load_in_8bit=True)\n-model = AutoModelForCausalLM.from_pretrained(\n-    \"google/gemma-2b\",\n-    quantization_config=quant_config,\n-    dtype=torch.bfloat16,\n-    attn_implementation=\"flash_attention_2\",\n-)\n-\n-# Change the model's attention dynamically after loading\n-model = AutoModelForCausalLM.from_pretrained(\n-    \"google/gemma-2b\",\n-    quantization_config=quant_config,\n-    dtype=torch.bfloat16\n-)\n-model.set_attention_implementation(\"flash_attention_2\")\n-```\n-\n-### PyTorch scaled dot product attention\n-\n-Scaled dot product attention (SDPA) is automatically enabled in PyTorch 2.0 and it supports FlashAttention, xFormers, and PyTorch's C++ implementation. SDPA chooses the most performant attention algorithm if you're using a CUDA backend. For other backends, SDPA defaults to the PyTorch C++ implementation.\n-\n-> [!TIP]\n-> SDPA automatically supports FlashAttention-2 as long as you have the latest PyTorch version installed.\n-\n-Use the [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) context manager to explicitly enable or disable any of the four attention algorithms. For example, use `SDPBackend.FLASH_ATTENTION` to enable FlashAttention.\n-\n-```py\n-import torch\n-from torch.nn.attention import SDPBackend, sdpa_kernel\n-from transformers import AutoModelForCausalLM\n-\n-model = AutoModelForCausalLM.from_pretrained(\n-    \"google/gemma-2b\",\n-    dtype=torch.bfloat16,\n-)\n-\n-with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n-    outputs = model.generate(**inputs)\n-```\n-\n-## Quantization\n-\n-Quantization reduces the size of model weights by storing them in a lower precision. This translates to lower memory usage and makes loading LLMs for inference more accessible if you're constrained by GPU memory.\n-\n-If you aren't limited by your GPU, you don't necessarily need to quantize your model because it can increase latency slightly (except for AWQ and fused AWQ modules) due to the extra step required to quantize and dequantize the weights.\n-\n-> [!TIP]\n-> There are many quantization libraries (see the [Quantization](./quantization) guide for more details) available, such as Quanto, AQLM, VPTQ, AWQ, and GPT-QModel. Feel free to try them out and see which one works best for your use case. We also recommend reading the [Overview of natively supported quantization schemes in ðŸ¤— Transformers](https://hf.co/blog/overview-quantization-transformers) blog post for a comparison of different approaches.\n-\n-Use the Model Memory Calculator below to estimate and compare how much memory is required to load a model. For example, try estimating the memory required to load [Mistral-7B-v0.1](https://hf.co/mistralai/Mistral-7B-v0.1).\n-\n-<iframe\n-\tsrc=\"https://hf-accelerate-model-memory-usage.hf.space\"\n-\tframeborder=\"0\"\n-\twidth=\"850\"\n-\theight=\"450\"\n-></iframe>\n-\n-To load a model in half-precision, set the [dtype](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.PreTrainedModel.from_pretrained.dtype) parameter in [`~transformers.AutoModelForCausalLM.from_pretrained`] to `torch.bfloat16`. This requires 13.74GB of memory.\n-\n-```py\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n-import torch\n-\n-model = AutoModelForCausalLM.from_pretrained(\n-    \"mistralai/Mistral-7B-v0.1\", dtype=torch.bfloat16, device_map=\"auto\",\n-)\n-```\n-\n-To load a quantized model (8-bit or 4-bit), try [bitsandbytes](https://hf.co/docs/bitsandbytes) and set the [load_in_4bit](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.BitsAndBytesConfig.load_in_4bit) or [load_in_8bit](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.BitsAndBytesConfig.load_in_8bit) parameters to `True`. Loading the model in 8-bits only requires 6.87 GB of memory.\n-\n-```py\n-from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n-import torch\n-\n-quant_config = BitsAndBytesConfig(load_in_8bit=True)\n-model = AutoModelForCausalLM.from_pretrained(\n-    \"mistralai/Mistral-7B-v0.1\", quantization_config=quant_config, device_map=\"auto\"\n-)\n-```\n-\n-## Continuous Batching\n-\n-When serving LLMs for inference, you may have multiple requests arriving at different times. Continuous Batching (CB) is a technique that groups incoming requests into batches to maximize GPU utilization and throughput.\n-\n-See the [Continuous Batching](./continuous_batching) guide for more details on how to use CB in transformers."
        },
        {
            "sha": "a6900cb6de5ecda907519d28c1116c7fc69039ec",
            "filename": "docs/source/en/perf_infer_cpu.md",
            "status": "removed",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/12fe95f88b8beb31344c905bb4931efcafe0d044/docs%2Fsource%2Fen%2Fperf_infer_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/12fe95f88b8beb31344c905bb4931efcafe0d044/docs%2Fsource%2Fen%2Fperf_infer_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_cpu.md?ref=12fe95f88b8beb31344c905bb4931efcafe0d044",
            "patch": "@@ -1,40 +0,0 @@\n-<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# CPU\n-\n-CPUs are a viable and cost-effective inference option. With a few optimization methods, it is possible to achieve good performance with large models on CPUs. These methods include fusing kernels to reduce overhead and compiling your code to a faster intermediate format that can be deployed in production environments.\n-\n-This guide will show you a few ways to optimize inference on a CPU.\n-\n-## Optimum\n-\n-[Optimum](https://hf.co/docs/optimum/en/index) is a Hugging Face library focused on optimizing model performance across various hardware. It supports [ONNX Runtime](https://onnxruntime.ai/docs/) (ORT), a model accelerator, for a wide range of hardware and frameworks including CPUs.\n-\n-Optimum provides the [`~optimum.onnxruntime.ORTModel`] class for loading ONNX models. For example, load the [optimum/roberta-base-squad2](https://hf.co/optimum/roberta-base-squad2) checkpoint for question answering inference. This checkpoint contains a [model.onnx](https://hf.co/optimum/roberta-base-squad2/blob/main/model.onnx) file.\n-\n-```py\n-from transformers import AutoTokenizer, pipeline\n-from optimum.onnxruntime import ORTModelForQuestionAnswering\n-\n-onnx_qa = pipeline(\"question-answering\", model=\"optimum/roberta-base-squad2\", tokenizer=\"deepset/roberta-base-squad2\")\n-\n-question = \"What's my name?\"\n-context = \"My name is Philipp and I live in Nuremberg.\"\n-pred = onnx_qa(question, context)\n-```\n-\n-> [!TIP]\n-> Optimum includes an [Intel](https://hf.co/docs/optimum/intel/index) extension that provides additional optimizations such as quantization, pruning, and knowledge distillation for Intel CPUs. This extension also includes tools to convert models to [OpenVINO](https://hf.co/docs/optimum/intel/inference), a toolkit for optimizing and deploying models, for even faster inference."
        },
        {
            "sha": "e4f28973326d66908dbf26f15b50e8546f953a5c",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "removed",
            "additions": 0,
            "deletions": 269,
            "changes": 269,
            "blob_url": "https://github.com/huggingface/transformers/blob/12fe95f88b8beb31344c905bb4931efcafe0d044/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/12fe95f88b8beb31344c905bb4931efcafe0d044/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=12fe95f88b8beb31344c905bb4931efcafe0d044",
            "patch": "@@ -1,269 +0,0 @@\n-<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# GPU\n-\n-GPUs are the standard hardware for machine learning because they're optimized for memory bandwidth and parallelism. With the increasing sizes of modern models, it's more important than ever to make sure GPUs are capable of efficiently handling and delivering the best possible performance.\n-\n-This guide will demonstrate a few ways to optimize inference on a GPU. The optimization methods shown below can be combined with each other to achieve even better performance, and they also work for distributed GPUs.\n-\n-## bitsandbytes\n-\n-[bitsandbytes](https://hf.co/docs/bitsandbytes/index) is a quantization library that supports 8-bit and 4-bit quantization. Quantization represents weights in a lower precision compared to the original full precision format. It reduces memory requirements and makes it easier to fit large model into memory.\n-\n-Make sure bitsandbytes and Accelerate are installed first.\n-\n-```bash\n-pip install bitsandbytes accelerate\n-```\n-\n-<hfoptions id=\"bnb\">\n-<hfoption id=\"8-bit\">\n-\n-For text generation with 8-bit quantization, you should use [`~GenerationMixin.generate`] instead of the high-level [`Pipeline`] API. The [`Pipeline`] returns slower performance because it isn't optimized for 8-bit models, and some sampling strategies (nucleus sampling) also aren't supported.\n-\n-Set up a [`BitsAndBytesConfig`] and set `load_in_8bit=True` to load a model in 8-bit precision. The [`BitsAndBytesConfig`] is passed to the `quantization_config` parameter in [`~PreTrainedModel.from_pretrained`].\n-\n-Allow Accelerate to automatically distribute the model across your available hardware by setting [device_map=\"auto\"](https://hf.co/docs/accelerate/concept_guides/big_model_inference#designing-a-device-map).\n-\n-Place all inputs on the same device as the model.\n-\n-```py\n-from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n-\n-quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", quantization_config=quantization_config)\n-\n-prompt = \"Hello, my llama is cute\"\n-inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n-generated_ids = model.generate(**inputs)\n-outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n-```\n-\n-For distributed setups, use the `max_memory` parameter to create a mapping of the amount of memory to allocate to each GPU. The example below distributes 16GB of memory to the first GPU and 16GB of memory to the second GPU.\n-\n-```py\n-max_memory_mapping = {0: \"16GB\", 1: \"16GB\"}\n-model_8bit = AutoModelForCausalLM.from_pretrained(\n-    \"meta-llama/Llama-3.1-8B\", device_map=\"auto\", quantization_config=quantization_config, max_memory=max_memory_mapping\n-)\n-```\n-\n-Learn in more detail the concepts underlying 8-bit quantization in the [Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://hf.co/blog/hf-bitsandbytes-integration) blog post.\n-\n-</hfoption>\n-<hfoption id=\"4-bit\">\n-\n-Set up a [`BitsAndBytesConfig`] and set `load_in_4bit=True` to load a model in 4-bit precision. The [`BitsAndBytesConfig`] is passed to the `quantization_config` parameter in [`~PreTrainedModel.from_pretrained`].\n-\n-Allow Accelerate to automatically distribute the model across your available hardware by setting `device_map=\"auto\"`.\n-\n-Place all inputs on the same device as the model.\n-\n-```py\n-from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n-\n-quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n-tokenizer = AutoTokenizer(\"meta-llama/Llama-3.1-8B\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", quantization_config=quantization_config)\n-\n-prompt = \"Hello, my llama is cute\"\n-inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_8bit.device)\n-generated_ids = model_8bit.generate(**inputs)\n-outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n-```\n-\n-For distributed setups, use the `max_memory` parameter to create a mapping of the amount of memory to allocate to each GPU. The example below distributes 16GB of memory to the first GPU and 16GB of memory to the second GPU.\n-\n-```py\n-max_memory_mapping = {0: \"16GB\", 1: \"16GB\"}\n-model_4bit = AutoModelForCausalLM.from_pretrained(\n-    \"meta-llama/Llama-3.1-8B\", device_map=\"auto\", quantization_config=quantization_config, max_memory=max_memory_mapping\n-)\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n-## Optimum\n-\n-[Optimum](https://hf.co/docs/optimum/en/index) is a Hugging Face library focused on optimizing model performance across various hardware. It supports [ONNX Runtime](https://onnxruntime.ai/docs/) (ORT), a model accelerator, for a wide range of hardware and frameworks including NVIDIA GPUs and AMD GPUs that use the [ROCm](https://www.amd.com/en/products/software/rocm.html) stack.\n-\n-ORT uses optimization techniques that fuse common operations into a single node and constant folding to reduce the number of computations. ORT also places the most computationally intensive operations on the GPU and the rest on the CPU to intelligently distribute the workload between the two devices.\n-\n-Optimum provides the [`~optimum.onnxruntime.ORTModel`] class for loading ONNX models. Set the `provider` parameter according to the table below.\n-\n-| provider | hardware |\n-|---|---|\n-| [CUDAExecutionProvider](https://hf.co/docs/optimum/main/en/onnxruntime/usage_guides/gpu#cudaexecutionprovider) | CUDA-enabled GPUs |\n-| [ROCMExecutionProvider](https://hf.co/docs/optimum/onnxruntime/usage_guides/amdgpu) | AMD Instinct, Radeon Pro, Radeon GPUs |\n-| [TensorrtExecutionProvider](https://hf.co/docs/optimum/onnxruntime/usage_guides/gpu#tensorrtexecutionprovider) | TensorRT |\n-\n-For example, load the [distilbert/distilbert-base-uncased-finetuned-sst-2-english](https://hf.co/optimum/roberta-base-squad2) checkpoint for sequence classification. This checkpoint contains a [model.onnx](https://hf.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english/blob/main/onnx/model.onnx) file. If a checkpoint doesn't have a `model.onnx` file, set `export=True` to convert a checkpoint on the fly to the ONNX format.\n-\n-```py\n-from optimum.onnxruntime import ORTModelForSequenceClassification\n-\n-ort_model = ORTModelForSequenceClassification.from_pretrained(\n-  \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n-  #export=True,\n-  provider=\"CUDAExecutionProvider\",\n-)\n-```\n-\n-Now you can use the model for inference in a [`Pipeline`].\n-\n-```py\n-from optimum.pipelines import pipeline\n-from transformers import AutoTokenizer\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n-pipeline = pipeline(task=\"text-classification\", model=ort_model, tokenizer=tokenizer, device=\"cuda:0\")\n-result = pipeline(\"Both the music and visual were astounding, not to mention the actors performance.\")\n-```\n-\n-Learn more details about using ORT with Optimum in the [Accelerated inference on NVIDIA GPUs](https://hf.co/docs/optimum/onnxruntime/usage_guides/gpu#accelerated-inference-on-nvidia-gpus) and [Accelerated inference on AMD GPUs](https://hf.co/docs/optimum/onnxruntime/usage_guides/amdgpu#accelerated-inference-on-amd-gpus) guides.\n-\n-## Scaled dot product attention (SDPA)\n-\n-PyTorch's [torch.nn.functional.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA) is a native implementation of the scaled dot product attention mechanism. SDPA is a more efficient and optimized version of the attention mechanism used in transformer models.\n-\n-There are three supported implementations available.\n-\n-- [FlashAttention2](https://github.com/Dao-AILab/flash-attention) only supports models with the fp16 or bf16 torch type. Make sure to cast your model to the appropriate type first.\n-- [xFormers](https://github.com/facebookresearch/xformers) or Memory-Efficient Attention is able to support models with the fp32 torch type.\n-- C++ implementation of scaled dot product attention\n-\n-SDPA is used by default for PyTorch v2.1.1. and greater when an implementation is available. You could explicitly enable SDPA by setting `attn_implementation=\"sdpa\"` in [`~PreTrainedModel.from_pretrained`] though. Certain attention parameters, such as `output_attentions=True`, are unsupported and returns a warning that Transformers will fall back to the (slower) eager implementation.\n-\n-Refer to the [AttentionInterface](./attention_interface) guide to learn how to change the attention implementation after loading a model.\n-\n-```py\n-from transformers import AutoModelForCausalLM\n-\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", attn_implementation=\"sdpa\")\n-\n-# Change the model's attention dynamically after loading it\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\")\n-model.set_attention_implementation(\"sdpa\")\n-```\n-\n-SDPA selects the most performant implementation available, but you can also explicitly select an implementation with [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel) as a context manager. The example below shows how to enable the FlashAttention2 implementation with `enable_flash=True`.\n-\n-```py\n-import torch\n-from torch.nn.attention import SDPBackend, sdpa_kernel\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\")\n-\n-input_text = \"Hello, my llama is cute\"\n-inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n-\n-with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n-    outputs = model.generate(**inputs)\n-\n-print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n-```\n-\n-If you encounter the following `RuntimeError`, try installing the nightly version of PyTorch which has broader coverage for FlashAttention.\n-\n-```bash\n-RuntimeError: No available kernel. Aborting execution.\n-\n-pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n-```\n-\n-## FlashAttention\n-\n-[FlashAttention](https://github.com/Dao-AILab/flash-attention) is also available as a standalone package. It can significantly speed up inference by:\n-\n-1. additionally parallelizing the attention computation over sequence length\n-2. partitioning the work between GPU threads to reduce communication and shared memory reads/writes between them\n-\n-Install FlashAttention first for the hardware you're using.\n-\n-<hfoptions id=\"install\">\n-<hfoption id=\"NVIDIA\">\n-\n-```bash\n-pip install flash-attn --no-build-isolation\n-```\n-\n-</hfoption>\n-<hfoption id=\"AMD\">\n-\n-FlashAttention2 support is currently limited to Instinct MI210, Instinct MI250 and Instinct MI300. We strongly suggest running this [Dockerfile](https://github.com/huggingface/optimum-amd/tree/main/docker/transformers-pytorch-amd-gpu-flash/Dockerfile) for FlashAttention2 on AMD GPUs.\n-\n-</hfoption>\n-</hfoptions>\n-\n-Enable FlashAttention2 by setting `attn_implementation=\"flash_attention_2\"` in [`~PreTrainedModel.from_pretrained`] or by setting `model.set_attention_implementation(\"flash_attention_2\")` to dynamically update the [attention interface](./attention_interface). FlashAttention2 is only supported for models with the fp16 or bf16 torch type. Make sure to cast your model to the appropriate data type first.\n-\n-```py\n-from transformers import AutoModelForCausalLM\n-\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\")\n-```\n-\n-### Benchmarks\n-\n-FlashAttention2 speeds up inference considerably especially for inputs with long sequences. However, since FlashAttention2 doesn't support computing attention scores with padding tokens, you must manually pad and unpad the attention scores for batched inference if a sequence contains padding tokens. The downside is batched generation is slower with padding tokens.\n-\n-<hfoptions id=\"padded\">\n-<hfoption id=\"short sequence length\">\n-\n-With a relatively small sequence length, a single forward pass creates overhead leading to a small speed up. The graph below shows the expected speed up for a single forward pass with [meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf) with padding.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-small-seqlen-padding.png\"/>\n-</div>\n-\n-</hfoption>\n-<hfoption id=\"long sequence length\">\n-\n-You can train on much longer sequence lengths without running into out-of-memory issues with FlashAttention2, and potentially reduce memory usage up to 20x. The speed up benefits are even better. The graph below shows the expected speed up for a single forward pass with [meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf) with padding on a longer sequence length.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-large-seqlen-padding.png\"/>\n-</div>\n-\n-</hfoption>\n-</hfoptions>\n-\n-To avoid this slowdown, use FlashAttention2 without padding tokens in the sequence during training. Pack the dataset or concatenate sequences until reaching the maximum sequence length.\n-\n-<hfoptions id=\"not-padded\">\n-<hfoption id=\"tiiuae/falcon-7b\">\n-\n-The graph below shows the expected speed up for a single forward pass with [tiiuae/falcon-7b](https://hf.co/tiiuae/falcon-7b) with a sequence length of 4096 and various batch sizes without padding tokens.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/falcon-7b-inference-large-seqlen.png\"/>\n-</div>\n-\n-</hfoption>\n-<hfoption id=\"meta-llama/Llama-7b-hf\">\n-\n-The graph below shows the expected speed up for a single forward pass with [meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf) with a sequence length of 4096 and various batch sizes without padding tokens.\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-7b-inference-large-seqlen.png\"/>\n-</div>\n-\n-</hfoption>\n-</hfoptions>"
        },
        {
            "sha": "db9e8fb754e2647ba294c79176cd0892570246c5",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ef16eddf9d9a0d8a50c6b65c2bf352f0c3cace5/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ef16eddf9d9a0d8a50c6b65c2bf352f0c3cace5/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=5ef16eddf9d9a0d8a50c6b65c2bf352f0c3cace5",
            "patch": "@@ -245,8 +245,6 @@ docs/source/en/notebooks.md\n docs/source/en/pad_truncation.md\n docs/source/en/peft.md\n docs/source/en/perf_hardware.md\n-docs/source/en/perf_infer_cpu.md\n-docs/source/en/perf_infer_gpu_one.md\n docs/source/en/perf_torch_compile.md\n docs/source/en/perf_train_cpu.md\n docs/source/en/perf_train_cpu_many.md"
        }
    ],
    "stats": {
        "total": 1021,
        "additions": 169,
        "deletions": 852
    }
}