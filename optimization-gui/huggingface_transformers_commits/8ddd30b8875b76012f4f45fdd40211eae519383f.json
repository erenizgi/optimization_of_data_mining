{
    "author": "LysandreJik",
    "message": "Future annotations break FastAPI (#42450)",
    "sha": "8ddd30b8875b76012f4f45fdd40211eae519383f",
    "files": [
        {
            "sha": "75e1c7f1e628c32a6a96986668cd4377868f28e4",
            "filename": "benchmark_v2/framework/benchmark_runner.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ddd30b8875b76012f4f45fdd40211eae519383f/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ddd30b8875b76012f4f45fdd40211eae519383f/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_runner.py?ref=8ddd30b8875b76012f4f45fdd40211eae519383f",
            "patch": "@@ -10,7 +10,6 @@\n from queue import Queue\n from typing import Any\n \n-import numpy as np\n import torch\n from datasets import Dataset\n from huggingface_hub import HfApi"
        },
        {
            "sha": "049c2de62d6c9f582b400ceb4d8e2ce60d67feda",
            "filename": "src/transformers/cli/serve.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ddd30b8875b76012f4f45fdd40211eae519383f/src%2Ftransformers%2Fcli%2Fserve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ddd30b8875b76012f4f45fdd40211eae519383f/src%2Ftransformers%2Fcli%2Fserve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fserve.py?ref=8ddd30b8875b76012f4f45fdd40211eae519383f",
            "patch": "@@ -11,7 +11,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from __future__ import annotations\n \n import asyncio\n import base64\n@@ -316,9 +315,9 @@ class TimedModel:\n \n     def __init__(\n         self,\n-        model: PreTrainedModel,\n+        model: \"PreTrainedModel\",\n         timeout_seconds: int,\n-        processor: Union[ProcessorMixin, PreTrainedTokenizerFast] | None = None,\n+        processor: Union[\"ProcessorMixin\", \"PreTrainedTokenizerFast\"] | None = None,\n     ):\n         self.model = model\n         self._name_or_path = str(model.name_or_path)\n@@ -666,7 +665,7 @@ def build_chat_completion_chunk(\n         finish_reason: str | None = None,\n         tool_calls: list[ChoiceDeltaToolCall] | None = None,\n         decode_stream: DecodeStream | None = None,\n-        tokenizer: PreTrainedTokenizerFast | None = None,\n+        tokenizer: Optional[\"PreTrainedTokenizerFast\"] = None,\n     ) -> ChatCompletionChunk:\n         \"\"\"\n         Builds a chunk of a streaming OpenAI Chat Completion response.\n@@ -914,7 +913,7 @@ def cancellation_wrapper_buffer(_request_id):\n             return JSONResponse(json_chunk, media_type=\"application/json\")\n \n     @staticmethod\n-    def get_model_modality(model: PreTrainedModel) -> Modality:\n+    def get_model_modality(model: \"PreTrainedModel\") -> Modality:\n         from transformers.models.auto.modeling_auto import (\n             MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n             MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES,\n@@ -1792,7 +1791,9 @@ def _load_model_and_data_processor(self, model_id_and_revision: str):\n         logger.info(f\"Loaded model {model_id_and_revision}\")\n         return model, data_processor\n \n-    def load_model_and_processor(self, model_id_and_revision: str) -> tuple[PreTrainedModel, PreTrainedTokenizerFast]:\n+    def load_model_and_processor(\n+        self, model_id_and_revision: str\n+    ) -> tuple[\"PreTrainedModel\", \"PreTrainedTokenizerFast\"]:\n         \"\"\"\n         Loads the text model and processor from the given model ID and revision into the ServeCommand instance.\n \n@@ -1817,7 +1818,7 @@ def load_model_and_processor(self, model_id_and_revision: str) -> tuple[PreTrain\n \n         return model, processor\n \n-    def load_audio_model_and_processor(self, model_id_and_revision: str) -> tuple[PreTrainedModel, ProcessorMixin]:\n+    def load_audio_model_and_processor(self, model_id_and_revision: str) -> tuple[\"PreTrainedModel\", \"ProcessorMixin\"]:\n         \"\"\"\n         Loads the audio model and processor from the given model ID and revision into the ServeCommand instance.\n "
        }
    ],
    "stats": {
        "total": 16,
        "additions": 8,
        "deletions": 8
    }
}