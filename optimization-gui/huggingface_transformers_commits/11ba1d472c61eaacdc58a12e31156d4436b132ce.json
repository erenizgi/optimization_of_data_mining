{
    "author": "LysandreJik",
    "message": "[Init refactor] Modular changes (#35240)\n\n* Modular changes\r\n\r\n* Gemma\r\n\r\n* Gemma",
    "sha": "11ba1d472c61eaacdc58a12e31156d4436b132ce",
    "files": [
        {
            "sha": "65fb1ca5edef4398753377796fc2609d0f17e1e8",
            "filename": "src/transformers/models/gemma/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 103,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2F__init__.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,111 +13,18 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_sentencepiece_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_gemma\": [\"GemmaConfig\"],\n-}\n-\n-try:\n-    if not is_sentencepiece_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_gemma\"] = [\"GemmaTokenizer\"]\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_gemma_fast\"] = [\"GemmaTokenizerFast\"]\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_gemma\"] = [\n-        \"GemmaForCausalLM\",\n-        \"GemmaModel\",\n-        \"GemmaPreTrainedModel\",\n-        \"GemmaForSequenceClassification\",\n-        \"GemmaForTokenClassification\",\n-    ]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_gemma\"] = [\n-        \"FlaxGemmaForCausalLM\",\n-        \"FlaxGemmaModel\",\n-        \"FlaxGemmaPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_gemma import GemmaConfig\n-\n-    try:\n-        if not is_sentencepiece_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_gemma import GemmaTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_gemma_fast import GemmaTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_gemma import (\n-            GemmaForCausalLM,\n-            GemmaForSequenceClassification,\n-            GemmaForTokenClassification,\n-            GemmaModel,\n-            GemmaPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_gemma import (\n-            FlaxGemmaForCausalLM,\n-            FlaxGemmaModel,\n-            FlaxGemmaPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_gemma import *\n+    from .modeling_flax_gemma import *\n+    from .modeling_gemma import *\n+    from .tokenization_gemma import *\n+    from .tokenization_gemma_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "dfe9739ba6555d9fc61e184b91e15648df784e22",
            "filename": "src/transformers/models/gemma/modeling_flax_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_flax_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_flax_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_flax_gemma.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -772,3 +772,6 @@ def update_inputs_for_generation(self, model_outputs, model_kwargs):\n     _CONFIG_FOR_DOC,\n     real_checkpoint=_REAL_CHECKPOINT_FOR_DOC,\n )\n+\n+\n+__all__ = [\"FlaxGemmaForCausalLM\", \"FlaxGemmaModel\", \"FlaxGemmaPreTrainedModel\"]"
        },
        {
            "sha": "b3253fdd5614e19d625136d58c991e3c8ecf1472",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -1295,4 +1295,10 @@ def forward(\n         )\n \n \n-__all__ = [\"GemmaModel\", \"GemmaForCausalLM\", \"GemmaForSequenceClassification\", \"GemmaForTokenClassification\"]\n+__all__ = [\n+    \"GemmaModel\",\n+    \"GemmaForCausalLM\",\n+    \"GemmaForSequenceClassification\",\n+    \"GemmaForTokenClassification\",\n+    \"GemmaPreTrainedModel\",\n+]"
        },
        {
            "sha": "778ef7e19b65b61cc37773342174a448ae813575",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -36,6 +36,7 @@\n     LlamaForSequenceClassification,\n     LlamaForTokenClassification,\n     LlamaModel,\n+    LlamaPreTrainedModel,\n     apply_rotary_pos_emb,\n     repeat_kv,\n )\n@@ -803,6 +804,10 @@ def forward(\n         return outputs\n \n \n+class GemmaPreTrainedModel(LlamaPreTrainedModel):\n+    pass\n+\n+\n class GemmaModel(LlamaModel):\n     def __init__(self, config: GemmaConfig):\n         super().__init__(config)\n@@ -1040,4 +1045,5 @@ def __init__(self, config):\n     \"GemmaForCausalLM\",\n     \"GemmaForSequenceClassification\",\n     \"GemmaForTokenClassification\",\n+    \"GemmaPreTrainedModel\",\n ]"
        },
        {
            "sha": "0e6f4a20b6d6d7380457940dba89ede8a9ee931e",
            "filename": "src/transformers/models/gemma/tokenization_gemma_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -197,3 +197,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n             output = output + bos_token_id + token_ids_1 + eos_token_id\n \n         return output\n+\n+\n+__all__ = [\"GemmaTokenizerFast\"]"
        },
        {
            "sha": "18905bac42cc6b19f21e069355504e46d070d814",
            "filename": "src/transformers/models/gemma2/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 41,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2F__init__.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,49 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_gemma2\": [\"Gemma2Config\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_gemma2\"] = [\n-        \"Gemma2ForCausalLM\",\n-        \"Gemma2Model\",\n-        \"Gemma2PreTrainedModel\",\n-        \"Gemma2ForSequenceClassification\",\n-        \"Gemma2ForTokenClassification\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_gemma2 import Gemma2Config\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_gemma2 import (\n-            Gemma2ForCausalLM,\n-            Gemma2ForSequenceClassification,\n-            Gemma2ForTokenClassification,\n-            Gemma2Model,\n-            Gemma2PreTrainedModel,\n-        )\n-\n+    from .configuration_gemma2 import *\n+    from .modeling_gemma2 import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "dc2eba7893a0584c13538aeb7d611c27f06e7263",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -153,3 +153,6 @@ def __init__(\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.cache_implementation = cache_implementation\n+\n+\n+__all__ = [\"Gemma2Config\"]"
        },
        {
            "sha": "288913697f2641f950a2b5bcec006535ff5aa0f5",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -1280,3 +1280,12 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"Gemma2ForCausalLM\",\n+    \"Gemma2Model\",\n+    \"Gemma2PreTrainedModel\",\n+    \"Gemma2ForSequenceClassification\",\n+    \"Gemma2ForTokenClassification\",\n+]"
        },
        {
            "sha": "5e04fe1b63a36296900820c75325c27a25bd19b9",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -903,3 +903,13 @@ def __init__(self, config):\n         super().__init__(config)\n         self.model = Gemma2Model(config)\n         self.post_init()\n+\n+\n+__all__ = [\n+    \"Gemma2Config\",\n+    \"Gemma2ForCausalLM\",\n+    \"Gemma2Model\",\n+    \"Gemma2PreTrainedModel\",\n+    \"Gemma2ForSequenceClassification\",\n+    \"Gemma2ForTokenClassification\",\n+]"
        },
        {
            "sha": "e3632c7a2a14278d69c129e6431134314cf59ee5",
            "filename": "src/transformers/models/llava_next_video/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 49,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fllava_next_video%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fllava_next_video%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2F__init__.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -13,58 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_llava_next_video\": [\"LlavaNextVideoConfig\"],\n-    \"processing_llava_next_video\": [\"LlavaNextVideoProcessor\"],\n-}\n-\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_llava_next_video\"] = [\"LlavaNextVideoImageProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_llava_next_video\"] = [\n-        \"LlavaNextVideoForConditionalGeneration\",\n-        \"LlavaNextVideoPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_llava_next_video import LlavaNextVideoConfig\n-    from .processing_llava_next_video import LlavaNextVideoProcessor\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_llava_next_video import LlavaNextVideoImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_llava_next_video import (\n-            LlavaNextVideoForConditionalGeneration,\n-            LlavaNextVideoPreTrainedModel,\n-        )\n-\n+    from .configuration_llava_next_video import *\n+    from .image_processing_llava_next_video import *\n+    from .modeling_llava_next_video import *\n+    from .processing_llava_next_video import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e608e5a0d20eced9321ebcb4f5984e3afa768194",
            "filename": "src/transformers/models/llava_next_video/configuration_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -158,3 +158,6 @@ def __init__(\n         self.text_config = text_config\n \n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+\n+\n+__all__ = [\"LlavaNextVideoConfig\"]"
        },
        {
            "sha": "f30e2c54fe90a31e2b9304045255572a8450add3",
            "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -414,3 +414,6 @@ def preprocess(\n \n         data = {\"pixel_values_videos\": pixel_values}\n         return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"LlavaNextVideoImageProcessor\"]"
        },
        {
            "sha": "7cd7e18abaf3e0db41a4dc096f38422698531379",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 18,
            "deletions": 15,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -122,21 +122,6 @@ def forward(self, image_features):\n         return image_features_spatial_pool.flatten(2).transpose(1, 2).contiguous()\n \n \n-class LlavaNextVideoMultiModalProjector(nn.Module):\n-    def __init__(self, config: LlavaNextVideoConfig):\n-        super().__init__()\n-\n-        self.linear_1 = nn.Linear(config.vision_config.hidden_size, config.text_config.hidden_size, bias=True)\n-        self.act = ACT2FN[config.projector_hidden_act]\n-        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size, bias=True)\n-\n-    def forward(self, image_features):\n-        hidden_states = self.linear_1(image_features)\n-        hidden_states = self.act(hidden_states)\n-        hidden_states = self.linear_2(hidden_states)\n-        return hidden_states\n-\n-\n LLAVA_NEXT_VIDEO_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -191,6 +176,21 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n+class LlavaNextVideoMultiModalProjector(nn.Module):\n+    def __init__(self, config: LlavaNextVideoConfig):\n+        super().__init__()\n+\n+        self.linear_1 = nn.Linear(config.vision_config.hidden_size, config.text_config.hidden_size, bias=True)\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size, bias=True)\n+\n+    def forward(self, image_features):\n+        hidden_states = self.linear_1(image_features)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n     \"\"\"\n     Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n@@ -1157,3 +1157,6 @@ def get_video_features(\n         video_features = self.multi_modal_projector(video_features)\n         video_features = torch.split(video_features, frames, dim=0)\n         return video_features\n+\n+\n+__all__ = [\"LlavaNextVideoForConditionalGeneration\", \"LlavaNextVideoPreTrainedModel\"]"
        },
        {
            "sha": "94c1432a41b1f1dba57a1585dd5ddaf81cd7be9a",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -24,6 +24,7 @@\n from transformers.models.llava_next.modeling_llava_next import (\n     LlavaNextCausalLMOutputWithPast,\n     LlavaNextForConditionalGeneration,\n+    LlavaNextPreTrainedModel,\n     image_size_to_num_patches,\n )\n \n@@ -218,6 +219,10 @@ def forward(self, image_features):\n         return image_features_spatial_pool.flatten(2).transpose(1, 2).contiguous()\n \n \n+class LlavaNextVideoPreTrainedModel(LlavaNextPreTrainedModel):\n+    pass\n+\n+\n class LlavaNextVideoForConditionalGeneration(LlavaNextForConditionalGeneration):\n     def __init__(self, config: LlavaNextVideoConfig, **super_kwargs):\n         super().__init__(config, **super_kwargs)\n@@ -641,3 +646,6 @@ def prepare_inputs_for_generation(\n             model_inputs[\"image_sizes\"] = image_sizes\n \n         return model_inputs\n+\n+\n+__all__ = [\"LlavaNextVideoConfig\", \"LlavaNextVideoForConditionalGeneration\", \"LlavaNextVideoPreTrainedModel\"]"
        },
        {
            "sha": "857ee28a08004137affd02da93fc59d382e4e61d",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -291,3 +291,6 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n         return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"LlavaNextVideoProcessor\"]"
        },
        {
            "sha": "6349255ed3a4756aa7d449ff6af35847886dd136",
            "filename": "src/transformers/models/starcoder2/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 44,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fstarcoder2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fstarcoder2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2F__init__.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2024 BigCode and The HuggingFace Inc. team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,52 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_starcoder2\": [\"Starcoder2Config\"],\n-}\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_starcoder2\"] = [\n-        \"Starcoder2ForCausalLM\",\n-        \"Starcoder2Model\",\n-        \"Starcoder2PreTrainedModel\",\n-        \"Starcoder2ForSequenceClassification\",\n-        \"Starcoder2ForTokenClassification\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_starcoder2 import Starcoder2Config\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_starcoder2 import (\n-            Starcoder2ForCausalLM,\n-            Starcoder2ForSequenceClassification,\n-            Starcoder2ForTokenClassification,\n-            Starcoder2Model,\n-            Starcoder2PreTrainedModel,\n-        )\n-\n-\n+    from .configuration_starcoder2 import *\n+    from .modeling_starcoder2 import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "7f21d1f12d8b22f1d01360dac289b1843a89b098",
            "filename": "src/transformers/models/starcoder2/configuration_starcoder2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -197,3 +197,6 @@ def __init__(\n             eos_token_id=eos_token_id,\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"Starcoder2Config\"]"
        },
        {
            "sha": "8047e23bb05bd84f9ae8dacc60eb9cc31f8dc065",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -1324,3 +1324,12 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"Starcoder2ForCausalLM\",\n+    \"Starcoder2Model\",\n+    \"Starcoder2PreTrainedModel\",\n+    \"Starcoder2ForSequenceClassification\",\n+    \"Starcoder2ForTokenClassification\",\n+]"
        },
        {
            "sha": "013c8e472b325db1b76f295798939685a575c45d",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11ba1d472c61eaacdc58a12e31156d4436b132ce/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=11ba1d472c61eaacdc58a12e31156d4436b132ce",
            "patch": "@@ -544,3 +544,12 @@ class Starcoder2ForSequenceClassification(LlamaForSequenceClassification):\n \n class Starcoder2ForTokenClassification(LlamaForTokenClassification):\n     pass\n+\n+\n+__all__ = [\n+    \"Starcoder2ForCausalLM\",\n+    \"Starcoder2Model\",\n+    \"Starcoder2PreTrainedModel\",\n+    \"Starcoder2ForSequenceClassification\",\n+    \"Starcoder2ForTokenClassification\",\n+]"
        }
    ],
    "stats": {
        "total": 382,
        "additions": 129,
        "deletions": 253
    }
}