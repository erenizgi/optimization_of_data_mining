{
    "author": "sywangyi",
    "message": "extend CwmIntegrationTest to xpu (#42314)\n\n* extend CwmIntegrationTest to xpu\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\n\n* fmt\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\n\n* rm cpu logits\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n---------\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "5dcda1b939bd25842dd49b4c1d376f88b8f1fe0e",
    "files": [
        {
            "sha": "1cb2eba08fa1eefbd6531dda3709f51bbab01d0a",
            "filename": "tests/models/cwm/test_modeling_cwm.py",
            "status": "modified",
            "additions": 26,
            "deletions": 8,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/5dcda1b939bd25842dd49b4c1d376f88b8f1fe0e/tests%2Fmodels%2Fcwm%2Ftest_modeling_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5dcda1b939bd25842dd49b4c1d376f88b8f1fe0e/tests%2Fmodels%2Fcwm%2Ftest_modeling_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcwm%2Ftest_modeling_cwm.py?ref=5dcda1b939bd25842dd49b4c1d376f88b8f1fe0e",
            "patch": "@@ -16,7 +16,9 @@\n \n from transformers import is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n+    require_deterministic_for_xpu,\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n@@ -95,6 +97,7 @@ def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n     @slow\n+    @require_deterministic_for_xpu\n     def test_cwm_integration(self):\n         from transformers import AutoTokenizer\n \n@@ -119,11 +122,18 @@ def test_cwm_integration(self):\n             out = model(**inputs)\n \n         # fmt: off\n-        expected_logits = torch.tensor(\n-            [0.5625, 2.9531, 9.1875, 0.5039, -0.3262, 2.2344, 3.0312, 1.5312, 0.5664, 1.5625, 2.7656, 3.4219, 2.0312, 2.1719, 1.5391, 2.5469, 2.8281, 1.8125, 1.7109, 1.3906, 1.0391, 0.1621, 0.4277, 0.1455, -0.1230, 0.8477, 2.2344, 5.2188, 1.2969, 1.5547, 0.8516, 0.7148],\n-            dtype=torch.bfloat16,\n-        ).to(model.device)\n+        expected_logits = Expectations(\n+            {\n+                (\"cuda\", None): torch.tensor(\n+                    [0.5625, 2.9531, 9.1875, 0.5039, -0.3262, 2.2344, 3.0312, 1.5312, 0.5664, 1.5625, 2.7656, 3.4219, 2.0312, 2.1719, 1.5391, 2.5469, 2.8281, 1.8125, 1.7109, 1.3906, 1.0391, 0.1621, 0.4277, 0.1455, -0.1230, 0.8477, 2.2344, 5.2188, 1.2969, 1.5547, 0.8516, 0.7148]\n+                ),\n+                (\"xpu\", None): torch.Tensor(\n+                    [0.5625, 2.9688, 9.1875, 0.4766, -0.3574, 2.2344, 3.0156, 1.4922, 0.5625, 1.5547, 2.7656, 3.4062, 2.0156, 2.1719, 1.5469, 2.5156, 2.8125, 1.7891, 1.7031, 1.3828, 1.0312, 0.1602, 0.4277, 0.1328, -0.1348, 0.8281, 2.2188, 5.2812, 1.2734, 1.5312, 0.8398, 0.7070]\n+                ),\n+            }\n+        )\n         # fmt: on\n+        expected_logits = expected_logits.get_expectation().to(model.device, torch.bfloat16)\n \n         torch.testing.assert_close(out.logits[0, -1, :32], expected_logits, atol=1e-2, rtol=1e-2)\n \n@@ -133,6 +143,7 @@ def test_cwm_integration(self):\n         self.assertFalse(torch.isinf(out.logits).any())\n \n     @slow\n+    @require_deterministic_for_xpu\n     def test_cwm_sliding_window_long_sequence(self):\n         from transformers import AutoTokenizer\n \n@@ -157,11 +168,18 @@ def test_cwm_sliding_window_long_sequence(self):\n             out = model(**inputs)\n \n         # fmt: off\n-        expected_logits = torch.tensor(\n-            [5.2812, 6.4688, 12.8125, 4.6875, 5.2500, 4.2500, 6.9688, 4.9375, 2.7656, 6.5938, 4.9688, 1.1016, 5.9375, 3.7500, 3.1094, 5.5312, 6.1250, 4.7500, 4.5312, 2.8281, 4.0625, 3.3125, 3.9219, 3.3906, 3.1406, 3.6719, 3.2031, 7.0938, 4.8750, 6.0000, 2.7188, 6.2500],\n-            dtype=torch.bfloat16,\n-        ).to(model.device)\n+        expected_logits = Expectations(\n+            {\n+                (\"cuda\", None): torch.tensor(\n+                    [5.2812, 6.4688, 12.8125, 4.6875, 5.2500, 4.2500, 6.9688, 4.9375, 2.7656, 6.5938, 4.9688, 1.1016, 5.9375, 3.7500, 3.1094, 5.5312, 6.1250, 4.7500, 4.5312, 2.8281, 4.0625, 3.3125, 3.9219, 3.3906, 3.1406, 3.6719, 3.2031, 7.0938, 4.8750, 6.0000, 2.7188, 6.2500]\n+                ),\n+                (\"xpu\", None): torch.Tensor(\n+                    [5.2500, 6.4688, 12.8125, 4.6562, 5.2812, 4.2812, 7.0000, 4.9062, 2.7344, 6.5938, 4.9062, 1.1094, 5.9375, 3.7188, 3.0469, 5.5000, 6.0938, 4.7188, 4.5000, 2.7344, 4.0312, 3.2812, 3.8750, 3.3438, 3.1094, 3.6406, 3.2031, 7.1250, 4.8750, 6.0000, 2.7031, 6.2188]\n+                ),\n+            }\n+        )\n         # fmt: on\n+        expected_logits = expected_logits.get_expectation().to(model.device, torch.bfloat16)\n \n         torch.testing.assert_close(out.logits[0, -1, :32], expected_logits, atol=1e-2, rtol=1e-2)\n "
        }
    ],
    "stats": {
        "total": 34,
        "additions": 26,
        "deletions": 8
    }
}