{
    "author": "vasqu",
    "message": "ðŸ”´[`Attention`] Attention refactor for Whisper-based models (#38235)\n\n* start refactoring whisper\n\n* revert for now\n\n* first step\n\n* carry over attn fixes\n\n* check if this works\n\n* whisper has an off by one somewhere - cutting mask in any interface\n\n* make it based on interface\n\n* remove some tests that were skipped but now work\n\n* some fixes for whisper tests\n\n* interface changes\n\n* change the order of fix\n\n* some attention adjustments for eager + TP\n\n* fix scaling\n\n* mask changes\n\n* why does whisper contain those extra seq lens?\n\n* fix from config for fa2 as input_ids is invalid\n\n* fix another test\n\n* another fix\n\n* disable flex attn due to compile issues\n\n* copies and refactor for qwen audio since it somewhat relies on whisper\n\n* fix scaling and smaller things\n\n* retrigger\n\n* new new interface version + more fixups\n\n* adjust qwen\n\n* add comment\n\n* forgot this one\n\n* change copies as whisper cuts on the mask\n\n* add guard\n\n* add flex attention\n\n* switch to new mask function + add skips for torchscript\n\n* remove old api with cache position\n\n* last changes?\n\n* trigger ci",
    "sha": "badc71b9f604ca910bb87a43979c795eaf6e7d64",
    "files": [
        {
            "sha": "1a3ba7f8df8384f14445482f919c7e5f41d0b204",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/badc71b9f604ca910bb87a43979c795eaf6e7d64/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/badc71b9f604ca910bb87a43979c795eaf6e7d64/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=badc71b9f604ca910bb87a43979c795eaf6e7d64",
            "patch": "@@ -1716,6 +1716,19 @@ def batch_select_indices(self, indices: torch.Tensor):\n         self.self_attention_cache.batch_select_indices(indices)\n         self.cross_attention_cache.batch_select_indices(indices)\n \n+    def get_max_cache_shape(self) -> Optional[int]:\n+        \"\"\"Returns the maximum sequence length (i.e. max capacity) of the cache object\"\"\"\n+        return self.self_attention_cache.get_max_cache_shape()\n+\n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"\n+        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n+        the given layer at `layer_idx`.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns (i.e. sliding_window, chunk_size),\n+        for each layer.\n+        \"\"\"\n+        return self.self_attention_cache.get_mask_sizes(cache_position, layer_idx)\n+\n \n class HybridCache(Cache):\n     \"\"\""
        },
        {
            "sha": "a4e8b5eda0d664fd38429a888825afca845d7f24",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/badc71b9f604ca910bb87a43979c795eaf6e7d64/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/badc71b9f604ca910bb87a43979c795eaf6e7d64/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=badc71b9f604ca910bb87a43979c795eaf6e7d64",
            "patch": "@@ -2051,6 +2051,10 @@ def set_inputs(self, inputs):\n         self.inputs = {**self.model.prepare_inputs_for_generation(**inputs), **inputs}\n         self.inputs[\"input_features\"] = self.inputs.pop(\"inputs\")\n \n+        # Whisper encoder-decoder does not accept the input_ids as input\n+        if \"input_ids\" not in inspect.signature(self.model.forward).parameters:\n+            self.inputs.pop(\"input_ids\", None)\n+\n     @property\n     def no_speech_prob(self):\n         return self._no_speech_prob"
        },
        {
            "sha": "713d57a8994d547c8731c53f5524fe97c58785fb",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/badc71b9f604ca910bb87a43979c795eaf6e7d64/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/badc71b9f604ca910bb87a43979c795eaf6e7d64/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=badc71b9f604ca910bb87a43979c795eaf6e7d64",
            "patch": "@@ -636,7 +636,7 @@ def prepare_inputs_for_generation(\n             and attention_mask is not None\n             and attention_mask.ndim == 2\n         ):\n-            if model_inputs[\"inputs_embeds\"] is not None:\n+            if not self.config.is_encoder_decoder and model_inputs[\"inputs_embeds\"] is not None:\n                 batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n             else:\n                 batch_size, sequence_length = model_inputs[input_ids_key].shape[:2]"
        },
        {
            "sha": "1f7b1447988f0ad8b05a597efc91606d816abd7f",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 52,
            "deletions": 194,
            "changes": 246,
            "blob_url": "https://github.com/huggingface/transformers/blob/badc71b9f604ca910bb87a43979c795eaf6e7d64/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/badc71b9f604ca910bb87a43979c795eaf6e7d64/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=badc71b9f604ca910bb87a43979c795eaf6e7d64",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -25,18 +25,13 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import auto_docstring, logging\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_qwen2_audio import Qwen2AudioConfig, Qwen2AudioEncoderConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -82,6 +77,37 @@ class Qwen2AudioCausalLMOutputWithPast(ModelOutput):\n     attention_mask: Optional[torch.FloatTensor] = None\n \n \n+# Copied from transformers.models.whisper.modeling_whisper.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attn_weights = attn_weights + attention_mask[:, :, :, : key.shape[-2]]\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Qwen2AudioAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -135,219 +161,51 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         bsz, tgt_len, _ = hidden_states.size()\n \n-        # get query proj\n+        # Scaling is susceptible to floating point arithmetics' inprecisions\n+        # which can lead to different results (this is dependent from model\n+        # to model, e.g. whisper is one such case). We therefore keep the\n+        # original order of scaling to follow the original implementation\n+        # and enforce no scaling (1.0) in the attention call below.\n         query_states = self._shape(self.q_proj(hidden_states) * self.scaling, tgt_len, bsz)\n         key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n         value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n \n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3))\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-        attn_output = torch.matmul(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights, None\n-\n-\n-class Qwen2AudioFlashAttention2(Qwen2AudioAttention):\n-    \"\"\"\n-    Qwen2Audio flash attention module. This module inherits from `Qwen2AudioAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    # Copied from transformers.models.whisper.modeling_whisper.WhisperFlashAttention2.__init__ with Whisper->Qwen2Audio\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # Qwen2AudioFlashAttention2 attention does not support output_attentions\n-        if output_attentions:\n-            raise ValueError(\"Qwen2AudioFlashAttention2 attention does not support output_attentions\")\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = torch.reshape(self.q_proj(hidden_states), (bsz, tgt_len, self.num_heads, self.head_dim))\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]\n-        #  We would need to refactor the KV cache to be able to avoid many of these transpose/reshape/view.\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, : key_states.shape[1]]\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            causal_mask,\n-            tgt_len,\n-            dropout=self.dropout if self.training else 0.0,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=1.0,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, tgt_len, -1)\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights, None\n \n \n-class Qwen2AudioSdpaAttention(Qwen2AudioAttention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Qwen2AudioModel is using Qwen2AudioSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self._shape(self.q_proj(hidden_states), tgt_len, bsz)\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and causal_mask is None and tgt_len > 1 else False\n-\n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, None, None\n-\n-\n-QWEN2AUDIO_ATTENTION_CLASSES = {\n-    \"eager\": Qwen2AudioAttention,\n-    \"flash_attention_2\": Qwen2AudioFlashAttention2,\n-    \"sdpa\": Qwen2AudioSdpaAttention,\n-}\n-\n-\n # Copied from transformers.models.whisper.modeling_whisper.WhisperEncoderLayer with Whisper->Qwen2Audio, WHISPER->QWEN2AUDIO\n class Qwen2AudioEncoderLayer(nn.Module):\n     def __init__(self, config: Qwen2AudioConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = QWEN2AUDIO_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = Qwen2AudioAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,"
        },
        {
            "sha": "08552d4114534cd7a6b035b6e507bfe5a512c1fc",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/badc71b9f604ca910bb87a43979c795eaf6e7d64/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/badc71b9f604ca910bb87a43979c795eaf6e7d64/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=badc71b9f604ca910bb87a43979c795eaf6e7d64",
            "patch": "@@ -636,6 +636,10 @@ def generate(\n         # passing `decoder_input_ids` is deprecated - the only exception is for assisted generation\n         # where the input ids are handled explicitly by the generate method\n         self._check_decoder_input_ids(kwargs=kwargs)\n+        # `output_attentions` is deprecated - we force eager attention if this feature is\n+        # indirectly requested, e.g. through return_token_timestamps\n+        if return_token_timestamps:\n+            self.model.config._attn_implementation = \"eager\"\n \n         # 3. Retrieve logits processors\n         device = kwargs[\"encoder_outputs\"][0].device if \"encoder_outputs\" in kwargs else input_features.device"
        },
        {
            "sha": "7bb07a6c1c6adf00d08ac5ed2181b234633cb0a4",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 75,
            "deletions": 413,
            "changes": 488,
            "blob_url": "https://github.com/huggingface/transformers/blob/badc71b9f604ca910bb87a43979c795eaf6e7d64/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/badc71b9f604ca910bb87a43979c795eaf6e7d64/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=badc71b9f604ca910bb87a43979c795eaf6e7d64",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"PyTorch Whisper model.\"\"\"\n \n import math\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -24,12 +24,11 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import (\n-    flash_attn_supports_top_left_mask,\n-    is_flash_attn_available,\n+    FlashAttentionKwargs,\n )\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -39,21 +38,13 @@\n     Seq2SeqModelOutput,\n     SequenceClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, logging\n from .configuration_whisper import WhisperConfig\n from .generation_whisper import WhisperGenerationMixin\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n _HIDDEN_STATES_START_POSITION = 1\n@@ -219,6 +210,36 @@ def forward(self, input_ids, past_key_values_length=0, position_ids=None):\n             return self.weight[position_ids]\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attn_weights = attn_weights + attention_mask[:, :, :, : key.shape[-2]]\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class WhisperAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -262,29 +283,36 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n \n-        # get query proj\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+\n+        # Scaling is susceptible to floating point arithmetics' inprecisions\n+        # which can lead to different results (this is dependent from model\n+        # to model, e.g. whisper is one such case). We therefore keep the\n+        # original order of scaling to follow the original implementation\n+        # and enforce no scaling (1.0) in the attention call below.\n         query_states = self.q_proj(hidden_states) * self.scaling\n-        query_states = query_states.view(bsz, tgt_len, self.num_heads, self.head_dim)\n+        query_states = query_states.view(*q_input_shape)\n         query_states = query_states.transpose(1, 2).contiguous()\n \n         if past_key_value is not None:\n@@ -314,278 +342,36 @@ def forward(\n                     key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n                 )\n \n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3))\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        if layer_head_mask is not None:\n-            if layer_head_mask.size() != (self.num_heads,):\n-                raise ValueError(\n-                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n-                    f\" {layer_head_mask.size()}\"\n-                )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights\n-\n-        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-        attn_output = torch.matmul(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class WhisperFlashAttention2(WhisperAttention):\n-    \"\"\"\n-    Whisper flash attention module. This module inherits from `WhisperAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n-            raise ValueError(\n-                \"The `static` cache implementation is not compatible with `attn_implementation='flash_attention_2'`. \"\n-                \"Use `attn_implementation='sdpa'` in the meantime, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = torch.reshape(self.q_proj(hidden_states), (bsz, tgt_len, self.num_heads, self.head_dim))\n-\n-        if past_key_value is not None:\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n-            if is_cross_attention:\n-                # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                past_key_value.is_updated[self.layer_idx] = True\n-                past_key_value = past_key_value.cross_attention_cache\n-            else:\n-                past_key_value = past_key_value.self_attention_cache\n-\n-        # use key_value_states if cross attention\n-        current_states = key_value_states if key_value_states is not None else hidden_states\n-        if is_cross_attention and past_key_value and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value.key_cache[self.layer_idx]\n-            value_states = past_key_value.value_cache[self.layer_idx]\n-        else:\n-            key_states = self.k_proj(current_states).view(bsz, tgt_len, self.num_heads, self.head_dim)\n-            value_states = self.v_proj(current_states).view(bsz, tgt_len, self.num_heads, self.head_dim)\n-            key_states = key_states.transpose(1, 2).contiguous()\n-            value_states = value_states.transpose(1, 2).contiguous()\n-            if past_key_value is not None:\n-                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = past_key_value.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]\n-        #  We would need to refactor the KV cache to be able to avoid many of these transpose/reshape/view.\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, : key_states.shape[1]]\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            causal_mask,\n-            tgt_len,\n-            dropout=self.dropout if self.training else 0.0,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=1.0,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, tgt_len, -1)\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights, past_key_value\n \n \n-class WhisperSdpaAttention(WhisperAttention):\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"WhisperModel is using WhisperSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention\"\n-                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n-            )\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-        bsz, tgt_len, _ = hidden_states.size()\n-\n-        # get query proj\n-        query_states = self.q_proj(hidden_states).view(bsz, tgt_len, self.num_heads, self.head_dim)\n-        query_states = query_states.transpose(1, 2).contiguous()\n-\n-        if past_key_value is not None:\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n-            if is_cross_attention:\n-                # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                past_key_value.is_updated[self.layer_idx] = True\n-                past_key_value = past_key_value.cross_attention_cache\n-            else:\n-                past_key_value = past_key_value.self_attention_cache\n-\n-        # use key_value_states if cross attention\n-        current_states = key_value_states if key_value_states is not None else hidden_states\n-        if is_cross_attention and past_key_value and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value.key_cache[self.layer_idx]\n-            value_states = past_key_value.value_cache[self.layer_idx]\n-        else:\n-            key_states = self.k_proj(current_states).view(bsz, -1, self.num_heads, self.head_dim)\n-            value_states = self.v_proj(current_states).view(bsz, -1, self.num_heads, self.head_dim)\n-            key_states = key_states.transpose(1, 2).contiguous()\n-            value_states = value_states.transpose(1, 2).contiguous()\n-            if past_key_value is not None:\n-                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = past_key_value.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n-        is_causal = True if self.is_causal and causal_mask is None and tgt_len > 1 else False\n-\n-        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n-        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-\n-        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n-        # partitioned across GPUs when using tensor-parallelism.\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n-        attn_output = self.out_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-WHISPER_ATTENTION_CLASSES = {\n-    \"eager\": WhisperAttention,\n-    \"flash_attention_2\": WhisperFlashAttention2,\n-    \"sdpa\": WhisperSdpaAttention,\n-}\n-\n-\n-# (BC Dep) Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Whisper, MBART->WHISPER\n-# TODO(vasqu): fix copies when enabling whisper attn interface\n+# Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Whisper, MBART->WHISPER\n class WhisperEncoderLayer(nn.Module):\n     def __init__(self, config: WhisperConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = WHISPER_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = WhisperAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -653,7 +439,7 @@ def __init__(self, config: WhisperConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n-        self.self_attn = WHISPER_ATTENTION_CLASSES[config._attn_implementation](\n+        self.self_attn = WhisperAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -667,7 +453,7 @@ def __init__(self, config: WhisperConfig, layer_idx: Optional[int] = None):\n         self.activation_dropout = config.activation_dropout\n \n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n-        self.encoder_attn = WHISPER_ATTENTION_CLASSES[config._attn_implementation](\n+        self.encoder_attn = WhisperAttention(\n             self.embed_dim,\n             config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n@@ -774,6 +560,7 @@ class WhisperPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"WhisperEncoderLayer\", \"WhisperDecoderLayer\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n \n@@ -1142,12 +929,12 @@ def forward(\n         hidden_states = inputs_embeds + positions.to(inputs_embeds.device)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            past_key_values.self_attention_cache if past_key_values is not None else None,\n-            output_attentions,\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n         )\n \n         if self.gradient_checkpointing and self.training:\n@@ -1237,131 +1024,6 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n @auto_docstring\n class WhisperModel(WhisperPreTrainedModel):"
        },
        {
            "sha": "571ac0737081812f109d03d168f33582deb543fb",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/badc71b9f604ca910bb87a43979c795eaf6e7d64/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/badc71b9f604ca910bb87a43979c795eaf6e7d64/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=badc71b9f604ca910bb87a43979c795eaf6e7d64",
            "patch": "@@ -156,6 +156,10 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n+    @unittest.skip(reason=\"Qwen2 Audio does not support right padding.\")\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        pass\n+\n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         # overwrite because Qwen2 is audio+text model (not vision+text)"
        },
        {
            "sha": "1397bbe4dc6106a6f844842f3dfc8cc5e6ca5bda",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 30,
            "deletions": 25,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/badc71b9f604ca910bb87a43979c795eaf6e7d64/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/badc71b9f604ca910bb87a43979c795eaf6e7d64/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=badc71b9f604ca910bb87a43979c795eaf6e7d64",
            "patch": "@@ -31,6 +31,7 @@\n from transformers.testing_utils import (\n     is_flaky,\n     require_flash_attn,\n+    require_read_token,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n@@ -542,8 +543,10 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip\n-    def test_generate_with_head_masking(self):\n+    @parameterized.expand([(\"offloaded\",)])\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"Whisper doesn't work with offloaded cache implementation yet\")\n+    def test_offloaded_cache_implementation(self, cache_implementation):\n         pass\n \n     @require_torch_fp16\n@@ -660,6 +663,9 @@ def test_attention_outputs(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.return_dict = True\n \n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n         seq_len = getattr(self.model_tester, \"seq_length\", None)\n         decoder_seq_length = getattr(self.model_tester, \"decoder_seq_length\", 1)\n         encoder_seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_len)\n@@ -849,7 +855,7 @@ def test_resize_embeddings_untied(self):\n             # Check that the model can still do a forward pass successfully (every parameter should be resized)\n             model(**self._prepare_for_class(inputs_dict, model_class))\n \n-    @unittest.skip\n+    @unittest.skip(reason=\"Whisper encoder-decoder requires the features directly and can not work on ids only.\")\n     def test_generate_without_input_ids(self):\n         pass\n \n@@ -1422,6 +1428,21 @@ def test_generate_compile_model_forward(self):\n     def test_generate_compilation_all_outputs(self):\n         pass\n \n+    # TODO (cyril): fix me :)\n+    @unittest.skip(reason=\"Torchscript doesn't work with the new mask creation functions\")\n+    def test_torchscript_output_attentions(self):\n+        pass\n+\n+    # TODO (cyril): fix me :)\n+    @unittest.skip(reason=\"Torchscript doesn't work with the new mask creation functions\")\n+    def test_torchscript_output_hidden_state(self):\n+        pass\n+\n+    # TODO (cyril): fix me :)\n+    @unittest.skip(reason=\"Torchscript doesn't work with the new mask creation functions\")\n+    def test_torchscript_simple(self):\n+        pass\n+\n \n @require_torch\n @require_torchaudio\n@@ -1684,6 +1705,7 @@ def test_large_batched_generation(self):\n         transcript = processor.batch_decode(generated_ids, skip_special_tokens=True)\n         self.assertListEqual(transcript, EXPECTED_TRANSCRIPT)\n \n+    @require_read_token\n     @slow\n     def test_large_batched_generation_multilingual(self):\n         processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n@@ -1775,7 +1797,7 @@ def test_tiny_timestamp_generation(self):\n         ])\n         # fmt: on\n \n-        torch.testing.assert_close(generated_ids, EXPECTED_OUTPUT)\n+        torch.testing.assert_close(generated_ids[0], EXPECTED_OUTPUT)\n \n         EXPECTED_TRANSCRIPT = [\n             {\n@@ -2016,7 +2038,7 @@ def test_large_timestamp_generation(self):\n             50365, 2221, 13, 2326, 388, 391, 307, 264, 50244, 295, 264, 2808, 5359, 11, 293, 321, 366, 5404, 281, 2928, 702, 14943, 13, 50629, 50682, 6966, 307, 2221, 13, 2326, 388, 391, 311, 9060, 1570, 1880, 813, 702, 1871, 13, 50870, 50911, 634, 5112, 505, 300, 412, 341, 42729, 3196, 295, 264, 1064, 11, 365, 5272, 293, 12904, 9256, 450, 10539, 949, 505, 11, 51245, 51287, 1034, 4680, 10117, 490, 3936, 293, 1080, 3542, 5160, 881, 26336, 281, 264, 1575, 13, 51494, 51523, 634, 575, 12525, 22618, 1968, 6144, 35617, 1456, 397, 266, 311, 589, 307, 534, 10281, 934, 439, 11, 51799, 51815, 50365, 293, 393, 4411, 50430\n         ])\n         # fmt: on\n-        torch.testing.assert_close(generated_ids, EXPECTED_OUTPUT)\n+        torch.testing.assert_close(generated_ids[0], EXPECTED_OUTPUT)\n \n         EXPECTED_TRANSCRIPT = [\n             {\n@@ -3610,27 +3632,10 @@ def test_decoder_model_attn_mask_past(self):\n             config=config, input_ids=inputs_dict[\"input_ids\"]\n         )\n \n-    @unittest.skip(reason=\"Tested implicitly through the encoder-decoder tests\")\n-    def test_custom_4d_attention_mask(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Generate needs input ids\")\n-    def test_generate_without_input_ids(self):\n-        # generate only works with input ids for whisper\n-        pass\n-\n     @unittest.skip(reason=\"Decoder can't keep attention grads\")\n     def test_retain_grad_hidden_states_attentions(self):\n         return\n \n-    @unittest.skip(\n-        \"Duplicated test with WhisperModelTest + the FA2 testing suite needs to be refactored to be compatible with WhisperDecoder for that test\"\n-    )\n-    def test_flash_attn_2_inference(self):\n-        pass\n-\n-    @unittest.skip(\n-        \"Duplicated test with WhisperModelTest + the FA2 testing suite needs to be refactored to be compatible with WhisperDecoder for that test\"\n-    )\n-    def test_flash_attn_2_inference_padding_right(self):\n-        pass\n+    @unittest.skip(reason=\"Decoder cannot keep gradients\")\n+    def test_flex_attention_with_grads():\n+        return"
        },
        {
            "sha": "9ee6a93dbbc87adb496dca186746b3488c7a51b8",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/badc71b9f604ca910bb87a43979c795eaf6e7d64/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/badc71b9f604ca910bb87a43979c795eaf6e7d64/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=badc71b9f604ca910bb87a43979c795eaf6e7d64",
            "patch": "@@ -4268,24 +4268,28 @@ def test_flash_attn_2_from_config(self):\n             if not model_class._supports_flash_attn_2:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n-            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             # TODO: to change it in the future with other relevant auto classes\n             fa2_model = model_class._from_config(\n-                config, attn_implementation=\"flash_attention_2\", torch_dtype=torch.bfloat16\n+                config, attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16\n             ).to(torch_device)\n \n-            dummy_input = torch.LongTensor([[0, 2, 3, 4], [0, 2, 3, 4]]).to(torch_device)\n-            dummy_attention_mask = torch.LongTensor([[1, 1, 1, 1], [0, 1, 1, 1]]).to(torch_device)\n+            dummy_input = inputs_dict[fa2_model.main_input_name]\n+            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n+                dummy_input = dummy_input.to(torch.float16)\n+            dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n \n-            if config.is_encoder_decoder:\n+            if fa2_model.config.is_encoder_decoder:\n+                dummy_decoder_input_ids = inputs_dict[\"decoder_input_ids\"]\n+                dummy_decoder_attention_mask = inputs_dict[\"decoder_attention_mask\"]\n                 _ = fa2_model(\n-                    input_ids=dummy_input,\n+                    dummy_input,\n                     attention_mask=dummy_attention_mask,\n-                    decoder_input_ids=dummy_input.clone(),\n-                    decoder_attention_mask=dummy_attention_mask.clone(),\n+                    decoder_input_ids=dummy_decoder_input_ids,\n+                    decoder_attention_mask=dummy_decoder_attention_mask,\n                 )\n             else:\n-                _ = fa2_model(input_ids=dummy_input, attention_mask=dummy_attention_mask)\n+                _ = fa2_model(dummy_input, attention_mask=dummy_attention_mask)\n \n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 fa2_model.save_pretrained(tmpdirname)"
        }
    ],
    "stats": {
        "total": 838,
        "additions": 196,
        "deletions": 642
    }
}