{
    "author": "co63oc",
    "message": "Fix typos in tests (#36547)\n\nSigned-off-by: co63oc <co63oc@users.noreply.github.com>",
    "sha": "996f512d52908beb7b89d835fe7eb7b900f78ad0",
    "files": [
        {
            "sha": "b123001f10232bb8f158994163fee21b44eaf090",
            "filename": "tests/bettertransformer/test_integration.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fbettertransformer%2Ftest_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fbettertransformer%2Ftest_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fbettertransformer%2Ftest_integration.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -38,7 +38,7 @@ class BetterTransformerIntegrationTest(unittest.TestCase):\n \n     def test_transform_and_reverse(self):\n         r\"\"\"\n-        Classic tests to simply check if the conversion has been successfull.\n+        Classic tests to simply check if the conversion has been successful.\n         \"\"\"\n         model_id = \"hf-internal-testing/tiny-random-t5\"\n         tokenizer = AutoTokenizer.from_pretrained(model_id)"
        },
        {
            "sha": "dc41929dca6087fa872bfca130cec6d72b3796f8",
            "filename": "tests/models/align/test_modeling_align.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_modeling_align.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -219,13 +219,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -361,13 +361,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "9fadadf4a76dcc252a0ef9db3c3e9b66b77b61f9",
            "filename": "tests/models/altclip/test_modeling_altclip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -187,13 +187,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -335,13 +335,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "e29a507d6a810e0e5e6a86ed2b8e24f374ae4fd5",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -241,19 +241,19 @@ def test_inputs_embeds_matches_input_ids(self):\n             torch.testing.assert_close(out_embeds, out_ids)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "92aa1ad4c9d62b9e35068094b38dd190f8dbe97f",
            "filename": "tests/models/autoformer/test_modeling_autoformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -245,19 +245,19 @@ def test_resize_tokens_embeddings(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass"
        },
        {
            "sha": "b6173de20e40b91c117020e78be26fc70fc65459",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -504,7 +504,7 @@ def test_generate_fp16(self):\n         model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass"
        },
        {
            "sha": "5c8d2215d4005bb6d42851cc169848787633cef6",
            "filename": "tests/models/beit/test_modeling_beit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -379,13 +379,13 @@ def test_training_gradient_checkpointing(self):\n             loss.backward()\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "9603ae9a755a22f2c30be3782d8c34909f6044b9",
            "filename": "tests/models/big_bird/test_modeling_big_bird.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -609,19 +609,19 @@ def test_for_change_to_full_attn(self):\n         self.model_tester.create_and_check_for_change_to_full_attn(*config_and_inputs)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "c4752097e9582e33047b72b161f42d850aada88d",
            "filename": "tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -464,7 +464,7 @@ def test_for_change_to_full_attn(self):\n         torch.testing.assert_close(outputs1, outputs2, rtol=1e-5, atol=1e-5)\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass"
        },
        {
            "sha": "50cc8baae5b811977a181b5bd20a8c720fec4760",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -202,13 +202,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -346,13 +346,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -977,13 +977,13 @@ def test_training_gradient_checkpointing(self):\n             loss.backward()\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "0be5d72002a9b5562b14136302e2226668a240c1",
            "filename": "tests/models/blip/test_modeling_blip_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -150,13 +150,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "6636ee3e216d903c0033043b57d714a4a6e6277d",
            "filename": "tests/models/blip/test_modeling_tf_blip_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip_text.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -148,13 +148,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "6815b5757115ff602ad2cc59d8d6428dabe00d11",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -209,13 +209,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "803e942d7aaf3918626d024edbfeff57f0424320",
            "filename": "tests/models/canine/test_modeling_canine.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -510,19 +510,19 @@ def test_model_get_set_embeddings(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "4a27e7292ec61af9c27048e56824d9f6477a795f",
            "filename": "tests/models/chinese_clip/test_modeling_chinese_clip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -397,13 +397,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -477,13 +477,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "559db26206b6d5a489e92053ad83a90f4e2f94fa",
            "filename": "tests/models/clap/test_modeling_clap.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -252,13 +252,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -417,13 +417,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "6b75769b5396b063d4be7133ccff6417fb874b78",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -435,13 +435,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -613,13 +613,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "c2f77e30667a7e686ae6723a1dc78e6260f1db90",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -191,13 +191,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -330,13 +330,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -493,19 +493,19 @@ def test_model_get_set_embeddings(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "f2e92701635c197b6437d7dd41de5d64c1539a55",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -254,19 +254,19 @@ def test_colpali_forward_inputs(self):\n             self.assertIsInstance(outputs, ColPaliForRetrievalOutput)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "333ed0fa95b9b4e948418a83098f05e4b940c894",
            "filename": "tests/models/deit/test_modeling_deit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -317,13 +317,13 @@ def test_training_gradient_checkpointing(self):\n             loss.backward()\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "bd02373707206f0906c8cff2b73453bfd5ee85c2",
            "filename": "tests/models/depth_pro/test_modeling_depth_pro.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -284,13 +284,13 @@ def test_training_gradient_checkpointing(self):\n             loss.backward()\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "a8f11c91729f8b01d30b1c98633995f123c32749",
            "filename": "tests/models/dinov2/test_modeling_dinov2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -250,19 +250,19 @@ def test_inputs_embeds(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "c40af0c197cf1ea46dd38d1107d260c9ed0dae10",
            "filename": "tests/models/dinov2_with_registers/test_modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -267,19 +267,19 @@ def test_inputs_embeds(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "fc652c24844be35898485a5ddc7cdfc9500c94fb",
            "filename": "tests/models/dpt/test_modeling_dpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -244,13 +244,13 @@ def test_training_gradient_checkpointing(self):\n             loss.backward()\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "4c0527687ce700aeb8b456418c9ea099b5c52c01",
            "filename": "tests/models/dpt/test_modeling_dpt_auto_backbone.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -230,13 +230,13 @@ def test_save_load_fast_init_to_base(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "cf166203147832a9d0076e46ce899c7f0550642a",
            "filename": "tests/models/dpt/test_modeling_dpt_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -258,13 +258,13 @@ def test_training_gradient_checkpointing(self):\n             loss.backward()\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "a6cf75a722856f7e53958838b2fd7498483baaa0",
            "filename": "tests/models/flava/test_modeling_flava.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -309,13 +309,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -470,13 +470,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -635,13 +635,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -766,13 +766,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -1247,19 +1247,19 @@ class FlavaForPreTrainingTest(FlavaModelTest):\n     test_torchscript = False\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "9fdb7f240f2d3a07eee3f56847797de7ba9fd6b7",
            "filename": "tests/models/fnet/test_modeling_fnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -332,19 +332,19 @@ def test_attention_outputs(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "a908567d24e76e471eb0848ec595ac5f8ba519eb",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -282,19 +282,19 @@ def setUp(self):\n         self.model_tester = FuyuModelTester(self)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "a2427dfbc158dd613f2fe65511f3b1ed77f63758",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -176,13 +176,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "4af2739ff5fb3cd7631318d6e4b2f74077517f71",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -601,19 +601,19 @@ def test_cached_forward_with_and_without_attention_mask(self):\n         self.model_tester.create_and_check_cached_forward_with_and_without_attention_mask(*config_and_inputs)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "6eebdbf2e45e279a53b5f11850e50c20c2b5aadd",
            "filename": "tests/models/groupvit/test_modeling_groupvit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -263,13 +263,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -461,13 +461,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "48860718ee5671dbb84e1aa2b00e2b55880916b5",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -478,13 +478,13 @@ def test_training_gradient_checkpointing(self):\n             loss.backward()\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -855,13 +855,13 @@ def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "b563dc69a798d57551da4a768e85cb41aef2a33b",
            "filename": "tests/models/imagegpt/test_modeling_imagegpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -188,7 +188,7 @@ def create_and_check_lm_head_model(self, config, input_ids, input_mask, head_mas\n         labels = ids_tensor([self.batch_size, self.seq_length], self.vocab_size - 1)\n         result = model(input_ids, token_type_ids=token_type_ids, labels=labels)\n         self.parent.assertEqual(result.loss.shape, ())\n-        # ImageGPTForCausalImageModeling doens't have tied input- and output embeddings\n+        # ImageGPTForCausalImageModeling doesn't have tied input- and output embeddings\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size - 1))\n \n     def create_and_check_imagegpt_for_image_classification(\n@@ -281,19 +281,19 @@ def test_imagegpt_image_classification(self):\n         self.model_tester.create_and_check_imagegpt_for_image_classification(*config_and_inputs)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "fcd2858ac591332e5df70d4ad1fb2b9fffd1a20e",
            "filename": "tests/models/informer/test_modeling_informer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -294,19 +294,19 @@ def test_batching_equivalence(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "bccc8e230e78c2f49ed6568a0df331900db5ffc5",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -205,13 +205,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "27ed2d42e7ce4887fa8bd739792e9bd5b0635d38",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -213,13 +213,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "68605c1d6a780b1b3a48591858c022eea60bc5b4",
            "filename": "tests/models/layoutlm/test_modeling_layoutlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_layoutlm.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -280,19 +280,19 @@ def test_for_question_answering(self):\n         self.model_tester.create_and_check_for_question_answering(*config_and_inputs)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "a67c7e4a1148410786f2c2db4232a8aadf98bf68",
            "filename": "tests/models/layoutlmv3/test_tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -54,7 +54,7 @@ class LayoutLMv3TokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     tokenizer_class = LayoutLMv3Tokenizer\n     rust_tokenizer_class = LayoutLMv3TokenizerFast\n     test_rust_tokenizer = True\n-    # determined by the tokenization algortihm and the way it's decoded by the fast tokenizers\n+    # determined by the tokenization algorithm and the way it's decoded by the fast tokenizers\n     space_between_special_tokens = False\n     test_seq2seq = False\n     from_pretrained_kwargs = {\"cls_token\": \"<s>\"}"
        },
        {
            "sha": "5ac02f942729ad1de1349b60d8da6cdd88e98352",
            "filename": "tests/models/lilt/test_modeling_lilt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -281,19 +281,19 @@ def test_for_question_answering(self):\n         self.model_tester.create_and_check_for_question_answering(*config_and_inputs)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "66756784bdd1a0577ac6914085f2fe3d37db10e1",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -259,7 +259,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successfull forward with no modifications\n+            _ = model(**input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n             input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n@@ -306,19 +306,19 @@ def test_vision_feature_layers(self, vision_feature_layer):\n             model(**input_dict)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "327e33a9b7f9a90ce25ec0eabd2c4954db0f609e",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -348,19 +348,19 @@ def test_vision_feature_layers(self, vision_feature_layer):\n             model(**input_dict)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "44c7deeffcb08879a2f607fa565dbc17c21c593c",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -365,19 +365,19 @@ def test_vision_feature_layers(self, vision_feature_layer):\n             model(**input_dict)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "29f01c1e0e35b745d8fb550813b49fa02698bdeb",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -319,19 +319,19 @@ def test_vision_feature_layers(self, vision_feature_layer):\n             model(**input_dict)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, SiglipVisionModel does not support standalone training\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, SiglipVisionModel does not support standalone training\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, SiglipVisionModel does not support standalone training\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, SiglipVisionModel does not support standalone training\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, SiglipVisionModel does not support standalone training\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, SiglipVisionModel does not support standalone training\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "06bb76b74de9f0213d04b23676848fa8c4b5851e",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -780,7 +780,7 @@ def _check_encoder_attention_for_generate(self, attentions, batch_size, config,\n         )\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass\n@@ -1125,7 +1125,7 @@ def test_attention_outputs(self):\n                 )\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass"
        },
        {
            "sha": "8864e5687eee4bee9d47e6be2fdd399f0fc8402c",
            "filename": "tests/models/luke/test_modeling_luke.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -863,19 +863,19 @@ def test_retain_grad_entity_hidden_states(self):\n         self.assertIsNotNone(entity_hidden_states.grad)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "308662335d661f0c0a547a2f8b7ce419ff73c393",
            "filename": "tests/models/lxmert/test_modeling_lxmert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -779,7 +779,7 @@ def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass"
        },
        {
            "sha": "60b3d220b15310f31d468d50f99e869332b1515f",
            "filename": "tests/models/m2m_100/test_modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -338,7 +338,7 @@ def test_generate_fp16(self):\n         model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass"
        },
        {
            "sha": "bab434d4bc48886809cf699566d3183446cb11eb",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -185,7 +185,7 @@ def create_and_check_state_equivalency(self, config, input_ids, *args):\n         output_two = outputs.last_hidden_state\n \n         self.parent.assertTrue(torch.allclose(torch.cat([output_one, output_two], dim=1), output_whole, atol=1e-5))\n-        # TODO the orignal mamba does not support decoding more than 1 token neither do we\n+        # TODO the original mamba does not support decoding more than 1 token neither do we\n \n     def create_and_check_mamba_cached_slow_forward_and_backwards(\n         self, config, input_ids, *args, gradient_checkpointing=False"
        },
        {
            "sha": "8a1e2b4a33b38033139592c93d03098f44e73bee",
            "filename": "tests/models/marian/test_modeling_marian.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -335,19 +335,19 @@ def test_tie_word_embeddings_decoder(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "6920ea9b74573a81979be9379d5fdb3b5ee26f36",
            "filename": "tests/models/mbart/test_modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -368,7 +368,7 @@ def test_ensure_weights_are_shared(self):\n         )\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass"
        },
        {
            "sha": "5c8e5c0e70fa24a9214fa4b4930034d7abb9d782",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -392,7 +392,7 @@ def test_assisted_decoding_with_num_logits_to_keep(self):\n         pass\n \n     @pytest.mark.generate\n-    # overriden because mllama has special cache for self and cross attentions\n+    # overridden because mllama has special cache for self and cross attentions\n     def test_past_key_values_format(self):\n         # Test that the KV cache is formatted correctly. Exceptions need to explicitly overwrite this test. Having a\n         # standard KV cache format is important for a consistent API (and for advanced generation methods).\n@@ -444,7 +444,7 @@ def test_past_key_values_format(self):\n                         past_kv[i][1].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n                     )\n \n-    # overriden because mllama has special cache for self and cross attentions\n+    # overridden because mllama has special cache for self and cross attentions\n     def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n         self.assertIsInstance(decoder_past_key_values, Cache)\n         self.assertListEqual("
        },
        {
            "sha": "d096b0a9d582f43d41785239693a1242cfd1493a",
            "filename": "tests/models/mra/test_modeling_mra.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -359,19 +359,19 @@ def test_attention_outputs(self):\n         return\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "8aee844b478a29027f20e33ddc5d3f527e3492cb",
            "filename": "tests/models/nllb_moe/test_modeling_nllb_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -353,7 +353,7 @@ def test_get_loss(self):\n         self.assertIsNotNone(model(**input_dict)[\"decoder_router_logits\"][0])\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass"
        },
        {
            "sha": "e1278d3c937b77015cc01b31b4e570adba9b7dc5",
            "filename": "tests/models/owlv2/test_modeling_owlv2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -196,13 +196,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -340,13 +340,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -702,13 +702,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "315cdf813a6c67a1dd9b90905528b68ab4d27f4a",
            "filename": "tests/models/owlvit/test_modeling_owlvit.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -194,13 +194,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -336,13 +336,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -695,13 +695,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "b05998d1d0e2b458ba7e153dd42f493661edfcd2",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -243,7 +243,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successfull forward with no modifications\n+            _ = model(**input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n             input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n@@ -264,19 +264,19 @@ def test_mismatching_num_image_tokens(self):\n             _ = model(input_ids=input_ids, pixel_values=pixel_values)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "a905d4835007a52ddf28fa0169d36e6ea4bb7f37",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -239,7 +239,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successfull forward with no modifications\n+            _ = model(**input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n             input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n@@ -260,19 +260,19 @@ def test_mismatching_num_image_tokens(self):\n             _ = model(input_ids=input_ids, pixel_values=pixel_values)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "b4367ff79b2ebd807f5994cb74ef3cffd9287e45",
            "filename": "tests/models/pegasus/test_modeling_pegasus.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -288,19 +288,19 @@ def test_generate_fp16(self):\n         model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "dd6846ac4c0ef01537f2e60d9ed5b224c561e96b",
            "filename": "tests/models/pix2struct/test_modeling_pix2struct.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -197,13 +197,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -346,13 +346,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "fec49c5559b5ffcb1afd63a8a97db77b1414239e",
            "filename": "tests/models/plbart/test_modeling_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -330,7 +330,7 @@ def test_sample_generate(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass"
        },
        {
            "sha": "235db79a9dbee42afab820188e70ae5d54c4d0a0",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -249,7 +249,7 @@ def create_and_check_reformer_model_with_attn_mask(\n         model = ReformerModel(config=config)\n         model.to(torch_device)\n         model.eval()\n-        # set all position encodings to zero so that postions don't matter\n+        # set all position encodings to zero so that positions don't matter\n         with torch.no_grad():\n             embedding = model.embeddings.position_embeddings.embedding\n             embedding.weight = nn.Parameter(torch.zeros(embedding.weight.shape).to(torch_device))"
        },
        {
            "sha": "cf7599014c451261602a7d209410d0f441ea73ee",
            "filename": "tests/models/reformer/test_tokenization_reformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Freformer%2Ftest_tokenization_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Freformer%2Ftest_tokenization_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_tokenization_reformer.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -216,7 +216,7 @@ def test_tokenization_base_easy_symbols(self):\n     def test_tokenization_base_hard_symbols(self):\n         symbols = (\n             'This is a very long text with a lot of weird characters, such as: . , ~ ? ( ) \" [ ] ! : - . Also we will'\n-            \" add words that should not exsist and be tokenized to <unk>, such as saoneuhaoesuth\"\n+            \" add words that should not exist and be tokenized to <unk>, such as saoneuhaoesuth\"\n         )\n         original_tokenizer_encodings = [\n             108,"
        },
        {
            "sha": "0c1198626840bc3bb19c0e69ca7764c5e52955e8",
            "filename": "tests/models/roformer/test_modeling_roformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -486,19 +486,19 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "3c7e5a34b62eec204353e3cc9f656fe6b6cf10b8",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -406,13 +406,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "25bbc1c3040d483ffd84480f52b9d82ed82e1774",
            "filename": "tests/models/seamless_m4t/test_modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -444,25 +444,25 @@ def test_save_load_fast_init_from_base(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass\n@@ -695,19 +695,19 @@ def test_save_load_fast_init_from_base(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -719,7 +719,7 @@ def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass"
        },
        {
            "sha": "c5b10ea34da702ca58cfa962fde0b69b3df10859",
            "filename": "tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -460,25 +460,25 @@ def test_save_load_fast_init_from_base(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass\n@@ -698,25 +698,25 @@ def test_save_load_fast_init_from_base(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass"
        },
        {
            "sha": "11a3569f0671c18163c78d24d0460a7e0f6079b3",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -382,13 +382,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "bcd76a87e959491e14a8e23903d53c6972f3b036",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -324,13 +324,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "1c80e666d0ab2a6f9ccb2cb31ccd6118a2570b4f",
            "filename": "tests/models/speech_to_text/test_modeling_tf_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -247,13 +247,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "3c316dfee2536d7142c953df58f8abda34ea7261",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -704,13 +704,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -1021,13 +1021,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -1723,13 +1723,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "5310835e9873e9447c330f7ce5acfc8030d9b345",
            "filename": "tests/models/swin2sr/test_modeling_swin2sr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -213,13 +213,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "622c579843c887c8b2184060f583c2c628320814",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -694,7 +694,7 @@ def test_encoder_decoder_shared_weights(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_encoder_decoder_shared_weights(*config_and_inputs)\n \n-    @unittest.skipIf(torch_device == \"cpu\", \"Cant do half precision\")\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)\n@@ -744,7 +744,7 @@ def test_generate_with_head_masking(self):\n             self.assertEqual(sum([w.sum().item() for w in attn_weights]), 0.0)\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass\n@@ -867,13 +867,13 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    @unittest.skipIf(torch_device == \"cpu\", \"Cant do half precision\")\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)\n \n     @unittest.skip(\n-        reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n+        reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )\n     def test_load_save_without_tied_weights(self):\n         pass"
        },
        {
            "sha": "50abfaa765e357a37997a5e533739d400fcb9eb0",
            "filename": "tests/models/time_series_transformer/test_modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -369,19 +369,19 @@ def test_attention_outputs(self):\n         )\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "c5f485d7973c1c3fb2b17645e9efd55c212e57cc",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -330,13 +330,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "6a5e3957b7ad92054e087f7522c15e7940ede58e",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -564,19 +564,19 @@ def test_generate_with_head_masking(self):\n             self.assertEqual(sum([w.sum().item() for w in attn_weights]), 0.0)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "d46b1ae111f490c84c3a3ce93725eee85c0fa5f2",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -209,19 +209,19 @@ def test_config(self):\n         self.config_tester.run_common_tests()\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "f85a851dd0a41253e221a46b9badad3a427549c5",
            "filename": "tests/models/vilt/test_modeling_vilt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -319,13 +319,13 @@ def test_training_gradient_checkpointing(self):\n             loss.backward()\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "8c8b3ea4df7e207090d726afee7dbb77f160319a",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -237,7 +237,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successfull forward with no modifications\n+            _ = model(**input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n             input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n@@ -289,19 +289,19 @@ def test_vision_feature_layers(self, vision_feature_layers):\n             model(**input_dict)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "1c32f26a62445e0f997744a7555b53a1149bd963",
            "filename": "tests/models/visual_bert/test_modeling_visual_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -555,19 +555,19 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "5abbc774135d81335d39e64fb0a54aeb1e0a38a0",
            "filename": "tests/models/vitmatte/test_modeling_vitmatte.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -171,13 +171,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "3dadab891e52ebe264662db05842d7c7c8e14e40",
            "filename": "tests/models/wav2vec2/test_modeling_flax_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_flax_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_flax_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_flax_wav2vec2.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -412,7 +412,7 @@ def test_sample_negatives(self):\n \n         features = (np.arange(sequence_length * hidden_size) // hidden_size).reshape(\n             sequence_length, hidden_size\n-        )  # each value in vector consits of same value\n+        )  # each value in vector consists of same value\n         features = np.broadcast_to(features[None, :], (batch_size, sequence_length, hidden_size))\n \n         negative_indices = _sample_negative_indices(features.shape, num_negatives)\n@@ -442,7 +442,7 @@ def test_sample_negatives_with_attn_mask(self):\n \n         features = (np.arange(sequence_length * hidden_size) // hidden_size).reshape(\n             sequence_length, hidden_size\n-        )  # each value in vector consits of same value\n+        )  # each value in vector consists of same value\n \n         # second half of last input tensor is padded\n         attention_mask = np.ones((batch_size, sequence_length), dtype=np.int8)"
        },
        {
            "sha": "8199ba04e1f8e597f6cd5984aec0f7a199a7597c",
            "filename": "tests/models/wav2vec2/test_modeling_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -1392,7 +1392,7 @@ def test_sample_negatives(self):\n         sequence = torch.div(\n             torch.arange(sequence_length * hidden_size, device=torch_device), hidden_size, rounding_mode=\"floor\"\n         )\n-        features = sequence.view(sequence_length, hidden_size)  # each value in vector consits of same value\n+        features = sequence.view(sequence_length, hidden_size)  # each value in vector consists of same value\n         features = features[None, :].expand(batch_size, sequence_length, hidden_size).contiguous()\n \n         # sample negative indices\n@@ -1422,7 +1422,7 @@ def test_sample_negatives_with_mask(self):\n         sequence = torch.div(\n             torch.arange(sequence_length * hidden_size, device=torch_device), hidden_size, rounding_mode=\"floor\"\n         )\n-        features = sequence.view(sequence_length, hidden_size)  # each value in vector consits of same value\n+        features = sequence.view(sequence_length, hidden_size)  # each value in vector consists of same value\n         features = features[None, :].expand(batch_size, sequence_length, hidden_size).contiguous()\n \n         # replace masked feature vectors with -100 to test that those are not sampled"
        },
        {
            "sha": "0bcdcf3e05a35bd8eda6f116d04b3f1d2eb38d38",
            "filename": "tests/models/wav2vec2_bert/test_modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -782,7 +782,7 @@ def test_sample_negatives(self):\n \n         features = (torch.arange(sequence_length * hidden_size, device=torch_device) // hidden_size).view(\n             sequence_length, hidden_size\n-        )  # each value in vector consits of same value\n+        )  # each value in vector consists of same value\n         features = features[None, :].expand(batch_size, sequence_length, hidden_size).contiguous()\n \n         # sample negative indices\n@@ -811,7 +811,7 @@ def test_sample_negatives_with_mask(self):\n \n         features = (torch.arange(sequence_length * hidden_size, device=torch_device) // hidden_size).view(\n             sequence_length, hidden_size\n-        )  # each value in vector consits of same value\n+        )  # each value in vector consists of same value\n         features = features[None, :].expand(batch_size, sequence_length, hidden_size).contiguous()\n \n         # replace masked feature vectors with -100 to test that those are not sampled"
        },
        {
            "sha": "0173135d52da9aabe2ad999fa276a9d925715f08",
            "filename": "tests/models/wav2vec2_conformer/test_modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -800,7 +800,7 @@ def test_sample_negatives(self):\n \n         features = (torch.arange(sequence_length * hidden_size, device=torch_device) // hidden_size).view(\n             sequence_length, hidden_size\n-        )  # each value in vector consits of same value\n+        )  # each value in vector consists of same value\n         features = features[None, :].expand(batch_size, sequence_length, hidden_size).contiguous()\n \n         # sample negative indices\n@@ -829,7 +829,7 @@ def test_sample_negatives_with_mask(self):\n \n         features = (torch.arange(sequence_length * hidden_size, device=torch_device) // hidden_size).view(\n             sequence_length, hidden_size\n-        )  # each value in vector consits of same value\n+        )  # each value in vector consists of same value\n         features = features[None, :].expand(batch_size, sequence_length, hidden_size).contiguous()\n \n         # replace masked feature vectors with -100 to test that those are not sampled"
        },
        {
            "sha": "a05e31bf932943948548d6ed2bee0867559fac76",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -546,13 +546,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "85265589050a1d6ff9dd4aa30d2dea378a0aa823",
            "filename": "tests/models/x_clip/test_modeling_x_clip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -195,13 +195,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -431,13 +431,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass"
        },
        {
            "sha": "7d9bc428ed1bccf12c1901f844c3251971c56417",
            "filename": "tests/pipelines/test_pipelines_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_common.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -251,7 +251,7 @@ def test_pipeline_with_task_parameters_no_side_effects(self):\n         self.assertTrue(model.generation_config.num_beams == 1)\n \n         # Under the hood: we now store a generation config in the pipeline. This generation config stores the\n-        # task-specific paremeters.\n+        # task-specific parameters.\n         self.assertTrue(pipe.generation_config.num_beams == 4)\n \n         # We can confirm that the task-specific parameters have an effect. (In this case, the default is `num_beams=1`,"
        },
        {
            "sha": "17f3b9adb9c716fac45bd7493864ef049541ab1d",
            "filename": "tests/pipelines/test_pipelines_document_question_answering.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_document_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_document_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_document_question_answering.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -147,7 +147,7 @@ def test_small_model_pt(self):\n         outputs = dqa_pipeline(image=image, question=question, top_k=2)\n         self.assertEqual(outputs, [])\n \n-        # We can optionnally pass directly the words and bounding boxes\n+        # We can optionally pass directly the words and bounding boxes\n         image = \"./tests/fixtures/tests_samples/COCO/000000039769.png\"\n         words = []\n         boxes = []\n@@ -183,7 +183,7 @@ def test_small_model_pt_bf16(self):\n         outputs = dqa_pipeline(image=image, question=question, top_k=2)\n         self.assertEqual(outputs, [])\n \n-        # We can optionnally pass directly the words and bounding boxes\n+        # We can optionally pass directly the words and bounding boxes\n         image = \"./tests/fixtures/tests_samples/COCO/000000039769.png\"\n         words = []\n         boxes = []"
        },
        {
            "sha": "12bc3dc655bd26da5898d028b4c233d4b075534d",
            "filename": "tests/pipelines/test_pipelines_feature_extraction.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_feature_extraction.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_feature_extraction.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_feature_extraction.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -196,7 +196,7 @@ def get_test_pipeline(\n         elif model.config.is_encoder_decoder:\n             self.skipTest(\n                 \"\"\"encoder_decoder models are trickier for this pipeline.\n-                Do we want encoder + decoder inputs to get some featues?\n+                Do we want encoder + decoder inputs to get some features?\n                 Do we want encoder only features ?\n                 For now ignore those.\n                 \"\"\""
        },
        {
            "sha": "d5d441bda694d283a92d2b5e470e134c70f82636",
            "filename": "tests/pipelines/test_pipelines_image_feature_extraction.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_image_feature_extraction.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_image_feature_extraction.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_feature_extraction.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -177,7 +177,7 @@ def get_test_pipeline(\n         elif model.config.is_encoder_decoder:\n             self.skipTest(\n                 \"\"\"encoder_decoder models are trickier for this pipeline.\n-                Do we want encoder + decoder inputs to get some featues?\n+                Do we want encoder + decoder inputs to get some features?\n                 Do we want encoder only features ?\n                 For now ignore those.\n                 \"\"\""
        },
        {
            "sha": "9b061032ea467b08c1198a41a33dd144c433eda0",
            "filename": "tests/pipelines/test_pipelines_question_answering.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_question_answering.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -347,7 +347,7 @@ def test_large_model_issue(self):\n                     \" Yes Bank a loss of  1,800 crore by extending credit facilities to Avantha Group, when it was\"\n                     \" not eligible for the same\"\n                 ),\n-                \"question\": \"Is this person invovled in fraud?\",\n+                \"question\": \"Is this person involved in fraud?\",\n             }\n         )\n         self.assertEqual("
        },
        {
            "sha": "b3e25dbe23064be4920294e75a9b99a3b900d2ba",
            "filename": "tests/pipelines/test_pipelines_text_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_text_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_text_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_classification.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -109,7 +109,7 @@ def test_small_model_pt(self):\n         )\n \n         # Do not apply any function to output for regression tasks\n-        # hack: changing problem_type artifically (so keep this test at last)\n+        # hack: changing problem_type artificially (so keep this test at last)\n         text_classifier.model.config.problem_type = \"regression\"\n         outputs = text_classifier(\"This is great !\")\n         self.assertEqual(nested_simplify(outputs), [{\"label\": \"LABEL_0\", \"score\": 0.01}])"
        },
        {
            "sha": "9df47d5a225891dbe408afaaa989f85fdb63978a",
            "filename": "tests/pipelines/test_pipelines_text_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_generation.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -500,7 +500,7 @@ def run_pipeline_test(self, text_generator, _):\n         with self.assertRaises(ValueError):\n             outputs = text_generator(\"test\", return_text=True, return_tensors=True)\n \n-        # Empty prompt is slighly special\n+        # Empty prompt is slightly special\n         # it requires BOS token to exist.\n         # Special case for Pegasus which will always append EOS so will\n         # work even without BOS.\n@@ -637,7 +637,7 @@ def test_pipeline_length_setting_warning(self):\n             logger = logging.get_logger(\"transformers.generation.tf_utils\")\n         else:\n             logger = logging.get_logger(\"transformers.generation.utils\")\n-        logger_msg = \"Both `max_new_tokens`\"  # The beggining of the message to be checked in this test\n+        logger_msg = \"Both `max_new_tokens`\"  # The beginning of the message to be checked in this test\n \n         # Both are set by the user -> log warning\n         with CaptureLogger(logger) as cl:"
        },
        {
            "sha": "d37dd92c715d3b88c627c661f66064c1f3f0c8e1",
            "filename": "tests/test_configuration_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Ftest_configuration_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Ftest_configuration_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_configuration_common.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -118,7 +118,7 @@ def create_and_test_config_from_and_save_pretrained_subfolder(self):\n \n     def create_and_test_config_from_and_save_pretrained_composite(self):\n         \"\"\"\n-        Tests that composite or nested cofigs can be loaded and saved correctly. In case the config\n+        Tests that composite or nested configs can be loaded and saved correctly. In case the config\n         has a sub-config, we should be able to call `sub_config.from_pretrained('general_config_file')`\n         and get a result same as if we loaded the whole config and obtained `config.sub_config` from it.\n         \"\"\""
        },
        {
            "sha": "0df30adada683c4cb1daefe3aee82f642f444078",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -392,7 +392,7 @@ def test_cast_dtype_device(self):\n                 image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n \n                 encoding = image_processor(image_inputs, return_tensors=\"pt\")\n-                # for layoutLM compatiblity\n+                # for layoutLM compatibility\n                 self.assertEqual(encoding.pixel_values.device, torch.device(\"cpu\"))\n                 self.assertEqual(encoding.pixel_values.dtype, torch.float32)\n "
        },
        {
            "sha": "0c0005d8287326148e69276e4c686f40930ca03d",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -1490,7 +1490,7 @@ def test_headmasking(self):\n             if model.config.is_encoder_decoder:\n                 signature = inspect.signature(model.forward)\n                 arg_names = [*signature.parameters.keys()]\n-                if \"decoder_head_mask\" in arg_names:  # necessary diferentiation because of T5 model\n+                if \"decoder_head_mask\" in arg_names:  # necessary differentiation because of T5 model\n                     inputs[\"decoder_head_mask\"] = head_mask\n                 if \"cross_attn_head_mask\" in arg_names:\n                     inputs[\"cross_attn_head_mask\"] = head_mask\n@@ -1852,7 +1852,7 @@ def test_resize_position_vector_embeddings(self):\n                 cloned_embeddings = model_embed.weight.clone()\n \n             # Check that resizing the position embeddings with a larger max_position_embeddings increases\n-            # the model's postion embeddings size\n+            # the model's position embeddings size\n             model.resize_position_embeddings(max_position_embeddings + 10)\n             self.assertEqual(model.config.max_position_embeddings, max_position_embeddings + 10)\n \n@@ -3998,7 +3998,7 @@ def test_flash_attn_2_can_dispatch_composite_models(self):\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             model = model_class(config)\n             if not self._is_composite:\n-                self.skipTest(\"This model is not a composte model!\")\n+                self.skipTest(\"This model is not a composite model!\")\n \n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n@@ -4411,9 +4411,9 @@ def recursively_check(eager_outputs, exported_outputs):\n                     exported_outputs = exported_model.module().forward(**inputs_dict)\n \n                 # Check if outputs are close:\n-                # is_tested is a boolean flag idicating if we comapre any outputs,\n+                # is_tested is a boolean flag indicating if we compare any outputs,\n                 # e.g. there might be a situation when outputs are empty list, then is_tested will be False.\n-                # In case of outputs are different the error will be rasied in `recursively_check` function.\n+                # In case of outputs are different the error will be raised in `recursively_check` function.\n                 is_tested = recursively_check(eager_outputs, exported_outputs)\n                 self.assertTrue(is_tested, msg=f\"No outputs were compared for {model_class.__name__}\")\n "
        },
        {
            "sha": "82cbf0901c93d2beca73f1004e56cb8137f8af09",
            "filename": "tests/test_modeling_flax_common.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Ftest_modeling_flax_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Ftest_modeling_flax_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_flax_common.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -680,14 +680,14 @@ def test_no_automatic_init(self):\n         for model_class in self.all_model_classes:\n             model = model_class(config, _do_init=False)\n \n-            # Check that accesing parmas raises an ValueError when _do_init is False\n+            # Check that accessing params raises an ValueError when _do_init is False\n             with self.assertRaises(ValueError):\n                 params = model.params\n \n             # Check if we params can be properly initialized when calling init_weights\n             params = model.init_weights(model.key, model.input_shape)\n             assert isinstance(params, (dict, FrozenDict)), f\"params are not an instance of {FrozenDict}\"\n-            # Check if all required parmas are initialized\n+            # Check if all required params are initialized\n             keys = set(flatten_dict(unfreeze(params)).keys())\n             self.assertTrue(all(k in keys for k in model.required_params))\n             # Check if the shapes match\n@@ -713,7 +713,7 @@ def test_from_pretrained_with_no_automatic_init(self):\n         config.return_dict = True\n \n         def _assert_all_params_initialised(model, params):\n-            # Check if all required parmas are loaded\n+            # Check if all required params are loaded\n             keys = set(flatten_dict(unfreeze(params)).keys())\n             self.assertTrue(all(k in keys for k in model.required_params))\n             # Check if the shapes match\n@@ -735,11 +735,11 @@ def _assert_all_params_initialised(model, params):\n                 model.save_pretrained(tmpdirname)\n                 model, params = model_class.from_pretrained(tmpdirname, _do_init=False)\n \n-            # Check that accesing parmas raises an ValueError when _do_init is False\n+            # Check that accessing params raises an ValueError when _do_init is False\n             with self.assertRaises(ValueError):\n                 params = model.params\n \n-            # Check if all required parmas are loaded\n+            # Check if all required params are loaded\n             _assert_all_params_initialised(model, params)\n \n             # Check that setting params raises an ValueError when _do_init is False\n@@ -757,7 +757,7 @@ def _assert_all_params_initialised(model, params):\n                 model, params = model_class.from_pretrained(tmpdirname, _do_init=False)\n \n                 params = model.init_weights(model.key, model.input_shape, params=params)\n-                # Check if all required parmas are loaded\n+                # Check if all required params are loaded\n                 _assert_all_params_initialised(model, params)\n \n     def test_checkpoint_sharding_from_hub(self):"
        },
        {
            "sha": "f5723d58320f9896055d2d190e9561db766847a2",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/996f512d52908beb7b89d835fe7eb7b900f78ad0/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=996f512d52908beb7b89d835fe7eb7b900f78ad0",
            "patch": "@@ -838,7 +838,7 @@ def test_added_tokens_do_lower_case(self):\n                 toks_after_adding = tokenizer.tokenize(text)\n                 toks_after_adding2 = tokenizer.tokenize(text2)\n \n-                # Rust tokenizers dont't lowercase added tokens at the time calling `tokenizer.add_tokens`,\n+                # Rust tokenizers don't lowercase added tokens at the time calling `tokenizer.add_tokens`,\n                 # while python tokenizers do, so new_toks 0 and 2 would be treated as the same, so do new_toks 1 and 3.\n                 self.assertIn(added, [2, 4])\n "
        }
    ],
    "stats": {
        "total": 564,
        "additions": 282,
        "deletions": 282
    }
}