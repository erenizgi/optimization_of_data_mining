{
    "author": "Vixel2006",
    "message": "[Feature] Support `is_split_into_words` in the `TokenClassificationPipeline`. (#38818)\n\n* some fixes\n\n* some fixes\n\n* now the pipeline can take list of tokens as input and is_split_into_words argument\n\n* now the pipeline can take list of tokens as input and is_split_into_words argument\n\n* now the pipeline can take list of tokens as input and is_split_into_words argument and we can handle batches of tokenized input\n\n* now the pipeline can take list of tokens as input and is_split_into_words argument and we can handle batches of tokenized input\n\n* solving test problems\n\n* some fixes\n\n* some fixes\n\n* modify tests\n\n* aligning start and end correctly\n\n* adding tests\n\n* some formatting\n\n* some formatting\n\n* some fixes\n\n* some fixes\n\n* some fixes\n\n* resolve conflicts\n\n* removing unimportant lines\n\n* removing unimportant lines\n\n* generalize to other languages\n\n* generalize to other languages\n\n* generalize to other languages\n\n* generalize to other languages",
    "sha": "9eac19eb598f10f76b1f276a36a8c1f6690d200e",
    "files": [
        {
            "sha": "2a2a6bbe109233fdfbdd9cdbb4e36723ec9fd455",
            "filename": "src/transformers/pipelines/token_classification.py",
            "status": "modified",
            "additions": 84,
            "deletions": 7,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/9eac19eb598f10f76b1f276a36a8c1f6690d200e/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9eac19eb598f10f76b1f276a36a8c1f6690d200e/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py?ref=9eac19eb598f10f76b1f276a36a8c1f6690d200e",
            "patch": "@@ -30,14 +30,17 @@ class TokenClassificationArgumentHandler(ArgumentHandler):\n     \"\"\"\n \n     def __call__(self, inputs: Union[str, list[str]], **kwargs):\n+        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n+        delimiter = kwargs.get(\"delimiter\", None)\n+\n         if inputs is not None and isinstance(inputs, (list, tuple)) and len(inputs) > 0:\n             inputs = list(inputs)\n             batch_size = len(inputs)\n         elif isinstance(inputs, str):\n             inputs = [inputs]\n             batch_size = 1\n         elif Dataset is not None and isinstance(inputs, Dataset) or isinstance(inputs, types.GeneratorType):\n-            return inputs, None\n+            return inputs, is_split_into_words, None, delimiter\n         else:\n             raise ValueError(\"At least one input is required.\")\n \n@@ -47,7 +50,7 @@ def __call__(self, inputs: Union[str, list[str]], **kwargs):\n                 offset_mapping = [offset_mapping]\n             if len(offset_mapping) != batch_size:\n                 raise ValueError(\"offset_mapping should have the same batch size as the input\")\n-        return inputs, offset_mapping\n+        return inputs, is_split_into_words, offset_mapping, delimiter\n \n \n class AggregationStrategy(ExplicitEnum):\n@@ -135,6 +138,7 @@ class TokenClassificationPipeline(ChunkPipeline):\n \n     def __init__(self, args_parser=TokenClassificationArgumentHandler(), *args, **kwargs):\n         super().__init__(*args, **kwargs)\n+\n         self.check_model_type(\n             TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES\n             if self.framework == \"tf\"\n@@ -151,9 +155,16 @@ def _sanitize_parameters(\n         ignore_subwords: Optional[bool] = None,\n         aggregation_strategy: Optional[AggregationStrategy] = None,\n         offset_mapping: Optional[list[tuple[int, int]]] = None,\n+        is_split_into_words: Optional[bool] = False,\n         stride: Optional[int] = None,\n+        delimiter: Optional[str] = None,\n     ):\n         preprocess_params = {}\n+        preprocess_params[\"is_split_into_words\"] = is_split_into_words\n+\n+        if is_split_into_words:\n+            preprocess_params[\"delimiter\"] = \" \" if delimiter is None else delimiter\n+\n         if offset_mapping is not None:\n             preprocess_params[\"offset_mapping\"] = offset_mapping\n \n@@ -230,8 +241,9 @@ def __call__(\n         Classify each token of the text(s) given as inputs.\n \n         Args:\n-            inputs (`str` or `list[str]`):\n-                One or several texts (or one list of texts) for token classification.\n+            inputs (`str` or `List[str]`):\n+                One or several texts (or one list of texts) for token classification. Can be pre-tokenized when\n+                `is_split_into_words=True`.\n \n         Return:\n             A list or a list of list of `dict`: Each result comes as a list of dictionaries (one for each token in the\n@@ -251,7 +263,11 @@ def __call__(\n               exists if the offsets are available within the tokenizer\n         \"\"\"\n \n-        _inputs, offset_mapping = self._args_parser(inputs, **kwargs)\n+        _inputs, is_split_into_words, offset_mapping, delimiter = self._args_parser(inputs, **kwargs)\n+        kwargs[\"is_split_into_words\"] = is_split_into_words\n+        kwargs[\"delimiter\"] = delimiter\n+        if is_split_into_words and not all(isinstance(input, list) for input in inputs):\n+            return super().__call__([inputs], **kwargs)\n         if offset_mapping:\n             kwargs[\"offset_mapping\"] = offset_mapping\n \n@@ -260,14 +276,43 @@ def __call__(\n     def preprocess(self, sentence, offset_mapping=None, **preprocess_params):\n         tokenizer_params = preprocess_params.pop(\"tokenizer_params\", {})\n         truncation = True if self.tokenizer.model_max_length and self.tokenizer.model_max_length > 0 else False\n+\n+        word_to_chars_map = None\n+        is_split_into_words = preprocess_params[\"is_split_into_words\"]\n+        if is_split_into_words:\n+            delimiter = preprocess_params[\"delimiter\"]\n+            if not isinstance(sentence, list):\n+                raise ValueError(\"When `is_split_into_words=True`, `sentence` must be a list of tokens.\")\n+            words = sentence\n+            sentence = delimiter.join(words)  # Recreate the sentence string for later display and slicing\n+            # This map will allows to convert back word => char indices\n+            word_to_chars_map = []\n+            delimiter_len = len(delimiter)\n+            char_offset = 0\n+            for word in words:\n+                word_to_chars_map.append((char_offset, char_offset + len(word)))\n+                char_offset += len(word) + delimiter_len\n+\n+            # We use `words` as the actual input for the tokenizer\n+            text_to_tokenize = words\n+            tokenizer_params[\"is_split_into_words\"] = True\n+        else:\n+            if not isinstance(sentence, str):\n+                raise ValueError(\"When `is_split_into_words=False`, `sentence` must be an untokenized string.\")\n+            text_to_tokenize = sentence\n+\n         inputs = self.tokenizer(\n-            sentence,\n+            text_to_tokenize,\n             return_tensors=self.framework,\n             truncation=truncation,\n             return_special_tokens_mask=True,\n             return_offsets_mapping=self.tokenizer.is_fast,\n             **tokenizer_params,\n         )\n+\n+        if is_split_into_words and not self.tokenizer.is_fast:\n+            raise ValueError(\"is_split_into_words=True is only supported with fast tokenizers.\")\n+\n         inputs.pop(\"overflow_to_sample_mapping\", None)\n         num_chunks = len(inputs[\"input_ids\"])\n \n@@ -278,8 +323,12 @@ def preprocess(self, sentence, offset_mapping=None, **preprocess_params):\n                 model_inputs = {k: v[i].unsqueeze(0) for k, v in inputs.items()}\n             if offset_mapping is not None:\n                 model_inputs[\"offset_mapping\"] = offset_mapping\n+\n             model_inputs[\"sentence\"] = sentence if i == 0 else None\n             model_inputs[\"is_last\"] = i == num_chunks - 1\n+            if word_to_chars_map is not None:\n+                model_inputs[\"word_ids\"] = inputs.word_ids(i)\n+                model_inputs[\"word_to_chars_map\"] = word_to_chars_map\n \n             yield model_inputs\n \n@@ -289,6 +338,9 @@ def _forward(self, model_inputs):\n         offset_mapping = model_inputs.pop(\"offset_mapping\", None)\n         sentence = model_inputs.pop(\"sentence\")\n         is_last = model_inputs.pop(\"is_last\")\n+        word_ids = model_inputs.pop(\"word_ids\", None)\n+        word_to_chars_map = model_inputs.pop(\"word_to_chars_map\", None)\n+\n         if self.framework == \"tf\":\n             logits = self.model(**model_inputs)[0]\n         else:\n@@ -301,13 +353,19 @@ def _forward(self, model_inputs):\n             \"offset_mapping\": offset_mapping,\n             \"sentence\": sentence,\n             \"is_last\": is_last,\n+            \"word_ids\": word_ids,\n+            \"word_to_chars_map\": word_to_chars_map,\n             **model_inputs,\n         }\n \n     def postprocess(self, all_outputs, aggregation_strategy=AggregationStrategy.NONE, ignore_labels=None):\n         if ignore_labels is None:\n             ignore_labels = [\"O\"]\n         all_entities = []\n+\n+        # Get map from the first output, it's the same for all chunks\n+        word_to_chars_map = all_outputs[0].get(\"word_to_chars_map\")\n+\n         for model_outputs in all_outputs:\n             if self.framework == \"pt\" and model_outputs[\"logits\"][0].dtype in (torch.bfloat16, torch.float16):\n                 logits = model_outputs[\"logits\"][0].to(torch.float32).numpy()\n@@ -320,6 +378,7 @@ def postprocess(self, all_outputs, aggregation_strategy=AggregationStrategy.NONE\n                 model_outputs[\"offset_mapping\"][0] if model_outputs[\"offset_mapping\"] is not None else None\n             )\n             special_tokens_mask = model_outputs[\"special_tokens_mask\"][0].numpy()\n+            word_ids = model_outputs.get(\"word_ids\")\n \n             maxes = np.max(logits, axis=-1, keepdims=True)\n             shifted_exp = np.exp(logits - maxes)\n@@ -330,7 +389,14 @@ def postprocess(self, all_outputs, aggregation_strategy=AggregationStrategy.NONE\n                 offset_mapping = offset_mapping.numpy() if offset_mapping is not None else None\n \n             pre_entities = self.gather_pre_entities(\n-                sentence, input_ids, scores, offset_mapping, special_tokens_mask, aggregation_strategy\n+                sentence,\n+                input_ids,\n+                scores,\n+                offset_mapping,\n+                special_tokens_mask,\n+                aggregation_strategy,\n+                word_ids=word_ids,\n+                word_to_chars_map=word_to_chars_map,\n             )\n             grouped_entities = self.aggregate(pre_entities, aggregation_strategy)\n             # Filter anything that is in self.ignore_labels\n@@ -374,6 +440,8 @@ def gather_pre_entities(\n         offset_mapping: Optional[list[tuple[int, int]]],\n         special_tokens_mask: np.ndarray,\n         aggregation_strategy: AggregationStrategy,\n+        word_ids: Optional[list[Optional[int]]] = None,\n+        word_to_chars_map: Optional[list[tuple[int, int]]] = None,\n     ) -> list[dict]:\n         \"\"\"Fuse various numpy arrays into dicts with all the information needed for aggregation\"\"\"\n         pre_entities = []\n@@ -385,6 +453,15 @@ def gather_pre_entities(\n             word = self.tokenizer.convert_ids_to_tokens(int(input_ids[idx]))\n             if offset_mapping is not None:\n                 start_ind, end_ind = offset_mapping[idx]\n+\n+                # If the input is pre-tokenized, we need to rescale the offsets to the absolute sentence.\n+                if word_ids is not None and word_to_chars_map is not None:\n+                    word_index = word_ids[idx]\n+                    if word_index is not None:\n+                        start_char, _ = word_to_chars_map[word_index]\n+                        start_ind += start_char\n+                        end_ind += start_char\n+\n                 if not isinstance(start_ind, int):\n                     if self.framework == \"pt\":\n                         start_ind = start_ind.item()"
        },
        {
            "sha": "643e4d6675d941474b15bcd6eab4787591c5de45",
            "filename": "tests/pipelines/test_pipelines_token_classification.py",
            "status": "modified",
            "additions": 57,
            "deletions": 4,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/9eac19eb598f10f76b1f276a36a8c1f6690d200e/tests%2Fpipelines%2Ftest_pipelines_token_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9eac19eb598f10f76b1f276a36a8c1f6690d200e/tests%2Fpipelines%2Ftest_pipelines_token_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_token_classification.py?ref=9eac19eb598f10f76b1f276a36a8c1f6690d200e",
            "patch": "@@ -308,6 +308,54 @@ def test_chunking(self):\n             ],\n         )\n \n+    @require_torch\n+    @slow\n+    def test_is_split_into_words(self):\n+        \"\"\"\n+        Tests the pipeline with pre-tokenized inputs (is_split_into_words=True)\n+        and validates that the character offsets are correct.\n+        \"\"\"\n+        token_classifier = pipeline(task=\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n+\n+        # Input is a list of words\n+        words = [\"Hello\", \"Sarah\", \"lives\", \"in\", \"New\", \"York\"]\n+\n+        # The reconstructed sentence will be \"Hello Sarah lives in New York\"\n+        # - \"Sarah\": starts at index 6, ends at 11\n+        # - \"New York\": starts at index 21, ends at 29\n+\n+        output = token_classifier(words, is_split_into_words=True)\n+\n+        self.assertEqual(\n+            nested_simplify(output),\n+            [\n+                {\"entity_group\": \"PER\", \"score\": ANY(float), \"word\": \"Sarah\", \"start\": 6, \"end\": 11},\n+                {\"entity_group\": \"LOC\", \"score\": ANY(float), \"word\": \"New York\", \"start\": 21, \"end\": 29},\n+            ],\n+        )\n+\n+        # Also test batching with pre-tokenized inputs\n+        words2 = [\"My\", \"name\", \"is\", \"Wolfgang\", \"and\", \"I\", \"live\", \"in\", \"Berlin\"]\n+        batch_output = token_classifier([words, words2], is_split_into_words=True)\n+\n+        # Expected for second sentence (\"My name is Wolfgang and I live in Berlin\")\n+        # - \"Wolfgang\": starts at 12, ends at 20\n+        # - \"Berlin\": starts at 36, ends at 42\n+\n+        self.assertEqual(\n+            nested_simplify(batch_output),\n+            [\n+                [\n+                    {\"entity_group\": \"PER\", \"score\": ANY(float), \"word\": \"Sarah\", \"start\": 6, \"end\": 11},\n+                    {\"entity_group\": \"LOC\", \"score\": ANY(float), \"word\": \"New York\", \"start\": 21, \"end\": 29},\n+                ],\n+                [\n+                    {\"entity_group\": \"PER\", \"score\": ANY(float), \"word\": \"Wolfgang\", \"start\": 12, \"end\": 20},\n+                    {\"entity_group\": \"LOC\", \"score\": ANY(float), \"word\": \"Berlin\", \"start\": 36, \"end\": 42},\n+                ],\n+            ],\n+        )\n+\n     @require_torch\n     def test_chunking_fast(self):\n         # Note: We cannot run the test on \"conflicts\" on the chunking.\n@@ -953,19 +1001,24 @@ def setUp(self):\n     def test_simple(self):\n         string = \"This is a simple input\"\n \n-        inputs, offset_mapping = self.args_parser(string)\n+        inputs, is_split_into_words, offset_mapping, delimiter = self.args_parser(string)\n         self.assertEqual(inputs, [string])\n+        self.assertFalse(is_split_into_words)\n         self.assertEqual(offset_mapping, None)\n \n-        inputs, offset_mapping = self.args_parser([string, string])\n+        inputs, is_split_into_words, offset_mapping, delimiter = self.args_parser([string, string])\n         self.assertEqual(inputs, [string, string])\n+        self.assertFalse(is_split_into_words)\n         self.assertEqual(offset_mapping, None)\n \n-        inputs, offset_mapping = self.args_parser(string, offset_mapping=[(0, 1), (1, 2)])\n+        inputs, is_split_into_words, offset_mapping, delimiter = self.args_parser(\n+            string, offset_mapping=[(0, 1), (1, 2)]\n+        )\n         self.assertEqual(inputs, [string])\n+        self.assertFalse(is_split_into_words)\n         self.assertEqual(offset_mapping, [[(0, 1), (1, 2)]])\n \n-        inputs, offset_mapping = self.args_parser(\n+        inputs, is_split_into_words, offset_mapping, delimiter = self.args_parser(\n             [string, string], offset_mapping=[[(0, 1), (1, 2)], [(0, 2), (2, 3)]]\n         )\n         self.assertEqual(inputs, [string, string])"
        }
    ],
    "stats": {
        "total": 152,
        "additions": 141,
        "deletions": 11
    }
}