{
    "author": "Rocketknight1",
    "message": "Fix how we compute the final non-padding token for ForSequenceClassification models (#35911)\n\n* Fix how we compute the final non-padding token for Gemma (and probably other models)\r\n\r\n* .size() -> .shape[]\r\n\r\n* Propagating changes to other models\r\n\r\n* Propagating changes to other models\r\n\r\n* Change it for all ForSequenceClassification models\r\n\r\n* Fix batch dim\r\n\r\n* More TF fixes\r\n\r\n* Copy the TF fix around as well\r\n\r\n* Correct layer name for TFCTRL\r\n\r\n* Cleaner .to()\r\n\r\n* Clean up the nested if-else\r\n\r\n* Use argmax() instead of .max().values",
    "sha": "694aaa7fbccb801de013a99cb558f5a20dfdc02a",
    "files": [
        {
            "sha": "02949cb1ae25b9bbe09e8878c3e56d6ed706a98d",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 12,
            "deletions": 13,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1127,21 +1127,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-                logger.warning_once(\n-                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-                )\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "3dec261e4c66207213b2896f79ef5c719a852e1e",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -789,23 +789,21 @@ def forward(\n \n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-                logger.warning_once(\n-                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-                )\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[range(batch_size), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "8f35076ff9496c5d96f7265a72f2db0c729fb82e",
            "filename": "src/transformers/models/ctrl/modeling_tf_ctrl.py",
            "status": "modified",
            "additions": 12,
            "deletions": 21,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -868,42 +868,33 @@ def call(\n             return_dict=return_dict,\n             training=training,\n         )\n-\n         hidden_states = transformer_outputs[0]\n         logits = self.classifier(hidden_states)\n-        in_logits = None\n+        logits_shape = shape_list(logits)\n+        batch_size = logits_shape[0]\n+\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = tf.fill((batch_size,), value=logits_shape[1] - 1)\n         else:\n             if input_ids is not None:\n-                sequence_lengths = (\n-                    tf.argmax(tf.cast(tf.math.equal(input_ids, self.config.pad_token_id), input_ids.dtype), axis=-1)\n-                    - 1\n-                )\n-                sequence_lengths = tf.where(sequence_lengths >= 0, sequence_lengths, input_ids.shape[-1] - 1)\n-                in_logits = tf.gather(logits, sequence_lengths, batch_dims=1, axis=1)\n+                token_indices = tf.range(shape_list(input_ids)[-1])\n+                non_pad_mask = tf.cast(input_ids != self.config.pad_token_id, token_indices.dtype)\n+                last_non_pad_token = tf.reduce_max(token_indices * non_pad_mask, axis=-1)\n             else:\n-                sequence_lengths = -1\n+                last_non_pad_token = tf.fill((batch_size,), value=logits_shape[1] - 1)\n                 logger.warning_once(\n                     f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n                     \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n                 )\n         loss = None\n \n+        pooled_logits = tf.gather(logits, last_non_pad_token, batch_dims=1, axis=1)\n+\n         if labels is not None:\n-            if input_ids is not None:\n-                batch_size, sequence_length = shape_list(input_ids)[:2]\n-            else:\n-                batch_size, sequence_length = shape_list(inputs_embeds)[:2]\n-            if self.config.pad_token_id is None and batch_size != 1:\n+            if self.config.pad_token_id is None and logits_shape[0] != 1:\n                 raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n \n-            if not tf.is_tensor(sequence_lengths):\n-                in_logits = logits[0:batch_size, sequence_lengths]\n-\n-            loss = self.hf_compute_loss(tf.reshape(labels, [-1, 1]), tf.reshape(in_logits, [-1, self.num_labels]))\n-\n-        pooled_logits = in_logits if in_logits is not None else logits\n+            loss = self.hf_compute_loss(tf.reshape(labels, [-1]), tf.reshape(pooled_logits, [-1, self.num_labels]))\n \n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]"
        },
        {
            "sha": "38d9d3ce001d52cf6e34beb4fd45e98fcbf110cf",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1216,17 +1216,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "d9862b904e492594a61ad6e5022465e80dcc30d6",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 12,
            "deletions": 13,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1359,21 +1359,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-                logger.warning_once(\n-                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-                )\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "01301c5c9bbf3f55219819a6fa850dc174052d35",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -948,17 +948,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "e21d0b656a869ec8f41f3244650c2392df21f70b",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1083,17 +1083,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "0e73af6b15540b2c432743137d4e5a6788f54eb6",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -958,17 +958,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "2efdce39a319457221813ba8f3c74ce5e7cbe601",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1393,25 +1393,23 @@ def forward(\n         else:\n             batch_size, sequence_length = inputs_embeds.shape[:2]\n \n-        assert (\n-            self.config.pad_token_id is not None or batch_size == 1\n-        ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n+        if self.config.pad_token_id is None and batch_size != 1:\n+            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-                logger.warning_once(\n-                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-                )\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "41eb5c19ef1923ee596f7890044b0fbdbe1076fc",
            "filename": "src/transformers/models/gpt2/modeling_tf_gpt2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 18,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1177,39 +1177,33 @@ def call(\n             return_dict=return_dict,\n             training=training,\n         )\n-\n         hidden_states = transformer_outputs[0]\n         logits = self.score(hidden_states)\n         logits_shape = shape_list(logits)\n-        in_logits = None\n+        batch_size = logits_shape[0]\n+\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = tf.fill((batch_size,), value=logits_shape[1] - 1)\n         else:\n             if input_ids is not None:\n-                sequence_lengths = (\n-                    tf.argmax(tf.cast(tf.math.equal(input_ids, self.config.pad_token_id), input_ids.dtype), axis=-1)\n-                    - 1\n-                )\n-                sequence_lengths = tf.where(sequence_lengths >= 0, sequence_lengths, input_ids.shape[-1] - 1)\n-                in_logits = tf.gather(logits, sequence_lengths, batch_dims=1, axis=1)\n+                token_indices = tf.range(shape_list(input_ids)[-1])\n+                non_pad_mask = tf.cast(input_ids != self.config.pad_token_id, token_indices.dtype)\n+                last_non_pad_token = tf.reduce_max(token_indices * non_pad_mask, axis=-1)\n             else:\n-                sequence_lengths = -1\n+                last_non_pad_token = tf.fill((batch_size,), value=logits_shape[1] - 1)\n                 logger.warning_once(\n                     f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n                     \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n                 )\n         loss = None\n \n-        if labels is not None:\n-            assert (\n-                self.config.pad_token_id is not None or logits_shape[0] == 1\n-            ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n+        pooled_logits = tf.gather(logits, last_non_pad_token, batch_dims=1, axis=1)\n \n-            if not tf.is_tensor(sequence_lengths):\n-                in_logits = logits[0 : logits_shape[0], sequence_lengths]\n+        if labels is not None:\n+            if self.config.pad_token_id is None and logits_shape[0] != 1:\n+                raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n \n-            loss = self.hf_compute_loss(tf.reshape(labels, [-1]), tf.reshape(in_logits, [-1, self.num_labels]))\n-        pooled_logits = in_logits if in_logits is not None else logits\n+            loss = self.hf_compute_loss(tf.reshape(labels, [-1]), tf.reshape(pooled_logits, [-1, self.num_labels]))\n \n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]"
        },
        {
            "sha": "c267c0ffed4743d68ce396989f01297ed0e98920",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1280,25 +1280,23 @@ def forward(\n         else:\n             batch_size, sequence_length = inputs_embeds.shape[:2]\n \n-        assert (\n-            self.config.pad_token_id is not None or batch_size == 1\n-        ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n+        if self.config.pad_token_id is None and batch_size != 1:\n+            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-                logger.warning_once(\n-                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-                )\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "fff90034c1a2db24bc0f2cdd769b0c7d0398ee53",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 12,
            "deletions": 13,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1101,21 +1101,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-                logger.warning_once(\n-                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-                )\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "1684dd6dc04a800206568762d9b8ffdb065f4f2e",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 12,
            "deletions": 13,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -935,21 +935,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-                logger.warning_once(\n-                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-                )\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "a918839f7eff41e05d34c6a5c977b2de5bc19c51",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 12,
            "deletions": 13,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1243,21 +1243,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-                logger.warning_once(\n-                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-                )\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "3684768c7d0093ad5d6646214dfba68b5810615c",
            "filename": "src/transformers/models/gptj/modeling_tf_gptj.py",
            "status": "modified",
            "additions": 12,
            "deletions": 17,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -941,35 +941,30 @@ def call(\n         hidden_states = transformer_outputs[0]\n         logits = self.score(hidden_states)\n         logits_shape = shape_list(logits)\n-        in_logits = None\n+        batch_size = logits_shape[0]\n+\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = tf.fill((batch_size,), value=logits_shape[1] - 1)\n         else:\n             if input_ids is not None:\n-                sequence_lengths = (\n-                    tf.argmax(tf.cast(tf.math.equal(input_ids, self.config.pad_token_id), input_ids.dtype), axis=-1)\n-                    - 1\n-                )\n-                sequence_lengths = tf.where(\n-                    sequence_lengths >= 0,\n-                    sequence_lengths,\n-                    tf.cast(shape_list(input_ids[-1]), sequence_lengths.dtype) - 1,\n-                )\n-                in_logits = tf.gather(logits, sequence_lengths, batch_dims=1, axis=1)\n+                token_indices = tf.range(shape_list(input_ids)[-1])\n+                non_pad_mask = tf.cast(input_ids != self.config.pad_token_id, token_indices.dtype)\n+                last_non_pad_token = tf.reduce_max(token_indices * non_pad_mask, axis=-1)\n             else:\n-                sequence_lengths = -1\n+                last_non_pad_token = tf.fill((batch_size,), value=logits_shape[1] - 1)\n                 logger.warning_once(\n                     f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n                     \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n                 )\n         loss = None\n \n+        pooled_logits = tf.gather(logits, last_non_pad_token, batch_dims=1, axis=1)\n+\n         if labels is not None:\n-            if not tf.is_tensor(sequence_lengths):\n-                in_logits = logits[0 : logits_shape[0], sequence_lengths]\n+            if self.config.pad_token_id is None and logits_shape[0] != 1:\n+                raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n \n-            loss = self.hf_compute_loss(tf.reshape(labels, [-1]), tf.reshape(in_logits, [-1, self.num_labels]))\n-        pooled_logits = in_logits if in_logits is not None else logits\n+            loss = self.hf_compute_loss(tf.reshape(labels, [-1]), tf.reshape(pooled_logits, [-1, self.num_labels]))\n \n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]"
        },
        {
            "sha": "3c17e18e4c114dfef6e22eb19186d015168db257",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -945,17 +945,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "fa95b126883a92323e75036f096bd69b01668e06",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1685,17 +1685,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "366538299570a15017c360443278e89dc4cd58e8",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1457,17 +1457,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "566fa57413f9503b7e842e03443efd392895f55f",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -947,17 +947,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "a6d9a54efc09b598b0b699e95e4843d10fc6d7ae",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1036,17 +1036,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "35e42fc0b2489c4766c637708eee35cf297516a8",
            "filename": "src/transformers/models/mistral/modeling_tf_mistral.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -996,39 +996,30 @@ def call(\n         hidden_states = transformer_outputs[0]\n         logits = self.score(hidden_states)\n         logits_shape = shape_list(logits)\n-        in_logits = None\n+        batch_size = logits_shape[0]\n \n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = tf.fill((batch_size,), value=logits_shape[1] - 1)\n         else:\n             if input_ids is not None:\n-                sequence_lengths = (\n-                    tf.argmax(tf.cast(tf.math.equal(input_ids, self.config.pad_token_id), input_ids.dtype), axis=-1)\n-                    - 1\n-                )\n-                sequence_lengths = tf.where(\n-                    sequence_lengths >= 0,\n-                    sequence_lengths,\n-                    tf.cast(shape_list(input_ids[-1]), sequence_lengths.dtype) - 1,\n-                )\n-                in_logits = tf.gather(logits, sequence_lengths, batch_dims=1, axis=1)\n+                token_indices = tf.range(shape_list(input_ids)[-1])\n+                non_pad_mask = tf.cast(input_ids != self.config.pad_token_id, token_indices.dtype)\n+                last_non_pad_token = tf.reduce_max(token_indices * non_pad_mask, axis=-1)\n             else:\n-                sequence_lengths = -1\n+                last_non_pad_token = tf.fill((batch_size,), value=logits_shape[1] - 1)\n                 logger.warning_once(\n                     f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n                     \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n                 )\n         loss = None\n \n+        pooled_logits = tf.gather(logits, last_non_pad_token, batch_dims=1, axis=1)\n+\n         if labels is not None:\n             if self.config.pad_token_id is None and logits_shape[0] != 1:\n                 raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n \n-            if not tf.is_tensor(sequence_lengths):\n-                in_logits = logits[0 : logits_shape[0], sequence_lengths]\n-\n-            loss = self.hf_compute_loss(tf.reshape(labels, [-1]), tf.reshape(in_logits, [-1, self.num_labels]))\n-        pooled_logits = in_logits if in_logits is not None else logits\n+            loss = self.hf_compute_loss(tf.reshape(labels, [-1]), tf.reshape(pooled_logits, [-1, self.num_labels]))\n \n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]"
        },
        {
            "sha": "251187677fd796aab13d33d77caae6325df22f55",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1189,17 +1189,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "37dcc9a70e3dfd45522e6b0fde08978891b17e7f",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 12,
            "deletions": 13,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -681,21 +681,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-                logger.warning_once(\n-                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-                )\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "c773b33a3517f07385f75ab98ddeffb024adfe6b",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1194,17 +1194,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "63c4dec2a0ab708a689f4c381c8b3790cf941aae",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -805,23 +805,21 @@ def forward(\n         # Ensure the batch size is > 1 if there is no padding.\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-                logger.warning_once(\n-                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-                )\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[range(batch_size), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "a30ce86ee7a18df2553aecdc9dbc578749aaa1b8",
            "filename": "src/transformers/models/openai/modeling_tf_openai.py",
            "status": "modified",
            "additions": 13,
            "deletions": 23,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_tf_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_tf_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_tf_openai.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -876,43 +876,33 @@ def call(\n             return_dict=return_dict,\n             training=training,\n         )\n-\n         hidden_states = transformer_outputs[0]\n         logits = self.score(hidden_states)\n-        in_logits = None\n+        logits_shape = shape_list(logits)\n+        batch_size = logits_shape[0]\n+\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = tf.fill((batch_size,), value=logits_shape[1] - 1)\n         else:\n             if input_ids is not None:\n-                sequence_lengths = (\n-                    tf.argmax(tf.cast(tf.math.equal(input_ids, self.config.pad_token_id), input_ids.dtype), axis=-1)\n-                    - 1\n-                )\n-                sequence_lengths = tf.where(sequence_lengths >= 0, sequence_lengths, input_ids.shape[-1] - 1)\n-                in_logits = tf.gather(logits, sequence_lengths, batch_dims=1, axis=1)\n+                token_indices = tf.range(shape_list(input_ids)[-1])\n+                non_pad_mask = tf.cast(input_ids != self.config.pad_token_id, token_indices.dtype)\n+                last_non_pad_token = tf.reduce_max(token_indices * non_pad_mask, axis=-1)\n             else:\n-                sequence_lengths = -1\n+                last_non_pad_token = tf.fill((batch_size,), value=logits_shape[1] - 1)\n                 logger.warning_once(\n                     f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n                     \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n                 )\n         loss = None\n \n-        if labels is not None:\n-            if input_ids is not None:\n-                batch_size, sequence_length = shape_list(input_ids)[:2]\n-            else:\n-                batch_size, sequence_length = shape_list(inputs_embeds)[:2]\n-            assert (\n-                self.config.pad_token_id is not None or batch_size == 1\n-            ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n-\n-            if not tf.is_tensor(sequence_lengths):\n-                in_logits = logits[0:batch_size, sequence_lengths]\n+        pooled_logits = tf.gather(logits, last_non_pad_token, batch_dims=1, axis=1)\n \n-            loss = self.hf_compute_loss(tf.reshape(labels, [-1, 1]), tf.reshape(in_logits, [-1, self.num_labels]))\n+        if labels is not None:\n+            if self.config.pad_token_id is None and logits_shape[0] != 1:\n+                raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n \n-        pooled_logits = in_logits if in_logits is not None else logits\n+            loss = self.hf_compute_loss(tf.reshape(labels, [-1]), tf.reshape(pooled_logits, [-1, self.num_labels]))\n \n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]"
        },
        {
            "sha": "c9385a201a956080b1e598bf48f244f0f67453aa",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1295,22 +1295,23 @@ def forward(\n         else:\n             batch_size, sequence_length = inputs_embeds.shape[:2]\n \n+        if self.config.pad_token_id is None and batch_size != 1:\n+            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-                logger.warning_once(\n-                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-                )\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "70f99b2a1011d6f2049459690a67198a026d7321",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1009,17 +1009,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "a62249460eefd4f2dc498582ca1853e0d2ca565d",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -921,17 +921,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "ca6992d377bb227696045cd6f6b648ec707b4703",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1057,17 +1057,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "f7d2fa2f485a01d41f2169040b8834f5ba6f32e5",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1597,17 +1597,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "8310379d83da3d2ddbe488dcd5862af3e0d3149d",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -932,17 +932,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "bfb4e81d3ec3883cdcd924c05695007fb4433e8b",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1438,17 +1438,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "706cd2b6a84827249e70d96a6502a0cf21b1a1fc",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1265,17 +1265,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "9314f05b496489d35ddd04e1bdcf9a08e2c4ae10",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -944,17 +944,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "54f57971a82e5ed00ddcc620678574fb2d8e1268",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1438,17 +1438,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "da4c8a4bb35280b43c3ba28636fcf8683590c562",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/694aaa7fbccb801de013a99cb558f5a20dfdc02a/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=694aaa7fbccb801de013a99cb558f5a20dfdc02a",
            "patch": "@@ -1875,17 +1875,20 @@ def forward(\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        }
    ],
    "stats": {
        "total": 885,
        "additions": 448,
        "deletions": 437
    }
}