{
    "author": "ArthurZucker",
    "message": "[`MllamaProcessor`] Update errors and API with multiple image (#33715)\n\n* update error\r\n\r\n* update and add a test\r\n\r\n* update\r\n\r\n* update",
    "sha": "46841d3eb24f444fc06b7402c273cb51a097c383",
    "files": [
        {
            "sha": "84c8eea466e75b0914fae7969a7219703e7a68f1",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/46841d3eb24f444fc06b7402c273cb51a097c383/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46841d3eb24f444fc06b7402c273cb51a097c383/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=46841d3eb24f444fc06b7402c273cb51a097c383",
            "patch": "@@ -12,11 +12,9 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"\n-Processor class for Mllama.\n-\"\"\"\n \n-from statistics import mean\n+\"\"\"Processor class for Mllama.\"\"\"\n+\n from typing import List, Optional, Union\n \n import numpy as np\n@@ -296,25 +294,27 @@ def __call__(\n             encoding = self.tokenizer(text, **text_kwargs)\n             data.update(encoding)\n \n+        n_images_in_images = [0]\n         if images is not None:\n             images = make_list_of_images(images)\n             n_images_in_images = [len(sample) for sample in images]\n \n-            if text is not None:\n-                if (\n-                    not all(batch_img_per_prompt == n_images_in_images for batch_img_per_prompt in n_images_in_text)\n-                    and len(text) > 1\n-                ):\n-                    raise ValueError(\n-                        f\"The number of images in each batch {n_images_in_text} should be the same  {n_images_in_images} should be the same. Yes, the model does not \\\n-                        support having a different number of images per batch.\"\n-                    )\n-                if int(mean(n_images_in_text)) != int(mean(n_images_in_images)):\n+        if text is not None:\n+            if any(batch_img == 0 for batch_img in n_images_in_text) and not all(\n+                batch_img == 0 for batch_img in n_images_in_text\n+            ):\n+                raise ValueError(\n+                    \"If a batch of text is provided, there should be either no images or at least one image per sample\"\n+                )\n+            if sum(n_images_in_images) != sum(n_images_in_text):\n+                if images is None:\n+                    raise ValueError(\"No image were provided, but there are image tokens in the prompt\")\n+                else:\n                     raise ValueError(\n-                        f\"The number of images in the text ({n_images_in_text}) should be the same as in the number of provided images ({n_images_in_images}) \\\n-                        should be the same.\"\n+                        f\"The number of image token ({sum(n_images_in_images)}) should be the same as in the number of provided images ({sum(n_images_in_images)})\"\n                     )\n \n+        if images is not None:\n             image_features = self.image_processor(images, **images_kwargs)\n             num_tiles = image_features.pop(\"num_tiles\")\n             data.update(image_features)"
        },
        {
            "sha": "b6233d9e177cdb7065f79335a07a25eec4228ad4",
            "filename": "tests/models/mllama/test_processor_mllama.py",
            "status": "modified",
            "additions": 118,
            "deletions": 0,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/46841d3eb24f444fc06b7402c273cb51a097c383/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46841d3eb24f444fc06b7402c273cb51a097c383/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py?ref=46841d3eb24f444fc06b7402c273cb51a097c383",
            "patch": "@@ -15,6 +15,8 @@\n \n import unittest\n \n+import numpy as np\n+\n from transformers import MllamaProcessor\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_vision_available\n@@ -177,3 +179,119 @@ def test_apply_chat_template(self):\n         rendered_list = self.processor.apply_chat_template(messages_list, add_generation_prompt=True, tokenize=False)\n         rendered_str = self.processor.apply_chat_template(messages_str, add_generation_prompt=True, tokenize=False)\n         self.assertEqual(rendered_list, rendered_str)\n+\n+    def test_process_interleaved_images_prompts_image_splitting(self):\n+        # Test that a single image is processed correctly\n+        inputs = self.processor(images=self.image2, size={\"width\": 224, \"height\": 224})\n+        self.assertEqual(inputs[\"pixel_values\"].shape, (1, 1, 4, 3, 224, 224))\n+\n+        # Test that text is processed correctly\n+        text = \"<|begin_of_text|>This is a test sentence.<|end_of_text|>\"\n+        inputs = self.processor(text=text)\n+        expected_ids = [128000, 2028, 374, 264, 1296, 11914, 13, 128001]\n+        self.assertEqual(inputs[\"input_ids\"][0], expected_ids)\n+        self.assertEqual(inputs[\"attention_mask\"][0], [1] * len(expected_ids))\n+        self.assertEqual(inputs.get(\"cross_attention_mask\"), None)\n+\n+        # Test a single sample with image and text\n+        image_str = \"<|image|>\"\n+        text_str = \"This is a test sentence.\"\n+        text = image_str + text_str\n+        inputs = self.processor(\n+            text=text,\n+            images=self.image1,\n+            size={\"width\": 128, \"height\": 128},\n+        )\n+        expected_ids = [self.image_token_id, self.bos_token_id] + [2028, 374, 264, 1296, 11914, 13]\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape, (1, 1, 4, 3, 128, 128))\n+        self.assertEqual(inputs[\"input_ids\"][0], expected_ids)\n+        self.assertEqual(inputs[\"attention_mask\"][0], [1] * len(expected_ids))\n+        cross_attention_mask = inputs[\"cross_attention_mask\"]\n+        self.assertEqual(cross_attention_mask.shape, (1, 8, 1, 4))\n+        self.assertTrue(\n+            np.all(cross_attention_mask == 1), f\"Cross attention mask is not all ones: {cross_attention_mask}\"\n+        )\n+\n+        # Test batch\n+        text = [\n+            \"<|image|>This is a test sentence.\",\n+            \"This is a test sentence.<|image|><|image|>This is a test sentence.\",\n+        ]\n+        # fmt: off\n+        expected_ids = [\n+            [self.image_token_id, self.bos_token_id, 2028, 374, 264, 1296, 11914, 13],\n+            [self.bos_token_id, 2028, 374, 264, 1296, 11914, 13, self.image_token_id, self.image_token_id, 2028, 374, 264, 1296, 11914, 13],\n+        ]\n+        # fmt: onn\n+        images = [[self.image1], [self.image1, self.image2]]\n+        inputs = self.processor(text=text, images=images, padding=True, size={\"width\": 256, \"height\": 256})\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape, (2, 2, 4, 3, 256, 256))\n+        for input_ids_i, attention_mask_i, expected_ids_i in zip(inputs[\"input_ids\"], inputs[\"attention_mask\"], expected_ids):\n+            pad_ids = [id for id, m in zip(input_ids_i, attention_mask_i) if m == 0]\n+            input_ids = [id for id, m in zip(input_ids_i, attention_mask_i) if m == 1]\n+            self.assertEqual(input_ids, expected_ids_i)\n+            self.assertEqual(pad_ids, [self.pad_token_id] * len(pad_ids))\n+\n+        cross_attention_mask = inputs[\"cross_attention_mask\"]\n+        self.assertEqual(cross_attention_mask.shape, (2, 15, 2, 4))\n+\n+        # Check that only first tile of first sample is attended to all text tokens\n+        first_sample_mask = cross_attention_mask[0].copy()\n+        first_image_first_tile_attention = first_sample_mask[:, :1, :1]  # text tokens, images, tiles\n+        self.assertTrue(np.all(first_image_first_tile_attention == 1), f\"Cross attention mask is not all ones: {first_image_first_tile_attention}\")\n+\n+        # zero out first tile of first image\n+        first_image_first_tile_attention[:, :1, :1] = 0\n+        self.assertTrue(np.all(first_image_first_tile_attention == 0), f\"Cross attention mask is not all zeros: {first_image_first_tile_attention}\")\n+\n+        # second sample\n+        second_sample_mask = cross_attention_mask[1].copy()\n+        first_image_first_tile_attention = second_sample_mask[7:, :1, :1]  # text tokens, images, tiles\n+        self.assertTrue(np.all(first_image_first_tile_attention == 1), f\"Cross attention mask is not all ones: {first_image_first_tile_attention}\")\n+\n+        second_image_two_tiles_attention = second_sample_mask[8:, 1:2, :2]  # text tokens, images, tiles\n+        self.assertTrue(np.all(second_image_two_tiles_attention == 1), f\"Cross attention mask is not all ones: {second_image_two_tiles_attention}\")\n+\n+        # zero out both images masks\n+        second_sample_mask[7:, :1, :1] = 0\n+        second_sample_mask[8:, 1:2, :2] = 0\n+        self.assertTrue(np.all(second_sample_mask == 0), f\"Cross attention mask is not all zeros: {second_sample_mask}\")\n+\n+    def test_process_interleaved_images_prompts_image_error(self):\n+        text = [\n+            \"This is a test sentence.\",\n+            \"In this other sentence we try some good things\",\n+        ]\n+        inputs = self.processor(text=text, images=None, padding=True)\n+        self.assertIsNotNone(inputs[\"input_ids\"])\n+\n+        text = [\n+            \"This is a test sentence.<|image|>\",\n+            \"In this other sentence we try some good things\",\n+        ]\n+        with self.assertRaises(ValueError):\n+            self.processor(text=text, images=None, padding=True)\n+\n+        images = [[self.image1], []]\n+        with self.assertRaises(ValueError):\n+            self.processor(text=text, images=images, padding=True)\n+\n+        text = [\n+            \"This is a test sentence.<|image|>\",\n+            \"In this other sentence we try some good things<|image|>\",\n+        ]\n+        with self.assertRaises(ValueError):\n+            self.processor(text=text, images=None, padding=True)\n+\n+        text = [\n+            \"This is a test sentence.<|image|>\",\n+            \"In this other sentence we try some good things<|image|>\",\n+        ]\n+        images = [[self.image1], [self.image2]]\n+        inputs = self.processor(text=text, images=images, padding=True)\n+\n+        images = [[self.image1, self.image2], []]\n+        with self.assertRaises(ValueError):\n+            self.processor(text=text, images=None, padding=True)"
        }
    ],
    "stats": {
        "total": 150,
        "additions": 134,
        "deletions": 16
    }
}