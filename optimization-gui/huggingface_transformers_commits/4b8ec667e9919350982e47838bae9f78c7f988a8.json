{
    "author": "Cyrilvallez",
    "message": "Remove all traces of `low_cpu_mem_usage` (#38792)\n\n* remove it from all py files\n\n* remove it from the doc\n\n* remove it from examples\n\n* style\n\n* remove traces of _fast_init\n\n* Update test_peft_integration.py\n\n* CIs",
    "sha": "4b8ec667e9919350982e47838bae9f78c7f988a8",
    "files": [
        {
            "sha": "887f718241f48367f742fbac7c8b37bed16b3a42",
            "filename": "docs/source/ar/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -231,7 +231,7 @@ flush()\n دعنا نرى ما هو استهلاك ذاكرة GPU الذروة الذي يوفره تكميم 4 بت. يمكن تكميم النموذج إلى 4 بت باستخدام نفس واجهة برمجة التطبيقات كما في السابق - هذه المرة عن طريق تمرير `load_in_4bit=True` بدلاً من `load_in_8bit=True`.\n \n ```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_4bit=True, low_cpu_mem_usage=True, pad_token_id=0)\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_4bit=True, pad_token_id=0)\n \n pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n "
        },
        {
            "sha": "8b0d05c783d0eca58a8b61ee398a485a7d68181d",
            "filename": "docs/source/ar/trainer.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Far%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Far%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftrainer.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -459,7 +459,7 @@ args = TrainingArguments(\n model_id = \"google/gemma-2b\"\n \n tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id، low_cpu_mem_usage=True).to(0)\n+model = AutoModelForCausalLM.from_pretrained(model_id).to(0)\n \n trainer = trl.SFTTrainer(\n     model=model،\n@@ -503,7 +503,7 @@ args = TrainingArguments(\n # تحميل النموذج والمجزىء اللغوي\n model_id = \"google/gemma-2b\"\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True).to(0)\n+model = AutoModelForCausalLM.from_pretrained(model_id).to(0)\n \n # تهيئة المدرب\n trainer = Trainer(\n@@ -547,7 +547,7 @@ args = TrainingArguments(\n model_id = \"google/gemma-2b\"\n \n tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True).to(0)\n+model = AutoModelForCausalLM.from_pretrained(model_id).to(0)\n \n trainer = trl.SFTTrainer(\n     model=model, "
        },
        {
            "sha": "f43b70ea9fb14d6c20b5ff3996b93fc863d6c14c",
            "filename": "docs/source/en/internal/model_debugging_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -51,7 +51,7 @@ torch.random.manual_seed(673)\n # load pretrained model and processor\n model_id = \"llava-hf/llava-1.5-7b-hf\"\n processor = LlavaProcessor.from_pretrained(model_id)\n-model = LlavaForConditionalGeneration.from_pretrained(model_id, low_cpu_mem_usage=True)\n+model = LlavaForConditionalGeneration.from_pretrained(model_id)\n \n # create random image input\n random_image = Image.fromarray(torch.randint(0, 256, (224, 224, 3), dtype=torch.uint8).numpy())"
        },
        {
            "sha": "038aa76689f4d5c0050c13a3d72467e0f2d24ca7",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -236,7 +236,7 @@ flush()\n Let's see what peak GPU memory consumption 4-bit quantization gives. Quantizing the model to 4-bit can be done with the same API as before - this time by passing `load_in_4bit=True` instead of `load_in_8bit=True`.\n \n ```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_4bit=True, low_cpu_mem_usage=True, pad_token_id=0)\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_4bit=True, pad_token_id=0)\n \n pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n "
        },
        {
            "sha": "e7edca9fd3a37bbb0be1e6519ef5ce7a42c5689a",
            "filename": "docs/source/en/model_doc/chameleon.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -170,7 +170,6 @@ model_id = \"facebook/chameleon-7b\"\n model = ChameleonForConditionalGeneration.from_pretrained(\n     model_id,\n     torch_dtype=torch.bfloat16,\n-    low_cpu_mem_usage=True,\n     attn_implementation=\"flash_attention_2\"\n ).to(0)\n ```"
        },
        {
            "sha": "cfc60d074c7ed5d8398b7fd260125735ef438a01",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -157,7 +157,7 @@ import requests\n \n processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n \n-model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n+model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16)\n model.to(\"cuda:0\")\n \n # prepare image and text prompt, using the appropriate prompt template\n@@ -292,7 +292,6 @@ from transformers import AutoModelForImageTextToText\n model = AutoModelForImageTextToText.from_pretrained(\n     model_id,\n     torch_dtype=torch.float16,\n-    low_cpu_mem_usage=True,\n     use_flash_attention_2=True\n ).to(0)\n ```"
        },
        {
            "sha": "da3359f7e3208876c4ad1e2ccbca4da751bb1983",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -121,7 +121,6 @@ processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-\n model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n     \"llava-hf/llava-onevision-qwen2-7b-ov-hf\",\n     torch_dtype=torch.float16,\n-    low_cpu_mem_usage=True,\n     device_map=\"cuda:0\"\n )\n \n@@ -286,7 +285,6 @@ from transformers import LlavaOnevisionForConditionalGeneration\n model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n     model_id,\n     torch_dtype=torch.float16,\n-    low_cpu_mem_usage=True,\n     use_flash_attention_2=True\n ).to(0)\n ```"
        },
        {
            "sha": "fb76f0264bec4854474fbfd1403be9b1673f85c2",
            "filename": "docs/source/en/models.md",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fen%2Fmodels.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fen%2Fmodels.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodels.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -148,11 +148,6 @@ You need enough memory to hold two copies of the model weights (random and pretr\n \n Transformers reduces some of these memory-related challenges with fast initialization, sharded checkpoints, Accelerate's [Big Model Inference](https://hf.co/docs/accelerate/usage_guides/big_modeling) feature, and supporting lower bit data types.\n \n-### Fast initialization\n-\n-A PyTorch model is instantiated with random weights, or \"empty\" tensors, that take up space in memory without filling it.\n-\n-Transformers boosts loading speed by skipping random weight initialization with the [_fast_init](https://github.com/huggingface/transformers/blob/c9f6e5e35156e068b227dd9b15521767f6afd4d2/src/transformers/modeling_utils.py#L2710) parameter if the pretrained weights are correctly initialized. This parameter is set to `True` by default.\n \n ### Sharded checkpoints\n \n@@ -245,7 +240,7 @@ Big Model Inference's second feature relates to how weights are loaded and dispa\n \n Both features combined reduces memory usage and loading times for big pretrained models.\n \n-Set [device_map](https://github.com/huggingface/transformers/blob/026a173a64372e9602a16523b8fae9de4b0ff428/src/transformers/modeling_utils.py#L3061) to `\"auto\"` to enable Big Model Inference. This also sets the [low_cpu_mem_usage](https://github.com/huggingface/transformers/blob/026a173a64372e9602a16523b8fae9de4b0ff428/src/transformers/modeling_utils.py#L3028) parameter to `True`, such that not more than 1x the model size is used in CPU memory.\n+Set [device_map](https://github.com/huggingface/transformers/blob/026a173a64372e9602a16523b8fae9de4b0ff428/src/transformers/modeling_utils.py#L3061) to `\"auto\"` to enable Big Model Inference.\n \n ```py\n from transformers import AutoModelForCausalLM"
        },
        {
            "sha": "0923fea1cc059594275c56fc9c6d2f3d1f5533e7",
            "filename": "docs/source/ja/main_classes/model.md",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fja%2Fmain_classes%2Fmodel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fja%2Fmain_classes%2Fmodel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fmodel.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -39,19 +39,8 @@ rendered properly in your Markdown viewer.\n \n Transformers 4.20.0では、[`~PreTrainedModel.from_pretrained`] メソッドが再設計され、[Accelerate](https://huggingface.co/docs/accelerate/big_modeling) を使用して大規模モデルを扱うことが可能になりました。これには Accelerate >= 0.9.0 と PyTorch >= 1.9.0 が必要です。以前の方法でフルモデルを作成し、その後事前学習の重みを読み込む代わりに（これにはメモリ内のモデルサイズが2倍必要で、ランダムに初期化されたモデル用と重み用の2つが必要でした）、モデルを空の外殻として作成し、事前学習の重みが読み込まれるときにパラメーターを実体化するオプションが追加されました。\n \n-このオプションは `low_cpu_mem_usage=True` で有効にできます。モデルはまず空の重みを持つメタデバイス上に作成され、その後状態辞書が内部に読み込まれます（シャードされたチェックポイントの場合、シャードごとに読み込まれます）。この方法で使用される最大RAMは、モデルの完全なサイズだけです。\n-\n-\n-```py\n-from transformers import AutoModelForSeq2SeqLM\n-\n-t0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", low_cpu_mem_usage=True)\n-```\n-\n さらに、モデルが完全にRAMに収まらない場合（現時点では推論のみ有効）、異なるデバイスにモデルを直接配置できます。`device_map=\"auto\"` を使用すると、Accelerateは各レイヤーをどのデバイスに配置するかを決定し、最速のデバイス（GPU）を最大限に活用し、残りの部分をCPU、あるいはGPU RAMが不足している場合はハードドライブにオフロードします。モデルが複数のデバイスに分割されていても、通常どおり実行されます。\n \n-`device_map` を渡す際、`low_cpu_mem_usage` は自動的に `True` に設定されるため、それを指定する必要はありません。\n-\n \n ```py\n from transformers import AutoModelForSeq2SeqLM"
        },
        {
            "sha": "5a95e2d9b5965adf60718eb560d93cd07a769c22",
            "filename": "docs/source/ko/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -227,7 +227,7 @@ flush()\n 이제 4비트 양자화가 제공하는 최대 GPU 메모리 사용량을 확인해 봅시다. 4비트로 모델을 양자화하려면 이전과 동일한 API를 사용하되 이번에는 `load_in_8bit=True` 대신 `load_in_4bit=True`를 전달하면 됩니다.\n \n ```python\n-model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_4bit=True, low_cpu_mem_usage=True, pad_token_id=0)\n+model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_4bit=True, pad_token_id=0)\n \n pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n "
        },
        {
            "sha": "ac2fa16b7703160cb3eb3419469234a5d1adf3e4",
            "filename": "docs/source/ko/model_doc/chameleon.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fko%2Fmodel_doc%2Fchameleon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fko%2Fmodel_doc%2Fchameleon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fchameleon.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -148,7 +148,6 @@ model_id = \"facebook/chameleon-7b\"\n model = ChameleonForConditionalGeneration.from_pretrained(\n     model_id,\n     torch_dtype=torch.bfloat16,\n-    low_cpu_mem_usage=True,\n     attn_implementation=\"flash_attention_2\"\n ).to(0)\n ```"
        },
        {
            "sha": "7def9cccd8945182983f26e9f9ed39ddd10fca6d",
            "filename": "docs/source/ko/trainer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fko%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fko%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftrainer.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -421,7 +421,7 @@ args = TrainingArguments(\n model_id = \"google/gemma-2b\"\n \n tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True).to(0)\n+model = AutoModelForCausalLM.from_pretrained(model_id).to(0)\n \n trainer = trl.SFTTrainer(\n     model=model, "
        },
        {
            "sha": "57c1b374ed1d3ff456c185aa586bf6a510860010",
            "filename": "docs/source/zh/main_classes/model.md",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fzh%2Fmain_classes%2Fmodel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/docs%2Fsource%2Fzh%2Fmain_classes%2Fmodel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fmodel.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -29,18 +29,8 @@ http://www.apache.org/licenses/LICENSE-2.0\n \n 在 Transformers 4.20.0 中，[`~PreTrainedModel.from_pretrained`] 方法已重新设计，以适应使用 [Accelerate](https://huggingface.co/docs/accelerate/big_modeling) 加载大型模型的场景。这需要您使用的 Accelerate 和 PyTorch 版本满足： Accelerate >= 0.9.0， PyTorch >= 1.9.0。除了创建完整模型，然后在其中加载预训练权重（这会占用两倍于模型大小的内存空间，一个用于随机初始化模型，一个用于预训练权重），我们提供了一种选项，将模型创建为空壳，然后只有在加载预训练权重时才实例化其参数。\n \n-您可以使用 `low_cpu_mem_usage=True` 激活此选项。首先，在 Meta 设备上创建模型（带有空权重），然后将状态字典加载到其中（在分片检查点的情况下逐片加载）。这样，最大使用的内存占用仅为模型的完整大小。\n-\n-```python\n-from transformers import AutoModelForSeq2SeqLM\n-\n-t0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", low_cpu_mem_usage=True)\n-```\n-\n 此外，如果内存不足以放下加载整个模型（目前仅适用于推理），您可以直接将模型放置在不同的设备上。使用 `device_map=\"auto\"`，Accelerate 将确定将每一层放置在哪个设备上，以最大化使用最快的设备（GPU），并将其余部分卸载到 CPU，甚至硬盘上（如果您没有足够的 GPU 内存 或 CPU 内存）。即使模型分布在几个设备上，它也将像您通常期望的那样运行。\n \n-在传递 `device_map` 时，`low_cpu_mem_usage` 会自动设置为 `True`，因此您不需要指定它：\n-\n ```python\n from transformers import AutoModelForSeq2SeqLM\n "
        },
        {
            "sha": "e1b2beddf4e9b339dbb445bb111249b7ddfb41df",
            "filename": "examples/pytorch/language-modeling/README.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2FREADME.md?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -229,10 +229,6 @@ sure all your batches have the same length.\n \n To use the streaming dataset mode which can be very useful for large datasets, add `--streaming` to the command line. This is supported by `run_mlm.py`, `run_clm.py` and `run_fim.py`. Make sure to adapt the other scripts to your use case by taking inspiration from them.\n \n-## Low Cpu Memory Usage\n-\n-To use low cpu memory mode which can be very useful for LLM, add `--low_cpu_mem_usage` to the command line. This is currently supported by `run_clm.py`,`run_mlm.py`, `run_plm.py`, `run_fim.py`, `run_mlm_no_trainer.py`, `run_clm_no_trainer.py` and `run_fim_no_trainer.py`.\n-\n ## Creating a model on the fly\n \n When training a model from scratch, configuration values may be overridden with the help of `--config_overrides`:"
        },
        {
            "sha": "8082df71e1ab6966352ed6d8ed2f2e2e42c36d8e",
            "filename": "examples/pytorch/language-modeling/run_clm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -139,15 +139,6 @@ class ModelArguments:\n             \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n         },\n     )\n-    low_cpu_mem_usage: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. \"\n-                \"set True will benefit LLM loading time and RAM consumption.\"\n-            )\n-        },\n-    )\n \n     def __post_init__(self):\n         if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):\n@@ -432,7 +423,6 @@ def main():\n             token=model_args.token,\n             trust_remote_code=model_args.trust_remote_code,\n             torch_dtype=torch_dtype,\n-            low_cpu_mem_usage=model_args.low_cpu_mem_usage,\n         )\n     else:\n         model = AutoModelForCausalLM.from_config(config, trust_remote_code=model_args.trust_remote_code)"
        },
        {
            "sha": "d11798e034a84b9df59734ccbdaec54d9c506321",
            "filename": "examples/pytorch/language-modeling/run_clm_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm_no_trainer.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -228,14 +228,6 @@ def parse_args():\n             \"Only applicable when `--with_tracking` is passed.\"\n         ),\n     )\n-    parser.add_argument(\n-        \"--low_cpu_mem_usage\",\n-        action=\"store_true\",\n-        help=(\n-            \"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. \"\n-            \"If passed, LLM loading time and RAM consumption will be benefited.\"\n-        ),\n-    )\n     args = parser.parse_args()\n \n     # Sanity checks\n@@ -409,7 +401,6 @@ def main():\n             args.model_name_or_path,\n             from_tf=bool(\".ckpt\" in args.model_name_or_path),\n             config=config,\n-            low_cpu_mem_usage=args.low_cpu_mem_usage,\n             trust_remote_code=args.trust_remote_code,\n         )\n     else:"
        },
        {
            "sha": "d1698a949ff3376e3a6e767a78c636278ef5a4b3",
            "filename": "examples/pytorch/language-modeling/run_fim.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -142,15 +142,6 @@ class ModelArguments:\n             \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n         },\n     )\n-    low_cpu_mem_usage: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. \"\n-                \"set True will benefit LLM loading time and RAM consumption.\"\n-            )\n-        },\n-    )\n     pad_to_multiple_of: bool = field(\n         default=False,\n         metadata={\n@@ -501,7 +492,6 @@ def main():\n             token=model_args.token,\n             trust_remote_code=model_args.trust_remote_code,\n             torch_dtype=torch_dtype,\n-            low_cpu_mem_usage=model_args.low_cpu_mem_usage,\n             attn_implementation=model_args.attn_implementation,\n         )\n "
        },
        {
            "sha": "8c601e4083062230b2727ba8a9338ab836f90ff8",
            "filename": "examples/pytorch/language-modeling/run_fim_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -288,14 +288,6 @@ def parse_args():\n             \"Only applicable when `--with_tracking` is passed.\"\n         ),\n     )\n-    parser.add_argument(\n-        \"--low_cpu_mem_usage\",\n-        action=\"store_true\",\n-        help=(\n-            \"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. \"\n-            \"If passed, LLM loading time and RAM consumption will be benefited.\"\n-        ),\n-    )\n     args = parser.parse_args()\n \n     # Sanity checks\n@@ -474,7 +466,6 @@ def main():\n             args.model_name_or_path,\n             from_tf=bool(\".ckpt\" in args.model_name_or_path),\n             config=config,\n-            low_cpu_mem_usage=args.low_cpu_mem_usage,\n             trust_remote_code=args.trust_remote_code,\n         )\n     else:"
        },
        {
            "sha": "79e7a585bd0663d89bd51b73fed567a92dbe404a",
            "filename": "examples/pytorch/language-modeling/run_mlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -136,15 +136,6 @@ class ModelArguments:\n             \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n         },\n     )\n-    low_cpu_mem_usage: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. \"\n-                \"set True will benefit LLM loading time and RAM consumption.\"\n-            )\n-        },\n-    )\n \n     def __post_init__(self):\n         if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):\n@@ -436,7 +427,6 @@ def main():\n             token=model_args.token,\n             trust_remote_code=model_args.trust_remote_code,\n             torch_dtype=torch_dtype,\n-            low_cpu_mem_usage=model_args.low_cpu_mem_usage,\n         )\n     else:\n         logger.info(\"Training new model from scratch\")"
        },
        {
            "sha": "134d23478299966ec7548e7e936b2edf54e987aa",
            "filename": "examples/pytorch/language-modeling/run_mlm_no_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm_no_trainer.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -235,14 +235,6 @@ def parse_args():\n             \"Only applicable when `--with_tracking` is passed.\"\n         ),\n     )\n-    parser.add_argument(\n-        \"--low_cpu_mem_usage\",\n-        action=\"store_true\",\n-        help=(\n-            \"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. \"\n-            \"If passed, LLM loading time and RAM consumption will be benefited.\"\n-        ),\n-    )\n     args = parser.parse_args()\n \n     # Sanity checks\n@@ -406,7 +398,6 @@ def main():\n             args.model_name_or_path,\n             from_tf=bool(\".ckpt\" in args.model_name_or_path),\n             config=config,\n-            low_cpu_mem_usage=args.low_cpu_mem_usage,\n             trust_remote_code=args.trust_remote_code,\n         )\n     else:"
        },
        {
            "sha": "b12d3526c2739ac7a3d94b9a2cd57f7a89bc7e1b",
            "filename": "examples/pytorch/language-modeling/run_plm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -103,15 +103,6 @@ class ModelArguments:\n             )\n         },\n     )\n-    low_cpu_mem_usage: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. \"\n-                \"set True will benefit LLM loading time and RAM consumption.\"\n-            )\n-        },\n-    )\n \n     def __post_init__(self):\n         if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):\n@@ -397,7 +388,6 @@ def main():\n             cache_dir=model_args.cache_dir,\n             revision=model_args.model_revision,\n             token=model_args.token,\n-            low_cpu_mem_usage=model_args.low_cpu_mem_usage,\n         )\n     else:\n         logger.info(\"Training new model from scratch\")"
        },
        {
            "sha": "2df9b2ac65f8a79605dd580b5ef00363b3a02090",
            "filename": "src/transformers/model_debugging_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodel_debugging_utils.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -429,7 +429,7 @@ def model_addition_debugger_context(\n     # load pretrained model and processor\n     model_id = \"llava-hf/llava-1.5-7b-hf\"\n     processor = LlavaProcessor.from_pretrained(model_id)\n-    model = LlavaForConditionalGeneration.from_pretrained(model_id, low_cpu_mem_usage=True)\n+    model = LlavaForConditionalGeneration.from_pretrained(model_id)\n \n     # create random image input\n     random_image = Image.fromarray(torch.randint(0, 256, (224, 224, 3), dtype=torch.uint8).numpy())"
        },
        {
            "sha": "e55c3475e5e199173424348414446ba2c8e16aad",
            "filename": "src/transformers/models/aria/convert_aria_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -37,7 +37,7 @@\n \n     # load model\n     kwargs = {\"device_map\": \"auto\", \"torch_dtype\": torch.float16}\n-    model = AriaTextForCausalLM.from_pretrained(\"rhymes-ai/Aria\", low_cpu_mem_usage=True, **kwargs)\n+    model = AriaTextForCausalLM.from_pretrained(\"rhymes-ai/Aria\", **kwargs)\n \n     # load vision tower\n     model.get_vision_tower().load_model()"
        },
        {
            "sha": "9c7363041d3368063ec173997bfa84eea8fed7a7",
            "filename": "src/transformers/models/falcon_h1/convert_mamba_ssm_checkpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconvert_mamba_ssm_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconvert_mamba_ssm_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconvert_mamba_ssm_checkpoint.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -41,9 +41,7 @@\n def convert_falcon_h1_to_hf(input_model_path, output_path):\n     tokenizer = AutoTokenizer.from_pretrained(input_model_path)\n \n-    model = AutoModelForCausalLM.from_pretrained(\n-        input_model_path, torch_dtype=torch.bfloat16, trust_remote_code=True, low_cpu_mem_usage=True\n-    )\n+    model = AutoModelForCausalLM.from_pretrained(input_model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)\n \n     intermediate_size = int(model.config.expansion_factor * model.config.hidden_size)\n "
        },
        {
            "sha": "a390166a042bf6acc1683fb3143905eb8d649b35",
            "filename": "src/transformers/models/internvl/convert_internvl_weights_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -187,7 +187,6 @@ def load_original_state_dict(input_base_path):\n     model = AutoModel.from_pretrained(\n         input_base_path,\n         torch_dtype=torch.bfloat16,\n-        low_cpu_mem_usage=True,\n         use_flash_attn=False,\n         trust_remote_code=True,\n     ).eval()"
        },
        {
            "sha": "5ba1418a113f822b532f3afc836512e8773dec78",
            "filename": "src/transformers/models/llama/convert_llama_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -419,7 +419,7 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n         gc.collect()\n \n         print(\"Loading the checkpoint in a Llama model.\")\n-        model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n+        model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16)\n \n         # Avoid saving this as part of the config.\n         del model.config._name_or_path"
        },
        {
            "sha": "3631de33af64e8ff91c742ad842001121d2a38e8",
            "filename": "src/transformers/models/llava/convert_llava_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -40,7 +40,7 @@\n \n     # load model\n     kwargs = {\"device_map\": \"auto\", \"torch_dtype\": torch.float16}\n-    model = LlavaLlamaForCausalLM.from_pretrained(\"liuhaotian/llava-v1.5-7b\", low_cpu_mem_usage=True, **kwargs)\n+    model = LlavaLlamaForCausalLM.from_pretrained(\"liuhaotian/llava-v1.5-7b\", **kwargs)\n \n     # load vision tower\n     model.get_vision_tower().load_model()"
        },
        {
            "sha": "c0b590a03058f2540a83afb540ad2e59fe789ef3",
            "filename": "src/transformers/models/olmo/convert_olmo_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Folmo%2Fconvert_olmo_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Folmo%2Fconvert_olmo_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fconvert_olmo_weights_to_hf.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -175,7 +175,7 @@ def write_model(model_path, input_base_path, tokenizer_path=None, safe_serializa\n         _write_tokenizer(model_path, config, tokenizer_path, fix_eos_token_id)\n \n     print(\"Loading the checkpoint in a OLMo model.\")\n-    model = OlmoForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.float32, low_cpu_mem_usage=True)\n+    model = OlmoForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.float32)\n     # Avoid saving this as part of the config.\n     del model.config._name_or_path\n     print(\"Saving in the Transformers format.\")"
        },
        {
            "sha": "86d403916a358293f0fa870b1ced66b8aff06ec5",
            "filename": "src/transformers/models/olmo2/convert_olmo2_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Folmo2%2Fconvert_olmo2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Folmo2%2Fconvert_olmo2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconvert_olmo2_weights_to_hf.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -205,7 +205,7 @@ def write_model(\n         _write_tokenizer(model_path, config, input_base_path, tokenizer_path)\n \n     print(\"Loading the checkpoint in a OLMo2 model.\")\n-    model = Olmo2ForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.float32, low_cpu_mem_usage=True)\n+    model = Olmo2ForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.float32)\n     # Avoid saving this as part of the config.\n     del model.config._name_or_path\n     print(\"Saving in the Transformers format.\")"
        },
        {
            "sha": "ecb5cfa4e12dab074fcd20576c0ff299672d8ba2",
            "filename": "src/transformers/models/video_llava/convert_video_llava_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconvert_video_llava_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconvert_video_llava_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconvert_video_llava_weights_to_hf.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -37,7 +37,7 @@\n \n     # load model\n     kwargs = {\"device_map\": \"auto\", \"torch_dtype\": torch.float16}\n-    model = VideoLlavaForCausalLM.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", low_cpu_mem_usage=True, **kwargs)\n+    model = VideoLlavaForCausalLM.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", **kwargs)\n \n     # load vision tower\n     model.get_vision_tower().load_model()"
        },
        {
            "sha": "8f820562f329118da36c008254f3ee862fe4c00c",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -2337,7 +2337,6 @@ def _test_attention_implementation(self, attn_implementation):\n                 model_eager = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n                     attn_implementation=\"eager\",\n                 ).to(torch_device)\n                 res_eager = model_eager.generate(**inputs_dict, **generate_kwargs)\n@@ -2347,7 +2346,6 @@ def _test_attention_implementation(self, attn_implementation):\n                 model_attn = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n                     attn_implementation=attn_implementation,\n                 ).to(torch_device)\n                 res_attn = model_attn.generate(**inputs_dict, **generate_kwargs)\n@@ -3724,7 +3722,6 @@ def test_validate_assistant(self):\n         processor = AutoProcessor.from_pretrained(model_id)\n         model = AutoModelForSpeechSeq2Seq.from_pretrained(\n             model_id,\n-            low_cpu_mem_usage=True,\n             use_safetensors=True,\n         )\n         model.to(torch_device)\n@@ -3743,7 +3740,6 @@ def test_validate_assistant(self):\n         # Load its decoder only version:\n         assistant_causal_lm = AutoModelForCausalLM.from_pretrained(\n             assistant_distil_model_id,\n-            low_cpu_mem_usage=True,\n             use_safetensors=True,\n         ).to(torch_device)\n         self.assertTrue(model.generate(**features, assistant_model=assistant_causal_lm).sum())\n@@ -3759,7 +3755,6 @@ def test_validate_assistant(self):\n         # Load its decoder only version:\n         assistant_causal_lm = AutoModelForCausalLM.from_pretrained(\n             assistant_distil_model_id,\n-            low_cpu_mem_usage=True,\n             use_safetensors=True,\n         ).to(torch_device)\n         # It will raise an error as the encoder of the main and assistant model are not compatible:"
        },
        {
            "sha": "1d192b8d9de91faa3862636af3be2fba47cce113",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -556,7 +556,6 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids_seq_id\n                         tmpdirname,\n                         torch_dtype=torch.float16,\n                         attn_implementation=\"flash_attention_2\",\n-                        low_cpu_mem_usage=True,\n                     )\n                     .to(torch_device)\n                     .eval()\n@@ -600,7 +599,7 @@ class BambaModelIntegrationTest(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         model_id = \"ibm-fms/Bamba-9B\"\n-        cls.model = BambaForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n+        cls.model = BambaForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n         cls.tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n         # feels a bit forced to have to do this for the generation test"
        },
        {
            "sha": "f2f009efb913f0a5f572472d625b0879fada18b7",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -238,9 +238,7 @@ def test_batched_small_model_logits(self):\n         ).to(device=torch_device, dtype=torch.float16)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        model = CohereForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16).to(\n-            torch_device\n-        )\n+        model = CohereForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(torch_device)\n \n         tokenizer.pad_token = tokenizer.eos_token\n "
        },
        {
            "sha": "4338e4a070e5979664877037faeef5891e774eb3",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -144,7 +144,7 @@ def test_model_bf16(self):\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"eager\"\n+            model_id, torch_dtype=torch.bfloat16, attn_implementation=\"eager\"\n         ).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n@@ -168,7 +168,7 @@ def test_model_fp16(self):\n         # fmt: on\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16, attn_implementation=\"eager\"\n+            model_id, torch_dtype=torch.float16, attn_implementation=\"eager\"\n         ).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n@@ -189,7 +189,7 @@ def test_model_pipeline_bf16(self):\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n+            model_id, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n         ).to(torch_device)\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
        },
        {
            "sha": "8de3fb818b7b1d535d2a301b4f78614099a58b08",
            "filename": "tests/models/dac/test_modeling_dac.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -280,18 +280,6 @@ def test_attention_outputs(self):\n     def test_hidden_states_output(self):\n         pass\n \n-    @unittest.skip(\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n-    @unittest.skip(\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n-        pass\n-\n-    @unittest.skip(\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n-        pass\n-\n     def test_determinism(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "e6a02626d84bb067a7f6846bbf6aa85c03a26896",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -459,15 +459,13 @@ def test_eager_matches_sdpa_generate(self):\n         model_sdpa = DeepseekV3ForCausalLM.from_pretrained(\n             \"bzantium/tiny-deepseek-v3\",\n             torch_dtype=torch.float16,\n-            low_cpu_mem_usage=True,\n         ).to(torch_device)\n \n         self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n \n         model_eager = DeepseekV3ForCausalLM.from_pretrained(\n             \"bzantium/tiny-deepseek-v3\",\n             torch_dtype=torch.float16,\n-            low_cpu_mem_usage=True,\n             attn_implementation=\"eager\",\n         ).to(torch_device)\n "
        },
        {
            "sha": "7c1c7ee1b06f72ad86c373548bb6954a19611e3e",
            "filename": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -605,18 +605,6 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n-        pass\n-\n     def test_two_stage_training(self):\n         model_class = DeformableDetrForObjectDetection\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "25ca02d5ba4336b0d6371a794b6c2e980e8597d8",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -514,15 +514,13 @@ def test_eager_matches_sdpa_generate(self):\n         model_sdpa = DiffLlamaForCausalLM.from_pretrained(\n             \"kajuma/DiffLlama-0.3B-handcut\",\n             torch_dtype=torch.float16,\n-            low_cpu_mem_usage=True,\n         ).to(torch_device)\n \n         self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n \n         model_eager = DiffLlamaForCausalLM.from_pretrained(\n             \"kajuma/DiffLlama-0.3B-handcut\",\n             torch_dtype=torch.float16,\n-            low_cpu_mem_usage=True,\n             attn_implementation=\"eager\",\n         ).to(torch_device)\n "
        },
        {
            "sha": "21e9ac10405656b6c03689e5807591399971a6f3",
            "filename": "tests/models/encodec/test_modeling_encodec.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -343,18 +343,6 @@ def test_feed_forward_chunking(self):\n     def test_hidden_states_output(self):\n         pass\n \n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n-        pass\n-\n     def test_determinism(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "ef906951a490d76460df3d4cb57eb5727a318f25",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -381,9 +381,7 @@ def test_initialization(self):\n     @slow\n     # Ignore copy\n     def test_model_from_pretrained(self):\n-        model = FalconMambaModel.from_pretrained(\n-            \"tiiuae/falcon-mamba-7b\", torch_dtype=torch.float16, low_cpu_mem_usage=True\n-        )\n+        model = FalconMambaModel.from_pretrained(\"tiiuae/falcon-mamba-7b\", torch_dtype=torch.float16)\n         self.assertIsNotNone(model)\n \n     def test_model_outputs_equivalence(self):"
        },
        {
            "sha": "f468d205ab7caaf384a2d9ef6d6b8239fe2d9000",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 11,
            "deletions": 25,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -126,9 +126,7 @@ def test_model_2b_fp16(self):\n             \"Hi today I am going to share with you a very easy and simple recipe of <strong><em>Kaju Kat\",\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16).to(\n-            torch_device\n-        )\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(torch_device)\n \n         model.generation_config.cache_implementation = \"static\"\n \n@@ -149,9 +147,7 @@ def test_model_2b_bf16(self):\n             \"Hi today I am going to share with you a very easy and simple recipe of <strong><em>Kaju Kat\",\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\n-            torch_device\n-        )\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n@@ -171,9 +167,7 @@ def test_model_2b_eager(self):\n         ]\n \n         # bfloat16 gives strange values, likely due to it has lower precision + very short prompts\n-        model = AutoModelForCausalLM.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16, attn_implementation=\"eager\"\n-        )\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, attn_implementation=\"eager\")\n         model.to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n@@ -195,7 +189,7 @@ def test_model_2b_flash_attn(self):\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+            model_id, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n         )\n         model.to(torch_device)\n \n@@ -216,7 +210,7 @@ def test_model_2b_4bit(self):\n             \"Hi today I'd like to share with you my experience with the new wattpad wattpad wattpad wattpad wattpad wattpad wattpad\",\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, load_in_4bit=True)\n+        model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n@@ -235,7 +229,7 @@ def test_model_7b_fp32(self):\n             \"Hi,\\n\\nI have a problem with my 2005 1.6 16\",\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True).to(torch_device)\n+        model = AutoModelForCausalLM.from_pretrained(model_id).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n@@ -256,9 +250,7 @@ def test_model_7b_fp16(self):\n             \"Hi today I am going to show you how to make a simple and easy to make a DIY 3D\",\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16).to(\n-            torch_device\n-        )\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n@@ -290,9 +282,7 @@ def test_model_7b_bf16(self):\n         # fmt: on\n         expected_text = EXPECTED_TEXTS.get_expectation()\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\n-            torch_device\n-        )\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n@@ -312,9 +302,7 @@ def test_model_7b_fp16_static_cache(self):\n             \"Hi today I am going to show you how to make a simple and easy to make a DIY 3D\",\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16).to(\n-            torch_device\n-        )\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(torch_device)\n \n         model.generation_config.cache_implementation = \"static\"\n \n@@ -333,7 +321,7 @@ def test_model_7b_4bit(self):\n             \"Hi today I am going to talk about the best way to get rid of acne. miniaturing is a very\",\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, load_in_4bit=True)\n+        model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n@@ -451,9 +439,7 @@ def test_model_2b_bf16_dola(self):\n             \"Hi today we have the review for a <strong>2016/2017</strong> season of\",\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\n-            torch_device\n-        )\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)"
        },
        {
            "sha": "808646186c26097e806a5302553b64b8ce80d511",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -197,7 +197,7 @@ def test_model_9b_bf16(self):\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"eager\"\n+            model_id, torch_dtype=torch.bfloat16, attn_implementation=\"eager\"\n         ).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n@@ -218,7 +218,7 @@ def test_model_9b_fp16(self):\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16, attn_implementation=\"eager\"\n+            model_id, torch_dtype=torch.float16, attn_implementation=\"eager\"\n         ).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n@@ -241,7 +241,7 @@ def test_model_9b_pipeline_bf16(self):\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n+            model_id, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n         ).to(torch_device)\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n@@ -271,7 +271,7 @@ def test_model_2b_pipeline_bf16_flex_attention(self):\n         EXPECTED_BATCH_TEXT = EXPECTED_BATCH_TEXTS.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n+            model_id, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n         ).to(torch_device)\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n@@ -419,7 +419,7 @@ def test_model_9b_bf16_flex_attention(self):\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n+            model_id, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n         ).to(torch_device)\n         assert model.config._attn_implementation == \"flex_attention\"\n         tokenizer = AutoTokenizer.from_pretrained(model_id)"
        },
        {
            "sha": "b0b25579b82d417f1420faa8f83f3987d59dd503",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 18,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -391,9 +391,7 @@ def tearDown(self):\n     def test_model_4b_bf16(self):\n         model_id = \"google/gemma-3-4b-it\"\n \n-        model = Gemma3ForConditionalGeneration.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n-        ).to(torch_device)\n+        model = Gemma3ForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(torch_device)\n \n         inputs = self.processor.apply_chat_template(\n             self.messages,\n@@ -421,9 +419,7 @@ def test_model_4b_bf16(self):\n     def test_model_4b_batch(self):\n         model_id = \"google/gemma-3-4b-it\"\n \n-        model = Gemma3ForConditionalGeneration.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n-        ).to(torch_device)\n+        model = Gemma3ForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(torch_device)\n \n         messages_2 = [\n             {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n@@ -474,9 +470,7 @@ def test_model_4b_batch(self):\n     def test_model_4b_crops(self):\n         model_id = \"google/gemma-3-4b-it\"\n \n-        model = Gemma3ForConditionalGeneration.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n-        ).to(torch_device)\n+        model = Gemma3ForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(torch_device)\n \n         crop_config = {\n             \"images_kwargs\": {\n@@ -516,9 +510,7 @@ def test_model_4b_crops(self):\n     def test_model_4b_batch_crops(self):\n         model_id = \"google/gemma-3-4b-it\"\n \n-        model = Gemma3ForConditionalGeneration.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n-        ).to(torch_device)\n+        model = Gemma3ForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(torch_device)\n         crop_config = {\n             \"images_kwargs\": {\n                 \"do_pan_and_scan\": True,\n@@ -576,9 +568,7 @@ def test_model_4b_batch_crops(self):\n     def test_model_4b_multiimage(self):\n         model_id = \"google/gemma-3-4b-it\"\n \n-        model = Gemma3ForConditionalGeneration.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n-        ).to(torch_device)\n+        model = Gemma3ForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(torch_device)\n \n         messages = [\n             {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n@@ -616,9 +606,7 @@ def test_model_4b_multiimage(self):\n     def test_model_1b_text_only(self):\n         model_id = \"google/gemma-3-1b-it\"\n \n-        model = Gemma3ForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\n-            torch_device\n-        )\n+        model = Gemma3ForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(torch_device)\n         tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n         inputs = tokenizer(\"Write a poem about Machine Learning.\", return_tensors=\"pt\").to(torch_device)\n "
        },
        {
            "sha": "212bcbbaf1a527530e829ea7b37a57e37123b73a",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -88,7 +88,7 @@ def test_model_9b_fp16(self):\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            self.model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16, revision=self.revision\n+            self.model_id, torch_dtype=torch.float16, revision=self.revision\n         ).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(self.model_id, revision=self.revision)\n@@ -106,7 +106,7 @@ def test_model_9b_bf16(self):\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            self.model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, revision=self.revision\n+            self.model_id, torch_dtype=torch.bfloat16, revision=self.revision\n         ).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(self.model_id, revision=self.revision)\n@@ -125,7 +125,6 @@ def test_model_9b_eager(self):\n \n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_id,\n-            low_cpu_mem_usage=True,\n             torch_dtype=torch.bfloat16,\n             attn_implementation=\"eager\",\n             revision=self.revision,\n@@ -149,7 +148,6 @@ def test_model_9b_sdpa(self):\n \n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_id,\n-            low_cpu_mem_usage=True,\n             torch_dtype=torch.bfloat16,\n             attn_implementation=\"sdpa\",\n             revision=self.revision,\n@@ -174,7 +172,6 @@ def test_model_9b_flash_attn(self):\n \n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_id,\n-            low_cpu_mem_usage=True,\n             torch_dtype=torch.bfloat16,\n             attn_implementation=\"flash_attention_2\",\n             revision=self.revision,"
        },
        {
            "sha": "5655e832223c7c3a484ab839f5414b98f8b5ea02",
            "filename": "tests/models/glm4/test_modeling_glm4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -104,9 +104,7 @@ def test_model_9b_fp16(self):\n         )\n         EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n \n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16\n-        ).to(torch_device)\n+        model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.float16).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n@@ -132,9 +130,7 @@ def test_model_9b_bf16(self):\n         )\n         EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n \n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n-        ).to(torch_device)\n+        model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n@@ -162,7 +158,6 @@ def test_model_9b_eager(self):\n \n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_id,\n-            low_cpu_mem_usage=True,\n             torch_dtype=torch.bfloat16,\n             attn_implementation=\"eager\",\n         )\n@@ -195,7 +190,6 @@ def test_model_9b_sdpa(self):\n \n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_id,\n-            low_cpu_mem_usage=True,\n             torch_dtype=torch.bfloat16,\n             attn_implementation=\"sdpa\",\n         )\n@@ -226,7 +220,6 @@ def test_model_9b_flash_attn(self):\n \n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_id,\n-            low_cpu_mem_usage=True,\n             torch_dtype=torch.bfloat16,\n             attn_implementation=\"flash_attention_2\",\n         )"
        },
        {
            "sha": "7a1881047177777726ed3bf4b0be6cd104a6a814",
            "filename": "tests/models/helium/test_modeling_helium.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -87,9 +87,9 @@ def test_model_2b(self):\n             \"Hello, today is a great day to start a new project. I have been working on a new project for a while now and I have\"\n         ]\n \n-        model = AutoModelForCausalLM.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, revision=\"refs/pr/1\"\n-        ).to(torch_device)\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, revision=\"refs/pr/1\").to(\n+            torch_device\n+        )\n         tokenizer = AutoTokenizer.from_pretrained(model_id, revision=\"refs/pr/1\")\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n "
        },
        {
            "sha": "5a7dbae587f82795479d1f3153d7916ab7fb6617",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -727,7 +727,7 @@ class InstructBlipModelIntegrationTest(unittest.TestCase):\n     def test_inference_vicuna_7b(self):\n         processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n         model = InstructBlipForConditionalGeneration.from_pretrained(\n-            \"Salesforce/instructblip-vicuna-7b\", load_in_8bit=True, low_cpu_mem_usage=True\n+            \"Salesforce/instructblip-vicuna-7b\", load_in_8bit=True\n         )\n \n         url = \"https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/Confusing-Pictures.jpg\"\n@@ -752,7 +752,6 @@ def test_inference_flant5_xl(self):\n         model = InstructBlipForConditionalGeneration.from_pretrained(\n             \"Salesforce/instructblip-flan-t5-xl\",\n             torch_dtype=torch.bfloat16,\n-            low_cpu_mem_usage=True,\n         ).to(torch_device)\n \n         url = \"https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/Confusing-Pictures.jpg\"\n@@ -789,7 +788,6 @@ def test_inference_interpolate_pos_encoding(self):\n         model = InstructBlipForConditionalGeneration.from_pretrained(\n             \"Salesforce/instructblip-flan-t5-xl\",\n             torch_dtype=torch.bfloat16,\n-            low_cpu_mem_usage=True,\n         ).to(torch_device)\n         processor.image_processor.size = {\"height\": 500, \"width\": 500}\n \n@@ -810,7 +808,6 @@ def test_expansion_in_processing(self):\n         model = InstructBlipForConditionalGeneration.from_pretrained(\n             \"Salesforce/instructblip-flan-t5-xl\",\n             torch_dtype=torch.bfloat16,\n-            low_cpu_mem_usage=True,\n         ).to(torch_device)\n \n         image = prepare_img()"
        },
        {
            "sha": "17e6b0a64d7534251db526abc8b34c5d14f2cbbd",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -744,7 +744,8 @@ class InstructBlipVideoModelIntegrationTest(unittest.TestCase):\n     def test_inference_vicuna_7b(self):\n         processor = InstructBlipVideoProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n         model = InstructBlipVideoForConditionalGeneration.from_pretrained(\n-            \"Salesforce/instructblip-vicuna-7b\", load_in_8bit=True, low_cpu_mem_usage=True\n+            \"Salesforce/instructblip-vicuna-7b\",\n+            load_in_8bit=True,\n         )\n \n         clip = prepare_video()\n@@ -762,7 +763,8 @@ def test_inference_vicuna_7b(self):\n     def test_expansion_in_processing(self):\n         processor = InstructBlipVideoProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n         model = InstructBlipVideoForConditionalGeneration.from_pretrained(\n-            \"Salesforce/instructblip-vicuna-7b\", load_in_8bit=True, low_cpu_mem_usage=True\n+            \"Salesforce/instructblip-vicuna-7b\",\n+            load_in_8bit=True,\n         )\n \n         clip = prepare_video()"
        },
        {
            "sha": "f73e05e2c1c7b7ca6c541789aafe6c9c189c56bb",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -527,7 +527,6 @@ def test_flash_attn_2_fp32_ln(self):\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n                     attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n                     load_in_4bit=True,\n                 )\n \n@@ -563,7 +562,10 @@ class JambaModelIntegrationTest(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         model_id = \"ai21labs/Jamba-tiny-dev\"\n-        cls.model = JambaForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n+        cls.model = JambaForCausalLM.from_pretrained(\n+            model_id,\n+            torch_dtype=torch.bfloat16,\n+        )\n         cls.tokenizer = AutoTokenizer.from_pretrained(model_id)\n         cls.device_properties = get_device_properties()\n "
        },
        {
            "sha": "754e06a3c729a7e816162df0580f72b8ed33b58a",
            "filename": "tests/models/lxmert/test_modeling_lxmert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -765,18 +765,6 @@ def prepare_tf_inputs_from_pt_inputs(self, pt_inputs_dict):\n \n         return tf_inputs_dict\n \n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n-        pass\n-\n     @unittest.skip(\n         reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )"
        },
        {
            "sha": "291814efde5dd5a6a4afc57ceda9894d5c763fc1",
            "filename": "tests/models/marian/test_modeling_marian.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -351,18 +351,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n-        pass\n-\n \n def assert_tensors_close(a, b, atol=1e-12, prefix=\"\"):\n     \"\"\"If tensors have different shapes, different values or a and b are not both tensors, raise a nice Assertion error.\"\"\""
        },
        {
            "sha": "b9ae9d451555c03bd9e7809c819fcabdaa015a2a",
            "filename": "tests/models/minimax/test_modeling_minimax.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -246,9 +246,10 @@ def test_small_model_logits(self):\n         model_id = \"hf-internal-testing/MiniMax-tiny\"\n         dummy_input = torch.LongTensor([[0, 1, 0], [0, 1, 0]]).to(torch_device)\n \n-        model = MiniMaxForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True).to(\n-            torch_device\n-        )\n+        model = MiniMaxForCausalLM.from_pretrained(\n+            model_id,\n+            torch_dtype=torch.bfloat16,\n+        ).to(torch_device)\n         expected_slice = torch.tensor(\n             [[1.0312, -0.5156, -0.3262], [-0.1152, 0.4336, 0.2412], [1.2188, -0.5898, -0.0381]]\n         ).to(torch_device)\n@@ -265,9 +266,10 @@ def test_small_model_generation(self):\n         model_id = \"hf-internal-testing/MiniMax-tiny\"\n         dummy_input = torch.LongTensor([[0, 1, 0], [0, 1, 0]]).to(torch_device)\n \n-        model = MiniMaxForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True).to(\n-            torch_device\n-        )\n+        model = MiniMaxForCausalLM.from_pretrained(\n+            model_id,\n+            torch_dtype=torch.bfloat16,\n+        ).to(torch_device)\n         expected_slice = (\n             torch.tensor([[0, 1, 0, 933, 307, 3102, 2457, 1208], [0, 1, 0, 933, 307, 3102, 2457, 1208]])\n             .to(torch.int64)"
        },
        {
            "sha": "97e00e13a63d380ecec69d65869d6845abccf9c2",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -156,9 +156,10 @@ def test_small_model_logits(self):\n         model_id = \"hf-internal-testing/Mixtral-tiny\"\n         dummy_input = torch.LongTensor([[0, 1, 0], [0, 1, 0]]).to(torch_device)\n \n-        model = MixtralForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True).to(\n-            torch_device\n-        )\n+        model = MixtralForCausalLM.from_pretrained(\n+            model_id,\n+            torch_dtype=torch.bfloat16,\n+        ).to(torch_device)\n         # TODO: might need to tweak it in case the logits do not match on our daily runners\n         # these logits have been obtained with the original megablocks implementation.\n         # (\"cuda\", 8) for A100/A10, and (\"cuda\", 7) for T4\n@@ -189,9 +190,10 @@ def test_small_model_logits_batched(self):\n         dummy_input = torch.LongTensor([[0, 0, 0, 0, 0, 0, 1, 2, 3], [1, 1, 2, 3, 4, 5, 6, 7, 8]]).to(torch_device)\n         attention_mask = dummy_input.ne(0).to(torch.long)\n \n-        model = MixtralForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True).to(\n-            torch_device\n-        )\n+        model = MixtralForCausalLM.from_pretrained(\n+            model_id,\n+            torch_dtype=torch.bfloat16,\n+        ).to(torch_device)\n \n         # TODO: might need to tweak it in case the logits do not match on our daily runners\n         #"
        },
        {
            "sha": "4f5b1689594e5bdf30cbded2d178f08df551da0a",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -722,15 +722,13 @@ def test_eager_matches_sdpa_generate(self):\n                 model_sdpa = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n                 ).to(torch_device)\n \n                 self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n \n                 model_eager = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n                     attn_implementation=\"eager\",\n                 ).to(torch_device)\n "
        },
        {
            "sha": "3386ea71a0a0ffbdede8b6422a22ccb46ed38bed",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -788,18 +788,6 @@ def test_tied_model_weights_key_ignore(self):\n     def test_tied_weights_keys(self):\n         pass\n \n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n-        pass\n-\n     # override since changing `output_hidden_states` / `output_attentions` from the top-level model config won't work\n     def test_retain_grad_hidden_states_attentions(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "37d7736d1c94fbfbeabe8b4c51256039d3a291d4",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -789,18 +789,6 @@ def test_tied_model_weights_key_ignore(self):\n     def test_tied_weights_keys(self):\n         pass\n \n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n-        pass\n-\n     # override since changing `output_hidden_states` / `output_attentions` from the top-level model config won't work\n     # Ignore copy\n     def test_retain_grad_hidden_states_attentions(self):"
        },
        {
            "sha": "d0d42f61d2c689219e6648c7f699c7aa66a11571",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -326,18 +326,6 @@ def test_determinism(self):\n     def test_feed_forward_chunking(self):\n         pass\n \n-    @unittest.skip(reason=\"PaliGemma does not support low_cpu_mem_usage.\")\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n-    @unittest.skip(reason=\"PaliGemma does not support low_cpu_mem_usage.\")\n-    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n-        pass\n-\n-    @unittest.skip(reason=\"PaliGemma does not support low_cpu_mem_usage.\")\n-    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n-        pass\n-\n     @unittest.skip(\n         reason=\"VLMs doesn't accept inputs embeds and pixel values at the same time. So if the test passed for backbone LM, it passes for VLM also\"\n     )"
        },
        {
            "sha": "c9a53efa14a7367eddc343195edeacc75c9838ff",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -316,18 +316,6 @@ def test_determinism(self):\n     def test_feed_forward_chunking(self):\n         pass\n \n-    @unittest.skip(reason=\"PaliGemma does not support low_cpu_mem_usage.\")\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n-    @unittest.skip(reason=\"PaliGemma does not support low_cpu_mem_usage.\")\n-    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n-        pass\n-\n-    @unittest.skip(reason=\"PaliGemma does not support low_cpu_mem_usage.\")\n-    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n-        pass\n-\n     @unittest.skip(\n         reason=\"VLMs doesn't accept inputs embeds and pixel values at the same time. So if the test passed for backbone LM, it passes for VLM also\"\n     )"
        },
        {
            "sha": "99c26096113f789a1d2057d6fba26177f28de5e9",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -368,10 +368,6 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_prompt_lookup_decoding_matches_greedy_search(self):\n         super().test_prompt_lookup_decoding_matches_greedy_search()\n \n-    @unittest.skip(reason=\"The base class is LM only and cannot be init with XModelConfig`\")\n-    def test_save_load_fast_init_from_base(self):\n-        pass\n-\n     # The multimodal base model embeds will not match ids, due to pixel values. We can't change base test\n     # because in some models `pixel_values` are required. Will be fixed when we add support for merging `embeds+pixels`\n     # TODO: @raushan"
        },
        {
            "sha": "5299b6a2c1195773664655272607a443eddcee7c",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -318,10 +318,6 @@ def test_model_is_small(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n-    @unittest.skip(reason=\"The base class is LM only and cannot be init with XModelConfig`\")\n-    def test_save_load_fast_init_from_base(self):\n-        pass\n-\n     # The multimodal base model embeds will not match ids, due to pixel values. We can't change base test\n     # because in some models `pixel_values` are required. Will be fixed when we add support for merging `embeds+pixels`\n     # TODO: @raushan"
        },
        {
            "sha": "62a0aef6f41d5706715f0348e3454da7544a35e7",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -182,7 +182,9 @@ class RecurrentGemmaIntegrationTest(unittest.TestCase):\n     @require_read_token\n     def test_2b_generate(self):\n         EXPECTED_TEXTS = ['Hello I am doing a project on the topic of \"The impact of the internet on the society\" and I am looking for some information on the topic. I am looking for some information on the impact of the internet on the society. I am looking for some information on the impact of the internet on the society. I am looking for some', 'Hi today is a new app that allows you to make money by watching videos.\\n\\nThe app is very simple to use and you can earn money by watching videos.\\n\\nThe app is available for both Android and iOS devices and you can download it from the Google Play Store or the App Store.\\n\\nOnce you have downloaded the app']  # fmt: skip\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, low_cpu_mem_usage=True).to(torch_device)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.model_id,\n+        ).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n         tokenizer.padding_side = \"right\"\n@@ -204,9 +206,7 @@ def test_2b_generate(self):\n \n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16\n-        ).to(torch_device)\n+        model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.float16).to(torch_device)\n         output = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n         del model\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n@@ -246,9 +246,7 @@ def test_model_2b_8bit(self):\n     def test_long_context(self):\n         EXPECTED_GENERATION = [' Jean-Paul Delannoy told CNN that the BEA is \"not aware of any video footage that could have been taken on board the plane.\" He added that the BEA is \"not aware of any video footage that could have been taken on board the plane.\" The BEA is the French equivalent of the National Transportation Safety Board']  # fmt: skip\n \n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16\n-        ).to(torch_device)\n+        model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.float16).to(torch_device)\n         tokenizer = AutoTokenizer.from_pretrained(self.model_id, padding_side=\"left\")\n         inputs = tokenizer(self.input_long_text, return_tensors=\"pt\").to(torch_device)\n         output = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n@@ -260,9 +258,7 @@ def test_long_context(self):\n     def test_longer_than_window(self):\n         EXPECTED_GENERATION = [\" Robin's comments follow claims by two magazines, German daily Bild and French Paris Match, of a cell phone video showing the harrowing final seconds from on board Germanwings Flight 9525 as it crashed into the French Alps. All 150 on board were killed. Paris Match and Bild reported that the\"]  # fmt: skip\n \n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16\n-        ).to(torch_device)\n+        model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.float16).to(torch_device)\n         model.config.attention_window_size = 256  # Make the attention window size shorter than the current prompt\n         tokenizer = AutoTokenizer.from_pretrained(self.model_id, padding_side=\"left\")\n         inputs = tokenizer(self.input_long_text, return_tensors=\"pt\").to(torch_device)"
        },
        {
            "sha": "fa1ada4f616018c472e3207e79d876b52621d0c6",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -248,14 +248,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(reason=\"SamVisionModel has no base class and is not available in MODEL_MAPPING\")\n-    def test_save_load_fast_init_from_base(self):\n-        pass\n-\n-    @unittest.skip(reason=\"SamVisionModel has no base class and is not available in MODEL_MAPPING\")\n-    def test_save_load_fast_init_to_base(self):\n-        pass\n-\n     @unittest.skip(reason=\"SamVisionModel does not support training\")\n     def test_retain_grad_hidden_states_attentions(self):\n         pass"
        },
        {
            "sha": "830b537031d049a1f0eefb94c7ba0027c717dfd8",
            "filename": "tests/models/sam_hq/test_modeling_sam_hq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -256,14 +256,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(reason=\"SamVisionModel has no base class and is not available in MODEL_MAPPING\")\n-    def test_save_load_fast_init_from_base(self):\n-        pass\n-\n-    @unittest.skip(reason=\"SamVisionModel has no base class and is not available in MODEL_MAPPING\")\n-    def test_save_load_fast_init_to_base(self):\n-        pass\n-\n     @unittest.skip(reason=\"SamVisionModel does not support training\")\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n@@ -695,14 +687,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(reason=\"SamHQModel has no base class and is not available in MODEL_MAPPING\")\n-    def test_save_load_fast_init_from_base(self):\n-        pass\n-\n-    @unittest.skip(reason=\"SamHQModel has no base class and is not available in MODEL_MAPPING\")\n-    def test_save_load_fast_init_to_base(self):\n-        pass\n-\n     @unittest.skip(reason=\"SamHQModel does not support training\")\n     def test_retain_grad_hidden_states_attentions(self):\n         pass"
        },
        {
            "sha": "6e049b4faba3cbc2f493b4ff683d835807f07b71",
            "filename": "tests/models/sew/test_modeling_sew.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -325,18 +325,6 @@ def test_resize_tokens_embeddings(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n-        pass\n-\n     def test_retain_grad_hidden_states_attentions(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.output_hidden_states = True"
        },
        {
            "sha": "4df373e8391ae87a8f38b055aa5d9f2dc11508f1",
            "filename": "tests/models/sew_d/test_modeling_sew_d.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -430,18 +430,6 @@ def _mock_init_weights(self, module):\n     def test_feed_forward_chunking(self):\n         pass\n \n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n-        pass\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model = SEWDModel.from_pretrained(\"asapp/sew-d-tiny-100k\")"
        },
        {
            "sha": "de41ad0fe0df529d5783122c4919d40a109a1698",
            "filename": "tests/models/shieldgemma2/test_modeling_shieldgemma2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fshieldgemma2%2Ftest_modeling_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fshieldgemma2%2Ftest_modeling_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fshieldgemma2%2Ftest_modeling_shieldgemma2.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -49,9 +49,9 @@ def test_model(self):\n         response = requests.get(url)\n         image = Image.open(BytesIO(response.content))\n \n-        model = ShieldGemma2ForImageClassification.from_pretrained(\n-            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n-        ).to(torch_device)\n+        model = ShieldGemma2ForImageClassification.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(\n+            torch_device\n+        )\n \n         inputs = processor(images=[image]).to(torch_device)\n         output = model(**inputs)"
        },
        {
            "sha": "2b8f0d9a9eb30b8527a56dac53f60c5bc5e2ab2f",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -1109,14 +1109,16 @@ def import_accelerate_mock(name, *args, **kwargs):\n \n         # Load using `accelerate` in bf16\n         model = T5ForConditionalGeneration.from_pretrained(\n-            \"google-t5/t5-small\", torch_dtype=torch.bfloat16, low_cpu_mem_usage=True\n+            \"google-t5/t5-small\",\n+            torch_dtype=torch.bfloat16,\n         )\n         self.assertTrue(model.decoder.block[0].layer[2].DenseReluDense.wo.weight.dtype == torch.bfloat16)\n         self.assertTrue(model.decoder.block[0].layer[2].DenseReluDense.wi.weight.dtype == torch.bfloat16)\n \n         # Load without using `accelerate`\n         model = T5ForConditionalGeneration.from_pretrained(\n-            \"google-t5/t5-small\", torch_dtype=torch.float16, low_cpu_mem_usage=True\n+            \"google-t5/t5-small\",\n+            torch_dtype=torch.float16,\n         )\n         self.assertTrue(model.decoder.block[0].layer[2].DenseReluDense.wo.weight.dtype == torch.float32)\n         self.assertTrue(model.decoder.block[0].layer[2].DenseReluDense.wi.weight.dtype == torch.float16)"
        },
        {
            "sha": "e915233a38faf4296737ed224bf7d2d65f6d8221",
            "filename": "tests/models/timm_backbone/test_modeling_timm_backbone.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -156,18 +156,6 @@ def test_from_pretrained_no_checkpoint(self):\n     def test_save_load(self):\n         pass\n \n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n-        pass\n-\n-    @unittest.skip(reason=\"No support for low_cpu_mem_usage=True.\")\n-    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n-        pass\n-\n     @unittest.skip(reason=\"TimmBackbone uses its own `from_pretrained` without device_map support\")\n     def test_can_load_with_device_context_manager(self):\n         pass"
        },
        {
            "sha": "86b7710c17641e4b11b01030cb30a75933bd2ef7",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -407,12 +407,6 @@ def test_custom_4d_attention_mask(self):\n             normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n             torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n \n-    @unittest.skip(\n-        \"Not currently compatible. Fails with - NotImplementedError: Cannot copy out of meta tensor; no data!\"\n-    )\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"microsoft/udop-large\"\n@@ -615,12 +609,6 @@ def test_custom_4d_attention_mask(self):\n             normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n             torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n \n-    @unittest.skip(\n-        \"Not currently compatible. Fails with - NotImplementedError: Cannot copy out of meta tensor; no data!\"\n-    )\n-    def test_save_load_low_cpu_mem_usage(self):\n-        pass\n-\n \n @require_torch\n @require_sentencepiece"
        },
        {
            "sha": "ab9c98484b722d682178fc54c25d8edcbb211d3b",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -2431,15 +2431,15 @@ def test_speculative_decoding_distil(self):\n         torch_dtype = torch.float16 if (torch.cuda.is_available() or is_torch_xpu_available()) else torch.float32\n         model_id = \"openai/whisper-large-v2\"\n         model = WhisperForConditionalGeneration.from_pretrained(\n-            model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n+            model_id, torch_dtype=torch_dtype, use_safetensors=True\n         )\n         model.to(torch_device)\n \n         processor = WhisperProcessor.from_pretrained(model_id)\n \n         assistant_model_id = \"distil-whisper/distil-large-v2\"\n         assistant_model = WhisperForCausalLM.from_pretrained(\n-            assistant_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n+            assistant_model_id, torch_dtype=torch_dtype, use_safetensors=True\n         )\n         assistant_model.to(torch_device)\n \n@@ -2481,15 +2481,15 @@ def test_speculative_decoding_non_distil(self):\n         torch_dtype = torch.float16 if torch_device in [\"cuda\", \"xpu\"] else torch.float32\n         model_id = \"openai/whisper-large-v2\"\n         model = WhisperForConditionalGeneration.from_pretrained(\n-            model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n+            model_id, torch_dtype=torch_dtype, use_safetensors=True\n         )\n         model.to(torch_device)\n \n         processor = WhisperProcessor.from_pretrained(model_id)\n \n         assistant_model_id = \"openai/whisper-tiny\"\n         assistant_model = WhisperForConditionalGeneration.from_pretrained(\n-            assistant_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n+            assistant_model_id, torch_dtype=torch_dtype, use_safetensors=True\n         )\n         assistant_model.to(torch_device)\n "
        },
        {
            "sha": "7140373081bb29ed94a41cb6dcbf45b2132854b3",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -531,7 +531,6 @@ def test_flash_attn_2_fp32_ln(self):\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n                     attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n                     load_in_4bit=True,\n                 )\n \n@@ -565,9 +564,7 @@ class ZambaModelIntegrationTest(unittest.TestCase):\n     @slow\n     def setUpClass(cls):\n         model_id = \"Zyphra/Zamba-7B-v1\"\n-        cls.model = ZambaForCausalLM.from_pretrained(\n-            model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True, use_mamba_kernels=False\n-        )\n+        cls.model = ZambaForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, use_mamba_kernels=False)\n         cls.tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n     @slow"
        },
        {
            "sha": "a40ea394f97243b45ff63c2f711dff1635c523ac",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -549,7 +549,6 @@ def test_flash_attn_2_fp32_ln(self):\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n                     attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n                     load_in_4bit=True,\n                 )\n \n@@ -610,9 +609,7 @@ class Zamba2ModelIntegrationTest(unittest.TestCase):\n     @slow\n     def setUpClass(cls):\n         model_id = \"Zyphra/Zamba2-1.2B\"\n-        cls.model = Zamba2ForCausalLM.from_pretrained(\n-            model_id, torch_dtype=torch.float32, low_cpu_mem_usage=True, revision=\"PR\"\n-        )\n+        cls.model = Zamba2ForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32, revision=\"PR\")\n         cls.tokenizer = AutoTokenizer.from_pretrained(model_id, revision=\"PR\")\n \n     @parameterized.expand([(torch_device,), (\"cpu\",)])"
        },
        {
            "sha": "e867deef1e707d59aa0d9fb64fce433a1a0635ed",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -328,7 +328,6 @@ def test_raise_save_pretrained(self):\n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n             quantization_config=quantization_config,\n-            low_cpu_mem_usage=True,\n             revision=self.model_revision,\n         ).to(torch_device)\n \n@@ -347,7 +346,6 @@ def test_fused_modules_to_not_convert(self):\n         model = AutoModelForCausalLM.from_pretrained(\n             model_id,\n             quantization_config=quantization_config,\n-            low_cpu_mem_usage=True,\n         ).to(torch_device)\n \n         # Check if model has been correctly fused\n@@ -370,7 +368,6 @@ def test_generation_fused(self):\n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n             quantization_config=quantization_config,\n-            low_cpu_mem_usage=True,\n             revision=self.model_revision,\n         ).to(torch_device)\n \n@@ -399,7 +396,6 @@ def test_generation_fused_batched(self):\n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n             quantization_config=quantization_config,\n-            low_cpu_mem_usage=True,\n             revision=self.model_revision,\n         ).to(torch_device)\n "
        },
        {
            "sha": "877f6a2cd8d771cf3cdfbc6742ef1959c062479c",
            "filename": "tests/quantization/hqq/test_hqq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhqq%2Ftest_hqq.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -42,7 +42,6 @@ def __init__(self, model_id, quant_config, compute_dtype, device, cache_dir=None\n             torch_dtype=compute_dtype,\n             device_map=device,\n             quantization_config=quant_config,\n-            low_cpu_mem_usage=True,\n             cache_dir=cache_dir,\n         )\n         self.tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n@@ -233,7 +232,9 @@ def test_model_serialization(self):\n \n         # Load and check if the logits match\n         model_loaded = AutoModelForCausalLM.from_pretrained(\n-            \"quant_model\", torch_dtype=torch.float16, device_map=torch_device, low_cpu_mem_usage=True\n+            \"quant_model\",\n+            torch_dtype=torch.float16,\n+            device_map=torch_device,\n         )\n \n         with torch.no_grad():"
        },
        {
            "sha": "fed16e2f028c4d2b9624203d1a13412de30d9dee",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 84,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -578,87 +578,6 @@ def seeded_initialize_weights(self, module):\n                 f\"The following keys are not properly handled by `_init_weights()`:\\n{different_weights}\",\n             )\n \n-    @slow\n-    @require_accelerate\n-    @mark.accelerate_tests\n-    def test_save_load_low_cpu_mem_usage(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        with tempfile.TemporaryDirectory() as saved_model_path:\n-            for model_class in self.all_model_classes:\n-                model_to_save = model_class(config)\n-                model_to_save.save_pretrained(saved_model_path)\n-\n-                self._check_save_load_low_cpu_mem_usage(model_class, saved_model_path)\n-\n-    @slow\n-    @require_accelerate\n-    @mark.accelerate_tests\n-    def test_save_load_low_cpu_mem_usage_checkpoints(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        with tempfile.TemporaryDirectory() as saved_model_path:\n-            for model_class in self.all_model_classes:\n-                model_to_save = model_class(config)\n-                model_to_save.config.save_pretrained(saved_model_path)\n-                torch.save(model_to_save.state_dict(), os.path.join(saved_model_path, \"pytorch_model.bin\"))\n-\n-                self._check_save_load_low_cpu_mem_usage(model_class, saved_model_path)\n-\n-    @slow\n-    @require_accelerate\n-    @mark.accelerate_tests\n-    def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n-        with tempfile.TemporaryDirectory() as saved_model_path:\n-            for model_class in self.all_model_classes:\n-                config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-                model_to_save = model_class(config)\n-\n-                model_to_save.save_pretrained(saved_model_path, safe_serialization=False)\n-                self._check_save_load_low_cpu_mem_usage(model_class, saved_model_path)\n-\n-    def _check_save_load_low_cpu_mem_usage(self, model_class, saved_model_path):\n-        from accelerate.utils.modeling import named_module_tensors\n-\n-        # Load the low usage and the normal models.\n-        model_low_usage, loading_info = model_class.from_pretrained(\n-            saved_model_path,\n-            low_cpu_mem_usage=True,\n-            output_loading_info=True,\n-        )\n-        model_non_low_usage = model_class.from_pretrained(saved_model_path)\n-\n-        # Check that there were no missing keys.\n-        self.assertEqual(loading_info[\"missing_keys\"], [])\n-\n-        # The low_cpu_mem_usage=True causes the model params to be initialized with device=meta, and then\n-        # subsequently loaded with the correct values and onto the correct device. We check if there are any\n-        # remaining params that were not properly loaded.\n-        for name, tensor in named_module_tensors(model_low_usage, recurse=True):\n-            self.assertNotEqual(\n-                tensor.device,\n-                torch.device(\"meta\"),\n-                \"Tensor '\" + name + \"' has not been properly loaded and has device=meta.\",\n-            )\n-\n-        # Check that the parameters are equal.\n-        for p1, p2 in zip(model_low_usage.parameters(), model_non_low_usage.parameters()):\n-            self.assertEqual(p1.data.ne(p2.data).sum(), 0)\n-\n-        # Check that the state dict keys are equal.\n-        self.assertEqual(set(model_low_usage.state_dict().keys()), set(model_non_low_usage.state_dict().keys()))\n-\n-        # Check that the shared tensors are equal.\n-        tensor_ptrs1 = collections.defaultdict(list)\n-        for name, tensor in model_low_usage.state_dict().items():\n-            tensor_ptrs1[id_tensor_storage(tensor)].append(name)\n-        tied_params1 = [names for _, names in tensor_ptrs1.items() if len(names) > 1]\n-\n-        tensor_ptrs2 = collections.defaultdict(list)\n-        for name, tensor in model_non_low_usage.state_dict().items():\n-            tensor_ptrs2[id_tensor_storage(tensor)].append(name)\n-        tied_params2 = [names for _, names in tensor_ptrs2.items() if len(names) > 1]\n-\n-        self.assertEqual(tied_params1, tied_params2)\n-\n     def test_torch_save_load(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         if config.__class__ not in MODEL_MAPPING:\n@@ -4100,7 +4019,6 @@ def test_flash_attn_2_fp32_ln(self):\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n                     attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n                     load_in_4bit=True,\n                 )\n \n@@ -4173,7 +4091,6 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n                         tmpdirname,\n                         torch_dtype=torch.float16,\n                         attn_implementation=\"flash_attention_2\",\n-                        low_cpu_mem_usage=True,\n                     )\n                     .to(torch_device)\n                     .eval()\n@@ -4248,7 +4165,6 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa\n                         tmpdirname,\n                         torch_dtype=torch.float16,\n                         attn_implementation=\"flash_attention_2\",\n-                        low_cpu_mem_usage=True,\n                     )\n                     .to(torch_device)\n                     .eval()"
        },
        {
            "sha": "92a38baf941918539d2dea8b106ac340e22a2404",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 63,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4b8ec667e9919350982e47838bae9f78c7f988a8/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=4b8ec667e9919350982e47838bae9f78c7f988a8",
            "patch": "@@ -64,7 +64,6 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_multi_accelerator,\n-    require_usr_bin_time,\n     slow,\n     torch_device,\n )\n@@ -1003,57 +1002,6 @@ def test_checkpoint_variant_save_load_bin(self):\n \n         self.assertIsNotNone(model)\n \n-    @require_accelerate\n-    @mark.accelerate_tests\n-    def test_from_pretrained_low_cpu_mem_usage_functional(self):\n-        # test that we can use `from_pretrained(..., low_cpu_mem_usage=True)` with normal and\n-        # sharded models\n-\n-        mnames = [\n-            \"hf-internal-testing/tiny-random-bert-sharded\",\n-            \"hf-internal-testing/tiny-random-bert\",\n-        ]\n-        for mname in mnames:\n-            _ = BertModel.from_pretrained(mname, low_cpu_mem_usage=True)\n-\n-    @slow\n-    @require_usr_bin_time\n-    @require_accelerate\n-    @mark.accelerate_tests\n-    def test_from_pretrained_low_cpu_mem_usage_equal(self):\n-        # Before this would test that `from_pretrained(..., low_cpu_mem_usage=True)` uses less cpu memory than default\n-        # Now though these should be around the same.\n-        # TODO: Look for good bounds to check that their timings are near the same\n-\n-        mname = \"HuggingFaceTB/SmolLM-135M\"\n-\n-        preamble = \"from transformers import AutoModel\"\n-        one_liner_str = f'{preamble}; AutoModel.from_pretrained(\"{mname}\", low_cpu_mem_usage=False)'\n-        # Save this output as `max_rss_normal` if testing memory results\n-        max_rss_normal = self.python_one_liner_max_rss(one_liner_str)\n-\n-        one_liner_str = f'{preamble};  AutoModel.from_pretrained(\"{mname}\", low_cpu_mem_usage=True)'\n-        # Save this output as `max_rss_low_mem` if testing memory results\n-        max_rss_low_mem = self.python_one_liner_max_rss(one_liner_str)\n-\n-        # Should be within 5MBs of each other (overhead)\n-        self.assertAlmostEqual(\n-            max_rss_normal / 1024 / 1024,\n-            max_rss_low_mem / 1024 / 1024,\n-            delta=5,\n-            msg=\"using `low_cpu_mem_usage` should incur the same memory usage in both cases.\",\n-        )\n-\n-        # if you want to compare things manually, let's first look at the size of the model in bytes\n-        # model = AutoModel.from_pretrained(mname, low_cpu_mem_usage=False)\n-        # total_numel = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())\n-        # total_bytes = total_numel * 4\n-        # Now the diff_bytes should be very close to total_bytes, but the reports are inconsistent.\n-        # The easiest way to test this is to switch the model and torch.load to do all the work on\n-        # gpu - that way one can measure exactly the total and peak memory used. Perhaps once we add\n-        # functionality to load models directly on gpu, this test can be rewritten to use torch's\n-        # cuda memory tracking and then we should be able to do a much more precise test.\n-\n     @require_accelerate\n     @mark.accelerate_tests\n     @require_torch_multi_accelerator\n@@ -1537,7 +1485,6 @@ def test_pretrained_low_mem_new_config(self):\n                 config=model_config,\n                 ignore_mismatched_sizes=True,\n                 torch_dtype=torch.float16,\n-                low_cpu_mem_usage=True,\n             )\n             model_ref = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id)\n \n@@ -1782,16 +1729,6 @@ def test_load_model_with_state_dict_only(self):\n         )\n         self.assertTrue(check_models_equal(model, model_loaded))\n \n-    def test_load_model_with_state_dict_only_low_cpu_mem_usage(self):\n-        model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n-        state_dict = model.state_dict()\n-        config = model.config\n-\n-        model_loaded = BertModel.from_pretrained(\n-            pretrained_model_name_or_path=None, config=config, state_dict=state_dict, low_cpu_mem_usage=True\n-        )\n-        self.assertTrue(check_models_equal(model, model_loaded))\n-\n     def test_cache_when_needed_at_train_time(self):\n         \"\"\"\n         Some fine-tuning methods require the use of cache, like prefix tuning in PEFT. This test checks that a cache"
        }
    ],
    "stats": {
        "total": 698,
        "additions": 100,
        "deletions": 598
    }
}