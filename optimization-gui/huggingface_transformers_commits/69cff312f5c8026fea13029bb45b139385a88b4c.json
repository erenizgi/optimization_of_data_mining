{
    "author": "geetu040",
    "message": "Add support for DeepseekAI's DeepseekVL (#36248)\n\n* upload initial code\n\n* update deepseek-vl adaptor\n\n* update hierarchy of vision model classes\n\n* udpate aligner model\n\n* add text model\n\n* Added Image Processor\n\n* Added Image Processor\n\n* Added Image Processor\n\n* apply masks\n\n* remove projection; add aligner\n\n* remove interpolate_pos_encoding\n\n* remove unused params in config\n\n* cleaning\n\n* Add the __init__ file\n\n* added processing deepseek_vl class\n\n* modified the deepseek-vl processor\n\n* modified the deepseek-vl processor\n\n* update __init__\n\n* Update the image processor class name\n\n* Added Deepseek to src/transformers/__init__.py file\n\n* Added Deepseek to image_processing_auto.py\n\n* update the __init__ file\n\n* update deepseek_vl image processor\n\n* Update Deepseek Processor\n\n* upload fast image processor\n\n* Revert \"upload fast image processor\"\n\nThis reverts commit 68c8fd50bafbb9770ac70c9de02448e2519219b4.\n\n* update image processor\n\n* flatten heirarchy\n\n* remove DeepseekVLModel\n\n* major update (complete modeling)\n\n* auto modeling and other files\n\n* formatting\n\n* fix quality\n\n* replace torchvision in modeling\n\n* set default do_normalize to False\n\n* add fast image processor template using tool\n\n* update image processors\n\n* add fast image processor to other files\n\n* update liscense\n\n* Added deepseek image testcases\n\n* update image test\n\n* update processor\n\n* write CHAT_TEMPLATE\n\n* update model for processor\n\n* fix processor\n\n* minor fixes and formatting\n\n* fix image processing and tests\n\n* fix interpolation in sam\n\n* fix output_attentions in DeepseekVLModel\n\n* upload test_modeling\n\n* fix tests because of vocab size\n\n* set use_high_res_vision=False in tests\n\n* fix all modeling tests\n\n* fix styling\n\n* remove explicit background_color from image processors\n\n* added test_processor\n\n* added test_processor\n\n* fix processor tests\n\n* update docs\n\n* update docs\n\n* update docs\n\n* update conversion script\n\n* Fixed typos\n\n* minor fixes from review\n\n- remove model_id comments in examples\n- remove from pre-trained auto mapping\n- move to image-text-to-text from vision-to-seq in auto mapping\n- add image_token_index to __init__ for config\n- remove outdated temporary config in conversion script\n- update example to use chat_template in docstring example\n- update liscense 2021->2025\n\n* fix type in config docstring\n\nCo-authored-by: Raushan Turganbay <raushan.turganbay@alumni.nu.edu.kz>\n\n* update get_image_features\n\n* fix config\n\n* improve DeepseekVLImageProcessor.preprocess\n\n* return image_hidden_states\n\n* use AutoTokenizer and AutoImageProcessor in Processor\n\n* fix model outputs\n\n* make num_image_tokens configurable\n\n* fix docstring of processor\n\n* move system prompt to chat template\n\n* fix repo consistency\n\n* fix return_dict\n\n* replace SamVisionEncoder with SamVisionModel\n\n* update to remove deepcopy\n\n* üõ†Ô∏è  Major Architectural Changes (Adds DeepseekVLHybrid)\n\n* fix quality checks\n\n* add missing hybrid in auto modeling\n\n* run make style\n\n* update sam_hq\n\n* update high_res_size in test\n\n* update docs following #36979\n\n* update code with auto_docstring\n\n* update conversion scripts\n\n* fix style\n\n* fix failing test because of tuple\n\n* set weights_only=True in conversion script\n\n* use safetensors.torch.load_file instead of torch.load in conversion script\n\n* make output_dir optional in conversion script\n\n* fix code snippets in docs (now the examples work fine)\n\n* integration tests for DeepseekVL\n\n* update expected texts\n\n* make style\n\n* integration tests for DeepseekVLHybrid\n\n* fix class name\n\n* update expected texts for hybrid\n\n* run \"make style\"\n\n* update since changes in main\n\n* run make-style\n\n* nits since changes in main\n\n* undo changes in sam\n\n* fix tests\n\n* fix tests; update with main\n\n* update with main: output_attention/output_hidden_states\n\n* fix copied part in deepseek_vl\n\n* run fix-copies\n\n* fix output_hidden_states\n\n* sam: fix _init_weigths\n\n* use modular for DeepseekVL\n\n* make image processor more modular\n\n* modular: use JanusPreTrainedModel\n\n* janus: provide kwargs in loss\n\n* update processors in conversion script\n\n* Revert \"sam: fix _init_weigths\"\n\nThis reverts commit db625d0c68956c0dad45edd7a469b6a074905c27.\n\n* run fix-copies\n\n---------\n\nCo-authored-by: Shakib-IO <shakib.khan17@northsouth.edu>\nCo-authored-by: Raushan Turganbay <raushan.turganbay@alumni.nu.edu.kz>",
    "sha": "69cff312f5c8026fea13029bb45b139385a88b4c",
    "files": [
        {
            "sha": "a7c79b002b701a95c20cddedb31af9d3ccfcf8c2",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -725,6 +725,10 @@\n         title: DAB-DETR\n       - local: model_doc/deepseek_v2\n         title: DeepSeek-V2\n+      - local: model_doc/deepseek_vl\n+        title: DeepseekVL\n+      - local: model_doc/deepseek_vl_hybrid\n+        title: DeepseekVLHybrid\n       - local: model_doc/deformable_detr\n         title: Deformable DETR\n       - local: model_doc/deit"
        },
        {
            "sha": "625a2c90b01fa0f5cc9e768f83ecad26f3fa7bf3",
            "filename": "docs/source/en/model_doc/deepseek_vl.md",
            "status": "added",
            "additions": 220,
            "deletions": 0,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,220 @@\n+<!--Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# DeepseekVL\n+\n+[Deepseek-VL](https://arxiv.org/abs/2403.05525) was introduced by the DeepSeek AI team. It is a vision-language model (VLM) designed to process both text and images for generating contextually relevant responses. The model leverages [LLaMA](./llama) as its text encoder, while [SigLip](./siglip) is used for encoding images.\n+\n+You can find all the original Deepseek-VL checkpoints under the [DeepSeek-community](https://huggingface.co/deepseek-community) organization.\n+\n+> [!TIP]\n+> Click on the Deepseek-VL models in the right sidebar for more examples of how to apply Deepseek-VL to different vision and language tasks.\n+\n+The example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipe = pipeline(\n+    task=\"image-text-to-text\",\n+    model=\"deepseek-community/deepseek-vl-1.3b-chat\",\n+    device=0,\n+    torch_dtype=torch.float16\n+)\n+\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"image\",\n+                \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+            },\n+            { \"type\": \"text\", \"text\": \"Describe this image.\"},\n+        ]\n+    }\n+]\n+\n+pipe(text=messages, max_new_tokens=20, return_full_text=False)\n+```\n+</hfoption>\n+\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import DeepseekVLForConditionalGeneration, AutoProcessor\n+\n+model = DeepseekVLForConditionalGeneration.from_pretrained(\n+    \"deepseek-community/deepseek-vl-1.3b-chat\",\n+    torch_dtype=torch.float16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+\n+processor = AutoProcessor.from_pretrained(\"deepseek-community/deepseek-vl-1.3b-chat\")\n+\n+messages = [\n+    {\n+        \"role\":\"user\",\n+        \"content\":[\n+            {\n+                \"type\":\"image\",\n+                \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+            },\n+            {\n+                \"type\":\"text\",\n+                \"text\":\"Describe this image.\"\n+            }\n+        ]\n+    }\n+\n+]\n+\n+inputs = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device, dtype=model.dtype)\n+\n+generated_ids = model.generate(**inputs, max_new_tokens=128)\n+generated_ids_trimmed = [\n+    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n+]\n+output_text = processor.batch_decode(\n+    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)\n+\n+print(output_text)\n+```\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [torchao](../quantization/torchao) to only quantize the weights to int4.\n+\n+```python\n+import torch\n+from transformers import TorchAoConfig, DeepseekVLForConditionalGeneration, AutoProcessor\n+\n+quantization_config = TorchAoConfig(\n+    \"int4_weight_only\",\n+    group_size=128\n+)\n+\n+model = DeepseekVLForConditionalGeneration.from_pretrained(\n+    \"deepseek-community/deepseek-vl-1.3b-chat\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+```\n+### Notes\n+\n+- Do inference with multiple images in a single conversation.\n+    ```py\n+    import torch\n+    from transformers import DeepseekVLForConditionalGeneration, AutoProcessor\n+\n+    model = DeepseekVLForConditionalGeneration.from_pretrained(\n+        \"deepseek-community/deepseek-vl-1.3b-chat\",\n+        torch_dtype=torch.float16,\n+        device_map=\"auto\",\n+        attn_implementation=\"sdpa\"\n+    )\n+\n+    processor = AutoProcessor.from_pretrained(\"deepseek-community/deepseek-vl-1.3b-chat\")\n+\n+    messages = [\n+        [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"What‚Äôs the difference between\"},\n+                    {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+                    {\"type\": \"text\", \"text\": \" and \"},\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+                ]\n+            }\n+        ],\n+        [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"What do you see in this image?\"}\n+                ]\n+            }\n+        ]\n+    ]\n+\n+    inputs = processor.apply_chat_template(\n+        messages,\n+        add_generation_prompt=True,\n+        padding=True,\n+        truncation=True,\n+        tokenize=True,\n+        return_dict=True,\n+        return_tensors=\"pt\"\n+    ).to(model.device, dtype=model.dtype)\n+\n+    generated_ids = model.generate(**inputs, max_new_tokens=128)\n+    generated_ids_trimmed = [\n+        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n+    ]\n+    output_text = processor.batch_decode(\n+        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+    )\n+\n+    print(output_text)\n+    ```\n+\n+## DeepseekVLConfig\n+\n+[[autodoc]] DeepseekVLConfig\n+\n+## DeepseekVLProcessor\n+\n+[[autodoc]] DeepseekVLProcessor\n+\n+## DeepseekVLImageProcessor\n+\n+[[autodoc]] DeepseekVLImageProcessor\n+\n+## DeepseekVLModel\n+\n+[[autodoc]] DeepseekVLModel\n+    - forward\n+\n+## DeepseekVLForConditionalGeneration\n+\n+[[autodoc]] DeepseekVLForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "86e1672bce59d237554919e439e9589702d7ca25",
            "filename": "docs/source/en/model_doc/deepseek_vl_hybrid.md",
            "status": "added",
            "additions": 219,
            "deletions": 0,
            "changes": 219,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,219 @@\n+<!--Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# DeepseekVLHybrid\n+\n+[Deepseek-VL-Hybrid](https://arxiv.org/abs/2403.05525) was introduced by the DeepSeek AI team. It is a vision-language model (VLM) designed to process both text and images for generating contextually relevant responses. The model leverages [LLaMA](./llama) as its text encoder, while [SigLip](./siglip) is used for encoding low-resolution images and [SAM (Segment Anything Model)](./sam) is incorporated to handle high-resolution image encoding, enhancing the model‚Äôs ability to process fine-grained visual details. Deepseek-VL-Hybrid is a variant of Deepseek-VL that uses [SAM (Segment Anything Model)](./sam) to handle high-resolution image encoding.\n+\n+You can find all the original Deepseek-VL-Hybrid checkpoints under the [DeepSeek-community](https://huggingface.co/deepseek-community) organization.\n+\n+> [!TIP]\n+> Click on the Deepseek-VL-Hybrid models in the right sidebar for more examples of how to apply Deepseek-VL-Hybrid to different vision and language tasks.\n+\n+The example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipe = pipeline(\n+    task=\"image-text-to-text\",\n+    model=\"deepseek-community/deepseek-vl-7b-chat\",\n+    device=0,\n+    torch_dtype=torch.float16\n+)\n+\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"image\",\n+                \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+            },\n+            { \"type\": \"text\", \"text\": \"Describe this image.\"},\n+        ]\n+    }\n+]\n+\n+pipe(text=messages, max_new_tokens=20, return_full_text=False)\n+```\n+</hfoption>\n+\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import DeepseekVLHybridForConditionalGeneration, AutoProcessor\n+\n+model = DeepseekVLHybridForConditionalGeneration.from_pretrained(\n+    \"deepseek-community/deepseek-vl-7b-chat\",\n+    torch_dtype=torch.float16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+\n+processor = AutoProcessor.from_pretrained(\"deepseek-community/deepseek-vl-7b-chat\")\n+\n+messages = [\n+    {\n+        \"role\":\"user\",\n+        \"content\":[\n+            {\n+                \"type\":\"image\",\n+                \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+            },\n+            {\n+                \"type\":\"text\",\n+                \"text\":\"Describe this image.\"\n+            }\n+        ]\n+    }\n+\n+]\n+\n+inputs = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device, dtype=model.dtype)\n+\n+generated_ids = model.generate(**inputs, max_new_tokens=128)\n+generated_ids_trimmed = [\n+    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n+]\n+output_text = processor.batch_decode(\n+    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)\n+\n+print(output_text)\n+```\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [torchao](../quantization/torchao) to only quantize the weights to int4.\n+\n+```python\n+import torch\n+from transformers import TorchAoConfig, DeepseekVLHybridForConditionalGeneration, AutoProcessor\n+\n+quantization_config = TorchAoConfig(\n+    \"int4_weight_only\",\n+    group_size=128\n+)\n+\n+model = DeepseekVLHybridForConditionalGeneration.from_pretrained(\n+    \"deepseek-community/deepseek-vl-7b-chat\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+```\n+### Notes\n+\n+- Do inference with multiple images in a single conversation.\n+    ```py\n+    import torch\n+    from transformers import DeepseekVLHybridForConditionalGeneration, AutoProcessor\n+\n+    model = DeepseekVLHybridForConditionalGeneration.from_pretrained(\n+        \"deepseek-community/deepseek-vl-7b-chat\",\n+        torch_dtype=torch.float16,\n+        device_map=\"auto\",\n+        attn_implementation=\"sdpa\"\n+    )\n+\n+    processor = AutoProcessor.from_pretrained(\"deepseek-community/deepseek-vl-7b-chat\")\n+\n+    messages = [\n+        [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"What‚Äôs the difference between\"},\n+                    {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+                    {\"type\": \"text\", \"text\": \" and \"},\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+                ]\n+            }\n+        ],\n+        [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"What do you see in this image?\"}\n+                ]\n+            }\n+        ]\n+    ]\n+\n+    inputs = processor.apply_chat_template(\n+        messages,\n+        add_generation_prompt=True,\n+        padding=True,\n+        truncation=True,\n+        tokenize=True,\n+        return_dict=True,\n+        return_tensors=\"pt\"\n+    ).to(model.device, dtype=model.dtype)\n+\n+    generated_ids = model.generate(**inputs, max_new_tokens=128)\n+    generated_ids_trimmed = [\n+        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n+    ]\n+    output_text = processor.batch_decode(\n+        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+    )\n+\n+    print(output_text)\n+    ```\n+\n+## DeepseekVLHybridConfig\n+\n+[[autodoc]] DeepseekVLHybridConfig\n+\n+## DeepseekVLHybridProcessor\n+\n+[[autodoc]] DeepseekVLHybridProcessor\n+\n+## DeepseekVLHybridImageProcessor\n+\n+[[autodoc]] DeepseekVLHybridImageProcessor\n+\n+## DeepseekVLHybridModel\n+\n+[[autodoc]] DeepseekVLHybridModel\n+    - forward\n+\n+## DeepseekVLHybridForConditionalGeneration\n+\n+[[autodoc]] DeepseekVLHybridForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "b691cea1128e255a9d16901771cb0a88eaad7717",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -84,6 +84,8 @@\n     from .decision_transformer import *\n     from .deepseek_v2 import *\n     from .deepseek_v3 import *\n+    from .deepseek_vl import *\n+    from .deepseek_vl_hybrid import *\n     from .deformable_detr import *\n     from .deit import *\n     from .deprecated import *"
        },
        {
            "sha": "eb25e0d0251510aa6dcca50bdfc0540722bdde64",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -103,6 +103,8 @@\n         (\"decision_transformer\", \"DecisionTransformerConfig\"),\n         (\"deepseek_v2\", \"DeepseekV2Config\"),\n         (\"deepseek_v3\", \"DeepseekV3Config\"),\n+        (\"deepseek_vl\", \"DeepseekVLConfig\"),\n+        (\"deepseek_vl_hybrid\", \"DeepseekVLHybridConfig\"),\n         (\"deformable_detr\", \"DeformableDetrConfig\"),\n         (\"deit\", \"DeiTConfig\"),\n         (\"depth_anything\", \"DepthAnythingConfig\"),\n@@ -495,6 +497,8 @@\n         (\"decision_transformer\", \"Decision Transformer\"),\n         (\"deepseek_v2\", \"DeepSeek-V2\"),\n         (\"deepseek_v3\", \"DeepSeek-V3\"),\n+        (\"deepseek_vl\", \"DeepseekVL\"),\n+        (\"deepseek_vl_hybrid\", \"DeepseekVLHybrid\"),\n         (\"deformable_detr\", \"Deformable DETR\"),\n         (\"deit\", \"DeiT\"),\n         (\"deplot\", \"DePlot\"),"
        },
        {
            "sha": "0a0cc6a38ca4218b3e5ca49cefa52e290a15c80a",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -77,6 +77,8 @@\n             (\"convnextv2\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"cvt\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"data2vec-vision\", (\"BeitImageProcessor\", \"BeitImageProcessorFast\")),\n+            (\"deepseek_vl\", (\"DeepseekVLImageProcessor\")),\n+            (\"deepseek_vl_hybrid\", (\"DeepseekVLHybridImageProcessor\")),\n             (\"deformable_detr\", (\"DeformableDetrImageProcessor\", \"DeformableDetrImageProcessorFast\")),\n             (\"deit\", (\"DeiTImageProcessor\", \"DeiTImageProcessorFast\")),\n             (\"depth_anything\", (\"DPTImageProcessor\", \"DPTImageProcessorFast\")),"
        },
        {
            "sha": "85eb8ff6bbbbea251f62f2497b3a1347884ab0d6",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -97,6 +97,8 @@\n         (\"decision_transformer\", \"DecisionTransformerModel\"),\n         (\"deepseek_v2\", \"DeepseekV2Model\"),\n         (\"deepseek_v3\", \"DeepseekV3Model\"),\n+        (\"deepseek_vl\", \"DeepseekVLModel\"),\n+        (\"deepseek_vl_hybrid\", \"DeepseekVLHybridModel\"),\n         (\"deformable_detr\", \"DeformableDetrModel\"),\n         (\"deit\", \"DeiTModel\"),\n         (\"depth_pro\", \"DepthProModel\"),\n@@ -935,6 +937,8 @@\n         (\"blip\", \"BlipForConditionalGeneration\"),\n         (\"blip-2\", \"Blip2ForConditionalGeneration\"),\n         (\"chameleon\", \"ChameleonForConditionalGeneration\"),\n+        (\"deepseek_vl\", \"DeepseekVLForConditionalGeneration\"),\n+        (\"deepseek_vl_hybrid\", \"DeepseekVLHybridForConditionalGeneration\"),\n         (\"emu3\", \"Emu3ForConditionalGeneration\"),\n         (\"evolla\", \"EvollaForProteinText2Text\"),\n         (\"fuyu\", \"FuyuForCausalLM\"),"
        },
        {
            "sha": "cc2be544f46ba745517eae5f8cc96a69a8538c64",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -62,6 +62,8 @@\n         (\"clvp\", \"ClvpProcessor\"),\n         (\"colpali\", \"ColPaliProcessor\"),\n         (\"colqwen2\", \"ColQwen2Processor\"),\n+        (\"deepseek_vl\", \"DeepseekVLProcessor\"),\n+        (\"deepseek_vl_hybrid\", \"DeepseekVLHybridProcessor\"),\n         (\"dia\", \"DiaProcessor\"),\n         (\"emu3\", \"Emu3Processor\"),\n         (\"evolla\", \"EvollaProcessor\"),"
        },
        {
            "sha": "6e5b07dddfec70191e8b10883fe48ef601195c8f",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -193,6 +193,20 @@\n                 \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n             ),\n         ),\n+        (\n+            \"deepseek_vl\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"deepseek_vl_hybrid\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n         (\"dia\", (\"DiaTokenizer\", None)),\n         (\n             \"diffllama\","
        },
        {
            "sha": "2422b31e31050b378f7373dd6ed73830ee1b9f0d",
            "filename": "src/transformers/models/deepseek_vl/__init__.py",
            "status": "added",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2F__init__.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,30 @@\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_deepseek_vl import *\n+    from .image_processing_deepseek_vl import *\n+    from .image_processing_deepseek_vl_fast import *\n+    from .modeling_deepseek_vl import *\n+    from .processing_deepseek_vl import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "af99ac9eeb3f6f73cc3712f71c9a3b8e664e832f",
            "filename": "src/transformers/models/deepseek_vl/configuration_deepseek_vl.py",
            "status": "added",
            "additions": 96,
            "deletions": 0,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,96 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/deepseek_vl/modular_deepseek_vl.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_deepseek_vl.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DeepseekVLConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DeepseekVLModel`]. It is used to instantiate a\n+    DeepseekVL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the DeepseekVL\n+    [deepseek-community/deepseek-vl-1.3b-chat](https://huggingface.co/deepseek-community/deepseek-vl-1.3b-chat) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SiglipVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        image_token_id (`int`, *optional*, defaults to 100015):\n+            The index representing image tokens in the model's token vocabulary.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import DeepseekVLConfig, DeepseekVLModel\n+\n+    >>> # Initializing a DeepseekVL deepseek-community/deepseek-vl-1.3b-chat style configuration\n+    >>> configuration = DeepseekVLConfig()\n+\n+    >>> # Initializing a model (with random weights) from the deepseek-community/deepseek-vl-1.3b-chat style configuration\n+    >>> model = DeepseekVLModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"deepseek_vl\"\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        text_config: AutoConfig = None,\n+        vision_config: AutoConfig = None,\n+        image_token_id: int = 100015,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        if text_config is None:\n+            text_config = {}\n+            logger.info(\"`text_config` is `None`. Initializing the `LlamaConfig` with default values.\")\n+\n+        if vision_config is None:\n+            vision_config = {}\n+            logger.info(\"`vision_config` is `None`. Initializing the `SiglipVisionConfig` with default values.\")\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"siglip_vision_model\")\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.image_token_id = image_token_id\n+\n+\n+__all__ = [\"DeepseekVLConfig\"]"
        },
        {
            "sha": "3e9b6a37fe09bca376e64d062a232f83ad870ef4",
            "filename": "src/transformers/models/deepseek_vl/convert_deepseek_vl_weights_to_hf.py",
            "status": "added",
            "additions": 356,
            "deletions": 0,
            "changes": 356,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconvert_deepseek_vl_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconvert_deepseek_vl_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconvert_deepseek_vl_weights_to_hf.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,356 @@\n+# coding=utf-8\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import gc\n+import json\n+import os\n+from typing import Optional\n+\n+import regex as re\n+import torch\n+from accelerate import init_empty_weights\n+from huggingface_hub import snapshot_download\n+from huggingface_hub.errors import HFValidationError\n+from safetensors.torch import load_file\n+\n+from transformers import (\n+    AutoTokenizer,\n+    DeepseekVLConfig,\n+    DeepseekVLForConditionalGeneration,\n+    DeepseekVLImageProcessor,\n+    DeepseekVLProcessor,\n+)\n+from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n+\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    # Siglip (Low Resolution)\n+    r\"vision_model.vision_tower.pos_embed\":                                  r\"model.vision_model.vision_model.embeddings.position_embedding.weight\",\n+    r\"vision_model.vision_tower.patch_embed.proj.(weight|bias)\":             r\"model.vision_model.vision_model.embeddings.patch_embedding.\\1\",\n+    r\"vision_model.vision_tower.blocks.(\\d+).attn.qkv.(weight|bias)\":        r\"model.vision_model.vision_model.encoder.layers.\\1.self_attn.(q|k|v)_proj.\\2\",\n+    r\"vision_model.vision_tower.blocks.(\\d+).attn.proj.(weight|bias)\":       r\"model.vision_model.vision_model.encoder.layers.\\1.self_attn.out_proj.\\2\",\n+    r\"vision_model.vision_tower.blocks.(\\d+).norm(\\d+).(weight|bias)\":       r\"model.vision_model.vision_model.encoder.layers.\\1.layer_norm\\2.\\3\",\n+    r\"vision_model.vision_tower.blocks.(\\d+).mlp.fc(\\d+).(weight|bias)\":     r\"model.vision_model.vision_model.encoder.layers.\\1.mlp.fc\\2.\\3\",\n+    r\"vision_model.vision_tower.norm.(weight|bias)\":                         r\"model.vision_model.vision_model.post_layernorm.\\1\",\n+    r\"vision_model.vision_tower.attn_pool.latent\":                           r\"model.vision_model.vision_model.head.probe\",\n+    r\"vision_model.vision_tower.attn_pool.proj.(weight|bias)\":               r\"model.vision_model.vision_model.head.attention.out_proj.\\1\",\n+    r\"vision_model.vision_tower.attn_pool.norm.(weight|bias)\":               r\"model.vision_model.vision_model.head.layernorm.\\1\",\n+    r\"vision_model.vision_tower.attn_pool.mlp.fc(\\d+).(weight|bias)\":        r\"model.vision_model.vision_model.head.mlp.fc\\1.\\2\",\n+\n+    # Aligner\n+    r\"aligner.layers.0.(weight|bias)\":               r\"model.aligner.linear1.\\1\",\n+    r\"aligner.layers.2.(weight|bias)\":               r\"model.aligner.linear2.\\1\",\n+\n+    # Llama (Text Model)\n+    r\"language_model.model.(\\w+)\":                   r\"model.language_model.\\1\",\n+    r\"language_model.lm_head.(weight|bias)\":         r\"lm_head.\\1\",\n+}\n+# fmt: on\n+\n+# Adopted from https://github.com/deepseek-ai/DeepSeek-VL/blob/main/deepseek_vl/utils/conversation.py#L80-L91\n+CHAT_TEMPLATE = (\n+    # Define separators and initialize counter\n+    \"{% set seps = ['\\n\\n', '<\\uff5cend\\u2581of\\u2581sentence\\uff5c>'] %}\"\n+    \"{% set i = 0 %}\"\n+    # Start with default system prompt\n+    \"You are a helpful language and vision assistant. \"\n+    \"You are able to understand the visual content that the user provides, \"\n+    \"and assist the user with a variety of tasks using natural language.\\n\\n\"\n+    # Iterate through messages\n+    \"{% for message in messages %}\"\n+    # Identify user or assistant role\n+    \"{% if message['role']|lower == 'user' %}\"\n+    \"User: \"\n+    \"{% elif message['role']|lower == 'assistant' %}\"\n+    \"Assistant:{% if not (loop.last and not add_generation_prompt and message['content'][0]['type']=='text' and message['content'][0]['text']=='') %} {% endif %}\"\n+    \"{% else %}\"\n+    \"{{ message['role'].capitalize() }}: \"\n+    \"{% endif %}\"\n+    # Iterate through message content (text/images)\n+    \"{% for content in message['content'] %}\"\n+    # If content is an image, replace with placeholder\n+    \"{% if content['type'] == 'image' %}\"\n+    \"<image_placeholder>\"\n+    # If content is text, handle formatting\n+    \"{% elif content['type'] == 'text' %}\"\n+    \"{% set text = content['text'] %}\"\n+    # Strip whitespace for first and last text blocks\n+    \"{% if loop.first %}{% set text = text.lstrip() %}{% endif %}\"\n+    \"{% if loop.last %}{% set text = text.rstrip() %}{% endif %}\"\n+    # If previous content was text, add space\n+    \"{% if not loop.first and message['content'][loop.index0-1]['type'] == 'text' %}\"\n+    \"{{ ' ' + text }}\"\n+    \"{% else %}\"\n+    \"{{ text }}\"\n+    \"{% endif %}\"\n+    \"{% endif %}\"\n+    \"{% endfor %}\"  # End message content loop\n+    # Add separators between messages\n+    \"{% if not loop.last or add_generation_prompt %}\"\n+    \"{% if message['role']|lower == 'user' %}\"\n+    \"{{ seps[0] }}\"\n+    \"{% else %}\"\n+    \"{{ seps[1] }}\"\n+    \"{% endif %}\"\n+    \"{% endif %}\"\n+    \"{% endfor %}\"  # End messages loop\n+    # Add final Assistant prompt if required\n+    \"{% if add_generation_prompt %}Assistant:{% endif %}\"\n+)\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: dict):\n+    output_dict = {}\n+\n+    old_text = \"\\n\".join(state_dict_keys)\n+    new_text = old_text\n+    for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+        if replacement is None:\n+            new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+            continue\n+        new_text = re.sub(pattern, replacement, new_text)\n+    output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+\n+    return output_dict\n+\n+\n+def get_qkv_state_dict(key, parameter):\n+    \"\"\"\n+    new key which looks like this\n+    xxxx.(q|k|v).xxx    (m, n)\n+\n+    is converted to\n+    xxxx.q.xxxx         (m//3, n)\n+    xxxx.k.xxxx         (m//3, n)\n+    xxxx.v.xxxx         (m//3, n)\n+    \"\"\"\n+    qkv_state_dict = {}\n+    placeholder = re.search(r\"(\\(.*?\\))\", key).group(1)  # finds   \"(query|key|value)\"\n+    replacements_keys = placeholder[1:-1].split(\"|\")  # creates ['query', 'key', 'value']\n+    replacements_vals = torch.split(\n+        parameter, split_size_or_sections=parameter.size(0) // len(replacements_keys), dim=0\n+    )\n+    for replacement_key, replacement_val in zip(replacements_keys, replacements_vals):\n+        qkv_state_dict[key.replace(placeholder, replacement_key)] = replacement_val\n+    return qkv_state_dict\n+\n+\n+def update_state_dict(old_state_dict):\n+    all_keys = list(old_state_dict.keys())\n+    new_keys = convert_old_keys_to_new_keys(all_keys)\n+\n+    state_dict = {}\n+    for key in all_keys:\n+        new_key = new_keys[key]\n+        current_parameter = old_state_dict.pop(key)\n+\n+        if \"qkv\" in key and \"vision_tower_high\" not in key:\n+            qkv_state_dict = get_qkv_state_dict(new_key, current_parameter)\n+            state_dict.update(qkv_state_dict)\n+        elif \"pos_embed\" in key:\n+            if \"vision_tower_high\" not in key:\n+                # timm implementation of siglip creates this param of size [1, 576, 1024]\n+                # transformers implementation of siglip creates this param of size [576, 1024]\n+                state_dict[new_key] = current_parameter.squeeze(0)\n+            else:\n+                state_dict[new_key] = current_parameter\n+        else:\n+            state_dict[new_key] = current_parameter\n+\n+    return state_dict\n+\n+\n+def load_model_state_dict(input_path: str) -> dict:\n+    \"\"\"\n+    Load model state dict, handling both single and sharded files.\n+    \"\"\"\n+    index_path = os.path.join(input_path, \"model.safetensors.index.json\")\n+    single_file_path = os.path.join(input_path, \"model.safetensors\")\n+\n+    # Check if we have a sharded model\n+    if os.path.exists(index_path):\n+        print(\"Loading sharded model...\")\n+        state_dict = {}\n+        with open(index_path, \"r\") as f:\n+            index = json.load(f)\n+\n+        # Get unique shard files and load each one only once\n+        unique_shard_files = sorted(set(index[\"weight_map\"].values()))\n+        for shard_file in unique_shard_files:\n+            print(f\"Loading shard {shard_file}...\")\n+            shard_path = os.path.join(input_path, shard_file)\n+            shard_dict = load_file(shard_path)\n+            state_dict.update(shard_dict)\n+\n+        return state_dict\n+\n+    # Single file model\n+    elif os.path.exists(single_file_path):\n+        print(\"Loading single file model...\")\n+        return load_file(single_file_path, device=\"cpu\")\n+\n+    else:\n+        raise ValueError(f\"No model files found in {input_path}\")\n+\n+\n+def convert_model(\n+    hf_repo_id: str,\n+    output_dir: Optional[str] = None,\n+    output_hub_path: Optional[str] = None,\n+    safe_serialization: bool = True,\n+):\n+    if output_dir:\n+        os.makedirs(output_dir, exist_ok=True)\n+\n+    try:\n+        input_path = snapshot_download(hf_repo_id)\n+    except HFValidationError:\n+        # If the input path is not a HF repo ID, assume it's a local path\n+        input_path = hf_repo_id\n+\n+    # ------------------------------------------------------------\n+    # Create and save config\n+    # ------------------------------------------------------------\n+\n+    config = DeepseekVLConfig(\n+        text_config={\n+            \"hidden_size\": 2048,\n+            \"intermediate_size\": 5632,\n+            \"max_position_embeddings\": 16384,\n+            \"num_attention_heads\": 16,\n+            \"num_hidden_layers\": 24,\n+            \"vocab_size\": 102400,\n+        },\n+        vision_config={\n+            \"hidden_size\": 1024,\n+            \"intermediate_size\": 4096,\n+            \"image_size\": 384,\n+            \"patch_size\": 16,\n+            \"hidden_act\": \"gelu\",\n+            \"vision_use_head\": False,\n+            \"num_attention_heads\": 16,\n+            \"num_hidden_layers\": 24,\n+        },\n+    )\n+\n+    # save config\n+    if output_dir:\n+        config.save_pretrained(output_dir)\n+        print(\"Model config saved successfully...\")\n+\n+    # ------------------------------------------------------------\n+    # Convert processor\n+    # ------------------------------------------------------------\n+\n+    image_processor = DeepseekVLImageProcessor(\n+        image_mean=IMAGENET_STANDARD_MEAN,\n+        image_std=IMAGENET_STANDARD_STD,\n+    )\n+\n+    tokenizer = AutoTokenizer.from_pretrained(\n+        input_path,\n+        extra_special_tokens={\n+            \"pad_token\": \"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\",\n+            \"image_token\": \"<image_placeholder>\",\n+        },\n+    )\n+\n+    processor = DeepseekVLProcessor(\n+        image_processor=image_processor,\n+        tokenizer=tokenizer,\n+        chat_template=CHAT_TEMPLATE,\n+    )\n+\n+    if output_dir:\n+        print(f\"Saving processor to {output_dir}...\")\n+        processor.save_pretrained(output_dir)\n+    if output_hub_path:\n+        print(f\"Pushing processor to hub at {output_hub_path}...\")\n+        processor.push_to_hub(output_hub_path)\n+\n+    # ------------------------------------------------------------\n+    # Convert weights\n+    # ------------------------------------------------------------\n+\n+    print(\"Creating empty model...\")\n+    with init_empty_weights():\n+        model = DeepseekVLForConditionalGeneration(config)\n+\n+    # Load and convert state dict\n+    print(\"Loading state dict...\")\n+    state_dict = load_model_state_dict(input_path)\n+    state_dict = update_state_dict(state_dict)\n+\n+    # Load converted state dict\n+    print(\"Loading converted weights into model...\")\n+    info = model.load_state_dict(state_dict, strict=False, assign=True)\n+    if len(info.missing_keys) > 0:\n+        raise ValueError(f\"Missing keys: {info.missing_keys}\")\n+\n+    # Tie weights before any device mapping\n+    print(\"Tying weights...\")\n+    model.tie_weights()\n+\n+    # Save the model\n+    if output_dir:\n+        print(f\"Saving model to {output_dir}...\")\n+        model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    if output_hub_path:\n+        print(f\"Pushing model to hub at {output_hub_path}...\")\n+        model.push_to_hub(output_hub_path, safe_serialization=safe_serialization)\n+\n+    del state_dict, model\n+    gc.collect()\n+\n+    # Validate the saved model if saved locally\n+    if output_dir:\n+        print(\"Reloading the local model to check if it's saved correctly...\")\n+        DeepseekVLForConditionalGeneration.from_pretrained(output_dir, device_map=\"auto\")\n+        print(\"Local model reloaded successfully.\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--hf_repo_id\",\n+        default=\"deepseek-ai/deepseek-vl-1.3b-chat\",\n+        help=\"Location of official weights from DeepseekAI on HF\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        default=None,\n+        help=\"Location to write the converted model and processor\",\n+    )\n+    parser.add_argument(\n+        \"--output_hub_path\",\n+        default=None,\n+        help=\"Repository ID to push model to hub (e.g. 'username/model-name')\",\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n+    )\n+    args = parser.parse_args()\n+\n+    convert_model(\n+        hf_repo_id=args.hf_repo_id,\n+        output_dir=args.output_dir,\n+        output_hub_path=args.output_hub_path,\n+        safe_serialization=args.safe_serialization,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "fad24220ef87a69390203665e2d07c870df68dda",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl.py",
            "status": "added",
            "additions": 414,
            "deletions": 0,
            "changes": 414,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,414 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/deepseek_vl/modular_deepseek_vl.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_deepseek_vl.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n+from ...image_transforms import convert_to_rgb, resize, to_channel_dimension_format\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    make_flat_list_of_images,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import (\n+    TensorType,\n+    filter_out_non_signature_kwargs,\n+    is_vision_available,\n+    logging,\n+)\n+\n+\n+if is_vision_available():\n+    import PIL\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DeepseekVLImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a DEEPSEEK_VL image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n+            `do_resize` parameter in the `preprocess` method.\n+        size (`dict`, *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n+            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n+            method.\n+        min_size (`int`, *optional*, defaults to 14):\n+            The minimum allowed size for the resized image. Ensures that neither the height nor width\n+            falls below this value after resizing.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n+            overridden by the `resample` parameter in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n+            `do_rescale` parameter in the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Only has an effect if `do_rescale` is set to `True`. Can be\n+            overridden by the `rescale_factor` parameter in the `preprocess` method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n+            method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n+            overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Optional[dict[str, int]] = None,\n+        min_size: int = 14,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"height\": 384, \"width\": 384}\n+        size = get_size_dict(size, default_to_square=True)\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n+        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n+        self.do_convert_rgb = do_convert_rgb\n+\n+        self.min_size = min_size\n+        if image_mean is None:\n+            self.background_color = (127, 127, 127)\n+        else:\n+            self.background_color = tuple([int(x * 255) for x in image_mean])\n+\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: Union[dict[str, int], int],\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Resize an image to dynamically calculated size.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n+                `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n+            data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the output image. If unset, the channel dimension format of the input\n+                image is used. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `None`: will be inferred from input\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+\n+        Returns:\n+            `np.ndarray`: The resized image.\n+        \"\"\"\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(image)\n+\n+        height, width = get_image_size(image, input_data_format)\n+        max_size = max(height, width)\n+\n+        size = get_size_dict(size, default_to_square=True)\n+        if size[\"height\"] != size[\"width\"]:\n+            raise ValueError(\n+                f\"Output height and width must be the same. Got height={size['height']} and width={size['width']}\"\n+            )\n+        size = size[\"height\"]\n+\n+        delta = size / max_size\n+        # Largest side becomes `size` and the other side is scaled according to the aspect ratio.\n+        output_size_nonpadded = [\n+            max(int(height * delta), self.min_size),\n+            max(int(width * delta), self.min_size),\n+        ]\n+\n+        image = resize(\n+            image,\n+            size=output_size_nonpadded,\n+            resample=resample,\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+            **kwargs,\n+        )\n+        # Expand and pad the images to obtain a square image of dimensions `size x size`\n+        image = self.pad_to_square(\n+            image=image,\n+            background_color=self.background_color,\n+            input_data_format=input_data_format,\n+        )\n+        return image\n+\n+    @filter_out_non_signature_kwargs()\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        data_format: ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> PIL.Image.Image:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n+                Controls the size of the image after `resize`. The shortest edge of the image is resized to\n+                `size[\"shortest_edge\"]` whilst preserving the aspect ratio. If the longest edge of this resized image\n+                is > `int(size[\"shortest_edge\"] * (1333 / 800))`, then the image is resized again to make the longest\n+                edge equal to `int(size[\"shortest_edge\"] * (1333 / 800))`.\n+            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image values between [0 - 1].\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to normalize the image by if `do_normalize` is set to `True`.\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to normalize the image by if `do_normalize` is set to `True`.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                    - Unset: Return a list of `np.ndarray`.\n+                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+\n+        size = size if size is not None else self.size\n+        size = get_size_dict(size, default_to_square=False)\n+        images = make_flat_list_of_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+        # PIL RGBA images are converted to RGB\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if do_rescale and is_scaled_image(images[0]):\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        if do_resize:\n+            images = [\n+                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        if do_rescale:\n+            images = [\n+                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        if do_normalize:\n+            images = [\n+                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        images = [\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+        ]\n+\n+        encoded_outputs = BatchFeature(data={\"pixel_values\": images}, tensor_type=return_tensors)\n+\n+        return encoded_outputs\n+\n+    def pad_to_square(\n+        self,\n+        image: np.ndarray,\n+        background_color: Union[int, tuple[int, int, int]] = 0,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.array:\n+        \"\"\"\n+        Pads an image to a square based on the longest edge.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                The image to pad.\n+            background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n+                The color to use for the padding. Can be an integer for single channel or a\n+                tuple of integers representing for multi-channel images. If passed as integer\n+                in mutli-channel mode, it will default to `0` in subsequent channels.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+\n+        Returns:\n+            `np.ndarray`: The padded image.\n+        \"\"\"\n+        height, width = get_image_size(image, input_data_format)\n+        num_channels = image.shape[0] if input_data_format == ChannelDimension.FIRST else image.shape[-1]\n+\n+        if height == width:\n+            image = (\n+                to_channel_dimension_format(image, data_format, input_data_format)\n+                if data_format is not None\n+                else image\n+            )\n+            return image\n+\n+        max_dim = max(height, width)\n+\n+        # Ensure background_color is the correct shape\n+        if isinstance(background_color, int):\n+            background_color = [background_color]\n+        elif len(background_color) != num_channels:\n+            raise ValueError(\n+                f\"background_color must have no more than {num_channels} elements to match the number of channels\"\n+            )\n+\n+        if input_data_format == ChannelDimension.FIRST:\n+            result = np.zeros((num_channels, max_dim, max_dim), dtype=image.dtype)\n+            for i, color in enumerate(background_color):\n+                result[i, :, :] = color\n+            if width > height:\n+                start = (max_dim - height) // 2\n+                result[:, start : start + height, :] = image\n+            else:\n+                start = (max_dim - width) // 2\n+                result[:, :, start : start + width] = image\n+        else:\n+            result = np.zeros((max_dim, max_dim, num_channels), dtype=image.dtype)\n+            for i, color in enumerate(background_color):\n+                result[:, :, i] = color\n+            if width > height:\n+                start = (max_dim - height) // 2\n+                result[start : start + height, :, :] = image\n+            else:\n+                start = (max_dim - width) // 2\n+                result[:, start : start + width, :] = image\n+\n+        return result\n+\n+    def postprocess(self):\n+        \"\"\"Applies post-processing to the decoded image tokens by reversing transformations applied during preprocessing.\"\"\"\n+        raise AttributeError(\"Not needed for DeepseekVL\")\n+\n+\n+__all__ = [\"DeepseekVLImageProcessor\"]"
        },
        {
            "sha": "ce85d739bc68cba99b8514cdcb0935aac2f6defe",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "added",
            "additions": 349,
            "deletions": 0,
            "changes": 349,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,349 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/deepseek_vl/modular_deepseek_vl.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_deepseek_vl.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import Optional, Union\n+\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import ModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TransformersKwargs,\n+    auto_docstring,\n+    can_return_tuple,\n+    is_torch_available,\n+)\n+from ..auto import AutoModel\n+from .configuration_deepseek_vl import DeepseekVLConfig\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn as nn\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for DeepseekVL model's outputs that may also contain a past key/values (to speed up sequential decoding).\n+    \"\"\"\n+)\n+class DeepseekVLBaseModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the model.\n+\n+        If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n+        hidden_size)` is output.\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n+        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n+        encoder_sequence_length, embed_size_per_head)`.\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n+        `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n+        input) to speed up sequential decoding.\n+    image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+        sequence_length, hidden_size)`.\n+\n+        image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for DeepseekVL causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n+class DeepseekVLCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+        sequence_length, hidden_size)`.\n+\n+        image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+\n+\n+class DeepseekVLAligner(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+\n+        in_features = config.vision_config.hidden_size\n+        out_features = config.text_config.hidden_size\n+\n+        self.linear1 = nn.Linear(in_features, out_features)\n+        self.activation = nn.GELU()\n+        self.linear2 = nn.Linear(out_features, out_features)\n+\n+    def forward(self, vision_encodings: torch.Tensor) -> torch.Tensor:\n+        x = self.linear1(vision_encodings)\n+        x = self.activation(x)\n+        x = self.linear2(x)\n+        return x\n+\n+\n+@auto_docstring\n+class DeepseekVLPreTrainedModel(PreTrainedModel):\n+    config: DeepseekVLConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"LlamaDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+\n+    _supports_static_cache = True\n+    _supports_param_buffer_assignment = False\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        # Required only for Linear layer in DeepseekVLAligner\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+\n+@auto_docstring\n+class DeepseekVLModel(DeepseekVLPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.vision_model = AutoModel.from_config(config.vision_config)\n+        self.aligner = DeepseekVLAligner(config)\n+\n+        self.language_model = AutoModel.from_config(config=config.text_config)\n+\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing.\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_image_features(self, pixel_values):\n+        image_embeds = self.vision_model(pixel_values)\n+        image_embeds = self.aligner(image_embeds.last_hidden_state)\n+        return image_embeds\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ):\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+            )\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            if input_ids is None:\n+                image_attention_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                image_attention_mask = image_attention_mask.all(-1)\n+            else:\n+                image_attention_mask = input_ids == self.config.image_token_id\n+\n+            image_attention_mask = image_attention_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_embeds = self.get_image_features(pixel_values)\n+            image_features = image_embeds.reshape(-1, inputs_embeds.shape[-1])\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(image_attention_mask, image_features)\n+\n+        lm_output = self.language_model(\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        return DeepseekVLBaseModelOutputWithPast(\n+            last_hidden_state=lm_output.last_hidden_state,\n+            past_key_values=lm_output.past_key_values,\n+            hidden_states=lm_output.hidden_states,\n+            attentions=lm_output.attentions,\n+            image_hidden_states=image_embeds if pixel_values is not None else None,\n+        )\n+\n+\n+class DeepseekVLForConditionalGeneration(DeepseekVLPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n+    _supports_static_cache = True\n+\n+    def __init__(self, config: DeepseekVLConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.model = DeepseekVLModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing.\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.language_model.set_input_embeddings(value)\n+\n+    def prepare_embeddings_for_image_generation(self) -> torch.Tensor:\n+        raise AttributeError(\"Not needed for DeepseekVL\")\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ):\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        \"\"\"\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n+\n+        return DeepseekVLCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        pixel_values=None,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- extra custom processing\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+        # Otherwise we need pixel values to be passed to model\n+        if cache_position[0] == 0:\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n+        return model_inputs\n+\n+\n+__all__ = [\"DeepseekVLPreTrainedModel\", \"DeepseekVLModel\", \"DeepseekVLForConditionalGeneration\"]"
        },
        {
            "sha": "a5190a280b029ee24d91896767b5786e1f2a839b",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "added",
            "additions": 326,
            "deletions": 0,
            "changes": 326,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,326 @@\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Union\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import (\n+    ImageInput,\n+    make_flat_list_of_images,\n+)\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import (\n+    PreTokenizedInput,\n+    TextInput,\n+)\n+from ...utils import (\n+    auto_docstring,\n+    is_torch_available,\n+    logging,\n+)\n+from ..auto import CONFIG_MAPPING, AutoConfig, AutoModel\n+from ..idefics.modeling_idefics import IdeficsBaseModelOutputWithPast, IdeficsCausalLMOutputWithPast\n+from ..janus.image_processing_janus import JanusImageProcessor\n+from ..janus.modeling_janus import JanusForConditionalGeneration, JanusModel, JanusPreTrainedModel\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn as nn\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DeepseekVLConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DeepseekVLModel`]. It is used to instantiate a\n+    DeepseekVL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the DeepseekVL\n+    [deepseek-community/deepseek-vl-1.3b-chat](https://huggingface.co/deepseek-community/deepseek-vl-1.3b-chat) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SiglipVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        image_token_id (`int`, *optional*, defaults to 100015):\n+            The index representing image tokens in the model's token vocabulary.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import DeepseekVLConfig, DeepseekVLModel\n+\n+    >>> # Initializing a DeepseekVL deepseek-community/deepseek-vl-1.3b-chat style configuration\n+    >>> configuration = DeepseekVLConfig()\n+\n+    >>> # Initializing a model (with random weights) from the deepseek-community/deepseek-vl-1.3b-chat style configuration\n+    >>> model = DeepseekVLModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"deepseek_vl\"\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        text_config: AutoConfig = None,\n+        vision_config: AutoConfig = None,\n+        image_token_id: int = 100015,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        if text_config is None:\n+            text_config = {}\n+            logger.info(\"`text_config` is `None`. Initializing the `LlamaConfig` with default values.\")\n+\n+        if vision_config is None:\n+            vision_config = {}\n+            logger.info(\"`vision_config` is `None`. Initializing the `SiglipVisionConfig` with default values.\")\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"siglip_vision_model\")\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.image_token_id = image_token_id\n+\n+\n+class DeepseekVLBaseModelOutputWithPast(IdeficsBaseModelOutputWithPast):\n+    pass\n+\n+\n+class DeepseekVLCausalLMOutputWithPast(IdeficsCausalLMOutputWithPast):\n+    pass\n+\n+\n+class DeepseekVLAligner(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+\n+        in_features = config.vision_config.hidden_size\n+        out_features = config.text_config.hidden_size\n+\n+        self.linear1 = nn.Linear(in_features, out_features)\n+        self.activation = nn.GELU()\n+        self.linear2 = nn.Linear(out_features, out_features)\n+\n+    def forward(self, vision_encodings: torch.Tensor) -> torch.Tensor:\n+        x = self.linear1(vision_encodings)\n+        x = self.activation(x)\n+        x = self.linear2(x)\n+        return x\n+\n+\n+class DeepseekVLPreTrainedModel(JanusPreTrainedModel):\n+    _no_split_modules = [\"LlamaDecoderLayer\"]\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        # Required only for Linear layer in DeepseekVLAligner\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+\n+@auto_docstring\n+class DeepseekVLModel(JanusModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.vision_model = AutoModel.from_config(config.vision_config)\n+        self.aligner = DeepseekVLAligner(config)\n+\n+        self.language_model = AutoModel.from_config(config=config.text_config)\n+\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing.\n+        self.post_init()\n+\n+        del self.vqmodel\n+        del self.generation_embeddings\n+        del self.generation_aligner\n+        del self.generation_head\n+\n+\n+class DeepseekVLForConditionalGeneration(JanusForConditionalGeneration):\n+    def prepare_embeddings_for_image_generation(self):\n+        raise AttributeError(\"Not needed for DeepseekVL\")\n+\n+    def decode_image_tokens(self):\n+        raise AttributeError(\"Not needed for DeepseekVL\")\n+\n+    def generate(self):\n+        raise AttributeError(\"Not needed for DeepseekVL\")\n+\n+\n+class DeepseekVLImageProcessor(JanusImageProcessor):\n+    def postprocess(self):\n+        raise AttributeError(\"Not needed for DeepseekVL\")\n+\n+    def unnormalize(self):\n+        raise AttributeError(\"Not needed for DeepseekVL\")\n+\n+\n+class DeepseekVLProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\"padding\": False},\n+        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+    }\n+\n+\n+class DeepseekVLProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a DeepseekVL processor which wraps a DeepseekVL Image Processor and a Llama tokenizer into a single processor.\n+\n+    [`DeepseekVLProcessor`] offers all the functionalities of [`DeepseekVLImageProcessor`] and [`LlamaTokenizerFast`]. See the\n+    [`~DeepseekVLProcessor.__call__`] and [`~DeepseekVLProcessor.decode`] for more information.\n+\n+    Args:\n+        image_processor ([`DeepseekVLImageProcessor`]):\n+            The image processor is a required input.\n+        tokenizer ([`LlamaTokenizerFast`]):\n+            The tokenizer is a required input.\n+        chat_template (`str`, *optional*):\n+            A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+        num_image_tokens (`int`, *optional*, defaults to 576):\n+            The number of special image tokens used as placeholders for visual content in text sequences.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\"chat_template\", \"num_image_tokens\"]\n+    image_processor_class = \"AutoImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        image_processor,\n+        tokenizer,\n+        chat_template=None,\n+        num_image_tokens=576,\n+    ):\n+        self.image_token = tokenizer.image_token\n+        self.num_image_tokens = num_image_tokens\n+\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+\n+    def __call__(\n+        self,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput = None,\n+        **kwargs: Unpack[DeepseekVLProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        DeepseekVLImageProcessor's [`~DeepseekVLImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        of the above two methods for more information.\n+\n+        Args:\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            DeepseekVLProcessorKwargs, tokenizer_init_kwargs=self.tokenizer.init_kwargs, **kwargs\n+        )\n+        if text is None and images is None:\n+            raise ValueError(\"You must specify either text or images.\")\n+\n+        if text is not None:\n+            if isinstance(text, str):\n+                text = [text]\n+            elif not (isinstance(text, (list, tuple)) and all(isinstance(t, str) for t in text)):\n+                raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        prompt_strings = []\n+        one_img_tokens = self.image_token * self.num_image_tokens\n+        for prompt in text:\n+            prompt = prompt.replace(self.image_token, one_img_tokens)\n+            prompt_strings.append(prompt)\n+\n+        data = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+\n+        # process images if pixel_values are provided\n+        if images is not None:\n+            images = make_flat_list_of_images(images)\n+            data[\"pixel_values\"] = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n+\n+        return BatchFeature(data=data)\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\n+    \"DeepseekVLConfig\",\n+    \"DeepseekVLPreTrainedModel\",\n+    \"DeepseekVLModel\",\n+    \"DeepseekVLForConditionalGeneration\",\n+    \"DeepseekVLImageProcessor\",\n+    \"DeepseekVLProcessor\",\n+]"
        },
        {
            "sha": "244e642d7c36b7cdce54d168686cbc35c0f8e666",
            "filename": "src/transformers/models/deepseek_vl/processing_deepseek_vl.py",
            "status": "added",
            "additions": 157,
            "deletions": 0,
            "changes": 157,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,157 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/deepseek_vl/modular_deepseek_vl.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_deepseek_vl.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import ImageInput, make_flat_list_of_images\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+\n+class DeepseekVLProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\"padding\": False},\n+        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+    }\n+\n+\n+class DeepseekVLProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a DeepseekVL processor which wraps a DeepseekVL Image Processor and a Llama tokenizer into a single processor.\n+\n+    [`DeepseekVLProcessor`] offers all the functionalities of [`DeepseekVLImageProcessor`] and [`LlamaTokenizerFast`]. See the\n+    [`~DeepseekVLProcessor.__call__`] and [`~DeepseekVLProcessor.decode`] for more information.\n+\n+    Args:\n+        image_processor ([`DeepseekVLImageProcessor`]):\n+            The image processor is a required input.\n+        tokenizer ([`LlamaTokenizerFast`]):\n+            The tokenizer is a required input.\n+        chat_template (`str`, *optional*):\n+            A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+        num_image_tokens (`int`, *optional*, defaults to 576):\n+            The number of special image tokens used as placeholders for visual content in text sequences.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\"chat_template\", \"num_image_tokens\"]\n+    image_processor_class = \"AutoImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        image_processor,\n+        tokenizer,\n+        chat_template=None,\n+        num_image_tokens=576,\n+    ):\n+        self.image_token = tokenizer.image_token\n+        self.num_image_tokens = num_image_tokens\n+\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+\n+    def __call__(\n+        self,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput = None,\n+        **kwargs: Unpack[DeepseekVLProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        DeepseekVLImageProcessor's [`~DeepseekVLImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        of the above two methods for more information.\n+\n+        Args:\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            DeepseekVLProcessorKwargs, tokenizer_init_kwargs=self.tokenizer.init_kwargs, **kwargs\n+        )\n+        if text is None and images is None:\n+            raise ValueError(\"You must specify either text or images.\")\n+\n+        if text is not None:\n+            if isinstance(text, str):\n+                text = [text]\n+            elif not (isinstance(text, (list, tuple)) and all(isinstance(t, str) for t in text)):\n+                raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        prompt_strings = []\n+        one_img_tokens = self.image_token * self.num_image_tokens\n+        for prompt in text:\n+            prompt = prompt.replace(self.image_token, one_img_tokens)\n+            prompt_strings.append(prompt)\n+\n+        data = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+\n+        # process images if pixel_values are provided\n+        if images is not None:\n+            images = make_flat_list_of_images(images)\n+            data[\"pixel_values\"] = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n+\n+        return BatchFeature(data=data)\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"DeepseekVLProcessor\"]"
        },
        {
            "sha": "1836d196ac0b7ea56d45c1a1a8a89fded14f6c1a",
            "filename": "src/transformers/models/deepseek_vl_hybrid/__init__.py",
            "status": "added",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2F__init__.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,30 @@\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_deepseek_vl_hybrid import *\n+    from .image_processing_deepseek_vl_fast_hybrid import *\n+    from .image_processing_deepseek_vl_hybrid import *\n+    from .modeling_deepseek_vl_hybrid import *\n+    from .processing_deepseek_vl_hybrid import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "c3a5aa5260f6bcfbd440239224983dd3979f0c31",
            "filename": "src/transformers/models/deepseek_vl_hybrid/configuration_deepseek_vl_hybrid.py",
            "status": "added",
            "additions": 108,
            "deletions": 0,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconfiguration_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconfiguration_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconfiguration_deepseek_vl_hybrid.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,108 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_deepseek_vl_hybrid.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DeepseekVLHybridConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DeepseekVLHybridModel`]. It is used to instantiate a\n+    DeepseekVLHybrid model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the DeepseekVLHybrid\n+    [deepseek-community/deepseek-vl-7b-chat](https://huggingface.co/deepseek-community/deepseek-vl-7b-chat) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SiglipVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        high_res_vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SamVisionConfig`):\n+            The config object or dictionary of the high resolution vision backbone.\n+        image_token_id (`int`, *optional*, defaults to 100015):\n+            The index representing image tokens in the model's token vocabulary.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import DeepseekVLHybridConfig, DeepseekVLHybridModel\n+\n+    >>> # Initializing a DeepseekVLHybrid deepseek-community/deepseek-vl-7b-chat style configuration\n+    >>> configuration = DeepseekVLHybridConfig()\n+\n+    >>> # Initializing a model (with random weights) from the deepseek-community/deepseek-vl-7b-chat style configuration\n+    >>> model = DeepseekVLHybridModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"deepseek_vl_hybrid\"\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig, \"high_res_vision_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        text_config: AutoConfig = None,\n+        vision_config: AutoConfig = None,\n+        high_res_vision_config: AutoConfig = None,\n+        image_token_id: int = 100015,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        if text_config is None:\n+            text_config = {}\n+            logger.info(\"`text_config` is `None`. Initializing the `LlamaConfig` with default values.\")\n+\n+        if vision_config is None:\n+            vision_config = {}\n+            logger.info(\"`vision_config` is `None`. Initializing the `SiglipVisionConfig` with default values.\")\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"siglip_vision_model\")\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.image_token_id = image_token_id\n+\n+        if high_res_vision_config is None:\n+            high_res_vision_config = {}\n+            logger.info(\"`high_res_vision_config` is `None`. Initializing the `SamVisionConfig` with default values.\")\n+\n+        if isinstance(high_res_vision_config, dict):\n+            high_res_vision_config[\"model_type\"] = high_res_vision_config.get(\"model_type\", \"sam_vision_model\")\n+            high_res_vision_config = CONFIG_MAPPING[high_res_vision_config[\"model_type\"]](**high_res_vision_config)\n+\n+        self.high_res_vision_config = high_res_vision_config\n+\n+\n+__all__ = [\"DeepseekVLHybridConfig\"]"
        },
        {
            "sha": "9f377a53c8f3da49b2b2d35ac1d355a97c2d0c42",
            "filename": "src/transformers/models/deepseek_vl_hybrid/convert_deepseek_vl_hybrid_weights_to_hf.py",
            "status": "added",
            "additions": 394,
            "deletions": 0,
            "changes": 394,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconvert_deepseek_vl_hybrid_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconvert_deepseek_vl_hybrid_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconvert_deepseek_vl_hybrid_weights_to_hf.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,394 @@\n+# coding=utf-8\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import gc\n+import json\n+import os\n+from typing import Optional\n+\n+import regex as re\n+import torch\n+from accelerate import init_empty_weights\n+from huggingface_hub import snapshot_download\n+from huggingface_hub.errors import HFValidationError\n+from safetensors.torch import load_file\n+\n+from transformers import (\n+    AutoTokenizer,\n+    DeepseekVLHybridConfig,\n+    DeepseekVLHybridForConditionalGeneration,\n+    DeepseekVLHybridImageProcessor,\n+    DeepseekVLHybridProcessor,\n+)\n+from transformers.image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    PILImageResampling,\n+)\n+\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    # # Sam (High Resolution)\n+    r\"vision_model.vision_tower_high.vision_tower.pos_embed\":                                 r\"model.high_res_vision_model.vision_encoder.pos_embed\",\n+    r\"vision_model.vision_tower_high.vision_tower.patch_embed.proj.(weight|bias)\":            r\"model.high_res_vision_model.vision_encoder.patch_embed.projection.\\1\",\n+    r\"vision_model.vision_tower_high.vision_tower.blocks.(\\d+).norm(\\d+).(weight|bias)\":      r\"model.high_res_vision_model.vision_encoder.layers.\\1.layer_norm\\2.\\3\",\n+    r\"vision_model.vision_tower_high.vision_tower.blocks.(\\d+).attn.rel_pos_(h|w)\":           r\"model.high_res_vision_model.vision_encoder.layers.\\1.attn.rel_pos_\\2\",\n+    r\"vision_model.vision_tower_high.vision_tower.blocks.(\\d+).attn.qkv.(weight|bias)\":       r\"model.high_res_vision_model.vision_encoder.layers.\\1.attn.qkv.\\2\",\n+    r\"vision_model.vision_tower_high.vision_tower.blocks.(\\d+).attn.proj.(weight|bias)\":      r\"model.high_res_vision_model.vision_encoder.layers.\\1.attn.proj.\\2\",\n+    r\"vision_model.vision_tower_high.vision_tower.blocks.(\\d+).mlp.lin(\\d+).(weight|bias)\":   r\"model.high_res_vision_model.vision_encoder.layers.\\1.mlp.lin\\2.\\3\",\n+    r\"vision_model.vision_tower_high.vision_tower.neck.0.weight\":                             r\"model.high_res_vision_model.vision_encoder.neck.conv1.weight\",\n+    r\"vision_model.vision_tower_high.vision_tower.neck.1.(weight|bias)\":                      r\"model.high_res_vision_model.vision_encoder.neck.layer_norm1.\\1\",\n+    r\"vision_model.vision_tower_high.vision_tower.neck.2.weight\":                             r\"model.high_res_vision_model.vision_encoder.neck.conv2.weight\",\n+    r\"vision_model.vision_tower_high.vision_tower.neck.3.(weight|bias)\":                      r\"model.high_res_vision_model.vision_encoder.neck.layer_norm2.\\1\",\n+    r\"vision_model.vision_tower_high.vision_tower.neck_hd.0.weight\":                          r\"model.high_res_vision_neck.conv1.weight\",\n+    r\"vision_model.vision_tower_high.vision_tower.neck_hd.1.(weight|bias)\":                   r\"model.high_res_vision_neck.layer_norm1.\\1\",\n+    r\"vision_model.vision_tower_high.vision_tower.neck_hd.2.weight\":                          r\"model.high_res_vision_neck.conv2.weight\",\n+    r\"vision_model.vision_tower_high.vision_tower.neck_hd.3.(weight|bias)\":                   r\"model.high_res_vision_neck.layer_norm2.\\1\",\n+    r\"vision_model.vision_tower_high.vision_tower.downsamples.0.weight\":                      r\"model.high_res_vision_proj.conv1.weight\",\n+    r\"vision_model.vision_tower_high.vision_tower.downsamples.1.weight\":                      r\"model.high_res_vision_proj.conv2.weight\",\n+    r\"vision_model.vision_tower_high.vision_tower.hd_alpha_downsamples\":                      r\"model.high_res_vision_alpha\",\n+\n+    # Siglip (Low Resolution)\n+    r\"vision_model.vision_tower_low.vision_tower.pos_embed\":                                  r\"model.vision_model.vision_model.embeddings.position_embedding.weight\",\n+    r\"vision_model.vision_tower_low.vision_tower.patch_embed.proj.(weight|bias)\":             r\"model.vision_model.vision_model.embeddings.patch_embedding.\\1\",\n+    r\"vision_model.vision_tower_low.vision_tower.blocks.(\\d+).attn.qkv.(weight|bias)\":        r\"model.vision_model.vision_model.encoder.layers.\\1.self_attn.(q|k|v)_proj.\\2\",\n+    r\"vision_model.vision_tower_low.vision_tower.blocks.(\\d+).attn.proj.(weight|bias)\":       r\"model.vision_model.vision_model.encoder.layers.\\1.self_attn.out_proj.\\2\",\n+    r\"vision_model.vision_tower_low.vision_tower.blocks.(\\d+).norm(\\d+).(weight|bias)\":       r\"model.vision_model.vision_model.encoder.layers.\\1.layer_norm\\2.\\3\",\n+    r\"vision_model.vision_tower_low.vision_tower.blocks.(\\d+).mlp.fc(\\d+).(weight|bias)\":     r\"model.vision_model.vision_model.encoder.layers.\\1.mlp.fc\\2.\\3\",\n+    r\"vision_model.vision_tower_low.vision_tower.norm.(weight|bias)\":                         r\"model.vision_model.vision_model.post_layernorm.\\1\",\n+    r\"vision_model.vision_tower_low.vision_tower.attn_pool.latent\":                           r\"model.vision_model.vision_model.head.probe\",\n+    r\"vision_model.vision_tower_low.vision_tower.attn_pool.proj.(weight|bias)\":               r\"model.vision_model.vision_model.head.attention.out_proj.\\1\",\n+    r\"vision_model.vision_tower_low.vision_tower.attn_pool.norm.(weight|bias)\":               r\"model.vision_model.vision_model.head.layernorm.\\1\",\n+    r\"vision_model.vision_tower_low.vision_tower.attn_pool.mlp.fc(\\d+).(weight|bias)\":        r\"model.vision_model.vision_model.head.mlp.fc\\1.\\2\",\n+\n+    # Vision Projection\n+    r\"aligner.layers.1.(weight|bias)\":        r\"model.aligner.proj.\\1\",\n+    r\"aligner.low_up_proj.(weight|bias)\":     r\"model.aligner.vision_proj.\\1\",\n+    r\"aligner.high_up_proj.(weight|bias)\":    r\"model.aligner.high_res_vision_proj.\\1\",\n+\n+    # Llama (Text Model)\n+    r\"language_model.model.(\\w+)\":            r\"model.language_model.\\1\",\n+    r\"language_model.lm_head.(weight|bias)\":  r\"lm_head.\\1\",\n+}\n+# fmt: on\n+\n+# Adopted from https://github.com/deepseek-ai/DeepSeek-VL/blob/main/deepseek_vl/utils/conversation.py#L80-L91\n+CHAT_TEMPLATE = (\n+    # Define separators and initialize counter\n+    \"{% set seps = ['\\n\\n', '<\\uff5cend\\u2581of\\u2581sentence\\uff5c>'] %}\"\n+    \"{% set i = 0 %}\"\n+    # Start with default system prompt\n+    \"You are a helpful language and vision assistant. \"\n+    \"You are able to understand the visual content that the user provides, \"\n+    \"and assist the user with a variety of tasks using natural language.\\n\\n\"\n+    # Iterate through messages\n+    \"{% for message in messages %}\"\n+    # Identify user or assistant role\n+    \"{% if message['role']|lower == 'user' %}\"\n+    \"User: \"\n+    \"{% elif message['role']|lower == 'assistant' %}\"\n+    \"Assistant:{% if not (loop.last and not add_generation_prompt and message['content'][0]['type']=='text' and message['content'][0]['text']=='') %} {% endif %}\"\n+    \"{% else %}\"\n+    \"{{ message['role'].capitalize() }}: \"\n+    \"{% endif %}\"\n+    # Iterate through message content (text/images)\n+    \"{% for content in message['content'] %}\"\n+    # If content is an image, replace with placeholder\n+    \"{% if content['type'] == 'image' %}\"\n+    \"<image_placeholder>\"\n+    # If content is text, handle formatting\n+    \"{% elif content['type'] == 'text' %}\"\n+    \"{% set text = content['text'] %}\"\n+    # Strip whitespace for first and last text blocks\n+    \"{% if loop.first %}{% set text = text.lstrip() %}{% endif %}\"\n+    \"{% if loop.last %}{% set text = text.rstrip() %}{% endif %}\"\n+    # If previous content was text, add space\n+    \"{% if not loop.first and message['content'][loop.index0-1]['type'] == 'text' %}\"\n+    \"{{ ' ' + text }}\"\n+    \"{% else %}\"\n+    \"{{ text }}\"\n+    \"{% endif %}\"\n+    \"{% endif %}\"\n+    \"{% endfor %}\"  # End message content loop\n+    # Add separators between messages\n+    \"{% if not loop.last or add_generation_prompt %}\"\n+    \"{% if message['role']|lower == 'user' %}\"\n+    \"{{ seps[0] }}\"\n+    \"{% else %}\"\n+    \"{{ seps[1] }}\"\n+    \"{% endif %}\"\n+    \"{% endif %}\"\n+    \"{% endfor %}\"  # End messages loop\n+    # Add final Assistant prompt if required\n+    \"{% if add_generation_prompt %}Assistant:{% endif %}\"\n+)\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: dict):\n+    output_dict = {}\n+\n+    old_text = \"\\n\".join(state_dict_keys)\n+    new_text = old_text\n+    for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+        if replacement is None:\n+            new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+            continue\n+        new_text = re.sub(pattern, replacement, new_text)\n+    output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+\n+    return output_dict\n+\n+\n+def get_qkv_state_dict(key, parameter):\n+    \"\"\"\n+    new key which looks like this\n+    xxxx.(q|k|v).xxx    (m, n)\n+\n+    is converted to\n+    xxxx.q.xxxx         (m//3, n)\n+    xxxx.k.xxxx         (m//3, n)\n+    xxxx.v.xxxx         (m//3, n)\n+    \"\"\"\n+    qkv_state_dict = {}\n+    placeholder = re.search(r\"(\\(.*?\\))\", key).group(1)  # finds   \"(query|key|value)\"\n+    replacements_keys = placeholder[1:-1].split(\"|\")  # creates ['query', 'key', 'value']\n+    replacements_vals = torch.split(\n+        parameter, split_size_or_sections=parameter.size(0) // len(replacements_keys), dim=0\n+    )\n+    for replacement_key, replacement_val in zip(replacements_keys, replacements_vals):\n+        qkv_state_dict[key.replace(placeholder, replacement_key)] = replacement_val\n+    return qkv_state_dict\n+\n+\n+def update_state_dict(old_state_dict):\n+    all_keys = list(old_state_dict.keys())\n+    new_keys = convert_old_keys_to_new_keys(all_keys)\n+\n+    state_dict = {}\n+    for key in all_keys:\n+        new_key = new_keys[key]\n+        current_parameter = old_state_dict.pop(key)\n+\n+        if \"qkv\" in key and \"vision_tower_high\" not in key:\n+            qkv_state_dict = get_qkv_state_dict(new_key, current_parameter)\n+            state_dict.update(qkv_state_dict)\n+        elif \"pos_embed\" in key:\n+            if \"vision_tower_high\" not in key:\n+                # timm implementation of siglip creates this param of size [1, 576, 1024]\n+                # transformers implementation of siglip creates this param of size [576, 1024]\n+                state_dict[new_key] = current_parameter.squeeze(0)\n+            else:\n+                state_dict[new_key] = current_parameter\n+        else:\n+            state_dict[new_key] = current_parameter\n+\n+    return state_dict\n+\n+\n+def load_model_state_dict(input_path: str) -> dict:\n+    \"\"\"\n+    Load model state dict, handling both single and sharded files.\n+    \"\"\"\n+    index_path = os.path.join(input_path, \"model.safetensors.index.json\")\n+    single_file_path = os.path.join(input_path, \"model.safetensors\")\n+\n+    # Check if we have a sharded model\n+    if os.path.exists(index_path):\n+        print(\"Loading sharded model...\")\n+        state_dict = {}\n+        with open(index_path, \"r\") as f:\n+            index = json.load(f)\n+\n+        # Get unique shard files and load each one only once\n+        unique_shard_files = sorted(set(index[\"weight_map\"].values()))\n+        for shard_file in unique_shard_files:\n+            print(f\"Loading shard {shard_file}...\")\n+            shard_path = os.path.join(input_path, shard_file)\n+            shard_dict = load_file(shard_path)\n+            state_dict.update(shard_dict)\n+\n+        return state_dict\n+\n+    # Single file model\n+    elif os.path.exists(single_file_path):\n+        print(\"Loading single file model...\")\n+        return load_file(single_file_path, device=\"cpu\")\n+\n+    else:\n+        raise ValueError(f\"No model files found in {input_path}\")\n+\n+\n+def convert_model(\n+    hf_repo_id: str,\n+    output_dir: Optional[str] = None,\n+    output_hub_path: Optional[str] = None,\n+    safe_serialization: bool = True,\n+):\n+    if output_dir:\n+        os.makedirs(output_dir, exist_ok=True)\n+\n+    try:\n+        input_path = snapshot_download(hf_repo_id)\n+    except HFValidationError:\n+        # If the input path is not a HF repo ID, assume it's a local path\n+        input_path = hf_repo_id\n+\n+    # ------------------------------------------------------------\n+    # Create and save config\n+    # ------------------------------------------------------------\n+\n+    config = DeepseekVLHybridConfig(\n+        text_config={\n+            \"hidden_size\": 4096,\n+            \"intermediate_size\": 11008,\n+            \"max_position_embeddings\": 16384,\n+            \"num_attention_heads\": 32,\n+            \"num_hidden_layers\": 30,\n+            \"vocab_size\": 102400,\n+        },\n+        vision_config={\n+            \"hidden_size\": 1024,\n+            \"intermediate_size\": 4096,\n+            \"image_size\": 384,\n+            \"patch_size\": 16,\n+            \"hidden_act\": \"gelu\",\n+            \"vision_use_head\": False,\n+            \"num_attention_heads\": 16,\n+            \"num_hidden_layers\": 24,\n+        },\n+        high_res_vision_config={\n+            \"hidden_size\": 768,\n+            \"intermediate_size\": 3072,\n+            \"image_size\": 1024,\n+            \"patch_size\": 16,\n+            \"num_attention_heads\": 12,\n+            \"num_hidden_layers\": 12,\n+        },\n+    )\n+\n+    # save config\n+    if output_dir:\n+        config.save_pretrained(output_dir)\n+        print(\"Model config saved successfully...\")\n+\n+    # ------------------------------------------------------------\n+    # Convert processor\n+    # ------------------------------------------------------------\n+\n+    image_processor = DeepseekVLHybridImageProcessor(\n+        image_mean=IMAGENET_STANDARD_MEAN,\n+        image_std=IMAGENET_STANDARD_STD,\n+        high_res_image_mean=OPENAI_CLIP_MEAN,\n+        high_res_image_std=OPENAI_CLIP_STD,\n+        resample=PILImageResampling.BILINEAR,\n+    )\n+\n+    tokenizer = AutoTokenizer.from_pretrained(\n+        input_path,\n+        extra_special_tokens={\n+            \"pad_token\": \"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\",\n+            \"image_token\": \"<image_placeholder>\",\n+        },\n+    )\n+\n+    processor = DeepseekVLHybridProcessor(\n+        image_processor=image_processor,\n+        tokenizer=tokenizer,\n+        chat_template=CHAT_TEMPLATE,\n+    )\n+\n+    if output_dir:\n+        print(f\"Saving processor to {output_dir}...\")\n+        processor.save_pretrained(output_dir)\n+    if output_hub_path:\n+        print(f\"Pushing processor to hub at {output_hub_path}...\")\n+        processor.push_to_hub(output_hub_path)\n+\n+    # ------------------------------------------------------------\n+    # Convert weights\n+    # ------------------------------------------------------------\n+\n+    print(\"Creating empty model...\")\n+    with init_empty_weights():\n+        model = DeepseekVLHybridForConditionalGeneration(config)\n+\n+    # Load and convert state dict\n+    print(\"Loading state dict...\")\n+    state_dict = load_model_state_dict(input_path)\n+    state_dict = update_state_dict(state_dict)\n+\n+    # Load converted state dict\n+    print(\"Loading converted weights into model...\")\n+    info = model.load_state_dict(state_dict, strict=False, assign=True)\n+    if len(info.missing_keys) > 0:\n+        raise ValueError(f\"Missing keys: {info.missing_keys}\")\n+\n+    # Tie weights before any device mapping\n+    print(\"Tying weights...\")\n+    model.tie_weights()\n+\n+    # Save the model\n+    if output_dir:\n+        print(f\"Saving model to {output_dir}...\")\n+        model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    if output_hub_path:\n+        print(f\"Pushing model to hub at {output_hub_path}...\")\n+        model.push_to_hub(output_hub_path, safe_serialization=safe_serialization)\n+\n+    del state_dict, model\n+    gc.collect()\n+\n+    # Validate the saved model if saved locally\n+    if output_dir:\n+        print(\"Reloading the local model to check if it's saved correctly...\")\n+        DeepseekVLHybridForConditionalGeneration.from_pretrained(output_dir, device_map=\"auto\")\n+        print(\"Local model reloaded successfully.\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--hf_repo_id\",\n+        default=\"deepseek-ai/deepseek-vl-7b-chat\",\n+        help=\"Location of official weights from DeepseekAI on HF\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        default=None,\n+        help=\"Location to write the converted model and processor\",\n+    )\n+    parser.add_argument(\n+        \"--output_hub_path\",\n+        default=None,\n+        help=\"Repository ID to push model to hub (e.g. 'username/model-name')\",\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n+    )\n+    args = parser.parse_args()\n+\n+    convert_model(\n+        hf_repo_id=args.hf_repo_id,\n+        output_dir=args.output_dir,\n+        output_hub_path=args.output_hub_path,\n+        safe_serialization=args.safe_serialization,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "d42cfbe38bf49f63cda7ad1ebad2106530681d4f",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "status": "added",
            "additions": 483,
            "deletions": 0,
            "changes": 483,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,483 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_deepseek_vl_hybrid.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor\n+from ...image_processing_utils_fast import BatchFeature, get_size_dict\n+from ...image_transforms import convert_to_rgb, resize, to_channel_dimension_format\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    make_list_of_images,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import (\n+    TensorType,\n+    filter_out_non_signature_kwargs,\n+    is_vision_available,\n+    logging,\n+)\n+\n+\n+if is_vision_available():\n+    import PIL\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DeepseekVLHybridImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a DEEPSEEK_VL_HYBRID image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n+            `do_resize` parameter in the `preprocess` method.\n+        size (`dict`, *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n+            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n+            method.\n+        high_res_size (`dict`, *optional*, defaults to `{\"height\": 1024, \"width\": 1024}`):\n+            Size of the high resolution output image after resizing. Can be overridden by the `high_res_size` parameter in the `preprocess`\n+            method.\n+        min_size (`int`, *optional*, defaults to 14):\n+            The minimum allowed size for the resized image. Ensures that neither the height nor width\n+            falls below this value after resizing.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n+            overridden by the `resample` parameter in the `preprocess` method.\n+        high_res_resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n+            overridden by the `high_res_resample` parameter in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n+            `do_rescale` parameter in the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Only has an effect if `do_rescale` is set to `True`. Can be\n+            overridden by the `rescale_factor` parameter in the `preprocess` method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n+            method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n+            overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        high_res_image_mean (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_MEAN`):\n+            Mean to use if normalizing the high resolution image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `high_res_image_mean` parameter in the `preprocess` method.\n+        high_res_image_std (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_STD`):\n+            Standard deviation to use if normalizing the high resolution image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `high_res_image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Optional[dict[str, int]] = None,\n+        high_res_size: Optional[dict[str, int]] = None,\n+        min_size: int = 14,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        high_res_resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        high_res_image_mean: Optional[Union[float, list[float]]] = None,\n+        high_res_image_std: Optional[Union[float, list[float]]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        high_res_size = high_res_size if high_res_size is not None else {\"height\": 1024, \"width\": 1024}\n+        high_res_size = get_size_dict(high_res_size, default_to_square=True)\n+\n+        self.high_res_size = high_res_size\n+        self.high_res_image_mean = high_res_image_mean if high_res_image_mean is not None else OPENAI_CLIP_MEAN\n+        self.high_res_image_std = high_res_image_std if high_res_image_std is not None else OPENAI_CLIP_STD\n+\n+        self.resample = resample\n+        self.high_res_resample = high_res_resample\n+        size = size if size is not None else {\"height\": 384, \"width\": 384}\n+        size = get_size_dict(size, default_to_square=True)\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n+        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n+        self.do_convert_rgb = do_convert_rgb\n+\n+        self.min_size = min_size\n+        if image_mean is None:\n+            self.background_color = (127, 127, 127)\n+        else:\n+            self.background_color = tuple([int(x * 255) for x in image_mean])\n+\n+        if high_res_image_mean is None:\n+            self.background_color = (127, 127, 127)\n+        else:\n+            self.background_color = tuple([int(x * 255) for x in high_res_image_mean])\n+\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: Union[dict[str, int], int],\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Resize an image to dynamically calculated size.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n+                `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n+            data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the output image. If unset, the channel dimension format of the input\n+                image is used. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `None`: will be inferred from input\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+\n+        Returns:\n+            `np.ndarray`: The resized image.\n+        \"\"\"\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(image)\n+\n+        height, width = get_image_size(image, input_data_format)\n+        max_size = max(height, width)\n+\n+        size = get_size_dict(size, default_to_square=True)\n+        if size[\"height\"] != size[\"width\"]:\n+            raise ValueError(\n+                f\"Output height and width must be the same. Got height={size['height']} and width={size['width']}\"\n+            )\n+        size = size[\"height\"]\n+\n+        delta = size / max_size\n+        # Largest side becomes `size` and the other side is scaled according to the aspect ratio.\n+        output_size_nonpadded = [\n+            max(int(height * delta), self.min_size),\n+            max(int(width * delta), self.min_size),\n+        ]\n+\n+        image = resize(\n+            image,\n+            size=output_size_nonpadded,\n+            resample=resample,\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+            **kwargs,\n+        )\n+        # Expand and pad the images to obtain a square image of dimensions `size x size`\n+        image = self.pad_to_square(\n+            image=image,\n+            background_color=self.background_color,\n+            input_data_format=input_data_format,\n+        )\n+        return image\n+\n+    @filter_out_non_signature_kwargs()\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[dict[str, int]] = None,\n+        high_res_size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = None,\n+        high_res_resample: PILImageResampling = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        high_res_image_mean: Optional[Union[float, list[float]]] = None,\n+        high_res_image_std: Optional[Union[float, list[float]]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+    ) -> PIL.Image.Image:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Dictionary in the format `{\"height\": h, \"width\": w}` specifying the size of the output image after\n+                resizing.\n+            high_res_size (`Dict[str, int]`, *optional*, defaults to `self.high_res_size`):\n+                Dictionary in the format `{\"height\": h, \"width\": w}` specifying the size of the high resolution output image after\n+                resizing.\n+            resample (`PILImageResampling` filter, *optional*, defaults to `self.resample`):\n+                `PILImageResampling` filter to use if resizing the image e.g. `PILImageResampling.BILINEAR`. Only has\n+                an effect if `do_resize` is set to `True`.\n+            high_res_resample (`PILImageResampling` filter, *optional*, defaults to `self.resample`):\n+                `PILImageResampling` filter to use if resizing the image e.g. `PILImageResampling.BICUBIC`. Only has\n+                an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image values between [0 - 1].\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use if `do_normalize` is set to `True`.\n+            high_res_image_mean (`float` or `List[float]`, *optional*, defaults to `self.high_res_image_mean`):\n+                Image mean to use if `do_normalize` is set to `True`.\n+            high_res_image_std (`float` or `List[float]`, *optional*, defaults to `self.high_res_image_std`):\n+                Image standard deviation to use if `do_normalize` is set to `True`.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        resample = resample if resample is not None else self.resample\n+        high_res_resample = high_res_resample if high_res_resample is not None else self.high_res_resample\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        high_res_image_mean = high_res_image_mean if high_res_image_mean is not None else self.high_res_image_mean\n+        high_res_image_std = high_res_image_std if high_res_image_std is not None else self.high_res_image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+\n+        size = size if size is not None else self.size\n+        size_dict = get_size_dict(size)\n+        high_res_size = high_res_size if high_res_size is not None else self.high_res_size\n+        high_res_size_dict = get_size_dict(high_res_size)\n+\n+        images = make_list_of_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if do_rescale and is_scaled_image(images[0]):\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        all_images = []\n+        all_high_res_images = []\n+        for image in images:\n+            # high_res_image: resize (high) -> rescale -> normalize (high)\n+            # low_res_image:  resize (high) -> rescale -> resize (low) -> normalize (low)\n+            high_res_image = image\n+\n+            if do_resize:\n+                high_res_image = self.resize(\n+                    image=high_res_image,\n+                    size=high_res_size_dict,\n+                    resample=high_res_resample,\n+                    input_data_format=input_data_format,\n+                )\n+                image = self.resize(\n+                    image=high_res_image, size=size_dict, resample=resample, input_data_format=input_data_format\n+                )\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+                high_res_image = self.rescale(\n+                    image=high_res_image, scale=rescale_factor, input_data_format=input_data_format\n+                )\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+                high_res_image = self.normalize(\n+                    image=high_res_image,\n+                    mean=high_res_image_mean,\n+                    std=high_res_image_std,\n+                    input_data_format=input_data_format,\n+                )\n+\n+            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            high_res_image = to_channel_dimension_format(\n+                high_res_image, data_format, input_channel_dim=input_data_format\n+            )\n+\n+            all_images.append(image)\n+            all_high_res_images.append(high_res_image)\n+\n+        data = {\"pixel_values\": all_images, \"high_res_pixel_values\": all_high_res_images}\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def pad_to_square(\n+        self,\n+        image: np.ndarray,\n+        background_color: Union[int, tuple[int, int, int]] = 0,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.array:\n+        \"\"\"\n+        Pads an image to a square based on the longest edge.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                The image to pad.\n+            background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n+                The color to use for the padding. Can be an integer for single channel or a\n+                tuple of integers representing for multi-channel images. If passed as integer\n+                in mutli-channel mode, it will default to `0` in subsequent channels.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+\n+        Returns:\n+            `np.ndarray`: The padded image.\n+        \"\"\"\n+        height, width = get_image_size(image, input_data_format)\n+        num_channels = image.shape[0] if input_data_format == ChannelDimension.FIRST else image.shape[-1]\n+\n+        if height == width:\n+            image = (\n+                to_channel_dimension_format(image, data_format, input_data_format)\n+                if data_format is not None\n+                else image\n+            )\n+            return image\n+\n+        max_dim = max(height, width)\n+\n+        # Ensure background_color is the correct shape\n+        if isinstance(background_color, int):\n+            background_color = [background_color]\n+        elif len(background_color) != num_channels:\n+            raise ValueError(\n+                f\"background_color must have no more than {num_channels} elements to match the number of channels\"\n+            )\n+\n+        if input_data_format == ChannelDimension.FIRST:\n+            result = np.zeros((num_channels, max_dim, max_dim), dtype=image.dtype)\n+            for i, color in enumerate(background_color):\n+                result[i, :, :] = color\n+            if width > height:\n+                start = (max_dim - height) // 2\n+                result[:, start : start + height, :] = image\n+            else:\n+                start = (max_dim - width) // 2\n+                result[:, :, start : start + width] = image\n+        else:\n+            result = np.zeros((max_dim, max_dim, num_channels), dtype=image.dtype)\n+            for i, color in enumerate(background_color):\n+                result[:, :, i] = color\n+            if width > height:\n+                start = (max_dim - height) // 2\n+                result[start : start + height, :, :] = image\n+            else:\n+                start = (max_dim - width) // 2\n+                result[:, start : start + width, :] = image\n+\n+        return result\n+\n+    def postprocess(self):\n+        \"\"\"Applies post-processing to the decoded image tokens by reversing transformations applied during preprocessing.\"\"\"\n+        raise AttributeError(\"Not needed for DeepseekVLHybrid\")\n+\n+\n+__all__ = [\"DeepseekVLHybridImageProcessor\"]"
        },
        {
            "sha": "67b67371f95254d06e34bb0e7cb86e7d066c82dc",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "added",
            "additions": 491,
            "deletions": 0,
            "changes": 491,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,491 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_deepseek_vl_hybrid.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import ModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TransformersKwargs,\n+    auto_docstring,\n+    can_return_tuple,\n+)\n+from ..auto import AutoModel\n+from .configuration_deepseek_vl_hybrid import DeepseekVLHybridConfig\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for DeepseekVLHybrid model's outputs that may also contain a past key/values (to speed up sequential decoding).\n+    \"\"\"\n+)\n+class DeepseekVLHybridBaseModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the model.\n+\n+        If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n+        hidden_size)` is output.\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n+        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n+        encoder_sequence_length, embed_size_per_head)`.\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n+        `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n+        input) to speed up sequential decoding.\n+    image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+        sequence_length, hidden_size)`.\n+\n+        image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for DeepseekVLHybrid causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n+class DeepseekVLHybridCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+        sequence_length, hidden_size)`.\n+\n+        image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+\n+\n+class DeepseekVLHybridLayerNorm(nn.Module):\n+    r\"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n+    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, height,\n+    width, channels) while channels_first corresponds to inputs with shape (batch_size, channels, height, width).\n+    \"\"\"\n+\n+    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(normalized_shape))\n+        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n+        self.eps = eps\n+        self.data_format = data_format\n+        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n+            raise NotImplementedError(f\"Unsupported data format: {self.data_format}\")\n+        self.normalized_shape = (normalized_shape,)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        if self.data_format == \"channels_last\":\n+            x = torch.nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n+        elif self.data_format == \"channels_first\":\n+            input_dtype = x.dtype\n+            x = x.float()\n+            u = x.mean(1, keepdim=True)\n+            s = (x - u).pow(2).mean(1, keepdim=True)\n+            x = (x - u) / torch.sqrt(s + self.eps)\n+            x = x.to(dtype=input_dtype)\n+            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n+        return x\n+\n+\n+class DeepseekVLSamVisionNeck(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+\n+        self.conv1 = nn.Conv2d(config.hidden_size, config.output_channels, kernel_size=1, bias=False)\n+        self.layer_norm1 = DeepseekVLHybridLayerNorm(config.output_channels, data_format=\"channels_first\")\n+        self.conv2 = nn.Conv2d(config.output_channels, config.output_channels, kernel_size=3, padding=1, bias=False)\n+        self.layer_norm2 = DeepseekVLHybridLayerNorm(config.output_channels, data_format=\"channels_first\")\n+\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.permute(0, 3, 1, 2)\n+        hidden_states = self.conv1(hidden_states)\n+        hidden_states = self.layer_norm1(hidden_states)\n+\n+        hidden_states = self.conv2(hidden_states)\n+        hidden_states = self.layer_norm2(hidden_states)\n+        return hidden_states\n+\n+\n+class DeepseekVLSamVisionProj(nn.Module):\n+    def __init__(self, config, output_size: int = 24):\n+        super().__init__()\n+        self.config = config\n+        self.output_size = output_size\n+\n+        self.conv1 = nn.Conv2d(\n+            config.output_channels, config.output_channels * 2, kernel_size=3, stride=2, padding=1, bias=False\n+        )\n+        self.conv2 = nn.Conv2d(\n+            config.output_channels * 2, config.output_channels * 4, kernel_size=3, stride=2, padding=1, bias=False\n+        )\n+\n+    def forward(self, features: torch.Tensor) -> torch.Tensor:\n+        # interpolate Sam encodings to match Siglip encodings\n+        features = torch.nn.functional.interpolate(\n+            features,\n+            size=(4 * self.output_size, 4 * self.output_size),\n+            mode=\"bilinear\",\n+            align_corners=False,\n+        )\n+        features = self.conv1(features)\n+        features = self.conv2(features)\n+        return features\n+\n+\n+class DeepseekVLHybridAligner(nn.Module):\n+    def __init__(self, config: DeepseekVLHybridConfig):\n+        super().__init__()\n+\n+        in_channels = config.vision_config.hidden_size\n+        high_res_in_channels = config.high_res_vision_config.output_channels * 4\n+        out_channels = config.text_config.hidden_size\n+\n+        self.vision_proj = nn.Linear(in_channels, out_channels // 2)\n+        self.high_res_vision_proj = nn.Linear(high_res_in_channels, out_channels // 2)\n+\n+        self.act = nn.GELU()\n+        self.proj = nn.Linear(out_channels, out_channels)\n+\n+    def forward(\n+        self,\n+        vision_encodings: torch.Tensor,\n+        high_res_vision_encodings: torch.Tensor,\n+    ) -> torch.Tensor:\n+        vision_encodings = self.vision_proj(vision_encodings)\n+        high_res_vision_encodings = self.high_res_vision_proj(high_res_vision_encodings)\n+\n+        encodings = torch.concat([high_res_vision_encodings, vision_encodings], dim=-1)\n+        encodings = self.act(encodings)\n+        encodings = self.proj(encodings)\n+\n+        return encodings\n+\n+\n+@auto_docstring\n+class DeepseekVLHybridPreTrainedModel(PreTrainedModel):\n+    config: DeepseekVLHybridConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"LlamaDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+\n+    _supports_static_cache = True\n+    _supports_param_buffer_assignment = False\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Conv2d):\n+            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, DeepseekVLHybridLayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, DeepseekVLHybridModel):\n+            module.high_res_vision_alpha.data.zero_()\n+\n+\n+DEEPSEEK_VL_COMMON_CUSTOM_ARGS = r\"\"\"\n+    high_res_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size), *optional*):\n+        The tensors corresponding to the input images. Pixel values can be obtained using\n+        [`AutoImageProcessor`].\n+\"\"\"\n+\n+\n+@auto_docstring\n+class DeepseekVLHybridModel(DeepseekVLHybridPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.output_size = config.vision_config.image_size // config.vision_config.patch_size\n+        self.global_attn_index = config.high_res_vision_config.global_attn_indexes[0]\n+\n+        self.high_res_vision_model = AutoModel.from_config(config.high_res_vision_config)\n+        self.high_res_vision_neck = DeepseekVLSamVisionNeck(config.high_res_vision_config)\n+        self.high_res_vision_proj = DeepseekVLSamVisionProj(\n+            config.high_res_vision_config, output_size=self.output_size\n+        )\n+        self.high_res_vision_alpha = nn.Parameter(torch.zeros(1))\n+        self.config = config\n+\n+        self.vision_model = AutoModel.from_config(config.vision_config)\n+        self.aligner = DeepseekVLHybridAligner(config)\n+\n+        self.language_model = AutoModel.from_config(config=config.text_config)\n+\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing.\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_image_features(self, pixel_values, high_res_pixel_values):\n+        vision_encodings = self.get_low_res_image_features(pixel_values)\n+        high_res_vision_encodings = self.get_high_res_image_features(high_res_pixel_values)\n+        images_embeds = self.aligner(vision_encodings, high_res_vision_encodings)\n+        return images_embeds\n+\n+    @can_return_tuple\n+    @auto_docstring(custom_args=DEEPSEEK_VL_COMMON_CUSTOM_ARGS)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        high_res_pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ):\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if pixel_values is not None and high_res_pixel_values is None:\n+            raise ValueError(\"Both pixel_values and high_res_pixel_values should be specified at the same time\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            if input_ids is None:\n+                image_attention_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                image_attention_mask = image_attention_mask.all(-1)\n+            else:\n+                image_attention_mask = input_ids == self.config.image_token_id\n+\n+            image_attention_mask = image_attention_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_embeds = self.get_image_features(pixel_values, high_res_pixel_values)\n+            image_features = image_embeds.reshape(-1, inputs_embeds.shape[-1])\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(image_attention_mask, image_features)\n+\n+        lm_output = self.language_model(\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        return DeepseekVLHybridBaseModelOutputWithPast(\n+            last_hidden_state=lm_output.last_hidden_state,\n+            past_key_values=lm_output.past_key_values,\n+            hidden_states=lm_output.hidden_states,\n+            attentions=lm_output.attentions,\n+            image_hidden_states=image_embeds if pixel_values is not None else None,\n+        )\n+\n+    def get_low_res_image_features(self, pixel_values):\n+        output = self.vision_model(pixel_values)\n+        output = output[0]\n+        return output\n+\n+    def get_high_res_image_features(self, pixel_values):\n+        output = self.high_res_vision_model(\n+            pixel_values=pixel_values,\n+            output_hidden_states=True,\n+            return_dict=True,\n+        )\n+        last_hidden_state = output.last_hidden_state\n+        last_hidden_state = self.high_res_vision_proj(last_hidden_state)\n+\n+        hidden_states = output.hidden_states\n+        global_hidden_state = hidden_states[self.global_attn_index + 1]  # +1 for embedding layer\n+        global_hidden_state = self.high_res_vision_neck(global_hidden_state)\n+        global_hidden_state = self.high_res_vision_proj(global_hidden_state)\n+\n+        output = last_hidden_state + global_hidden_state * self.high_res_vision_alpha\n+\n+        # batch_size, hidden_size, height, width -> batch_size, seq_len, hidden_size\n+        output = output.permute(0, 2, 3, 1)\n+        output = output.reshape(output.shape[0], -1, output.shape[-1])\n+\n+        return output\n+\n+\n+class DeepseekVLHybridForConditionalGeneration(DeepseekVLHybridPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n+    _supports_static_cache = True\n+\n+    def __init__(self, config: DeepseekVLHybridConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.model = DeepseekVLHybridModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing.\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.language_model.set_input_embeddings(value)\n+\n+    def prepare_embeddings_for_image_generation(self) -> torch.Tensor:\n+        raise AttributeError(\"Not needed for DeepseekVLHybrid\")\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @auto_docstring(custom_args=DEEPSEEK_VL_COMMON_CUSTOM_ARGS)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        high_res_pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ):\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        \"\"\"\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            high_res_pixel_values=high_res_pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n+\n+        return DeepseekVLHybridCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        high_res_pixel_values=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+            model_inputs[\"high_res_pixel_values\"] = high_res_pixel_values\n+\n+        return model_inputs\n+\n+\n+__all__ = [\"DeepseekVLHybridPreTrainedModel\", \"DeepseekVLHybridModel\", \"DeepseekVLHybridForConditionalGeneration\"]"
        },
        {
            "sha": "aa0a4f87ba3e0c8b2699d1207adde832d3c24da2",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "added",
            "additions": 777,
            "deletions": 0,
            "changes": 777,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,777 @@\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...cache_utils import Cache\n+from ...image_processing_utils_fast import (\n+    BatchFeature,\n+    get_size_dict,\n+)\n+from ...image_transforms import convert_to_rgb, to_channel_dimension_format\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    make_flat_list_of_images,\n+    make_list_of_images,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...processing_utils import Unpack\n+from ...tokenization_utils_base import (\n+    PreTokenizedInput,\n+    TextInput,\n+)\n+from ...utils import (\n+    TensorType,\n+    TransformersKwargs,\n+    auto_docstring,\n+    can_return_tuple,\n+    filter_out_non_signature_kwargs,\n+    logging,\n+)\n+from ..auto import CONFIG_MAPPING, AutoConfig, AutoModel\n+from ..deepseek_vl.configuration_deepseek_vl import DeepseekVLConfig\n+from ..deepseek_vl.image_processing_deepseek_vl import DeepseekVLImageProcessor\n+from ..deepseek_vl.modeling_deepseek_vl import (\n+    DeepseekVLForConditionalGeneration,\n+    DeepseekVLModel,\n+    DeepseekVLPreTrainedModel,\n+)\n+from ..deepseek_vl.processing_deepseek_vl import DeepseekVLProcessor, DeepseekVLProcessorKwargs\n+from ..idefics.modeling_idefics import IdeficsBaseModelOutputWithPast, IdeficsCausalLMOutputWithPast\n+from ..sam.modeling_sam import SamLayerNorm, SamVisionNeck\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+DEEPSEEK_VL_COMMON_CUSTOM_ARGS = r\"\"\"\n+    high_res_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size), *optional*):\n+        The tensors corresponding to the input images. Pixel values can be obtained using\n+        [`AutoImageProcessor`].\n+\"\"\"\n+\n+\n+class DeepseekVLHybridConfig(DeepseekVLConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DeepseekVLHybridModel`]. It is used to instantiate a\n+    DeepseekVLHybrid model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the DeepseekVLHybrid\n+    [deepseek-community/deepseek-vl-7b-chat](https://huggingface.co/deepseek-community/deepseek-vl-7b-chat) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SiglipVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        high_res_vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SamVisionConfig`):\n+            The config object or dictionary of the high resolution vision backbone.\n+        image_token_id (`int`, *optional*, defaults to 100015):\n+            The index representing image tokens in the model's token vocabulary.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import DeepseekVLHybridConfig, DeepseekVLHybridModel\n+\n+    >>> # Initializing a DeepseekVLHybrid deepseek-community/deepseek-vl-7b-chat style configuration\n+    >>> configuration = DeepseekVLHybridConfig()\n+\n+    >>> # Initializing a model (with random weights) from the deepseek-community/deepseek-vl-7b-chat style configuration\n+    >>> model = DeepseekVLHybridModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"deepseek_vl_hybrid\"\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig, \"high_res_vision_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        text_config: AutoConfig = None,\n+        vision_config: AutoConfig = None,\n+        high_res_vision_config: AutoConfig = None,\n+        image_token_id: int = 100015,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            text_config=text_config,\n+            vision_config=vision_config,\n+            image_token_id=image_token_id,\n+            **kwargs,\n+        )\n+\n+        if high_res_vision_config is None:\n+            high_res_vision_config = {}\n+            logger.info(\"`high_res_vision_config` is `None`. Initializing the `SamVisionConfig` with default values.\")\n+\n+        if isinstance(high_res_vision_config, dict):\n+            high_res_vision_config[\"model_type\"] = high_res_vision_config.get(\"model_type\", \"sam_vision_model\")\n+            high_res_vision_config = CONFIG_MAPPING[high_res_vision_config[\"model_type\"]](**high_res_vision_config)\n+\n+        self.high_res_vision_config = high_res_vision_config\n+\n+\n+class DeepseekVLHybridBaseModelOutputWithPast(IdeficsBaseModelOutputWithPast):\n+    pass\n+\n+\n+class DeepseekVLHybridCausalLMOutputWithPast(IdeficsCausalLMOutputWithPast):\n+    pass\n+\n+\n+class DeepseekVLHybridLayerNorm(SamLayerNorm):\n+    pass\n+\n+\n+class DeepseekVLSamVisionNeck(SamVisionNeck):\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+\n+class DeepseekVLSamVisionProj(nn.Module):\n+    def __init__(self, config, output_size: int = 24):\n+        super().__init__()\n+        self.config = config\n+        self.output_size = output_size\n+\n+        self.conv1 = nn.Conv2d(\n+            config.output_channels, config.output_channels * 2, kernel_size=3, stride=2, padding=1, bias=False\n+        )\n+        self.conv2 = nn.Conv2d(\n+            config.output_channels * 2, config.output_channels * 4, kernel_size=3, stride=2, padding=1, bias=False\n+        )\n+\n+    def forward(self, features: torch.Tensor) -> torch.Tensor:\n+        # interpolate Sam encodings to match Siglip encodings\n+        features = torch.nn.functional.interpolate(\n+            features,\n+            size=(4 * self.output_size, 4 * self.output_size),\n+            mode=\"bilinear\",\n+            align_corners=False,\n+        )\n+        features = self.conv1(features)\n+        features = self.conv2(features)\n+        return features\n+\n+\n+class DeepseekVLHybridAligner(nn.Module):\n+    def __init__(self, config: DeepseekVLHybridConfig):\n+        super().__init__()\n+\n+        in_channels = config.vision_config.hidden_size\n+        high_res_in_channels = config.high_res_vision_config.output_channels * 4\n+        out_channels = config.text_config.hidden_size\n+\n+        self.vision_proj = nn.Linear(in_channels, out_channels // 2)\n+        self.high_res_vision_proj = nn.Linear(high_res_in_channels, out_channels // 2)\n+\n+        self.act = nn.GELU()\n+        self.proj = nn.Linear(out_channels, out_channels)\n+\n+    def forward(\n+        self,\n+        vision_encodings: torch.Tensor,\n+        high_res_vision_encodings: torch.Tensor,\n+    ) -> torch.Tensor:\n+        vision_encodings = self.vision_proj(vision_encodings)\n+        high_res_vision_encodings = self.high_res_vision_proj(high_res_vision_encodings)\n+\n+        encodings = torch.concat([high_res_vision_encodings, vision_encodings], dim=-1)\n+        encodings = self.act(encodings)\n+        encodings = self.proj(encodings)\n+\n+        return encodings\n+\n+\n+class DeepseekVLHybridPreTrainedModel(DeepseekVLPreTrainedModel):\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Conv2d):\n+            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, DeepseekVLHybridLayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, DeepseekVLHybridModel):\n+            module.high_res_vision_alpha.data.zero_()\n+\n+\n+class DeepseekVLHybridModel(DeepseekVLModel):\n+    def __init__(self, config):\n+        self.output_size = config.vision_config.image_size // config.vision_config.patch_size\n+        self.global_attn_index = config.high_res_vision_config.global_attn_indexes[0]\n+\n+        self.high_res_vision_model = AutoModel.from_config(config.high_res_vision_config)\n+        self.high_res_vision_neck = DeepseekVLSamVisionNeck(config.high_res_vision_config)\n+        self.high_res_vision_proj = DeepseekVLSamVisionProj(\n+            config.high_res_vision_config, output_size=self.output_size\n+        )\n+        self.high_res_vision_alpha = nn.Parameter(torch.zeros(1))\n+\n+        super().__init__(config)\n+\n+    def get_low_res_image_features(self, pixel_values):\n+        output = self.vision_model(pixel_values)\n+        output = output[0]\n+        return output\n+\n+    def get_high_res_image_features(self, pixel_values):\n+        output = self.high_res_vision_model(\n+            pixel_values=pixel_values,\n+            output_hidden_states=True,\n+            return_dict=True,\n+        )\n+        last_hidden_state = output.last_hidden_state\n+        last_hidden_state = self.high_res_vision_proj(last_hidden_state)\n+\n+        hidden_states = output.hidden_states\n+        global_hidden_state = hidden_states[self.global_attn_index + 1]  # +1 for embedding layer\n+        global_hidden_state = self.high_res_vision_neck(global_hidden_state)\n+        global_hidden_state = self.high_res_vision_proj(global_hidden_state)\n+\n+        output = last_hidden_state + global_hidden_state * self.high_res_vision_alpha\n+\n+        # batch_size, hidden_size, height, width -> batch_size, seq_len, hidden_size\n+        output = output.permute(0, 2, 3, 1)\n+        output = output.reshape(output.shape[0], -1, output.shape[-1])\n+\n+        return output\n+\n+    def get_image_features(self, pixel_values, high_res_pixel_values):\n+        vision_encodings = self.get_low_res_image_features(pixel_values)\n+        high_res_vision_encodings = self.get_high_res_image_features(high_res_pixel_values)\n+        images_embeds = self.aligner(vision_encodings, high_res_vision_encodings)\n+        return images_embeds\n+\n+    @can_return_tuple\n+    @auto_docstring(custom_args=DEEPSEEK_VL_COMMON_CUSTOM_ARGS)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        high_res_pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ):\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if pixel_values is not None and high_res_pixel_values is None:\n+            raise ValueError(\"Both pixel_values and high_res_pixel_values should be specified at the same time\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            if input_ids is None:\n+                image_attention_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                image_attention_mask = image_attention_mask.all(-1)\n+            else:\n+                image_attention_mask = input_ids == self.config.image_token_id\n+\n+            image_attention_mask = image_attention_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_embeds = self.get_image_features(pixel_values, high_res_pixel_values)\n+            image_features = image_embeds.reshape(-1, inputs_embeds.shape[-1])\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(image_attention_mask, image_features)\n+\n+        lm_output = self.language_model(\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        return DeepseekVLHybridBaseModelOutputWithPast(\n+            last_hidden_state=lm_output.last_hidden_state,\n+            past_key_values=lm_output.past_key_values,\n+            hidden_states=lm_output.hidden_states,\n+            attentions=lm_output.attentions,\n+            image_hidden_states=image_embeds if pixel_values is not None else None,\n+        )\n+\n+\n+class DeepseekVLHybridForConditionalGeneration(DeepseekVLForConditionalGeneration):\n+    @can_return_tuple\n+    @auto_docstring(custom_args=DEEPSEEK_VL_COMMON_CUSTOM_ARGS)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        high_res_pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ):\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        \"\"\"\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            high_res_pixel_values=high_res_pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n+\n+        return DeepseekVLHybridCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        high_res_pixel_values=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+            model_inputs[\"high_res_pixel_values\"] = high_res_pixel_values\n+\n+        return model_inputs\n+\n+\n+class DeepseekVLHybridImageProcessor(DeepseekVLImageProcessor):\n+    r\"\"\"\n+    Constructs a DEEPSEEK_VL_HYBRID image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n+            `do_resize` parameter in the `preprocess` method.\n+        size (`dict`, *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n+            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n+            method.\n+        high_res_size (`dict`, *optional*, defaults to `{\"height\": 1024, \"width\": 1024}`):\n+            Size of the high resolution output image after resizing. Can be overridden by the `high_res_size` parameter in the `preprocess`\n+            method.\n+        min_size (`int`, *optional*, defaults to 14):\n+            The minimum allowed size for the resized image. Ensures that neither the height nor width\n+            falls below this value after resizing.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n+            overridden by the `resample` parameter in the `preprocess` method.\n+        high_res_resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n+            overridden by the `high_res_resample` parameter in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n+            `do_rescale` parameter in the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Only has an effect if `do_rescale` is set to `True`. Can be\n+            overridden by the `rescale_factor` parameter in the `preprocess` method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n+            method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n+            overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        high_res_image_mean (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_MEAN`):\n+            Mean to use if normalizing the high resolution image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `high_res_image_mean` parameter in the `preprocess` method.\n+        high_res_image_std (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_STD`):\n+            Standard deviation to use if normalizing the high resolution image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `high_res_image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Optional[dict[str, int]] = None,\n+        high_res_size: Optional[dict[str, int]] = None,\n+        min_size: int = 14,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        high_res_resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        high_res_image_mean: Optional[Union[float, list[float]]] = None,\n+        high_res_image_std: Optional[Union[float, list[float]]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        **kwargs,\n+    ) -> None:\n+        high_res_size = high_res_size if high_res_size is not None else {\"height\": 1024, \"width\": 1024}\n+        high_res_size = get_size_dict(high_res_size, default_to_square=True)\n+\n+        self.high_res_size = high_res_size\n+        self.high_res_image_mean = high_res_image_mean if high_res_image_mean is not None else OPENAI_CLIP_MEAN\n+        self.high_res_image_std = high_res_image_std if high_res_image_std is not None else OPENAI_CLIP_STD\n+\n+        self.resample = resample\n+        self.high_res_resample = high_res_resample\n+\n+        super().__init__(\n+            do_resize=do_resize,\n+            size=size,\n+            min_size=min_size,\n+            resample=resample,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_convert_rgb=do_convert_rgb,\n+            **kwargs,\n+        )\n+\n+        if high_res_image_mean is None:\n+            self.background_color = (127, 127, 127)\n+        else:\n+            self.background_color = tuple([int(x * 255) for x in high_res_image_mean])\n+\n+    @filter_out_non_signature_kwargs()\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[dict[str, int]] = None,\n+        high_res_size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = None,\n+        high_res_resample: PILImageResampling = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        high_res_image_mean: Optional[Union[float, list[float]]] = None,\n+        high_res_image_std: Optional[Union[float, list[float]]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+    ):\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Dictionary in the format `{\"height\": h, \"width\": w}` specifying the size of the output image after\n+                resizing.\n+            high_res_size (`Dict[str, int]`, *optional*, defaults to `self.high_res_size`):\n+                Dictionary in the format `{\"height\": h, \"width\": w}` specifying the size of the high resolution output image after\n+                resizing.\n+            resample (`PILImageResampling` filter, *optional*, defaults to `self.resample`):\n+                `PILImageResampling` filter to use if resizing the image e.g. `PILImageResampling.BILINEAR`. Only has\n+                an effect if `do_resize` is set to `True`.\n+            high_res_resample (`PILImageResampling` filter, *optional*, defaults to `self.resample`):\n+                `PILImageResampling` filter to use if resizing the image e.g. `PILImageResampling.BICUBIC`. Only has\n+                an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image values between [0 - 1].\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use if `do_normalize` is set to `True`.\n+            high_res_image_mean (`float` or `List[float]`, *optional*, defaults to `self.high_res_image_mean`):\n+                Image mean to use if `do_normalize` is set to `True`.\n+            high_res_image_std (`float` or `List[float]`, *optional*, defaults to `self.high_res_image_std`):\n+                Image standard deviation to use if `do_normalize` is set to `True`.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        resample = resample if resample is not None else self.resample\n+        high_res_resample = high_res_resample if high_res_resample is not None else self.high_res_resample\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        high_res_image_mean = high_res_image_mean if high_res_image_mean is not None else self.high_res_image_mean\n+        high_res_image_std = high_res_image_std if high_res_image_std is not None else self.high_res_image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+\n+        size = size if size is not None else self.size\n+        size_dict = get_size_dict(size)\n+        high_res_size = high_res_size if high_res_size is not None else self.high_res_size\n+        high_res_size_dict = get_size_dict(high_res_size)\n+\n+        images = make_list_of_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if do_rescale and is_scaled_image(images[0]):\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        all_images = []\n+        all_high_res_images = []\n+        for image in images:\n+            # high_res_image: resize (high) -> rescale -> normalize (high)\n+            # low_res_image:  resize (high) -> rescale -> resize (low) -> normalize (low)\n+            high_res_image = image\n+\n+            if do_resize:\n+                high_res_image = self.resize(\n+                    image=high_res_image,\n+                    size=high_res_size_dict,\n+                    resample=high_res_resample,\n+                    input_data_format=input_data_format,\n+                )\n+                image = self.resize(\n+                    image=high_res_image, size=size_dict, resample=resample, input_data_format=input_data_format\n+                )\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+                high_res_image = self.rescale(\n+                    image=high_res_image, scale=rescale_factor, input_data_format=input_data_format\n+                )\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+                high_res_image = self.normalize(\n+                    image=high_res_image,\n+                    mean=high_res_image_mean,\n+                    std=high_res_image_std,\n+                    input_data_format=input_data_format,\n+                )\n+\n+            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            high_res_image = to_channel_dimension_format(\n+                high_res_image, data_format, input_channel_dim=input_data_format\n+            )\n+\n+            all_images.append(image)\n+            all_high_res_images.append(high_res_image)\n+\n+        data = {\"pixel_values\": all_images, \"high_res_pixel_values\": all_high_res_images}\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+class DeepseekVLHybridProcessorKwargs(DeepseekVLProcessorKwargs):\n+    pass\n+\n+\n+class DeepseekVLHybridProcessor(DeepseekVLProcessor):\n+    def __call__(\n+        self,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput = None,\n+        **kwargs: Unpack[DeepseekVLHybridProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        DeepseekVLHybridImageProcessor's [`~DeepseekVLHybridImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        of the above two methods for more information.\n+\n+        Args:\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+            `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            DeepseekVLHybridProcessorKwargs, tokenizer_init_kwargs=self.tokenizer.init_kwargs, **kwargs\n+        )\n+        if text is None and images is None:\n+            raise ValueError(\"You must specify either text or images.\")\n+\n+        if text is not None:\n+            if isinstance(text, str):\n+                text = [text]\n+            elif not (isinstance(text, (list, tuple)) and all(isinstance(t, str) for t in text)):\n+                raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        prompt_strings = []\n+        one_img_tokens = self.image_token * self.num_image_tokens\n+        for prompt in text:\n+            prompt = prompt.replace(self.image_token, one_img_tokens)\n+            prompt_strings.append(prompt)\n+\n+        data = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+\n+        # process images if pixel_values are provided\n+        if images is not None:\n+            images = make_flat_list_of_images(images)\n+            inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+            data[\"pixel_values\"] = inputs[\"pixel_values\"]\n+            data[\"high_res_pixel_values\"] = inputs[\"high_res_pixel_values\"]\n+\n+        return BatchFeature(data=data)\n+\n+\n+__all__ = [\n+    \"DeepseekVLHybridConfig\",\n+    \"DeepseekVLHybridPreTrainedModel\",\n+    \"DeepseekVLHybridModel\",\n+    \"DeepseekVLHybridForConditionalGeneration\",\n+    \"DeepseekVLHybridImageProcessor\",\n+    \"DeepseekVLHybridProcessor\",\n+]"
        },
        {
            "sha": "4fb765c79764c0bf366d95caa1470d9c78817f46",
            "filename": "src/transformers/models/deepseek_vl_hybrid/processing_deepseek_vl_hybrid.py",
            "status": "added",
            "additions": 159,
            "deletions": 0,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,159 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_deepseek_vl_hybrid.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Union\n+\n+from ...image_processing_utils_fast import BatchFeature\n+from ...image_utils import ImageInput, make_flat_list_of_images\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+\n+class DeepseekVLHybridProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\"padding\": False},\n+        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+    }\n+\n+\n+class DeepseekVLHybridProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a DeepseekVLHybrid processor which wraps a DeepseekVLHybrid Image Processor and a Llama tokenizer into a single processor.\n+\n+    [`DeepseekVLHybridProcessor`] offers all the functionalities of [`DeepseekVLHybridImageProcessor`] and [`LlamaTokenizerFast`]. See the\n+    [`~DeepseekVLHybridProcessor.__call__`] and [`~DeepseekVLHybridProcessor.decode`] for more information.\n+\n+    Args:\n+        image_processor ([`DeepseekVLHybridImageProcessor`]):\n+            The image processor is a required input.\n+        tokenizer ([`LlamaTokenizerFast`]):\n+            The tokenizer is a required input.\n+        chat_template (`str`, *optional*):\n+            A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+        num_image_tokens (`int`, *optional*, defaults to 576):\n+            The number of special image tokens used as placeholders for visual content in text sequences.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\"chat_template\", \"num_image_tokens\"]\n+    image_processor_class = \"AutoImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        image_processor,\n+        tokenizer,\n+        chat_template=None,\n+        num_image_tokens=576,\n+    ):\n+        self.image_token = tokenizer.image_token\n+        self.num_image_tokens = num_image_tokens\n+\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+\n+    def __call__(\n+        self,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        images: ImageInput = None,\n+        **kwargs: Unpack[DeepseekVLHybridProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        DeepseekVLHybridImageProcessor's [`~DeepseekVLHybridImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        of the above two methods for more information.\n+\n+        Args:\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+            `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            DeepseekVLHybridProcessorKwargs, tokenizer_init_kwargs=self.tokenizer.init_kwargs, **kwargs\n+        )\n+        if text is None and images is None:\n+            raise ValueError(\"You must specify either text or images.\")\n+\n+        if text is not None:\n+            if isinstance(text, str):\n+                text = [text]\n+            elif not (isinstance(text, (list, tuple)) and all(isinstance(t, str) for t in text)):\n+                raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        prompt_strings = []\n+        one_img_tokens = self.image_token * self.num_image_tokens\n+        for prompt in text:\n+            prompt = prompt.replace(self.image_token, one_img_tokens)\n+            prompt_strings.append(prompt)\n+\n+        data = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+\n+        # process images if pixel_values are provided\n+        if images is not None:\n+            images = make_flat_list_of_images(images)\n+            inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+            data[\"pixel_values\"] = inputs[\"pixel_values\"]\n+            data[\"high_res_pixel_values\"] = inputs[\"high_res_pixel_values\"]\n+\n+        return BatchFeature(data=data)\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"DeepseekVLHybridProcessor\"]"
        },
        {
            "sha": "ebdc2f23ea67a6fb261298d5eb6937aa16ab22cb",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -1147,7 +1147,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1173,7 +1173,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return JanusCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "11b0848620b7a765a2194dd8e1116eb1dccd6d5b",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -1007,7 +1007,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1033,7 +1033,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n \n         return JanusCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/deepseek_vl/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl%2F__init__.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c"
        },
        {
            "sha": "c1092f05d33695462318844d3ebadb4d9d2f9662",
            "filename": "tests/models/deepseek_vl/test_image_processing_deepseek_vl.py",
            "status": "added",
            "additions": 119,
            "deletions": 0,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl%2Ftest_image_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl%2Ftest_image_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl%2Ftest_image_processing_deepseek_vl.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,119 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_vision_available():\n+    from transformers import DeepseekVLImageProcessor\n+\n+\n+# Copied from tests.models.vit.test_image_processing_vit.ViTImageProcessingTester with ViT->DeepseekVL\n+class DeepseekVLImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+    ):\n+        size = size if size is not None else {\"height\": 18, \"width\": 18}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_normalize\": self.do_normalize,\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+        }\n+\n+    # Ignore copy\n+    def expected_output_image_shape(self, images):\n+        max_size = max(self.size[\"height\"], self.size[\"width\"])\n+        return self.num_channels, max_size, max_size\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+# Copied from tests.models.vit.test_image_processing_vit.ViTImageProcessingTest with ViT->DeepseekVL\n+class DeepseekVLImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    # Ignore copy\n+    image_processing_class = DeepseekVLImageProcessor if is_vision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = DeepseekVLImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n+\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+\n+    # Ignore copy\n+    @unittest.skip(reason=\"Not supported\")\n+    def test_call_numpy_4_channels(self):\n+        pass"
        },
        {
            "sha": "bff23e9dd5da5c6ac2b68a986012e34741df0f9c",
            "filename": "tests/models/deepseek_vl/test_modeling_deepseek_vl.py",
            "status": "added",
            "additions": 359,
            "deletions": 0,
            "changes": 359,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,359 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch DeepseekVL model.\"\"\"\n+\n+import re\n+import tempfile\n+import unittest\n+\n+from transformers import (\n+    AutoProcessor,\n+    DeepseekVLConfig,\n+    DeepseekVLForConditionalGeneration,\n+    DeepseekVLModel,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class DeepseekVLModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=2,\n+        seq_length=25,\n+        num_channels=3,\n+        initializer_range=0.02,\n+        is_training=True,\n+        use_cache=False,\n+        text_config={\n+            \"num_hidden_layers\": 2,\n+            \"vocab_size\": 99,\n+            \"hidden_size\": 16,\n+            \"intermediate_size\": 37,\n+            \"max_position_embeddings\": 512,\n+            \"num_attention_heads\": 4,\n+            \"pad_token_id\": 1,\n+        },\n+        vision_config={\n+            \"num_hidden_layers\": 1,\n+            \"hidden_size\": 16,\n+            \"intermediate_size\": 37,\n+            \"image_size\": 32,\n+            \"patch_size\": 8,\n+            \"hidden_act\": \"gelu\",\n+            \"vision_use_head\": False,\n+            \"num_attention_heads\": 4,\n+        },\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.num_channels = num_channels\n+        self.initializer_range = initializer_range\n+        self.is_training = is_training\n+        self.use_cache = use_cache\n+\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.vision_config[\"num_channels\"] = self.num_channels\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.image_size = vision_config[\"image_size\"]\n+        self.num_image_tokens = vision_config[\"image_size\"] // vision_config[\"patch_size\"]\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n+        self.image_token_id = self.vocab_size - 1\n+\n+    def get_config(self):\n+        return DeepseekVLConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            image_token_id=self.image_token_id,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+\n+        # create text and vision inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 2) + 1\n+        attention_mask = random_attention_mask([self.batch_size, self.seq_length])\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.num_channels,\n+                self.image_size,\n+                self.image_size,\n+            ]\n+        )\n+        # fill image_tokens\n+        input_ids[:, : self.num_image_tokens] = self.image_token_id\n+\n+        return config, input_ids, attention_mask, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_ids, attention_mask, pixel_values = config_and_inputs\n+        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class DeepseekVLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    all_model_classes = (DeepseekVLModel, DeepseekVLForConditionalGeneration) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": DeepseekVLModel,\n+            \"image-text-to-text\": DeepseekVLForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    _is_composite = True\n+    test_pruning = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = DeepseekVLModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=DeepseekVLConfig, has_text_modality=False)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for VLMs.\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            torch.testing.assert_close(out_embeds, out_ids)\n+\n+    @unittest.skip(reason=\"Siglip uses the same initialization scheme as the Flax original implementation\")\n+    # Copied from tests.models.siglip.test_modeling_siglip.SiglipVisionModelTest.test_initialization\n+    def test_initialization(self):\n+        pass\n+\n+    @require_torch_sdpa\n+    # Copied from tests.models.janus.test_modeling_janus.JanusVisionText2TextModelTest.test_sdpa_can_dispatch_composite_models\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # Load the model with SDPA\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                # Load model with eager attention\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    attn_implementation=\"eager\",\n+                )\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+            # SigLip has one shared cls attr for all models, so we assign both submodels heer\n+            vision_attn = language_attn = \"sdpa\" if model._supports_sdpa else \"eager\"\n+\n+            if hasattr(model_sdpa, \"vision_model\") and hasattr(model_sdpa, \"language_model\"):\n+                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == vision_attn)\n+                self.assertTrue(model_sdpa.language_model.config._attn_implementation == language_attn)\n+                self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.language_model.config._attn_implementation == \"eager\")\n+\n+            self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+            self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+            for name, submodule in model_eager.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if any(re.finditer(r\"Attention(?!Pool)\", class_name)):\n+                    self.assertTrue(submodule.config._attn_implementation == \"eager\")\n+\n+            for name, submodule in model_sdpa.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if any(re.finditer(r\"Attention(?!Pool)\", class_name)):\n+                    self.assertTrue(submodule.config._attn_implementation == \"sdpa\")\n+\n+\n+@require_torch\n+@require_torch_accelerator\n+@slow\n+class DeepseekVLIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.model_id = \"deepseek-community/deepseek-vl-1.3b-chat\"\n+\n+    def test_model_text_generation(self):\n+        model = DeepseekVLForConditionalGeneration.from_pretrained(\n+            self.model_id, torch_dtype=\"auto\", device_map=\"auto\"\n+        )\n+        model.to(torch_device)\n+        model.eval()\n+        processor = AutoProcessor.from_pretrained(self.model_id)\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe this image.\"},\n+                ],\n+            }\n+        ]\n+        EXPECTED_TEXT = 'You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\\n\\nUser: Describe this image.\\n\\nAssistant:In the image, a majestic snow leopard is captured in a moment of tranquility. The snow leopard'  # fmt: skip\n+\n+        inputs = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        )\n+        inputs = inputs.to(model.device, dtype=model.dtype)\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        text = processor.decode(output[0], skip_special_tokens=True)\n+\n+        self.assertEqual(\n+            text,\n+            EXPECTED_TEXT,\n+        )\n+\n+    def test_model_text_generation_batched(self):\n+        model = DeepseekVLForConditionalGeneration.from_pretrained(\n+            self.model_id, torch_dtype=\"auto\", device_map=\"auto\"\n+        )\n+        model.to(torch_device)\n+        model.eval()\n+        processor = AutoProcessor.from_pretrained(self.model_id)\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"Describe this image.\"},\n+                    ],\n+                }\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"What animal do you see in the image?\"},\n+                    ],\n+                }\n+            ],\n+        ]\n+        EXPECTED_TEXT = [\n+            \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\\n\\nUser: Describe this image.\\n\\nAssistant:In the image, a majestic snow leopard is captured in a moment of tranquility. The snow leopard\",  # fmt: skip\n+            \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\\n\\nUser: What animal do you see in the image?\\n\\nAssistant:I see a bear in the image.What is the significance of the color red in the\",  # fmt: skip\n+        ]\n+\n+        inputs = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, padding=True, return_dict=True, return_tensors=\"pt\"\n+        )\n+        inputs = inputs.to(model.device, dtype=model.dtype)\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        text = processor.batch_decode(output, skip_special_tokens=True)\n+\n+        self.assertEqual(EXPECTED_TEXT, text)\n+\n+    def test_model_text_generation_with_multi_image(self):\n+        model = DeepseekVLForConditionalGeneration.from_pretrained(\n+            self.model_id, torch_dtype=\"auto\", device_map=\"auto\"\n+        )\n+        model.to(torch_device)\n+        model.eval()\n+        processor = AutoProcessor.from_pretrained(self.model_id)\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"What's the difference between\"},\n+                    {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+                    {\"type\": \"text\", \"text\": \" and \"},\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                ],\n+            }\n+        ]\n+        EXPECTED_TEXT = \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\\n\\nUser: What's the difference between and \\n\\nAssistant:The image is a photograph featuring two cats lying on a pink blanket. The cat on the left is\"  # fmt: skip\n+\n+        inputs = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        )\n+        inputs = inputs.to(model.device, dtype=model.dtype)\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        text = processor.decode(output[0], skip_special_tokens=True)\n+\n+        self.assertEqual(\n+            text,\n+            EXPECTED_TEXT,\n+        )"
        },
        {
            "sha": "3c61f377e26424c763ca064fbb6394ce49a1f779",
            "filename": "tests/models/deepseek_vl/test_processor_deepseek_vl.py",
            "status": "added",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl%2Ftest_processor_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl%2Ftest_processor_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl%2Ftest_processor_deepseek_vl.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,54 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import tempfile\n+import unittest\n+\n+from transformers import DeepseekVLProcessor, LlamaTokenizer\n+from transformers.models.deepseek_vl.convert_deepseek_vl_weights_to_hf import CHAT_TEMPLATE\n+from transformers.testing_utils import get_tests_dir\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import DeepseekVLImageProcessor\n+\n+\n+SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n+\n+\n+class DeepseekVLProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = DeepseekVLProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = DeepseekVLImageProcessor()\n+        tokenizer = LlamaTokenizer(\n+            vocab_file=SAMPLE_VOCAB,\n+            extra_special_tokens={\n+                \"pad_token\": \"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\",\n+                \"image_token\": \"<image_placeholder>\",\n+            },\n+        )\n+        processor = self.processor_class(\n+            image_processor=image_processor,\n+            tokenizer=tokenizer,\n+            chat_template=CHAT_TEMPLATE,\n+        )\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def prepare_processor_dict(self):\n+        return {\"chat_template\": CHAT_TEMPLATE, \"num_image_tokens\": 576}"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/deepseek_vl_hybrid/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl_hybrid%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl_hybrid%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2F__init__.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c"
        },
        {
            "sha": "b7eaefd71a816370537be1b06bbf2c6613c52bb4",
            "filename": "tests/models/deepseek_vl_hybrid/test_image_processing_deepseek_vl_hybrid.py",
            "status": "added",
            "additions": 218,
            "deletions": 0,
            "changes": 218,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_image_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_image_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_image_processing_deepseek_vl_hybrid.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,218 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import DeepseekVLHybridImageProcessor\n+\n+\n+class DeepseekVLHybridImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        high_res_size=None,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+        high_res_image_mean=[0.5, 0.5, 0.5],\n+        high_res_image_std=[0.5, 0.5, 0.5],\n+    ):\n+        size = size if size is not None else {\"height\": 18, \"width\": 18}\n+        high_res_size = high_res_size if high_res_size is not None else {\"height\": 36, \"width\": 36}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.high_res_size = high_res_size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.high_res_image_mean = high_res_image_mean\n+        self.high_res_image_std = high_res_image_std\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"high_res_image_mean\": self.high_res_image_mean,\n+            \"high_res_image_std\": self.high_res_image_std,\n+            \"do_normalize\": self.do_normalize,\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"high_res_size\": self.high_res_size,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        max_size = max(self.size[\"height\"], self.size[\"width\"])\n+        return self.num_channels, max_size, max_size\n+\n+    def expected_output_high_res_image_shape(self, images):\n+        max_size = max(self.high_res_size[\"height\"], self.high_res_size[\"width\"])\n+        return self.num_channels, max_size, max_size\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class DeepseekVLHybridImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = DeepseekVLHybridImageProcessor if is_vision_available() else None\n+\n+    # Copied from tests.models.vit.test_image_processing_vit.ViTImageProcessingTester.setUp with ViT->DeepseekVLHybrid\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = DeepseekVLHybridImageProcessingTester(self)\n+\n+    @property\n+    # Copied from tests.models.vit.test_image_processing_vit.ViTImageProcessingTester.image_processor_dict with ViT->DeepseekVLHybrid\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    # Copied from tests.models.vit.test_image_processing_vit.ViTImageProcessingTester.test_image_processor_from_dict_with_kwargs\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n+\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"high_res_image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"high_res_image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"high_res_size\"))\n+\n+    def test_call_pil_high_res(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").high_res_pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_high_res_image_shape(\n+                [image_inputs[0]]\n+            )\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").high_res_pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_high_res_image_shape(\n+                image_inputs\n+            )\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n+\n+    def test_call_numpy_high_res(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").high_res_pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_high_res_image_shape(\n+                [image_inputs[0]]\n+            )\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").high_res_pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_high_res_image_shape(\n+                image_inputs\n+            )\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n+\n+    def test_call_pytorch_high_res(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").high_res_pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_high_res_image_shape(\n+                [image_inputs[0]]\n+            )\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            expected_output_image_shape = self.image_processor_tester.expected_output_high_res_image_shape(\n+                image_inputs\n+            )\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").high_res_pixel_values\n+            self.assertEqual(\n+                tuple(encoded_images.shape),\n+                (self.image_processor_tester.batch_size, *expected_output_image_shape),\n+            )\n+\n+    @unittest.skip(reason=\"Not supported\")\n+    def test_call_numpy_4_channels(self):\n+        pass"
        },
        {
            "sha": "8e68ee19a13b87f81580a443835e8ec953a343c2",
            "filename": "tests/models/deepseek_vl_hybrid/test_modeling_deepseek_vl_hybrid.py",
            "status": "added",
            "additions": 403,
            "deletions": 0,
            "changes": 403,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,403 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch DeepseekVLHybrid model.\"\"\"\n+\n+import re\n+import tempfile\n+import unittest\n+\n+from transformers import (\n+    AutoProcessor,\n+    DeepseekVLHybridConfig,\n+    DeepseekVLHybridForConditionalGeneration,\n+    DeepseekVLHybridModel,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class DeepseekVLHybridModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=2,\n+        seq_length=25,\n+        num_channels=3,\n+        initializer_range=0.02,\n+        is_training=True,\n+        use_cache=False,\n+        text_config={\n+            \"num_hidden_layers\": 2,\n+            \"vocab_size\": 99,\n+            \"hidden_size\": 16,\n+            \"intermediate_size\": 37,\n+            \"max_position_embeddings\": 512,\n+            \"num_attention_heads\": 4,\n+            \"pad_token_id\": 1,\n+        },\n+        vision_config={\n+            \"num_hidden_layers\": 1,\n+            \"hidden_size\": 16,\n+            \"intermediate_size\": 37,\n+            \"image_size\": 32,\n+            \"patch_size\": 8,\n+            \"hidden_act\": \"gelu\",\n+            \"vision_use_head\": False,\n+            \"num_attention_heads\": 4,\n+        },\n+        high_res_vision_config={\n+            \"num_hidden_layers\": 2,\n+            \"global_attn_indexes\": [0],\n+            \"hidden_size\": 16,\n+            \"intermediate_size\": 37,\n+            \"mlp_dim\": 24,\n+            \"output_channels\": 4,\n+            \"image_size\": 128,\n+            \"patch_size\": 32,\n+            \"num_attention_heads\": 4,\n+        },\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.num_channels = num_channels\n+        self.initializer_range = initializer_range\n+        self.is_training = is_training\n+        self.use_cache = use_cache\n+\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.high_res_vision_config = high_res_vision_config\n+        self.vision_config[\"num_channels\"] = self.num_channels\n+        self.high_res_vision_config[\"num_channels\"] = self.num_channels\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.high_res_image_size = high_res_vision_config[\"image_size\"]\n+        self.image_size = vision_config[\"image_size\"]\n+        self.num_image_tokens = vision_config[\"image_size\"] // vision_config[\"patch_size\"]\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n+        self.image_token_id = self.vocab_size - 1\n+\n+    def get_config(self):\n+        return DeepseekVLHybridConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            high_res_vision_config=self.high_res_vision_config,\n+            image_token_id=self.image_token_id,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+\n+        # create text and vision inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 2) + 1\n+        attention_mask = random_attention_mask([self.batch_size, self.seq_length])\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.num_channels,\n+                self.image_size,\n+                self.image_size,\n+            ]\n+        )\n+        high_res_pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.num_channels,\n+                self.high_res_image_size,\n+                self.high_res_image_size,\n+            ]\n+        )\n+        # fill image_tokens\n+        input_ids[:, : self.num_image_tokens] = self.image_token_id\n+\n+        return config, input_ids, attention_mask, pixel_values, high_res_pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_ids, attention_mask, pixel_values, high_res_pixel_values = config_and_inputs\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"pixel_values\": pixel_values,\n+            \"high_res_pixel_values\": high_res_pixel_values,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class DeepseekVLHybridModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    all_model_classes = (\n+        (DeepseekVLHybridModel, DeepseekVLHybridForConditionalGeneration) if is_torch_available() else ()\n+    )\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": DeepseekVLHybridModel,\n+            \"image-text-to-text\": DeepseekVLHybridForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    _is_composite = True\n+    test_pruning = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = DeepseekVLHybridModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=DeepseekVLHybridConfig, has_text_modality=False)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+            del inputs[\"high_res_pixel_values\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for VLMs.\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+            del inputs[\"high_res_pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            torch.testing.assert_close(out_embeds, out_ids)\n+\n+    @unittest.skip(reason=\"Siglip uses the same initialization scheme as the Flax original implementation\")\n+    # Copied from tests.models.siglip.test_modeling_siglip.SiglipVisionModelTest.test_initialization\n+    def test_initialization(self):\n+        pass\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # Load the model with SDPA\n+                model_sdpa = model_class.from_pretrained(\n+                    tmpdirname,\n+                    attn_implementation=\"sdpa\",\n+                )\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                # Load model with eager attention\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    attn_implementation=\"eager\",\n+                )\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+            self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+            self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+            if (\n+                hasattr(model_sdpa, \"vision_model\")\n+                and hasattr(model_sdpa, \"high_res_vision_model\")\n+                and hasattr(model_sdpa, \"language_model\")\n+            ):\n+                self.assertTrue(model_sdpa.language_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model_sdpa.high_res_vision_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model_eager.language_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.high_res_vision_model.config._attn_implementation == \"eager\")\n+\n+            for name, submodule in model_eager.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if (\n+                    any(re.finditer(r\"Attention(?!Pool)\", class_name))\n+                    and getattr(submodule, \"config\", None)\n+                    and submodule.config._attn_implementation == \"sdpa\"\n+                ):\n+                    self.assertTrue(submodule.config._attn_implementation == \"eager\")\n+\n+            for name, submodule in model_sdpa.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if (\n+                    any(re.finditer(r\"Attention(?!Pool)\", class_name))\n+                    and getattr(submodule, \"config\", None)\n+                    and submodule.config._attn_implementation == \"eager\"\n+                ):\n+                    self.assertTrue(submodule.config._attn_implementation == \"sdpa\")\n+\n+\n+@require_torch\n+@require_torch_accelerator\n+@slow\n+class DeepseekVLHybridIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.model_id = \"deepseek-community/deepseek-vl-7b-chat\"\n+\n+    def test_model_text_generation(self):\n+        model = DeepseekVLHybridForConditionalGeneration.from_pretrained(\n+            self.model_id, torch_dtype=\"auto\", device_map=\"auto\"\n+        )\n+        model.to(torch_device)\n+        model.eval()\n+        processor = AutoProcessor.from_pretrained(self.model_id)\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe this image.\"},\n+                ],\n+            }\n+        ]\n+        EXPECTED_TEXT = 'You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\\n\\nUser: Describe this image.\\n\\nAssistant:The image depicts a fluffy, beige-colored animal with a long tail, walking on snow. The'  # fmt: skip\n+\n+        inputs = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        )\n+        inputs = inputs.to(model.device, dtype=model.dtype)\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        text = processor.decode(output[0], skip_special_tokens=True)\n+\n+        self.assertEqual(\n+            text,\n+            EXPECTED_TEXT,\n+        )\n+\n+    def test_model_text_generation_batched(self):\n+        model = DeepseekVLHybridForConditionalGeneration.from_pretrained(\n+            self.model_id, torch_dtype=\"auto\", device_map=\"auto\"\n+        )\n+        model.to(torch_device)\n+        model.eval()\n+        processor = AutoProcessor.from_pretrained(self.model_id)\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"Describe this image.\"},\n+                    ],\n+                }\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"What animal do you see in the image?\"},\n+                    ],\n+                }\n+            ],\n+        ]\n+        EXPECTED_TEXT = [\n+            \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\\n\\nUser: Describe this image.\\n\\nAssistant:The image depicts a fluffy, beige-colored animal with a long tail, walking on snow. The\",  # fmt: skip\n+            \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\\n\\nUser: What animal do you see in the image?\\n\\nAssistant:I see a large, furry animal that appears to be a type of bear.The \",  # fmt: skip\n+        ]\n+\n+        inputs = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, padding=True, return_dict=True, return_tensors=\"pt\"\n+        )\n+        inputs = inputs.to(model.device, dtype=model.dtype)\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        text = processor.batch_decode(output, skip_special_tokens=True)\n+\n+        self.assertEqual(EXPECTED_TEXT, text)\n+\n+    def test_model_text_generation_with_multi_image(self):\n+        model = DeepseekVLHybridForConditionalGeneration.from_pretrained(\n+            self.model_id, torch_dtype=\"auto\", device_map=\"auto\"\n+        )\n+        model.to(torch_device)\n+        model.eval()\n+        processor = AutoProcessor.from_pretrained(self.model_id)\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"What's the difference between\"},\n+                    {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+                    {\"type\": \"text\", \"text\": \" and \"},\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                ],\n+            }\n+        ]\n+        EXPECTED_TEXT = \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\\n\\nUser: What's the difference between and \\n\\nAssistant:The image shows a street scene with a prominent red stop sign in the foreground. The sign has the\"  # fmt: skip\n+\n+        inputs = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        )\n+        inputs = inputs.to(model.device, dtype=model.dtype)\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        text = processor.decode(output[0], skip_special_tokens=True)\n+\n+        self.assertEqual(\n+            text,\n+            EXPECTED_TEXT,\n+        )"
        },
        {
            "sha": "10608d8bdba764d509194f1f71d47b8dc976b895",
            "filename": "tests/models/deepseek_vl_hybrid/test_processor_deepseek_vl_hybrid.py",
            "status": "added",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_processor_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69cff312f5c8026fea13029bb45b139385a88b4c/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_processor_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_processor_deepseek_vl_hybrid.py?ref=69cff312f5c8026fea13029bb45b139385a88b4c",
            "patch": "@@ -0,0 +1,54 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import tempfile\n+import unittest\n+\n+from transformers import DeepseekVLHybridProcessor, LlamaTokenizer\n+from transformers.models.deepseek_vl.convert_deepseek_vl_weights_to_hf import CHAT_TEMPLATE\n+from transformers.testing_utils import get_tests_dir\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import DeepseekVLHybridImageProcessor\n+\n+\n+SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n+\n+\n+class DeepseekVLHybridProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = DeepseekVLHybridProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = DeepseekVLHybridImageProcessor()\n+        tokenizer = LlamaTokenizer(\n+            vocab_file=SAMPLE_VOCAB,\n+            extra_special_tokens={\n+                \"pad_token\": \"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\",\n+                \"image_token\": \"<image_placeholder>\",\n+            },\n+        )\n+        processor = self.processor_class(\n+            image_processor=image_processor,\n+            tokenizer=tokenizer,\n+            chat_template=CHAT_TEMPLATE,\n+        )\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def prepare_processor_dict(self):\n+        return {\"chat_template\": CHAT_TEMPLATE, \"num_image_tokens\": 576}"
        }
    ],
    "stats": {
        "total": 5860,
        "additions": 5856,
        "deletions": 4
    }
}