{
    "author": "MekkCyber",
    "message": "Fix: Falcon tie_word_embeddings in GGUF (#35715)\n\n* fix falcon tie_word_embeddings\r\n\r\n* fix style",
    "sha": "fd4f14c968cf186a4dec325143cc89c10ca6dc5e",
    "files": [
        {
            "sha": "21385233a779f1175c4c91967005b7e2a659a072",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd4f14c968cf186a4dec325143cc89c10ca6dc5e/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd4f14c968cf186a4dec325143cc89c10ca6dc5e/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=fd4f14c968cf186a4dec325143cc89c10ca6dc5e",
            "patch": "@@ -400,8 +400,9 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n \n     # Handle tie_word_embeddings, if lm_head.weight is not present in tensors,\n     # tie_word_embeddings is true otherwise false\n-    parsed_parameters[\"config\"][\"tie_word_embeddings\"] = all(\n-        \"output.weight\" != tensor.name for tensor in reader.tensors\n+    exceptions = [\"falcon\"]\n+    parsed_parameters[\"config\"][\"tie_word_embeddings\"] = (\n+        all(\"output.weight\" != tensor.name for tensor in reader.tensors) or architecture in exceptions\n     )\n \n     # List all key-value pairs in a columnized format"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 3,
        "deletions": 2
    }
}