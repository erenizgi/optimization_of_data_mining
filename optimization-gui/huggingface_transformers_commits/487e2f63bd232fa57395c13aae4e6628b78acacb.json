{
    "author": "Isotr0py",
    "message": "Fix `head_dim` in config extracted from Gemma2 GGUF model (#35818)\n\nfix gemma2 head dim\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "487e2f63bd232fa57395c13aae4e6628b78acacb",
    "files": [
        {
            "sha": "db7d93cad0427acd2c12683edd177bf28c1ceb07",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/487e2f63bd232fa57395c13aae4e6628b78acacb/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/487e2f63bd232fa57395c13aae4e6628b78acacb/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=487e2f63bd232fa57395c13aae4e6628b78acacb",
            "patch": "@@ -198,6 +198,9 @@\n         \"embedding_length\": \"hidden_size\",\n         \"rope.dimension_count\": None,\n         \"rope.freq_base\": \"rope_theta\",\n+        # NOTE: Gemma2 has key_length==value_length==head_dim\n+        # See: https://github.com/ggerganov/llama.cpp/blob/2e2f8f093cd4fb6bbb87ba84f6b9684fa082f3fa/convert_hf_to_gguf.py#L3293-L3294\n+        \"attention.key_length\": \"head_dim\",\n         \"attention.head_count\": \"num_attention_heads\",\n         \"attention.head_count_kv\": \"num_key_value_heads\",\n         \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\","
        }
    ],
    "stats": {
        "total": 3,
        "additions": 3,
        "deletions": 0
    }
}