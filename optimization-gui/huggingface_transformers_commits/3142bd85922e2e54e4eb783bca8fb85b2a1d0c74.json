{
    "author": "eustlb",
    "message": "[CSM] infer codec model with no_grad + audio eos label (#38215)\n\n* infer codec model with no_grad\n\n* codec_model eval\n\n* training labels: add audio eos token",
    "sha": "3142bd85922e2e54e4eb783bca8fb85b2a1d0c74",
    "files": [
        {
            "sha": "53c24a5eba5ccbf2a4fd3aea3b85c81dbc084d9d",
            "filename": "docs/source/en/model_doc/csm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3142bd85922e2e54e4eb783bca8fb85b2a1d0c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3142bd85922e2e54e4eb783bca8fb85b2a1d0c74/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md?ref=3142bd85922e2e54e4eb783bca8fb85b2a1d0c74",
            "patch": "@@ -315,6 +315,7 @@ device = \"cuda\"\n processor = AutoProcessor.from_pretrained(model_id)\n model = CsmForConditionalGeneration.from_pretrained(model_id, device_map=device)\n model.train()\n+model.codec_model.eval()\n \n ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n # ensure the audio is 24kHz"
        },
        {
            "sha": "777169565440f94ae3289759b09b518f189c7e39",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 18,
            "deletions": 16,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/3142bd85922e2e54e4eb783bca8fb85b2a1d0c74/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3142bd85922e2e54e4eb783bca8fb85b2a1d0c74/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=3142bd85922e2e54e4eb783bca8fb85b2a1d0c74",
            "patch": "@@ -981,22 +981,23 @@ def _merge_input_ids_with_input_values(\n             # =======================================\n             # TODO: @eustlb, this should be batched !!!\n             # but requires making sure batched inference of the codec model works as intended\n-            audio_tokens_list = []\n-            for batch_input_values, batch_input_values_cutoffs in zip(input_values, input_values_cutoffs):\n-                batch_input_values_cutoffs = batch_input_values_cutoffs[batch_input_values_cutoffs >= 0]\n-                for i in range(batch_input_values_cutoffs.shape[0] - 1):\n-                    start_idx = batch_input_values_cutoffs[i]\n-                    end_idx = batch_input_values_cutoffs[i + 1]\n-                    audio_batch = batch_input_values[..., start_idx:end_idx]\n-                    codec_outputs = self.codec_model.encode(audio_batch.unsqueeze(0))\n-                    codebook_ids = codec_outputs.audio_codes.transpose(1, -1)\n-                    audio_tokens_list.append(codebook_ids[0])\n-\n-            max_audio_frames = max(el.shape[0] for el in audio_tokens_list)\n-            batched_audio_token_ids = torch.stack(\n-                [nn.functional.pad(el, (0, 0, 0, max_audio_frames - el.shape[0])) for el in audio_tokens_list]\n-            )\n-            audio_codes_mask = self.codec_model.get_audio_codes_mask(input_values_mask)\n+            with torch.no_grad():\n+                audio_tokens_list = []\n+                for batch_input_values, batch_input_values_cutoffs in zip(input_values, input_values_cutoffs):\n+                    batch_input_values_cutoffs = batch_input_values_cutoffs[batch_input_values_cutoffs >= 0]\n+                    for i in range(batch_input_values_cutoffs.shape[0] - 1):\n+                        start_idx = batch_input_values_cutoffs[i]\n+                        end_idx = batch_input_values_cutoffs[i + 1]\n+                        audio_batch = batch_input_values[..., start_idx:end_idx]\n+                        codec_outputs = self.codec_model.encode(audio_batch.unsqueeze(0))\n+                        codebook_ids = codec_outputs.audio_codes.transpose(1, -1)\n+                        audio_tokens_list.append(codebook_ids[0])\n+\n+                max_audio_frames = max(el.shape[0] for el in audio_tokens_list)\n+                batched_audio_token_ids = torch.stack(\n+                    [nn.functional.pad(el, (0, 0, 0, max_audio_frames - el.shape[0])) for el in audio_tokens_list]\n+                )\n+                audio_codes_mask = self.codec_model.get_audio_codes_mask(input_values_mask)\n             # =======================================\n             audio_token_id = self.config.audio_token_id\n             audio_token_mask = input_ids == audio_token_id\n@@ -1018,6 +1019,7 @@ def _merge_input_ids_with_input_values(\n             if labels is not None:\n                 labels_expanded = labels.unsqueeze(-1).repeat(1, 1, self.config.num_codebooks)\n                 labels_expanded[audio_token_mask] = batched_audio_token_ids[audio_codes_mask]\n+                labels_expanded[audio_eos_token_mask] = audio_eos_frame_ids\n                 # mask depth decoder\n                 depth_decoder_ignore_frames_idxs = (labels == -101).nonzero(as_tuple=True)\n                 labels_expanded[depth_decoder_ignore_frames_idxs[0], depth_decoder_ignore_frames_idxs[1], 1:] = -100"
        },
        {
            "sha": "86483076d301cc9755b021e0883519dba21eafa9",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 18,
            "deletions": 16,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/3142bd85922e2e54e4eb783bca8fb85b2a1d0c74/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3142bd85922e2e54e4eb783bca8fb85b2a1d0c74/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=3142bd85922e2e54e4eb783bca8fb85b2a1d0c74",
            "patch": "@@ -595,22 +595,23 @@ def _merge_input_ids_with_input_values(\n             # =======================================\n             # TODO: @eustlb, this should be batched !!!\n             # but requires making sure batched inference of the codec model works as intended\n-            audio_tokens_list = []\n-            for batch_input_values, batch_input_values_cutoffs in zip(input_values, input_values_cutoffs):\n-                batch_input_values_cutoffs = batch_input_values_cutoffs[batch_input_values_cutoffs >= 0]\n-                for i in range(batch_input_values_cutoffs.shape[0] - 1):\n-                    start_idx = batch_input_values_cutoffs[i]\n-                    end_idx = batch_input_values_cutoffs[i + 1]\n-                    audio_batch = batch_input_values[..., start_idx:end_idx]\n-                    codec_outputs = self.codec_model.encode(audio_batch.unsqueeze(0))\n-                    codebook_ids = codec_outputs.audio_codes.transpose(1, -1)\n-                    audio_tokens_list.append(codebook_ids[0])\n-\n-            max_audio_frames = max(el.shape[0] for el in audio_tokens_list)\n-            batched_audio_token_ids = torch.stack(\n-                [nn.functional.pad(el, (0, 0, 0, max_audio_frames - el.shape[0])) for el in audio_tokens_list]\n-            )\n-            audio_codes_mask = self.codec_model.get_audio_codes_mask(input_values_mask)\n+            with torch.no_grad():\n+                audio_tokens_list = []\n+                for batch_input_values, batch_input_values_cutoffs in zip(input_values, input_values_cutoffs):\n+                    batch_input_values_cutoffs = batch_input_values_cutoffs[batch_input_values_cutoffs >= 0]\n+                    for i in range(batch_input_values_cutoffs.shape[0] - 1):\n+                        start_idx = batch_input_values_cutoffs[i]\n+                        end_idx = batch_input_values_cutoffs[i + 1]\n+                        audio_batch = batch_input_values[..., start_idx:end_idx]\n+                        codec_outputs = self.codec_model.encode(audio_batch.unsqueeze(0))\n+                        codebook_ids = codec_outputs.audio_codes.transpose(1, -1)\n+                        audio_tokens_list.append(codebook_ids[0])\n+\n+                max_audio_frames = max(el.shape[0] for el in audio_tokens_list)\n+                batched_audio_token_ids = torch.stack(\n+                    [nn.functional.pad(el, (0, 0, 0, max_audio_frames - el.shape[0])) for el in audio_tokens_list]\n+                )\n+                audio_codes_mask = self.codec_model.get_audio_codes_mask(input_values_mask)\n             # =======================================\n             audio_token_id = self.config.audio_token_id\n             audio_token_mask = input_ids == audio_token_id\n@@ -632,6 +633,7 @@ def _merge_input_ids_with_input_values(\n             if labels is not None:\n                 labels_expanded = labels.unsqueeze(-1).repeat(1, 1, self.config.num_codebooks)\n                 labels_expanded[audio_token_mask] = batched_audio_token_ids[audio_codes_mask]\n+                labels_expanded[audio_eos_token_mask] = audio_eos_frame_ids\n                 # mask depth decoder\n                 depth_decoder_ignore_frames_idxs = (labels == -101).nonzero(as_tuple=True)\n                 labels_expanded[depth_decoder_ignore_frames_idxs[0], depth_decoder_ignore_frames_idxs[1], 1:] = -100"
        },
        {
            "sha": "a0f91a1c3dfbe7602cece69f468751ecce263fff",
            "filename": "src/transformers/models/csm/processing_csm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3142bd85922e2e54e4eb783bca8fb85b2a1d0c74/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3142bd85922e2e54e4eb783bca8fb85b2a1d0c74/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py?ref=3142bd85922e2e54e4eb783bca8fb85b2a1d0c74",
            "patch": "@@ -353,7 +353,11 @@ def __call__(\n             else:\n                 skip_frames_idxs = audio_frame_idxs\n \n-            labels = torch.where(data[\"input_ids\"] == self.audio_token_id, data[\"input_ids\"], -100)\n+            labels = torch.where(\n+                (data[\"input_ids\"] == self.audio_token_id) | (data[\"input_ids\"] == self.audio_eos_token_id),\n+                data[\"input_ids\"],\n+                -100,\n+            )\n             labels[skip_frames_idxs[:, 0], skip_frames_idxs[:, 1]] = -101\n \n             data[\"labels\"] = labels"
        }
    ],
    "stats": {
        "total": 75,
        "additions": 42,
        "deletions": 33
    }
}