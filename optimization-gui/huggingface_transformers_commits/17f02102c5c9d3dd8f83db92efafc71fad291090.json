{
    "author": "yonigozlan",
    "message": "ðŸš¨[Fast Image Processor] Force Fast Image Processor for Qwen2_VL/2_5_VL + Refactor (#39591)\n\n* init\n\n* Force qwen2VL image proc to fast\n\n* refactor qwen2 vl fast\n\n* fix copies\n\n* Update after PR review and update tests to use return_tensors=\"pt\"\n\n* fix processor tests\n\n* add BC for min pixels/max pixels",
    "sha": "17f02102c5c9d3dd8f83db92efafc71fad291090",
    "files": [
        {
            "sha": "cd0473a2d7cd90e8c4cc523e92964e53196ec73c",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/17f02102c5c9d3dd8f83db92efafc71fad291090/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17f02102c5c9d3dd8f83db92efafc71fad291090/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=17f02102c5c9d3dd8f83db92efafc71fad291090",
            "patch": "@@ -49,6 +49,9 @@\n logger = logging.get_logger(__name__)\n \n \n+FORCE_FAST_IMAGE_PROCESSOR = [\"Qwen2VLImageProcessor\"]\n+\n+\n if TYPE_CHECKING:\n     # This significantly improves completion suggestion performance when\n     # the transformers package is used with Microsoft's Pylance language server.\n@@ -514,6 +517,13 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n             # if use_fast is not set and the processor was saved with a fast processor, we use it, otherwise we use the slow processor.\n             if use_fast is None:\n                 use_fast = image_processor_type.endswith(\"Fast\")\n+                if not use_fast and image_processor_type in FORCE_FAST_IMAGE_PROCESSOR and is_torchvision_available():\n+                    use_fast = True\n+                    logger.warning_once(\n+                        f\"The image processor of type `{image_processor_type}` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. \"\n+                        \"This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \"\n+                        \"Note that this behavior will be extended to all models in a future release.\"\n+                    )\n                 if not use_fast:\n                     logger.warning_once(\n                         \"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. \""
        },
        {
            "sha": "8e06d2ef32f7c46003a1f145d0434b44988a2f60",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/17f02102c5c9d3dd8f83db92efafc71fad291090/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17f02102c5c9d3dd8f83db92efafc71fad291090/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=17f02102c5c9d3dd8f83db92efafc71fad291090",
            "patch": "@@ -67,7 +67,7 @@ class ColQwen2Processor(ColPaliProcessor):\n         query_prefix (`str`, *optional*): A prefix to be used for the query.\n     \"\"\"\n \n-    image_processor_class = \"Qwen2VLImageProcessor\"\n+    image_processor_class = \"AutoImageProcessor\"\n     tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n \n     def __init__("
        },
        {
            "sha": "59af4bdd42e8ae5b4e40eddb444cd2597dbfe5a0",
            "filename": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/17f02102c5c9d3dd8f83db92efafc71fad291090/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17f02102c5c9d3dd8f83db92efafc71fad291090/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py?ref=17f02102c5c9d3dd8f83db92efafc71fad291090",
            "patch": "@@ -66,7 +66,7 @@ class ColQwen2Processor(ProcessorMixin):\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n \n-    image_processor_class = \"Qwen2VLImageProcessor\"\n+    image_processor_class = \"AutoImageProcessor\"\n     tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n \n     def __init__("
        },
        {
            "sha": "099384419e02bba513c9c09fe45c57c9392e9dc6",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17f02102c5c9d3dd8f83db92efafc71fad291090/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17f02102c5c9d3dd8f83db92efafc71fad291090/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py?ref=17f02102c5c9d3dd8f83db92efafc71fad291090",
            "patch": "@@ -138,6 +138,7 @@ def _preprocess(\n         processed_images_grouped = {}\n         processed_grids = {}\n         for shape, stacked_images in grouped_images.items():\n+            resized_height, resized_width = stacked_images.shape[-2:]\n             # Fused rescale and normalize\n             stacked_images = self.rescale_and_normalize(\n                 stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n@@ -188,9 +189,6 @@ def preprocess(\n         images: ImageInput,\n         **kwargs: Unpack[Glm4vFastImageProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Preprocess an image or batch of images.\n-        \"\"\"\n         return super().preprocess(images, **kwargs)\n \n "
        },
        {
            "sha": "27628e2f74424da93b3917be769a8e5d8f679551",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 123,
            "deletions": 246,
            "changes": 369,
            "blob_url": "https://github.com/huggingface/transformers/blob/17f02102c5c9d3dd8f83db92efafc71fad291090/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17f02102c5c9d3dd8f83db92efafc71fad291090/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=17f02102c5c9d3dd8f83db92efafc71fad291090",
            "patch": "@@ -35,9 +35,6 @@\n     ImageInput,\n     PILImageResampling,\n     SizeDict,\n-    get_image_size,\n-    make_flat_list_of_images,\n-    valid_images,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -57,8 +54,6 @@\n \n \n if is_torchvision_available():\n-    from ...image_utils import pil_torch_interpolation_mapping\n-\n     if is_torchvision_v2_available():\n         from torchvision.transforms.v2 import functional as F\n     else:\n@@ -110,18 +105,90 @@ def __init__(self, **kwargs: Unpack[Qwen2VLFastImageProcessorKwargs]):\n         size = kwargs.pop(\"size\", None)\n         min_pixels = kwargs.pop(\"min_pixels\", None)\n         max_pixels = kwargs.pop(\"max_pixels\", None)\n-        if size is not None and (\"shortest_edge\" not in size or \"longest_edge\" not in size):\n-            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n-        else:\n-            size = self.size\n         # backward compatibility: override size with min_pixels and max_pixels if they are provided\n+        size = self.size if size is None else size\n         if min_pixels is not None:\n             size[\"shortest_edge\"] = min_pixels\n+            size.pop(\"min_pixels\", None)\n         if max_pixels is not None:\n             size[\"longest_edge\"] = max_pixels\n+            size.pop(\"max_pixels\", None)\n+        if \"shortest_edge\" not in size or \"longest_edge\" not in size:\n+            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n \n         super().__init__(size=size, min_pixels=min_pixels, max_pixels=max_pixels, **kwargs)\n \n+    def _further_process_kwargs(\n+        self,\n+        size: Optional[SizeDict] = None,\n+        min_pixels: Optional[int] = None,\n+        max_pixels: Optional[int] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if min_pixels is not None and max_pixels is not None:\n+            size = {\"shortest_edge\": min_pixels, \"longest_edge\": max_pixels}\n+        elif size is not None:\n+            if \"shortest_edge\" not in size or \"longest_edge\" not in size:\n+                raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+            min_pixels = size[\"shortest_edge\"]\n+            max_pixels = size[\"longest_edge\"]\n+        else:\n+            size = {**self.size}\n+\n+        return super()._further_process_kwargs(size=size, min_pixels=min_pixels, max_pixels=max_pixels, **kwargs)\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        videos: Optional[VideoInput] = None,\n+        **kwargs: Unpack[Qwen2VLFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        return super().preprocess(images, videos, **kwargs)\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        videos: VideoInput,\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[DefaultFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        To be overriden by subclasses when image-like inputs other than images should be processed.\n+        It can be used for segmentation maps, depth maps, etc.\n+        \"\"\"\n+        # Prepare input images\n+        batch_feature = BatchFeature()\n+        if images is not None:\n+            images = self._prepare_image_like_inputs(\n+                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+            )\n+            batch_feature = self._preprocess(images, **kwargs)\n+        if videos is not None:\n+            logger.warning(\n+                \"`Qwen2VLImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n+                \"This is a deprecated behavior and will be removed in v5.0. \"\n+                \"Your videos should be forwarded to `Qwen2VLVideoProcessor`. \"\n+            )\n+            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n+            videos = make_batched_videos(videos)\n+            videos = [\n+                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n+                for video in videos\n+            ]\n+            video_outputs = self._preprocess(videos, **kwargs)\n+            batch_feature.update(\n+                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n+            )\n+        return batch_feature\n+\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n@@ -136,65 +203,15 @@ def _preprocess(\n         patch_size: int,\n         temporal_patch_size: int,\n         merge_size: int,\n-        do_convert_rgb: bool,\n-        input_data_format: Optional[Union[str, ChannelDimension]],\n-        device: Optional[Union[str, torch.device]],\n         disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ):\n-        \"\"\"\n-        Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n-\n-        Args:\n-            images (`ImageInput`):\n-                Image or batch of images to preprocess. Expects pixel values ranging from 0 to 255. If pixel values range from 0 to 1, set `do_rescale=False`.\n-            vision_info (`list[Dict]`, *optional*):\n-                Optional list of dictionaries containing additional information about vision inputs.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the image.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the image after resizing. `shortest_edge` and `longest_edge` keys must be present.\n-            interpolation (`InterpolationMode`):\n-                Resampling filter to use if resizing the image.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the image.\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Scale factor to use if rescaling the image.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the image.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n-            patch_size (`int`, *optional*, defaults to `self.patch_size`):\n-                The spatial patch size of the vision encoder.\n-            temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):\n-                The temporal patch size of the vision encoder.\n-            merge_size (`int`, *optional*, defaults to `self.merge_size`):\n-                The merge size of the vision encoder to llm encoder.\n-            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-                Whether to convert the image to RGB.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.   - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-            device (`torch.device`, *optional*):\n-                The device to process the images on. If unset, the device is inferred from the input images.\n-        \"\"\"\n-        images = self._prepare_image_like_inputs(\n-            images=images,\n-            do_convert_rgb=do_convert_rgb,\n-            input_data_format=input_data_format,\n-            device=device,\n-        )\n-\n-        height, width = get_image_size(images[0], channel_dim=ChannelDimension.FIRST)\n-        resized_height, resized_width = height, width\n-\n         # Group images by size for batched resizing\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n+            height, width = stacked_images.shape[-2:]\n             if do_resize:\n                 resized_height, resized_width = smart_resize(\n                     height,\n@@ -215,203 +232,63 @@ def _preprocess(\n         # Needed in case do_resize is False, or resize returns images with different sizes\n         grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n+        processed_grids = {}\n         for shape, stacked_images in grouped_images.items():\n+            resized_height, resized_width = stacked_images.shape[-2:]\n             # Fused rescale and normalize\n-            stacked_images = self.rescale_and_normalize(\n+            patches = self.rescale_and_normalize(\n                 stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n             )\n-            processed_images_grouped[shape] = stacked_images\n-\n-        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        patches = torch.stack(processed_images, dim=0)\n-        if patches.shape[0] % temporal_patch_size != 0:\n-            repeats = patches[-1].unsqueeze(0).repeat(temporal_patch_size - 1, 1, 1, 1)\n-            patches = torch.cat([patches, repeats], dim=0)\n-\n-        channel = patches.shape[1]\n-        grid_t = patches.shape[0] // temporal_patch_size\n-        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n-\n-        patches = patches.view(\n-            grid_t,\n-            temporal_patch_size,\n-            channel,\n-            grid_h // merge_size,\n-            merge_size,\n-            patch_size,\n-            grid_w // merge_size,\n-            merge_size,\n-            patch_size,\n-        )\n-        patches = patches.permute(0, 3, 6, 4, 7, 2, 1, 5, 8)\n-        flatten_patches = patches.reshape(\n-            grid_t * grid_h * grid_w, channel * temporal_patch_size * patch_size * patch_size\n-        )\n-\n-        return flatten_patches, (grid_t, grid_h, grid_w)\n-\n-    @auto_docstring\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        videos: VideoInput = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        min_pixels: Optional[int] = None,\n-        max_pixels: Optional[int] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        device: Optional[\"torch.device\"] = None,\n-        disable_grouping: Optional[bool] = False,\n-        **kwargs,\n-    ):\n-        r\"\"\"\n-        min_pixels (`int`, *optional*, defaults to `56 * 56`):\n-            The min pixels of the image to resize the image.\n-        max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):\n-            The max pixels of the image to resize the image.\n-        patch_size (`int`, *optional*, defaults to 14):\n-            The spatial patch size of the vision encoder.\n-        temporal_patch_size (`int`, *optional*, defaults to 2):\n-            The temporal patch size of the vision encoder.\n-        merge_size (`int`, *optional*, defaults to 2):\n-            The merge size of the vision encoder to llm encoder.\n-        \"\"\"\n-        min_pixels = min_pixels if min_pixels is not None else self.min_pixels\n-        max_pixels = max_pixels if max_pixels is not None else self.max_pixels\n-\n-        if size is not None:\n-            if \"shortest_edge\" not in size or \"longest_edge\" not in size:\n-                raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n-            min_pixels = size[\"shortest_edge\"]\n-        elif min_pixels is not None and max_pixels is not None:\n-            # backward compatibility: override size with min_pixels and max_pixels if they are provided\n-            size = {\"shortest_edge\": min_pixels, \"longest_edge\": max_pixels}\n-        else:\n-            size = {**self.size}\n-\n-        do_resize = do_resize if do_resize is not None else self.do_resize\n-        size = size if size is not None else self.size\n-        resample = resample if resample is not None else self.resample\n-        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n-        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n-        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-        patch_size = patch_size if patch_size is not None else self.patch_size\n-        temporal_patch_size = temporal_patch_size if temporal_patch_size is not None else self.temporal_patch_size\n-        merge_size = merge_size if merge_size is not None else self.merge_size\n-        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n-\n-        # Make hashable for cache\n-        size = SizeDict(**size) if size is not None else None\n-        image_mean = tuple(image_mean) if image_mean is not None else None\n-        image_std = tuple(image_std) if image_std is not None else None\n-\n-        self._validate_preprocess_kwargs(\n-            do_rescale=do_rescale,\n-            rescale_factor=rescale_factor,\n-            do_normalize=do_normalize,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n-            return_tensors=return_tensors,\n-            data_format=data_format,\n-        )\n-        interpolation = (\n-            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n-        )\n-\n-        if images is not None:\n-            images = make_flat_list_of_images(images)\n-\n-        if images is not None and not valid_images(images):\n-            raise ValueError(\n-                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n-                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            if patches.ndim == 4:\n+                # add a temporal dimension if we have images\n+                patches = patches.unsqueeze(1)\n+            if patches.shape[1] % temporal_patch_size != 0:\n+                repeats = patches[:, -1:].repeat(1, temporal_patch_size - 1, 1, 1, 1)\n+                patches = torch.cat([patches, repeats], dim=1)\n+            batch_size, grid_t, channel = patches.shape[:3]\n+            grid_t = grid_t // temporal_patch_size\n+            grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+\n+            patches = patches.view(\n+                batch_size,\n+                grid_t,\n+                temporal_patch_size,\n+                channel,\n+                grid_h // merge_size,\n+                merge_size,\n+                patch_size,\n+                grid_w // merge_size,\n+                merge_size,\n+                patch_size,\n+            )\n+            # Reorder dimensions to group grid and patch information for subsequent flattening.\n+            # (batch, grid_t, grid_h, grid_w, merge_h, merge_w, channel, temp_patch_size, patch_h, patch_w)\n+            patches = patches.permute(0, 1, 4, 7, 5, 8, 3, 2, 6, 9)\n+            flatten_patches = patches.reshape(\n+                batch_size,\n+                grid_t * grid_h * grid_w,\n+                channel * temporal_patch_size * patch_size * patch_size,\n             )\n \n-        data = {}\n-        if images is not None:\n-            pixel_values, vision_grid_thws = [], []\n-            for image in images:\n-                patches, image_grid_thw = self._preprocess(\n-                    image,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    interpolation=interpolation,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    patch_size=patch_size,\n-                    temporal_patch_size=temporal_patch_size,\n-                    merge_size=merge_size,\n-                    do_convert_rgb=do_convert_rgb,\n-                    input_data_format=input_data_format,\n-                    device=device,\n-                    disable_grouping=disable_grouping,\n-                )\n-                pixel_values.extend(patches)\n-                vision_grid_thws.append(image_grid_thw)\n-            pixel_values = torch.stack(pixel_values)\n-            vision_grid_thws = torch.tensor(vision_grid_thws)\n-            data.update({\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws})\n+            processed_images_grouped[shape] = flatten_patches\n+            processed_grids[shape] = [[grid_t, grid_h, grid_w]] * batch_size\n \n-        # kept for BC only and should be removed after v5.0\n-        if videos is not None:\n-            logger.warning(\n-                \"`Qwen2VLImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `Qwen2VLVideoProcessor`. \"\n-            )\n-            videos = make_batched_videos(videos)\n-            pixel_values_videos, vision_grid_thws_videos = [], []\n-            for images in videos:\n-                patches, video_grid_thw = self._preprocess(\n-                    images,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    interpolation=interpolation,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    patch_size=patch_size,\n-                    temporal_patch_size=temporal_patch_size,\n-                    merge_size=merge_size,\n-                    do_convert_rgb=do_convert_rgb,\n-                    input_data_format=input_data_format,\n-                    device=device,\n-                    disable_grouping=disable_grouping,\n-                )\n-                pixel_values_videos.extend(patches)\n-                vision_grid_thws_videos.append(video_grid_thw)\n-            pixel_values_videos = torch.stack(pixel_values_videos)\n-            vision_grid_thws_videos = torch.tensor(vision_grid_thws_videos)\n-            data.update({\"pixel_values_videos\": pixel_values_videos, \"video_grid_thw\": vision_grid_thws_videos})\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_grids = reorder_images(processed_grids, grouped_images_index)\n+        pixel_values = torch.cat(processed_images, dim=0)\n+        image_grid_thw = torch.tensor(processed_grids)\n \n-        return BatchFeature(data=data, tensor_type=return_tensors)\n+        return BatchFeature(\n+            data={\"pixel_values\": pixel_values, \"image_grid_thw\": image_grid_thw}, tensor_type=return_tensors\n+        )\n \n     def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n         \"\"\"\n         A utility that returns number of image patches for a given image size.\n \n+        Note: Do not remove this method! It is used by vLLM to infer the number of patches and placeholders\n+        without an image input.\n+\n         Args:\n             height (`int`):\n                 Height of the input image."
        },
        {
            "sha": "6c62a568f6cfada0a722d558fdb66f3cb8f755e5",
            "filename": "src/transformers/models/qwen2_vl/video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/17f02102c5c9d3dd8f83db92efafc71fad291090/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17f02102c5c9d3dd8f83db92efafc71fad291090/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py?ref=17f02102c5c9d3dd8f83db92efafc71fad291090",
            "patch": "@@ -116,8 +116,21 @@ class Qwen2VLVideoProcessor(BaseVideoProcessor):\n     model_input_names = [\"pixel_values_videos\", \"video_grid_thw\"]\n \n     def __init__(self, **kwargs: Unpack[Qwen2VLVideoProcessorInitKwargs]):\n-        super().__init__(**kwargs)\n-        self.size = {\"shortest_edge\": self.min_pixels, \"longest_edge\": self.max_pixels}\n+        size = kwargs.pop(\"size\", None)\n+        min_pixels = kwargs.pop(\"min_pixels\", None)\n+        max_pixels = kwargs.pop(\"max_pixels\", None)\n+        # backward compatibility: override size with min_pixels and max_pixels if they are provided\n+        size = self.size if size is None else size\n+        if min_pixels is not None:\n+            size[\"shortest_edge\"] = min_pixels\n+            size.pop(\"min_pixels\", None)\n+        if max_pixels is not None:\n+            size[\"longest_edge\"] = max_pixels\n+            size.pop(\"max_pixels\", None)\n+        if \"shortest_edge\" not in size or \"longest_edge\" not in size:\n+            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+\n+        super().__init__(size=size, min_pixels=min_pixels, max_pixels=max_pixels, **kwargs)\n \n     def sample_frames(\n         self,"
        },
        {
            "sha": "0baea494cdb654998b554103e1d8c2a2c89a9e55",
            "filename": "tests/models/qwen2_5_omni/test_processor_qwen2_5_omni.py",
            "status": "modified",
            "additions": 16,
            "deletions": 8,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/17f02102c5c9d3dd8f83db92efafc71fad291090/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processor_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17f02102c5c9d3dd8f83db92efafc71fad291090/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processor_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processor_qwen2_5_omni.py?ref=17f02102c5c9d3dd8f83db92efafc71fad291090",
            "patch": "@@ -25,10 +25,17 @@\n from transformers import (\n     AutoProcessor,\n     Qwen2_5OmniProcessor,\n-    Qwen2Tokenizer,\n+    Qwen2TokenizerFast,\n     WhisperFeatureExtractor,\n )\n-from transformers.testing_utils import require_av, require_librosa, require_torch, require_torchaudio, require_vision\n+from transformers.testing_utils import (\n+    require_av,\n+    require_librosa,\n+    require_torch,\n+    require_torchaudio,\n+    require_torchvision,\n+    require_vision,\n+)\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -38,12 +45,13 @@\n     import torch\n \n if is_vision_available():\n-    from transformers import Qwen2VLImageProcessor\n+    from transformers import Qwen2VLImageProcessorFast\n \n \n @require_vision\n @require_torch\n @require_torchaudio\n+@require_torchvision\n class Qwen2_5OmniProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = Qwen2_5OmniProcessor\n \n@@ -244,13 +252,13 @@ def test_save_load_pretrained_default(self):\n         )\n \n         processor.save_pretrained(self.tmpdirname)\n-        processor = Qwen2_5OmniProcessor.from_pretrained(self.tmpdirname, use_fast=False)\n+        processor = Qwen2_5OmniProcessor.from_pretrained(self.tmpdirname, use_fast=True)\n \n         self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n         self.assertEqual(processor.image_processor.to_json_string(), image_processor.to_json_string())\n         self.assertEqual(processor.feature_extractor.to_json_string(), feature_extractor.to_json_string())\n-        self.assertIsInstance(processor.tokenizer, Qwen2Tokenizer)\n-        self.assertIsInstance(processor.image_processor, Qwen2VLImageProcessor)\n+        self.assertIsInstance(processor.tokenizer, Qwen2TokenizerFast)\n+        self.assertIsInstance(processor.image_processor, Qwen2VLImageProcessorFast)\n         self.assertIsInstance(processor.feature_extractor, WhisperFeatureExtractor)\n \n     def test_image_processor(self):\n@@ -267,8 +275,8 @@ def test_image_processor(self):\n \n         image_input = self.prepare_image_inputs()\n \n-        input_image_proc = image_processor(image_input, return_tensors=\"np\")\n-        input_processor = processor(images=image_input, text=\"dummy\", return_tensors=\"np\")\n+        input_image_proc = image_processor(image_input, return_tensors=\"pt\")\n+        input_processor = processor(images=image_input, text=\"dummy\", return_tensors=\"pt\")\n \n         for key in input_image_proc.keys():\n             self.assertAlmostEqual(input_image_proc[key].sum(), input_processor[key].sum(), delta=1e-2)"
        },
        {
            "sha": "b8aa49b0043641d3392c7ebd5b7de46db21880bc",
            "filename": "tests/models/qwen2_5_vl/test_processor_qwen2_5_vl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/17f02102c5c9d3dd8f83db92efafc71fad291090/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17f02102c5c9d3dd8f83db92efafc71fad291090/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py?ref=17f02102c5c9d3dd8f83db92efafc71fad291090",
            "patch": "@@ -20,22 +20,23 @@\n import numpy as np\n import pytest\n \n-from transformers import AutoProcessor, Qwen2Tokenizer\n-from transformers.testing_utils import require_av, require_torch, require_vision\n+from transformers import AutoProcessor, Qwen2TokenizerFast\n+from transformers.testing_utils import require_av, require_torch, require_torchvision, require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n \n if is_vision_available():\n-    from transformers import Qwen2_5_VLProcessor, Qwen2VLImageProcessor\n+    from transformers import Qwen2_5_VLProcessor, Qwen2VLImageProcessorFast\n \n if is_torch_available():\n     import torch\n \n \n @require_vision\n @require_torch\n+@require_torchvision\n class Qwen2_5_VLProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = Qwen2_5_VLProcessor\n \n@@ -73,12 +74,12 @@ def test_save_load_pretrained_default(self):\n             tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n         )\n         processor.save_pretrained(self.tmpdirname)\n-        processor = Qwen2_5_VLProcessor.from_pretrained(self.tmpdirname, use_fast=False)\n+        processor = Qwen2_5_VLProcessor.from_pretrained(self.tmpdirname, use_fast=True)\n \n         self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n         self.assertEqual(processor.image_processor.to_json_string(), image_processor.to_json_string())\n-        self.assertIsInstance(processor.tokenizer, Qwen2Tokenizer)\n-        self.assertIsInstance(processor.image_processor, Qwen2VLImageProcessor)\n+        self.assertIsInstance(processor.tokenizer, Qwen2TokenizerFast)\n+        self.assertIsInstance(processor.image_processor, Qwen2VLImageProcessorFast)\n \n     def test_image_processor(self):\n         image_processor = self.get_image_processor()\n@@ -91,8 +92,8 @@ def test_image_processor(self):\n \n         image_input = self.prepare_image_inputs()\n \n-        input_image_proc = image_processor(image_input, return_tensors=\"np\")\n-        input_processor = processor(images=image_input, text=\"dummy\", return_tensors=\"np\")\n+        input_image_proc = image_processor(image_input, return_tensors=\"pt\")\n+        input_processor = processor(images=image_input, text=\"dummy\", return_tensors=\"pt\")\n \n         for key in input_image_proc.keys():\n             self.assertAlmostEqual(input_image_proc[key].sum(), input_processor[key].sum(), delta=1e-2)"
        },
        {
            "sha": "6ff2fa70c06b72ca8e060de2ceddbe33ce804e78",
            "filename": "tests/models/qwen2_vl/test_image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 35,
            "deletions": 4,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/17f02102c5c9d3dd8f83db92efafc71fad291090/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17f02102c5c9d3dd8f83db92efafc71fad291090/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py?ref=17f02102c5c9d3dd8f83db92efafc71fad291090",
            "patch": "@@ -22,7 +22,7 @@\n from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs, prepare_video_inputs\n \n@@ -35,8 +35,8 @@\n \n     from transformers import Qwen2VLImageProcessor\n \n-    # if is_torchvision_available():\n-    #     from transformers import Qwen2VLImageProcessorFast\n+    if is_torchvision_available():\n+        from transformers import Qwen2VLImageProcessorFast\n \n \n class Qwen2VLImageProcessingTester:\n@@ -119,7 +119,7 @@ def prepare_video_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class Qwen2VLImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = Qwen2VLImageProcessor if is_vision_available() else None\n-    # fast_image_processing_class = Qwen2VLImageProcessorFast if is_torchvision_available() else None\n+    fast_image_processing_class = Qwen2VLImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -363,3 +363,34 @@ def test_slow_fast_equivalence(self):\n         encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n \n         self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self.assertEqual(encoding_slow.image_grid_thw.dtype, encoding_fast.image_grid_thw.dtype)\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.image_grid_thw.float(), encoding_fast.image_grid_thw.float()\n+        )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self.assertEqual(encoding_slow.image_grid_thw.dtype, encoding_fast.image_grid_thw.dtype)\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.image_grid_thw.float(), encoding_fast.image_grid_thw.float()\n+        )"
        },
        {
            "sha": "eb5fdc79d071ec05f636e0d49cf50bd1f398b7fd",
            "filename": "tests/models/qwen2_vl/test_processor_qwen2_vl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/17f02102c5c9d3dd8f83db92efafc71fad291090/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17f02102c5c9d3dd8f83db92efafc71fad291090/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py?ref=17f02102c5c9d3dd8f83db92efafc71fad291090",
            "patch": "@@ -20,25 +20,26 @@\n import numpy as np\n import pytest\n \n-from transformers import AutoProcessor, Qwen2Tokenizer\n-from transformers.testing_utils import require_av, require_torch, require_vision\n+from transformers import AutoProcessor, Qwen2TokenizerFast\n+from transformers.testing_utils import require_av, require_torch, require_torchvision, require_vision\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n \n if is_vision_available():\n-    from transformers import Qwen2VLImageProcessor, Qwen2VLProcessor\n+    from transformers import Qwen2VLProcessor\n \n     if is_torchvision_available():\n-        from transformers import Qwen2VLVideoProcessor\n+        from transformers import Qwen2VLImageProcessorFast, Qwen2VLVideoProcessor\n \n if is_torch_available():\n     import torch\n \n \n @require_vision\n @require_torch\n+@require_torchvision\n class Qwen2VLProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = Qwen2VLProcessor\n \n@@ -76,12 +77,12 @@ def test_save_load_pretrained_default(self):\n             tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n         )\n         processor.save_pretrained(self.tmpdirname)\n-        processor = Qwen2VLProcessor.from_pretrained(self.tmpdirname, use_fast=False)\n+        processor = Qwen2VLProcessor.from_pretrained(self.tmpdirname, use_fast=True)\n \n         self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n         self.assertEqual(processor.image_processor.to_json_string(), image_processor.to_json_string())\n-        self.assertIsInstance(processor.tokenizer, Qwen2Tokenizer)\n-        self.assertIsInstance(processor.image_processor, Qwen2VLImageProcessor)\n+        self.assertIsInstance(processor.tokenizer, Qwen2TokenizerFast)\n+        self.assertIsInstance(processor.image_processor, Qwen2VLImageProcessorFast)\n         self.assertIsInstance(processor.video_processor, Qwen2VLVideoProcessor)\n \n     def test_image_processor(self):\n@@ -95,8 +96,8 @@ def test_image_processor(self):\n \n         image_input = self.prepare_image_inputs()\n \n-        input_image_proc = image_processor(image_input, return_tensors=\"np\")\n-        input_processor = processor(images=image_input, text=\"dummy\", return_tensors=\"np\")\n+        input_image_proc = image_processor(image_input, return_tensors=\"pt\")\n+        input_processor = processor(images=image_input, text=\"dummy\", return_tensors=\"pt\")\n \n         for key in input_image_proc.keys():\n             self.assertAlmostEqual(input_image_proc[key].sum(), input_processor[key].sum(), delta=1e-2)"
        },
        {
            "sha": "c9b9b09cbb6ffcbfb95f63741ee58693cab9d013",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/17f02102c5c9d3dd8f83db92efafc71fad291090/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17f02102c5c9d3dd8f83db92efafc71fad291090/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=17f02102c5c9d3dd8f83db92efafc71fad291090",
            "patch": "@@ -937,7 +937,7 @@ def test_apply_chat_template_video(self, batch_size: int, return_tensors: str):\n             \"video\", batch_size, return_tensors, \"videos_input_name\", \"video_processor\", MODALITY_INPUT_DATA[\"videos\"]\n         )\n \n-    @parameterized.expand([(1, \"np\"), (1, \"pt\"), (2, \"np\"), (2, \"pt\")])\n+    @parameterized.expand([(1, \"pt\"), (2, \"pt\")])  # fast image processors supports only torchvision\n     def test_apply_chat_template_image(self, batch_size: int, return_tensors: str):\n         self._test_apply_chat_template(\n             \"image\", batch_size, return_tensors, \"images_input_name\", \"image_processor\", MODALITY_INPUT_DATA[\"images\"]"
        }
    ],
    "stats": {
        "total": 505,
        "additions": 222,
        "deletions": 283
    }
}