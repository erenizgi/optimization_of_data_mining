{
    "author": "Rocketknight1",
    "message": "Fix doc builds (#42478)\n\nfix doc builds",
    "sha": "230c3234cb8720fc9bf467e55992c02aaa1db109",
    "files": [
        {
            "sha": "3aa48d7dd68710d19bffdf8898f19979e806e22a",
            "filename": "src/transformers/models/llama/tokenization_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/230c3234cb8720fc9bf467e55992c02aaa1db109/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/230c3234cb8720fc9bf467e55992c02aaa1db109/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py?ref=230c3234cb8720fc9bf467e55992c02aaa1db109",
            "patch": "@@ -77,15 +77,6 @@ class LlamaTokenizer(TokenizersBackend):\n             Whether or not to add an `eos_token` at the end of sequences.\n         use_default_system_prompt (`bool`, *optional*, defaults to `False`):\n             Whether or not the default system prompt for Llama should be used\n-            A simple example:\n-            ```python\n-    >>> from transformers import LlamaTokenizer\n-\n-            >>> tokenizer = LlamaTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n-            >>> tokenizer.encode(\"Hello <s>.\") # 869 is '‚ñÅ.'\n-            [1, 15043, 29871, 1, 869]\n-            ```\n-            Checkout the [pull request](https://github.com/huggingface/transformers/pull/24565) for more details.\n         add_prefix_space (`bool`, *optional*):\n             Whether or not the tokenizer should automatically add a prefix space\n     \"\"\""
        }
    ],
    "stats": {
        "total": 9,
        "additions": 0,
        "deletions": 9
    }
}