{
    "author": "YenFuLin",
    "message": "Add mean_resizing for every VLMs' resizing_token_embeddings() (#35717)\n\n* refine all resize_token_embedding()\n\n* ruff format\n\n* hotfix",
    "sha": "9d2056f12b66e64978f78a2dcb023f65b2be2108",
    "files": [
        {
            "sha": "eb23aa08bff2b8b8c039512a2d21a81cb847eccd",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -455,8 +455,9 @@ def resize_token_embeddings(\n         self,\n         new_num_tokens: Optional[int] = None,\n         pad_to_multiple_of=None,\n+        mean_resizing=True\n     ) -> nn.Embedding:\n-        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n \n         # Update vocab size\n         self.config.text_config.vocab_size = model_embeds.num_embeddings"
        },
        {
            "sha": "a67cf2752fb2093dad74615cbd07bee9e9faaf2a",
            "filename": "examples/modular-transformers/modular_new_task_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_new_task_model.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -73,8 +73,9 @@ def resize_token_embeddings(\n         self,\n         new_num_tokens: Optional[int] = None,\n         pad_to_multiple_of=None,\n+        mean_resizing=True\n     ) -> nn.Embedding:\n-        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n \n         # Update vocab size\n         self.config.text_config.vocab_size = model_embeds.num_embeddings"
        },
        {
            "sha": "56f8ce4d1006ef9fd2ca7e91a94f55c8a2ad4305",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 15,
            "deletions": 4,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -1189,11 +1189,11 @@ def set_output_embeddings(self, new_output_embeddings):\n         # one lm_head for each codebook\n         self.lm_heads = new_output_embeddings\n \n-    def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None):\n+    def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None, mean_resizing=True):\n         old_embeddings_list = self.get_input_embeddings()\n         new_embeddings_list = nn.ModuleList(\n             [\n-                self._get_resized_embeddings(old_embeddings, new_num_tokens, pad_to_multiple_of)\n+                self._get_resized_embeddings(old_embeddings, new_num_tokens, pad_to_multiple_of, mean_resizing)\n                 for old_embeddings in old_embeddings_list\n             ]\n         )\n@@ -1211,7 +1211,10 @@ def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None):\n         return self.get_input_embeddings()\n \n     def resize_token_embeddings(\n-        self, new_num_tokens: Optional[int] = None, pad_to_multiple_of: Optional[int] = None\n+        self,\n+        new_num_tokens: Optional[int] = None,\n+        pad_to_multiple_of: Optional[int] = None,\n+        mean_resizing: bool = True,\n     ) -> nn.Embedding:\n         \"\"\"\n         Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n@@ -1230,11 +1233,19 @@ def resize_token_embeddings(\n                 `>= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128. For more\n                 details about this, or help on choosing the correct value for resizing, refer to this guide:\n                 https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n+            mean_resizing (`bool`):\n+                Whether to initialize the added embeddings from a multivariate normal distribution that has old embeddings' mean and\n+                covariance or to initialize them with a normal distribution that has a mean of zero and std equals `config.initializer_range`.\n+\n+                Setting `mean_resizing` to `True` is useful when increasing the size of the embeddings of causal language models,\n+                where the generated tokens' probabilities won't be affected by the added embeddings because initializing the new embeddings with the\n+                old embeddings' mean will reduce the kl-divergence between the next token probability before and after adding the new embeddings.\n+                Refer to this article for more information: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n \n         Return:\n             `torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\n         \"\"\"\n-        model_embeds = self._resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+        model_embeds = self._resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         if new_num_tokens is None and pad_to_multiple_of is None:\n             return model_embeds\n "
        },
        {
            "sha": "e64ab3b2d0414fbe37bd294cd48efa4615f89a92",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -1577,8 +1577,10 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n-        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+    def resize_token_embeddings(\n+        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n+        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n         return new_embeddings\n "
        },
        {
            "sha": "61634901289507d5bab9d5cc7ec7d2b32fb81eb3",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -2457,8 +2457,10 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n-        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+    def resize_token_embeddings(\n+        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n+        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n         return new_embeddings\n "
        },
        {
            "sha": "16bea0a09f4162268334be7d72a534a166333cf7",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -1232,8 +1232,10 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n-        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+    def resize_token_embeddings(\n+        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n+        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n         return new_embeddings\n "
        },
        {
            "sha": "dec50328b76ea71520fb64d5507d58288c5e68f1",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -1184,8 +1184,10 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n-        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+    def resize_token_embeddings(\n+        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n+        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n         return new_embeddings\n "
        },
        {
            "sha": "7274f5c02c35a5fbf6d3a880064ec7156f84f4b4",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -1295,8 +1295,10 @@ def prepare_inputs_for_generation(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return self._shift_right(labels)\n \n-    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n-        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+    def resize_token_embeddings(\n+        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n+        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n         return new_embeddings\n "
        },
        {
            "sha": "e72ed197645cf1cb41f9054276c91845a6bf6649",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -2319,8 +2319,10 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.led.get_decoder()\n \n-    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n-        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+    def resize_token_embeddings(\n+        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n+        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n         return new_embeddings\n "
        },
        {
            "sha": "36dae0ee1d7eeac511c31ae1750a3905137112fc",
            "filename": "src/transformers/models/lxmert/modeling_lxmert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -1072,9 +1072,11 @@ def __init__(self, config):\n             }\n         self.visual_losses = visual_losses\n \n-    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n+    def resize_token_embeddings(\n+        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n         # Adding the following steps to resize bias to match the shape of resized embeddings\n-        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self.cls.predictions.bias = self._resize_bias(self.cls.predictions.bias, new_num_tokens)\n         return new_embeddings\n "
        },
        {
            "sha": "b64970e8063c733361035e67bc882263872c9242",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -1252,8 +1252,10 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n-        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+    def resize_token_embeddings(\n+        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n+        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         if self.config.share_encoder_decoder_embeddings:\n             self._resize_final_logits_bias(new_num_tokens)\n         return new_embeddings"
        },
        {
            "sha": "8b42755ce35d3c853e66f35c56fb2ee898af8c24",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -1546,8 +1546,10 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n-        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+    def resize_token_embeddings(\n+        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n+        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n         return new_embeddings\n "
        },
        {
            "sha": "ea1d12af0c8e11e2b5167edbdc738aee71a2f475",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -1370,8 +1370,10 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n-        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+    def resize_token_embeddings(\n+        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n+        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_num_tokens)\n         return new_embeddings\n "
        },
        {
            "sha": "31fcf1e44f15c39afa6feb5430e19d209e87b837",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -1658,9 +1658,11 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_backbone.model.set_input_embeddings(value)\n \n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n+    def resize_token_embeddings(\n+        self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n         model_embeds = self.language_backbone.model.resize_token_embeddings(\n-            new_num_tokens=new_num_tokens, pad_to_multiple_of=pad_to_multiple_of\n+            new_num_tokens=new_num_tokens, pad_to_multiple_of=pad_to_multiple_of, mean_resizing=mean_resizing\n         )\n         self.config.text_config.vocab_size = model_embeds.num_embeddings\n         self.vocab_size = model_embeds.num_embeddings"
        },
        {
            "sha": "fb560452a940445082c19d2b1c09b2bdedb6515c",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -1265,8 +1265,10 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n-        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+    def resize_token_embeddings(\n+        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n+        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n         return new_embeddings\n "
        },
        {
            "sha": "e2f11d97b8fd4df04472307afdc900b386b2ab18",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9d2056f12b66e64978f78a2dcb023f65b2be2108/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=9d2056f12b66e64978f78a2dcb023f65b2be2108",
            "patch": "@@ -1274,8 +1274,10 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n-        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n+    def resize_token_embeddings(\n+        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+    ) -> nn.Embedding:\n+        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n         return new_embeddings\n "
        }
    ],
    "stats": {
        "total": 103,
        "additions": 71,
        "deletions": 32
    }
}