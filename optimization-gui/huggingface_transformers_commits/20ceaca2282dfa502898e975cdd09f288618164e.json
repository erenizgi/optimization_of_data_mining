{
    "author": "keetrap",
    "message": "Add Fast owlvit Processor (#37164)\n\n* Add Fast Owlvit Processor\n\n* Update image_processing_owlvit_fast.py\n\n* Update image_processing_owlvit_fast.py\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "20ceaca2282dfa502898e975cdd09f288618164e",
    "files": [
        {
            "sha": "bbcfe0de9f982171644c7d118bfa4d22577414f4",
            "filename": "docs/source/en/model_doc/owlvit.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/20ceaca2282dfa502898e975cdd09f288618164e/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/20ceaca2282dfa502898e975cdd09f288618164e/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md?ref=20ceaca2282dfa502898e975cdd09f288618164e",
            "patch": "@@ -94,6 +94,11 @@ A demo notebook on using OWL-ViT for zero- and one-shot (image-guided) object de\n \n [[autodoc]] OwlViTImageProcessor\n     - preprocess\n+\n+## OwlViTImageProcessorFast\n+\n+[[autodoc]] OwlViTImageProcessorFast\n+    - preprocess\n     - post_process_object_detection\n     - post_process_image_guided_detection\n "
        },
        {
            "sha": "caeed80281156a4a6e8a7bd3767ef6502ef264e2",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20ceaca2282dfa502898e975cdd09f288618164e/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20ceaca2282dfa502898e975cdd09f288618164e/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=20ceaca2282dfa502898e975cdd09f288618164e",
            "patch": "@@ -123,7 +123,7 @@\n             (\"nougat\", (\"NougatImageProcessor\",)),\n             (\"oneformer\", (\"OneFormerImageProcessor\",)),\n             (\"owlv2\", (\"Owlv2ImageProcessor\",)),\n-            (\"owlvit\", (\"OwlViTImageProcessor\",)),\n+            (\"owlvit\", (\"OwlViTImageProcessor\", \"OwlViTImageProcessorFast\")),\n             (\"paligemma\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"perceiver\", (\"PerceiverImageProcessor\", \"PerceiverImageProcessorFast\")),\n             (\"phi4_multimodal\", \"Phi4MultimodalImageProcessorFast\"),"
        },
        {
            "sha": "b3fd80c2bcc6b268cd47a7afe321364ce3ed1aa5",
            "filename": "src/transformers/models/owlvit/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/20ceaca2282dfa502898e975cdd09f288618164e/src%2Ftransformers%2Fmodels%2Fowlvit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20ceaca2282dfa502898e975cdd09f288618164e/src%2Ftransformers%2Fmodels%2Fowlvit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2F__init__.py?ref=20ceaca2282dfa502898e975cdd09f288618164e",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_owlvit import *\n     from .feature_extraction_owlvit import *\n     from .image_processing_owlvit import *\n+    from .image_processing_owlvit_fast import *\n     from .modeling_owlvit import *\n     from .processing_owlvit import *\n else:"
        },
        {
            "sha": "f048f1a0da51e45d821faa2823b7cb074a1992eb",
            "filename": "src/transformers/models/owlvit/image_processing_owlvit_fast.py",
            "status": "added",
            "additions": 240,
            "deletions": 0,
            "changes": 240,
            "blob_url": "https://github.com/huggingface/transformers/blob/20ceaca2282dfa502898e975cdd09f288618164e/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20ceaca2282dfa502898e975cdd09f288618164e/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit_fast.py?ref=20ceaca2282dfa502898e975cdd09f288618164e",
            "patch": "@@ -0,0 +1,240 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for OwlViT\"\"\"\n+\n+import warnings\n+from typing import TYPE_CHECKING, List, Optional, Tuple, Union\n+\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BaseImageProcessorFast,\n+)\n+from ...image_transforms import center_to_corners_format\n+from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n+from ...utils import TensorType, add_start_docstrings, is_torch_available, logging\n+\n+\n+if TYPE_CHECKING:\n+    from .modeling_owlvit import OwlViTObjectDetectionOutput\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from .image_processing_owlvit import _scale_boxes, box_iou\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast OwlViT image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+)\n+class OwlViTImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = OPENAI_CLIP_MEAN\n+    image_std = OPENAI_CLIP_STD\n+    size = {\"height\": 768, \"width\": 768}\n+    default_to_square = True\n+    crop_size = {\"height\": 768, \"width\": 768}\n+    do_resize = True\n+    do_center_crop = False\n+    do_rescale = True\n+    do_normalize = None\n+    do_convert_rgb = None\n+    model_input_names = [\"pixel_values\"]\n+\n+    # Copied from transformers.models.owlvit.image_processing_owlvit.OwlViTImageProcessor.post_process\n+    def post_process(self, outputs, target_sizes):\n+        \"\"\"\n+        Converts the raw output of [`OwlViTForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format.\n+\n+        Args:\n+            outputs ([`OwlViTObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n+                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n+                image size (before any data augmentation). For visualization, this should be the image size after data\n+                augment, but before padding.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        # TODO: (amy) add support for other frameworks\n+        warnings.warn(\n+            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n+            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n+            FutureWarning,\n+        )\n+\n+        logits, boxes = outputs.logits, outputs.pred_boxes\n+\n+        if len(logits) != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n+        if target_sizes.shape[1] != 2:\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        probs = torch.max(logits, dim=-1)\n+        scores = torch.sigmoid(probs.values)\n+        labels = probs.indices\n+\n+        # Convert to [x0, y0, x1, y1] format\n+        boxes = center_to_corners_format(boxes)\n+\n+        # Convert from relative [0, 1] to absolute [0, height] coordinates\n+        img_h, img_w = target_sizes.unbind(1)\n+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n+        boxes = boxes * scale_fct[:, None, :]\n+\n+        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n+\n+        return results\n+\n+    # Copied from transformers.models.owlvit.image_processing_owlvit.OwlViTImageProcessor.post_process_object_detection\n+    def post_process_object_detection(\n+        self,\n+        outputs: \"OwlViTObjectDetectionOutput\",\n+        threshold: float = 0.1,\n+        target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`OwlViTForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format.\n+\n+        Args:\n+            outputs ([`OwlViTObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.1):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the following keys:\n+            - \"scores\": The confidence scores for each predicted box on the image.\n+            - \"labels\": Indexes of the classes predicted by the model on the image.\n+            - \"boxes\": Image bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format.\n+        \"\"\"\n+        batch_logits, batch_boxes = outputs.logits, outputs.pred_boxes\n+        batch_size = len(batch_logits)\n+\n+        if target_sizes is not None and len(target_sizes) != batch_size:\n+            raise ValueError(\"Make sure that you pass in as many target sizes as images\")\n+\n+        # batch_logits of shape (batch_size, num_queries, num_classes)\n+        batch_class_logits = torch.max(batch_logits, dim=-1)\n+        batch_scores = torch.sigmoid(batch_class_logits.values)\n+        batch_labels = batch_class_logits.indices\n+\n+        # Convert to [x0, y0, x1, y1] format\n+        batch_boxes = center_to_corners_format(batch_boxes)\n+\n+        # Convert from relative [0, 1] to absolute [0, height] coordinates\n+        if target_sizes is not None:\n+            batch_boxes = _scale_boxes(batch_boxes, target_sizes)\n+\n+        results = []\n+        for scores, labels, boxes in zip(batch_scores, batch_labels, batch_boxes):\n+            keep = scores > threshold\n+            scores = scores[keep]\n+            labels = labels[keep]\n+            boxes = boxes[keep]\n+            results.append({\"scores\": scores, \"labels\": labels, \"boxes\": boxes})\n+\n+        return results\n+\n+    # Copied from transformers.models.owlvit.image_processing_owlvit.OwlViTImageProcessor.post_process_image_guided_detection\n+    def post_process_image_guided_detection(self, outputs, threshold=0.0, nms_threshold=0.3, target_sizes=None):\n+        \"\"\"\n+        Converts the output of [`OwlViTForObjectDetection.image_guided_detection`] into the format expected by the COCO\n+        api.\n+\n+        Args:\n+            outputs ([`OwlViTImageGuidedObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                Minimum confidence threshold to use to filter out predicted boxes.\n+            nms_threshold (`float`, *optional*, defaults to 0.3):\n+                IoU threshold for non-maximum suppression of overlapping boxes.\n+            target_sizes (`torch.Tensor`, *optional*):\n+                Tensor of shape (batch_size, 2) where each entry is the (height, width) of the corresponding image in\n+                the batch. If set, predicted normalized bounding boxes are rescaled to the target sizes. If left to\n+                None, predictions will not be unnormalized.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model. All labels are set to None as\n+            `OwlViTForObjectDetection.image_guided_detection` perform one-shot object detection.\n+        \"\"\"\n+        logits, target_boxes = outputs.logits, outputs.target_pred_boxes\n+\n+        if target_sizes is not None and len(logits) != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n+        if target_sizes is not None and target_sizes.shape[1] != 2:\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        probs = torch.max(logits, dim=-1)\n+        scores = torch.sigmoid(probs.values)\n+\n+        # Convert to [x0, y0, x1, y1] format\n+        target_boxes = center_to_corners_format(target_boxes)\n+\n+        # Apply non-maximum suppression (NMS)\n+        if nms_threshold < 1.0:\n+            for idx in range(target_boxes.shape[0]):\n+                for i in torch.argsort(-scores[idx]):\n+                    if not scores[idx][i]:\n+                        continue\n+\n+                    ious = box_iou(target_boxes[idx][i, :].unsqueeze(0), target_boxes[idx])[0][0]\n+                    ious[i] = -1.0  # Mask self-IoU.\n+                    scores[idx][ious > nms_threshold] = 0.0\n+\n+        # Convert from relative [0, 1] to absolute [0, height] coordinates\n+        if target_sizes is not None:\n+            target_boxes = _scale_boxes(target_boxes, target_sizes)\n+\n+        # Compute box display alphas based on prediction scores\n+        results = []\n+        alphas = torch.zeros_like(scores)\n+\n+        for idx in range(target_boxes.shape[0]):\n+            # Select scores for boxes matching the current query:\n+            query_scores = scores[idx]\n+            if not query_scores.nonzero().numel():\n+                continue\n+\n+            # Apply threshold on scores before scaling\n+            query_scores[query_scores < threshold] = 0.0\n+\n+            # Scale box alpha such that the best box for each query has alpha 1.0 and the worst box has alpha 0.1.\n+            # All other boxes will either belong to a different query, or will not be shown.\n+            max_score = torch.max(query_scores) + 1e-6\n+            query_alphas = (query_scores - (max_score * 0.1)) / (max_score * 0.9)\n+            query_alphas = torch.clip(query_alphas, 0.0, 1.0)\n+            alphas[idx] = query_alphas\n+\n+            mask = alphas[idx] > 0\n+            box_scores = alphas[idx][mask]\n+            boxes = target_boxes[idx][mask]\n+            results.append({\"scores\": box_scores, \"labels\": None, \"boxes\": boxes})\n+\n+        return results\n+\n+\n+__all__ = [\"OwlViTImageProcessorFast\"]"
        },
        {
            "sha": "00b6512326794de9162c54c9960bd3432ed9e312",
            "filename": "tests/models/owlvit/test_image_processing_owlvit.py",
            "status": "modified",
            "additions": 23,
            "deletions": 17,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/20ceaca2282dfa502898e975cdd09f288618164e/tests%2Fmodels%2Fowlvit%2Ftest_image_processing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20ceaca2282dfa502898e975cdd09f288618164e/tests%2Fmodels%2Fowlvit%2Ftest_image_processing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_image_processing_owlvit.py?ref=20ceaca2282dfa502898e975cdd09f288618164e",
            "patch": "@@ -16,14 +16,17 @@\n import unittest\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n if is_vision_available():\n     from transformers import OwlViTImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import OwlViTImageProcessorFast\n+\n \n class OwlViTImageProcessingTester:\n     def __init__(\n@@ -89,6 +92,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class OwlViTImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = OwlViTImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = OwlViTImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -99,21 +103,23 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n-\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n+\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})"
        }
    ],
    "stats": {
        "total": 288,
        "additions": 270,
        "deletions": 18
    }
}