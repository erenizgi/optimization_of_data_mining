{
    "author": "zRzRzRzRzRzRzR",
    "message": "Glm 4 doc (#39247)\n\n* update the glm4 model readme\n\n* update test\n\n* update GLM-4.1V model\n\n* update as format\n\n* update\n\n* fix some tests\n\n* fix the rest\n\n* fix on a10, not t4\n\n* nit: dummy import\n\n---------\n\nCo-authored-by: raushan <raushan@huggingface.co>",
    "sha": "17b3c96c00cd8421bff85282aec32422bdfebd31",
    "files": [
        {
            "sha": "a7df833039deb0614e0400986457342105ae8117",
            "filename": "docs/source/en/model_doc/glm4.md",
            "status": "modified",
            "additions": 31,
            "deletions": 1,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/17b3c96c00cd8421bff85282aec32422bdfebd31/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17b3c96c00cd8421bff85282aec32422bdfebd31/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4.md?ref=17b3c96c00cd8421bff85282aec32422bdfebd31",
            "patch": "@@ -18,7 +18,37 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-To be released with the official model launch.\n+The GLM family welcomes new members [GLM-4-0414](https://arxiv.org/pdf/2406.12793) series models.\n+\n+The **GLM-4-32B-0414** series models, featuring 32 billion parameters. Its performance is comparable to OpenAI’s GPT\n+series and DeepSeek’s V3/R1 series. It also supports very user-friendly local deployment features. GLM-4-32B-Base-0414\n+was pre-trained on 15T of high-quality data, including substantial reasoning-type synthetic data. This lays the\n+foundation for subsequent reinforcement learning extensions. In the post-training stage, we employed human preference\n+alignment for dialogue scenarios. Additionally, using techniques like rejection sampling and reinforcement learning, we\n+enhanced the model’s performance in instruction following, engineering code, and function calling, thus strengthening\n+the atomic capabilities required for agent tasks. GLM-4-32B-0414 achieves good results in engineering code, Artifact\n+generation, function calling, search-based Q&A, and report generation. In particular, on several benchmarks, such as\n+code generation or specific Q&A tasks, GLM-4-32B-Base-0414 achieves comparable performance with those larger models like\n+GPT-4o and DeepSeek-V3-0324 (671B).\n+\n+**GLM-Z1-32B-0414** is a reasoning model with deep thinking capabilities. This was developed based on GLM-4-32B-0414\n+through cold start, extended reinforcement learning, and further training on tasks including mathematics, code, and\n+logic. Compared to the base model, GLM-Z1-32B-0414 significantly improves mathematical abilities and the capability to\n+solve complex tasks. During training, we also introduced general reinforcement learning based on pairwise ranking\n+feedback, which enhances the model's general capabilities.\n+\n+**GLM-Z1-Rumination-32B-0414** is a deep reasoning model with rumination capabilities (against OpenAI's Deep Research).\n+Unlike typical deep thinking models, the rumination model is capable of deeper and longer thinking to solve more\n+open-ended and complex problems (e.g., writing a comparative analysis of AI development in two cities and their future\n+development plans). Z1-Rumination is trained through scaling end-to-end reinforcement learning with responses graded by\n+the ground truth answers or rubrics and can make use of search tools during its deep thinking process to handle complex\n+tasks. The model shows significant improvements in research-style writing and complex tasks.\n+\n+Finally, **GLM-Z1-9B-0414** is a surprise. We employed all the aforementioned techniques to train a small model (9B).\n+GLM-Z1-9B-0414 exhibits excellent capabilities in mathematical reasoning and general tasks. Its overall performance is\n+top-ranked among all open-source models of the same size. Especially in resource-constrained scenarios, this model\n+achieves an excellent balance between efficiency and effectiveness, providing a powerful option for users seeking\n+lightweight deployment.\n \n ## Glm4Config\n "
        },
        {
            "sha": "08842421504f63f85bcdfbecd02051c7d7d25aef",
            "filename": "docs/source/en/model_doc/glm4v.md",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/17b3c96c00cd8421bff85282aec32422bdfebd31/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17b3c96c00cd8421bff85282aec32422bdfebd31/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md?ref=17b3c96c00cd8421bff85282aec32422bdfebd31",
            "patch": "@@ -23,6 +23,29 @@ rendered properly in your Markdown viewer.\n \n # GLM-4.1V\n \n+## Overview\n+\n+**GLM-4.1V-9B-Thinking** is a bilingual vision-language model optimized for reasoning, built on GLM-4-9B. It introduces\n+a \"thinking paradigm\" with reinforcement learning, achieving state-of-the-art results among 10B-class models and\n+rivaling 72B-scale models. It supports 64k context, 4K resolution, and arbitrary aspect ratios, with an open-source base\n+model for further research. You can check our paper [here](https://huggingface.co/papers/2507.01006). and below is a abstract.\n+\n+*We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal understanding\n+and reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework.\n+We first develop a capable vision foundation model with significant potential through large-scale pre-training, which\n+arguably sets the upper bound for the final performance. We then propose Reinforcement Learning with Curriculum\n+Sampling (RLCS) to unlock the full potential of the model, leading to comprehensive capability enhancement across a\n+diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding,\n+GUI-based agents, and long document understanding. We open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art\n+performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model\n+outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks\n+relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or\n+superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document\n+understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information\n+are released at https://github.com/THUDM/GLM-4.1V-Thinking.*\n+\n+## Usage\n+\n The example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n \n <hfoptions id=\"usage\">"
        },
        {
            "sha": "4f3ffed27e59e1f9a1e2d20a0f8b2a2818aa28d2",
            "filename": "src/transformers/models/glm4v/video_processing_glm4v.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/17b3c96c00cd8421bff85282aec32422bdfebd31/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17b3c96c00cd8421bff85282aec32422bdfebd31/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py?ref=17b3c96c00cd8421bff85282aec32422bdfebd31",
            "patch": "@@ -173,7 +173,10 @@ def _preprocess(\n                 timestamps_list.append(timestamps)\n                 processed_videos.append(video)\n         else:\n-            raise AssertionError(\"Must set `do_sample_frames=True` to sample frames from GLM-4.1V Model.\")\n+            # Assume 24 fps by default and prepare timestamps for the whole video when all frames are sampled\n+            processed_videos = videos\n+            timestamps_list = [[idx // 24 for idx in range(len(video))] for video in videos]\n+            timestamps_list = timestamps_list[::2]  # mrope\n \n         grouped_videos, grouped_videos_index = group_videos_by_shape(processed_videos)\n         resized_videos_grouped = {}"
        },
        {
            "sha": "39b66875c2d4495b91de51339f0d101c2c8c5a46",
            "filename": "tests/models/glm4v/test_modeling_glm4v.py",
            "status": "modified",
            "additions": 92,
            "deletions": 72,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/17b3c96c00cd8421bff85282aec32422bdfebd31/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17b3c96c00cd8421bff85282aec32422bdfebd31/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py?ref=17b3c96c00cd8421bff85282aec32422bdfebd31",
            "patch": "@@ -16,15 +16,12 @@\n import gc\n import unittest\n \n-import requests\n-\n from transformers import (\n     AutoProcessor,\n     Glm4vConfig,\n     Glm4vForConditionalGeneration,\n     Glm4vModel,\n     is_torch_available,\n-    is_vision_available,\n )\n from transformers.testing_utils import (\n     require_flash_attn,\n@@ -47,10 +44,6 @@\n     import torch\n \n \n-if is_vision_available():\n-    from PIL import Image\n-\n-\n class Glm4vVisionText2TextModelTester:\n     def __init__(\n         self,\n@@ -177,6 +170,8 @@ class Glm4vModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase)\n     all_model_classes = (Glm4vModel, Glm4vForConditionalGeneration) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n+    test_torchscript = False\n+    model_split_percents = [0.7, 0.9]  # model too big to split at 0.5\n     _is_composite = True\n \n     def setUp(self):\n@@ -264,22 +259,34 @@ def test_inputs_embeds_matches_input_ids(self):\n             torch.testing.assert_close(out_embeds, out_ids)\n \n \n-@unittest.skip(\"Model checkpoint not yet released\")\n @require_torch\n class Glm4vIntegrationTest(unittest.TestCase):\n     def setUp(self):\n-        self.processor = AutoProcessor.from_pretrained(\"z\")\n-        self.messages = [\n+        self.processor = AutoProcessor.from_pretrained(\"THUDM/GLM-4.1V-9B-Thinking\")\n+        self.message = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What kind of dog is this?\"},\n+                ],\n+            }\n+        ]\n+        self.message2 = [\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\",\n+                    },\n                     {\"type\": \"text\", \"text\": \"What kind of dog is this?\"},\n                 ],\n             }\n         ]\n-        url = \"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/demo_small.jpg\"\n-        self.image = Image.open(requests.get(url, stream=True).raw)\n \n     def tearDown(self):\n         gc.collect()\n@@ -291,20 +298,20 @@ def test_small_model_integration_test(self):\n             \"THUDM/GLM-4.1V-9B-Thinking\", torch_dtype=\"auto\", device_map=\"auto\"\n         )\n \n-        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n-        inputs = self.processor(text=[text], images=[self.image], return_tensors=\"pt\")\n-\n-        expected_input_ids = [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655]  # fmt: skip\n+        inputs = self.processor.apply_chat_template(\n+            self.message, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\"\n+        )\n+        expected_input_ids = [151331, 151333, 151336, 198, 151339, 151343, 151343, 151343, 151343, 151343, 151343, 151343, 151343, 151343, 151343, 151343, 151343]  # fmt: skip\n         assert expected_input_ids == inputs.input_ids[0].tolist()[:17]\n \n         expected_pixel_slice = torch.tensor(\n             [\n-                [0.8792, 0.8792, 0.9084],\n-                [1.1858, 1.1858, 1.2296],\n-                [1.2004, 1.2004, 1.2150],\n-                [1.4340, 1.4340, 1.4194],\n-                [1.3902, 1.4048, 1.4194],\n-                [1.5216, 1.5362, 1.5362],\n+                [-0.0988, -0.0842, -0.0842],\n+                [-0.5660, -0.5514, -0.4200],\n+                [-0.0259, -0.0259, -0.0259],\n+                [-0.1280, -0.0988, -0.2010],\n+                [-0.4638, -0.5806, -0.6974],\n+                [-1.2083, -1.2229, -1.2083],\n             ],\n             dtype=torch.float32,\n             device=\"cpu\",\n@@ -315,8 +322,7 @@ def test_small_model_integration_test(self):\n         inputs = inputs.to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=30)\n-        EXPECTED_DECODED_TEXT = \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices\"\n-\n+        EXPECTED_DECODED_TEXT = \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\"\n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,\n@@ -327,17 +333,17 @@ def test_small_model_integration_test_batch(self):\n         model = Glm4vForConditionalGeneration.from_pretrained(\n             \"THUDM/GLM-4.1V-9B-Thinking\", torch_dtype=\"auto\", device_map=\"auto\"\n         )\n-        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n-        inputs = self.processor(text=[text, text], images=[self.image, self.image], return_tensors=\"pt\").to(\n-            torch_device\n-        )\n+        batch_messages = [self.message] * 2\n+        inputs = self.processor.apply_chat_template(\n+            batch_messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(torch_device)\n \n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n-            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\",\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\"\n         ]  # fmt: skip\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n@@ -349,15 +355,15 @@ def test_small_model_integration_test_expand(self):\n         model = Glm4vForConditionalGeneration.from_pretrained(\n             \"THUDM/GLM-4.1V-9B-Thinking\", torch_dtype=\"auto\", device_map=\"auto\"\n         )\n-        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n-        inputs = self.processor(text=[text], images=[self.image], return_tensors=\"pt\").to(torch_device)\n+        inputs = self.processor.apply_chat_template(\n+            self.message, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(torch_device)\n \n-        output = model.generate(**inputs, max_new_tokens=30, num_return_sequences=3)\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, num_beams=2, num_return_sequences=2)\n \n         EXPECTED_DECODED_TEXT = [\n-            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n-            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n-            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat. Specifically\",\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat, specifically\"\n         ]  # fmt: skip\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n@@ -369,22 +375,25 @@ def test_small_model_integration_test_batch_wo_image(self):\n         model = Glm4vForConditionalGeneration.from_pretrained(\n             \"THUDM/GLM-4.1V-9B-Thinking\", torch_dtype=\"auto\", device_map=\"auto\"\n         )\n-        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n-        messages2 = [\n-            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n-            {\"role\": \"user\", \"content\": \"Who are you?\"},\n+        message_wo_image = [\n+            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who are you?\"}]},\n         ]\n-        text2 = self.processor.apply_chat_template(messages2, tokenize=False, add_generation_prompt=True)\n-        inputs = self.processor(text=[text, text2], images=[self.image], padding=True, return_tensors=\"pt\").to(\n-            torch_device\n-        )\n+        batched_messages = [self.message, message_wo_image]\n+        inputs = self.processor.apply_chat_template(\n+            batched_messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device)\n \n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n-            'system\\nYou are a helpful assistant.\\nuser\\nWho are you?\\nassistant\\nI am a large language model created by Alibaba Cloud. I am called Qwen.'\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\",\n+            '\\nWho are you?\\n<think>Got it, the user is asking \"Who are you?\" I need to respond appropriately. First, I should clarify that I\\'m an AI assistant'\n         ]  # fmt: skip\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n@@ -396,19 +405,22 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n         model = Glm4vForConditionalGeneration.from_pretrained(\n             \"THUDM/GLM-4.1V-9B-Thinking\", torch_dtype=\"auto\", device_map=\"auto\"\n         )\n-        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n-        text2 = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n-        image2 = self.image.resize((224, 224))\n-        inputs = self.processor(text=[text, text2], images=[self.image, image2], padding=True, return_tensors=\"pt\").to(\n-            torch_device\n-        )\n+        batched_messages = [self.message, self.message2]\n+        inputs = self.processor.apply_chat_template(\n+            batched_messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device)\n \n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n-            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular pets'\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture has a stocky build, thick fur, and a face that's\",\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. Wait, the animals here are cats, not dogs. The question is about a dog, but\"\n         ]  # fmt: skip\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n@@ -425,18 +437,23 @@ def test_small_model_integration_test_batch_flashatt2(self):\n             attn_implementation=\"flash_attention_2\",\n             device_map=\"auto\",\n         )\n-        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n-        inputs = self.processor(text=[text, text], images=[self.image, self.image], return_tensors=\"pt\").to(\n-            torch_device\n-        )\n+        batched_messages = [self.message, self.message2]\n+        inputs = self.processor.apply_chat_template(\n+            batched_messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device)\n \n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices\",\n-            \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices\",\n-        ]\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture has a stocky build, thick fur, and a face that's\",\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. Wait, the animals here are cats, not dogs. The question is about a dog, but\"\n+        ]  # fmt: skip\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,\n@@ -452,22 +469,25 @@ def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n             attn_implementation=\"flash_attention_2\",\n             device_map=\"auto\",\n         )\n-        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n-        messages2 = [\n-            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n-            {\"role\": \"user\", \"content\": \"Who are you?\"},\n+        message_wo_image = [\n+            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who are you?\"}]},\n         ]\n-        text2 = self.processor.apply_chat_template(messages2, tokenize=False, add_generation_prompt=True)\n-        inputs = self.processor(text=[text, text2], images=[self.image], padding=True, return_tensors=\"pt\").to(\n-            torch_device\n-        )\n+        batched_messages = [self.message, message_wo_image]\n+        inputs = self.processor.apply_chat_template(\n+            batched_messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device)\n \n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n-            'system\\nYou are a helpful assistant.\\nuser\\nWho are you?\\nassistant\\nI am a large language model created by Alibaba Cloud. I am called Qwen.'\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\",\n+            '\\nWho are you?\\n<think>Got it, let\\'s look at the question. The user is asking \"Who are you?\" which is a common question when someone meets an AI'\n         ]  # fmt: skip\n \n         self.assertEqual("
        },
        {
            "sha": "ec6e059f91f650fca143df7074c1306644fc0d00",
            "filename": "tests/test_video_processing_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/17b3c96c00cd8421bff85282aec32422bdfebd31/tests%2Ftest_video_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17b3c96c00cd8421bff85282aec32422bdfebd31/tests%2Ftest_video_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_video_processing_common.py?ref=17b3c96c00cd8421bff85282aec32422bdfebd31",
            "patch": "@@ -176,10 +176,12 @@ def test_can_compile_fast_video_processor(self):\n         torch.compiler.reset()\n         video_inputs = self.video_processor_tester.prepare_video_inputs(equal_resolution=False, return_tensors=\"torch\")\n         video_processor = self.fast_video_processing_class(**self.video_processor_dict)\n-        output_eager = video_processor(video_inputs, device=torch_device, return_tensors=\"pt\")\n+        output_eager = video_processor(video_inputs, device=torch_device, do_sample_frames=False, return_tensors=\"pt\")\n \n         video_processor = torch.compile(video_processor, mode=\"reduce-overhead\")\n-        output_compiled = video_processor(video_inputs, device=torch_device, return_tensors=\"pt\")\n+        output_compiled = video_processor(\n+            video_inputs, device=torch_device, do_sample_frames=False, return_tensors=\"pt\"\n+        )\n \n         torch.testing.assert_close(\n             output_eager[self.input_name], output_compiled[self.input_name], rtol=1e-4, atol=1e-4"
        }
    ],
    "stats": {
        "total": 230,
        "additions": 154,
        "deletions": 76
    }
}