{
    "author": "yao-matrix",
    "message": "enable finegrained_fp8 and granite_speech cases on XPU (#38036)\n\n* enable finegrained_fp8 cases on XPU\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n* change back to auto\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n* rename per comments\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "9b5ce556aae70c5d201b20a20c4a50aa89a2176c",
    "files": [
        {
            "sha": "c259a7dbfaf92183aff37e406259e25840300814",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b5ce556aae70c5d201b20a20c4a50aa89a2176c/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b5ce556aae70c5d201b20a20c4a50aa89a2176c/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=9b5ce556aae70c5d201b20a20c4a50aa89a2176c",
            "patch": "@@ -15,7 +15,7 @@\n \n from typing import List, Optional, Tuple\n \n-from ..utils import is_accelerate_available, is_torch_available, logging\n+from ..utils import is_accelerate_available, is_torch_accelerator_available, is_torch_available, logging\n \n \n if is_torch_available():\n@@ -332,8 +332,10 @@ def forward(self, input: torch.Tensor) -> torch.Tensor:\n         if self.weight.element_size() > 1:\n             return F.linear(input, self.weight, self.bias)\n         else:\n-            # Context manager used to switch among the available cuda devices\n-            with torch.cuda.device(input.device):\n+            # Context manager used to switch among the available accelerators\n+            device_type = torch.accelerator.current_accelerator().type if is_torch_accelerator_available() else \"cuda\"\n+            torch_accelerator_module = getattr(torch, device_type, torch.cuda)\n+            with torch_accelerator_module.device(input.device):\n                 qinput, scale = act_quant(input, self.block_size[1])\n                 output = w8a8_block_fp8_matmul_triton(\n                     qinput,\n@@ -343,9 +345,9 @@ def forward(self, input: torch.Tensor) -> torch.Tensor:\n                     self.block_size,\n                     output_dtype=input.dtype,\n                 )\n-            # Blocks the CPU until all CUDA operations on the specified device are complete. It is used to ensure that the results of the\n+            # Blocks the CPU until all accelerator operations on the specified device are complete. It is used to ensure that the results of the\n             # preceding operations are ready before proceeding\n-            torch.cuda.synchronize()\n+            torch_accelerator_module.synchronize()\n             if self.bias is not None:\n                 output = output + self.bias\n             return output.to(dtype=input.dtype)"
        },
        {
            "sha": "ed87b70bbc9b459138db444aa5a1f0a9bd8da903",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b5ce556aae70c5d201b20a20c4a50aa89a2176c/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b5ce556aae70c5d201b20a20c4a50aa89a2176c/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=9b5ce556aae70c5d201b20a20c4a50aa89a2176c",
            "patch": "@@ -1,6 +1,6 @@\n from typing import TYPE_CHECKING, Any, Dict, List, Optional\n \n-from ..utils import is_accelerate_available, is_torch_available, logging\n+from ..utils import is_accelerate_available, is_torch_available, is_torch_xpu_available, logging\n from .base import HfQuantizer\n from .quantizers_utils import get_module_from_name\n \n@@ -44,16 +44,17 @@ def validate_environment(self, *args, **kwargs):\n                 \"please make sure the weights are in PyTorch format.\"\n             )\n \n-        if not torch.cuda.is_available():\n-            raise RuntimeError(\"No GPU found. A GPU is needed for FP8 quantization.\")\n+        if not (torch.cuda.is_available() or is_torch_xpu_available()):\n+            raise RuntimeError(\"No GPU or XPU found. A GPU or XPU is needed for FP8 quantization.\")\n \n-        compute_capability = torch.cuda.get_device_capability()\n-        major, minor = compute_capability\n-        if (major < 8) or (major == 8 and minor < 9):\n-            raise ValueError(\n-                \"FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100)\"\n-                f\", actual = `{major}.{minor}`\"\n-            )\n+        if torch.cuda.is_available():\n+            compute_capability = torch.cuda.get_device_capability()\n+            major, minor = compute_capability\n+            if (major < 8) or (major == 8 and minor < 9):\n+                raise ValueError(\n+                    \"FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100)\"\n+                    f\", actual = `{major}.{minor}`\"\n+                )\n \n         device_map = kwargs.get(\"device_map\", None)\n         if device_map is None:\n@@ -217,7 +218,7 @@ def update_tp_plan(self, config):\n \n             config.base_model_tp_plan = text_plan\n \n-            return config\n+        return config\n \n     def is_serializable(self, safe_serialization=None):\n         return True"
        },
        {
            "sha": "569ac9cfbc191c7afa36b073bf1b6d6df69d0e59",
            "filename": "tests/models/granite_speech/test_processor_granite_speech.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b5ce556aae70c5d201b20a20c4a50aa89a2176c/tests%2Fmodels%2Fgranite_speech%2Ftest_processor_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b5ce556aae70c5d201b20a20c4a50aa89a2176c/tests%2Fmodels%2Fgranite_speech%2Ftest_processor_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2Ftest_processor_granite_speech.py?ref=9b5ce556aae70c5d201b20a20c4a50aa89a2176c",
            "patch": "@@ -23,8 +23,9 @@\n from transformers import AutoTokenizer, GPT2TokenizerFast\n from transformers.testing_utils import (\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     require_torchaudio,\n+    torch_device,\n )\n from transformers.utils import is_torchaudio_available\n \n@@ -195,7 +196,7 @@ def test_audio_token_filling_varying_len_feature_list(self):\n         assert num_calculated_features == [90, 171]\n         assert sum(num_expected_features) == num_audio_tokens\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_device_override(self):\n         \"\"\"Ensure that we regardless of the processing device, the tensors\n         produced are on the CPU.\n@@ -214,7 +215,7 @@ def test_device_override(self):\n             text=f\"{processor.audio_token} Can you transcribe this audio?\",\n             audio=wav,\n             return_tensors=\"pt\",\n-            device=\"cuda\",\n+            device=torch_device,\n         )\n \n         assert inputs[\"input_features\"].device.type == \"cpu\""
        },
        {
            "sha": "b5a586b0302fb737f9d64f8ae32ecc0c47d4acc4",
            "filename": "tests/quantization/finegrained_fp8/test_fp8.py",
            "status": "modified",
            "additions": 19,
            "deletions": 15,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b5ce556aae70c5d201b20a20c4a50aa89a2176c/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b5ce556aae70c5d201b20a20c4a50aa89a2176c/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py?ref=9b5ce556aae70c5d201b20a20c4a50aa89a2176c",
            "patch": "@@ -18,11 +18,13 @@\n \n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, FineGrainedFP8Config, OPTForCausalLM\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_accelerate,\n     require_read_token,\n-    require_torch_gpu,\n-    require_torch_multi_gpu,\n+    require_torch_accelerator,\n+    require_torch_multi_accelerator,\n     slow,\n+    torch_device,\n )\n from transformers.utils import is_accelerate_available, is_torch_available\n \n@@ -34,7 +36,7 @@\n     from accelerate import init_empty_weights\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class FineGrainedFP8ConfigTest(unittest.TestCase):\n     def test_to_dict(self):\n         \"\"\"\n@@ -60,13 +62,13 @@ def test_from_dict(self):\n @slow\n @require_accelerate\n @require_read_token\n-@require_torch_gpu\n+@require_torch_accelerator\n class FP8QuantizerTest(unittest.TestCase):\n     model_name = \"meta-llama/Llama-3.2-1B\"\n     input_text = \"Once upon a time\"\n     max_new_tokens = 10\n     EXPECTED_OUTPUT = \"Once upon a time, there was a man who was very rich.\"\n-    device_map = \"cuda\"\n+    device_map = torch_device\n     offload_device_map = {\n         \"model.embed_tokens\": 0,\n         \"model.layers.0\": 0,\n@@ -103,7 +105,7 @@ def setUpClass(cls):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_quantized_model_conversion(self):\n@@ -151,7 +153,8 @@ def test_quantized_model(self):\n         input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(self.device_map)\n \n         output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n-        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+        output_tokens = self.tokenizer.decode(output[0], skip_special_tokens=True)\n+        self.assertEqual(output_tokens, self.EXPECTED_OUTPUT)\n \n     def test_save_pretrained(self):\n         \"\"\"\n@@ -188,11 +191,12 @@ def test_block_size(self):\n         )\n         self.assertEqual(quantized_model.config.quantization_config.weight_block_size, (32, 32))\n \n-    @require_torch_multi_gpu\n-    def test_quantized_model_multi_gpu(self):\n+    @require_torch_multi_accelerator\n+    def test_quantized_model_multi_accelerator(self):\n         \"\"\"\n-        Simple test that checks if the quantized model is working properly with multiple GPUs\n-        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs\n+        Simple test that checks if the quantized model is working properly with multiple accelerators\n+        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs; or set ZE_AFFINITY_MASK=0,1 if you\n+        have more than 2 XPUs.\n         \"\"\"\n         input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(self.device_map)\n         quantization_config = FineGrainedFP8Config()\n@@ -204,8 +208,8 @@ def test_quantized_model_multi_gpu(self):\n         output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n         self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n-    @require_torch_multi_gpu\n-    def test_save_pretrained_multi_gpu(self):\n+    @require_torch_multi_accelerator\n+    def test_save_pretrained_multi_accelerators(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly after being saved and loaded\n         \"\"\"\n@@ -245,9 +249,9 @@ def test_save_pretrained_offload(self):\n             self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class FP8LinearTest(unittest.TestCase):\n-    device = \"cuda\"\n+    device = torch_device\n \n     @unittest.skipIf(\n         torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 9,"
        }
    ],
    "stats": {
        "total": 76,
        "additions": 42,
        "deletions": 34
    }
}