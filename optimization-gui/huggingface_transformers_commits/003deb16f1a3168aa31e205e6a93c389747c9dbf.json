{
    "author": "LysandreJik",
    "message": "Support for transformers explicit filename (#38152)\n\n* Support for transformers explicit filename\n\n* Tests\n\n* Rerun tests",
    "sha": "003deb16f1a3168aa31e205e6a93c389747c9dbf",
    "files": [
        {
            "sha": "18cc0ff3a5dd7dc93b67e00ed2892faff51b1e19",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/003deb16f1a3168aa31e205e6a93c389747c9dbf/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/003deb16f1a3168aa31e205e6a93c389747c9dbf/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=003deb16f1a3168aa31e205e6a93c389747c9dbf",
            "patch": "@@ -408,6 +408,10 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n             repo_id = self._create_repo(repo_id, **kwargs)\n             files_timestamps = self._get_files_timestamps(save_directory)\n \n+        # This attribute is important to know on load, but should not be serialized on save.\n+        if \"transformers_weights\" in self:\n+            delattr(self, \"transformers_weights\")\n+\n         # If we have a custom config, we copy the file defining it in the folder and set the attributes so it can be\n         # loaded from the Hub.\n         if self._auto_class is not None:"
        },
        {
            "sha": "6e4c631d481db646440c9358abd93c1aa232ee50",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 23,
            "deletions": 2,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/003deb16f1a3168aa31e205e6a93c389747c9dbf/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/003deb16f1a3168aa31e205e6a93c389747c9dbf/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=003deb16f1a3168aa31e205e6a93c389747c9dbf",
            "patch": "@@ -881,6 +881,7 @@ def _get_resolved_checkpoint_files(\n     user_agent: dict,\n     revision: str,\n     commit_hash: Optional[str],\n+    transformers_explicit_filename: Optional[str] = None,\n ) -> Tuple[Optional[List[str]], Optional[Dict]]:\n     \"\"\"Get all the checkpoint filenames based on `pretrained_model_name_or_path`, and optional metadata if the\n     checkpoints are sharded.\n@@ -892,7 +893,11 @@ def _get_resolved_checkpoint_files(\n         pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n         is_local = os.path.isdir(pretrained_model_name_or_path)\n         if is_local:\n-            if from_tf and os.path.isfile(\n+            if transformers_explicit_filename is not None:\n+                # If the filename is explicitly defined, load this by default.\n+                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, transformers_explicit_filename)\n+                is_sharded = transformers_explicit_filename.endswith(\".safetensors.index.json\")\n+            elif from_tf and os.path.isfile(\n                 os.path.join(pretrained_model_name_or_path, subfolder, TF_WEIGHTS_NAME + \".index\")\n             ):\n                 # Load from a TF 1.0 checkpoint in priority if from_tf\n@@ -980,7 +985,10 @@ def _get_resolved_checkpoint_files(\n             resolved_archive_file = download_url(pretrained_model_name_or_path)\n         else:\n             # set correct filename\n-            if from_tf:\n+            if transformers_explicit_filename is not None:\n+                filename = transformers_explicit_filename\n+                is_sharded = transformers_explicit_filename.endswith(\".safetensors.index.json\")\n+            elif from_tf:\n                 filename = TF2_WEIGHTS_NAME\n             elif from_flax:\n                 filename = FLAX_WEIGHTS_NAME\n@@ -4362,6 +4370,18 @@ def from_pretrained(\n \n             model_kwargs = kwargs\n \n+        transformers_explicit_filename = getattr(config, \"transformers_weights\", None)\n+\n+        if transformers_explicit_filename is not None:\n+            if not transformers_explicit_filename.endswith(\n+                \".safetensors\"\n+            ) and not transformers_explicit_filename.endswith(\".safetensors.index.json\"):\n+                raise ValueError(\n+                    \"The transformers file in the config seems to be incorrect: it is neither a safetensors file \"\n+                    \"(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \"\n+                    f\"{transformers_explicit_filename}\"\n+                )\n+\n         pre_quantized = hasattr(config, \"quantization_config\")\n         if pre_quantized and not AutoHfQuantizer.supports_quant_method(config.quantization_config):\n             pre_quantized = False\n@@ -4430,6 +4450,7 @@ def from_pretrained(\n             user_agent=user_agent,\n             revision=revision,\n             commit_hash=commit_hash,\n+            transformers_explicit_filename=transformers_explicit_filename,\n         )\n \n         is_sharded = sharded_metadata is not None"
        },
        {
            "sha": "896b8771c418156c4a7400c442c31691dfecfd0d",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 74,
            "deletions": 0,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/003deb16f1a3168aa31e205e6a93c389747c9dbf/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/003deb16f1a3168aa31e205e6a93c389747c9dbf/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=003deb16f1a3168aa31e205e6a93c389747c9dbf",
            "patch": "@@ -1958,6 +1958,80 @@ def test_loading_is_fast_on_gpu(self, model_id: str, max_loading_time: float):\n             except subprocess.CalledProcessError as e:\n                 raise Exception(f\"The following error was captured: {e.stderr}\")\n \n+    def test_explicit_transformers_weights(self):\n+        \"\"\"\n+        Transformers supports loading from repos where the weights file is explicitly set in the config.\n+        When loading a config file, transformers will see whether `transformers_weights` is defined in the config.\n+        If so, it will load from that file.\n+\n+        Here, we ensure that the correct file is loaded.\n+        \"\"\"\n+        model = BertModel.from_pretrained(\"hf-internal-testing/explicit_transformers_weight_in_config\")\n+        self.assertEqual(model.num_parameters(), 87929)\n+\n+    def test_explicit_transformers_weights_index(self):\n+        \"\"\"\n+        Transformers supports loading from repos where the weights file is explicitly set in the config.\n+        When loading a config file, transformers will see whether `transformers_weights` is defined in the config.\n+        If so, it will load from that file.\n+\n+        Here, we ensure that the correct file is loaded, given the file is an index of multiple weights.\n+        \"\"\"\n+        model = BertModel.from_pretrained(\"hf-internal-testing/explicit_transformers_weight_in_config_sharded\")\n+        self.assertEqual(model.num_parameters(), 87929)\n+\n+    def test_explicit_transformers_weights_save_and_reload(self):\n+        \"\"\"\n+        Transformers supports loading from repos where the weights file is explicitly set in the config.\n+        When loading a config file, transformers will see whether `transformers_weights` is defined in the config.\n+        If so, it will load from that file.\n+\n+        When saving the model, we should be careful not to safe the `transformers_weights` attribute in the config;\n+        otherwise, transformers will try to load from that file whereas it should simply load from the default file.\n+\n+        We test that for a non-sharded repo.\n+        \"\"\"\n+        model = BertModel.from_pretrained(\"hf-internal-testing/explicit_transformers_weight_in_config\")\n+        explicit_transformers_weights = model.config.transformers_weights\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            model.save_pretrained(tmpdirname)\n+\n+            # The config should not have a mention of transformers_weights\n+            with open(os.path.join(tmpdirname, \"config.json\")) as f:\n+                config = json.loads(f.read())\n+                self.assertFalse(\"transformers_weights\" in config)\n+\n+            # The serialized weights should be in model.safetensors and not the transformers_weights\n+            self.assertTrue(explicit_transformers_weights not in os.listdir(tmpdirname))\n+            self.assertTrue(\"model.safetensors\" in os.listdir(tmpdirname))\n+\n+    def test_explicit_transformers_weights_index_save_and_reload(self):\n+        \"\"\"\n+        Transformers supports loading from repos where the weights file is explicitly set in the config.\n+        When loading a config file, transformers will see whether `transformers_weights` is defined in the config.\n+        If so, it will load from that file.\n+\n+        When saving the model, we should be careful not to safe the `transformers_weights` attribute in the config;\n+        otherwise, transformers will try to load from that file whereas it should simply load from the default file.\n+\n+        We test that for a sharded repo.\n+        \"\"\"\n+        model = BertModel.from_pretrained(\"hf-internal-testing/explicit_transformers_weight_in_config_sharded\")\n+        explicit_transformers_weights = model.config.transformers_weights\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            model.save_pretrained(tmpdirname, max_shard_size=\"100kb\")\n+\n+            # The config should not have a mention of transformers_weights\n+            with open(os.path.join(tmpdirname, \"config.json\")) as f:\n+                config = json.loads(f.read())\n+                self.assertFalse(\"transformers_weights\" in config)\n+\n+            # The serialized weights should be in model.safetensors and not the transformers_weights\n+            self.assertTrue(explicit_transformers_weights not in os.listdir(tmpdirname))\n+            self.assertTrue(\"model.safetensors.index.json\" in os.listdir(tmpdirname))\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 103,
        "additions": 101,
        "deletions": 2
    }
}