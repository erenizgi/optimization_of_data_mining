{
    "author": "molbap",
    "message": "Minor Gemma 3 fixes  (#36884)\n\nfix attention mask dtype + outputs type",
    "sha": "3f9ff19b4ec7dcf4112225079f26ea756aafd211",
    "files": [
        {
            "sha": "70b96c6f0bdba1bb20649d2754aa6acbe1e699ec",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f9ff19b4ec7dcf4112225079f26ea756aafd211/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f9ff19b4ec7dcf4112225079f26ea756aafd211/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=3f9ff19b4ec7dcf4112225079f26ea756aafd211",
            "patch": "@@ -361,13 +361,15 @@ def forward(\n                 )\n             else:\n                 attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n-\n+        if attention_mask is not None:\n+            # backwards compatibility\n+            attention_mask = attention_mask.to(query_states)\n         attn_output, attn_weights = attention_interface(\n             self,\n             query_states,\n             key_states,\n             value_states,\n-            attention_mask.to(query_states),\n+            attention_mask,\n             dropout=self.attention_dropout if self.training else 0.0,\n             scaling=self.scaling,\n             sliding_window=self.sliding_window,\n@@ -1360,7 +1362,7 @@ def forward(\n             **lm_kwargs,\n         )\n \n-        logits = outputs.logits\n+        logits = outputs[0]\n         loss = None\n         if labels is not None:\n             # Upcast to float if we need to compute the loss to avoid potential precision issues"
        },
        {
            "sha": "8684912775735b659b7fd476d316e4c57e8ecf73",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f9ff19b4ec7dcf4112225079f26ea756aafd211/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f9ff19b4ec7dcf4112225079f26ea756aafd211/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=3f9ff19b4ec7dcf4112225079f26ea756aafd211",
            "patch": "@@ -418,13 +418,15 @@ def forward(\n                 )\n             else:\n                 attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n-\n+        if attention_mask is not None:\n+            # backwards compatibility\n+            attention_mask = attention_mask.to(query_states)\n         attn_output, attn_weights = attention_interface(\n             self,\n             query_states,\n             key_states,\n             value_states,\n-            attention_mask.to(query_states),\n+            attention_mask,\n             dropout=self.attention_dropout if self.training else 0.0,\n             scaling=self.scaling,\n             sliding_window=self.sliding_window,\n@@ -974,7 +976,7 @@ def forward(\n             **lm_kwargs,\n         )\n \n-        logits = outputs.logits\n+        logits = outputs[0]\n         loss = None\n         if labels is not None:\n             # Upcast to float if we need to compute the loss to avoid potential precision issues"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 10,
        "deletions": 6
    }
}