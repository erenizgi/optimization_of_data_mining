{
    "author": "vasqu",
    "message": "Fix ernie moe (#42535)\n\n* fix\n\n* style",
    "sha": "0fa49db1205e0a2745161ccac46184e7e46b6e2b",
    "files": [
        {
            "sha": "27602d04d7a123f1d88b36317e608b9f5fc8f8c4",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 32,
            "deletions": 45,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fa49db1205e0a2745161ccac46184e7e46b6e2b/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fa49db1205e0a2745161ccac46184e7e46b6e2b/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=0fa49db1205e0a2745161ccac46184e7e46b6e2b",
            "patch": "@@ -318,51 +318,41 @@ def forward(self, hidden_states):\n \n \n class Ernie4_5_MoeExperts(nn.Module):\n+    \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n+\n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.moe_num_experts\n         self.hidden_dim = config.hidden_size\n         self.intermediate_dim = config.moe_intermediate_size\n-        self.use_bias = config.use_bias\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n         self.act_fn = ACT2FN[config.hidden_act]\n \n-        self.gate_up_proj = nn.Parameter(torch.zeros(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n-        self.down_proj = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim, self.intermediate_dim))\n-        if self.use_bias:\n-            self.gate_up_proj_bias = nn.Parameter(torch.zeros(self.num_experts, 2 * self.intermediate_dim))\n-            self.down_proj_bias = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim))\n-        else:\n-            self.gate_up_proj_bias = None\n-            self.down_proj_bias = None\n-\n     def forward(\n-        self, hidden_states: torch.Tensor, selected_experts: torch.Tensor, routing_weights: torch.Tensor\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        if selected_experts.numel() == 0:\n-            return final_hidden_states\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n-\n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hit:\n-            expert_idx = int(expert_idx.item())\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            gate_inputs = F.linear(\n-                current_state,\n-                self.gate_up_proj[expert_idx],\n-                None if self.gate_up_proj_bias is None else self.gate_up_proj_bias[expert_idx],\n-            )\n-            gate, up = gate_inputs.chunk(2, dim=-1)\n+            expert_idx = expert_idx[0]\n+            if expert_idx == self.num_experts:\n+                continue\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states[token_idx]\n+            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n-            current_hidden_states = F.linear(\n-                current_hidden_states,\n-                self.down_proj[expert_idx],\n-                None if self.down_proj_bias is None else self.down_proj_bias[expert_idx],\n-            )\n-            current_hidden_states = current_hidden_states * routing_weights[top_x, idx, None]\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n         return final_hidden_states\n \n \n@@ -383,14 +373,14 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n \n         with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             router_logits = F.linear(hidden_states.float(), self.weight)\n-            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n-            routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n-            routing_weights = routing_weights / torch.clamp(\n-                routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n+            router_logits = F.softmax(router_logits, dim=1, dtype=torch.float)\n+            router_top_value, router_indices = torch.topk(self.moe_statics(router_logits), self.top_k, dim=-1)\n+            router_top_value = router_top_value / torch.clamp(\n+                router_top_value.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n-        routing_weights = routing_weights.to(router_logits.dtype)\n-        return routing_weights, selected_experts\n+            router_scores = router_top_value\n+        router_scores = router_scores.to(hidden_states.dtype)\n+        return router_logits, router_scores, router_indices\n \n \n class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n@@ -413,8 +403,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         if self.shared_experts is not None:\n             shared_output = self.shared_experts(hidden_states)\n \n-        routing_weights, selected_experts = self.gate(hidden_states)\n-        final_hidden_states = self.experts(hidden_states, selected_experts, routing_weights)\n+        _, top_k_weights, top_k_index = self.gate(hidden_states)\n+        final_hidden_states = self.experts(hidden_states, top_k_index, top_k_weights)\n \n         if self.shared_experts is not None:\n             final_hidden_states = final_hidden_states + shared_output\n@@ -489,7 +479,7 @@ class Ernie4_5_MoePreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(Ernie4_5_MoeTopKRouter, layer_name=\"mlp.gate\", index=0),\n+        \"router_logits\": OutputRecorder(Ernie4_5_MoeTopKRouter, index=0),\n         \"hidden_states\": Ernie4_5_MoeDecoderLayer,\n         \"attentions\": Ernie4_5_MoeAttention,\n     }\n@@ -505,9 +495,6 @@ def _init_weights(self, module):\n         elif isinstance(module, Ernie4_5_MoeExperts):\n             init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n             init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n-            if module.gate_up_proj_bias is not None:\n-                init.zeros_(module.gate_up_proj_bias)\n-                init.zeros_(module.down_proj_bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "74feb925a7ce2af636482ee9be517607810947e2",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 30,
            "deletions": 49,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/0fa49db1205e0a2745161ccac46184e7e46b6e2b/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0fa49db1205e0a2745161ccac46184e7e46b6e2b/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=0fa49db1205e0a2745161ccac46184e7e46b6e2b",
            "patch": "@@ -20,7 +20,6 @@\n from torch import nn\n \n from ... import initialization as init\n-from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import MoeModelOutputWithPast\n@@ -31,6 +30,7 @@\n from ..ernie4_5.modeling_ernie4_5 import Ernie4_5RotaryEmbedding, apply_rotary_pos_emb, rotate_half  # noqa: F401\n from ..llama.modeling_llama import LlamaAttention, LlamaRMSNorm\n from ..mixtral.modeling_mixtral import (\n+    MixtralExperts,\n     MixtralForCausalLM,\n     MixtralPreTrainedModel,\n )\n@@ -98,52 +98,36 @@ def forward(self, hidden_states):\n         return hidden_states + self.e_score_correction_bias.squeeze()\n \n \n-class Ernie4_5_MoeExperts(nn.Module):\n+class Ernie4_5_MoeExperts(MixtralExperts):\n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.moe_num_experts\n-        self.hidden_dim = config.hidden_size\n         self.intermediate_dim = config.moe_intermediate_size\n-        self.use_bias = config.use_bias\n-        self.act_fn = ACT2FN[config.hidden_act]\n-\n-        self.gate_up_proj = nn.Parameter(torch.zeros(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n-        self.down_proj = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim, self.intermediate_dim))\n-        if self.use_bias:\n-            self.gate_up_proj_bias = nn.Parameter(torch.zeros(self.num_experts, 2 * self.intermediate_dim))\n-            self.down_proj_bias = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim))\n-        else:\n-            self.gate_up_proj_bias = None\n-            self.down_proj_bias = None\n \n     def forward(\n-        self, hidden_states: torch.Tensor, selected_experts: torch.Tensor, routing_weights: torch.Tensor\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        if selected_experts.numel() == 0:\n-            return final_hidden_states\n-\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hit:\n-            expert_idx = int(expert_idx.item())\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n-            gate_inputs = F.linear(\n-                current_state,\n-                self.gate_up_proj[expert_idx],\n-                None if self.gate_up_proj_bias is None else self.gate_up_proj_bias[expert_idx],\n-            )\n-            gate, up = gate_inputs.chunk(2, dim=-1)\n+            expert_idx = expert_idx[0]\n+            if expert_idx == self.num_experts:\n+                continue\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states[token_idx]\n+            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n-            current_hidden_states = F.linear(\n-                current_hidden_states,\n-                self.down_proj[expert_idx],\n-                None if self.down_proj_bias is None else self.down_proj_bias[expert_idx],\n-            )\n-            current_hidden_states = current_hidden_states * routing_weights[top_x, idx, None]\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n         return final_hidden_states\n \n \n@@ -164,14 +148,14 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n \n         with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             router_logits = F.linear(hidden_states.float(), self.weight)\n-            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n-            routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n-            routing_weights = routing_weights / torch.clamp(\n-                routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n+            router_logits = F.softmax(router_logits, dim=1, dtype=torch.float)\n+            router_top_value, router_indices = torch.topk(self.moe_statics(router_logits), self.top_k, dim=-1)\n+            router_top_value = router_top_value / torch.clamp(\n+                router_top_value.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n-        routing_weights = routing_weights.to(router_logits.dtype)\n-        return routing_weights, selected_experts\n+            router_scores = router_top_value\n+        router_scores = router_scores.to(hidden_states.dtype)\n+        return router_logits, router_scores, router_indices\n \n \n class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n@@ -194,8 +178,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         if self.shared_experts is not None:\n             shared_output = self.shared_experts(hidden_states)\n \n-        routing_weights, selected_experts = self.gate(hidden_states)\n-        final_hidden_states = self.experts(hidden_states, selected_experts, routing_weights)\n+        _, top_k_weights, top_k_index = self.gate(hidden_states)\n+        final_hidden_states = self.experts(hidden_states, top_k_index, top_k_weights)\n \n         if self.shared_experts is not None:\n             final_hidden_states = final_hidden_states + shared_output\n@@ -231,7 +215,7 @@ class Ernie4_5_MoePreTrainedModel(MixtralPreTrainedModel):\n     # Not supporting multi-token prediction (MTP) atm\n     _keys_to_ignore_on_load_unexpected = [\"mtp\"]\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(Ernie4_5_MoeTopKRouter, layer_name=\"mlp.gate\", index=0),\n+        \"router_logits\": OutputRecorder(Ernie4_5_MoeTopKRouter, index=0),\n         \"hidden_states\": Ernie4_5_MoeDecoderLayer,\n         \"attentions\": Ernie4_5_MoeAttention,\n     }\n@@ -245,9 +229,6 @@ def _init_weights(self, module):\n         elif isinstance(module, Ernie4_5_MoeExperts):\n             init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n             init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n-            if module.gate_up_proj_bias is not None:\n-                init.zeros_(module.gate_up_proj_bias)\n-                init.zeros_(module.down_proj_bias)\n \n \n @auto_docstring"
        }
    ],
    "stats": {
        "total": 156,
        "additions": 62,
        "deletions": 94
    }
}