{
    "author": "Cyrilvallez",
    "message": "Fix DynamicCache and simplify Cache classes a bit (#39590)\n\n* fix\n\n* use kwargs\n\n* simplify\n\n* Update cache_utils.py\n\n* Update cache_utils.py\n\n* Update test_cache_utils.py\n\n* fix\n\n* style",
    "sha": "5dba4bc7b2c1ef517ed44bba76bb70b59001c737",
    "files": [
        {
            "sha": "8fb19adc980737ec85d098a3fd5a03b1748b0f3b",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 136,
            "deletions": 173,
            "changes": 309,
            "blob_url": "https://github.com/huggingface/transformers/blob/5dba4bc7b2c1ef517ed44bba76bb70b59001c737/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5dba4bc7b2c1ef517ed44bba76bb70b59001c737/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=5dba4bc7b2c1ef517ed44bba76bb70b59001c737",
            "patch": "@@ -4,6 +4,7 @@\n import inspect\n import json\n import os\n+from abc import ABC, abstractmethod\n from collections.abc import Iterable\n from dataclasses import dataclass\n from typing import Any, Callable, Optional, Union\n@@ -24,34 +25,30 @@\n logger = logging.get_logger(__name__)\n \n \n-class CacheLayerMixin:\n+class CacheLayerMixin(ABC):\n     \"\"\"Base, abstract class for a single layer's cache.\"\"\"\n \n     is_compileable = False\n \n     def __init__(self):\n         self.keys, self.values = None, None\n \n+    @abstractmethod\n     def update(\n         self,\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n         cache_kwargs: Optional[dict[str, Any]] = None,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\"Updates KV cache, returns updated keys/values of the layer.\"\"\"\n-        raise NotImplementedError(f\"Make sure to implement `update` in {self.__class__.__name__}.\")\n+    ) -> tuple[torch.Tensor, torch.Tensor]: ...\n \n-    def get_seq_length(self, cache_position=None) -> int:\n-        \"\"\"Returns the sequence length of this layer's cache.\"\"\"\n-        raise NotImplementedError(f\"Make sure to implement `get_seq_length` in {self.__class__.__name__}.\")\n+    @abstractmethod\n+    def get_seq_length(self, cache_position=None) -> int: ...\n \n-    def get_max_cache_shape(self) -> int:\n-        \"\"\"Returns the maximum sequence length (i.e. max capacity) of this layer's cache.\"\"\"\n-        raise NotImplementedError(f\"Make sure to implement `get_max_cache_shape` in {self.__class__.__name__}.\")\n+    @abstractmethod\n+    def get_max_cache_shape(self) -> int: ...\n \n-    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n-        \"\"\"Returns mask sizes for the layer.\"\"\"\n-        raise NotImplementedError(f\"Make sure to implement `get_mask_sizes` in {self.__class__.__name__}.\")\n+    @abstractmethod\n+    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...\n \n     def reset(self) -> None:\n         \"\"\"Resets the cache values while preserving the objects\"\"\"\n@@ -76,26 +73,6 @@ class DynamicLayer(CacheLayerMixin):\n     See `CacheLayerMixin` for details on common methods that are implemented by all cache layers.\n     \"\"\"\n \n-    @classmethod\n-    def from_tensors(cls, keys: torch.Tensor, values: torch.Tensor) -> \"DynamicLayer\":\n-        \"\"\"\n-        Build a `DynamicLayer` instance from pre-existing key/value tensors.\n-\n-        Args:\n-            keys (`torch.Tensor`):\n-                Key cache tensor of shape ``[batch_size, num_heads, seq_len, head_dim]``.\n-            values (`torch.Tensor`):\n-                Value cache tensor of shape ``[batch_size, num_heads, seq_len, head_dim]``.\n-\n-        Returns:\n-            `DynamicLayer`: The newly constructed layer whose internal cache directly references\n-            the supplied tensors.\n-        \"\"\"\n-        layer = cls()\n-        layer.keys = keys\n-        layer.values = values\n-        return layer\n-\n     def update(\n         self,\n         key_states: torch.Tensor,\n@@ -175,6 +152,26 @@ def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n         kv_length = query_length + past_seen_tokens\n         return kv_length, kv_offset\n \n+    @classmethod\n+    def from_tensors(cls, keys: torch.Tensor, values: torch.Tensor) -> \"DynamicLayer\":\n+        \"\"\"\n+        Build a `DynamicLayer` instance from pre-existing key/value tensors.\n+\n+        Args:\n+            keys (`torch.Tensor`):\n+                Key cache tensor of shape ``[batch_size, num_heads, seq_len, head_dim]``.\n+            values (`torch.Tensor`):\n+                Value cache tensor of shape ``[batch_size, num_heads, seq_len, head_dim]``.\n+\n+        Returns:\n+            `DynamicLayer`: The newly constructed layer whose internal cache directly references\n+            the supplied tensors.\n+        \"\"\"\n+        layer = cls()\n+        layer.keys = keys\n+        layer.values = values\n+        return layer\n+\n \n class StaticLayer(CacheLayerMixin):\n     \"\"\"\n@@ -558,10 +555,10 @@ def __init__(self, cache: \"Cache\", offload_device: Union[str, torch.device] = \"c\n         self.is_static = any(isinstance(layer, StaticLayer) for layer in cache.layers)\n         if self.is_static:\n             for i, layer in enumerate(cache.layers):\n-                device = cache.layer_init_args[\"device\"] if i == 0 else self.offload_device\n+                device = cache.layer_init_kwargs[\"device\"] if i == 0 else self.offload_device\n                 layer.keys = layer.keys.to(device)\n                 layer.values = layer.values.to(device)\n-                self.original_device.append(cache.layer_init_args[\"device\"])\n+                self.original_device.append(cache.layer_init_kwargs[\"device\"])\n             if len(cache) != cache.num_hidden_layers:\n                 raise ValueError(\"If static layers are used, all cache layers must be initialized\")\n \n@@ -1030,15 +1027,15 @@ class Cache:\n     ```\n \n     Parameters:\n+        layer_classes (`type[CacheLayerMixin]` or `list[type[CacheLayerMixin]]`):\n+            A list of `CacheLayerMixin` classes to instantiate for the cache. If only a `CacheLayerMixin` class is\n+            provided, then it is used for all layers.\n         config (`PretrainedConfig`, *optional*):\n             Model configuration used to infer number of layers, head sizes, default\n             device/dtype, etc.\n         cache_processor (`CacheProcessor` or `str`, *optional*):\n             Cache processor to apply (e.g., \"offloaded\", \"quanto_quantized\", \"hqq_quantized\")\n             or a CacheProcessor class.\n-        layer_classes (`list[type[CacheLayerMixin]]`, *optional*):\n-            List of `CacheLayerMixin` classes to instantiate for the cache. When shorter than the\n-            required number of layers the list is cycled. Default is [DynamicLayer].\n         max_batch_size (`int`, *optional*): Maximum batch size for static caches.\n         max_cache_len (`int`, *optional*): Maximum sequence length. For hybrid caches, SlidingWindowLayers are\n             clamped to `min(sliding_window, max_cache_len)`, StaticLayers use full `max_cache_len`.\n@@ -1053,9 +1050,9 @@ class Cache:\n \n     def __init__(\n         self,\n+        layer_classes: Union[list[type[CacheLayerMixin]], type[CacheLayerMixin]],\n         config: Optional[PretrainedConfig] = None,\n-        cache_processor: Optional[Union[str, type[\"CacheProcessor\"]]] = None,\n-        layer_classes: Optional[list[type[\"CacheLayerMixin\"]]] = None,\n+        cache_processor: Optional[Union[str, type[CacheProcessor]]] = None,\n         max_batch_size: Optional[int] = None,\n         max_cache_len: Optional[int] = None,\n         device: Union[torch.device, str, None] = None,\n@@ -1064,13 +1061,10 @@ def __init__(\n         tp_size: Optional[int] = None,\n         **kwargs,\n     ):\n-        self.layers: list[\"CacheLayerMixin\"] = []\n-        processor_class = PROCESSOR_CLASS_MAP[cache_processor] if isinstance(cache_processor, str) else cache_processor\n-\n-        if layer_classes is None:\n-            layer_classes = [DynamicLayer]\n-\n+        self.layers: list[CacheLayerMixin] = []\n         self.layer_classes = layer_classes\n+\n+        processor_class = PROCESSOR_CLASS_MAP[cache_processor] if isinstance(cache_processor, str) else cache_processor\n         kwargs.update(\n             max_batch_size=max_batch_size,\n             max_cache_len=max_cache_len,\n@@ -1080,7 +1074,8 @@ def __init__(\n             tp_size=tp_size,\n         )\n         processor_kwargs, kwargs = parse_processor_args(processor_class, kwargs)\n-        self.layer_init_args = parse_layer_args_from_model_config(config, **kwargs)\n+\n+        self.layer_init_kwargs = parse_layer_args_from_model_config(config, **kwargs)\n         self.num_hidden_layers = getattr(config, \"num_hidden_layers\", 1)\n \n         self.append_new_layers(self.num_hidden_layers - 1)\n@@ -1138,10 +1133,14 @@ def append_new_layers(self, layer_idx: int) -> None:\n                 The index of the layer to append.\n         \"\"\"\n         while len(self.layers) <= layer_idx:\n-            args = self.layer_init_args.copy()\n-            if self.layer_init_args.get(\"layer_device_map\", None) is not None:\n-                args[\"device\"] = args.pop(\"layer_device_map\")[layer_idx]\n-            new_layer = self.layer_classes[len(self.layers) % len(self.layer_classes)](**args)\n+            kwargs = self.layer_init_kwargs.copy()\n+            if self.layer_init_kwargs.get(\"layer_device_map\", None) is not None:\n+                kwargs[\"device\"] = kwargs.pop(\"layer_device_map\")[layer_idx]\n+\n+            new_layer_class = (\n+                self.layer_classes[len(self.layers)] if isinstance(self.layer_classes, list) else self.layer_classes\n+            )\n+            new_layer = new_layer_class(**kwargs)\n             self.layers.append(new_layer)\n \n     @apply_processors\n@@ -1294,6 +1293,7 @@ class DynamicCache(Cache):\n \n     # Specialized constructor for DDP cache data, needed for BC\n     def __init__(self, ddp_cache_data: Optional[Iterable[tuple[torch.Tensor, torch.Tensor]]] = None, *args, **kwargs):\n+        super().__init__(layer_classes=DynamicLayer, *args, **kwargs)\n         # `ddp_cache_data` was originally added for compatibility with `torch.distributed` (DDP). See #36212\n         # and #36373 for more information. In a nutshell, it is `map(gather_map, zip(*caches))`, i.e. each item in the\n         # iterable contains the key and value states for a layer gathered across replicas by torch.distributed\n@@ -1303,7 +1303,6 @@ def __init__(self, ddp_cache_data: Optional[Iterable[tuple[torch.Tensor, torch.T\n         if ddp_cache_data is not None:\n             for key_states, value_states in ddp_cache_data:\n                 self.layers.append(DynamicLayer.from_tensors(key_states, value_states))\n-        super().__init__(*args, **kwargs)\n \n     def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor], ...]:\n         \"\"\"\n@@ -1390,9 +1389,9 @@ class OffloadedCache(DynamicCache):\n     ensure the eviction is scheduled after all computations on that cache are finished.\n     \"\"\"\n \n-    def __init__(self, config: Optional[PretrainedConfig] = None) -> None:\n+    def __init__(self) -> None:\n         # Create the underlying cache with offload processor\n-        super().__init__(cache_processor=OffloadedCacheProcessor, config=config)\n+        super().__init__(cache_processor=OffloadedCacheProcessor)\n \n \n class StaticCache(Cache):\n@@ -1422,44 +1421,45 @@ class StaticCache(Cache):\n     \"\"\"\n \n     def __init__(self, *args, **kwargs):\n-        super().__init__(layer_classes=[StaticLayer], *args, **kwargs)\n+        super().__init__(layer_classes=StaticLayer, *args, **kwargs)\n \n \n-class HybridCache(Cache):\n+class OffloadedStaticCache(StaticCache):\n     \"\"\"\n-    Hybrid Cache class to be used with `torch.compile` for models that alternate between a local sliding window\n-    attention and global attention in every other layer (originally implemented for Gemma2).\n-    Under the hood, Hybrid Cache leverages [\"SlidingWindowCache\"] for sliding window attention and [\"StaticCache\"]\n-    for global attention. For more information, see the documentation of those layer types.\n+    A drop-in replacement for StaticCache that conserves accelerator memory by offloading\n+    cache tensors to CPU when not actively being used.\n+\n+    This cache maintains the compilation-friendly properties of StaticCache while enabling\n+    much longer sequences by offloading inactive layers to CPU memory.\n \n     See `Cache` for details on common methods that are implemented by all cache classes.\n \n     Example:\n-\n         ```python\n-        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, HybridCache\n+        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, OffloadedStaticCache\n \n-        >>> model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n+        >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n \n-        >>> inputs = tokenizer(text=\"My name is Gemma\", return_tensors=\"pt\")\n+        >>> inputs = tokenizer(text=\"My name is GPT2\", return_tensors=\"pt\")\n \n-        >>> # Prepare a cache class and pass it to model's forward\n-        >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n+        >>> # Prepare a cache class with offloading\n         >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = HybridCache(config=model.config, max_batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n+        >>> past_key_values = OffloadedStaticCache(\n+        ...     config=model.config,\n+        ...     max_batch_size=1,\n+        ...     max_cache_len=max_generated_length,\n+        ...     device=model.device,\n+        ...     dtype=model.dtype\n+        ... )\n         >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values # access cache filled with key/values from generation\n-        HybridCache()\n+        >>> outputs.past_key_values # access cache with offloaded layers\n+        OffloadedStaticCache()\n         ```\n     \"\"\"\n \n-    def __init__(self, config: PretrainedConfig, *args, **kwargs):\n-        if hasattr(config, \"layer_types\") and getattr(config, \"layer_types\", None) is not None:\n-            layer_classes = [LAYER_CLASS_MAP[layer_type] for layer_type in config.layer_types]\n-        else:\n-            layer_classes = [StaticLayer]\n-        super().__init__(config=config, layer_classes=layer_classes, *args, **kwargs)\n+    def __init__(self, *args, **kwargs) -> None:\n+        super().__init__(*args, cache_processor=OffloadedCacheProcessor, **kwargs)\n \n \n class SlidingWindowCache(Cache):\n@@ -1502,7 +1502,64 @@ class SlidingWindowCache(Cache):\n     \"\"\"\n \n     def __init__(self, *args, **kwargs):\n-        super().__init__(layer_classes=[SlidingWindowLayer], *args, **kwargs)\n+        super().__init__(layer_classes=SlidingWindowLayer, *args, **kwargs)\n+\n+\n+class HybridCache(Cache):\n+    \"\"\"\n+    Hybrid Cache class to be used with `torch.compile` for models that alternate between a local sliding window\n+    attention and global attention in every other layer (originally implemented for Gemma2).\n+    Under the hood, Hybrid Cache leverages [\"SlidingWindowCache\"] for sliding window attention and [\"StaticCache\"]\n+    for global attention. For more information, see the documentation of those layer types.\n+\n+    See `Cache` for details on common methods that are implemented by all cache classes.\n+\n+    Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, HybridCache\n+\n+        >>> model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n+\n+        >>> inputs = tokenizer(text=\"My name is Gemma\", return_tensors=\"pt\")\n+\n+        >>> # Prepare a cache class and pass it to model's forward\n+        >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n+        >>> max_generated_length = inputs.input_ids.shape[1] + 10\n+        >>> past_key_values = HybridCache(config=model.config, max_batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n+        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n+        >>> outputs.past_key_values # access cache filled with key/values from generation\n+        HybridCache()\n+        ```\n+    \"\"\"\n+\n+    def __init__(self, config: PretrainedConfig, *args, **kwargs):\n+        if hasattr(config, \"layer_types\"):\n+            layer_classes = [LAYER_CLASS_MAP[layer_type] for layer_type in config.layer_types]\n+        else:\n+            # In this case, fall back to StaticCache\n+            layer_classes = [StaticLayer] * config.num_hidden_layers\n+        super().__init__(config=config, layer_classes=layer_classes, *args, **kwargs)\n+\n+\n+# The mapping already handles dispatching the correct layers in Hybrid, this is only used for BC\n+class HybridChunkedCache(HybridCache): ...\n+\n+\n+class OffloadedHybridCache(HybridChunkedCache):\n+    \"\"\"\n+    A drop-in replacement for HybridChunkedCache that conserves accelerator memory by offloading\n+    cache tensors to CPU when not actively being used.\n+\n+    This cache maintains the compilation-friendly properties of HybridChunkedCache while enabling\n+    much longer sequences by offloading inactive layers to CPU memory.\n+\n+    See `Cache` for details on common methods that are implemented by all cache classes.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs) -> None:\n+        super().__init__(*args, cache_processor=OffloadedCacheProcessor, **kwargs)\n \n \n class QuantizedCache(DynamicCache):\n@@ -1615,100 +1672,6 @@ def __init__(self, backend=\"HQQ\", **kwargs) -> None:\n         Cache.__init__(self, cache_processor=HQQQuantizedCacheProcessor, **kwargs)\n \n \n-class OffloadedStaticCache(StaticCache):\n-    \"\"\"\n-    A drop-in replacement for StaticCache that conserves accelerator memory by offloading\n-    cache tensors to CPU when not actively being used.\n-\n-    This cache maintains the compilation-friendly properties of StaticCache while enabling\n-    much longer sequences by offloading inactive layers to CPU memory.\n-\n-    See `Cache` for details on common methods that are implemented by all cache classes.\n-\n-    Example:\n-        ```python\n-        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, OffloadedStaticCache\n-\n-        >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n-\n-        >>> inputs = tokenizer(text=\"My name is GPT2\", return_tensors=\"pt\")\n-\n-        >>> # Prepare a cache class with offloading\n-        >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = OffloadedStaticCache(\n-        ...     config=model.config,\n-        ...     max_batch_size=1,\n-        ...     max_cache_len=max_generated_length,\n-        ...     device=model.device,\n-        ...     dtype=model.dtype\n-        ... )\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values # access cache with offloaded layers\n-        OffloadedStaticCache()\n-        ```\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs) -> None:\n-        super().__init__(*args, cache_processor=OffloadedCacheProcessor, **kwargs)\n-\n-\n-class HybridChunkedCache(Cache):\n-    \"\"\"\n-    Hybrid Cache class to be used with `torch.compile` for models that alternate between a local sliding window\n-    attention and global attention in every other layer, with support for prefill chunking (originally implemented\n-    for Llama4).\n-    Under the hood, Hybrid Cache leverages [\"SlidingWindowCache\"] for sliding window attention and [\"StaticCache\"]\n-    for global attention. For more information, see the documentation of each subcomponent cache class.\n-\n-    See `Cache` for details on common methods that are implemented by all cache classes.\n-\n-    Example:\n-\n-        ```python\n-        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, HybridCache\n-\n-        >>> model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n-\n-        >>> inputs = tokenizer(text=\"My name is Gemma\", return_tensors=\"pt\")\n-\n-        >>> # Prepare a cache class and pass it to model's forward\n-        >>> # Leave empty space for 10 new tokens, which can be used when calling forward iteratively 10 times to generate\n-        >>> max_generated_length = inputs.input_ids.shape[1] + 10\n-        >>> past_key_values = HybridCache(config=model.config, max_batch_size=1, max_cache_len=max_generated_length, device=model.device, dtype=model.dtype)\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values # access cache filled with key/values from generation\n-        HybridCache()\n-        ```\n-    \"\"\"\n-\n-    def __init__(self, config: PretrainedConfig, *args, **kwargs):\n-        hybrid_map = LAYER_CLASS_MAP.copy()\n-        hybrid_map[\"sliding_attention\"] = ChunkedSlidingLayer\n-        hybrid_map[\"chunked_attention\"] = ChunkedSlidingLayer\n-        if hasattr(config, \"layer_types\") and getattr(config, \"layer_types\", None) is not None:\n-            layer_classes = [hybrid_map[layer_type] for layer_type in config.layer_types]\n-        else:\n-            layer_classes = [StaticLayer]\n-        super().__init__(config=config, layer_classes=layer_classes, *args, **kwargs)\n-\n-\n-class OffloadedHybridCache(HybridChunkedCache):\n-    \"\"\"\n-    A drop-in replacement for HybridChunkedCache that conserves accelerator memory by offloading\n-    cache tensors to CPU when not actively being used.\n-\n-    This cache maintains the compilation-friendly properties of HybridChunkedCache while enabling\n-    much longer sequences by offloading inactive layers to CPU memory.\n-\n-    See `Cache` for details on common methods that are implemented by all cache classes.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs) -> None:\n-        super().__init__(*args, cache_processor=OffloadedCacheProcessor, **kwargs)\n-\n-\n class EncoderDecoderCache(Cache):\n     \"\"\"\n     Base, abstract class for all encoder-decoder caches. Can be used to hold combinations of self-attention and\n@@ -1741,7 +1704,7 @@ class EncoderDecoderCache(Cache):\n     is_compileable = None\n \n     def __init__(self, self_attention_cache: Cache, cross_attention_cache: Cache):\n-        super().__init__()\n+        super().__init__(layer_classes=DynamicLayer)\n         self.self_attention_cache = self_attention_cache\n         self.cross_attention_cache = cross_attention_cache\n         self.is_compileable = getattr(self.self_attention_cache, \"is_compileable\", False)\n@@ -1998,7 +1961,7 @@ def parse_layer_args_from_model_config(\n LAYER_CLASS_MAP: dict[str, type[\"CacheLayerMixin\"]] = {\n     \"full_attention\": StaticLayer,\n     \"sliding_attention\": SlidingWindowLayer,\n-    \"chunked_attention\": SlidingWindowLayer,\n+    \"chunked_attention\": ChunkedSlidingLayer,\n }\n PROCESSOR_CLASS_MAP: dict[str, type[\"CacheProcessor\"]] = {\n     \"offloaded\": OffloadedCacheProcessor,"
        },
        {
            "sha": "ef75f254cc20db192cad0f9c233c9639cad7c7a1",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5dba4bc7b2c1ef517ed44bba76bb70b59001c737/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5dba4bc7b2c1ef517ed44bba76bb70b59001c737/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=5dba4bc7b2c1ef517ed44bba76bb70b59001c737",
            "patch": "@@ -31,7 +31,7 @@\n \n from transformers.activations import ACT2FN\n \n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import Cache, DynamicCache, DynamicLayer\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -104,7 +104,7 @@ class HybridMambaAttentionDynamicCache(Cache):\n     is_compileable = False\n \n     def __init__(self, config: BambaConfig, batch_size, dtype=torch.float16, device=None):\n-        super().__init__()\n+        super().__init__(layer_classes=DynamicLayer)\n         self.layers_block_type = config.layers_block_type\n         self.has_previous_state = False  # only used by mamba\n         conv_kernel_size = config.mamba_d_conv"
        },
        {
            "sha": "be58fd3abd42936d282b65c0b5ca61a5f303232f",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5dba4bc7b2c1ef517ed44bba76bb70b59001c737/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5dba4bc7b2c1ef517ed44bba76bb70b59001c737/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=5dba4bc7b2c1ef517ed44bba76bb70b59001c737",
            "patch": "@@ -42,7 +42,7 @@\n     segment_sum,\n )\n \n-from ...cache_utils import Cache\n+from ...cache_utils import DynamicLayer\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n@@ -99,7 +99,7 @@ class BambaFlashAttentionKwargs(TypedDict, total=False):\n \n \n # Adapted from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache for the v2 mixer\n-class HybridMambaAttentionDynamicCache(HybridMambaAttentionDynamicCache, Cache):\n+class HybridMambaAttentionDynamicCache(HybridMambaAttentionDynamicCache):\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -114,7 +114,7 @@ class HybridMambaAttentionDynamicCache(HybridMambaAttentionDynamicCache, Cache):\n     \"\"\"\n \n     def __init__(self, config: BambaConfig, batch_size, dtype=torch.float16, device=None):\n-        Cache.__init__()\n+        HybridMambaAttentionDynamicCache.__init__(layer_classes=DynamicLayer)\n         self.layers_block_type = config.layers_block_type\n         self.has_previous_state = False  # only used by mamba\n         conv_kernel_size = config.mamba_d_conv"
        },
        {
            "sha": "598673586cb4266cef3467e040715b3f6570f265",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5dba4bc7b2c1ef517ed44bba76bb70b59001c737/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5dba4bc7b2c1ef517ed44bba76bb70b59001c737/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=5dba4bc7b2c1ef517ed44bba76bb70b59001c737",
            "patch": "@@ -27,7 +27,7 @@\n \n from transformers.activations import ACT2FN\n \n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import Cache, DynamicCache, DynamicLayer\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -240,7 +240,7 @@ class HybridMambaAttentionDynamicCache(Cache):\n     is_compileable = False\n \n     def __init__(self, config: GraniteMoeHybridConfig, batch_size, dtype=torch.float16, device=None):\n-        super().__init__()\n+        super().__init__(layer_classes=DynamicLayer)\n         self.layers_block_type = config.layers_block_type\n         self.has_previous_state = False  # only used by mamba\n         conv_kernel_size = config.mamba_d_conv"
        },
        {
            "sha": "817e181ec25e63c83b5b3888971310bf92aa0aa7",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5dba4bc7b2c1ef517ed44bba76bb70b59001c737/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5dba4bc7b2c1ef517ed44bba76bb70b59001c737/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=5dba4bc7b2c1ef517ed44bba76bb70b59001c737",
            "patch": "@@ -28,7 +28,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import Cache, DynamicCache, DynamicLayer\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n@@ -202,7 +202,7 @@ class HybridMambaAttentionDynamicCache(Cache):\n     is_compileable = False\n \n     def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n-        super().__init__()\n+        super().__init__(layer_classes=DynamicLayer)\n         self.dtype = dtype\n         self.layers_block_type = config.layers_block_type\n         self.has_previous_state = False  # only used by mamba"
        },
        {
            "sha": "14b29344f1903172f26d5f6b38d0cdc1c48e8564",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5dba4bc7b2c1ef517ed44bba76bb70b59001c737/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5dba4bc7b2c1ef517ed44bba76bb70b59001c737/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=5dba4bc7b2c1ef517ed44bba76bb70b59001c737",
            "patch": "@@ -1307,7 +1307,7 @@ def test_hybrid_chunked_cache(self):\n \n         config = copy.deepcopy(self.config)\n         config.num_hidden_layers = 2\n-        config.layer_types = [\"full_attention\", \"sliding_attention\"]\n+        config.layer_types = [\"full_attention\", \"chunked_attention\"]\n         config.sliding_window = 2\n         max_cache_len = 4\n         chunked_cache = HybridChunkedCache(config=config, max_batch_size=1, max_cache_len=max_cache_len)\n@@ -1387,7 +1387,7 @@ def test_hybrid_chunked_cache_extra_cases(self):\n \n         config = copy.deepcopy(self.config)\n         config.num_hidden_layers = 1\n-        config.layer_types = [\"sliding_attention\"]\n+        config.layer_types = [\"chunked_attention\"]\n         config.sliding_window = 3\n         cache = HybridChunkedCache(config, max_batch_size=1, max_cache_len=3)\n "
        }
    ],
    "stats": {
        "total": 331,
        "additions": 147,
        "deletions": 184
    }
}