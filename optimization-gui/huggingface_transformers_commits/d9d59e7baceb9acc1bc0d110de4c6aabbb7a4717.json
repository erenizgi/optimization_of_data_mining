{
    "author": "gante",
    "message": "Generate: check that `attention_mask` is 2D (#33575)\n\ncheck attention mask in generate",
    "sha": "d9d59e7baceb9acc1bc0d110de4c6aabbb7a4717",
    "files": [
        {
            "sha": "2ba330101e0cbbb6d00040654095e5ebb3569d4c",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d9d59e7baceb9acc1bc0d110de4c6aabbb7a4717/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d9d59e7baceb9acc1bc0d110de4c6aabbb7a4717/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=d9d59e7baceb9acc1bc0d110de4c6aabbb7a4717",
            "patch": "@@ -1864,6 +1864,10 @@ def generate(\n             model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n                 inputs_tensor, generation_config._pad_token_tensor, generation_config._eos_token_tensor\n             )\n+        elif kwargs_has_attention_mask:\n+            # TODO (joao): generalize this check with other types of inputs\n+            if model_input_name == \"input_ids\" and len(model_kwargs[\"attention_mask\"].shape) > 2:\n+                raise ValueError(\"`attention_mask` passed to `generate` must be 2D.\")\n \n         if self.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n             # if model is encoder decoder encoder_outputs are created and added to `model_kwargs`"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 4,
        "deletions": 0
    }
}