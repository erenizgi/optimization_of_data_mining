{
    "author": "bzantium",
    "message": "[DeepSeek-V3] implement when q_lora_rank is None (#38743)\n\n* implement when q_lora_rank is None\n\n* make style and quality",
    "sha": "0bf53e69e2b20a31f06161e58c516a4f88b8272c",
    "files": [
        {
            "sha": "99d39be02a0afe0d20f5e43db4a10ac65a5c8303",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/0bf53e69e2b20a31f06161e58c516a4f88b8272c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0bf53e69e2b20a31f06161e58c516a4f88b8272c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=0bf53e69e2b20a31f06161e58c516a4f88b8272c",
            "patch": "@@ -338,9 +338,12 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n         self.qk_head_dim = config.qk_head_dim\n \n         self.is_causal = True\n-        self.q_a_proj = nn.Linear(config.hidden_size, config.q_lora_rank, bias=config.attention_bias)\n-        self.q_a_layernorm = DeepseekV3RMSNorm(config.q_lora_rank)\n-        self.q_b_proj = nn.Linear(config.q_lora_rank, self.num_heads * self.qk_head_dim, bias=False)\n+        if self.q_lora_rank is None:\n+            self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.qk_head_dim, bias=False)\n+        else:\n+            self.q_a_proj = nn.Linear(config.hidden_size, config.q_lora_rank, bias=config.attention_bias)\n+            self.q_a_layernorm = DeepseekV3RMSNorm(config.q_lora_rank)\n+            self.q_b_proj = nn.Linear(config.q_lora_rank, self.num_heads * self.qk_head_dim, bias=False)\n \n         self.kv_a_proj_with_mqa = nn.Linear(\n             config.hidden_size,\n@@ -381,7 +384,11 @@ def forward(\n         query_shape = (batch_size, seq_length, -1, self.qk_head_dim)\n         key_shape = (batch_size, seq_length, -1, self.qk_nope_head_dim + self.v_head_dim)\n \n-        q_states = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states))).view(query_shape).transpose(1, 2)\n+        if self.q_lora_rank is None:\n+            q_states = self.q_proj(hidden_states)\n+        else:\n+            q_states = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states)))\n+        q_states = q_states.view(query_shape).transpose(1, 2)\n         q_pass, q_rot = torch.split(q_states, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n \n         compressed_kv = self.kv_a_proj_with_mqa(hidden_states)"
        },
        {
            "sha": "463a2eaed55eebd6d3dc39fe74cde69b0f793ab6",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/0bf53e69e2b20a31f06161e58c516a4f88b8272c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0bf53e69e2b20a31f06161e58c516a4f88b8272c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=0bf53e69e2b20a31f06161e58c516a4f88b8272c",
            "patch": "@@ -219,9 +219,12 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n         self.qk_head_dim = config.qk_head_dim\n \n         self.is_causal = True\n-        self.q_a_proj = nn.Linear(config.hidden_size, config.q_lora_rank, bias=config.attention_bias)\n-        self.q_a_layernorm = DeepseekV3RMSNorm(config.q_lora_rank)\n-        self.q_b_proj = nn.Linear(config.q_lora_rank, self.num_heads * self.qk_head_dim, bias=False)\n+        if self.q_lora_rank is None:\n+            self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.qk_head_dim, bias=False)\n+        else:\n+            self.q_a_proj = nn.Linear(config.hidden_size, config.q_lora_rank, bias=config.attention_bias)\n+            self.q_a_layernorm = DeepseekV3RMSNorm(config.q_lora_rank)\n+            self.q_b_proj = nn.Linear(config.q_lora_rank, self.num_heads * self.qk_head_dim, bias=False)\n \n         self.kv_a_proj_with_mqa = nn.Linear(\n             config.hidden_size,\n@@ -262,7 +265,11 @@ def forward(\n         query_shape = (batch_size, seq_length, -1, self.qk_head_dim)\n         key_shape = (batch_size, seq_length, -1, self.qk_nope_head_dim + self.v_head_dim)\n \n-        q_states = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states))).view(query_shape).transpose(1, 2)\n+        if self.q_lora_rank is None:\n+            q_states = self.q_proj(hidden_states)\n+        else:\n+            q_states = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states)))\n+        q_states = q_states.view(query_shape).transpose(1, 2)\n         q_pass, q_rot = torch.split(q_states, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n \n         compressed_kv = self.kv_a_proj_with_mqa(hidden_states)"
        }
    ],
    "stats": {
        "total": 30,
        "additions": 22,
        "deletions": 8
    }
}