{
    "author": "gante",
    "message": "SynthID: better example (#34372)\n\n* better example\r\n\r\n* Update src/transformers/generation/configuration_utils.py\r\n\r\n* Update src/transformers/generation/logits_process.py\r\n\r\n* nits",
    "sha": "8814043c8c62034277b04e73a44e25231ab020ad",
    "files": [
        {
            "sha": "eb25ddb6329755bee1332364bbca31fda79306bc",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8814043c8c62034277b04e73a44e25231ab020ad/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8814043c8c62034277b04e73a44e25231ab020ad/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=8814043c8c62034277b04e73a44e25231ab020ad",
            "patch": "@@ -428,13 +428,11 @@ A [`Constraint`] can be used to force the generation to include specific tokens\n     - __call__\n \n [[autodoc]] BayesianDetectorConfig\n-    - __call__\n \n [[autodoc]] BayesianDetectorModel\n-    - __call__\n+    - forward\n \n [[autodoc]] SynthIDTextWatermarkingConfig\n-    - __call__\n \n [[autodoc]] SynthIDTextWatermarkDetector\n     - __call__"
        },
        {
            "sha": "3c204481b04296785a999d4e29cab4c1624dd81d",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8814043c8c62034277b04e73a44e25231ab020ad/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8814043c8c62034277b04e73a44e25231ab020ad/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=8814043c8c62034277b04e73a44e25231ab020ad",
            "patch": "@@ -1471,8 +1471,8 @@ class SynthIDTextWatermarkingConfig(BaseWatermarkingConfig):\n     ```python\n     >>> from transformers import AutoModelForCausalLM, AutoTokenizer, SynthIDTextWatermarkingConfig\n \n-    >>> tokenizer = AutoTokenizer.from_pretrained('google/gemma-2-2b-it')\n-    >>> model = AutoModelForCausalLM.from_pretrained('google/gemma-2-2b-it')\n+    >>> tokenizer = AutoTokenizer.from_pretrained('google/gemma-2-2b', padding_side=\"left\")\n+    >>> model = AutoModelForCausalLM.from_pretrained('google/gemma-2-2b')\n \n     >>> # SynthID Text configuration\n     >>> watermarking_config = SynthIDTextWatermarkingConfig(\n@@ -1481,11 +1481,11 @@ class SynthIDTextWatermarkingConfig(BaseWatermarkingConfig):\n     ... )\n \n     >>> # Generation with watermarking\n-    >>> tokenized_prompts = tokenizer([\"your prompts here\"])\n+    >>> tokenized_prompts = tokenizer([\"Once upon a time, \"], return_tensors=\"pt\", padding=True)\n     >>> output_sequences = model.generate(\n-    ...     **tokenized_prompts, watermarking_config=watermarking_config, do_sample=True,\n+    ...     **tokenized_prompts, watermarking_config=watermarking_config, do_sample=True, max_new_tokens=10\n     ... )\n-    >>> watermarked_text = tokenizer.batch_decode(output_sequences)\n+    >>> watermarked_text = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n     ```\n     \"\"\"\n "
        },
        {
            "sha": "9d244191da811c8de45a14701b309c0b04d54d9a",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8814043c8c62034277b04e73a44e25231ab020ad/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8814043c8c62034277b04e73a44e25231ab020ad/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=8814043c8c62034277b04e73a44e25231ab020ad",
            "patch": "@@ -2565,8 +2565,8 @@ class SynthIDTextWatermarkLogitsProcessor(LogitsProcessor):\n     ```python\n     >>> from transformers import AutoModelForCausalLM, AutoTokenizer, SynthIDTextWatermarkingConfig\n \n-    >>> tokenizer = AutoTokenizer.from_pretrained('google/gemma-2-2b-it')\n-    >>> model = AutoModelForCausalLM.from_pretrained('google/gemma-2-2b-it')\n+    >>> tokenizer = AutoTokenizer.from_pretrained('google/gemma-2-2b', padding_side=\"left\")\n+    >>> model = AutoModelForCausalLM.from_pretrained('google/gemma-2-2b')\n \n     >>> # SynthID Text configuration\n     >>> watermarking_config = SynthIDTextWatermarkingConfig(\n@@ -2575,11 +2575,11 @@ class SynthIDTextWatermarkLogitsProcessor(LogitsProcessor):\n     ... )\n \n     >>> # Generation with watermarking\n-    >>> tokenized_prompts = tokenizer([\"your prompts here\"])\n+    >>> tokenized_prompts = tokenizer([\"Once upon a time, \"], return_tensors=\"pt\", padding=True)\n     >>> output_sequences = model.generate(\n-    ...     **tokenized_prompts, watermarking_config=watermarking_config, do_sample=True,\n+    ...     **tokenized_prompts, watermarking_config=watermarking_config, do_sample=True, max_new_tokens=10\n     ... )\n-    >>> watermarked_text = tokenizer.batch_decode(output_sequences)\n+    >>> watermarked_text = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n     ```\n     \"\"\"\n "
        }
    ],
    "stats": {
        "total": 24,
        "additions": 11,
        "deletions": 13
    }
}