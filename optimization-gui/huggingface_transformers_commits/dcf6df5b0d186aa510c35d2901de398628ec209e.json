{
    "author": "zucchini-nlp",
    "message": "[qwen-omni] fix training (#37517)\n\n* fix\n\n* add text config\n\n* fixup\n\n* fix docs",
    "sha": "dcf6df5b0d186aa510c35d2901de398628ec209e",
    "files": [
        {
            "sha": "f3153d9f2260d1416853a934573b50b7bd4fb503",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/dcf6df5b0d186aa510c35d2901de398628ec209e/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dcf6df5b0d186aa510c35d2901de398628ec209e/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=dcf6df5b0d186aa510c35d2901de398628ec209e",
            "patch": "@@ -112,8 +112,6 @@ input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n-</hfoption>\n-\n </hfoption>\n <hfoption id=\"int4-weight-only\">\n \n@@ -332,6 +330,7 @@ quantized_model.push_to_hub(f\"{USER_ID}/llama3-8b-int4wo-128\", safe_serializatio\n tokenizer.push_to_hub(f\"{USER_ID}/llama3-8b-int4wo-128\")\n ```\n </hfoption>\n+</hfoptions>\n \n \n ## Loading quantized models"
        },
        {
            "sha": "f2e2333f67564f3f4200a58dff0021484eec95d7",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/dcf6df5b0d186aa510c35d2901de398628ec209e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dcf6df5b0d186aa510c35d2901de398628ec209e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=dcf6df5b0d186aa510c35d2901de398628ec209e",
            "patch": "@@ -1045,5 +1045,20 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n+    @classmethod\n+    def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n+        \"\"\"\n+        Returns the config that is meant to be used with text IO. On most models, it is the original config instance\n+        itself. On specific composite models, it is under a set of valid names.\n+\n+        Args:\n+            decoder (`Optional[bool]`, *optional*, defaults to `False`):\n+                If set to `True`, then only search for decoder config names.\n+        \"\"\"\n+        # Overriden for deeply nested config like Qwen2-Omni. We don't have any omni model\n+        # except for Qwen yet. This has to be generalized if more deeply nested configs are\n+        # added. NOTE: currently method used only by vLLM\n+        return self.thinker_config.get_text_config()\n+\n \n __all__ = [\"Qwen2_5OmniConfig\", \"Qwen2_5OmniThinkerConfig\", \"Qwen2_5OmniTalkerConfig\", \"Qwen2_5OmniToken2WavConfig\"]"
        },
        {
            "sha": "3a35b2749046d1c46e0fc5e3fc6f481d2125bdbf",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/dcf6df5b0d186aa510c35d2901de398628ec209e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dcf6df5b0d186aa510c35d2901de398628ec209e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=dcf6df5b0d186aa510c35d2901de398628ec209e",
            "patch": "@@ -2503,7 +2503,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.get_text_config().vocab_size\n+            )\n \n         if not return_dict:\n             output = (logits,) + outputs\n@@ -4384,6 +4386,7 @@ def __init__(self, config):\n         self.speaker_map = {}\n         if config.enable_audio_output:\n             self.enable_talker()\n+        self.post_init()\n \n     def enable_talker(self):\n         self.talker = Qwen2_5OmniTalkerForConditionalGeneration(self.config.talker_config)"
        },
        {
            "sha": "b6494f7aa1bfcdabe0f49ad434b0872dac829559",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 19,
            "deletions": 1,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/dcf6df5b0d186aa510c35d2901de398628ec209e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dcf6df5b0d186aa510c35d2901de398628ec209e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=dcf6df5b0d186aa510c35d2901de398628ec209e",
            "patch": "@@ -1030,6 +1030,21 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n+    @classmethod\n+    def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n+        \"\"\"\n+        Returns the config that is meant to be used with text IO. On most models, it is the original config instance\n+        itself. On specific composite models, it is under a set of valid names.\n+\n+        Args:\n+            decoder (`Optional[bool]`, *optional*, defaults to `False`):\n+                If set to `True`, then only search for decoder config names.\n+        \"\"\"\n+        # Overriden for deeply nested config like Qwen2-Omni. We don't have any omni model\n+        # except for Qwen yet. This has to be generalized if more deeply nested configs are\n+        # added. NOTE: currently method used only by vLLM\n+        return self.thinker_config.get_text_config()\n+\n \n class Qwen2_5OmniPreTrainedModel(Qwen2_5_VLPreTrainedModel):\n     config_class = Qwen2_5OmniConfig\n@@ -2463,7 +2478,9 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.get_text_config().vocab_size\n+            )\n \n         if not return_dict:\n             output = (logits,) + outputs\n@@ -4053,6 +4070,7 @@ def __init__(self, config):\n         self.speaker_map = {}\n         if config.enable_audio_output:\n             self.enable_talker()\n+        self.post_init()\n \n     def enable_talker(self):\n         self.talker = Qwen2_5OmniTalkerForConditionalGeneration(self.config.talker_config)"
        }
    ],
    "stats": {
        "total": 43,
        "additions": 39,
        "deletions": 4
    }
}