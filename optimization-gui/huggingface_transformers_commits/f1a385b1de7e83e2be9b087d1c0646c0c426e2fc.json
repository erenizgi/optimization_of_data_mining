{
    "author": "hackyon",
    "message": "[RoBERTa-based] Add support for sdpa (#30510)\n\n* Adding SDPA support for RoBERTa-based models\r\n\r\n* add not is_cross_attention\r\n\r\n* fix copies\r\n\r\n* fix test\r\n\r\n* add minimal test for camembert and xlm_roberta as their test class does not inherit from ModelTesterMixin\r\n\r\n* address some review comments\r\n\r\n* use copied from\r\n\r\n* style\r\n\r\n* consistency\r\n\r\n* fix lists\r\n\r\n---------\r\n\r\nCo-authored-by: fxmarty <9808326+fxmarty@users.noreply.github.com>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "f1a385b1de7e83e2be9b087d1c0646c0c426e2fc",
    "files": [
        {
            "sha": "3517d93bfc8a296f1ae9516e1eff1d38248d7340",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 20,
            "deletions": 7,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=f1a385b1de7e83e2be9b087d1c0646c0c426e2fc",
            "patch": "@@ -74,7 +74,6 @@ FlashAttention-2 is currently supported for the following architectures:\n * [OPT](https://huggingface.co/docs/transformers/model_doc/opt#transformers.OPTModel)\n * [Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)\n * [Phi3](https://huggingface.co/docs/transformers/model_doc/phi3#transformers.Phi3Model)\n-* [SigLIP](https://huggingface.co/docs/transformers/model_doc/siglip)\n * [StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm#transformers.StableLmModel)\n * [Starcoder2](https://huggingface.co/docs/transformers/model_doc/starcoder2#transformers.Starcoder2Model)\n * [Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)\n@@ -86,6 +85,7 @@ FlashAttention-2 is currently supported for the following architectures:\n * [Hubert](https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertModel)\n * [data2vec_audio](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel)\n * [Sew](https://huggingface.co/docs/transformers/main/en/model_doc/sew#transformers.SEWModel)\n+* [SigLIP](https://huggingface.co/docs/transformers/model_doc/siglip)\n * [UniSpeech](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech#transformers.UniSpeechModel)\n * [unispeech_sat](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)\n \n@@ -204,9 +204,11 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer#transformers.ASTModel)\n * [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)\n * [Bert](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)\n+* [CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert#transformers.CamembertModel)\n * [Chameleon](https://huggingface.co/docs/transformers/model_doc/chameleon#transformers.Chameleon)\n * [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPModel)\n * [Cohere](https://huggingface.co/docs/transformers/model_doc/cohere#transformers.CohereModel)\n+* [data2vec_audio](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel)\n * [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)\n * [DeiT](https://huggingface.co/docs/transformers/model_doc/deit#transformers.DeiTModel)\n * [Dpr](https://huggingface.co/docs/transformers/model_doc/dpr#transformers.DprReader)\n@@ -216,10 +218,16 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)\n * [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)\n * [GPTNeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXModel)\n+* [Hubert](https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertModel)\n+* [Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)\n * [Granite](https://huggingface.co/docs/transformers/model_doc/granite#transformers.GraniteModel)\n * [JetMoe](https://huggingface.co/docs/transformers/model_doc/jetmoe#transformers.JetMoeModel)\n * [Jamba](https://huggingface.co/docs/transformers/model_doc/jamba#transformers.JambaModel)\n * [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)\n+* [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)\n+* [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)\n+* [Musicgen](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenModel)\n+* [MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody#transformers.MusicgenMelodyModel)\n * [OLMo](https://huggingface.co/docs/transformers/model_doc/olmo#transformers.OlmoModel)\n * [PaliGemma](https://huggingface.co/docs/transformers/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration)\n * [Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)\n@@ -233,6 +241,14 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)\n * [Qwen2Audio](https://huggingface.co/docs/transformers/model_doc/qwen2_audio#transformers.Qwen2AudioEncoder)\n * [Qwen2MoE](https://huggingface.co/docs/transformers/model_doc/qwen2_moe#transformers.Qwen2MoeModel)\n+* [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel)\n+* [Sew](https://huggingface.co/docs/transformers/main/en/model_doc/sew#transformers.SEWModel)\n+* [SigLIP](https://huggingface.co/docs/transformers/model_doc/siglip)\n+* [StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm#transformers.StableLmModel)\n+* [Starcoder2](https://huggingface.co/docs/transformers/model_doc/starcoder2#transformers.Starcoder2Model)\n+* [UniSpeech](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech#transformers.UniSpeechModel)\n+* [unispeech_sat](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)\n+* [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel)\n * [Qwen2VL](https://huggingface.co/docs/transformers/model_doc/qwen2_vl#transformers.Qwen2VLModel)\n * [Musicgen](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenModel)\n * [MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody#transformers.MusicgenMelodyModel)\n@@ -243,12 +259,9 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn#transformers.ViTMSNModel)\n * [VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae#transformers.VideoMAEModell)\n * [wav2vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2Model)\n-* [Hubert](https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertModel)\n-* [data2vec_audio](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel)\n-* [SigLIP](https://huggingface.co/docs/transformers/model_doc/siglip)\n-* [Sew](https://huggingface.co/docs/transformers/main/en/model_doc/sew#transformers.SEWModel)\n-* [UniSpeech](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech#transformers.UniSpeechModel)\n-* [unispeech_sat](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)\n+* [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)\n+* [XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta#transformers.XLMRobertaModel)\n+* [XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel)\n * [YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos#transformers.YolosModel)\n \n "
        },
        {
            "sha": "81785e147db9568158c99923da8296d14f0d5a2f",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=f1a385b1de7e83e2be9b087d1c0646c0c426e2fc",
            "patch": "@@ -1063,7 +1063,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    # Copied from transformers.models.roberta.modeling_roberta.RobertaModel.forward\n+    # Copied from transformers.models.clap.modeling_clap.ClapTextModel.forward\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "03a60a2856be2ea8a5de1e3fe91657640795e19d",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 161,
            "deletions": 16,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=f1a385b1de7e83e2be9b087d1c0646c0c426e2fc",
            "patch": "@@ -20,10 +20,15 @@\n \n import torch\n import torch.utils.checkpoint\n+from packaging import version\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask_for_sdpa,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n+)\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -40,6 +45,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    get_torch_version,\n     logging,\n     replace_return_docstrings,\n )\n@@ -294,6 +300,108 @@ def forward(\n         return outputs\n \n \n+# Copied from transformers.models.roberta.modeling_roberta.RobertaSdpaSelfAttention with Roberta->Camembert\n+class CamembertSdpaSelfAttention(CamembertSelfAttention):\n+    def __init__(self, config, position_embedding_type=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type)\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n+\n+    # Adapted from CamembertSelfAttention\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n+            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n+            logger.warning_once(\n+                \"CamembertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n+                \"the manual attention implementation, but specifying the manual implementation will be required from \"\n+                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n+                '`attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states,\n+                attention_mask,\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n+\n+        bsz, tgt_len, _ = hidden_states.size()\n+\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n+        # mask needs to be such that the encoder's padding tokens are not attended to.\n+        is_cross_attention = encoder_hidden_states is not None\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+\n+        # Check `seq_length` of `past_key_value` == `len(current_states)` to support prefix tuning\n+        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n+            key_layer, value_layer = past_key_value\n+        else:\n+            key_layer = self.transpose_for_scores(self.key(current_states))\n+            value_layer = self.transpose_for_scores(self.value(current_states))\n+            if past_key_value is not None and not is_cross_attention:\n+                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n+                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_layer, value_layer)\n+\n+        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n+        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577\n+        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n+            query_layer = query_layer.contiguous()\n+            key_layer = key_layer.contiguous()\n+            value_layer = value_layer.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n+        # a causal mask in case tgt_len == 1.\n+        is_causal = (\n+            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n+        )\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attn_mask=attention_mask,\n+            dropout_p=self.dropout_prob if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n+\n+        outputs = (attn_output,)\n+        if self.is_decoder:\n+            outputs = outputs + (past_key_value,)\n+        return outputs\n+\n+\n # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfOutput with Roberta->Camembert\n class CamembertSelfOutput(nn.Module):\n     def __init__(self, config):\n@@ -311,6 +419,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n CAMEMBERT_SELF_ATTENTION_CLASSES = {\n     \"eager\": CamembertSelfAttention,\n+    \"sdpa\": CamembertSdpaSelfAttention,\n }\n \n \n@@ -603,6 +712,7 @@ class CamembertPreTrainedModel(PreTrainedModel):\n     config_class = CamembertConfig\n     base_model_prefix = \"roberta\"\n     supports_gradient_checkpointing = True\n+    _supports_sdpa = True\n \n     # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n     def _init_weights(self, module):\n@@ -749,7 +859,7 @@ class CamembertModel(CamembertPreTrainedModel):\n \n     _no_split_modules = []\n \n-    # Copied from transformers.models.clap.modeling_clap.ClapTextModel.__init__ with ClapText->Camembert\n+    # Copied from transformers.models.roberta.modeling_roberta.RobertaModel.__init__ with Roberta->Camembert\n     def __init__(self, config, add_pooling_layer=True):\n         super().__init__(config)\n         self.config = config\n@@ -759,6 +869,9 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = CamembertPooler(config) if add_pooling_layer else None\n \n+        self.attn_implementation = config._attn_implementation\n+        self.position_embedding_type = config.position_embedding_type\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -782,7 +895,7 @@ class PreTrainedModel\n         output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n         config_class=_CONFIG_FOR_DOC,\n     )\n-    # Copied from transformers.models.clap.modeling_clap.ClapTextModel.forward\n+    # Copied from transformers.models.roberta.modeling_roberta.RobertaModel.forward\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -846,9 +959,6 @@ def forward(\n         # past_key_values_length\n         past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n-\n         if token_type_ids is None:\n             if hasattr(self.embeddings, \"token_type_ids\"):\n                 buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n@@ -857,9 +967,43 @@ def forward(\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n \n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n+        embedding_output = self.embeddings(\n+            input_ids=input_ids,\n+            position_ids=position_ids,\n+            token_type_ids=token_type_ids,\n+            inputs_embeds=inputs_embeds,\n+            past_key_values_length=past_key_values_length,\n+        )\n+\n+        if attention_mask is None:\n+            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n+\n+        use_sdpa_attention_masks = (\n+            self.attn_implementation == \"sdpa\"\n+            and self.position_embedding_type == \"absolute\"\n+            and head_mask is None\n+            and not output_attentions\n+        )\n+\n+        # Expand the attention mask\n+        if use_sdpa_attention_masks:\n+            # Expand the attention mask for SDPA.\n+            # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n+            if self.config.is_decoder:\n+                extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                    attention_mask,\n+                    input_shape,\n+                    embedding_output,\n+                    past_key_values_length,\n+                )\n+            else:\n+                extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    attention_mask, embedding_output.dtype, tgt_len=seq_length\n+                )\n+        else:\n+            # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n+            # ourselves in which case we just need to make it broadcastable to all heads.\n+            extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n@@ -868,7 +1012,15 @@ def forward(\n             encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n             if encoder_attention_mask is None:\n                 encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+            if use_sdpa_attention_masks:\n+                # Expand the attention mask for SDPA.\n+                # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n+                encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask, embedding_output.dtype, tgt_len=seq_length\n+                )\n+            else:\n+                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n         else:\n             encoder_extended_attention_mask = None\n \n@@ -879,13 +1031,6 @@ def forward(\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n-        embedding_output = self.embeddings(\n-            input_ids=input_ids,\n-            position_ids=position_ids,\n-            token_type_ids=token_type_ids,\n-            inputs_embeds=inputs_embeds,\n-            past_key_values_length=past_key_values_length,\n-        )\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,"
        },
        {
            "sha": "f1f83147527dfb2ac54aa764bf1171c47fd94a7a",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 166,
            "deletions": 23,
            "changes": 189,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=f1a385b1de7e83e2be9b087d1c0646c0c426e2fc",
            "patch": "@@ -20,10 +20,15 @@\n \n import torch\n import torch.utils.checkpoint\n+from packaging import version\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask_for_sdpa,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n+)\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -40,6 +45,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    get_torch_version,\n     logging,\n     replace_return_docstrings,\n )\n@@ -276,6 +282,108 @@ def forward(\n         return outputs\n \n \n+# Copied from transformers.models.bert.modeling_bert.BertSdpaSelfAttention with Bert->Roberta\n+class RobertaSdpaSelfAttention(RobertaSelfAttention):\n+    def __init__(self, config, position_embedding_type=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type)\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n+\n+    # Adapted from RobertaSelfAttention\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n+            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n+            logger.warning_once(\n+                \"RobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n+                \"the manual attention implementation, but specifying the manual implementation will be required from \"\n+                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n+                '`attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states,\n+                attention_mask,\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n+\n+        bsz, tgt_len, _ = hidden_states.size()\n+\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n+        # mask needs to be such that the encoder's padding tokens are not attended to.\n+        is_cross_attention = encoder_hidden_states is not None\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+\n+        # Check `seq_length` of `past_key_value` == `len(current_states)` to support prefix tuning\n+        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n+            key_layer, value_layer = past_key_value\n+        else:\n+            key_layer = self.transpose_for_scores(self.key(current_states))\n+            value_layer = self.transpose_for_scores(self.value(current_states))\n+            if past_key_value is not None and not is_cross_attention:\n+                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n+                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_layer, value_layer)\n+\n+        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n+        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577\n+        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n+            query_layer = query_layer.contiguous()\n+            key_layer = key_layer.contiguous()\n+            value_layer = value_layer.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n+        # a causal mask in case tgt_len == 1.\n+        is_causal = (\n+            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n+        )\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attn_mask=attention_mask,\n+            dropout_p=self.dropout_prob if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n+\n+        outputs = (attn_output,)\n+        if self.is_decoder:\n+            outputs = outputs + (past_key_value,)\n+        return outputs\n+\n+\n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n class RobertaSelfOutput(nn.Module):\n     def __init__(self, config):\n@@ -293,6 +401,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n ROBERTA_SELF_ATTENTION_CLASSES = {\n     \"eager\": RobertaSelfAttention,\n+    \"sdpa\": RobertaSdpaSelfAttention,\n }\n \n \n@@ -585,7 +694,8 @@ class RobertaPreTrainedModel(PreTrainedModel):\n     config_class = RobertaConfig\n     base_model_prefix = \"roberta\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"RobertaEmbeddings\", \"RobertaSelfAttention\"]\n+    _no_split_modules = [\"RobertaEmbeddings\", \"RobertaSelfAttention\", \"RobertaSdpaSelfAttention\"]\n+    _supports_sdpa = True\n \n     # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n     def _init_weights(self, module):\n@@ -676,23 +786,22 @@ def _init_weights(self, module):\n     \"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\n     ROBERTA_START_DOCSTRING,\n )\n+# Copied from transformers.models.bert.modeling_bert.BertModel with Bert->Roberta, BERT->ROBERTA\n class RobertaModel(RobertaPreTrainedModel):\n     \"\"\"\n \n     The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n-    cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n-    all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n-    Kaiser and Illia Polosukhin.\n+    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n+    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n+    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n \n     To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n     to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n     `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n-\n-    .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n-\n     \"\"\"\n \n-    # Copied from transformers.models.clap.modeling_clap.ClapTextModel.__init__ with ClapText->Roberta\n+    _no_split_modules = [\"RobertaEmbeddings\", \"RobertaLayer\"]\n+\n     def __init__(self, config, add_pooling_layer=True):\n         super().__init__(config)\n         self.config = config\n@@ -702,6 +811,9 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = RobertaPooler(config) if add_pooling_layer else None\n \n+        self.attn_implementation = config._attn_implementation\n+        self.position_embedding_type = config.position_embedding_type\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -725,7 +837,6 @@ class PreTrainedModel\n         output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n         config_class=_CONFIG_FOR_DOC,\n     )\n-    # Copied from transformers.models.clap.modeling_clap.ClapTextModel.forward\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -789,9 +900,6 @@ def forward(\n         # past_key_values_length\n         past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n-\n         if token_type_ids is None:\n             if hasattr(self.embeddings, \"token_type_ids\"):\n                 buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n@@ -800,9 +908,43 @@ def forward(\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n \n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n+        embedding_output = self.embeddings(\n+            input_ids=input_ids,\n+            position_ids=position_ids,\n+            token_type_ids=token_type_ids,\n+            inputs_embeds=inputs_embeds,\n+            past_key_values_length=past_key_values_length,\n+        )\n+\n+        if attention_mask is None:\n+            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n+\n+        use_sdpa_attention_masks = (\n+            self.attn_implementation == \"sdpa\"\n+            and self.position_embedding_type == \"absolute\"\n+            and head_mask is None\n+            and not output_attentions\n+        )\n+\n+        # Expand the attention mask\n+        if use_sdpa_attention_masks:\n+            # Expand the attention mask for SDPA.\n+            # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n+            if self.config.is_decoder:\n+                extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                    attention_mask,\n+                    input_shape,\n+                    embedding_output,\n+                    past_key_values_length,\n+                )\n+            else:\n+                extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    attention_mask, embedding_output.dtype, tgt_len=seq_length\n+                )\n+        else:\n+            # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n+            # ourselves in which case we just need to make it broadcastable to all heads.\n+            extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n@@ -811,7 +953,15 @@ def forward(\n             encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n             if encoder_attention_mask is None:\n                 encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+            if use_sdpa_attention_masks:\n+                # Expand the attention mask for SDPA.\n+                # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n+                encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask, embedding_output.dtype, tgt_len=seq_length\n+                )\n+            else:\n+                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n         else:\n             encoder_extended_attention_mask = None\n \n@@ -822,13 +972,6 @@ def forward(\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n-        embedding_output = self.embeddings(\n-            input_ids=input_ids,\n-            position_ids=position_ids,\n-            token_type_ids=token_type_ids,\n-            inputs_embeds=inputs_embeds,\n-            past_key_values_length=past_key_values_length,\n-        )\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,"
        },
        {
            "sha": "95657c260dc7a4663abe9e2379243402be58c446",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=f1a385b1de7e83e2be9b087d1c0646c0c426e2fc",
            "patch": "@@ -569,7 +569,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.RobertaPreTrainedModel with Roberta->RobertaPreLayerNorm,roberta->roberta_prelayernorm\n class RobertaPreLayerNormPreTrainedModel(PreTrainedModel):\n     \"\"\"\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained"
        },
        {
            "sha": "3ac94e75f92fe4aafcfb0d31952d1655da6ec5a7",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 165,
            "deletions": 23,
            "changes": 188,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=f1a385b1de7e83e2be9b087d1c0646c0c426e2fc",
            "patch": "@@ -20,10 +20,15 @@\n \n import torch\n import torch.utils.checkpoint\n+from packaging import version\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask_for_sdpa,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n+)\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -40,6 +45,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    get_torch_version,\n     logging,\n     replace_return_docstrings,\n )\n@@ -277,6 +283,108 @@ def forward(\n         return outputs\n \n \n+# Copied from transformers.models.roberta.modeling_roberta.RobertaSdpaSelfAttention with Roberta->XLMRoberta\n+class XLMRobertaSdpaSelfAttention(XLMRobertaSelfAttention):\n+    def __init__(self, config, position_embedding_type=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type)\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n+\n+    # Adapted from XLMRobertaSelfAttention\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n+            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n+            logger.warning_once(\n+                \"XLMRobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n+                \"the manual attention implementation, but specifying the manual implementation will be required from \"\n+                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n+                '`attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states,\n+                attention_mask,\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n+\n+        bsz, tgt_len, _ = hidden_states.size()\n+\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n+        # mask needs to be such that the encoder's padding tokens are not attended to.\n+        is_cross_attention = encoder_hidden_states is not None\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+\n+        # Check `seq_length` of `past_key_value` == `len(current_states)` to support prefix tuning\n+        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n+            key_layer, value_layer = past_key_value\n+        else:\n+            key_layer = self.transpose_for_scores(self.key(current_states))\n+            value_layer = self.transpose_for_scores(self.value(current_states))\n+            if past_key_value is not None and not is_cross_attention:\n+                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n+                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_layer, value_layer)\n+\n+        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n+        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577\n+        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n+            query_layer = query_layer.contiguous()\n+            key_layer = key_layer.contiguous()\n+            value_layer = value_layer.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n+        # a causal mask in case tgt_len == 1.\n+        is_causal = (\n+            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n+        )\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attn_mask=attention_mask,\n+            dropout_p=self.dropout_prob if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n+\n+        outputs = (attn_output,)\n+        if self.is_decoder:\n+            outputs = outputs + (past_key_value,)\n+        return outputs\n+\n+\n # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfOutput with Roberta->XLMRoberta\n class XLMRobertaSelfOutput(nn.Module):\n     def __init__(self, config):\n@@ -294,6 +402,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n XLM_ROBERTA_SELF_ATTENTION_CLASSES = {\n     \"eager\": XLMRobertaSelfAttention,\n+    \"sdpa\": XLMRobertaSdpaSelfAttention,\n }\n \n \n@@ -587,7 +696,8 @@ class XLMRobertaPreTrainedModel(PreTrainedModel):\n     config_class = XLMRobertaConfig\n     base_model_prefix = \"roberta\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"XLMRobertaEmbeddings\", \"XLMRobertaSelfAttention\"]\n+    _no_split_modules = [\"XLMRobertaEmbeddings\", \"XLMRobertaSelfAttention\", \"XLMRobertaSdpaSelfAttention\"]\n+    _supports_sdpa = True\n \n     # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n     def _init_weights(self, module):\n@@ -682,19 +792,17 @@ class XLMRobertaModel(XLMRobertaPreTrainedModel):\n     \"\"\"\n \n     The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n-    cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n-    all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n-    Kaiser and Illia Polosukhin.\n+    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n+    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n+    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n \n     To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n     to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n     `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n-\n-    .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n-\n     \"\"\"\n \n-    # Copied from transformers.models.clap.modeling_clap.ClapTextModel.__init__ with ClapText->XLMRoberta\n+    _no_split_modules = [\"XLMRobertaEmbeddings\", \"XLMRobertaLayer\"]\n+\n     def __init__(self, config, add_pooling_layer=True):\n         super().__init__(config)\n         self.config = config\n@@ -704,6 +812,9 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = XLMRobertaPooler(config) if add_pooling_layer else None\n \n+        self.attn_implementation = config._attn_implementation\n+        self.position_embedding_type = config.position_embedding_type\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -727,7 +838,6 @@ class PreTrainedModel\n         output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n         config_class=_CONFIG_FOR_DOC,\n     )\n-    # Copied from transformers.models.clap.modeling_clap.ClapTextModel.forward\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -791,9 +901,6 @@ def forward(\n         # past_key_values_length\n         past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n-\n         if token_type_ids is None:\n             if hasattr(self.embeddings, \"token_type_ids\"):\n                 buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n@@ -802,9 +909,43 @@ def forward(\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n \n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n+        embedding_output = self.embeddings(\n+            input_ids=input_ids,\n+            position_ids=position_ids,\n+            token_type_ids=token_type_ids,\n+            inputs_embeds=inputs_embeds,\n+            past_key_values_length=past_key_values_length,\n+        )\n+\n+        if attention_mask is None:\n+            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n+\n+        use_sdpa_attention_masks = (\n+            self.attn_implementation == \"sdpa\"\n+            and self.position_embedding_type == \"absolute\"\n+            and head_mask is None\n+            and not output_attentions\n+        )\n+\n+        # Expand the attention mask\n+        if use_sdpa_attention_masks:\n+            # Expand the attention mask for SDPA.\n+            # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n+            if self.config.is_decoder:\n+                extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                    attention_mask,\n+                    input_shape,\n+                    embedding_output,\n+                    past_key_values_length,\n+                )\n+            else:\n+                extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    attention_mask, embedding_output.dtype, tgt_len=seq_length\n+                )\n+        else:\n+            # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n+            # ourselves in which case we just need to make it broadcastable to all heads.\n+            extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n@@ -813,7 +954,15 @@ def forward(\n             encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n             if encoder_attention_mask is None:\n                 encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+            if use_sdpa_attention_masks:\n+                # Expand the attention mask for SDPA.\n+                # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n+                encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask, embedding_output.dtype, tgt_len=seq_length\n+                )\n+            else:\n+                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n         else:\n             encoder_extended_attention_mask = None\n \n@@ -824,13 +973,6 @@ def forward(\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n-        embedding_output = self.embeddings(\n-            input_ids=input_ids,\n-            position_ids=position_ids,\n-            token_type_ids=token_type_ids,\n-            inputs_embeds=inputs_embeds,\n-            past_key_values_length=past_key_values_length,\n-        )\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,"
        },
        {
            "sha": "f66a32291794f1f6153fa69727c8b8b405cec5fc",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 178,
            "deletions": 23,
            "changes": 201,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=f1a385b1de7e83e2be9b087d1c0646c0c426e2fc",
            "patch": "@@ -19,10 +19,15 @@\n \n import torch\n import torch.utils.checkpoint\n+from packaging import version\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask_for_sdpa,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n+)\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -39,6 +44,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    get_torch_version,\n     logging,\n     replace_return_docstrings,\n )\n@@ -274,6 +280,108 @@ def forward(\n         return outputs\n \n \n+# Copied from transformers.models.bert.modeling_bert.BertSdpaSelfAttention with Bert->XLMRobertaXL\n+class XLMRobertaXLSdpaSelfAttention(XLMRobertaXLSelfAttention):\n+    def __init__(self, config, position_embedding_type=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type)\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n+\n+    # Adapted from XLMRobertaXLSelfAttention\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n+            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n+            logger.warning_once(\n+                \"XLMRobertaXLSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n+                \"the manual attention implementation, but specifying the manual implementation will be required from \"\n+                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n+                '`attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states,\n+                attention_mask,\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n+\n+        bsz, tgt_len, _ = hidden_states.size()\n+\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n+        # mask needs to be such that the encoder's padding tokens are not attended to.\n+        is_cross_attention = encoder_hidden_states is not None\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+\n+        # Check `seq_length` of `past_key_value` == `len(current_states)` to support prefix tuning\n+        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n+            key_layer, value_layer = past_key_value\n+        else:\n+            key_layer = self.transpose_for_scores(self.key(current_states))\n+            value_layer = self.transpose_for_scores(self.value(current_states))\n+            if past_key_value is not None and not is_cross_attention:\n+                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n+                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_layer, value_layer)\n+\n+        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n+        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577\n+        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n+            query_layer = query_layer.contiguous()\n+            key_layer = key_layer.contiguous()\n+            value_layer = value_layer.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n+        # a causal mask in case tgt_len == 1.\n+        is_causal = (\n+            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n+        )\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attn_mask=attention_mask,\n+            dropout_p=self.dropout_prob if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n+\n+        outputs = (attn_output,)\n+        if self.is_decoder:\n+            outputs = outputs + (past_key_value,)\n+        return outputs\n+\n+\n class XLMRobertaXLSelfOutput(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -287,11 +395,19 @@ def forward(self, hidden_states, input_tensor):\n         return hidden_states\n \n \n+XLMROBERTAXL_SELF_ATTENTION_CLASSES = {\n+    \"eager\": XLMRobertaXLSelfAttention,\n+    \"sdpa\": XLMRobertaXLSdpaSelfAttention,\n+}\n+\n+\n class XLMRobertaXLAttention(nn.Module):\n     def __init__(self, config, position_embedding_type=None):\n         super().__init__()\n         self.self_attn_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.self = XLMRobertaXLSelfAttention(config, position_embedding_type=position_embedding_type)\n+        self.self = XLMROBERTAXL_SELF_ATTENTION_CLASSES[config._attn_implementation](\n+            config, position_embedding_type=position_embedding_type\n+        )\n         self.output = XLMRobertaXLSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -573,6 +689,7 @@ class XLMRobertaXLPreTrainedModel(PreTrainedModel):\n     config_class = XLMRobertaXLConfig\n     base_model_prefix = \"roberta\"\n     _no_split_modules = [\"XLMRobertaXLEmbeddings\", \"XLMRobertaXLLayer\"]\n+    _supports_sdpa = True\n \n     # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n     def _init_weights(self, module):\n@@ -651,18 +768,22 @@ def _init_weights(self, module):\n     \"The bare XLM-RoBERTa-XL Model transformer outputting raw hidden-states without any specific head on top.\",\n     XLM_ROBERTA_XL_START_DOCSTRING,\n )\n+# Copied from transformers.models.bert.modeling_bert.BertModel with Bert->XLMRobertaXL, BERT->XLM_ROBERTA_XL\n class XLMRobertaXLModel(XLMRobertaXLPreTrainedModel):\n     \"\"\"\n+\n     The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n-    cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n-    all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n-    Kaiser and Illia Polosukhin. To behave as an decoder the model needs to be initialized with the `is_decoder`\n-    argument of the configuration set to `True`. To be used in a Seq2Seq model, the model needs to initialized with\n-    both `is_decoder` argument and `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as\n-    an input to the forward pass. .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n+    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n+    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n+    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n+\n+    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n+    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n+    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n     \"\"\"\n \n-    # Copied from transformers.models.clap.modeling_clap.ClapTextModel.__init__ with ClapText->XLMRobertaXL\n+    _no_split_modules = [\"XLMRobertaXLEmbeddings\", \"XLMRobertaXLLayer\"]\n+\n     def __init__(self, config, add_pooling_layer=True):\n         super().__init__(config)\n         self.config = config\n@@ -672,6 +793,9 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = XLMRobertaXLPooler(config) if add_pooling_layer else None\n \n+        self.attn_implementation = config._attn_implementation\n+        self.position_embedding_type = config.position_embedding_type\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -695,7 +819,6 @@ class PreTrainedModel\n         output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n         config_class=_CONFIG_FOR_DOC,\n     )\n-    # Copied from transformers.models.clap.modeling_clap.ClapTextModel.forward\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -759,9 +882,6 @@ def forward(\n         # past_key_values_length\n         past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n-\n         if token_type_ids is None:\n             if hasattr(self.embeddings, \"token_type_ids\"):\n                 buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n@@ -770,9 +890,43 @@ def forward(\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n \n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n+        embedding_output = self.embeddings(\n+            input_ids=input_ids,\n+            position_ids=position_ids,\n+            token_type_ids=token_type_ids,\n+            inputs_embeds=inputs_embeds,\n+            past_key_values_length=past_key_values_length,\n+        )\n+\n+        if attention_mask is None:\n+            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n+\n+        use_sdpa_attention_masks = (\n+            self.attn_implementation == \"sdpa\"\n+            and self.position_embedding_type == \"absolute\"\n+            and head_mask is None\n+            and not output_attentions\n+        )\n+\n+        # Expand the attention mask\n+        if use_sdpa_attention_masks:\n+            # Expand the attention mask for SDPA.\n+            # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n+            if self.config.is_decoder:\n+                extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                    attention_mask,\n+                    input_shape,\n+                    embedding_output,\n+                    past_key_values_length,\n+                )\n+            else:\n+                extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    attention_mask, embedding_output.dtype, tgt_len=seq_length\n+                )\n+        else:\n+            # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n+            # ourselves in which case we just need to make it broadcastable to all heads.\n+            extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n@@ -781,7 +935,15 @@ def forward(\n             encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n             if encoder_attention_mask is None:\n                 encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+            if use_sdpa_attention_masks:\n+                # Expand the attention mask for SDPA.\n+                # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n+                encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask, embedding_output.dtype, tgt_len=seq_length\n+                )\n+            else:\n+                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n         else:\n             encoder_extended_attention_mask = None\n \n@@ -792,13 +954,6 @@ def forward(\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n-        embedding_output = self.embeddings(\n-            input_ids=input_ids,\n-            position_ids=position_ids,\n-            token_type_ids=token_type_ids,\n-            inputs_embeds=inputs_embeds,\n-            past_key_values_length=past_key_values_length,\n-        )\n         encoder_outputs = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,"
        },
        {
            "sha": "f779c3a80909b9d88c120d3bf83c985d4ded9b74",
            "filename": "tests/models/camembert/test_modeling_camembert.py",
            "status": "modified",
            "additions": 30,
            "deletions": 2,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/tests%2Fmodels%2Fcamembert%2Ftest_modeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/tests%2Fmodels%2Fcamembert%2Ftest_modeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcamembert%2Ftest_modeling_camembert.py?ref=f1a385b1de7e83e2be9b087d1c0646c0c426e2fc",
            "patch": "@@ -16,7 +16,14 @@\n import unittest\n \n from transformers import is_torch_available\n-from transformers.testing_utils import require_sentencepiece, require_tokenizers, require_torch, slow, torch_device\n+from transformers.testing_utils import (\n+    require_sentencepiece,\n+    require_tokenizers,\n+    require_torch,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n \n \n if is_torch_available():\n@@ -31,7 +38,7 @@\n class CamembertModelIntegrationTest(unittest.TestCase):\n     @slow\n     def test_output_embeds_base_model(self):\n-        model = CamembertModel.from_pretrained(\"almanach/camembert-base\")\n+        model = CamembertModel.from_pretrained(\"almanach/camembert-base\", attn_implementation=\"eager\")\n         model.to(torch_device)\n \n         input_ids = torch.tensor(\n@@ -54,3 +61,24 @@ def test_output_embeds_base_model(self):\n         # expected_slice = roberta.model.forward(input_ids)[0][:, :3, :3].detach()\n \n         self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+\n+    @slow\n+    @require_torch_sdpa\n+    def test_output_embeds_base_model_sdpa(self):\n+        input_ids = torch.tensor(\n+            [[5, 121, 11, 660, 16, 730, 25543, 110, 83, 6]],\n+            device=torch_device,\n+            dtype=torch.long,\n+        )  # J'aime le camembert !\n+\n+        expected_slice = torch.tensor(\n+            [[[-0.0254, 0.0235, 0.1027], [0.0606, -0.1811, -0.0418], [-0.1561, -0.1127, 0.2687]]],\n+            device=torch_device,\n+            dtype=torch.float,\n+        )\n+\n+        model = CamembertModel.from_pretrained(\"almanach/camembert-base\", attn_implementation=\"sdpa\").to(torch_device)\n+        with torch.no_grad():\n+            output = model(input_ids)[\"last_hidden_state\"].detach()\n+\n+        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))"
        },
        {
            "sha": "f8ec1f5b7671a5c911312a0b899873c01e117653",
            "filename": "tests/models/xlm_roberta/test_modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 25,
            "deletions": 2,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_xlm_roberta.py?ref=f1a385b1de7e83e2be9b087d1c0646c0c426e2fc",
            "patch": "@@ -17,7 +17,13 @@\n import unittest\n \n from transformers import is_torch_available\n-from transformers.testing_utils import require_sentencepiece, require_tokenizers, require_torch, slow\n+from transformers.testing_utils import (\n+    require_sentencepiece,\n+    require_tokenizers,\n+    require_torch,\n+    require_torch_sdpa,\n+    slow,\n+)\n \n \n if is_torch_available():\n@@ -32,7 +38,7 @@\n class XLMRobertaModelIntegrationTest(unittest.TestCase):\n     @slow\n     def test_xlm_roberta_base(self):\n-        model = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n+        model = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-base\", attn_implementation=\"eager\")\n         input_ids = torch.tensor([[0, 581, 10269, 83, 99942, 136, 60742, 23, 70, 80583, 18276, 2]])\n         # The dog is cute and lives in the garden house\n \n@@ -49,6 +55,23 @@ def test_xlm_roberta_base(self):\n         # compare the actual values for a slice of last dim\n         self.assertTrue(torch.allclose(output[:, :, -1], expected_output_values_last_dim, atol=1e-3))\n \n+    @require_torch_sdpa\n+    def test_xlm_roberta_base_sdpa(self):\n+        input_ids = torch.tensor([[0, 581, 10269, 83, 99942, 136, 60742, 23, 70, 80583, 18276, 2]])\n+        # The dog is cute and lives in the garden house\n+\n+        expected_output_shape = torch.Size((1, 12, 768))  # batch_size, sequence_length, embedding_vector_dim\n+        expected_output_values_last_dim = torch.tensor(\n+            [[-0.0101, 0.1218, -0.0803, 0.0801, 0.1327, 0.0776, -0.1215, 0.2383, 0.3338, 0.3106, 0.0300, 0.0252]]\n+        )\n+\n+        model = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-base\", attn_implementation=\"sdpa\")\n+        with torch.no_grad():\n+            output = model(input_ids)[\"last_hidden_state\"].detach()\n+        self.assertEqual(output.shape, expected_output_shape)\n+        # compare the actual values for a slice of last dim\n+        self.assertTrue(torch.allclose(output[:, :, -1], expected_output_values_last_dim, atol=1e-3))\n+\n     @slow\n     def test_xlm_roberta_large(self):\n         model = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-large\")"
        },
        {
            "sha": "a73f5618ff7edafb4962304bb6d920c643d4011d",
            "filename": "tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 80,
            "deletions": 1,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py?ref=f1a385b1de7e83e2be9b087d1c0646c0c426e2fc",
            "patch": "@@ -14,10 +14,11 @@\n # limitations under the License.\n \n \n+import tempfile\n import unittest\n \n from transformers import XLMRobertaXLConfig, is_torch_available\n-from transformers.testing_utils import require_torch, slow, torch_device\n+from transformers.testing_utils import require_torch, require_torch_sdpa, slow, torch_device\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -515,6 +516,84 @@ def test_create_position_ids_from_inputs_embeds(self):\n         self.assertEqual(position_ids.shape, expected_positions.shape)\n         self.assertTrue(torch.all(torch.eq(position_ids, expected_positions)))\n \n+    # TODO: Remove this and use the parent method (in common tests) once XLM RoBERTa XL supports low_cpu_mem_usage=True.\n+    @require_torch_sdpa\n+    @slow\n+    # Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_generate\n+    def test_eager_matches_sdpa_generate(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        max_new_tokens = 30\n+\n+        if len(self.all_generative_model_classes) == 0:\n+            self.skipTest(f\"{self.__class__.__name__} tests a model that does support generate: skipping this test\")\n+\n+        for model_class in self.all_generative_model_classes:\n+            if not model_class._supports_sdpa:\n+                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            dummy_input = inputs_dict[model_class.main_input_name]\n+            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n+                dummy_input = dummy_input.to(torch.float16)\n+\n+            # make sure that all models have enough positions for generation\n+            if hasattr(config, \"max_position_embeddings\"):\n+                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n+\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n+\n+                # Ignore copy\n+                model_sdpa = model_class.from_pretrained(\n+                    tmpdirname,\n+                    torch_dtype=torch.float16,\n+                    low_cpu_mem_usage=False,\n+                ).to(torch_device)\n+\n+                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+\n+                # Ignore copy\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    torch_dtype=torch.float16,\n+                    low_cpu_mem_usage=False,\n+                    attn_implementation=\"eager\",\n+                ).to(torch_device)\n+\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+                has_sdpa = False\n+                for name, submodule in model_sdpa.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        has_sdpa = True\n+                        break\n+                if not has_sdpa:\n+                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n+                # Just test that a large cache works as expected\n+                res_eager = model_eager.generate(\n+                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n+                )\n+\n+                res_sdpa = model_sdpa.generate(\n+                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n+                )\n+\n+                self.assertTrue(torch.allclose(res_eager, res_sdpa))\n+\n \n @require_torch\n class XLMRobertaModelXLIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "55d93611f4ce5fb9962bccd6335205e019769543",
            "filename": "utils/check_support_list.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/utils%2Fcheck_support_list.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1a385b1de7e83e2be9b087d1c0646c0c426e2fc/utils%2Fcheck_support_list.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_support_list.py?ref=f1a385b1de7e83e2be9b087d1c0646c0c426e2fc",
            "patch": "@@ -70,6 +70,7 @@ def check_sdpa_support_list():\n             \"For now, Transformers supports SDPA inference and training for the following architectures:\"\n         )[1]\n         doctext = doctext.split(\"Note that FlashAttention can only be used for models using the\")[0]\n+        doctext = doctext.lower()\n \n     patterns = glob(os.path.join(REPO_PATH, \"src/transformers/models/**/modeling_*.py\"))\n     patterns_tf = glob(os.path.join(REPO_PATH, \"src/transformers/models/**/modeling_tf_*.py\"))\n@@ -85,7 +86,7 @@ def check_sdpa_support_list():\n                 archs_supporting_sdpa.append(model_name)\n \n     for arch in archs_supporting_sdpa:\n-        if arch not in doctext and arch not in doctext.replace(\"-\", \"_\"):\n+        if not any(term in doctext for term in [arch, arch.replace(\"_\", \"-\"), arch.replace(\"_\", \" \")]):\n             raise ValueError(\n                 f\"{arch} should be in listed in the SDPA documentation but is not. Please update the documentation.\"\n             )"
        }
    ],
    "stats": {
        "total": 928,
        "additions": 828,
        "deletions": 100
    }
}