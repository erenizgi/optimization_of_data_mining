{
    "author": "LysandreJik",
    "message": "Enable non-streaming mode in `transformers serve` (#41446)\n\n* Enable non-streaming in transformers serve\n\nRemove typos\n\nRemove typos\n\nRemove typos\n\n* Fix tests\n\n* Arthur review",
    "sha": "13a35a5057cbdc34b6c93a26d4e57987cbdd205c",
    "files": [
        {
            "sha": "b94032280e08dca93ead758763bb486a5d6674aa",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 157,
            "deletions": 52,
            "changes": 209,
            "blob_url": "https://github.com/huggingface/transformers/blob/13a35a5057cbdc34b6c93a26d4e57987cbdd205c/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13a35a5057cbdc34b6c93a26d4e57987cbdd205c/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=13a35a5057cbdc34b6c93a26d4e57987cbdd205c",
            "patch": "@@ -26,7 +26,7 @@\n import time\n import uuid\n from argparse import ArgumentParser, Namespace\n-from collections.abc import AsyncGenerator, Generator, Iterable\n+from collections.abc import Generator, Iterable\n from contextlib import asynccontextmanager\n from dataclasses import dataclass, field\n from io import BytesIO\n@@ -35,6 +35,7 @@\n \n from huggingface_hub import model_info\n from huggingface_hub.constants import HF_HUB_OFFLINE\n+from openai.types.chat.chat_completion import Choice\n from tokenizers.decoders import DecodeStream\n \n import transformers\n@@ -90,14 +91,16 @@\n     from fastapi.responses import JSONResponse, StreamingResponse\n     from openai.types.audio.transcription import Transcription\n     from openai.types.audio.transcription_create_params import TranscriptionCreateParamsBase\n-    from openai.types.chat import ChatCompletionMessageParam\n+    from openai.types.chat import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageParam\n     from openai.types.chat.chat_completion_chunk import (\n         ChatCompletionChunk,\n-        Choice,\n         ChoiceDelta,\n         ChoiceDeltaToolCall,\n         ChoiceDeltaToolCallFunction,\n     )\n+    from openai.types.chat.chat_completion_chunk import (\n+        Choice as ChoiceChunk,\n+    )\n     from openai.types.chat.completion_create_params import CompletionCreateParamsStreaming\n     from openai.types.responses import (\n         Response,\n@@ -345,8 +348,11 @@ def delete_model(self):\n             self._timer.cancel()\n \n     def timeout_reached(self):\n-        self.delete_model()\n-        logger.info(f\"{self._name_or_path} was removed from memory after {self.timeout_seconds} seconds of inactivity\")\n+        if self.timeout_seconds > 0:\n+            self.delete_model()\n+            logger.info(\n+                f\"{self._name_or_path} was removed from memory after {self.timeout_seconds} seconds of inactivity\"\n+            )\n \n     def is_deleted(self):\n         \"\"\"Check if the instances have been deleted.\"\"\"\n@@ -412,9 +418,13 @@ class ServeArguments:\n     # Serving settings\n     host: str = field(default=\"localhost\", metadata={\"help\": \"Interface the server will listen to.\"})\n     port: int = field(default=8000, metadata={\"help\": \"Port the server will listen to.\"})\n-    model_timeout: int = field(\n-        default=300,\n-        metadata={\"help\": \"Time in seconds after which a model will be removed from memory.\"},\n+    model_timeout: Optional[int] = field(\n+        default=None,\n+        metadata={\n+            \"help\": \"Time in seconds after which a model will be removed from memory; defaults to 300 unless \"\n+            \"`force_model` is set, in which case the model will not be removed from memory unless a value\"\n+            \"is specified here.\"\n+        },\n     )\n \n     # Other settings\n@@ -512,6 +522,14 @@ def __init__(self, args: ServeArguments):\n         self.last_kv_cache = None\n         self.last_model = None\n \n+        if self.args.model_timeout is None:\n+            self.args.model_timeout = -1 if self.args.force_model else 300\n+\n+        if self.args.force_model:\n+            model_id_and_revision = self.process_model_name(self.args.force_model)\n+            self.last_model = model_id_and_revision\n+            self.load_model_and_processor(model_id_and_revision)\n+\n     def _validate_request(\n         self,\n         request: dict,\n@@ -595,7 +613,7 @@ def build_chat_completion_chunk(\n         tool_calls: Optional[list[\"ChoiceDeltaToolCall\"]] = None,\n         decode_stream: Optional[DecodeStream] = None,\n         tokenizer: Optional[PreTrainedTokenizerFast] = None,\n-    ) -> str:\n+    ) -> ChatCompletionChunk:\n         \"\"\"\n         Builds a chunk of a streaming OpenAI Chat Completion response.\n \n@@ -621,12 +639,13 @@ def build_chat_completion_chunk(\n         \"\"\"\n         if decode_stream is not None and content is not None and tokenizer is not None:\n             content = decode_stream.step(tokenizer._tokenizer, content)\n+\n         chunk = ChatCompletionChunk(\n             id=request_id,\n             created=int(time.time()),\n             model=model,\n             choices=[\n-                Choice(\n+                ChoiceChunk(\n                     delta=ChoiceDelta(\n                         content=content,\n                         role=role,\n@@ -639,23 +658,25 @@ def build_chat_completion_chunk(\n             system_fingerprint=\"\",\n             object=\"chat.completion.chunk\",\n         )\n-        return f\"data: {chunk.model_dump_json(exclude_none=True)}\\n\\n\"\n \n-    def build_response_event(self, response: \"BaseModel\") -> str:\n+        return chunk\n+\n+    @staticmethod\n+    def chunk_to_sse_element(chunk: ChatCompletionChunk | BaseModel) -> str:\n         \"\"\"\n-        Builds a event of a streaming OpenAI Response response.\n+        Builds an event of a streaming OpenAI Response model or a ChatCompletion chunk.\n \n         IMPORTANT: The serialized chunk won't contain empty fields (fields with `None`). Some downstream apps,\n         like Cursor, assume that when the field exists, it has data.\n \n         Args:\n-            response (`BaseModel`):\n+            chunk (`BaseModel` or `ChatCompletionChunk`):\n                 The response to build an event from. One of the multiple OpenAI Response output types\n \n         Returns:\n             `str`: The built chunk, a string containing a JSON string with the payload.\n         \"\"\"\n-        return f\"data: {response.model_dump_json(exclude_none=True)}\\n\\n\"\n+        return f\"data: {chunk.model_dump_json(exclude_none=True)}\\n\\n\"\n \n     def run(self):\n         \"\"\"\n@@ -668,6 +689,7 @@ def run(self):\n         - POST /v1/responses: Generates responses.\n         - POST /v1/audio/transcriptions: Generates transcriptions from audio.\n         - GET /v1/models: Lists available models for 3rd party tools.\n+        - GET /health: Health check.\n \n         Requires FastAPI and Uvicorn to be installed.\n         \"\"\"\n@@ -703,10 +725,9 @@ def chat_completion(request: Request, body: dict):\n             self.validate_chat_completion_request(request=body)\n \n             if self.use_continuous_batching:\n-                output = self.continuous_batching_chat_completion(body, request.state.request_id)\n+                return self.continuous_batching_chat_completion(body, request.state.request_id)\n             else:\n-                output = self.generate_chat_completion(body)\n-            return StreamingResponse(output, media_type=\"text/event-stream\")\n+                return self.generate_chat_completion(body)\n \n         @app.post(\"/v1/responses\")\n         def responses(request: dict):\n@@ -803,7 +824,7 @@ def get_gen_models(self) -> list[dict[str, any]]:\n                 for model in model_infos\n             ]\n \n-    def continuous_batching_chat_completion(self, req: dict, request_id: str) -> AsyncGenerator[str, None]:\n+    def continuous_batching_chat_completion(self, req: dict, request_id: str) -> StreamingResponse | JSONResponse:\n         \"\"\"\n         Generates an OpenAI Chat Completion using continuous batching.\n \n@@ -816,14 +837,16 @@ def continuous_batching_chat_completion(self, req: dict, request_id: str) -> Asy\n \n         model_id_and_revision = self.process_model_name(req[\"model\"])\n         must_discard_cache = model_id_and_revision != self.last_model\n+\n         self.last_model = model_id_and_revision\n+\n+        # When switching models, terminate a continuous batching manager if it is running.\n         if must_discard_cache:\n-            # When switching models, terminate a continuous batching manager if it is running.\n             if self.running_continuous_batching_manager is not None:\n                 self.running_continuous_batching_manager.stop(block=True, timeout=2)\n                 self.running_continuous_batching_manager = None\n-        model, processor = self.load_model_and_processor(model_id_and_revision)\n \n+        model, processor = self.load_model_and_processor(model_id_and_revision)\n         tokenizer = processor.tokenizer if hasattr(processor, \"tokenizer\") else processor\n \n         generation_config = create_generation_config_from_req(\n@@ -838,18 +861,17 @@ def continuous_batching_chat_completion(self, req: dict, request_id: str) -> Asy\n \n         if self.running_continuous_batching_manager is None:\n             self.running_continuous_batching_manager = model.init_continuous_batching(\n-                generation_config=generation_config, streaming=True\n+                generation_config=generation_config\n             )\n \n-            # TODO (Joao, Lysandre): the logits processors should be fixed in continuous batching\n-            # and correctly applied in non-cb\n+            # TODO (Joao, Lysandre): the logits processors should be fixed in continuous batching and correctly applied in non-cb\n             self.running_continuous_batching_manager.logit_processor = LogitsProcessorList()\n             self.running_continuous_batching_manager.start()\n \n         # TODO (Joao, Lysandre): this should also work with tool support\n         inputs = processor.apply_chat_template(req[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True).to(\n             model.device\n-        )\n+        )[0]\n \n         def stream_chat_completion(request_id, decode_stream):\n             try:\n@@ -879,21 +901,61 @@ def stream_chat_completion(request_id, decode_stream):\n                 self.running_continuous_batching_manager.cancel_request(request_id)\n                 yield f'data: {{\"error\": \"{str(e)}\"}}'\n \n-        async def cancellation_wrapper(_inputs, request_id):\n+        def buffer_chat_completion(_request_id):\n+            result = None\n+            while self.running_continuous_batching_manager.is_running() and result is None:\n+                result = self.running_continuous_batching_manager.get_result(request_id=_request_id, timeout=1)\n+\n+            content = tokenizer.decode(result.generated_tokens)\n+\n+            chat_completion_result = ChatCompletion(\n+                id=_request_id,\n+                created=int(time.time()),\n+                object=\"chat.completion\",\n+                model=model_id_and_revision,\n+                choices=[\n+                    Choice(\n+                        # TODO check the index\n+                        index=0,\n+                        message=ChatCompletionMessage(content=content, role=\"assistant\"),\n+                        finish_reason=\"stop\",\n+                    )\n+                ],\n+                # TODO implement function calling\n+                # TODO implement usage\n+            )\n+\n+            return chat_completion_result\n+\n+        async def cancellation_wrapper_stream(_request_id):\n+            # Enables cancellation in an async context\n             try:\n-                decode_stream = DecodeStream(_inputs.tolist(), False)\n-                # XXX: using returned request_id as safety in case it is None\n-                request_id = self.running_continuous_batching_manager.add_request(\n-                    _inputs, request_id=request_id, max_new_tokens=generation_config.max_new_tokens\n-                )\n-                for chunk in stream_chat_completion(request_id, decode_stream):\n-                    yield chunk\n-                    await asyncio.sleep(0)  # Yield control to the event loop to check for cancellations\n+                decode_stream = DecodeStream(inputs.tolist(), False)\n+                for _chunk in stream_chat_completion(_request_id, decode_stream):\n+                    yield self.chunk_to_sse_element(_chunk)\n+                    await asyncio.sleep(0)\n             except asyncio.CancelledError:\n-                self.running_continuous_batching_manager.cancel_request(request_id)\n-                logger.warning(f\"Request {request_id} was cancelled.\")\n+                self.running_continuous_batching_manager.cancel_request(_request_id)\n+                logger.warning(f\"Request {_request_id} was cancelled.\")\n \n-        return cancellation_wrapper(inputs[0], request_id)\n+        def cancellation_wrapper_buffer(_request_id):\n+            # Enables cancellation in an async context\n+            try:\n+                return buffer_chat_completion(_request_id)\n+            except asyncio.CancelledError:\n+                self.running_continuous_batching_manager.cancel_request(_request_id)\n+                logger.warning(f\"Request {_request_id} was cancelled.\")\n+\n+        request_id = self.running_continuous_batching_manager.add_request(\n+            inputs, request_id=request_id, max_new_tokens=generation_config.max_new_tokens, streaming=req.get(\"stream\")\n+        )\n+\n+        if req.get(\"stream\"):\n+            return StreamingResponse(cancellation_wrapper_stream(request_id), media_type=\"text/event-stream\")\n+        else:\n+            chunk = cancellation_wrapper_buffer(request_id)\n+            json_chunk = chunk.model_dump_json(exclude_none=True)\n+            return JSONResponse(json_chunk, media_type=\"application/json\")\n \n     @staticmethod\n     def get_model_modality(model: \"PreTrainedModel\") -> Modality:\n@@ -953,7 +1015,7 @@ def get_processor_inputs_from_inbound_messages(messages, modality: Modality):\n             processor_inputs.append(parsed_message)\n         return processor_inputs\n \n-    def generate_chat_completion(self, req: dict) -> Generator[str, None, None]:\n+    def generate_chat_completion(self, req: dict) -> StreamingResponse | JSONResponse:\n         \"\"\"\n         Generates an OpenAI Chat Completion using `generate`.\n \n@@ -1132,7 +1194,10 @@ def generate_with_cache(**kwargs):\n                                 )\n \n                             yield self.build_chat_completion_chunk(\n-                                request_id=_request_id, role=None, tool_calls=[tool], model=model_id_and_revision\n+                                request_id=_request_id,\n+                                role=None,\n+                                tool_calls=[tool],\n+                                model=model_id_and_revision,\n                             )\n                             continue\n                     # ====== END OF TOOL CALL LOGIC ======\n@@ -1152,7 +1217,47 @@ def generate_with_cache(**kwargs):\n             finally:\n                 thread.join()\n \n-        return stream_chat_completion(generation_streamer, request_id)\n+        if req.get(\"stream\"):\n+            return StreamingResponse(\n+                map(self.chunk_to_sse_element, stream_chat_completion(generation_streamer, request_id)),\n+                media_type=\"text/event-stream\",\n+            )\n+        else:\n+            content = []\n+            finish_reason = \"stop\"\n+\n+            generator = stream_chat_completion(generation_streamer, request_id)\n+            usage = None\n+\n+            for chunk in generator:\n+                choice = chunk.choices[0]\n+                if getattr(choice.delta, \"content\", None):\n+                    content.append(choice.delta.content)\n+                if choice.finish_reason:\n+                    finish_reason = choice.finish_reason\n+                if getattr(chunk, \"usage\", None):\n+                    usage = chunk.usage\n+\n+            chat_completion_result = ChatCompletion(\n+                id=request_id,\n+                created=int(time.time()),\n+                object=\"chat.completion\",\n+                model=model_id_and_revision,\n+                choices=[\n+                    Choice(\n+                        # TODO check the index\n+                        index=0,\n+                        message=ChatCompletionMessage(content=\"\".join(content), role=\"assistant\"),\n+                        finish_reason=finish_reason,\n+                    )\n+                ],\n+                # TODO implement function calling\n+                usage=usage,\n+            )\n+\n+            result = chat_completion_result.model_dump(exclude_none=True)\n+\n+            return JSONResponse(result, media_type=\"application/json\")\n \n     def generate_response(self, req: dict) -> Generator[str, None, None]:\n         \"\"\"\n@@ -1263,7 +1368,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_created)\n+                yield self.chunk_to_sse_element(response_created)\n \n                 response_in_progress = ResponseInProgressEvent(\n                     type=\"response.in_progress\",\n@@ -1284,7 +1389,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_in_progress)\n+                yield self.chunk_to_sse_element(response_in_progress)\n \n                 # Start the output item. Emit the assistant role to start the stream. Other chunks won't have a role,\n                 # as it is implicit\n@@ -1297,7 +1402,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_output_item_added)\n+                yield self.chunk_to_sse_element(response_output_item_added)\n \n                 # Start the content part of the event\n                 response_content_part_added = ResponseContentPartAddedEvent(\n@@ -1309,7 +1414,7 @@ def generate_with_cache(**kwargs):\n                     part=ResponseOutputText(type=\"output_text\", text=\"\", annotations=[]),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_content_part_added)\n+                yield self.chunk_to_sse_element(response_content_part_added)\n \n                 # Stream the actual generated text\n                 results = \"\"\n@@ -1336,7 +1441,7 @@ def generate_with_cache(**kwargs):\n                                 logprobs=[],\n                             )\n                             sequence_number += 1\n-                            yield self.build_response_event(response_output_text_delta)\n+                            yield self.chunk_to_sse_element(response_output_text_delta)\n                     else:\n                         # Normal path: emit token deltas when not filtering CoT\n                         if result:\n@@ -1350,7 +1455,7 @@ def generate_with_cache(**kwargs):\n                                 logprobs=[],\n                             )\n                             sequence_number += 1\n-                            yield self.build_response_event(response_output_text_delta)\n+                            yield self.chunk_to_sse_element(response_output_text_delta)\n \n                 # Signal the end of the text generation\n                 response_output_text_done = ResponseTextDoneEvent(\n@@ -1363,7 +1468,7 @@ def generate_with_cache(**kwargs):\n                     logprobs=[],\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_output_text_done)\n+                yield self.chunk_to_sse_element(response_output_text_done)\n \n                 # Complete the content part\n                 response_content_part_done = ResponseContentPartDoneEvent(\n@@ -1376,7 +1481,7 @@ def generate_with_cache(**kwargs):\n                 )\n                 sequence_number += 1\n                 content_index += 1\n-                yield self.build_response_event(response_content_part_done)\n+                yield self.chunk_to_sse_element(response_content_part_done)\n \n                 # Complete the output item\n                 response_output_item_done = ResponseOutputItemDoneEvent(\n@@ -1394,7 +1499,7 @@ def generate_with_cache(**kwargs):\n                 )\n                 sequence_number += 1\n                 output_index += 1\n-                yield self.build_response_event(response_output_item_done)\n+                yield self.chunk_to_sse_element(response_output_item_done)\n \n                 # Finally, Complete the event\n                 response_completed = ResponseCompletedEvent(\n@@ -1416,7 +1521,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_completed)\n+                yield self.chunk_to_sse_element(response_completed)\n \n                 thread.join()\n             except Exception as e:\n@@ -1427,7 +1532,7 @@ def generate_with_cache(**kwargs):\n                     message=str(e),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(error_event)\n+                yield self.chunk_to_sse_element(error_event)\n \n                 response_failed = ResponseFailedEvent(\n                     type=\"response.failed\",\n@@ -1452,7 +1557,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_failed)\n+                yield self.chunk_to_sse_element(response_failed)\n \n             finally:\n                 thread.join()"
        },
        {
            "sha": "dd0a206469283d87817cc0f3cef0dc0781216282",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/13a35a5057cbdc34b6c93a26d4e57987cbdd205c/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13a35a5057cbdc34b6c93a26d4e57987cbdd205c/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=13a35a5057cbdc34b6c93a26d4e57987cbdd205c",
            "patch": "@@ -907,7 +907,6 @@ def get_result(\n             if request_id is not None and result.request_id != request_id:\n                 self.output_queue.put(result)\n                 return None\n-            logger.debug(f\"Retrieved result for request {result.request_id}\")\n             return result\n         except queue.Empty:\n             return None"
        },
        {
            "sha": "8a8090bcad23a2fdd691c7f4523390b7929a6c06",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/13a35a5057cbdc34b6c93a26d4e57987cbdd205c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13a35a5057cbdc34b6c93a26d4e57987cbdd205c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=13a35a5057cbdc34b6c93a26d4e57987cbdd205c",
            "patch": "@@ -2510,14 +2510,14 @@ def _check_and_adjust_attn_implementation(\n             try:\n                 load_and_register_attn_kernel(applicable_attn_implementation)\n                 # log that we used kernel fallback if successful\n-                if attn_implementation.startswith(\"flash_attention\"):\n+                if \"flash_\" in attn_implementation:\n                     logger.warning_once(\n                         f\"You do not have `flash_attn` installed, using `{applicable_attn_implementation}` \"\n                         \"from the `kernels` library instead!\"\n                     )\n             except Exception as e:\n                 # raise the proper exception for requested flash attention\n-                if attn_implementation.startswith(\"flash_attention\"):\n+                if attn_implementation.startswith(\"flash_\"):\n                     if attn_implementation.endswith(\"2\"):\n                         self._flash_attn_2_can_dispatch()\n                     else:\n@@ -2530,7 +2530,7 @@ def _check_and_adjust_attn_implementation(\n                 applicable_attn_implementation, is_init_check\n             )\n             # preload flash attention here to allow compile with fullgraph\n-            if applicable_attn_implementation.startswith(\"flash_attention\"):\n+            if applicable_attn_implementation.startswith(\"flash_\"):\n                 lazy_import_flash_attention(applicable_attn_implementation, force_import=True)\n         return applicable_attn_implementation\n "
        },
        {
            "sha": "6a775e119aedfdebaf1911e50af77116139edaff",
            "filename": "tests/commands/test_serving.py",
            "status": "modified",
            "additions": 51,
            "deletions": 17,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/13a35a5057cbdc34b6c93a26d4e57987cbdd205c/tests%2Fcommands%2Ftest_serving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13a35a5057cbdc34b6c93a26d4e57987cbdd205c/tests%2Fcommands%2Ftest_serving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcommands%2Ftest_serving.py?ref=13a35a5057cbdc34b6c93a26d4e57987cbdd205c",
            "patch": "@@ -85,6 +85,7 @@ def test_build_chat_completion_chunk(self):\n         chunk = ServeCommand.build_chat_completion_chunk(\n             dummy, request_id=\"req0\", content=\"hello\", finish_reason=\"stop\", role=\"user\", model=\"dummy_model@main\"\n         )\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn(\n@@ -93,12 +94,14 @@ def test_build_chat_completion_chunk(self):\n \n         # Case 2: only the role is provided -- other fields in 'choices' are omitted\n         chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", role=\"user\", model=\"dummy_model@main\")\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn('\"choices\":[{\"delta\":{\"role\":\"user\"},\"index\":0}]', chunk)\n \n         # Case 3: only the content is provided -- other fields in 'choices' are omitted\n         chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", content=\"hello\", model=\"dummy_model@main\")\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn('\"choices\":[{\"delta\":{\"content\":\"hello\"},\"index\":0}]', chunk)\n@@ -110,6 +113,7 @@ def test_build_chat_completion_chunk(self):\n             type=\"function\",\n         )\n         chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", tool_calls=[tool_call], model=\"dummy_model@main\")\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         expected_choices_content = (\n@@ -147,7 +151,7 @@ def test_build_response_event(self):\n             ),\n         )\n \n-        event = dummy.build_response_event(response_created)\n+        event = dummy.chunk_to_sse_element(response_created)\n         self.assertTrue(event.startswith(\"data: \"))  # Sanity check: event formatting\n         self.assertIn('\"model\":\"dummy_model@main\"', event)  # Sanity check: set field\n         self.assertIn('\"status\":\"queued\"', event)\n@@ -411,10 +415,18 @@ def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n         cls.port = 8001\n         args = ServeArguments(port=cls.port)\n-        serve_command = ServeCommand(args)\n-        thread = Thread(target=serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.serve_command = ServeCommand(args)\n+        cls.thread = Thread(target=cls.serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     @slow\n     def test_tool_call(self):\n@@ -548,13 +560,19 @@ class ServeCompletionsContinuousBatchingIntegrationTest(ServeCompletionsMixin, u\n     def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n         cls.port = 8002\n-        args = ServeArguments(\n-            port=cls.port, continuous_batching=True, attn_implementation=\"sdpa_paged\", default_seed=42\n-        )\n+        args = ServeArguments(port=cls.port, continuous_batching=True, default_seed=42)\n         cls.serve_command = ServeCommand(args)\n-        thread = Thread(target=cls.serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.thread = Thread(target=cls.serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     def test_full_request(self):\n         \"\"\"Tests that an inference using the Responses API and Continuous Batching works\"\"\"\n@@ -703,9 +721,17 @@ def setUpClass(cls):\n         cls.port = 8003\n         args = ServeArguments(port=cls.port, default_seed=42)\n         serve_command = ServeCommand(args)\n-        thread = Thread(target=serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.thread = Thread(target=serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     @slow\n     def test_full_request(self):\n@@ -767,9 +793,17 @@ def setUpClass(cls):\n         cls.port = 8042\n         args = ServeArguments(port=cls.port)\n         serve_command = ServeCommand(args)\n-        thread = Thread(target=serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.thread = Thread(target=serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     def test_healthcheck(self):\n         \"\"\"Tests that the healthcheck endpoint works.\"\"\""
        }
    ],
    "stats": {
        "total": 284,
        "additions": 211,
        "deletions": 73
    }
}