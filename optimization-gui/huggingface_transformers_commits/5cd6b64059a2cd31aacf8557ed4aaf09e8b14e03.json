{
    "author": "yonigozlan",
    "message": "Process inputs directly in apply_chat_template in image-text-to-text pipeline (#35616)\n\n* tokenize inputs directly in apply_chat_template\n\n* refactor processing\n\n* revert changes processing llava\n\n* Update docs\n\n* fix issue with str being iterable\n\n* add test chat text only\n\n* change function name",
    "sha": "5cd6b64059a2cd31aacf8557ed4aaf09e8b14e03",
    "files": [
        {
            "sha": "a98b1e5509cc3f0f925c666429fcc5f10cf2b158",
            "filename": "docs/source/en/tasks/image_text_to_text.md",
            "status": "modified",
            "additions": 42,
            "deletions": 1,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/5cd6b64059a2cd31aacf8557ed4aaf09e8b14e03/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5cd6b64059a2cd31aacf8557ed4aaf09e8b14e03/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md?ref=5cd6b64059a2cd31aacf8557ed4aaf09e8b14e03",
            "patch": "@@ -160,7 +160,48 @@ outputs[0][\"generated_text\"]\n #  with a yellow center in the foreground. The flower is surrounded by red and white flowers with green stems\n ```\n \n-## Streaming\n+If you prefer, you can also load the images separately and pass them to the pipeline like so:\n+\n+```python\n+pipe = pipeline(\"image-text-to-text\", model=\"HuggingFaceTB/SmolVLM-256M-Instruct\")\n+\n+img_urls = [\n+    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\",\n+    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\n+]\n+images = [\n+    Image.open(requests.get(img_urls[0], stream=True).raw),\n+    Image.open(requests.get(img_urls[1], stream=True).raw),\n+]\n+\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\"},\n+            {\"type\": \"image\"},\n+            {\"type\": \"text\", \"text\": \"What do you see in these images?\"},\n+        ],\n+    }\n+]\n+outputs = pipe(text=messages, images=images, max_new_tokens=50, return_full_text=False)\n+outputs[0][\"generated_text\"]\n+\" In the first image, there are two cats sitting on a plant. In the second image, there are flowers with a pinkish hue.\"\n+```\n+\n+The images will still be included in the `\"input_text\"` field of the output:\n+\n+```python\n+outputs[0]['input_text']\n+\"\"\"\n+[{'role': 'user',\n+  'content': [{'type': 'image',\n+    'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=622x412>},\n+   {'type': 'image',\n+    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=5184x3456>},\n+   {'type': 'text', 'text': 'What do you see in these images?'}]}]## Streaming\n+\"\"\"\n+```\n \n We can use [text streaming](./generation_strategies#streaming) for a better generation experience. Transformers supports streaming with the [`TextStreamer`] or [`TextIteratorStreamer`] classes. We will use the [`TextIteratorStreamer`] with IDEFICS-8B.\n "
        },
        {
            "sha": "537d0a854370955c413568e22d90d4cebd1cc2bf",
            "filename": "src/transformers/pipelines/image_text_to_text.py",
            "status": "modified",
            "additions": 43,
            "deletions": 49,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/5cd6b64059a2cd31aacf8557ed4aaf09e8b14e03/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5cd6b64059a2cd31aacf8557ed4aaf09e8b14e03/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py?ref=5cd6b64059a2cd31aacf8557ed4aaf09e8b14e03",
            "patch": "@@ -58,60 +58,56 @@ def __init__(self, messages: Dict, images: Union[str, List[str], \"Image.Image\",\n         for message in messages:\n             if not (\"role\" in message and \"content\" in message):\n                 raise ValueError(\"When passing chat dicts as input, each dict must have a 'role' and 'content' key.\")\n-        images = retrieve_images_in_messages(messages, images)\n+        messages = add_images_to_messages(messages, images)\n \n         self.messages = messages\n-        self.images = images\n \n \n-def retrieve_images_in_messages(\n+def add_images_to_messages(\n     messages: dict, images: Optional[Union[str, List[str], \"Image.Image\", List[\"Image.Image\"]]]\n ):\n     \"\"\"\n     Retrieve and combine images from the chat and the images passed as input.\n     \"\"\"\n     if images is None:\n         images = []\n-    elif not isinstance(images, Iterable):\n+    elif not isinstance(images, Iterable) or isinstance(images, str):\n         images = [images]\n     idx_images = 0\n-    retrieved_images = []\n     for message in messages:\n         for content in message[\"content\"]:\n-            if isinstance(content, dict):\n-                if content.get(\"type\") == \"image\":\n-                    for key in [\"image\", \"url\", \"path\", \"base64\"]:\n-                        if key in content:\n-                            retrieved_images.append(content[key])\n-                            break\n-                    else:\n-                        if idx_images < len(images):\n-                            retrieved_images.append(images[idx_images])\n-                            idx_images += 1\n-                        else:\n-                            raise ValueError(\n-                                \"The number of images in the chat messages should be the same as the number of images passed to the pipeline.\"\n-                            )\n-                # Add support for OpenAI/TGI chat format\n-                elif content.get(\"type\") == \"image_url\":\n-                    if isinstance(content.get(\"image_url\"), dict) and \"url\" in content[\"image_url\"]:\n-                        retrieved_images.append(content[\"image_url\"][\"url\"])\n-                        # Rewrite content to be in the Transformers chat format\n-                        content[\"type\"] = \"image\"\n-                        content[\"image\"] = content[\"image_url\"][\"url\"]\n-                        del content[\"image_url\"]\n+            if not isinstance(content, dict):\n+                continue\n+            content_type = content.get(\"type\")\n+            if content_type == \"image\":\n+                if not any(key in content for key in [\"image\", \"url\", \"path\", \"base64\"]):\n+                    if idx_images < len(images):\n+                        # Insert the image passed as argument in the chat message\n+                        content[\"image\"] = images[idx_images]\n+                        idx_images += 1\n                     else:\n                         raise ValueError(\n-                            \"Wrong format for 'image_url' content type. The content should have an 'image_url' dict with a 'url' key.\"\n+                            \"The number of images in the chat messages should be the same as the number of images passed to the pipeline.\"\n                         )\n+            # Add support for OpenAI/TGI chat format\n+            elif content_type == \"image_url\":\n+                if isinstance(content.get(\"image_url\"), dict) and \"url\" in content[\"image_url\"]:\n+                    # Rewrite content to be in the Transformers chat format\n+                    content[\"type\"] = \"image\"\n+                    content[\"image\"] = content[\"image_url\"][\"url\"]\n+                    del content[\"image_url\"]\n+                else:\n+                    raise ValueError(\n+                        \"Wrong format for 'image_url' content type. The content should have an 'image_url' dict with a 'url' key.\"\n+                    )\n \n     # The number of images passed should be consistent with the number of images in the chat without an image key\n     if idx_images != len(images):\n         raise ValueError(\n             \"The number of images in the chat messages should be the same as the number of images passed to the pipeline.\"\n         )\n \n-    return retrieved_images\n+    return messages\n \n \n @add_end_docstrings(build_pipeline_init_args(has_processor=True))\n@@ -331,32 +327,30 @@ def __call__(\n         return super().__call__({\"images\": images, \"text\": text}, **kwargs)\n \n     def preprocess(self, inputs=None, timeout=None, continue_final_message=None, **processing_kwargs):\n+        if isinstance(inputs, Chat):\n+            # If the user passes a chat that ends in an assistant message, we treat it as a prefill by default\n+            # because very few models support multiple separate, consecutive assistant messages\n+            if continue_final_message is None:\n+                continue_final_message = inputs.messages[-1][\"role\"] == \"assistant\"\n+            model_inputs = self.processor.apply_chat_template(\n+                inputs.messages,\n+                add_generation_prompt=not continue_final_message,\n+                continue_final_message=continue_final_message,\n+                return_tensors=self.framework,\n+                tokenize=True,\n+                return_dict=True,\n+            )\n+            model_inputs[\"text\"] = inputs\n+            return model_inputs\n         # In case we only have text inputs\n         if isinstance(inputs, (list, tuple, str)):\n             images = None\n             text = inputs\n             inputs_text = inputs\n         else:\n-            if isinstance(inputs, Chat):\n-                # If the user passes a chat that ends in an assistant message, we treat it as a prefill by default\n-                # because very few models support multiple separate, consecutive assistant messages\n-                if continue_final_message is None:\n-                    continue_final_message = inputs.messages[-1][\"role\"] == \"assistant\"\n-                text = self.processor.apply_chat_template(\n-                    inputs.messages,\n-                    add_generation_prompt=not continue_final_message,\n-                    continue_final_message=continue_final_message,\n-                    return_tensors=self.framework,\n-                    **processing_kwargs,\n-                )\n-                inputs_text = inputs\n-                images = inputs.images\n-            else:\n-                text = inputs[\"text\"]\n-                inputs_text = inputs[\"text\"]\n-                images = inputs[\"images\"]\n-\n-            images = load_images(images, timeout=timeout)\n+            images = load_images(inputs[\"images\"], timeout=timeout)\n+            text = inputs[\"text\"]\n+            inputs_text = inputs[\"text\"]\n \n         # if batched text inputs, we set padding to True unless specified otherwise\n         if isinstance(text, (list, tuple)) and len(text) > 1:"
        },
        {
            "sha": "b32c6f608c7452f6906ba0bb1c661830e23e55be",
            "filename": "tests/pipelines/test_pipelines_image_text_to_text.py",
            "status": "modified",
            "additions": 101,
            "deletions": 4,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/5cd6b64059a2cd31aacf8557ed4aaf09e8b14e03/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5cd6b64059a2cd31aacf8557ed4aaf09e8b14e03/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py?ref=5cd6b64059a2cd31aacf8557ed4aaf09e8b14e03",
            "patch": "@@ -66,6 +66,78 @@ def run_pipeline_test(self, pipe, examples):\n             ],\n         )\n \n+    @require_torch\n+    def test_small_model_pt_token_text_only(self):\n+        pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-interleave-qwen-0.5b-hf\")\n+        text = \"What is the capital of France? Assistant:\"\n+\n+        outputs = pipe(text=text)\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\n+                    \"input_text\": \"What is the capital of France? Assistant:\",\n+                    \"generated_text\": \"What is the capital of France? Assistant: The capital of France is Paris.\",\n+                }\n+            ],\n+        )\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is the capital of France?\"},\n+                    ],\n+                },\n+            ],\n+        ]\n+        outputs = pipe(text=messages)\n+        self.assertEqual(\n+            outputs,\n+            [\n+                [\n+                    {\n+                        \"input_text\": [\n+                            {\n+                                \"role\": \"user\",\n+                                \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"}],\n+                            }\n+                        ],\n+                        \"generated_text\": [\n+                            {\n+                                \"role\": \"user\",\n+                                \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"}],\n+                            },\n+                            {\n+                                \"role\": \"assistant\",\n+                                \"content\": \"Hugging Face, a company of minds\\nWith tools and services that make our lives easier\\nFrom\",\n+                            },\n+                        ],\n+                    }\n+                ],\n+                [\n+                    {\n+                        \"input_text\": [\n+                            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is the capital of France?\"}]}\n+                        ],\n+                        \"generated_text\": [\n+                            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is the capital of France?\"}]},\n+                            {\"role\": \"assistant\", \"content\": \"Paris\"},\n+                        ],\n+                    }\n+                ],\n+            ],\n+        )\n+\n     @require_torch\n     def test_small_model_pt_token(self):\n         pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-interleave-qwen-0.5b-hf\")\n@@ -124,7 +196,7 @@ def test_model_pt_chat_template(self):\n                 ],\n             }\n         ]\n-        outputs = pipe([image_ny, image_chicago], text=messages, return_full_text=False, max_new_tokens=10)\n+        outputs = pipe([image_ny, image_chicago], text=messages, return_full_text=True, max_new_tokens=10)\n         self.assertEqual(\n             outputs,\n             [\n@@ -134,12 +206,37 @@ def test_model_pt_chat_template(self):\n                             \"role\": \"user\",\n                             \"content\": [\n                                 {\"type\": \"text\", \"text\": \"What’s the difference between these two images?\"},\n-                                {\"type\": \"image\"},\n-                                {\"type\": \"image\"},\n+                                {\n+                                    \"type\": \"image\",\n+                                    \"image\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n+                                },\n+                                {\n+                                    \"type\": \"image\",\n+                                    \"image\": \"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\",\n+                                },\n                             ],\n                         }\n                     ],\n-                    \"generated_text\": \"The first image shows a statue of Liberty in the\",\n+                    \"generated_text\": [\n+                        {\n+                            \"role\": \"user\",\n+                            \"content\": [\n+                                {\"type\": \"text\", \"text\": \"What’s the difference between these two images?\"},\n+                                {\n+                                    \"type\": \"image\",\n+                                    \"image\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n+                                },\n+                                {\n+                                    \"type\": \"image\",\n+                                    \"image\": \"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\",\n+                                },\n+                            ],\n+                        },\n+                        {\n+                            \"role\": \"assistant\",\n+                            \"content\": \"The first image shows a statue of Liberty in the\",\n+                        },\n+                    ],\n                 }\n             ],\n         )"
        }
    ],
    "stats": {
        "total": 240,
        "additions": 186,
        "deletions": 54
    }
}