{
    "author": "YangKai0616",
    "message": "[XPU] Fix fp8 UT patch func (#42584)\n\n* [XPU] Fix fp8 UT patch\n\n* make style\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "75c135d76f6db226d83213c6bb658a474dddb07d",
    "files": [
        {
            "sha": "04b001a53b39a137a079ce47704d28e972629dfb",
            "filename": "tests/quantization/finegrained_fp8/test_fp8.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/75c135d76f6db226d83213c6bb658a474dddb07d/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75c135d76f6db226d83213c6bb658a474dddb07d/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py?ref=75c135d76f6db226d83213c6bb658a474dddb07d",
            "patch": "@@ -46,6 +46,9 @@ def _patch_no_accelerator():\n         stack.enter_context(patch(\"torch.cuda.is_available\", return_value=False))\n         if hasattr(torch, \"xpu\"):\n             stack.enter_context(patch(\"torch.xpu.is_available\", return_value=False))\n+            stack.enter_context(\n+                patch(\"transformers.quantizers.quantizer_finegrained_fp8.is_torch_xpu_available\", return_value=False)\n+            )\n         yield\n \n "
        }
    ],
    "stats": {
        "total": 3,
        "additions": 3,
        "deletions": 0
    }
}