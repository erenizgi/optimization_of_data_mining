{
    "author": "liangel-02",
    "message": "[torchao safetensors] renaming get_state_dict function (#40774)\n\nrenaming get_state_dict function\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "f6999b00c33cef75d3aea9f94ae1270e5c2d86fd",
    "files": [
        {
            "sha": "33ecceacb17a573512bd2e1e482cc3d486e62b22",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6999b00c33cef75d3aea9f94ae1270e5c2d86fd/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6999b00c33cef75d3aea9f94ae1270e5c2d86fd/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=f6999b00c33cef75d3aea9f94ae1270e5c2d86fd",
            "patch": "@@ -4015,8 +4015,11 @@ def save_pretrained(\n             repo_id = self._create_repo(repo_id, **kwargs)\n             files_timestamps = self._get_files_timestamps(save_directory)\n \n+        metadata = {}\n         if hf_quantizer is not None:\n-            state_dict = hf_quantizer.get_state_dict(self)\n+            state_dict, metadata = hf_quantizer.get_state_dict_and_metadata(self, safe_serialization)\n+        metadata[\"format\"] = \"pt\"\n+\n         # Only save the model itself if we are using distributed training\n         model_to_save = unwrap_model(self)\n         # save the string version of dtype to the config, e.g. convert torch.float32 => \"float32\"\n@@ -4294,7 +4297,7 @@ def save_pretrained(\n             if safe_serialization:\n                 # At some point we will need to deal better with save_function (used for TPU and other distributed\n                 # joyfulness), but for now this enough.\n-                safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={\"format\": \"pt\"})\n+                safe_save_file(shard, os.path.join(save_directory, shard_file), metadata=metadata)\n             else:\n                 save_function(shard, os.path.join(save_directory, shard_file))\n "
        },
        {
            "sha": "323faa9c17e2df6f20039bc4fff1ebff1d93e72c",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6999b00c33cef75d3aea9f94ae1270e5c2d86fd/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6999b00c33cef75d3aea9f94ae1270e5c2d86fd/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=f6999b00c33cef75d3aea9f94ae1270e5c2d86fd",
            "patch": "@@ -338,9 +338,9 @@ def is_compileable(self) -> bool:\n         \"\"\"Flag indicating whether the quantized model can be compiled\"\"\"\n         return False\n \n-    def get_state_dict(self, model):\n-        \"\"\"Get state dict. Useful when we need to modify a bit the state dict due to quantization\"\"\"\n-        return None\n+    def get_state_dict_and_metadata(self, model, safe_serialization=False):\n+        \"\"\"Get state dict and metadata. Useful when we need to modify a bit the state dict due to quantization\"\"\"\n+        return None, {}\n \n     @abstractmethod\n     def _process_model_before_weight_loading(self, model, **kwargs): ..."
        },
        {
            "sha": "d0d370a11df6c170c53bd6918b500635522e1bb6",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6999b00c33cef75d3aea9f94ae1270e5c2d86fd/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6999b00c33cef75d3aea9f94ae1270e5c2d86fd/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=f6999b00c33cef75d3aea9f94ae1270e5c2d86fd",
            "patch": "@@ -379,7 +379,7 @@ def update_param_name(self, param_name: str) -> str:\n                 return param_name.replace(\"down_proj\", \"down_proj_blocks\")\n         return param_name\n \n-    def get_state_dict(self, model):\n+    def get_state_dict_and_metadata(self, model):\n         from ..integrations import Mxfp4GptOssExperts\n \n         state_dict = model.state_dict()\n@@ -411,7 +411,8 @@ def get_state_dict(self, model):\n                     ).transpose(-1, -2)\n                 )\n \n-        return state_dict\n+        metadata = {}\n+        return state_dict, metadata\n \n     def is_serializable(self, safe_serialization=None):\n         return True"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 11,
        "deletions": 7
    }
}