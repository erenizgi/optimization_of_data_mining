{
    "author": "hmellor",
    "message": "Use `getattr` in `standardize_rope_params` because `rope_parameters` not always present (#42593)\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>",
    "sha": "232ecf2cd82c7d3a0bcb13e181d3a10f87eb9fa7",
    "files": [
        {
            "sha": "90a9b1f346bc74b00b03d1c1b18aebc050e50785",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/232ecf2cd82c7d3a0bcb13e181d3a10f87eb9fa7/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/232ecf2cd82c7d3a0bcb13e181d3a10f87eb9fa7/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=232ecf2cd82c7d3a0bcb13e181d3a10f87eb9fa7",
            "patch": "@@ -657,7 +657,7 @@ def standardize_rope_params(self):\n         # Move `rope_theta` and `partial_rotary_factor` to the params dict, if not there yet\n         rope_theta = getattr(self, \"rope_theta\", None)\n         partial_rotary_factor = getattr(self, \"partial_rotary_factor\", None)\n-        rope_parameters = self.rope_parameters or {}\n+        rope_parameters = getattr(self, \"rope_parameters\", None) or {}\n \n         # Case 1: RoPE param keys do not intersect with possible `layer_types` -> one global dict\n         if getattr(self, \"layer_types\", None) is None or not set(rope_parameters.keys()).issubset(self.layer_types):"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}