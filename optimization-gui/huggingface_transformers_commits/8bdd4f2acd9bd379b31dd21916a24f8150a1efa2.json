{
    "author": "gante",
    "message": "[generate] skip compilation on cpu offload (#37709)\n\n* skip compilation on cpu offload\n\n* add test\n\n* better logic\n\n* docstring\n\n* boolean logic\n\n* add disk offload check\n\n* warn users if compilation options are set but compilation doesn happen\n\n* fix test\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "8bdd4f2acd9bd379b31dd21916a24f8150a1efa2",
    "files": [
        {
            "sha": "100b11fc7482f670595bb12bca78f16552e4e14e",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bdd4f2acd9bd379b31dd21916a24f8150a1efa2/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bdd4f2acd9bd379b31dd21916a24f8150a1efa2/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=8bdd4f2acd9bd379b31dd21916a24f8150a1efa2",
            "patch": "@@ -381,10 +381,12 @@ class GenerationConfig(PushToHubMixin):\n         > Parameters related to performances and compilation\n \n         compile_config (CompileConfig, *optional*):\n-            If using a static cache, this controls how `generate` will `compile` the forward pass for performance\n-            gains.\n-\n-        disable_compile (`bool`, *optional*): Whether to disable the automatic compilation of the forward pass. Automatic compilation happens when specific criteria are met, including using a compileable cache. Please open an issue if you find the need to use this flag.\n+            If using a compilable cache, this controls how `generate` will `compile` the forward pass for faster\n+            inference.\n+        disable_compile (`bool`, *optional*):\n+            Whether to disable the automatic compilation of the forward pass. Automatic compilation happens when\n+            specific criteria are met, including using a compileable cache. Please open an issue if you find the\n+            need to use this flag.\n \n         > Wild card\n \n@@ -489,7 +491,7 @@ def __init__(self, **kwargs):\n         self.target_lookbehind = kwargs.pop(\"target_lookbehind\", 10)\n \n         # Performance\n-        self.compile_config = kwargs.pop(\"compile_config\", CompileConfig())\n+        self.compile_config = kwargs.pop(\"compile_config\", None)\n         self.disable_compile = kwargs.pop(\"disable_compile\", False)\n         # Wild card\n         self.generation_kwargs = kwargs.pop(\"generation_kwargs\", {})\n@@ -811,9 +813,10 @@ def validate(self, is_init=False):\n             self.watermarking_config.validate()\n \n         # 7. performances arguments\n-        if not isinstance(self.compile_config, CompileConfig):\n+        if self.compile_config is not None and not isinstance(self.compile_config, CompileConfig):\n             raise ValueError(\n-                f\"You provided `compile_config` as an instance of {type(self.compile_config)}, but it must be an instance of `CompileConfig`.\"\n+                f\"You provided `compile_config` as an instance of {type(self.compile_config)}, but it must be an \"\n+                \"instance of `CompileConfig`.\"\n             )\n \n         # 8. other incorrect combinations"
        },
        {
            "sha": "6dc9acf8a09ca257a62dd6e254244dd5853e8d91",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 44,
            "deletions": 10,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bdd4f2acd9bd379b31dd21916a24f8150a1efa2/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bdd4f2acd9bd379b31dd21916a24f8150a1efa2/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=8bdd4f2acd9bd379b31dd21916a24f8150a1efa2",
            "patch": "@@ -2097,6 +2097,47 @@ def _tensor_or_none(token, device=None):\n         generation_config._pad_token_tensor = pad_token_tensor\n         generation_config._decoder_start_token_tensor = decoder_start_token_tensor\n \n+    def _valid_auto_compile_criteria(self, model_kwargs: Dict, generation_config: GenerationConfig) -> bool:\n+        \"\"\"\n+        Determines whether to trigger auto-compilation of the model's forward pass at generation time.\n+        \"\"\"\n+        # Override: honor `disable_compile` flag\n+        if generation_config.disable_compile:\n+            return False\n+\n+        # Base logic\n+        valid_hardware = self.device.type == \"cuda\" or bool(\n+            generation_config.compile_config is not None and generation_config.compile_config._compile_all_devices\n+        )\n+        using_compilable_cache = (\n+            isinstance(model_kwargs.get(\"past_key_values\"), Cache) and model_kwargs[\"past_key_values\"].is_compileable\n+        )\n+        can_compile = valid_hardware and using_compilable_cache and self._supports_static_cache\n+\n+        # Exception 1: Some quantization methods do not support compilation\n+        if getattr(self, \"hf_quantizer\", None) is not None:\n+            can_compile &= self.hf_quantizer.is_compileable\n+\n+        if hasattr(self, \"hf_device_map\"):\n+            all_model_devices = set(self.hf_device_map.values())\n+            # Exception 2: Don't compile if the model is using CPU offload (as of April 2025, this results in a crash)\n+            has_cpu_offload = \"cpu\" in all_model_devices and len(all_model_devices) > 1\n+            can_compile &= not has_cpu_offload\n+\n+            # Exception 3: Disk offload is not supported for compilation\n+            has_disk_offload = \"disk\" in all_model_devices\n+            can_compile &= not has_disk_offload\n+\n+        # Finally: if the user has manually specified compilation options, but compilation is not possible, let's warn\n+        # them\n+        if generation_config.compile_config is not None and not can_compile:\n+            logger.warning_once(\n+                \"You have set `compile_config`, but we are unable to meet the criteria for compilation. Compilation \"\n+                \"will be skipped.\"\n+            )\n+\n+        return can_compile\n+\n     @torch.no_grad()\n     def generate(\n         self,\n@@ -3389,16 +3430,9 @@ def _sample(\n         model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n \n         model_forward = self.__call__\n-        if isinstance(model_kwargs.get(\"past_key_values\"), Cache):\n-            is_compileable = model_kwargs[\"past_key_values\"].is_compileable and self._supports_static_cache\n-            if getattr(self, \"hf_quantizer\", None) is not None:\n-                is_compileable &= self.hf_quantizer.is_compileable\n-            is_compileable = is_compileable and not generation_config.disable_compile\n-            if is_compileable and (\n-                self.device.type == \"cuda\" or generation_config.compile_config._compile_all_devices\n-            ):\n-                os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n-                model_forward = self.get_compiled_call(generation_config.compile_config)\n+        if self._valid_auto_compile_criteria(model_kwargs, generation_config):\n+            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n+            model_forward = self.get_compiled_call(generation_config.compile_config)\n \n         if generation_config.prefill_chunk_size is not None:\n             model_kwargs = self._prefill_chunking(input_ids, generation_config, **model_kwargs)"
        },
        {
            "sha": "adac0890e6dbaedb32e6bb6cdf64c081e75f25b6",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bdd4f2acd9bd379b31dd21916a24f8150a1efa2/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bdd4f2acd9bd379b31dd21916a24f8150a1efa2/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=8bdd4f2acd9bd379b31dd21916a24f8150a1efa2",
            "patch": "@@ -5262,15 +5262,15 @@ def loss_function(self):\n     def loss_function(self, value):\n         self._loss_function = value\n \n-    def get_compiled_call(self, compile_config: CompileConfig):\n+    def get_compiled_call(self, compile_config: Optional[CompileConfig]) -> Callable:\n         \"\"\"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n         non-compiled/compiled `forward` during inference, especially to switch between prefill (where we don't\n         want to use compiled version to avoid recomputing the graph with new shapes) and iterative decoding\n         (where we want the speed-ups of compiled version with static shapes).\"\"\"\n         # Only reset it if not present or different from previous config\n         if \"llama4\" in self.config.model_type:  # TODO try to enable for FULL COMPILE HYBRID CACHE SUPPORT\n             return self.__call__\n-        default_config = getattr(self.generation_config, \"compile_config\", CompileConfig())\n+        default_config = getattr(self.generation_config, \"compile_config\", None) or CompileConfig()\n         if (\n             not hasattr(self, \"_compiled_call\")\n             or getattr(self, \"_last_compile_config\", default_config) != compile_config"
        },
        {
            "sha": "b9916e8dfa8b48f5071fa7878ecda8e1d1ab3179",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 35,
            "deletions": 2,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bdd4f2acd9bd379b31dd21916a24f8150a1efa2/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bdd4f2acd9bd379b31dd21916a24f8150a1efa2/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=8bdd4f2acd9bd379b31dd21916a24f8150a1efa2",
            "patch": "@@ -2245,13 +2245,15 @@ def test_generate_compilation_all_outputs(self):\n             # BLIP is the only exception with custom generate which call `self.lm.generate()`\n             # We should avoid such calls in all subsequent multimodal models and try to make `generate()`\n             # compatible with multimodality\n+            compile_config = CompileConfig()\n+            compile_config._compile_all_devices = True\n             if \"blip\" in model.__class__.__name__.lower():\n-                model.language_model.generation_config.compile_config._compile_all_devices = True\n+                model.language_model.generation_config.compile_config = compile_config\n                 if not has_defined_cache_implementation:\n                     model.language_model.generation_config.cache_implementation = \"static\"\n             else:\n                 # force compilation (e.g. fast CI, CPU)\n-                model.generation_config.compile_config._compile_all_devices = True\n+                model.generation_config.compile_config = compile_config\n                 if not has_defined_cache_implementation:\n                     model.generation_config.cache_implementation = \"static\"\n \n@@ -4907,6 +4909,37 @@ def test_cache_device_map_with_vision_layer_device_map(self):\n         # If the generate doesn't infer the DECODER device map correctly, this will fail\n         _ = model.generate(**inputs, max_new_tokens=2, do_sample=False)\n \n+    @require_torch_gpu\n+    def test_cpu_offload_doesnt_compile(self):\n+        \"\"\"Test that CPU offload doesn't trigger compilation\"\"\"\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n+        tokenized_inputs = tokenizer([\"Hello world\"], return_tensors=\"pt\")\n+        generate_kwargs = {\"max_new_tokens\": 3, \"cache_implementation\": \"static\"}\n+\n+        # Sanity check: if we don't specify a device map, the model will get compiled\n+        model_gpu = AutoModelForCausalLM.from_pretrained(\n+            \"hf-internal-testing/tiny-random-MistralForCausalLM\", device_map=\"auto\"\n+        )\n+        input_ids = tokenized_inputs.input_ids.to(model_gpu.device)\n+        _ = model_gpu.generate(input_ids, **generate_kwargs)\n+        self.assertTrue(hasattr(model_gpu, \"_compiled_call\"))\n+\n+        # If we specify a device map, the model will not be compiled\n+        # (as of April 2025, compiling with CPU offload results in a crash)\n+        device_map = {\n+            \"model.embed_tokens\": 0,\n+            \"model.layers.0\": 0,\n+            \"model.layers.1\": \"cpu\",\n+            \"model.norm\": \"cpu\",\n+            \"lm_head\": 0,\n+        }\n+        model_cpu = AutoModelForCausalLM.from_pretrained(\n+            \"hf-internal-testing/tiny-random-MistralForCausalLM\", device_map=device_map\n+        )\n+        input_ids = tokenized_inputs.input_ids.to(model_cpu.device)\n+        _ = model_cpu.generate(input_ids, **generate_kwargs)\n+        self.assertFalse(hasattr(model_cpu, \"_compiled_call\"))\n+\n \n @require_torch\n class TokenHealingTestCase(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 112,
        "additions": 91,
        "deletions": 21
    }
}