{
    "author": "kashif",
    "message": "[Continous Batching] set head_dim when config.head_dim is None (#40159)\n\n* set head_dim when config.head_dim is None\n\n* use model's actual TP setting",
    "sha": "cfe52ff4db1aea64a7faf3eaa1a00a854abe4a45",
    "files": [
        {
            "sha": "bfd965ea9dc6119edf8dd72ea9245e34b6479f79",
            "filename": "src/transformers/generation/continuous_batching.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cfe52ff4db1aea64a7faf3eaa1a00a854abe4a45/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cfe52ff4db1aea64a7faf3eaa1a00a854abe4a45/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py?ref=cfe52ff4db1aea64a7faf3eaa1a00a854abe4a45",
            "patch": "@@ -185,7 +185,9 @@ def __init__(\n             # self.num_key_value_heads //= tp_size\n \n         self.head_dim = (\n-            config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n+            config.head_dim\n+            if hasattr(config, \"head_dim\") and config.head_dim is not None\n+            else config.hidden_size // config.num_attention_heads\n         )\n         self.num_hidden_layers = config.num_hidden_layers\n \n@@ -1259,7 +1261,7 @@ def _run_generation_loop(self):\n                 self.model.device,\n                 self.model.dtype,\n                 num_requests=len(self.input_queue.queue),\n-                tp_size=getattr(self.model, \"_tp_size\", 8),  # TODO quantized converted don't set this\n+                tp_size=getattr(self.model, \"_tp_size\", None),  # Use model's actual TP setting\n             )\n \n             scheduler = None"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}