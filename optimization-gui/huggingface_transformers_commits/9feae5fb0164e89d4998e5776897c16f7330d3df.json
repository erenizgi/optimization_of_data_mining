{
    "author": "eustlb",
    "message": "[Whisper] patch float type on mps (#35295)\n\n* fix float type on mps\n\n* make",
    "sha": "9feae5fb0164e89d4998e5776897c16f7330d3df",
    "files": [
        {
            "sha": "fdaeff14d78867585aef155d73848d70fa31894e",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/9feae5fb0164e89d4998e5776897c16f7330d3df/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9feae5fb0164e89d4998e5776897c16f7330d3df/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=9feae5fb0164e89d4998e5776897c16f7330d3df",
            "patch": "@@ -632,7 +632,9 @@ def generate(\n                 cur_bsz=cur_bsz,\n                 batch_idx_map=batch_idx_map,\n             )\n-            time_offset = seek.to(torch.float64) * time_precision / input_stride\n+            time_offset = (\n+                seek.to(torch.float32 if device.type == \"mps\" else torch.float64) * time_precision / input_stride\n+            )\n             seek_num_frames = (max_frames - seek).clamp(max=num_segment_frames)\n \n             # 6.2 cut out next 30s segment from input features\n@@ -1805,6 +1807,7 @@ def _retrieve_segment(\n         timestamp_segment_indices = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0]\n         timestamp_segment_indices.add_(1)\n         token_timestamps = seek_outputs[idx][\"token_timestamps\"] if return_token_timestamps else []\n+        device = seek_sequence.device\n \n         # If whisper predicted a \"end of segment\" via a timestep token, let's go ever each\n         # \"end of segment\" prediction and slice the decoding into segments accordingly\n@@ -1828,8 +1831,12 @@ def _retrieve_segment(\n                 end_timestamp_pos = sliced_tokens[idx_sliced_tokens] - timestamp_begin\n                 segments.append(\n                     {\n-                        \"start\": time_offset[prev_idx] + start_timestamp_pos.to(torch.float64) * time_precision,\n-                        \"end\": time_offset[prev_idx] + end_timestamp_pos.to(torch.float64) * time_precision,\n+                        \"start\": time_offset[prev_idx]\n+                        + start_timestamp_pos.to(torch.float32 if device.type == \"mps\" else torch.float64)\n+                        * time_precision,\n+                        \"end\": time_offset[prev_idx]\n+                        + end_timestamp_pos.to(torch.float32 if device.type == \"mps\" else torch.float64)\n+                        * time_precision,\n                         \"tokens\": sliced_tokens,\n                         \"result\": seek_outputs[idx],\n                     }\n@@ -1856,7 +1863,9 @@ def _retrieve_segment(\n             last_timestamp_pos = int(seek_num_frames[prev_idx] * time_precision_features / time_precision)\n             if timestamps.numel() > 0 and timestamps[-1] != timestamp_begin:\n                 # no consecutive timestamps but it has a timestamp; use the last one.\n-                last_timestamp_pos = (timestamps[-1] - timestamp_begin).to(torch.float64)\n+                last_timestamp_pos = (timestamps[-1] - timestamp_begin).to(\n+                    torch.float32 if device.type == \"mps\" else torch.float64\n+                )\n             segments = [\n                 {\n                     \"start\": time_offset[prev_idx],"
        }
    ],
    "stats": {
        "total": 17,
        "additions": 13,
        "deletions": 4
    }
}