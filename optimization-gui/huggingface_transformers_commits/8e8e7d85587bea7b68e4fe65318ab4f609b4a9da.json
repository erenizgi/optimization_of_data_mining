{
    "author": "maciej-adamiak",
    "message": "fixed Mask2Former image processor segmentation maps handling (#33364)\n\n* fixed mask2former image processor segmentation maps handling\r\n\r\n* introduced review suggestions\r\n\r\n* introduced review suggestions",
    "sha": "8e8e7d85587bea7b68e4fe65318ab4f609b4a9da",
    "files": [
        {
            "sha": "28ad6002958eae3eec30850eb8006954df3bfd4b",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e8e7d85587bea7b68e4fe65318ab4f609b4a9da/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e8e7d85587bea7b68e4fe65318ab4f609b4a9da/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=8e8e7d85587bea7b68e4fe65318ab4f609b4a9da",
            "patch": "@@ -935,7 +935,7 @@ def encode_inputs(\n         if segmentation_maps is not None:\n             mask_labels = []\n             class_labels = []\n-            pad_size = get_max_height_width(pixel_values_list)\n+            pad_size = get_max_height_width(pixel_values_list, input_data_format=input_data_format)\n             # Convert to list of binary masks and labels\n             for idx, segmentation_map in enumerate(segmentation_maps):\n                 segmentation_map = to_numpy_array(segmentation_map)"
        },
        {
            "sha": "7468c3fd476a6e65be69b8d9076c226682f38b3c",
            "filename": "tests/models/mask2former/test_image_processing_mask2former.py",
            "status": "modified",
            "additions": 53,
            "deletions": 7,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e8e7d85587bea7b68e4fe65318ab4f609b4a9da/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e8e7d85587bea7b68e4fe65318ab4f609b4a9da/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py?ref=8e8e7d85587bea7b68e4fe65318ab4f609b4a9da",
            "patch": "@@ -20,6 +20,7 @@\n from datasets import load_dataset\n from huggingface_hub import hf_hub_download\n \n+from transformers.image_utils import ChannelDimension\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n@@ -180,31 +181,44 @@ def test_image_processor_from_dict_with_kwargs(self):\n         self.assertEqual(image_processor.size_divisor, 8)\n \n     def comm_get_image_processing_inputs(\n-        self, with_segmentation_maps=False, is_instance_map=False, segmentation_type=\"np\"\n+        self,\n+        image_processor_tester,\n+        with_segmentation_maps=False,\n+        is_instance_map=False,\n+        segmentation_type=\"np\",\n+        numpify=False,\n+        input_data_format=None,\n     ):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        image_processing = self.image_processing_class(**image_processor_tester.prepare_image_processor_dict())\n         # prepare image and target\n-        num_labels = self.image_processor_tester.num_labels\n+        num_labels = image_processor_tester.num_labels\n         annotations = None\n         instance_id_to_semantic_id = None\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+        image_inputs = image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=numpify)\n         if with_segmentation_maps:\n             high = num_labels\n             if is_instance_map:\n                 labels_expanded = list(range(num_labels)) * 2\n                 instance_id_to_semantic_id = dict(enumerate(labels_expanded))\n             annotations = [\n-                np.random.randint(0, high * 2, (img.size[1], img.size[0])).astype(np.uint8) for img in image_inputs\n+                np.random.randint(0, high * 2, img.shape[:2] if numpify else (img.size[1], img.size[0])).astype(\n+                    np.uint8\n+                )\n+                for img in image_inputs\n             ]\n             if segmentation_type == \"pil\":\n                 annotations = [Image.fromarray(annotation) for annotation in annotations]\n \n+        if input_data_format is ChannelDimension.FIRST and numpify:\n+            image_inputs = [np.moveaxis(img, -1, 0) for img in image_inputs]\n+\n         inputs = image_processing(\n             image_inputs,\n             annotations,\n             return_tensors=\"pt\",\n             instance_id_to_semantic_id=instance_id_to_semantic_id,\n             pad_and_return_pixel_mask=True,\n+            input_data_format=input_data_format,\n         )\n \n         return inputs\n@@ -223,9 +237,29 @@ def test_with_size_divisor(self):\n                 self.assertTrue((pixel_values.shape[-2] % size_divisor) == 0)\n \n     def test_call_with_segmentation_maps(self):\n-        def common(is_instance_map=False, segmentation_type=None):\n+        def common(\n+            is_instance_map=False,\n+            segmentation_type=None,\n+            numpify=False,\n+            num_channels=3,\n+            input_data_format=None,\n+            do_resize=True,\n+        ):\n+            image_processor_tester = Mask2FormerImageProcessingTester(\n+                self,\n+                num_channels=num_channels,\n+                do_resize=do_resize,\n+                image_mean=[0.5] * num_channels,\n+                image_std=[0.5] * num_channels,\n+            )\n+\n             inputs = self.comm_get_image_processing_inputs(\n-                with_segmentation_maps=True, is_instance_map=is_instance_map, segmentation_type=segmentation_type\n+                image_processor_tester=image_processor_tester,\n+                with_segmentation_maps=True,\n+                is_instance_map=is_instance_map,\n+                segmentation_type=segmentation_type,\n+                numpify=numpify,\n+                input_data_format=input_data_format,\n             )\n \n             mask_labels = inputs[\"mask_labels\"]\n@@ -243,6 +277,18 @@ def common(is_instance_map=False, segmentation_type=None):\n         common(is_instance_map=False, segmentation_type=\"pil\")\n         common(is_instance_map=True, segmentation_type=\"pil\")\n \n+        common(num_channels=1, numpify=True)\n+        common(num_channels=1, numpify=True, input_data_format=ChannelDimension.FIRST)\n+        common(num_channels=2, numpify=True, input_data_format=ChannelDimension.LAST)\n+        common(num_channels=5, numpify=True, input_data_format=ChannelDimension.LAST, do_resize=False)\n+        common(num_channels=5, numpify=True, input_data_format=ChannelDimension.FIRST, do_resize=False)\n+\n+        with self.assertRaisesRegex(ValueError, expected_regex=\"Unable to infer channel dimension format\"):\n+            common(num_channels=5, numpify=True, do_resize=False)\n+\n+        with self.assertRaisesRegex(TypeError, expected_regex=r\"Cannot handle this data type: .*\"):\n+            common(num_channels=5, numpify=True, input_data_format=ChannelDimension.LAST)\n+\n     def test_integration_instance_segmentation(self):\n         # load 2 images and corresponding annotations from the hub\n         repo_id = \"nielsr/image-segmentation-toy-data\""
        }
    ],
    "stats": {
        "total": 62,
        "additions": 54,
        "deletions": 8
    }
}