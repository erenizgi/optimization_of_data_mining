{
    "author": "faaany",
    "message": "[tests] make more tests device-agnostic (#33580)\n\n* enable\r\n\r\n* fix\r\n\r\n* add xpu skip\r\n\r\n* add marker\r\n\r\n* skip for xpu\r\n\r\n* add more\r\n\r\n* enable on accelerator\r\n\r\n* add more cases\r\n\r\n* add more tests\r\n\r\n* add more",
    "sha": "8bd1f2f33888ec8a86cecd6b9d448d8e26940b63",
    "files": [
        {
            "sha": "c6e9671dd59ae066fbfd4c547ab501b304abf9c1",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=8bd1f2f33888ec8a86cecd6b9d448d8e26940b63",
            "patch": "@@ -30,7 +30,7 @@\n from transformers.testing_utils import (\n     require_timm,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     require_vision,\n     slow,\n     torch_device,\n@@ -676,7 +676,7 @@ def test_inference_object_detection_head(self):\n         self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes, atol=1e-2))\n         self.assertListEqual(results[\"labels\"], expected_labels)\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n         processor = self.default_processor\n         image = prepare_img()\n@@ -690,8 +690,8 @@ def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n             cpu_outputs = model(**encoding)\n \n         # 2. run model on GPU\n-        model.to(\"cuda\")\n-        encoding = encoding.to(\"cuda\")\n+        model.to(torch_device)\n+        encoding = encoding.to(torch_device)\n         with torch.no_grad():\n             gpu_outputs = model(**encoding)\n "
        },
        {
            "sha": "a21665c822f2f97e74cc36de536c9e88cc0844c3",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=8bd1f2f33888ec8a86cecd6b9d448d8e26940b63",
            "patch": "@@ -24,10 +24,12 @@\n \n from transformers import AutoTokenizer, LlamaConfig, StaticCache, is_torch_available, set_seed\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_read_token,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     require_torch_sdpa,\n     slow,\n@@ -899,11 +901,11 @@ def test_compile_static_cache(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n class Mask4DTestHard(unittest.TestCase):\n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     def setUp(self):\n         model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
        },
        {
            "sha": "0730f8ba444140b01b9e61974692f9a90b411e5a",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=8bd1f2f33888ec8a86cecd6b9d448d8e26940b63",
            "patch": "@@ -29,6 +29,7 @@\n     require_flash_attn,\n     require_read_token,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     require_torch_sdpa,\n     slow,\n@@ -719,14 +720,14 @@ def test_compile_static_cache(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n class Mask4DTestHard(unittest.TestCase):\n     model_name = \"mistralai/Mistral-7B-v0.1\"\n     _model = None\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     @property\n     def model(self):"
        },
        {
            "sha": "23dace68cf21a9ae9634859bb928e5f46866ad0a",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=8bd1f2f33888ec8a86cecd6b9d448d8e26940b63",
            "patch": "@@ -21,7 +21,7 @@\n     require_bitsandbytes,\n     require_read_token,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -418,7 +418,7 @@ def test_inputs_embeds_matches_input_ids_with_generate(self):\n         pass\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n @slow\n class RecurrentGemmaIntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]"
        },
        {
            "sha": "f26a423a1a2f5b4218be749296561e6fe13ad129",
            "filename": "tests/models/univnet/test_modeling_univnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py?ref=8bd1f2f33888ec8a86cecd6b9d448d8e26940b63",
            "patch": "@@ -21,9 +21,10 @@\n \n from transformers import UnivNetConfig, UnivNetFeatureExtractor\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     is_torch_available,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -207,13 +208,13 @@ def test_unbatched_inputs_outputs(self):\n             self.assertTrue(outputs.shape[0] == 1, msg=\"Unbatched input should create batched output with bsz = 1\")\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n @slow\n class UnivNetModelIntegrationTests(unittest.TestCase):\n     def tearDown(self):\n         super().tearDown()\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     def _load_datasamples(self, num_samples, sampling_rate=24000):\n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
        },
        {
            "sha": "4bcf4252a60dc69f50fe86ac865c235fc1976c2d",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=8bd1f2f33888ec8a86cecd6b9d448d8e26940b63",
            "patch": "@@ -34,10 +34,12 @@\n     is_flaky,\n     is_pt_flax_cross_test,\n     require_flash_attn,\n+    require_non_xpu,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_fp16,\n     require_torch_gpu,\n-    require_torch_multi_gpu,\n+    require_torch_multi_accelerator,\n     require_torchaudio,\n     slow,\n     torch_device,\n@@ -2612,6 +2614,7 @@ def test_generate_with_prompt_ids_and_no_non_prompt_forced_decoder_ids(self):\n \n         self.assertTrue(prompt in text)\n \n+    @require_non_xpu\n     @slow\n     @require_torch_gpu\n     def test_speculative_decoding_distil(self):\n@@ -3239,7 +3242,7 @@ def test_whisper_longform_no_speech_detection(self):\n         for i in range(num_samples):\n             assert decoded_all[i] == EXPECTED_TEXT[i]\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_whisper_empty_longform(self):\n         processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n@@ -3278,7 +3281,7 @@ def test_whisper_empty_longform(self):\n         torch.manual_seed(0)\n         model.generate(**inputs, **gen_kwargs)\n \n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     @slow\n     def test_whisper_empty_longform_multi_gpu(self):\n         processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")"
        },
        {
            "sha": "d55399a951c9c538fc471e60a2b2af9f034079ba",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd1f2f33888ec8a86cecd6b9d448d8e26940b63/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=8bd1f2f33888ec8a86cecd6b9d448d8e26940b63",
            "patch": "@@ -4751,7 +4751,7 @@ def test_static_cache_matches_dynamic(self):\n \n     # For now, Let's focus only on GPU for `torch.compile`\n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_read_token\n     def test_torch_compile(self):\n         if version.parse(torch.__version__) < version.parse(\"2.3\"):"
        }
    ],
    "stats": {
        "total": 41,
        "additions": 24,
        "deletions": 17
    }
}