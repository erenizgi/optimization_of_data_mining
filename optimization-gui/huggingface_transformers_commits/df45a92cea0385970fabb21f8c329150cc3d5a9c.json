{
    "author": "yonigozlan",
    "message": "Enforce check_auto_docstring (#41635)\n\nfix issues and enforce check_auto_docstring",
    "sha": "df45a92cea0385970fabb21f8c329150cc3d5a9c",
    "files": [
        {
            "sha": "443bed268d55813fddee040947b19f14535d2217",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/df45a92cea0385970fabb21f8c329150cc3d5a9c/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/df45a92cea0385970fabb21f8c329150cc3d5a9c/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=df45a92cea0385970fabb21f8c329150cc3d5a9c",
            "patch": "@@ -22,7 +22,7 @@\n from ....cache_utils import Cache\n from ....modeling_outputs import MoECausalLMOutputWithPast, MoEModelOutputWithPastAndCrossAttentions\n from ....modeling_utils import PreTrainedModel\n-from ....utils import DUMMY_INPUTS, DUMMY_MASK, auto_docstring\n+from ....utils import DUMMY_INPUTS, DUMMY_MASK\n from .configuration_gptsan_japanese import GPTSanJapaneseConfig\n \n \n@@ -635,7 +635,6 @@ def __init__(self, config: GPTSanJapaneseConfig):\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n \n-    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "6ec22509b5d9b9454c17723a0ed5041e08d409eb",
            "filename": "src/transformers/models/nougat/tokenization_nougat_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/df45a92cea0385970fabb21f8c329150cc3d5a9c/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/df45a92cea0385970fabb21f8c329150cc3d5a9c/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py?ref=df45a92cea0385970fabb21f8c329150cc3d5a9c",
            "patch": "@@ -23,9 +23,7 @@\n \n import numpy as np\n \n-from transformers.tokenization_utils_base import INIT_TOKENIZER_DOCSTRING\n from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n-from transformers.utils import add_end_docstrings\n \n from ...utils import is_levenshtein_available, is_nltk_available, logging, requires_backends\n \n@@ -40,16 +38,6 @@\n logger = logging.get_logger(__name__)\n \n \n-INIT_TOKENIZER_DOCSTRING += \"\"\"\n-        tokenizer_object ([`tokenizers.Tokenizer`]):\n-            A [`tokenizers.Tokenizer`] object from ðŸ¤— tokenizers to instantiate from. See [Using tokenizers from ðŸ¤—\n-            tokenizers](../fast_tokenizers) for more information.\n-        tokenizer_file ([`str`]):\n-            A path to a local JSON file representing a previously serialized [`tokenizers.Tokenizer`] object from ðŸ¤—\n-            tokenizers.\n-\"\"\"\n-\n-\n VOCAB_FILES_NAMES = {\"tokenizer_file\": \"tokenizer.json\"}\n \n \n@@ -358,7 +346,6 @@ def remove_slice_from_lines(lines, clean_text, slice) -> str:\n     return to_delete.strip()\n \n \n-@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)\n class NougatTokenizerFast(PreTrainedTokenizerFast):\n     \"\"\"\n     Fast tokenizer for Nougat (backed by HuggingFace tokenizers library)."
        },
        {
            "sha": "72a2f245cf1999b44da58e835596ca02fe9858a0",
            "filename": "src/transformers/utils/auto_docstring.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/df45a92cea0385970fabb21f8c329150cc3d5a9c/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/df45a92cea0385970fabb21f8c329150cc3d5a9c/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fauto_docstring.py?ref=df45a92cea0385970fabb21f8c329150cc3d5a9c",
            "patch": "@@ -1729,9 +1729,9 @@ def auto_method_docstring(\n     model_name_lowercase, class_name, config_class = _get_model_info(func, parent_class)\n     func_documentation = func.__doc__\n     if custom_args is not None and func_documentation is not None:\n-        func_documentation = set_min_indent(custom_args, indent_level + 4) + \"\\n\" + func_documentation\n+        func_documentation = \"\\n\" + set_min_indent(custom_args.strip(\"\\n\"), 0) + \"\\n\" + func_documentation\n     elif custom_args is not None:\n-        func_documentation = custom_args\n+        func_documentation = \"\\n\" + set_min_indent(custom_args.strip(\"\\n\"), 0)\n \n     # Add intro to the docstring before args description if needed\n     if custom_intro is not None:"
        },
        {
            "sha": "1e57ee66479ae58ab583ed55250fecb68f7d9db9",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/df45a92cea0385970fabb21f8c329150cc3d5a9c/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/df45a92cea0385970fabb21f8c329150cc3d5a9c/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=df45a92cea0385970fabb21f8c329150cc3d5a9c",
            "patch": "@@ -1378,6 +1378,10 @@ def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n             print(f\"[ERROR] Docstring needs to be filled for the following arguments in {candidate_file}:\")\n             for warning in fill_docstring_args_warnings:\n                 print(warning)\n+        if missing_docstring_args_warnings or docstring_args_ro_remove_warnings or fill_docstring_args_warnings:\n+            raise ValueError(\n+                \"There was at least one problem when checking docstrings of objects decorated with @auto_docstring.\"\n+            )\n \n \n def check_docstrings(overwrite: bool = False, check_all: bool = False):"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 7,
        "deletions": 17
    }
}