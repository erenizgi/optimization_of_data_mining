{
    "author": "rahul-tuli",
    "message": "Improve model loading for compressed tensor models (#36152)\n\n* Disable warnings for stacked compressors\n* Introduce two new hooks in HfQuantizer lifecycle\nto allow updates to missing and unexpected keys\n* Update missing and unexpected keys\nfor stacked compressors\n* Add tests\n* Fix: run_compressed cases\n* Fix: uncompressed cases\n\n* Rename compressed_tensor folder to compressed_tensors\nMove RunCompressedTest to the same file\nUpdate tests to unittest",
    "sha": "884a8ea1f058716c24adb52a0a6a0bf41fbb973d",
    "files": [
        {
            "sha": "e3b2209f026a090fc3f253974e6ca433eeb5f35b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/884a8ea1f058716c24adb52a0a6a0bf41fbb973d/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/884a8ea1f058716c24adb52a0a6a0bf41fbb973d/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=884a8ea1f058716c24adb52a0a6a0bf41fbb973d",
            "patch": "@@ -4673,6 +4673,7 @@ def _load_pretrained_model(\n                 unexpected_keys = [k for k in unexpected_keys if re.search(pat, k) is None]\n         if hf_quantizer is not None:\n             missing_keys = hf_quantizer.update_missing_keys(model, missing_keys, prefix)\n+            unexpected_keys = hf_quantizer.update_unexpected_keys(model, unexpected_keys, prefix)\n \n         # retrieve weights on meta device and put them back on CPU.\n         # This is not ideal in terms of memory, but if we don't do that not, we can't initialize them in the next step\n@@ -4993,6 +4994,9 @@ def _find_mismatched_keys(\n                 load_offloaded_weights(model_to_load, state_dict_index, state_dict_folder)\n                 shutil.rmtree(state_dict_folder)\n \n+        if hf_quantizer is not None:\n+            missing_keys = hf_quantizer.update_missing_keys_after_loading(model_to_load, missing_keys, prefix)\n+\n         if len(error_msgs) > 0:\n             error_msg = \"\\n\\t\".join(error_msgs)\n             if \"size mismatch\" in error_msg:"
        },
        {
            "sha": "a1096c110df53ced14dbdc57cce09e1be489ffc6",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/884a8ea1f058716c24adb52a0a6a0bf41fbb973d/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/884a8ea1f058716c24adb52a0a6a0bf41fbb973d/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=884a8ea1f058716c24adb52a0a6a0bf41fbb973d",
            "patch": "@@ -109,6 +109,27 @@ def update_missing_keys(self, model, missing_keys: List[str], prefix: str) -> Li\n         \"\"\"\n         return missing_keys\n \n+    def update_unexpected_keys(self, model, unexpected_keys: List[str], prefix: str) -> List[str]:\n+        \"\"\"\n+        Override this method if you want to adjust the `unexpected_keys`.\n+\n+        Args:\n+            unexpected_keys (`List[str]`, *optional*):\n+                The list of unexpected keys in the checkpoint compared to the state dict of the model\n+        \"\"\"\n+        return unexpected_keys\n+\n+    def update_missing_keys_after_loading(self, model, missing_keys: List[str], prefix: str) -> List[str]:\n+        \"\"\"\n+        Override this method if you want to adjust the `missing_keys` after loading the model params,\n+        but before the model is post-processed.\n+\n+        Args:\n+            missing_keys (`List[str]`, *optional*):\n+                The list of missing keys in the checkpoint compared to the state dict of the model\n+        \"\"\"\n+        return missing_keys\n+\n     def update_expected_keys(self, model, expected_keys: List[str], loaded_keys: List[str]) -> List[str]:\n         \"\"\"\n         Override this method if you want to adjust the `update_expected_keys`."
        },
        {
            "sha": "3e65b103d5357a86b13b57a6c145e529c07925a6",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 51,
            "deletions": 2,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/884a8ea1f058716c24adb52a0a6a0bf41fbb973d/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/884a8ea1f058716c24adb52a0a6a0bf41fbb973d/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=884a8ea1f058716c24adb52a0a6a0bf41fbb973d",
            "patch": "@@ -14,6 +14,8 @@\n \n \n import os\n+import re\n+from typing import List\n \n from ..utils import is_compressed_tensors_available, is_torch_available, logging\n from ..utils.quantization_config import CompressedTensorsConfig\n@@ -50,6 +52,45 @@ def __init__(self, quantization_config: CompressedTensorsConfig, **kwargs):\n         self.run_compressed = quantization_config.run_compressed\n         self.quantization_config = quantization_config\n \n+    def update_missing_keys_after_loading(self, model, missing_keys: List[str], prefix: str) -> List[str]:\n+        \"\"\"\n+        Update missing keys after loading the model. This is necessary for compressed tensors\n+        to load the model correctly. We expect weights to be present in missing keys.\n+        The weight's are re-constructed by ModelCompressor in _process_model_after_weight_loading\n+\n+        This function cleans up expected missing keys and returns the remaining missing keys\n+        \"\"\"\n+\n+        if self.run_compressed:\n+            return missing_keys\n+\n+        # We expect some keys to be missing for\n+        # compresed models\n+        # This is fine as the weights are reconstructed by ModelCompressor\n+        # in _process_model_after_weight_loading\n+\n+        expected_missing_keys = self.compressor.get_missing_module_keys(model)\n+        return [\n+            key for key in missing_keys if not any(re.match(f\".*{pattern}\", key) for pattern in expected_missing_keys)\n+        ]\n+\n+    def update_unexpected_keys(self, model, unexpected_keys: List[str], prefix: str) -> List[str]:\n+        \"\"\"\n+        Override this method if you want to adjust the `unexpected_keys`.\n+\n+        Args:\n+            unexpected_keys (`List[str]`, *optional*):\n+                The list of unexpected keys in the checkpoint compared to the state dict of the model\n+        \"\"\"\n+\n+        if self.run_compressed:\n+            return unexpected_keys\n+\n+        # We expect some unexpected keys in model\n+        # safetensors file for compressed models\n+        keys_to_ignore = self.compressor.get_unexpected_file_keys(model)\n+        return [key for key in unexpected_keys if not any(re.match(f\".*{pattern}\", key) for pattern in keys_to_ignore)]\n+\n     def validate_environment(self, *args, **kwargs):\n         if not is_compressed_tensors_available():\n             raise ImportError(\n@@ -75,9 +116,11 @@ def _process_model_before_weight_loading(self, model, **kwargs):\n \n         ct_quantization_config = self.compressor.quantization_config\n \n-        if self.run_compressed and self.is_quantization_compressed:\n+        if self.run_compressed:\n+            if not self.is_quantization_compressed:\n+                raise ValueError(\"`run_compressed` is only supported for quantized_compressed models\")\n             apply_quantization_config(model, ct_quantization_config, run_compressed=True)\n-        elif not self.is_quantization_compressed:\n+        elif self.is_quantized and not self.is_quantization_compressed:\n             apply_quantization_config(model, ct_quantization_config)\n \n     def _process_model_after_weight_loading(self, model, **kwargs):\n@@ -99,6 +142,12 @@ def _process_model_after_weight_loading(self, model, **kwargs):\n                 self.compressor.quantization_config.quantization_status = QuantizationStatus.FROZEN\n             self.compressor.decompress(model_path=cache_path, model=model)\n \n+    @property\n+    def is_quantized(self):\n+        return self.quantization_config.quantization_config is not None and bool(\n+            self.quantization_config.quantization_config.config_groups\n+        )\n+\n     @property\n     def is_quantization_compressed(self):\n         from compressed_tensors.quantization import QuantizationStatus"
        },
        {
            "sha": "8992cd3d9bd470b3b1bd9209ceaa440d5155e65b",
            "filename": "tests/quantization/compressed_tensor/test_load_sparse_model.py",
            "status": "removed",
            "additions": 0,
            "deletions": 80,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/4dbf17c17f5834eb68f296457acc605a8c533b5a/tests%2Fquantization%2Fcompressed_tensor%2Ftest_load_sparse_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4dbf17c17f5834eb68f296457acc605a8c533b5a/tests%2Fquantization%2Fcompressed_tensor%2Ftest_load_sparse_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensor%2Ftest_load_sparse_model.py?ref=4dbf17c17f5834eb68f296457acc605a8c533b5a",
            "patch": "@@ -1,80 +0,0 @@\n-import gc\n-import unittest\n-\n-from transformers import AutoModelForCausalLM\n-from transformers.testing_utils import require_compressed_tensors, require_torch\n-from transformers.utils import is_torch_available\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-\n-@require_compressed_tensors\n-@require_torch\n-class CompressedTensorsTest(unittest.TestCase):\n-    model_sparse_uncompressed = \"horheynm/llama2.c_stories15M_pruned_50.2of4_uncompressed\"\n-    model_sparse_compressed = \"horheynm/llama2.c_stories15M_pruned_50.2of4_compressed\"\n-\n-    prompt = \"Paris is the capital of which country?\"\n-\n-    stubs = [model_sparse_uncompressed, model_sparse_compressed]\n-\n-    def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n-        gc.collect()\n-\n-    def test_compressed_uncompressed_model_shapes(self):\n-        \"\"\"\n-        Check that the weights are the same between\n-         uncompressed and compressed-decompressed model\n-        Sparse compressed modules' weights are \"packed\" and shape/value will\n-         differ\n-        \"\"\"\n-\n-        def _has_nested_attr(obj, attr_path):\n-            attrs = attr_path.split(\".\")\n-            for attr in attrs:\n-                if not hasattr(obj, attr):\n-                    return None\n-                obj = getattr(obj, attr)\n-            return obj\n-\n-        from compressed_tensors.quantization.utils import iter_named_leaf_modules\n-\n-        uncompressed_model = AutoModelForCausalLM.from_pretrained(\n-            self.model_sparse_uncompressed,\n-        )\n-\n-        compressed_model_decompressed = AutoModelForCausalLM.from_pretrained(\n-            self.model_sparse_compressed,\n-        )\n-\n-        for name, submodule in iter_named_leaf_modules(\n-            uncompressed_model,\n-        ):\n-            if comp_decomp_obj := _has_nested_attr(compressed_model_decompressed, name):\n-                if hasattr(submodule, \"weight\"):\n-                    assert torch.equal(submodule.weight, comp_decomp_obj.weight)\n-\n-    def test_run_compressed_outputs_match(self):\n-        \"\"\"Check that uncompressed and compressed-decompressed model outputs are the same\"\"\"\n-\n-        from transformers import AutoTokenizer\n-\n-        for stub in self.stubs:\n-            tokenizer = AutoTokenizer.from_pretrained(stub)\n-            input_ids = tokenizer(self.prompt, return_tensors=\"pt\").input_ids\n-\n-            uncompressed_model = AutoModelForCausalLM.from_pretrained(\n-                self.model_sparse_uncompressed,\n-            )\n-            output_rc_true = uncompressed_model.generate(input_ids, max_new_tokens=100)\n-\n-            compressed_model_decompressed = AutoModelForCausalLM.from_pretrained(\n-                self.model_sparse_compressed,\n-            )\n-            output_rc_false = compressed_model_decompressed.generate(input_ids, max_new_tokens=100)\n-\n-            assert tokenizer.decode(output_rc_true[0]) == tokenizer.decode(output_rc_false[0])"
        },
        {
            "sha": "b168ca382ccefa194a1f264390649d1b8f3e87b2",
            "filename": "tests/quantization/compressed_tensor/test_run_compressed_model.py",
            "status": "removed",
            "additions": 0,
            "deletions": 94,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/4dbf17c17f5834eb68f296457acc605a8c533b5a/tests%2Fquantization%2Fcompressed_tensor%2Ftest_run_compressed_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4dbf17c17f5834eb68f296457acc605a8c533b5a/tests%2Fquantization%2Fcompressed_tensor%2Ftest_run_compressed_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensor%2Ftest_run_compressed_model.py?ref=4dbf17c17f5834eb68f296457acc605a8c533b5a",
            "patch": "@@ -1,94 +0,0 @@\n-import gc\n-import unittest\n-\n-from transformers import AutoModelForCausalLM\n-from transformers.testing_utils import require_compressed_tensors, require_torch\n-from transformers.utils import is_torch_available\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-\n-@require_compressed_tensors\n-@require_torch\n-class CompressedTensorsTest(unittest.TestCase):\n-    tinyllama_w4a16 = \"nm-testing/tinyllama-w4a16-compressed-hf-quantizer\"\n-    tinyllama_w8a8 = \"nm-testing/tinyllama-w8a8-compressed-hf-quantizer\"\n-\n-    prompt = \"Paris is the capital of which country?\"\n-\n-    stubs = [tinyllama_w4a16, tinyllama_w8a8]\n-\n-    def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n-        gc.collect()\n-\n-    def test_default_run_compressed__True(self):\n-        from compressed_tensors.linear.compressed_linear import CompressedLinear\n-        from compressed_tensors.quantization.utils import iter_named_leaf_modules\n-\n-        for stub in self.stubs:\n-            model = AutoModelForCausalLM.from_pretrained(\n-                stub,\n-            )\n-            compressed_linear_counts = 0\n-\n-            for _, submodule in iter_named_leaf_modules(\n-                model,\n-            ):\n-                if isinstance(submodule, CompressedLinear):\n-                    compressed_linear_counts += 1\n-\n-            # some linear models are not compressed - ex. lm_head\n-            assert compressed_linear_counts > 0\n-\n-    def test_default_run_compressed__False(self):\n-        from compressed_tensors.linear.compressed_linear import CompressedLinear\n-        from compressed_tensors.quantization.utils import iter_named_leaf_modules\n-\n-        from transformers.utils.quantization_config import CompressedTensorsConfig\n-\n-        quantization_config = CompressedTensorsConfig(run_compressed=False)\n-\n-        for stub in self.stubs:\n-            model = AutoModelForCausalLM.from_pretrained(\n-                stub,\n-                quantization_config=quantization_config,\n-            )\n-            compressed_linear_counts = 0\n-\n-            for _, submodule in iter_named_leaf_modules(\n-                model,\n-            ):\n-                if isinstance(submodule, CompressedLinear):\n-                    compressed_linear_counts += 1\n-\n-            # No modules should be CompressedLinear\n-            assert compressed_linear_counts == 0\n-\n-    def test_run_compressed_outputs_match(self):\n-        \"\"\"Check that run_compressed=True/False output are the same\"\"\"\n-\n-        from transformers import AutoTokenizer\n-        from transformers.utils.quantization_config import CompressedTensorsConfig\n-\n-        quantization_config = CompressedTensorsConfig(run_compressed=False)\n-\n-        for stub in self.stubs:\n-            tokenizer = AutoTokenizer.from_pretrained(stub)\n-            input_ids = tokenizer(self.prompt, return_tensors=\"pt\").input_ids\n-\n-            model_run_compressed__True = AutoModelForCausalLM.from_pretrained(\n-                stub,\n-            )\n-            output_rc_true = model_run_compressed__True.generate(input_ids, max_new_tokens=100)\n-\n-            model_run_compressed__False = AutoModelForCausalLM.from_pretrained(\n-                stub,\n-                quantization_config=quantization_config,\n-            )\n-            output_rc_false = model_run_compressed__False.generate(input_ids, max_new_tokens=100)\n-\n-            assert tokenizer.decode(output_rc_true[0]) == tokenizer.decode(output_rc_false[0])"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/quantization/compressed_tensors/__init__.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/884a8ea1f058716c24adb52a0a6a0bf41fbb973d/tests%2Fquantization%2Fcompressed_tensors%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/884a8ea1f058716c24adb52a0a6a0bf41fbb973d/tests%2Fquantization%2Fcompressed_tensors%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensors%2F__init__.py?ref=884a8ea1f058716c24adb52a0a6a0bf41fbb973d",
            "previous_filename": "tests/quantization/compressed_tensor/__init__.py"
        },
        {
            "sha": "bc64f77ce9c24c591f497b4b86620289a2a76fa3",
            "filename": "tests/quantization/compressed_tensors/test_compressed_models.py",
            "status": "added",
            "additions": 231,
            "deletions": 0,
            "changes": 231,
            "blob_url": "https://github.com/huggingface/transformers/blob/884a8ea1f058716c24adb52a0a6a0bf41fbb973d/tests%2Fquantization%2Fcompressed_tensors%2Ftest_compressed_models.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/884a8ea1f058716c24adb52a0a6a0bf41fbb973d/tests%2Fquantization%2Fcompressed_tensors%2Ftest_compressed_models.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensors%2Ftest_compressed_models.py?ref=884a8ea1f058716c24adb52a0a6a0bf41fbb973d",
            "patch": "@@ -0,0 +1,231 @@\n+import gc\n+import unittest\n+import warnings\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers.testing_utils import require_compressed_tensors, require_torch\n+from transformers.utils import is_torch_available\n+from transformers.utils.quantization_config import CompressedTensorsConfig\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+@require_compressed_tensors\n+@require_torch\n+class StackCompressedModelTest(unittest.TestCase):\n+    # Define stubs as class attributes\n+    compressed_uncompressed_model_stubs = [\n+        (\n+            \"nm-testing/llama2.c-stories42M-gsm8k-quantized-only-compressed\",\n+            \"nm-testing/llama2.c-stories42M-gsm8k-quantized-only-uncompressed\",\n+        ),\n+        (\n+            \"nm-testing/llama2.c-stories42M-gsm8k-sparse-only-compressed\",\n+            \"nm-testing/llama2.c-stories42M-gsm8k-sparse-only-uncompressed\",\n+        ),\n+        (\n+            \"nm-testing/llama2.c-stories42M-gsm8k-stacked-compressed\",\n+            \"nm-testing/llama2.c-stories42M-gsm8k-stacked-uncompressed\",\n+        ),\n+    ]\n+    # Flatten the list for tests that require a single list of stubs.\n+    model_stubs = [stub for pair in compressed_uncompressed_model_stubs for stub in pair]\n+\n+    # For the outputs matching test, use the sparse-only pair.\n+    sparse_compressed_model = \"nm-testing/llama2.c-stories42M-gsm8k-sparse-only-compressed\"\n+    sparse_uncompressed_model = \"nm-testing/llama2.c-stories42M-gsm8k-sparse-only-uncompressed\"\n+\n+    prompt = \"Paris is the capital of which country?\"\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+        gc.collect()\n+\n+    def test_compressed_uncompressed_model_shapes(self):\n+        \"\"\"\n+        Verify that the weights of an uncompressed model and its decompressed compressed counterpart match.\n+        Note: Weights for sparsely compressed models may differ due to packing.\n+        \"\"\"\n+\n+        def _has_nested_attr(obj, attr_path):\n+            attrs = attr_path.split(\".\")\n+            for attr in attrs:\n+                if not hasattr(obj, attr):\n+                    return None\n+                obj = getattr(obj, attr)\n+            return obj\n+\n+        from compressed_tensors.quantization.utils import iter_named_leaf_modules\n+\n+        for compressed_model, uncompressed_model in self.compressed_uncompressed_model_stubs:\n+            with self.subTest(compressed_model=compressed_model, uncompressed_model=uncompressed_model):\n+                uncompressed = AutoModelForCausalLM.from_pretrained(\n+                    uncompressed_model,\n+                    device_map=\"auto\",\n+                    torch_dtype=\"auto\",\n+                    quantization_config=CompressedTensorsConfig(run_compressed=False),\n+                )\n+                compressed_decompressed = AutoModelForCausalLM.from_pretrained(\n+                    compressed_model,\n+                    device_map=\"auto\",\n+                    torch_dtype=\"auto\",\n+                    quantization_config=CompressedTensorsConfig(run_compressed=False),\n+                )\n+\n+                for name, submodule in iter_named_leaf_modules(uncompressed):\n+                    comp_decomp_obj = _has_nested_attr(compressed_decompressed, name)\n+                    if comp_decomp_obj is not None and hasattr(submodule, \"weight\"):\n+                        if \"sparse-only\" in uncompressed_model:\n+                            self.assertTrue(\n+                                torch.equal(submodule.weight, comp_decomp_obj.weight),\n+                                f\"Weight mismatch for module '{name}' in sparse-only model.\",\n+                            )\n+                        else:\n+                            self.assertTrue(\n+                                torch.allclose(submodule.weight, comp_decomp_obj.weight, atol=0.2),\n+                                f\"Weight mismatch for module '{name}' in quantized-only or stacked model.\",\n+                            )\n+\n+    def test_outputs_match(self):\n+        \"\"\"\n+        Ensure that the generated outputs match between the uncompressed model\n+        and its decompressed compressed counterpart.\n+        \"\"\"\n+        tokenizer = AutoTokenizer.from_pretrained(self.sparse_uncompressed_model)\n+        input_ids = tokenizer(self.prompt, return_tensors=\"pt\").input_ids\n+\n+        uncompressed = AutoModelForCausalLM.from_pretrained(\n+            self.sparse_uncompressed_model,\n+            device_map=\"auto\",\n+            torch_dtype=\"auto\",\n+            quantization_config=CompressedTensorsConfig(run_compressed=False),\n+        )\n+\n+        output_uncompressed = uncompressed.generate(input_ids.to(uncompressed.device), max_new_tokens=100)\n+\n+        decompressed = AutoModelForCausalLM.from_pretrained(\n+            self.sparse_compressed_model,\n+            device_map=\"auto\",\n+            torch_dtype=\"auto\",\n+            quantization_config=CompressedTensorsConfig(run_compressed=False),\n+        )\n+        output_decompressed = decompressed.generate(input_ids.to(decompressed.device), max_new_tokens=100)\n+\n+        self.assertEqual(\n+            tokenizer.decode(output_uncompressed[0]),\n+            tokenizer.decode(output_decompressed[0]),\n+            \"Generated outputs do not match between compressed and uncompressed models.\",\n+        )\n+\n+    def test_no_warnings_for_all_models(self):\n+        \"\"\"\n+        Confirm that loading any model using compressed tensors does not trigger\n+        warnings about missing or unexpected keys.\n+        \"\"\"\n+        for model_stub in self.model_stubs:\n+            with self.subTest(model_stub=model_stub):\n+                with warnings.catch_warnings(record=True) as caught_warnings:\n+                    warnings.simplefilter(\"always\")\n+                    AutoModelForCausalLM.from_pretrained(\n+                        model_stub,\n+                        device_map=\"auto\",\n+                        torch_dtype=\"auto\",\n+                        quantization_config=CompressedTensorsConfig(run_compressed=False),\n+                    )\n+                    for warning in caught_warnings:\n+                        self.assertNotIn(\n+                            \"missing keys\",\n+                            str(warning.message).lower(),\n+                            f\"'missing keys' found in warnings for model {model_stub}\",\n+                        )\n+                        self.assertNotIn(\n+                            \"unexpected keys\",\n+                            str(warning.message).lower(),\n+                            f\"'unexpected keys' found in warnings for model {model_stub}\",\n+                        )\n+\n+\n+@require_compressed_tensors\n+@require_torch\n+class RunCompressedTest(unittest.TestCase):\n+    tinyllama_w4a16 = \"nm-testing/tinyllama-w4a16-compressed-hf-quantizer\"\n+    tinyllama_w8a8 = \"nm-testing/tinyllama-w8a8-compressed-hf-quantizer\"\n+\n+    prompt = \"Paris is the capital of which country?\"\n+\n+    stubs = [tinyllama_w4a16, tinyllama_w8a8]\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+        gc.collect()\n+\n+    def test_default_run_compressed__True(self):\n+        from compressed_tensors.linear.compressed_linear import CompressedLinear\n+        from compressed_tensors.quantization.utils import iter_named_leaf_modules\n+\n+        for stub in self.stubs:\n+            model = AutoModelForCausalLM.from_pretrained(\n+                stub,\n+            )\n+            compressed_linear_counts = 0\n+\n+            for _, submodule in iter_named_leaf_modules(\n+                model,\n+            ):\n+                if isinstance(submodule, CompressedLinear):\n+                    compressed_linear_counts += 1\n+\n+            # some linear models are not compressed - ex. lm_head\n+            assert compressed_linear_counts > 0\n+\n+    def test_default_run_compressed__False(self):\n+        from compressed_tensors.linear.compressed_linear import CompressedLinear\n+        from compressed_tensors.quantization.utils import iter_named_leaf_modules\n+        from transformers.utils.quantization_config import CompressedTensorsConfig\n+\n+        quantization_config = CompressedTensorsConfig(run_compressed=False)\n+\n+        for stub in self.stubs:\n+            model = AutoModelForCausalLM.from_pretrained(\n+                stub,\n+                quantization_config=quantization_config,\n+            )\n+            compressed_linear_counts = 0\n+\n+            for _, submodule in iter_named_leaf_modules(\n+                model,\n+            ):\n+                if isinstance(submodule, CompressedLinear):\n+                    compressed_linear_counts += 1\n+\n+            # No modules should be CompressedLinear\n+            assert compressed_linear_counts == 0\n+\n+    def test_run_compressed_outputs_match(self):\n+        \"\"\"Check that run_compressed=True/False output are the same\"\"\"\n+\n+        from transformers import AutoTokenizer\n+        from transformers.utils.quantization_config import CompressedTensorsConfig\n+\n+        quantization_config = CompressedTensorsConfig(run_compressed=False)\n+\n+        for stub in self.stubs:\n+            tokenizer = AutoTokenizer.from_pretrained(stub)\n+            input_ids = tokenizer(self.prompt, return_tensors=\"pt\").input_ids\n+\n+            model_run_compressed__True = AutoModelForCausalLM.from_pretrained(\n+                stub,\n+            )\n+            output_rc_true = model_run_compressed__True.generate(input_ids, max_new_tokens=100)\n+\n+            model_run_compressed__False = AutoModelForCausalLM.from_pretrained(\n+                stub,\n+                quantization_config=quantization_config,\n+            )\n+            output_rc_false = model_run_compressed__False.generate(input_ids, max_new_tokens=100)\n+\n+            assert tokenizer.decode(output_rc_true[0]) == tokenizer.decode(output_rc_false[0])"
        },
        {
            "sha": "cbcf492f7c97533a6b6ed5294f2fcf3aaa8ede28",
            "filename": "tests/quantization/compressed_tensors/test_compressed_tensors.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/884a8ea1f058716c24adb52a0a6a0bf41fbb973d/tests%2Fquantization%2Fcompressed_tensors%2Ftest_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/884a8ea1f058716c24adb52a0a6a0bf41fbb973d/tests%2Fquantization%2Fcompressed_tensors%2Ftest_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensors%2Ftest_compressed_tensors.py?ref=884a8ea1f058716c24adb52a0a6a0bf41fbb973d",
            "previous_filename": "tests/quantization/compressed_tensor/test_compressed_tensors.py"
        }
    ],
    "stats": {
        "total": 483,
        "additions": 307,
        "deletions": 176
    }
}