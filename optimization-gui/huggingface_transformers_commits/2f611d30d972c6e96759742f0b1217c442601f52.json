{
    "author": "zucchini-nlp",
    "message": "Qwen2-VL: clean-up and add more tests (#33354)\n\n* clean-up on qwen2-vl and add generation tests\r\n\r\n* add video tests\r\n\r\n* Update tests/models/qwen2_vl/test_processing_qwen2_vl.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* fix and add better tests\r\n\r\n* Update src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* update docs and address comments\r\n\r\n* Update docs/source/en/model_doc/qwen2_vl.md\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Update docs/source/en/model_doc/qwen2_vl.md\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* update\r\n\r\n* remove size at all\r\n\r\n---------\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",
    "sha": "2f611d30d972c6e96759742f0b1217c442601f52",
    "files": [
        {
            "sha": "448a462152ee60287a9cb942a9a8e8a14ff6ffb8",
            "filename": "docs/source/en/model_doc/qwen2_vl.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f611d30d972c6e96759742f0b1217c442601f52/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f611d30d972c6e96759742f0b1217c442601f52/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md?ref=2f611d30d972c6e96759742f0b1217c442601f52",
            "patch": "@@ -229,8 +229,6 @@ processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixel\n \n ```\n \n-\n-\n #### Multiple Image Inputs\n \n By default, images and video content are directly included in the conversation. When handling multiple images, it's helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:"
        },
        {
            "sha": "5838c460e7c5dd6bd5f1a44984e32bd09801b823",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f611d30d972c6e96759742f0b1217c442601f52/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f611d30d972c6e96759742f0b1217c442601f52/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=2f611d30d972c6e96759742f0b1217c442601f52",
            "patch": "@@ -844,6 +844,7 @@ def forward(\n                 past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n             )\n \n         bsz, q_len, _ = hidden_states.size()\n@@ -1190,9 +1191,12 @@ def forward(\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n+        # the hard coded `3` is for temporal, height and width.\n         if position_ids is None:\n-            # the hard coded `3` is for temporal, height and width.\n             position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n+        elif position_ids.dim() == 2:\n+            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n \n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n@@ -1777,13 +1781,13 @@ def prepare_inputs_for_generation(\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n         if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n+            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n         else:\n-            model_inputs = {\"input_ids\": input_ids}\n+            model_inputs = {\"input_ids\": input_ids, \"inputs_embeds\": None}\n \n         if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if inputs_embeds is not None:\n-                batch_size, sequence_length = inputs_embeds.shape\n+            if model_inputs[\"inputs_embeds\"] is not None:\n+                batch_size, sequence_length, _ = inputs_embeds.shape\n                 device = inputs_embeds.device\n             else:\n                 batch_size, sequence_length = input_ids.shape"
        },
        {
            "sha": "591b82f053c8f368a8e00ccf98660ad60ed2808c",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 31,
            "deletions": 29,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f611d30d972c6e96759742f0b1217c442601f52/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f611d30d972c6e96759742f0b1217c442601f52/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=2f611d30d972c6e96759742f0b1217c442601f52",
            "patch": "@@ -21,25 +21,40 @@\n Processor class for Qwen2-VL.\n \"\"\"\n \n-from typing import List, Optional, Union\n+from typing import List, Union\n+\n+\n+try:\n+    from typing import Unpack\n+except ImportError:\n+    from typing_extensions import Unpack\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, VideoInput\n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType, logging\n+from ...processing_utils import (\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+)\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n+class Qwen2VLProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+        },\n+    }\n+\n+\n class Qwen2VLProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a Qwen2-VL processor which wraps a Qwen2-VL image processor and a Qwen2 tokenizer into a single processor.\n-\n     [`Qwen2VLProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n     [`~Qwen2VLProcessor.__call__`] and [`~Qwen2VLProcessor.decode`] for more information.\n-\n     Args:\n         image_processor ([`Qwen2VLImageProcessor`], *optional*):\n             The image processor is a required input.\n@@ -62,10 +77,7 @@ def __call__(\n         images: ImageInput = None,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n         videos: VideoInput = None,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: int = None,\n-        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n+        **kwargs: Unpack[Qwen2VLProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n@@ -84,22 +96,8 @@ def __call__(\n             videos (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n                 The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n                 tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n-                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n-                index) among:\n-                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n-                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-                  acceptable input length for the model if that argument is not provided.\n-                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-                  lengths).\n-            max_length (`int`, *optional*):\n-                Maximum length of the returned list and optionally padding length (see above).\n-            truncation (`bool`, *optional*):\n-                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:\n-\n                 - `'tf'`: Return TensorFlow `tf.constant` objects.\n                 - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                 - `'np'`: Return NumPy `np.ndarray` objects.\n@@ -117,15 +115,20 @@ def __call__(\n             - **image_grid_thw** -- List of image 3D grid in LLM. Returned when `images` is not `None`.\n             - **video_grid_thw** -- List of video 3D grid in LLM. Returned when `videos` is not `None`.\n         \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            Qwen2VLProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n         if images is not None:\n-            image_inputs = self.image_processor(images=images, videos=None, return_tensors=return_tensors)\n+            image_inputs = self.image_processor(images=images, videos=None, **output_kwargs[\"images_kwargs\"])\n             image_grid_thw = image_inputs[\"image_grid_thw\"]\n         else:\n             image_inputs = {}\n             image_grid_thw = None\n \n         if videos is not None:\n-            videos_inputs = self.image_processor(images=None, videos=videos, return_tensors=return_tensors)\n+            videos_inputs = self.image_processor(images=None, videos=videos, **output_kwargs[\"videos_kwargs\"])\n             video_grid_thw = videos_inputs[\"video_grid_thw\"]\n         else:\n             videos_inputs = {}\n@@ -156,9 +159,8 @@ def __call__(\n                     index += 1\n                 text[i] = text[i].replace(\"<|placeholder|>\", \"<|video_pad|>\")\n \n-        text_inputs = self.tokenizer(\n-            text, return_tensors=return_tensors, padding=padding, truncation=truncation, max_length=max_length\n-        )\n+        _ = output_kwargs[\"text_kwargs\"].pop(\"padding_side\", None)\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n \n         return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs})\n "
        },
        {
            "sha": "6b8072fc184f73ea564183ccb7b67ee4af4222c7",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f611d30d972c6e96759742f0b1217c442601f52/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f611d30d972c6e96759742f0b1217c442601f52/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=2f611d30d972c6e96759742f0b1217c442601f52",
            "patch": "@@ -65,6 +65,7 @@ def __init__(\n         image_size=28,\n         bos_token_id=0,\n         eos_token_id=1,\n+        pad_token_id=2,\n         vision_start_token_id=151652,\n         image_token_id=151655,\n         video_token_id=151656,\n@@ -76,7 +77,7 @@ def __init__(\n         max_window_layers=3,\n         model_type=\"qwen2_vl\",\n         num_attention_heads=4,\n-        num_hidden_layers=3,\n+        num_hidden_layers=4,\n         num_key_value_heads=2,\n         rope_theta=10000,\n         tie_word_embeddings=True,\n@@ -98,6 +99,7 @@ def __init__(\n         self.ignore_index = ignore_index\n         self.bos_token_id = bos_token_id\n         self.eos_token_id = eos_token_id\n+        self.pad_token_id = pad_token_id\n         self.vision_start_token_id = vision_start_token_id\n         self.image_token_id = image_token_id\n         self.video_token_id = video_token_id\n@@ -137,6 +139,7 @@ def get_config(self):\n             tie_word_embeddings=self.tie_word_embeddings,\n             bos_token_id=self.bos_token_id,\n             eos_token_id=self.eos_token_id,\n+            pad_token_id=self.pad_token_id,\n             vision_start_token_id=self.vision_start_token_id,\n             image_token_id=self.image_token_id,\n             video_token_id=self.video_token_id,\n@@ -162,6 +165,8 @@ def prepare_config_and_inputs_for_common(self):\n         vision_seqlen = pixel_values.shape[0] // self.batch_size // (self.vision_config[\"spatial_merge_size\"] ** 2)\n         input_ids = ids_tensor([self.batch_size, self.seq_length - 1 + vision_seqlen], self.vocab_size)\n         attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n+\n+        input_ids[input_ids == self.image_token_id] = self.pad_token_id\n         input_ids[:, torch.arange(vision_seqlen, device=torch_device) + 1] = self.image_token_id\n         labels = torch.zeros(\n             (self.batch_size, self.seq_length - 1 + vision_seqlen),\n@@ -221,6 +226,7 @@ class Qwen2VLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCas\n     \"\"\"\n \n     all_model_classes = (Qwen2VLForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (Qwen2VLForConditionalGeneration,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n \n@@ -300,6 +306,12 @@ def test_multi_gpu_data_parallel_forward(self):\n     def test_model_is_small(self):\n         pass\n \n+    @unittest.skip(\n+        reason=\"Qwen2-VL can't do low-memory generation because position IDs have extra dimension and split function doesn't work for that\"\n+    )\n+    def test_beam_search_low_memory(self):\n+        pass\n+\n \n @require_torch\n class Qwen2VLIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "d1ae16a9aa46e25e0c5fb66f4fbd29841c35c31a",
            "filename": "tests/models/qwen2_vl/test_processing_qwen2_vl.py",
            "status": "added",
            "additions": 237,
            "deletions": 0,
            "changes": 237,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f611d30d972c6e96759742f0b1217c442601f52/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f611d30d972c6e96759742f0b1217c442601f52/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py?ref=2f611d30d972c6e96759742f0b1217c442601f52",
            "patch": "@@ -0,0 +1,237 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import shutil\n+import tempfile\n+import unittest\n+\n+import pytest\n+\n+from transformers import AutoProcessor, Qwen2Tokenizer\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import Qwen2VLImageProcessor, Qwen2VLProcessor\n+\n+\n+@require_vision\n+@require_torch\n+class Qwen2VLProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Qwen2VLProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", patch_size=4)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def test_save_load_pretrained_default(self):\n+        tokenizer = self.get_tokenizer()\n+        image_processor = self.get_image_processor()\n+\n+        processor = Qwen2VLProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+        processor.save_pretrained(self.tmpdirname)\n+        processor = Qwen2VLProcessor.from_pretrained(self.tmpdirname, use_fast=False)\n+\n+        self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n+        self.assertEqual(processor.image_processor.to_json_string(), image_processor.to_json_string())\n+        self.assertIsInstance(processor.tokenizer, Qwen2Tokenizer)\n+        self.assertIsInstance(processor.image_processor, Qwen2VLImageProcessor)\n+\n+    def test_image_processor(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = Qwen2VLProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+\n+        image_input = self.prepare_image_inputs()\n+\n+        input_image_proc = image_processor(image_input, return_tensors=\"np\")\n+        input_processor = processor(images=image_input, text=\"dummy\", return_tensors=\"np\")\n+\n+        for key in input_image_proc.keys():\n+            self.assertAlmostEqual(input_image_proc[key].sum(), input_processor[key].sum(), delta=1e-2)\n+\n+    def test_processor(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = Qwen2VLProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(text=input_str, images=image_input)\n+\n+        self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values\", \"image_grid_thw\"])\n+\n+        # test if it raises when no input is passed\n+        with pytest.raises(ValueError):\n+            processor()\n+\n+        # test if it raises when no text is passed\n+        with pytest.raises(TypeError):\n+            processor(images=image_input)\n+\n+    def test_model_input_names(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = Qwen2VLProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        video_inputs = self.prepare_video_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, videos=video_inputs)\n+\n+        self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n+\n+    # Qwen2-VL doesn't accept `size` and resized to an optimal size using image_processor attrbutes\n+    # defined at `init`. Therefore, all tests are overwritten and don't actually test if kwargs are passed\n+    # to image processors\n+    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[0], 800)\n+\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        image_processor = self.get_component(\n+            \"image_processor\",\n+        )\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[0], 800)\n+\n+    def test_unstructured_kwargs(self):\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[0], 800)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[0], 1600)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 4)\n+\n+    def test_structured_kwargs_nested(self):\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[0], 800)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    def test_structured_kwargs_nested_from_dict(self):\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[0], 800)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    def test_image_processor_defaults_preserved_by_video_kwargs(self):\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        video_input = self.prepare_video_inputs()\n+\n+        inputs = processor(text=input_str, videos=video_input)\n+        self.assertEqual(inputs[\"pixel_values_videos\"].shape[0], 9600)"
        },
        {
            "sha": "b8ca7a6d6733feea98a2d0b5c400fbb374e26900",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 7,
            "deletions": 69,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f611d30d972c6e96759742f0b1217c442601f52/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f611d30d972c6e96759742f0b1217c442601f52/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=2f611d30d972c6e96759742f0b1217c442601f52",
            "patch": "@@ -23,15 +23,12 @@\n     from typing import Unpack\n except ImportError:\n     from typing_extensions import Unpack\n-import unittest\n \n import numpy as np\n \n-from transformers import CLIPTokenizerFast, ProcessorMixin\n from transformers.models.auto.processing_auto import processor_class_from_name\n from transformers.testing_utils import (\n     check_json_file_has_correct_format,\n-    require_tokenizers,\n     require_torch,\n     require_vision,\n )\n@@ -41,8 +38,6 @@\n if is_vision_available():\n     from PIL import Image\n \n-    from transformers import CLIPImageProcessor\n-\n \n def prepare_image_inputs():\n     \"\"\"This function prepares a list of PIL images\"\"\"\n@@ -53,7 +48,6 @@ def prepare_image_inputs():\n \n @require_torch\n @require_vision\n-@require_torch\n class ProcessorTesterMixin:\n     processor_class = None\n \n@@ -91,6 +85,13 @@ def prepare_image_inputs(self):\n         \"\"\"This function prepares a list of PIL images for testing\"\"\"\n         return prepare_image_inputs()\n \n+    @require_vision\n+    def prepare_video_inputs(self):\n+        \"\"\"This function prepares a list of numpy videos.\"\"\"\n+        video_input = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)] * 8\n+        image_inputs = [video_input] * 3  # batch-size=3\n+        return image_inputs\n+\n     def test_processor_to_json_string(self):\n         processor = self.get_processor()\n         obj = json.loads(processor.to_json_string())\n@@ -125,8 +126,6 @@ def skip_processor_without_typed_kwargs(self, processor):\n         if not is_kwargs_typed_dict:\n             self.skipTest(f\"{self.processor_class} doesn't have typed kwargs.\")\n \n-    @require_vision\n-    @require_torch\n     def test_tokenizer_defaults_preserved_by_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n@@ -141,8 +140,6 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n         self.assertEqual(len(inputs[\"input_ids\"][0]), 117)\n \n-    @require_torch\n-    @require_vision\n     def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n@@ -158,8 +155,6 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         inputs = processor(text=input_str, images=image_input)\n         self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 234)\n \n-    @require_vision\n-    @require_torch\n     def test_kwargs_overrides_default_tokenizer_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n@@ -176,8 +171,6 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n         )\n         self.assertEqual(len(inputs[\"input_ids\"][0]), 112)\n \n-    @require_torch\n-    @require_vision\n     def test_kwargs_overrides_default_image_processor_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n@@ -193,8 +186,6 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n         inputs = processor(text=input_str, images=image_input, size=[224, 224])\n         self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 224)\n \n-    @require_torch\n-    @require_vision\n     def test_unstructured_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n@@ -218,8 +209,6 @@ def test_unstructured_kwargs(self):\n         self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n         self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n \n-    @require_torch\n-    @require_vision\n     def test_unstructured_kwargs_batched(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n@@ -244,8 +233,6 @@ def test_unstructured_kwargs_batched(self):\n \n         self.assertEqual(len(inputs[\"input_ids\"][0]), 6)\n \n-    @require_torch\n-    @require_vision\n     def test_doubly_passed_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n@@ -265,8 +252,6 @@ def test_doubly_passed_kwargs(self):\n                 size={\"height\": 214, \"width\": 214},\n             )\n \n-    @require_torch\n-    @require_vision\n     def test_structured_kwargs_nested(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n@@ -293,8 +278,6 @@ def test_structured_kwargs_nested(self):\n \n         self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n \n-    @require_torch\n-    @require_vision\n     def test_structured_kwargs_nested_from_dict(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n@@ -318,48 +301,3 @@ def test_structured_kwargs_nested_from_dict(self):\n         self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n \n         self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-\n-class MyProcessor(ProcessorMixin):\n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"CLIPImageProcessor\"\n-    tokenizer_class = (\"CLIPTokenizer\", \"CLIPTokenizerFast\")\n-\n-    def __init__(self, image_processor=None, tokenizer=None, processor_attr_1=1, processor_attr_2=True):\n-        super().__init__(image_processor, tokenizer)\n-\n-        self.processor_attr_1 = processor_attr_1\n-        self.processor_attr_2 = processor_attr_2\n-\n-\n-@require_tokenizers\n-@require_vision\n-class ProcessorTest(unittest.TestCase):\n-    processor_class = MyProcessor\n-\n-    def prepare_processor_dict(self):\n-        return {\"processor_attr_1\": 1, \"processor_attr_2\": False}\n-\n-    def get_processor(self):\n-        image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n-        tokenizer = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-large-patch14\")\n-        processor = MyProcessor(image_processor, tokenizer, **self.prepare_processor_dict())\n-\n-        return processor\n-\n-    def test_processor_to_json_string(self):\n-        processor = self.get_processor()\n-        obj = json.loads(processor.to_json_string())\n-        for key, value in self.prepare_processor_dict().items():\n-            self.assertEqual(obj[key], value)\n-            self.assertEqual(getattr(processor, key, None), value)\n-\n-    def test_processor_from_and_save_pretrained(self):\n-        processor_first = self.get_processor()\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            saved_file = processor_first.save_pretrained(tmpdirname)[0]\n-            check_json_file_has_correct_format(saved_file)\n-            processor_second = self.processor_class.from_pretrained(tmpdirname)\n-\n-        self.assertEqual(processor_second.to_dict(), processor_first.to_dict())"
        }
    ],
    "stats": {
        "total": 403,
        "additions": 297,
        "deletions": 106
    }
}