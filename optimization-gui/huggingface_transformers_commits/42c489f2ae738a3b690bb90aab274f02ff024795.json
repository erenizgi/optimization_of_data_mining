{
    "author": "zucchini-nlp",
    "message": "Gemma3: fix test (#36820)\n\n* fix test\n\n* require_read_token and public repo ids\n\n* flash-attn test uncomment\n\n* fix torchscript",
    "sha": "42c489f2ae738a3b690bb90aab274f02ff024795",
    "files": [
        {
            "sha": "aa6d456cf4a792ec216eb5822c76b50901bcbeab",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/42c489f2ae738a3b690bb90aab274f02ff024795/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42c489f2ae738a3b690bb90aab274f02ff024795/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=42c489f2ae738a3b690bb90aab274f02ff024795",
            "patch": "@@ -1363,7 +1363,7 @@ def forward(\n             **lm_kwargs,\n         )\n \n-        logits = outputs.logits\n+        logits = outputs[0]\n         loss = None\n         if labels is not None:\n             # Upcast to float if we need to compute the loss to avoid potential precision issues"
        },
        {
            "sha": "64ebee00a01a00ebafff687b318494bb1272cf92",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/42c489f2ae738a3b690bb90aab274f02ff024795/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42c489f2ae738a3b690bb90aab274f02ff024795/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=42c489f2ae738a3b690bb90aab274f02ff024795",
            "patch": "@@ -557,7 +557,7 @@ def forward(\n             **lm_kwargs,\n         )\n \n-        logits = outputs.logits\n+        logits = outputs[0]\n         loss = None\n         if labels is not None:\n             # Upcast to float if we need to compute the loss to avoid potential precision issues"
        },
        {
            "sha": "06a476c69a3e9dd8f2fbeb7d98f3f6c1c519921e",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 35,
            "deletions": 33,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/42c489f2ae738a3b690bb90aab274f02ff024795/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42c489f2ae738a3b690bb90aab274f02ff024795/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=42c489f2ae738a3b690bb90aab274f02ff024795",
            "patch": "@@ -30,6 +30,8 @@\n )\n from transformers.testing_utils import (\n     cleanup,\n+    require_flash_attn,\n+    require_read_token,\n     require_torch,\n     require_torch_gpu,\n     slow,\n@@ -355,10 +357,10 @@ def test_automodelforcausallm(self):\n \n @slow\n @require_torch_gpu\n-# @require_read_token\n+@require_read_token\n class Gemma3IntegrationTest(unittest.TestCase):\n     def setUp(self):\n-        self.processor = Gemma3Processor.from_pretrained(\"gg-hf-g/gemma-3-4b-it\", padding_side=\"left\")\n+        self.processor = Gemma3Processor.from_pretrained(\"google/gemma-3-4b-it\", padding_side=\"left\")\n \n         url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\"\n         self.messages = [\n@@ -376,7 +378,7 @@ def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n     def test_model_4b_bf16(self):\n-        model_id = \"gg-hf-g/gemma-3-4b-it\"\n+        model_id = \"google/gemma-3-4b-it\"\n \n         model = Gemma3ForConditionalGeneration.from_pretrained(\n             model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n@@ -397,7 +399,7 @@ def test_model_4b_bf16(self):\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     def test_model_4b_batch(self):\n-        model_id = \"gg-hf-g/gemma-3-4b-it\"\n+        model_id = \"google/gemma-3-4b-it\"\n \n         model = Gemma3ForConditionalGeneration.from_pretrained(\n             model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n@@ -437,7 +439,7 @@ def test_model_4b_batch(self):\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     def test_model_4b_crops(self):\n-        model_id = \"gg-hf-g/gemma-3-4b-it\"\n+        model_id = \"google/gemma-3-4b-it\"\n \n         model = Gemma3ForConditionalGeneration.from_pretrained(\n             model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n@@ -465,12 +467,12 @@ def test_model_4b_crops(self):\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_NUM_IMAGES = 3  # one for the origin image and two crops of images\n-        EXPECTED_TEXTS = [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nDescribe this image in detail.\\nmodel\\nHere's a detailed description of the image:\\n\\n**Overall Impression:**\\n\\nThe image is a close-up shot of a garden scene featuring several\"]  # fmt: skip\n+        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a beach with a turquoise ocean and blue sky in the background.']  # fmt: skip\n         self.assertEqual(len(inputs[\"pixel_values\"]), EXPECTED_NUM_IMAGES)\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     def test_model_4b_multiimage(self):\n-        model_id = \"gg-hf-g/gemma-3-4b-it\"\n+        model_id = \"google/gemma-3-4b-it\"\n \n         model = Gemma3ForConditionalGeneration.from_pretrained(\n             model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n@@ -503,7 +505,7 @@ def test_model_4b_multiimage(self):\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     def test_model_1b_text_only(self):\n-        model_id = \"gg-hf-g/gemma-3-1b-it\"\n+        model_id = \"google/gemma-3-1b-it\"\n \n         model = Gemma3ForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\n             torch_device\n@@ -518,37 +520,37 @@ def test_model_1b_text_only(self):\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     # TODO: raushan FA2 generates gibberish for no reason, check later\n-    # @require_flash_attn\n-    # @require_torch_gpu\n-    # @mark.flash_attn_test\n-    # def test_model_4b_flash_attn(self):\n-    #     model_id = \"gg-hf-g/gemma-3-4b-it\"\n-    #\n-    #     model = Gemma3ForConditionalGeneration.from_pretrained(\n-    #         model_id, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n-    #     ).to(torch_device)\n-    #\n-    #     inputs = self.processor.apply_chat_template(\n-    #         self.messages,\n-    #         tokenize=True,\n-    #         return_dict=True,\n-    #         return_tensors=\"pt\",\n-    #         add_generation_prompt=True,\n-    #     ).to(torch_device)\n-    #\n-    #     output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n-    #     output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n-    #\n-    #     EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nPlease look out that you are what Grammy and Vi- ||.xfairesr--ith alerts themselves are||ِّ\\n\\n**General Note:**']  # fmt: skip\n-    #     self.assertEqual(output_text, EXPECTED_TEXTS)\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n+    def test_model_4b_flash_attn(self):\n+        model_id = \"google/gemma-3-4b-it\"\n+\n+        model = Gemma3ForConditionalGeneration.from_pretrained(\n+            model_id, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+        ).to(torch_device)\n+\n+        inputs = self.processor.apply_chat_template(\n+            self.messages,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            add_generation_prompt=True,\n+        ).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+\n+        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. It looks like a very sunny and']  # fmt: skip\n+        self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     @parameterized.expand([(\"flash_attention_2\",), (\"sdpa\",), (\"eager\",)])\n     def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         \"\"\"Test that we can correctly generate beyond the sliding window. This is non trivial as\n         we need to correctly slice the attention mask in all cases (because we use a HybridCache).\n         Outputs for every attention functions should be coherent and identical.\n         \"\"\"\n-        model_id = \"gg-hf-g/gemma-3-1b-it\"\n+        model_id = \"google/gemma-3-1b-it\"\n \n         input_text = [\n             \"This is a nice place. \" * 800 + \"I really enjoy the scenery,\",  # This is larger than 4096 tokens\n@@ -576,7 +578,7 @@ def test_generation_beyond_sliding_window_with_generation_config(self):\n         Same as `test_generation_beyond_sliding_window`, but passing a GenerationConfig. Regression test for #36684 --\n         ensures `cache_implementation='hybrid'` is correctly inherited from the base `model.generation_config`.\n         \"\"\"\n-        model_id = \"gg-hf-g/gemma-3-1b-it\"\n+        model_id = \"google/gemma-3-1b-it\"\n         attn_implementation = \"sdpa\"\n \n         input_text = ["
        }
    ],
    "stats": {
        "total": 72,
        "additions": 37,
        "deletions": 35
    }
}