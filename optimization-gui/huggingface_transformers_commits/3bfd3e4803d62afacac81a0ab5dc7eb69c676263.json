{
    "author": "vasqu",
    "message": "Fix: Jamba batched generation (#32914)\n\n* init fix\r\n\r\n* fix mask during cached forward, move mask related stuff to own function\r\n\r\n* adjust tests as left padding does not change logits as much anymore + batch gen (with todo on logits comp)\r\n\r\n* revert overwriting new integration tests\r\n\r\n* move some comments to docstring",
    "sha": "3bfd3e4803d62afacac81a0ab5dc7eb69c676263",
    "files": [
        {
            "sha": "60e1670a3c2784264e2c2d5c637b9f8b83190d5a",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 46,
            "deletions": 8,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bfd3e4803d62afacac81a0ab5dc7eb69c676263/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bfd3e4803d62afacac81a0ab5dc7eb69c676263/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=3bfd3e4803d62afacac81a0ab5dc7eb69c676263",
            "patch": "@@ -649,7 +649,12 @@ def __init__(self, config: JambaConfig, layer_idx):\n                 \" https://github.com/Dao-AILab/causal-conv1d. If you want to use the naive implementation, set `use_mamba_kernels=False` in the model config\"\n             )\n \n-    def cuda_kernels_forward(self, hidden_states: torch.Tensor, cache_params: HybridMambaAttentionDynamicCache = None):\n+    def cuda_kernels_forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cache_params: HybridMambaAttentionDynamicCache = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+    ):\n         batch_size, seq_len, _ = hidden_states.shape\n         use_precomputed_states = (\n             cache_params is not None\n@@ -666,6 +671,9 @@ def cuda_kernels_forward(self, hidden_states: torch.Tensor, cache_params: Hybrid\n         # inner layernorms which isn't supported by this fused kernel\n         hidden_states, gate = projected_states.chunk(2, dim=1)\n \n+        if attention_mask is not None:\n+            hidden_states = hidden_states * attention_mask.unsqueeze(1)\n+\n         # 2. Convolution sequence transformation\n         conv_weights = self.conv1d.weight.view(self.conv1d.weight.size(0), self.conv1d.weight.size(2))\n         if use_precomputed_states:\n@@ -683,6 +691,9 @@ def cuda_kernels_forward(self, hidden_states: torch.Tensor, cache_params: Hybrid\n                 cache_params.conv_states[self.layer_idx].copy_(conv_states)\n             hidden_states = causal_conv1d_fn(hidden_states, conv_weights, self.conv1d.bias, activation=self.activation)\n \n+        if attention_mask is not None:\n+            hidden_states = hidden_states * attention_mask.unsqueeze(1)\n+\n         # 3. State Space Model sequence transformation\n         # 3.a. input varying initialization of time_step, B and C\n         ssm_parameters = self.x_proj(hidden_states.transpose(1, 2))\n@@ -742,14 +753,17 @@ def cuda_kernels_forward(self, hidden_states: torch.Tensor, cache_params: Hybrid\n         return contextualized_states\n \n     # fmt: off\n-    def slow_forward(self, input_states, cache_params: HybridMambaAttentionDynamicCache = None):\n+    def slow_forward(self, input_states, cache_params: HybridMambaAttentionDynamicCache = None, attention_mask: Optional[torch.LongTensor] = None):\n         batch_size, seq_len, _ = input_states.shape\n         dtype = input_states.dtype\n         # 1. Gated MLP's linear projection\n         projected_states = self.in_proj(input_states).transpose(1, 2)                   # [batch, 2 * intermediate_size, seq_len]\n         hidden_states, gate = projected_states.chunk(2, dim=1)\n \n-        use_cache = isinstance(cache_params,HybridMambaAttentionDynamicCache)\n+        if attention_mask is not None:\n+            hidden_states = hidden_states * attention_mask.unsqueeze(1)\n+\n+        use_cache = isinstance(cache_params, HybridMambaAttentionDynamicCache)\n         # 2. Convolution sequence transformation\n         if use_cache and cache_params.ssm_states[self.layer_idx].shape[0] == batch_size:\n             if self.training:\n@@ -784,6 +798,9 @@ def slow_forward(self, input_states, cache_params: HybridMambaAttentionDynamicCa\n             )\n             hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])         # [batch, intermediate_size, seq_len]\n \n+        if attention_mask is not None:\n+            hidden_states = hidden_states * attention_mask.unsqueeze(1)\n+\n         # 3. State Space Model sequence transformation\n         # 3.a. Selection:  [batch, seq_len, self.time_step_rank + self.ssm_state_size * 2]\n         ssm_parameters = self.x_proj(hidden_states.transpose(1, 2))\n@@ -821,14 +838,19 @@ def slow_forward(self, input_states, cache_params: HybridMambaAttentionDynamicCa\n         return contextualized_states\n     # fmt: on\n \n-    def forward(self, hidden_states, cache_params: HybridMambaAttentionDynamicCache = None):\n+    def forward(\n+        self,\n+        hidden_states,\n+        cache_params: HybridMambaAttentionDynamicCache = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+    ):\n         if self.use_fast_kernels:\n             if not is_fast_path_available or \"cuda\" not in self.x_proj.weight.device.type:\n                 raise ValueError(\n                     \"Fast Mamba kernels are not available. Make sure to they are installed and that the mamba module is on a CUDA device\"\n                 )\n-            return self.cuda_kernels_forward(hidden_states, cache_params)\n-        return self.slow_forward(hidden_states, cache_params)\n+            return self.cuda_kernels_forward(hidden_states, cache_params, attention_mask)\n+        return self.slow_forward(hidden_states, cache_params, attention_mask)\n \n \n # Copied from transformers.models.mistral.modeling_mistral.MistralMLP with Mistral->Jamba\n@@ -1040,6 +1062,7 @@ def forward(\n         hidden_states = self.mamba(\n             hidden_states=hidden_states,\n             cache_params=past_key_value,\n+            attention_mask=attention_mask,\n         )\n         self_attn_weights = None\n \n@@ -1279,20 +1302,24 @@ def forward(\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(attention_mask, inputs_embeds, cache_position)\n+        mamba_mask = self._update_mamba_mask(attention_mask, cache_position)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_router_logits = () if output_router_logits else None\n \n         for decoder_layer in self.layers:\n+            # Depending on the layer type we opt for 2D base attention mask (Mamba) or 4D causal mask (Attention)\n+            layer_mask = mamba_mask if isinstance(decoder_layer, JambaMambaDecoderLayer) else causal_mask\n+\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    causal_mask,\n+                    layer_mask,\n                     position_ids,\n                     past_key_values,\n                     output_attentions,\n@@ -1303,7 +1330,7 @@ def forward(\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=causal_mask,\n+                    attention_mask=layer_mask,\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n@@ -1384,6 +1411,17 @@ def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n \n         return causal_mask\n \n+    def _update_mamba_mask(self, attention_mask, cache_position):\n+        \"\"\"\n+        No need for zeroing states when\n+            1. Cached forward\n+            2. Attending to all inputs\n+        \"\"\"\n+        mamba_mask = attention_mask\n+        if cache_position[0] > 0 or (attention_mask is not None and torch.all(attention_mask == 1)):\n+            mamba_mask = None\n+        return mamba_mask\n+\n \n # Adapted from transformers.models.mixtral.modeling_mixtral.MixtralForCausalLM with MIXTRAL->JAMBA, Mixtral->Jamba\n class JambaForCausalLM(JambaPreTrainedModel):"
        },
        {
            "sha": "6cbfe62cfe172b2e5c870c51785300079938589a",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 48,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bfd3e4803d62afacac81a0ab5dc7eb69c676263/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bfd3e4803d62afacac81a0ab5dc7eb69c676263/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=3bfd3e4803d62afacac81a0ab5dc7eb69c676263",
            "patch": "@@ -458,51 +458,6 @@ def test_attention_outputs(self):\n                 [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n             )\n \n-    def test_left_padding_compatibility(self):\n-        r\"\"\"\n-        Overriding the test_left_padding_compatibility test as the mamba layers accentuate the numerical differences\n-        effect of the left padding discussed in the issue in the note. Using a more permissive tolerance value.\n-        \"\"\"\n-        import inspect\n-        # NOTE: left-padding results in small numerical differences. This is expected.\n-        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n-\n-        # First, filter out models that don't support left padding - generative and decoder-only.\n-        # Jamba is a decoder-only architecture\n-        decoder_only_classes = self.all_generative_model_classes\n-\n-        # Then, test left-padding\n-        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n-            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-            if \"position_ids\" in signature:\n-                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n-                position_ids.masked_fill_(attention_mask == 0, 1)\n-                model_kwargs[\"position_ids\"] = position_ids\n-            if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n-                model_kwargs[\"cache_position\"] = cache_position\n-            return model_kwargs\n-\n-        for model_class in decoder_only_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            model = model_class(config).to(torch_device).eval()\n-            signature = inspect.signature(model.forward).parameters.keys()\n-\n-            # Without padding\n-            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n-            next_logits_wo_padding = model(**model_kwargs).logits[:, -1, :]\n-\n-            # With left-padding (length 32)\n-            pad_size = (input_ids.shape[0], 32)\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * config.pad_token_id\n-            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n-            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n-            next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n-\n-            # They should result in very similar logits\n-            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=3e-3))\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @require_bitsandbytes\n@@ -692,7 +647,7 @@ def test_simple_generate(self):\n             EXPECTED_LOGITS_NO_GRAD = torch.tensor(\n                 [\n                     0.0134, -0.2197,  0.0396, -0.1011,  0.0459,  0.2793, -0.1465,  0.1660,\n-                  -0.2930, -0.0278,  0.0269, -0.5586, -0.2109, -0.1426, -0.1553,  0.1279,\n+                    -0.2930, -0.0278,  0.0269, -0.5586, -0.2109, -0.1426, -0.1553,  0.1279,\n                     0.0713,  0.2246,  0.1660, -0.2314, -0.1187, -0.1162, -0.1377,  0.0292,\n                     0.1245,  0.2275,  0.0374,  0.1089, -0.1348, -0.2305,  0.1484, -0.3906,\n                     0.1709, -0.4590, -0.0447,  0.2422,  0.1592, -0.1855,  0.2441, -0.0562\n@@ -737,10 +692,11 @@ def test_simple_batched_generate_with_padding(self):\n             with torch.no_grad():\n                 logits = self.model(input_ids=inputs[\"input_ids\"]).logits\n \n+            # TODO fix logits\n             EXPECTED_LOGITS_NO_GRAD_0 = torch.tensor(\n                 [\n                     0.0166, -0.2227,  0.0396, -0.1035,  0.0459,  0.2754, -0.1445,  0.1641,\n-                  -0.2910, -0.0273,  0.0227, -0.5547, -0.2139, -0.1396, -0.1582,  0.1289,\n+                    -0.2910, -0.0273,  0.0227, -0.5547, -0.2139, -0.1396, -0.1582,  0.1289,\n                     0.0713,  0.2256,  0.1699, -0.2295, -0.1182, -0.1167, -0.1387,  0.0261,\n                     0.1270,  0.2285,  0.0403,  0.1108, -0.1318, -0.2334,  0.1455, -0.3945,\n                     0.1729, -0.4609, -0.0410,  0.2412,  0.1572, -0.1895,  0.2402, -0.0583\n@@ -749,7 +705,7 @@ def test_simple_batched_generate_with_padding(self):\n \n             EXPECTED_LOGITS_NO_GRAD_1 = torch.tensor(\n                 [\n-                   -0.1318,  0.2354, -0.4160, -0.0325, -0.0461,  0.0342,  0.2578,  0.0874,\n+                    -0.1318,  0.2354, -0.4160, -0.0325, -0.0461,  0.0342,  0.2578,  0.0874,\n                     0.1484,  0.2266, -0.1182, -0.1396, -0.1494, -0.1089, -0.0019, -0.2852,\n                     0.1973, -0.2676,  0.0586, -0.1992, -0.2520, -0.1147, -0.1973,  0.2129,\n                     0.0520,  0.1699,  0.1816,  0.1289,  0.1699, -0.1216, -0.2656, -0.2891,"
        }
    ],
    "stats": {
        "total": 106,
        "additions": 50,
        "deletions": 56
    }
}