{
    "author": "yaswanth19",
    "message": "Cleanup Attention class for Siglip and dependent models (#39040)\n\n* cleanup attention class\n\n* More models\n\n* more models\n\n* Changes\n\n* make style\n\n* Should fix CI\n\n* This should work ðŸ™",
    "sha": "0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
    "files": [
        {
            "sha": "8f6f0ff7fbc9a553954a39b765701939d2f49b15",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
            "patch": "@@ -623,6 +623,7 @@ def eager_attention_forward(\n \n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n+\n     return attn_output, attn_weights\n \n "
        },
        {
            "sha": "732712c517c06e9b66c5870b4ef44be57e23e217",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
            "patch": "@@ -275,6 +275,7 @@ def eager_attention_forward(\n \n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n+\n     return attn_output, attn_weights\n \n "
        },
        {
            "sha": "3487138234b7b7277cc683225f7dc45b0aabc6a9",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
            "patch": "@@ -606,7 +606,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -622,13 +622,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -644,9 +638,6 @@ def forward(\n         attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n "
        },
        {
            "sha": "805192cf5a1213e53828ff755d79a560a80b31fc",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
            "patch": "@@ -620,6 +620,7 @@ def eager_attention_forward(\n \n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n+\n     return attn_output, attn_weights\n \n "
        },
        {
            "sha": "c92bd7ba9c49520eba8d36b67dfdf72b74026512",
            "filename": "src/transformers/models/idefics/vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py?ref=0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
            "patch": "@@ -185,6 +185,7 @@ def eager_attention_forward(\n \n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n+\n     return attn_output, attn_weights\n \n "
        },
        {
            "sha": "9757a42049f9a829fbf3ab1932f2484c146fd08c",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
            "patch": "@@ -219,7 +219,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -235,13 +235,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -257,9 +251,6 @@ def forward(\n         attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n "
        },
        {
            "sha": "a2e0bc78d0f4c1b539acade38c60cd8c10695bbd",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
            "patch": "@@ -216,7 +216,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -232,13 +232,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -254,9 +248,6 @@ def forward(\n         attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n "
        },
        {
            "sha": "b8d6d50f9aeab15327f53333fcea9ae9d1713c73",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 16,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
            "patch": "@@ -21,7 +21,6 @@\n \n import numpy as np\n import torch\n-import torch.utils.checkpoint\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n from torch.nn.init import _calculate_fan_in_and_fan_out\n@@ -31,13 +30,10 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, torch_int\n from .configuration_siglip import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n def _trunc_normal_(tensor, mean, std, a, b):\n     # Cut & paste from PyTorch official master until it's in a few official releases - RW\n     # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n@@ -372,7 +368,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -388,13 +384,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -410,9 +400,6 @@ def forward(\n         attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n "
        },
        {
            "sha": "876a84e0259dfff10cad9e41819fb6ce5a111355",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 15,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
            "patch": "@@ -35,13 +35,10 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple\n from .configuration_siglip2 import Siglip2Config, Siglip2TextConfig, Siglip2VisionConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n @dataclass\n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -266,7 +263,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -282,13 +279,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -304,9 +295,6 @@ def forward(\n         attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n "
        },
        {
            "sha": "1b128a0fb636fe65bdf9054466d1f3b674cb4826",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
            "patch": "@@ -186,7 +186,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -202,13 +202,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -224,9 +218,6 @@ def forward(\n         attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n "
        },
        {
            "sha": "feccf6d7d9fdb535f417891ec106d0bcce744292",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
            "patch": "@@ -1008,8 +1008,6 @@ def forward(\n         decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n             Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-\n-        **flash_attn_kwargs: flash attention related parameters.\n         \"\"\"\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n@@ -1084,10 +1082,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> BaseModelOutput:\n-        r\"\"\"\n-        **flash_attn_kwargs: flash attention related parameters.\n-        \"\"\"\n-\n         encoder_outputs = self.encoder(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1162,7 +1156,6 @@ def forward(\n         decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n             Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1234,7 +1227,7 @@ def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n @auto_docstring\n class T5GemmaForSequenceClassification(T5GemmaPreTrainedModel):\n     def __init__(self, config: T5GemmaConfig, is_encoder_decoder: Optional[bool] = None):\n-        \"\"\"\n+        r\"\"\"\n         is_encoder_decoder (`Optional`, *optional*):\n             Whether use encoder_decoder for sequence classification. When set to False, only encoder is used.\n         \"\"\"\n@@ -1286,7 +1279,6 @@ def forward(\n         decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n             Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1382,7 +1374,7 @@ def forward(\n @auto_docstring\n class T5GemmaForTokenClassification(T5GemmaPreTrainedModel):\n     def __init__(self, config: T5GemmaConfig, is_encoder_decoder: Optional[bool] = None):\n-        \"\"\"\n+        r\"\"\"\n         is_encoder_decoder (`Optional`, *optional*):\n             Whether use encoder_decoder for token classification. When set to False, only encoder is used.\n         \"\"\"\n@@ -1435,7 +1427,6 @@ def forward(\n         decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n             Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "ae69ae9910092c9f4148df6bd170943ae0597e02",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
            "patch": "@@ -955,8 +955,6 @@ def forward(\n         decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n             Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-\n-        **flash_attn_kwargs: flash attention related parameters.\n         \"\"\"\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n@@ -1031,10 +1029,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> BaseModelOutput:\n-        r\"\"\"\n-        **flash_attn_kwargs: flash attention related parameters.\n-        \"\"\"\n-\n         encoder_outputs = self.encoder(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1109,7 +1103,6 @@ def forward(\n         decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n             Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1181,7 +1174,7 @@ def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n @auto_docstring\n class T5GemmaForSequenceClassification(T5GemmaPreTrainedModel):\n     def __init__(self, config: T5GemmaConfig, is_encoder_decoder: Optional[bool] = None):\n-        \"\"\"\n+        r\"\"\"\n         is_encoder_decoder (`Optional`, *optional*):\n             Whether use encoder_decoder for sequence classification. When set to False, only encoder is used.\n         \"\"\"\n@@ -1233,7 +1226,6 @@ def forward(\n         decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n             Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1329,7 +1321,7 @@ def forward(\n @auto_docstring\n class T5GemmaForTokenClassification(T5GemmaPreTrainedModel):\n     def __init__(self, config: T5GemmaConfig, is_encoder_decoder: Optional[bool] = None):\n-        \"\"\"\n+        r\"\"\"\n         is_encoder_decoder (`Optional`, *optional*):\n             Whether use encoder_decoder for token classification. When set to False, only encoder is used.\n         \"\"\"\n@@ -1382,7 +1374,6 @@ def forward(\n         decoder_position_ids (`torch.LongTensor` of shape `(batch_size, decoder_sequence_length)`, *optional*):\n             Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.decoder.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "0e043f354ee217f103b9a27d24dc81c54e1ff010",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=0d66ef77921fc77644fe698f2c7c3f49cdd0ffc0",
            "patch": "@@ -240,6 +240,7 @@ def eager_attention_forward(\n \n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n+\n     return attn_output, attn_weights\n \n "
        }
    ],
    "stats": {
        "total": 120,
        "additions": 23,
        "deletions": 97
    }
}