{
    "author": "zucchini-nlp",
    "message": "[glm4v] fix video inference (#39174)\n\nfix video inference",
    "sha": "91221da2f1f68df9eb97c980a7206b14c4d3a9b0",
    "files": [
        {
            "sha": "a97d9c0f8270afeed0d0457066d00857d0b504d7",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/91221da2f1f68df9eb97c980a7206b14c4d3a9b0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91221da2f1f68df9eb97c980a7206b14c4d3a9b0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=91221da2f1f68df9eb97c980a7206b14c4d3a9b0",
            "patch": "@@ -1269,13 +1269,13 @@ def forward(\n \n             if input_ids is None:\n                 video_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n                 video_mask = video_mask.all(-1)\n             else:\n-                video_mask = input_ids == self.config.video_token_id\n+                video_mask = input_ids == self.config.image_token_id\n \n-            n_video_tokens = (video_mask).sum()\n+            n_video_tokens = video_mask.sum()\n             n_video_features = video_embeds.shape[0]\n             video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:"
        },
        {
            "sha": "5732503daa6ee80ffe343e9e85a8bfda34df62d6",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/91221da2f1f68df9eb97c980a7206b14c4d3a9b0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91221da2f1f68df9eb97c980a7206b14c4d3a9b0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=91221da2f1f68df9eb97c980a7206b14c4d3a9b0",
            "patch": "@@ -1269,13 +1269,13 @@ def forward(\n \n             if input_ids is None:\n                 video_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n                 video_mask = video_mask.all(-1)\n             else:\n-                video_mask = input_ids == self.config.video_token_id\n+                video_mask = input_ids == self.config.image_token_id\n \n-            n_video_tokens = (video_mask).sum()\n+            n_video_tokens = video_mask.sum()\n             n_video_features = video_embeds.shape[0]\n             video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}