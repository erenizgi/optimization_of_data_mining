{
    "author": "SunMarc",
    "message": "fix torch_dtype, contiguous, and load_state_dict regression (#36512)\n\n* fix regression\n\n* fix param\n\n* fix load_state_dict\n\n* style\n\n* better fix for module\n\n* fix tests\n\n* quick fix for now\n\n* rm print",
    "sha": "0463901c92e08cefbccf19f409b6cc43c153352d",
    "files": [
        {
            "sha": "9cb3de74c2b46c7689a30d26880926386953a08e",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 61,
            "deletions": 33,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/0463901c92e08cefbccf19f409b6cc43c153352d/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0463901c92e08cefbccf19f409b6cc43c153352d/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=0463901c92e08cefbccf19f409b6cc43c153352d",
            "patch": "@@ -67,6 +67,7 @@\n     translate_to_torch_parallel_style,\n )\n from .quantizers import AutoHfQuantizer, HfQuantizer\n+from .quantizers.quantizers_utils import get_module_from_name\n from .safetensors_conversion import auto_conversion\n from .utils import (\n     ACCELERATE_MIN_VERSION,\n@@ -536,11 +537,11 @@ def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n def load_state_dict(\n     checkpoint_file: Union[str, os.PathLike],\n     is_quantized: bool = False,\n-    map_location: Optional[Union[str, torch.device]] = \"meta\",\n+    map_location: Optional[Union[str, torch.device]] = \"cpu\",\n     weights_only: bool = True,\n ):\n     \"\"\"\n-    Reads a `safetensor` or a `.bin` checkpoint file into `meta` if requested.\n+    Reads a `safetensor` or a `.bin` checkpoint file. We load the checkpoint on \"cpu\" by default.\n     \"\"\"\n     if checkpoint_file.endswith(\".safetensors\") and is_safetensors_available():\n         with safe_open(checkpoint_file, framework=\"pt\") as f:\n@@ -771,6 +772,7 @@ def _load_state_dict_into_meta_model(\n     unexpected_keys=None,  # passing `unexpected` for cleanup from quantization items\n     device_mesh=None,\n     shard_file=None,\n+    weights_only=True,\n ):\n     \"\"\"\n     This is somewhat similar to `_load_state_dict_into_model`, but deals with a model that has some or all of its\n@@ -800,7 +802,15 @@ def _load_state_dict_into_meta_model(\n     if shard_file.endswith(\".safetensors\"):\n         file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device)\n     else:\n-        bin_state_dict = load_state_dict(shard_file, map_location=\"cpu\")\n+        map_location = \"cpu\"\n+        if (\n+            device_map is not None\n+            and hf_quantizer is not None\n+            and hf_quantizer.quantization_config.quant_method == QuantizationMethod.TORCHAO\n+            and hf_quantizer.quantization_config.quant_type in [\"int4_weight_only\", \"autoquant\"]\n+        ):\n+            map_location = torch.device([d for d in device_map.values() if d not in [\"cpu\", \"disk\"]][0])\n+        bin_state_dict = load_state_dict(shard_file, map_location=map_location, weights_only=weights_only)\n \n     error_msgs = []\n \n@@ -822,23 +832,36 @@ def _load_state_dict_into_meta_model(\n             if shard_file.endswith(\".safetensors\")\n             else bin_state_dict[serialized_param_name]\n         )\n+\n+        # For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model, and which\n+        # uses `param.copy_(input_param)` that preserves the contiguity of the parameter in the model.\n+        # Reference: https://github.com/pytorch/pytorch/blob/db79ceb110f6646523019a59bbd7b838f43d4a86/torch/nn/modules/module.py#L2040C29-L2040C29\n+\n+        old_param = model\n+        splits = fixed_param_name.split(\".\")\n+        for split in splits:\n+            # We shouldn't hit the default value unless for quant methods like hqq that modifies expected_keys.\n+            old_param = getattr(old_param, split, None)\n+            if old_param is None:\n+                break\n+\n+        if not isinstance(old_param, (torch.nn.Parameter, torch.Tensor)):\n+            old_param = None\n+\n         # We convert floating dtypes to the `dtype` passed except for float8_e4m3fn type. We also want to keep the buffers/params\n         # in int/uint/bool and not cast them.\n         param_casting_dtype = None\n         is_param_float8_e4m3fn = is_torch_e4m3fn_available and empty_param.dtype == torch.float8_e4m3fn\n-\n-        if dtype is not None and empty_param.dtype.is_floating_point and not is_param_float8_e4m3fn:\n-            if (\n-                keep_in_fp32_modules is not None\n-                and keep_in_fp32_modules.search(fixed_param_name)\n-                and dtype == torch.float16\n-            ):\n+        if empty_param.dtype.is_floating_point and not is_param_float8_e4m3fn:\n+            if keep_in_fp32_modules is not None and keep_in_fp32_modules.search(fixed_param_name):\n                 param_casting_dtype = torch.float32\n-            else:\n+            elif dtype is not None:\n                 param_casting_dtype = dtype\n+            elif old_param is not None:\n+                param_casting_dtype = old_param.dtype\n \n         if device_mesh is not None:  # In this case, the param is already on the correct device!\n-            module_to_tp, param_type = find_submodule_and_param_name(model, fixed_param_name)\n+            module_to_tp, param_type = get_module_from_name(model, fixed_param_name)\n             current_module_plan = None\n             full_tp_plan_ = \"|\".join(full_tp_plan.keys()).replace(\"*\", \"[0-9]+\")\n             if plan := re.search(full_tp_plan_, fixed_param_name):\n@@ -859,8 +882,10 @@ def _load_state_dict_into_meta_model(\n                 else:\n                     param = param[rank * (row // device_mesh.size()) : (rank + 1) * (row // device_mesh.size()), :]\n                     shard = Shard(0)\n-                if param_casting_dtype is not None and param_casting_dtype != empty_param.dtype:\n+                if param_casting_dtype is not None:\n                     param = param.to(param_casting_dtype)\n+                if old_param.is_contiguous():\n+                    param = param.contiguous()\n                 local_parameter = DTensor.from_local(\n                     param,\n                     device_mesh=device_mesh,\n@@ -873,9 +898,18 @@ def _load_state_dict_into_meta_model(\n                 output_fn = partial(tp_layer._prepare_output_fn, tp_layer.output_layouts, tp_layer.use_local_output)\n                 distribute_module(module_to_tp, device_mesh, None, input_fn, output_fn)\n             else:\n-                module_to_tp.load_state_dict({param_type: param[:]}, strict=False, assign=True)\n+                param = param[:]\n+                if old_param is not None and old_param.is_contiguous():\n+                    param = param.contiguous()\n+                module_to_tp.load_state_dict({param_type: param}, strict=False, assign=True)\n \n         else:\n+            param = param[:]\n+            if param_casting_dtype is not None:\n+                param = param.to(param_casting_dtype)\n+            if old_param is not None and old_param.is_contiguous():\n+                param = param.contiguous()\n+\n             if device_map is None:\n                 param_device = \"cpu\"\n             else:\n@@ -887,9 +921,9 @@ def _load_state_dict_into_meta_model(\n \n             if param_device == \"disk\":\n                 if not is_safetensors:\n-                    offload_index = offload_weight(param[:], fixed_param_name, offload_folder, offload_index)\n+                    offload_index = offload_weight(param, fixed_param_name, offload_folder, offload_index)\n             elif param_device == \"cpu\" and state_dict_index is not None:\n-                state_dict_index = offload_weight(param[:], fixed_param_name, state_dict_folder, state_dict_index)\n+                state_dict_index = offload_weight(param, fixed_param_name, state_dict_folder, state_dict_index)\n             elif (\n                 not is_quantized\n                 or (not hf_quantizer.requires_parameters_quantization)\n@@ -906,23 +940,21 @@ def _load_state_dict_into_meta_model(\n             ):\n                 if is_fsdp_enabled():\n                     param_device = \"cpu\" if is_local_dist_rank_0() else \"meta\"\n-                module, param_type = find_submodule_and_param_name(model, fixed_param_name)\n-                if param_casting_dtype is not None and param_casting_dtype != empty_param.dtype:\n-                    param = param[:].to(param_casting_dtype)\n+                module, param_type = get_module_from_name(model, fixed_param_name)\n                 module.load_state_dict(\n-                    {param_type: param[:].to(param_device)},\n+                    {param_type: param.to(param_device)},\n                     strict=False,\n                     assign=True,\n                 )\n             else:\n                 hf_quantizer.create_quantized_param(\n-                    model, param[:], fixed_param_name, param_device, state_dict, unexpected_keys\n+                    model, param, fixed_param_name, param_device, state_dict, unexpected_keys\n                 )\n                 # For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\n                 # and then cast it to CPU to avoid excessive memory usage on each GPU\n                 # in comparison to the sharded model across GPUs.\n                 if is_fsdp_enabled() or is_deepspeed_zero3_enabled():\n-                    module, param_type = find_submodule_and_param_name(model, fixed_param_name)\n+                    module, param_type = get_module_from_name(model, fixed_param_name)\n                     value = getattr(module, param_type)\n                     param_to = \"cpu\"\n                     if is_fsdp_enabled() and not is_local_dist_rank_0():\n@@ -4203,7 +4235,9 @@ def from_pretrained(\n                             elif not is_sharded:\n                                 torch_dtype = get_state_dict_dtype(state_dict)\n                             else:\n-                                one_state_dict = load_state_dict(resolved_archive_file[0], weights_only=weights_only)\n+                                one_state_dict = load_state_dict(\n+                                    resolved_archive_file[0], map_location=\"meta\", weights_only=weights_only\n+                                )\n                                 torch_dtype = get_state_dict_dtype(one_state_dict)\n                                 del one_state_dict  # free CPU memory\n                             logger.info(\n@@ -4848,7 +4882,7 @@ def _load_pretrained_model(\n         else:\n             folder = None\n \n-        model.expected_keys = expected_keys\n+        model_to_load.expected_keys = expected_keys\n         if device_map is not None:\n             expanded_device_map = expand_device_map(device_map, original_loaded_keys, start_prefix)\n             if hf_quantizer is None:\n@@ -4907,6 +4941,7 @@ def _load_pretrained_model(\n                     unexpected_keys=unexpected_keys,\n                     device_mesh=device_mesh,\n                     resolved_archive_file=resolved_archive_file,\n+                    weights_only=weights_only,\n                 )\n             else:\n                 # We need to read the state dict as it is meta otherwise\n@@ -4957,16 +4992,8 @@ def _load_pretrained_model(\n                 # Skip the load for shards that only contain disk-offloaded weights when using safetensors for the offload.\n                 if shard_file in disk_only_shard_files:\n                     continue\n-                map_location = None\n-                if (\n-                    device_map is not None\n-                    and hf_quantizer is not None\n-                    and hf_quantizer.quantization_config.quant_method == QuantizationMethod.TORCHAO\n-                    and hf_quantizer.quantization_config.quant_type in [\"int4_weight_only\", \"autoquant\"]\n-                ):\n-                    map_location = torch.device([d for d in device_map.values() if d not in [\"cpu\", \"disk\"]][0])\n                 state_dict = load_state_dict(\n-                    shard_file, is_quantized=is_quantized, map_location=map_location, weights_only=weights_only\n+                    shard_file, is_quantized=is_quantized, map_location=\"meta\", weights_only=weights_only\n                 )\n \n                 # Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\n@@ -5006,6 +5033,7 @@ def _load_pretrained_model(\n                             unexpected_keys=unexpected_keys,\n                             device_mesh=device_mesh,\n                             shard_file=shard_file,\n+                            weights_only=weights_only,\n                         )\n                         error_msgs += new_error_msgs\n                 else:"
        }
    ],
    "stats": {
        "total": 94,
        "additions": 61,
        "deletions": 33
    }
}