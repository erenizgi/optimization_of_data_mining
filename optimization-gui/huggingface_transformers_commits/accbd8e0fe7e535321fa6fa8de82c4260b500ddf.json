{
    "author": "zucchini-nlp",
    "message": "[sliding window] revert and deprecate (#39301)\n\n* bring back and deprecate\n\n* oops\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "accbd8e0fe7e535321fa6fa8de82c4260b500ddf",
    "files": [
        {
            "sha": "96efb4460f34e9c7c00ba16fc92145b73b830715",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 19,
            "deletions": 2,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/accbd8e0fe7e535321fa6fa8de82c4260b500ddf/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accbd8e0fe7e535321fa6fa8de82c4260b500ddf/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=accbd8e0fe7e535321fa6fa8de82c4260b500ddf",
            "patch": "@@ -19,6 +19,8 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import warnings\n+\n from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n \n@@ -216,14 +218,29 @@ def __init__(\n             **kwargs,\n         )\n \n+        # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n+        self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 4)\n+\n         if self.layer_types is None:\n             # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n-            sliding_window_pattern = getattr(self, \"sliding_window_pattern\", 4)\n+            self._sliding_window_pattern = getattr(self, \"sliding_window_pattern\", 4)\n             self.layer_types = [\n-                \"sliding_attention\" if bool((i + 1) % sliding_window_pattern) else \"full_attention\"\n+                \"sliding_attention\" if bool((i + 1) % self._sliding_window_pattern) else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types)\n \n+    @property\n+    def sliding_window_pattern(self):\n+        warnings.warn(\n+            \"The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\",\n+            FutureWarning,\n+        )\n+        return self._sliding_window_pattern\n+\n+    @sliding_window_pattern.setter\n+    def sliding_window_pattern(self, value):\n+        self._sliding_window_pattern = value\n+\n \n __all__ = [\"Cohere2Config\"]"
        },
        {
            "sha": "4552f57bf189e56f2d2a9d08fe9f62628353371a",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/accbd8e0fe7e535321fa6fa8de82c4260b500ddf/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accbd8e0fe7e535321fa6fa8de82c4260b500ddf/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=accbd8e0fe7e535321fa6fa8de82c4260b500ddf",
            "patch": "@@ -13,6 +13,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import warnings\n from typing import Callable, Optional\n \n import torch\n@@ -237,15 +238,30 @@ def __init__(\n             **kwargs,\n         )\n \n+        # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n+        self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 4)\n+\n         if self.layer_types is None:\n             # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n-            sliding_window_pattern = getattr(self, \"sliding_window_pattern\", 4)\n+            self._sliding_window_pattern = getattr(self, \"sliding_window_pattern\", 4)\n             self.layer_types = [\n-                \"sliding_attention\" if bool((i + 1) % sliding_window_pattern) else \"full_attention\"\n+                \"sliding_attention\" if bool((i + 1) % self._sliding_window_pattern) else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types)\n \n+    @property\n+    def sliding_window_pattern(self):\n+        warnings.warn(\n+            \"The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\",\n+            FutureWarning,\n+        )\n+        return self._sliding_window_pattern\n+\n+    @sliding_window_pattern.setter\n+    def sliding_window_pattern(self, value):\n+        self._sliding_window_pattern = value\n+\n \n class Cohere2RotaryEmbedding(CohereRotaryEmbedding):\n     pass"
        },
        {
            "sha": "c0184c1993d3dc5ee63f30296905f70af3e020ba",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 17,
            "deletions": 7,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/accbd8e0fe7e535321fa6fa8de82c4260b500ddf/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accbd8e0fe7e535321fa6fa8de82c4260b500ddf/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=accbd8e0fe7e535321fa6fa8de82c4260b500ddf",
            "patch": "@@ -19,6 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import warnings\n from typing import Any, Optional, Union\n \n from ...configuration_utils import PretrainedConfig, layer_type_validation\n@@ -145,10 +146,6 @@ class Gemma3TextConfig(PretrainedConfig):\n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n     ```\n-        rope_local_base_freq (float, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings for local attention.\n-        sliding_window_pattern (`int`, *optional*, defaults to 6):\n-            Pattern for the sliding window attention.\n     \"\"\"\n \n     model_type = \"gemma3_text\"\n@@ -230,15 +227,28 @@ def __init__(\n         self.rope_scaling = rope_scaling\n         rope_config_validation(self)\n \n+        # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n+        self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 6)\n+\n         if self.layer_types is None:\n-            # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n-            sliding_window_pattern = getattr(self, \"sliding_window_pattern\", 6)\n             self.layer_types = [\n-                \"sliding_attention\" if bool((i + 1) % sliding_window_pattern) else \"full_attention\"\n+                \"sliding_attention\" if bool((i + 1) % self._sliding_window_pattern) else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types)\n \n+    @property\n+    def sliding_window_pattern(self):\n+        warnings.warn(\n+            \"The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\",\n+            FutureWarning,\n+        )\n+        return self._sliding_window_pattern\n+\n+    @sliding_window_pattern.setter\n+    def sliding_window_pattern(self, value):\n+        self._sliding_window_pattern = value\n+\n \n class Gemma3Config(PretrainedConfig):\n     r\"\"\""
        },
        {
            "sha": "85715626cc1032e92353beecc7c04949cbc9b6f4",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 17,
            "deletions": 7,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/accbd8e0fe7e535321fa6fa8de82c4260b500ddf/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/accbd8e0fe7e535321fa6fa8de82c4260b500ddf/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=accbd8e0fe7e535321fa6fa8de82c4260b500ddf",
            "patch": "@@ -14,6 +14,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import copy\n+import warnings\n from collections.abc import Callable\n from typing import Any, Optional, Union\n \n@@ -171,10 +172,6 @@ class Gemma3TextConfig(Gemma2Config, PretrainedConfig):\n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n     ```\n-        rope_local_base_freq (float, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings for local attention.\n-        sliding_window_pattern (`int`, *optional*, defaults to 6):\n-            Pattern for the sliding window attention.\n     \"\"\"\n \n     model_type = \"gemma3_text\"\n@@ -241,15 +238,28 @@ def __init__(\n         self.rope_scaling = rope_scaling\n         rope_config_validation(self)\n \n+        # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n+        self._sliding_window_pattern = kwargs.get(\"sliding_window_pattern\", 6)\n+\n         if self.layer_types is None:\n-            # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n-            sliding_window_pattern = getattr(self, \"sliding_window_pattern\", 6)\n             self.layer_types = [\n-                \"sliding_attention\" if bool((i + 1) % sliding_window_pattern) else \"full_attention\"\n+                \"sliding_attention\" if bool((i + 1) % self._sliding_window_pattern) else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types)\n \n+    @property\n+    def sliding_window_pattern(self):\n+        warnings.warn(\n+            \"The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\",\n+            FutureWarning,\n+        )\n+        return self._sliding_window_pattern\n+\n+    @sliding_window_pattern.setter\n+    def sliding_window_pattern(self, value):\n+        self._sliding_window_pattern = value\n+\n \n class Gemma3Config(PretrainedConfig):\n     r\"\"\""
        }
    ],
    "stats": {
        "total": 89,
        "additions": 71,
        "deletions": 18
    }
}