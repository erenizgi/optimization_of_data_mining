{
    "author": "casinca",
    "message": "perf: Optimization for Min-p sampling implementation (#42248)\n\n* refactor(MinPLogitsWarper): optimizing min_tokens_to_keep\n\n* Fix(MinPLogitsWarper): edge case when min_tokens_to_keep > vocab_size",
    "sha": "4391cfd336281a68f5ff931a7b35024186519d69",
    "files": [
        {
            "sha": "02be98961f74eb3105fcee58cc784a4b54d7918e",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/4391cfd336281a68f5ff931a7b35024186519d69/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4391cfd336281a68f5ff931a7b35024186519d69/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=4391cfd336281a68f5ff931a7b35024186519d69",
            "patch": "@@ -748,19 +748,18 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n         # Convert logits to probabilities\n         probs = torch.softmax(scores, dim=-1)\n         # Get the probability of the top token for each sequence in the batch\n-        top_probs, _ = probs.max(dim=-1, keepdim=True)\n+        top_probs = probs.amax(dim=-1, keepdim=True)\n         # Calculate the actual min_p threshold by scaling min_p with the top token's probability\n         scaled_min_p = self.min_p * top_probs\n         # Create a mask for tokens that have a probability less than the scaled min_p\n         tokens_to_remove = probs < scaled_min_p\n \n-        sorted_indices = torch.argsort(scores, descending=True, dim=-1)\n-        sorted_indices_to_remove = torch.gather(tokens_to_remove, dim=-1, index=sorted_indices)\n-        # Keep at least min_tokens_to_keep\n-        sorted_indices_to_remove[..., : self.min_tokens_to_keep] = False\n+        # Keep at least min_tokens_to_keep tokens (clip k to vocab size if needed, avoids index out of range)\n+        k = min(self.min_tokens_to_keep, probs.shape[-1])\n+        sorted_indices = torch.topk(probs, k, dim=-1).indices\n+        tokens_to_remove.scatter_(-1, sorted_indices, False)\n \n-        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n-        scores_processed = scores.masked_fill(indices_to_remove, self.filter_value)\n+        scores_processed = scores.masked_fill(tokens_to_remove, self.filter_value)\n         return scores_processed\n \n "
        }
    ],
    "stats": {
        "total": 13,
        "additions": 6,
        "deletions": 7
    }
}