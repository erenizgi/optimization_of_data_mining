{
    "author": "itazap",
    "message": "Support reading tiktoken tokenizer.model file (#31656)\n\n* use existing TikTokenConverter to read tiktoken tokenizer.model file\r\n\r\n* del test file\r\n\r\n* create titktoken integration file\r\n\r\n* adding tiktoken llama test\r\n\r\n* ALTNATIVE IMPLEMENTATION: supports llama 405B\r\n\r\n* fix one char\r\n\r\n* remove redundant line\r\n\r\n* small fix\r\n\r\n* rm unused import\r\n\r\n* flag for converting from tiktokeng\r\n\r\n* remove unneeded file\r\n\r\n* ruff\r\n\r\n* remove llamatiktokenconverter, stick to general converter\r\n\r\n* tiktoken support v2\r\n\r\n* update test\r\n\r\n* remove stale changes\r\n\r\n* udpate doc\r\n\r\n* protect import\r\n\r\n* use is_protobuf_available\r\n\r\n* add templateprocessor in tiktokenconverter\r\n\r\n* reverting templateprocessor from tiktoken support\r\n\r\n* update test\r\n\r\n* add require_tiktoken\r\n\r\n* dev-ci\r\n\r\n* trigger build\r\n\r\n* trigger build again\r\n\r\n* dev-ci\r\n\r\n* [build-ci-image] tiktoken\r\n\r\n* dev-ci\r\n\r\n* dev-ci\r\n\r\n* dev-ci\r\n\r\n* dev-ci\r\n\r\n* change tiktoken file name\r\n\r\n* feedback review\r\n\r\n* feedback rev\r\n\r\n* applying feedback, removing tiktoken converters\r\n\r\n* conform test\r\n\r\n* adding docs for review\r\n\r\n* add doc file for review\r\n\r\n* add doc file for review\r\n\r\n* add doc file for review\r\n\r\n* support loading model without config.json file\r\n\r\n* Revert \"support loading model without config.json file\"\r\n\r\nThis reverts commit 2753602e51c34cef2f184eb11f36d2ad1b02babb.\r\n\r\n* remove dev var\r\n\r\n* updating docs\r\n\r\n* safely import protobuf\r\n\r\n* fix protobuf import error\r\n\r\n* fix protobuf import error\r\n\r\n* trying isort to fix ruff error\r\n\r\n* fix ruff error\r\n\r\n* try to fix ruff again\r\n\r\n* try to fix ruff again\r\n\r\n* try to fix ruff again\r\n\r\n* doc table of contents\r\n\r\n* add fix for consistency.dockerfile torchaudio\r\n\r\n* ruff\r\n\r\n* applying feedback\r\n\r\n* minor typo\r\n\r\n* merging with push-ci-image\r\n\r\n* clean up imports\r\n\r\n* revert dockerfile consistency",
    "sha": "e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
    "files": [
        {
            "sha": "1f09626d8904f7693899c72b0284c7af985d97f5",
            "filename": "docker/consistency.dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/docker%2Fconsistency.dockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/docker%2Fconsistency.dockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Fconsistency.dockerfile?ref=e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
            "patch": "@@ -13,4 +13,4 @@ RUN uv pip install --no-cache-dir \"git+https://github.com/huggingface/transforme\n RUN git lfs install\n \n RUN pip uninstall -y transformers\n-RUN apt-get clean && rm -rf /var/lib/apt/lists/* && apt-get autoremove && apt-get autoclean\n+RUN apt-get clean && rm -rf /var/lib/apt/lists/* && apt-get autoremove && apt-get autoclean\n\\ No newline at end of file"
        },
        {
            "sha": "710a599abbe935698b8e3b8b93a270a85f0b9d3e",
            "filename": "docker/torch-light.dockerfile",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/docker%2Ftorch-light.dockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/docker%2Ftorch-light.dockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftorch-light.dockerfile?ref=e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
            "patch": "@@ -6,6 +6,6 @@ RUN apt-get update &&  apt-get install -y --no-install-recommends libsndfile1-de\n ENV UV_PYTHON=/usr/local/bin/python\n RUN pip --no-cache-dir install uv && uv venv && uv pip install --no-cache-dir -U pip setuptools\n RUN pip install --no-cache-dir 'torch' 'torchvision' 'torchaudio' --index-url https://download.pytorch.org/whl/cpu\n-RUN uv pip install --no-deps timm accelerate --extra-index-url https://download.pytorch.org/whl/cpu \n-RUN uv pip install --no-cache-dir librosa \"git+https://github.com/huggingface/transformers.git@${REF}#egg=transformers[sklearn,sentencepiece,vision,testing]\"\n+RUN uv pip install --no-deps timm accelerate --extra-index-url https://download.pytorch.org/whl/cpu\n+RUN uv pip install --no-cache-dir librosa \"git+https://github.com/huggingface/transformers.git@${REF}#egg=transformers[sklearn,sentencepiece,vision,testing,tiktoken]\"\n RUN pip uninstall -y transformers\n\\ No newline at end of file"
        },
        {
            "sha": "abcfac64d038f59b94ee2dbeca3fe5a0fe7b0eb3",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
            "patch": "@@ -145,6 +145,8 @@\n     title: Troubleshoot\n   - local: gguf\n     title: Interoperability with GGUF files\n+  - local: tiktoken\n+    title: Interoperability with TikToken files\n   title: Developer guides\n - sections:\n   - local: quantization/overview"
        },
        {
            "sha": "528ff4f76dc5f6d3b85d28794f9f53ce93bb9df9",
            "filename": "docs/source/en/tiktoken.md",
            "status": "added",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/docs%2Fsource%2Fen%2Ftiktoken.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/docs%2Fsource%2Fen%2Ftiktoken.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftiktoken.md?ref=e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
            "patch": "@@ -0,0 +1,38 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+``\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Tiktoken and interaction with Transformers\n+\n+Support for tiktoken model files is seamlessly integrated in ü§ó transformers when loading models \n+`from_pretrained` with a `tokenizer.model` tiktoken file on the Hub, which is automatically converted into our \n+[fast tokenizer](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast). \n+\n+### Known models that were released with a `tiktoken.model`:\n+\t- gpt2\n+\t- llama3\n+\n+## Example usage\n+ \n+In order to load `tiktoken` files in `transformers`, ensure that the `tokenizer.model` file is a tiktoken file and it \n+will automatically be loaded when loading `from_pretrained`. Here is how one would load a tokenizer and a model, which \n+ can be loaded from the exact same file:\n+\n+```py\n+from transformers import AutoTokenizer\n+\n+model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n+tokenizer = AutoTokenizer.from_pretrained(model_id, subfolder=\"original\") \n+```"
        },
        {
            "sha": "43d051df8b8378fef08a2ec8c6332125d1d13ee2",
            "filename": "setup.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
            "patch": "@@ -99,6 +99,7 @@\n     \"accelerate>=0.26.0\",\n     \"av==9.2.0\",  # Latest version of PyAV (10.0.0) has issues with audio stream.\n     \"beautifulsoup4\",\n+    \"blobfile\",\n     \"codecarbon==1.2.0\",\n     \"cookiecutter==1.7.3\",\n     \"dataclasses\",\n@@ -177,6 +178,7 @@\n     \"tensorflow-probability<0.24\",\n     \"tf2onnx\",\n     \"timeout-decorator\",\n+    \"tiktoken\",\n     \"timm<=0.9.16\",\n     \"tokenizers>=0.19,<0.20\",\n     \"torch\",\n@@ -311,6 +313,7 @@ def run(self):\n extras[\"video\"] = deps_list(\"decord\", \"av\")\n \n extras[\"sentencepiece\"] = deps_list(\"sentencepiece\", \"protobuf\")\n+extras[\"tiktoken\"] = deps_list(\"tiktoken\", \"blobfile\")\n extras[\"testing\"] = (\n     deps_list(\n         \"pytest\","
        },
        {
            "sha": "297da4f57c928903db4c415e7f9614d290cd67a5",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 29,
            "deletions": 12,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
            "patch": "@@ -26,10 +26,13 @@\n from tokenizers import AddedToken, Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import BPE, Unigram, WordPiece\n \n-from .utils import is_protobuf_available, requires_backends\n+from .utils import is_protobuf_available, logging, requires_backends\n from .utils.import_utils import PROTOBUF_IMPORT_ERROR\n \n \n+logger = logging.get_logger(__name__)\n+\n+\n def import_protobuf(error_message=\"\"):\n     if is_protobuf_available():\n         import google.protobuf\n@@ -1451,12 +1454,15 @@ def __init__(\n         vocab_file=None,\n         pattern=r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\",\n         add_prefix_space=False,\n+        additional_special_tokens=None,\n         *args,\n+        **kwargs,\n     ):\n         super().__init__(*args)\n         self.vocab_file = vocab_file\n         self.pattern = pattern\n         self.add_prefix_space = add_prefix_space\n+        self.additional_special_tokens = additional_special_tokens\n \n     def extract_vocab_merges_from_model(self, tiktoken_url: str):\n         try:\n@@ -1505,7 +1511,10 @@ def converted(self) -> Tokenizer:\n             ]\n         )\n         tokenizer.decoder = decoders.ByteLevel()\n+        tokenizer.add_special_tokens(self.additional_special_tokens)\n+\n         tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n+\n         return tokenizer\n \n \n@@ -1569,29 +1578,37 @@ def converted(self) -> Tokenizer:\n }\n \n \n-def convert_slow_tokenizer(transformer_tokenizer) -> Tokenizer:\n+def convert_slow_tokenizer(transformer_tokenizer, from_tiktoken=False) -> Tokenizer:\n     \"\"\"\n     Utilities to convert a slow tokenizer instance in a fast tokenizer instance.\n \n     Args:\n         transformer_tokenizer ([`~tokenization_utils_base.PreTrainedTokenizer`]):\n             Instance of a slow tokenizer to convert in the backend tokenizer for\n             [`~tokenization_utils_base.PreTrainedTokenizerFast`].\n+       from_tiktoken (bool, optional): Whether to use the `tiktoken` library to convert the tokenizer instead of sentencepiece.\n+            Defaults to False.\n \n     Return:\n         A instance of [`~tokenizers.Tokenizer`] to be used as the backend tokenizer of a\n         [`~tokenization_utils_base.PreTrainedTokenizerFast`]\n     \"\"\"\n \n     tokenizer_class_name = transformer_tokenizer.__class__.__name__\n+    if tokenizer_class_name in SLOW_TO_FAST_CONVERTERS and not from_tiktoken:\n+        converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n+        return converter_class(transformer_tokenizer).converted()\n \n-    if tokenizer_class_name not in SLOW_TO_FAST_CONVERTERS:\n-        raise ValueError(\n-            f\"An instance of tokenizer class {tokenizer_class_name} cannot be converted in a Fast tokenizer instance.\"\n-            \" No converter was found. Currently available slow->fast convertors:\"\n-            f\" {list(SLOW_TO_FAST_CONVERTERS.keys())}\"\n-        )\n-\n-    converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n-\n-    return converter_class(transformer_tokenizer).converted()\n+    else:\n+        try:\n+            logger.info(\"Converting from Tiktoken\")\n+            return TikTokenConverter(\n+                vocab_file=transformer_tokenizer.vocab_file,\n+                additional_special_tokens=transformer_tokenizer.additional_special_tokens,\n+            ).converted()\n+        except Exception:\n+            raise ValueError(\n+                f\"Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path \"\n+                f\"with a SentencePiece tokenizer.model file.\"\n+                f\"Currently available slow->fast convertors: {list(SLOW_TO_FAST_CONVERTERS.keys())}\"\n+            )"
        },
        {
            "sha": "23d686efd51d1a1698f50dcc2e65b02c53912186",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
            "patch": "@@ -6,6 +6,7 @@\n     \"accelerate\": \"accelerate>=0.26.0\",\n     \"av\": \"av==9.2.0\",\n     \"beautifulsoup4\": \"beautifulsoup4\",\n+    \"blobfile\": \"blobfile\",\n     \"codecarbon\": \"codecarbon==1.2.0\",\n     \"cookiecutter\": \"cookiecutter==1.7.3\",\n     \"dataclasses\": \"dataclasses\",\n@@ -82,6 +83,7 @@\n     \"tensorflow-probability\": \"tensorflow-probability<0.24\",\n     \"tf2onnx\": \"tf2onnx\",\n     \"timeout-decorator\": \"timeout-decorator\",\n+    \"tiktoken\": \"tiktoken\",\n     \"timm\": \"timm<=0.9.16\",\n     \"tokenizers\": \"tokenizers>=0.19,<0.20\",\n     \"torch\": \"torch\","
        },
        {
            "sha": "46f0c2f3560a23b814abcc024ff385b233d3245e",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
            "patch": "@@ -114,6 +114,7 @@\n     is_tensorflow_text_available,\n     is_tf2onnx_available,\n     is_tf_available,\n+    is_tiktoken_available,\n     is_timm_available,\n     is_tokenizers_available,\n     is_torch_available,\n@@ -1228,6 +1229,13 @@ def require_cython(test_case):\n     return unittest.skipUnless(is_cython_available(), \"test requires cython\")(test_case)\n \n \n+def require_tiktoken(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires TikToken. These tests are skipped when TikToken isn't installed.\n+    \"\"\"\n+    return unittest.skipUnless(is_tiktoken_available(), \"test requires TikToken\")(test_case)\n+\n+\n def get_gpu_count():\n     \"\"\"\n     Return the number of available gpus (regardless of whether torch, tf or jax is used)"
        },
        {
            "sha": "5e9170456a07ea454b36e6a6d97770d78c6318a5",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
            "patch": "@@ -53,6 +53,7 @@\n     is_mlx_available,\n     is_numpy_array,\n     is_offline_mode,\n+    is_protobuf_available,\n     is_remote_url,\n     is_tf_available,\n     is_tf_tensor,\n@@ -65,6 +66,7 @@\n     to_py_obj,\n )\n from .utils.chat_template_utils import _compile_jinja_template, _render_with_assistant_indices\n+from .utils.import_utils import PROTOBUF_IMPORT_ERROR\n \n \n if TYPE_CHECKING:\n@@ -75,6 +77,16 @@\n     if is_flax_available():\n         import jax.numpy as jnp  # noqa: F401\n \n+\n+def import_protobuf_decode_error(error_message=\"\"):\n+    if is_protobuf_available():\n+        from google.protobuf.message import DecodeError\n+\n+        return DecodeError\n+    else:\n+        raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))\n+\n+\n if is_tokenizers_available():\n     from tokenizers import AddedToken\n     from tokenizers import Encoding as EncodingFast\n@@ -2434,6 +2446,19 @@ def _from_pretrained(\n         # Instantiate the tokenizer.\n         try:\n             tokenizer = cls(*init_inputs, **init_kwargs)\n+        except import_protobuf_decode_error():\n+            logger.info(\n+                \"Unable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\"\n+                \"(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\",\n+            )\n+            return False\n+        except RuntimeError as e:\n+            if \"sentencepiece_processor.cc\" in str(e):\n+                logger.info(\n+                    \"Unable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\"\n+                    \"(SentencePiece RuntimeError: Tried to load SPM model with non-SPM vocab file).\",\n+                )\n+            return False\n         except OSError:\n             raise OSError(\n                 \"Unable to load vocabulary from file. \""
        },
        {
            "sha": "7d5446d7cbf233f989d722dabe80521c01458a93",
            "filename": "src/transformers/tokenization_utils_fast.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_fast.py?ref=e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
            "patch": "@@ -54,6 +54,7 @@\n TOKENIZER_FILE = \"tokenizer.json\"\n SPECIAL_TOKENS_MAP_FILE = \"special_tokens_map.json\"\n TOKENIZER_CONFIG_FILE = \"tokenizer_config.json\"\n+TIKTOKEN_VOCAB_FILE = \"tokenizer.model\"\n \n # Slow tokenizers have an additional added tokens files\n ADDED_TOKENS_FILE = \"added_tokens.json\"\n@@ -74,7 +75,7 @@\n     \"WordPiece\": WordPieceTrainer,\n }\n \n-VOCAB_FILES_NAMES = {\"tokenizer_file\": TOKENIZER_FILE}\n+VOCAB_FILES_NAMES = {\"tokenizer_file\": TOKENIZER_FILE, \"vocab_file\": TIKTOKEN_VOCAB_FILE}\n \n \n @add_end_docstrings(INIT_TOKENIZER_DOCSTRING)\n@@ -113,7 +114,7 @@ def __init__(self, *args, **kwargs):\n         elif fast_tokenizer_file is not None and not from_slow:\n             # We have a serialization from tokenizers which let us directly build the backend\n             fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\n-        elif slow_tokenizer is not None:\n+        elif slow_tokenizer:\n             # We need to convert a slow tokenizer to build the backend\n             fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n         elif gguf_file is not None:\n@@ -123,22 +124,26 @@ def __init__(self, *args, **kwargs):\n             tokenizer_dict = gguf_param[\"tokenizer\"]\n             tokenizer_config = gguf_param[\"tokenizer_config\"]\n             fast_tokenizer, additional_kwargs = convert_gguf_tokenizer(architecture, tokenizer_dict)\n-\n             kwargs.update(tokenizer_config)\n             if len(additional_kwargs) > 0:\n                 kwargs.update(additional_kwargs)\n-\n-        elif self.slow_tokenizer_class is not None:\n+        elif self.slow_tokenizer_class is not None and slow_tokenizer is not False:\n             # We need to create and convert a slow tokenizer to build the backend\n             slow_tokenizer = self.slow_tokenizer_class(*args, **kwargs)\n             fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n+        elif not slow_tokenizer:\n+            # We tried loading a slow_tokenizer with spm and failed, try to load with tiktoken\n+            self.vocab_file = kwargs.get(\"vocab_file\", None)\n+            self.additional_special_tokens = kwargs.get(\"additional_special_tokens\", [])\n+            fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)\n+            slow_tokenizer = None\n         else:\n             raise ValueError(\n                 \"Couldn't instantiate the backend tokenizer from one of: \\n\"\n                 \"(1) a `tokenizers` library serialization file, \\n\"\n                 \"(2) a slow tokenizer instance to convert or \\n\"\n                 \"(3) an equivalent slow tokenizer class to instantiate and convert. \\n\"\n-                \"You need to have sentencepiece installed to convert a slow tokenizer to a fast one.\"\n+                \"You need to have sentencepiece or tiktoken installed to convert a slow tokenizer to a fast one.\"\n             )\n \n         self._tokenizer = fast_tokenizer"
        },
        {
            "sha": "dc8e8c88f25f2ce3de53a364e0424adfe52e98ee",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
            "patch": "@@ -188,6 +188,7 @@\n     is_tensorflow_text_available,\n     is_tf2onnx_available,\n     is_tf_available,\n+    is_tiktoken_available,\n     is_timm_available,\n     is_tokenizers_available,\n     is_torch_available,"
        },
        {
            "sha": "3d03c158947740464921821dba0a4e1a30ba8343",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
            "patch": "@@ -179,6 +179,8 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _torchvision_available = _is_package_available(\"torchvision\")\n _mlx_available = _is_package_available(\"mlx\")\n _hqq_available = _is_package_available(\"hqq\")\n+_tiktoken_available = _is_package_available(\"tiktoken\")\n+_blobfile_available = _is_package_available(\"blobfile\")\n _liger_kernel_available = _is_package_available(\"liger_kernel\")\n \n \n@@ -1171,6 +1173,10 @@ def is_mlx_available():\n     return _mlx_available\n \n \n+def is_tiktoken_available():\n+    return _tiktoken_available and _blobfile_available\n+\n+\n def is_liger_kernel_available():\n     if not _liger_kernel_available:\n         return False"
        },
        {
            "sha": "a4b6c8ebeb763dd5b43dedf4486f4aff368cf4d0",
            "filename": "tests/models/llama/test_tokenization_llama.py",
            "status": "modified",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e48e5f1f13e05380e24f4f31f5fee07aa6f959eb/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py?ref=e48e5f1f13e05380e24f4f31f5fee07aa6f959eb",
            "patch": "@@ -25,15 +25,19 @@\n from transformers import (\n     SPIECE_UNDERLINE,\n     AddedToken,\n+    AutoTokenizer,\n     LlamaTokenizer,\n     LlamaTokenizerFast,\n+    PreTrainedTokenizerFast,\n )\n from transformers.convert_slow_tokenizer import convert_slow_tokenizer\n from transformers.testing_utils import (\n     get_tests_dir,\n     nested_simplify,\n     require_jinja,\n+    require_read_token,\n     require_sentencepiece,\n+    require_tiktoken,\n     require_tokenizers,\n     require_torch,\n     slow,\n@@ -832,3 +836,66 @@ def test_special_tokens_strip(self):\n         self.assertEqual(input_ids, [284, 1, 156])\n         tokens = self.tokenizer.tokenize(\"No <s> ‚ñÅHe\")\n         self.assertEqual(tokens, [\"‚ñÅNo\", \"<s>\", \"‚ñÅHe\"])  # spaces are eaten by rstrip / lstrip\n+\n+\n+@require_tiktoken\n+@require_read_token\n+class TikTokenIntegrationTests(unittest.TestCase):\n+    \"\"\"\n+    A class that regroups important test to make sure that we properly handle the special tokens.\n+    \"\"\"\n+\n+    def test_tiktoken_llama(self):\n+        model_path = \"hf-internal-testing/Llama3-Instruct-Internal\"\n+        test_text = \"This is a test sentence.\"\n+        test_tokens = [128000, 2028, 374, 264, 1296, 11914, 13, 128001]\n+        num_reserved_special_tokens = 256\n+        special_tokens = [\n+            \"<|begin_of_text|>\",\n+            \"<|end_of_text|>\",\n+            \"<|reserved_special_token_0|>\",\n+            \"<|reserved_special_token_1|>\",\n+            \"<|reserved_special_token_2|>\",\n+            \"<|reserved_special_token_3|>\",\n+            \"<|start_header_id|>\",\n+            \"<|end_header_id|>\",\n+            \"<|reserved_special_token_4|>\",\n+            \"<|eot_id|>\",\n+            \"<|python_tag|>\",  # end of turn\n+        ] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, num_reserved_special_tokens - 5)]\n+\n+        tiktoken_tokenizer = PreTrainedTokenizerFast.from_pretrained(\n+            model_path,\n+            additional_special_tokens=special_tokens,\n+            bos_token=\"<|begin_of_text|>\",\n+            eos_token=\"<|end_of_text|>\",\n+        )\n+        tokens = tiktoken_tokenizer.tokenize(\"<|begin_of_text|> \" + test_text)\n+        self.assertEqual(tokens[0], \"<|begin_of_text|>\")\n+\n+        tiktoken_tokenizer = AutoTokenizer.from_pretrained(\n+            model_path, legacy=False, additional_special_tokens=special_tokens, add_bos_token=True, add_eos_token=True\n+        )\n+        self.assertTrue(isinstance(tiktoken_tokenizer, PreTrainedTokenizerFast))\n+\n+        tokens = tiktoken_tokenizer.encode(test_text, add_special_tokens=True)\n+        self.assertEqual(tokens, test_tokens)\n+\n+        tmpdirname = tempfile.mkdtemp()\n+        tiktoken_tokenizer.save_pretrained(tmpdirname)\n+        tokenizer_reload = AutoTokenizer.from_pretrained(tmpdirname)\n+\n+        self.assertTrue(isinstance(tokenizer_reload, PreTrainedTokenizerFast))\n+        tokens = tokenizer_reload.encode(test_text, add_special_tokens=True)\n+        self.assertEqual(tokens, test_tokens)\n+        shutil.rmtree(tmpdirname)\n+\n+        tiktoken_tokenizer = AutoTokenizer.from_pretrained(\n+            model_path,\n+            additional_special_tokens=special_tokens,\n+            from_slow=True,\n+            add_bos_token=True,\n+            add_eos_token=True,\n+        )\n+        tokens = tiktoken_tokenizer.encode(test_text, add_special_tokens=True)\n+        self.assertEqual(tokens, test_tokens)"
        }
    ],
    "stats": {
        "total": 216,
        "additions": 195,
        "deletions": 21
    }
}