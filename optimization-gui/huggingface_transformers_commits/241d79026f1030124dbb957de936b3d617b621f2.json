{
    "author": "molbap",
    "message": "fix pixtral processor (#34486)\n\n* fix pixtral processor\r\n\r\n* test out full length batches + remove undue ValueError\r\n\r\n* fix up processing\r\n\r\n* fix tests\r\n\r\n* fix\r\n\r\n* last fixup\r\n\r\n* style\r\n\r\n* [run-slow] pixtral\r\n\r\n* [run-slow] pixtral\r\n\r\n* fix config key\r\n\r\n* skip torchscript tests\r\n\r\n* [run-slow] pixtral\r\n\r\n* add missing key\r\n\r\n* [run-slow] pixtral\r\n\r\n* fix docs\r\n\r\n* [run-slow] pixtral\r\n\r\n* fix wrong url for integration test\r\n\r\n* [run-slow] pixtral\r\n\r\n* pixtralVisionModel does not have a lm head\r\n\r\n* [run-slow] pixtral",
    "sha": "241d79026f1030124dbb957de936b3d617b621f2",
    "files": [
        {
            "sha": "14db51b947e664cc5877ec67665e887ff58fc272",
            "filename": "src/transformers/models/pixtral/configuration_pixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/241d79026f1030124dbb957de936b3d617b621f2/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/241d79026f1030124dbb957de936b3d617b621f2/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py?ref=241d79026f1030124dbb957de936b3d617b621f2",
            "patch": "@@ -52,6 +52,8 @@ class PixtralVisionConfig(PretrainedConfig):\n             Dropout probability for the attention layers.\n         rope_theta (`float`, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n \n     Example:\n \n@@ -82,6 +84,7 @@ def __init__(\n         hidden_act=\"gelu\",\n         attention_dropout=0.0,\n         rope_theta=10000.0,\n+        initializer_range=0.02,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -97,3 +100,4 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.rope_theta = rope_theta\n         self.head_dim = hidden_size // num_attention_heads\n+        self.initializer_range = initializer_range"
        },
        {
            "sha": "b65fbd634ba789cc1535be095e3119d8d3fdbef1",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/241d79026f1030124dbb957de936b3d617b621f2/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/241d79026f1030124dbb957de936b3d617b621f2/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=241d79026f1030124dbb957de936b3d617b621f2",
            "patch": "@@ -407,7 +407,7 @@ def _init_weights(self, module):\n         std = (\n             self.config.initializer_range\n             if hasattr(self.config, \"initializer_range\")\n-            else self.config.text_config.initializer_range\n+            else self.config.initializer_range\n         )\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):"
        },
        {
            "sha": "5913e8688d00be2b47ec3dccc915a987130a4ca3",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/241d79026f1030124dbb957de936b3d617b621f2/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/241d79026f1030124dbb957de936b3d617b621f2/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=241d79026f1030124dbb957de936b3d617b621f2",
            "patch": "@@ -206,14 +206,15 @@ def __call__(\n             if is_image_or_image_url(images):\n                 images = [[images]]\n             elif isinstance(images, list) and is_image_or_image_url(images[0]):\n-                images = [images]\n-            elif (\n-                not isinstance(images, list)\n-                and not isinstance(images[0], list)\n-                and not is_image_or_image_url(images[0][0])\n-            ):\n+                if isinstance(text, list):\n+                    images = [[im] for im in images]\n+                else:\n+                    images = [images]\n+            elif isinstance(images, list) and isinstance(images[0], list) and is_image_or_image_url(images[0][0]):\n+                pass\n+            else:\n                 raise ValueError(\n-                    \"Invalid input images. Please provide a single image or a list of images or a list of list of images.\"\n+                    \"Invalid input images. Please provide a single image, a list of images, or a list of lists of images.\"\n                 )\n             images = [[load_image(im) for im in sample] for sample in images]\n             image_inputs = self.image_processor(images, patch_size=self.patch_size, **output_kwargs[\"images_kwargs\"])"
        },
        {
            "sha": "0c36cb5a4e05544859cfb37be78698acc7308b68",
            "filename": "tests/models/pixtral/test_modeling_pixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 39,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/241d79026f1030124dbb957de936b3d617b621f2/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/241d79026f1030124dbb957de936b3d617b621f2/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py?ref=241d79026f1030124dbb957de936b3d617b621f2",
            "patch": "@@ -14,22 +14,16 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Pixtral model.\"\"\"\n \n-import gc\n import unittest\n \n-import requests\n-\n from transformers import (\n-    AutoProcessor,\n     PixtralVisionConfig,\n     PixtralVisionModel,\n     is_torch_available,\n     is_vision_available,\n )\n from transformers.testing_utils import (\n-    require_bitsandbytes,\n     require_torch,\n-    slow,\n     torch_device,\n )\n \n@@ -43,7 +37,7 @@\n     is_torch_greater_or_equal_than_2_0 = False\n \n if is_vision_available():\n-    from PIL import Image\n+    pass\n \n \n class PixtralVisionModelTester:\n@@ -148,6 +142,7 @@ class PixtralVisionModelModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (PixtralVisionModel,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n+    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = PixtralVisionModelTester(self)\n@@ -258,35 +253,3 @@ def test_disk_offload_safetensors(self):\n     @unittest.skip(reason=\"Not supported yet\")\n     def test_determinism(self):\n         pass\n-\n-\n-@require_torch\n-class PixtralVisionModelIntegrationTest(unittest.TestCase):\n-    def setUp(self):\n-        self.processor = AutoProcessor.from_pretrained(\"hf-internal-testing/pixtral-12b\")\n-\n-    def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n-\n-    @slow\n-    @require_bitsandbytes\n-    def test_small_model_integration_test(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n-        model = PixtralVisionModel.from_pretrained(\"hf-internal-testing/pixtral-12b\", load_in_4bit=True)\n-\n-        prompt = \"<s>[INST][IMG]\\nWhat are the things I should be cautious about when I visit this place?[/INST]\"\n-        image_file = \"https://pixtral-vl.github.io/static/images/view.jpg\"\n-        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-        inputs = self.processor(prompt, raw_image, return_tensors=\"pt\")\n-\n-        EXPECTED_INPUT_IDS = torch.tensor([[1, 32000, 28705, 13, 11123, 28747, 1824, 460, 272, 1722,315, 1023, 347, 13831, 925, 684, 739, 315, 3251, 456,1633, 28804, 13, 4816, 8048, 12738, 28747]])  # fmt: skip\n-        self.assertTrue(torch.equal(inputs[\"input_ids\"], EXPECTED_INPUT_IDS))\n-\n-        output = model.generate(**inputs, max_new_tokens=20)\n-        EXPECTED_DECODED_TEXT = \"\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT: When visiting this place, there are a few things one should be cautious about. Firstly,\"  # fmt: skip\n-\n-        self.assertEqual(\n-            self.processor.decode(output[0], skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n-        )"
        },
        {
            "sha": "c3496dff3cdf8177ced436f1ef9b20e190057249",
            "filename": "tests/models/pixtral/test_processor_pixtral.py",
            "status": "modified",
            "additions": 20,
            "deletions": 1,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/241d79026f1030124dbb957de936b3d617b621f2/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/241d79026f1030124dbb957de936b3d617b621f2/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py?ref=241d79026f1030124dbb957de936b3d617b621f2",
            "patch": "@@ -171,7 +171,7 @@ def test_processor_with_multiple_images_single_list(self):\n             input_ids[0].tolist(),\n             # Equivalent to [\"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END][IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the difference between these two images? ASSISTANT:\"]\n             [21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n-        )\n+                    )\n         # fmt: on\n \n         # Test passing in a url\n@@ -246,6 +246,25 @@ def test_processor_with_multiple_images_multiple_lists(self):\n         )\n         # fmt: on\n \n+    def test_processor_returns_full_length_batches(self):\n+        # to avoid https://github.com/huggingface/transformers/issues/34204\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        prompt_string = [\n+            \"USER: [IMG]\\nWhat's the content of the image? ASSISTANT:\",\n+        ] * 5\n+        processor.tokenizer.pad_token = \"</s>\"\n+        image_inputs = [self.image_0] * 5\n+\n+        # Make small for checking image token expansion\n+        processor.image_processor.size = {\"longest_edge\": 30}\n+        processor.image_processor.patch_size = {\"height\": 2, \"width\": 2}\n+\n+        # Test passing in an image\n+        inputs_image = processor(text=prompt_string, images=image_inputs, return_tensors=\"pt\", padding=True)\n+        self.assertIn(\"input_ids\", inputs_image)\n+        self.assertTrue(len(inputs_image[\"input_ids\"]) == 5)\n+        self.assertTrue(len(inputs_image[\"pixel_values\"]) == 5)\n+\n     # Override as PixtralProcessor needs nested images to work properly with batched inputs\n     @require_vision\n     def prepare_image_inputs(self, batch_size: Optional[int] = None):"
        }
    ],
    "stats": {
        "total": 83,
        "additions": 35,
        "deletions": 48
    }
}