{
    "author": "remi-or",
    "message": "Continuous batching refactor (#40426)\n\n* Rework of the CB example\n\n* Further rework of CB example\n\n* Refactor PA cache, slice on tokens, add debug prints -- WIP\n\n* Slice cache -- WIP\n\n* Added a mechanism to check batched outputs in CB script\n\n* Less logging, debug flag for slice, !better reset! -- WIP\n\n* QOL and safety margins\n\n* Refactor and style\n\n* Better saving of cb example\n\n* Fix\n\n* Fixes and QOL\n\n* Mor einformations about metrics\n\n* Further logging\n\n* Style\n\n* Licenses\n\n* Removed some comments\n\n* Add a slice input flag\n\n* Fix in example\n\n* Added back some open-telemetry deps\n\n* Removed some aux function\n\n* Added FA2 option to example script\n\n* Fixed math (all of it)\n\n* Added a simple example\n\n* Renamed core to classes\n\n* Made allocation of attention mask optionnal\n\n* Style",
    "sha": "34108a2230c4591c9a20e299ca22edc147c3918f",
    "files": [
        {
            "sha": "62150f3a10b774e26928447e85ee778e701c9801",
            "filename": "examples/metrics-monitoring/README.md",
            "status": "modified",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/34108a2230c4591c9a20e299ca22edc147c3918f/examples%2Fmetrics-monitoring%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/34108a2230c4591c9a20e299ca22edc147c3918f/examples%2Fmetrics-monitoring%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmetrics-monitoring%2FREADME.md?ref=34108a2230c4591c9a20e299ca22edc147c3918f",
            "patch": "@@ -2,3 +2,40 @@\n \n ## Continuous Batching Metrics in Transformers\n \n+To setup metric monitoring with continuous batching, you will want to have tempo and prometheus running.\n+\n+For this, we provide a docker compose image in `examples/metrics-monitoring`.\n+\n+To run it:\n+\n+```sh\n+cd examples/metrics-monitoring\n+docker compose up\n+```\n+\n+Then, in your srcipt running CB, you will need to create a MeterProvider and TracerProvider as follows:\n+\n+```py\n+from opentelemetry import metrics, trace\n+from opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter\n+from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n+from opentelemetry.sdk.metrics import MeterProvider\n+from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\n+from opentelemetry.sdk.resources import Resource\n+from opentelemetry.sdk.trace import TracerProvider\n+from opentelemetry.sdk.trace.export import BatchSpanProcessor\n+\n+resource = Resource.create({\"service.name\": \"transformers\"})\n+\n+metrics_exporter = PeriodicExportingMetricReader(\n+    OTLPMetricExporter(endpoint=\"http://localhost:9090/api/v1/otlp/v1/metrics\"),  # Uses OTEL_EXPORTER_OTLP_METRICS_ENDPOINT env var\n+    export_interval_millis=1000\n+)\n+meter_provider = MeterProvider(resource=resource, metric_readers=[metrics_exporter])\n+metrics.set_meter_provider(meter_provider)\n+\n+trace_exporter = OTLPSpanExporter(endpoint=\"http://localhost:4318/v1/traces\")  # Uses OTEL_EXPORTER_OTLP_TRACES_ENDPOINT env var\n+tracer_provider = TracerProvider(resource=resource)\n+tracer_provider.add_span_processor(BatchSpanProcessor(trace_exporter))\n+trace.set_tracer_provider(tracer_provider)\n+```"
        },
        {
            "sha": "b5ad94ed3f11b8d0bd0f63f4c35384a6f83c06b1",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 244,
            "deletions": 87,
            "changes": 331,
            "blob_url": "https://github.com/huggingface/transformers/blob/34108a2230c4591c9a20e299ca22edc147c3918f/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34108a2230c4591c9a20e299ca22edc147c3918f/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=34108a2230c4591c9a20e299ca22edc147c3918f",
            "patch": "@@ -1,4 +1,22 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import json\n+import os\n import time\n+from typing import Optional\n \n import datasets\n import torch\n@@ -7,108 +25,247 @@\n from transformers.generation import GenerationConfig\n \n \n-torch.set_float32_matmul_precision(\"high\")\n+MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n \n-model_id = \"meta-llama/Llama-3.2-3b-Instruct\"\n-model = (\n-    AutoModelForCausalLM.from_pretrained(\n-        model_id,\n-        attn_implementation=\"paged_attention|kernels-community/flash-attn\",\n-        dtype=torch.bfloat16,\n+\n+def generate_simple(\n+    attn_implementation: str, simple_batch_inputs: list[int], generation_config: GenerationConfig\n+) -> list[str]:\n+    attn_implementation = {\n+        \"sdpa_paged\": \"sdpa\",\n+        \"eager_paged\": \"eager\",\n+        \"flash_paged\": \"flash_attention_2\",\n+    }[attn_implementation]\n+\n+    model = (\n+        AutoModelForCausalLM.from_pretrained(\n+            MODEL_ID,\n+            torch_dtype=torch.bfloat16,\n+            attn_implementation=attn_implementation,\n+        )\n+        .cuda()\n+        .eval()\n     )\n-    .eval()\n-    .cuda()\n-)\n-tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n-\n-generation_config = GenerationConfig(\n-    max_new_tokens=512,\n-    # use_cuda_graph=False,\n-    eos_token_id=tokenizer.eos_token_id,\n-    pad_token_id=tokenizer.pad_token_id,\n-    do_sample=False,\n-)\n-\n-train_dataset = datasets.load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")\n-train_dataset = train_dataset.select(range(500))  # Use only 5 examples for the simple version\n-print(\"--- Running CB Generation Example ---\")\n-\n-\n-def tokenize_function(examples):\n-    return tokenizer(examples[\"question\"])\n-\n-\n-tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n-simple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\n-\n-start_time_simple = time.time()\n-model.forward = torch.compile(model.forward, mode=\"max-autotune-no-cudagraphs\")\n-batch_outputs = model.generate_batch(\n-    inputs=simple_batch_inputs,\n-    generation_config=generation_config,\n-)\n-end_time_simple = time.time()\n-token_count = 0\n-for request in batch_outputs:\n-    input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=False)\n+\n+    decoded_outputs = []\n+    for input_ids in simple_batch_inputs:\n+        input_ids = torch.tensor([input_ids]).to(\"cuda\")\n+        attention_mask = torch.ones_like(input_ids)\n+        outputs = model.generate(input_ids, attention_mask=attention_mask, generation_config=generation_config)\n+        generated_tokens = outputs[0][input_ids.shape[1] :]\n+        decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n+        decoded_outputs.append(decoded_output)\n+\n+    return decoded_outputs\n+\n+\n+def setup_metrics():\n     try:\n-        output_text = tokenizer.decode(batch_outputs[request].generated_tokens, skip_special_tokens=False)\n-        token_count += len(batch_outputs[request].generated_tokens[1:])\n+        from opentelemetry import metrics, trace\n+        from opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter\n+        from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n+        from opentelemetry.sdk.metrics import MeterProvider\n+        from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\n+        from opentelemetry.sdk.resources import Resource\n+        from opentelemetry.sdk.trace import TracerProvider\n+        from opentelemetry.sdk.trace.export import BatchSpanProcessor\n+\n+        resource = Resource.create({\"service.name\": \"transformers\"})\n+        metrics_exporter = PeriodicExportingMetricReader(\n+            OTLPMetricExporter(\n+                endpoint=\"http://localhost:9090/api/v1/otlp/v1/metrics\"\n+            ),  # Uses OTEL_EXPORTER_OTLP_METRICS_ENDPOINT env var\n+            export_interval_millis=1000,\n+        )\n+        meter_provider = MeterProvider(resource=resource, metric_readers=[metrics_exporter])\n+        metrics.set_meter_provider(meter_provider)\n+        trace_exporter = OTLPSpanExporter(\n+            endpoint=\"http://localhost:4318/v1/traces\"\n+        )  # Uses OTEL_EXPORTER_OTLP_TRACES_ENDPOINT env var\n+        tracer_provider = TracerProvider(resource=resource)\n+        tracer_provider.add_span_processor(BatchSpanProcessor(trace_exporter))\n+        trace.set_tracer_provider(tracer_provider)\n     except Exception as e:\n-        print(f\"Decoding failed for request {request}: {e}\")\n-        token_count += len(batch_outputs[request].generated_tokens[1:])\n-        output_text = tokenizer.decode(batch_outputs[request].generated_tokens[1:], skip_special_tokens=False)\n-    if len(output_text) > 0:\n+        print(f\"Error setting up metrics: {e}\")\n+\n+\n+def batch_generate(\n+    model: AutoModelForCausalLM,\n+    simple_batch_inputs: list,\n+    generation_config: GenerationConfig,\n+    tokenizer: AutoTokenizer,\n+    displayed_samples: int = 0,  # -1: no display, 0: display stats, >0: display inputs and some outputs\n+    output_file: Optional[str] = None,\n+    expected_outputs: Optional[list[str]] = None,\n+    slice_inputs: bool = True,\n+) -> tuple[float, float]:\n+    # Actual batch generation\n+    if displayed_samples >= 0:\n+        print(\"--- Running CB Generation Example ---\")\n+    start_time_simple = time.time()\n+    batch_outputs = model.generate_batch(\n+        inputs=simple_batch_inputs,\n+        generation_config=generation_config,\n+        slice_inputs=slice_inputs,  # TODO: move this to the generation config\n+    )\n+    end_time_simple = time.time()\n+    if displayed_samples >= 0:\n+        print(\"Done with batch generation.\")\n+\n+    # Decode outputs\n+    token_count = 0\n+    data = []\n+    for i, request in enumerate(batch_outputs):\n+        input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=True)\n+        data.append({\"input\": input_text})\n+\n+        # Try to decode the output\n+        try:\n+            output_text = tokenizer.decode(batch_outputs[request].generated_tokens, skip_special_tokens=True)\n+            token_count += len(batch_outputs[request].generated_tokens[1:])\n+            data[-1][\"output\"] = output_text\n+        except Exception as e:\n+            print(f\"Decoding failed for request {request}: {e}\")\n+            data[-1][\"output\"] = \"__ERROR__\"\n+            continue\n+\n+        # Display sample if asked\n+        if i < displayed_samples:\n+            if len(output_text) > 0:\n+                print(\"-\" * 20)\n+                print(f\"{request} Input:  {input_text}\")\n+                print(f\"{request} Output: {output_text}\")\n+            else:\n+                print(f\"{request} Input:  {input_text}\")\n+                print(\"[WARN]\")\n+                print(f\"{request} Output was empty!\")\n+\n+        # Compare with classic generate if asked\n+        if expected_outputs is not None:\n+            matches = output_text == expected_outputs[i]\n+            data[-1][\"ref\"] = expected_outputs[i]\n+            data[-1][\"matches\"] = matches\n+            print(f\"Request {i} matches\" if matches else f\"Request {i} does NOT match!\")\n+\n+    # Compute stats and maybe print them\n+    gen_time = end_time_simple - start_time_simple\n+    tok_per_sec = token_count / gen_time\n+    if displayed_samples >= 0:\n         print(\"-\" * 20)\n-        print(f\"{request} Input:  {input_text}\")\n-        print(f\"{request} Output: {output_text}\")\n-    else:\n-        print(\"\", end=\"\\r\\r\\r\\r\")\n-print(\"-\" * 20)\n-print(\"--- Finished CB Generation Example ---\\n\\n\")\n+        print(\"--- Finished CB Generation Example ---\\n\")\n+        print(f\"CB generation took: {gen_time:.2f} seconds for {token_count} tokens. {tok_per_sec:.2f}tok/s\")\n+    stats = {\n+        \"num_blocks\": generation_config.num_blocks,\n+        \"max_batch_tokens\": generation_config.max_batch_tokens,\n+        \"gen_time\": gen_time,\n+        \"token_count\": token_count,\n+        \"tok_per_sec\": tok_per_sec,\n+    }\n \n+    # If an output file is provided, save the reordered data to it\n+    data.sort(key=lambda x: x[\"input\"])\n+    data = [stats] + data\n+    if output_file is not None:\n+        with open(output_file, \"w\") as f:\n+            json.dump(data, f, indent=4)\n \n-print(\n-    f\"CB generation took: {end_time_simple - start_time_simple:.2f} seconds for {token_count} tokens. {token_count / (end_time_simple - start_time_simple)}tok/s\"\n-)\n+    return gen_time, tok_per_sec\n \n \n-# train_dataset = train_dataset.select(range(5))  # Use only 5 examples for the simple version\n+if __name__ == \"__main__\":\n+    # Parse args\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--num-blocks\", \"-n\", type=int, default=None)\n+    parser.add_argument(\"--max-batch-tokens\", \"-b\", type=int, default=None)\n \n-# tokenized_test_prompts = tokenizer(_TEST_PROMPTS, padding=True, padding_side=\"left\", truncation=True, max_length=512)\n-# simple_batch_inputs = list(tokenized_test_prompts[\"input_ids\"])\n+    parser.add_argument(\n+        \"--attn\", type=str, default=\"paged_attention|kernels-community/flash-attn\", help=\"Attention implementation\"\n+    )\n+    parser.add_argument(\"--matmul-precision\", \"-mp\", type=str, default=\"high\")  # set to \"none\" to disable\n+    parser.add_argument(\"--slice-inputs\", action=\"store_true\", default=False)\n+    parser.add_argument(\"--use-cuda-graph\", action=\"store_true\", default=False)\n+    parser.add_argument(\"--compile\", action=\"store_true\", default=False)\n \n-# def tokenize_function(examples):\n-#     # Truncate to avoid overly long prompts exceeding max context length\n-#     return tokenizer(examples[\"question\"], padding=True, truncation=True, max_length=512)\n+    parser.add_argument(\"--samples\", type=int, default=500)\n+    parser.add_argument(\"--displayed\", type=int, default=0, help=\"Number of samples to display\")\n+    parser.add_argument(\"--output-file\", type=str, default=None)\n+    parser.add_argument(\"--compare\", action=\"store_true\", default=False)\n+    parser.add_argument(\"--metrics\", action=\"store_true\", default=False)\n+    args = parser.parse_args()\n \n+    # If turned on, we setup metrics\n+    if args.metrics:\n+        setup_metrics()\n \n-# tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n-# simple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\n+    # Set matmul precision if not none\n+    if args.matmul_precision != \"none\":\n+        torch.set_float32_matmul_precision(args.matmul_precision)\n \n+    # Prepare model\n+    model = AutoModelForCausalLM.from_pretrained(\n+        MODEL_ID,\n+        attn_implementation=args.attn,\n+        dtype=torch.bfloat16,\n+    )\n+    model = model.cuda().eval()\n \n-# model.config.attn_implementation = \"sdpa\"\n-# start_time_simple = time.time()\n-# batch_size = 64\n-# full_outputs = []\n-# from tqdm import tqdm\n+    # If turned on, we compile the model\n+    if args.compile:\n+        model.forward = torch.compile(model.forward, mode=\"max-autotune-no-cudagraphs\")\n+    if args.slice_inputs:\n+        assert not args.compile, \"Slicing inputs requires is not the model to be compiled\"\n+        assert not args.use_cuda_graph, \"Slicing inputs is not compatible with cuda graphs\"\n \n-# for i in tqdm(range(0, len(simple_batch_inputs)-batch_size, batch_size)):\n-#     outputs = model.generate(\n-#         torch.tensor(simple_batch_inputs[i:i+batch_size], device=model.device),\n-#         generation_config=GenerationConfig(\n-#             max_new_tokens=16, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id\n-#         ),\n-#     )\n-#     full_outputs.extend(outputs.tolist())\n+    # Prepare tokenizer and dataset\n+    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n+    dataset = datasets.load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")\n+    dataset = dataset.select(range(args.samples))  # Use only 5 examples for the simple version\n+    tokenized_datasets = dataset.map(lambda x: tokenizer(x[\"question\"]), batched=True)\n+    simple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\n \n-# end_time_simple = time.time()\n-# print(f\"\\nSimple batch generation took: {end_time_simple - start_time_simple:.2f} seconds\")\n+    # Prepare generation config\n+    generation_config = GenerationConfig(\n+        max_new_tokens=512,\n+        use_cuda_graph=args.use_cuda_graph,\n+        eos_token_id=tokenizer.eos_token_id,\n+        pad_token_id=tokenizer.pad_token_id,\n+        do_sample=False,\n+        num_blocks=args.num_blocks,\n+        max_batch_tokens=args.max_batch_tokens,\n+    )\n+\n+    # If we need to compare, we need to generate the reference outputs\n+    expected_outputs = generate_simple(args.attn, simple_batch_inputs, generation_config) if args.compare else None\n+\n+    # If no output file is provided, we pick a name based on the args\n+    if args.output_file is None:\n+        os.makedirs(\"runs/cb\", exist_ok=True)\n+        attn = args.attn.replace(\"|\", \"_\").replace(\"/\", \"_\")\n+        args.output_file = (\n+            f\"runs/cb/{args.num_blocks}_{args.max_batch_tokens}_{attn}_{args.matmul_precision}_{args.samples}.json\"\n+        )\n+\n+    # Run warmup batch generation\n+    batch_generate(\n+        model,\n+        simple_batch_inputs[: min(5, args.samples)],\n+        generation_config,\n+        tokenizer,\n+        displayed_samples=-1,\n+        slice_inputs=args.slice_inputs,\n+    )\n+\n+    # Run batch generation\n+    gen_time, tok_per_sec = batch_generate(\n+        model,\n+        simple_batch_inputs,\n+        generation_config,\n+        tokenizer,\n+        displayed_samples=args.displayed,\n+        output_file=args.output_file,\n+        expected_outputs=expected_outputs,\n+        slice_inputs=args.slice_inputs,\n+    )\n \n-# print(\"\\nResults from simple generate_batch:\")\n-# for i, request in enumerate(full_outputs):\n-#     output_text = tokenizer.decode(request, skip_special_tokens=False)\n-#     print(\"-\" * 20)\n-#     print(f\"  Output: {output_text}\")\n-# print(\"-\" * 20)\n-# print(\"--- Finished Simple Batch Generation Example ---\\n\\n\")\n+# Example usage:\n+# python examples/pytorch/continuous_batching.py --num-blocks 369 --max-batch-tokens 23 --attn sdpa_paged -mp none --samples 1 --displayed 0 --output-file sliced.json"
        },
        {
            "sha": "3ae5e3d838702dc813de95354cd326a3bf746e50",
            "filename": "examples/pytorch/continuous_batching_simple.py",
            "status": "added",
            "additions": 110,
            "deletions": 0,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/34108a2230c4591c9a20e299ca22edc147c3918f/examples%2Fpytorch%2Fcontinuous_batching_simple.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34108a2230c4591c9a20e299ca22edc147c3918f/examples%2Fpytorch%2Fcontinuous_batching_simple.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching_simple.py?ref=34108a2230c4591c9a20e299ca22edc147c3918f",
            "patch": "@@ -0,0 +1,110 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import time\n+\n+import datasets\n+import torch\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers.generation import GenerationConfig\n+\n+\n+MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n+DISPLAYED_SAMPLES = 3\n+\n+\n+if __name__ == \"__main__\":\n+    # Parse args\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--num-blocks\", \"-n\", type=int, default=None)\n+    parser.add_argument(\"--max-batch-tokens\", \"-b\", type=int, default=None)\n+    parser.add_argument(\n+        \"--attn\", type=str, default=\"paged_attention|kernels-community/flash-attn\", help=\"Attention implementation\"\n+    )\n+    parser.add_argument(\"--samples\", type=int, default=500)\n+    args = parser.parse_args()\n+\n+    # Prepare model\n+    model = AutoModelForCausalLM.from_pretrained(\n+        MODEL_ID,\n+        attn_implementation=args.attn,\n+        dtype=torch.bfloat16,\n+    )\n+    model = model.cuda().eval()\n+\n+    # Prepare tokenizer and dataset\n+    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n+    dataset = datasets.load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")\n+    dataset = dataset.select(range(args.samples))\n+    tokenized_datasets = dataset.map(lambda x: tokenizer(x[\"question\"]), batched=True)\n+    simple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\n+\n+    # Prepare generation config\n+    generation_config = GenerationConfig(\n+        max_new_tokens=512,\n+        use_cuda_graph=False,  # Not supported for simple version\n+        eos_token_id=tokenizer.eos_token_id,\n+        pad_token_id=tokenizer.pad_token_id,\n+        do_sample=False,\n+        num_blocks=args.num_blocks,\n+        max_batch_tokens=args.max_batch_tokens,\n+    )\n+\n+    # Warmup iterations\n+    _ = model.generate_batch(\n+        inputs=simple_batch_inputs[: min(5, args.samples)],\n+        generation_config=generation_config,\n+        slice_inputs=True,\n+    )\n+\n+    # Actual batch generation\n+    print(\"--- Running CB Generation Example ---\")\n+    start_time = time.time()\n+    batch_outputs = model.generate_batch(\n+        inputs=simple_batch_inputs,\n+        generation_config=generation_config,\n+        slice_inputs=True,\n+    )\n+    end_time = time.time()\n+    print(\"Done with batch generation.\")\n+\n+    # Decode outputs\n+    token_count = 0\n+    for i, request in enumerate(batch_outputs):\n+        input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=True)\n+        # Try to decode the output\n+        try:\n+            output_text = tokenizer.decode(batch_outputs[request].generated_tokens, skip_special_tokens=True)\n+            token_count += len(batch_outputs[request].generated_tokens[1:])\n+        except Exception as e:\n+            print(f\"Decoding failed for request {request}: {e}\")\n+            continue\n+\n+        # Display sample if asked\n+        if i < DISPLAYED_SAMPLES:\n+            print(\"-\" * 20)\n+            print(f\"{request} Input:  {input_text}\")\n+            if len(output_text) > 0:\n+                print(f\"{request} Output: {output_text}\")\n+            else:\n+                print(f\"[WARN] {request} Output was empty!\")\n+\n+    # Compute stats and maybe print them\n+    gen_time = end_time - start_time\n+    tok_per_sec = token_count / gen_time\n+    print(\"-\" * 20)\n+    print(\"--- Finished CB Generation Example ---\\n\")\n+    print(f\"CB generation took: {gen_time:.2f} seconds for {token_count} tokens. {tok_per_sec:.2f}tok/s\")"
        },
        {
            "sha": "79bb0f9ef0d8f3f7829ec2673afc2d943ca42e00",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/34108a2230c4591c9a20e299ca22edc147c3918f/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34108a2230c4591c9a20e299ca22edc147c3918f/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=34108a2230c4591c9a20e299ca22edc147c3918f",
            "patch": "@@ -445,7 +445,7 @@ def run(self):\n extras[\"benchmark\"] = deps_list(\"optimum-benchmark\")\n \n # OpenTelemetry dependencies for metrics collection in continuous batching\n-extras[\"open-telemetry\"] = deps_list(\"opentelemetry-api\")\n+extras[\"open-telemetry\"] = deps_list(\"opentelemetry-api\") + [\"opentelemetry-exporter-otlp\", \"opentelemetry-sdk\"]\n \n # when modifying the following list, make sure to update src/transformers/dependency_versions_check.py\n install_requires = ["
        },
        {
            "sha": "2d903da05b620a67aa7e325c0cb5800417910cc4",
            "filename": "src/transformers/generation/continuous_batching.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1459,
            "changes": 1459,
            "blob_url": "https://github.com/huggingface/transformers/blob/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49e168ff08ed837f1d7c4f4dccac4ddc427f887a/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py?ref=49e168ff08ed837f1d7c4f4dccac4ddc427f887a",
            "patch": "@@ -1,1459 +0,0 @@\n-# coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team.\n-# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import queue\n-import threading\n-import time\n-from abc import ABC, abstractmethod\n-from collections import deque\n-from dataclasses import dataclass, field\n-from enum import Enum\n-from functools import partial\n-from typing import Optional, Union\n-\n-import torch\n-import torch.nn as nn\n-from tokenizers.decoders import DecodeStream\n-from tqdm import tqdm\n-\n-from ..configuration_utils import PretrainedConfig\n-from ..generation.configuration_utils import GenerationConfig\n-from ..tokenization_utils_fast import PreTrainedTokenizerFast\n-from ..utils.logging import logging\n-from ..utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n-\n-\n-class RequestStatus(Enum):\n-    \"\"\"Status of a generation request through its lifecycle.\"\"\"\n-\n-    PENDING = \"pending\"\n-    PREFILLING = \"prefilling\"\n-    PREFILLING_SPLIT = \"prefilling_split\"\n-    SPLIT_PENDING_REMAINDER = \"split_pending_remainder\"\n-    DECODING = \"decoding\"\n-    FINISHED = \"finished\"\n-    FAILED = \"failed\"\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-@dataclass\n-class GenerationOutput:\n-    \"\"\"Tracks the output of a generation request.\n-\n-    Attributes:\n-        request_id (str): The ID of the generation request.\n-        prompt_ids (list[int]): The IDs of the prompt tokens.\n-        generated_tokens (list[int]): The generated tokens.\n-        logprobs (list[float]): The log probabilities of the generated tokens.\n-        error (Optional[str]): Any error message associated with the request. When None, the request was successful.\n-    \"\"\"\n-\n-    request_id: str\n-    prompt_ids: list[int] = field(default_factory=list)\n-    generated_tokens: list[int] = field(default_factory=list)\n-    logprobs: list[float] = field(default_factory=list)\n-    error: Optional[str] = None\n-    status: RequestStatus = RequestStatus.PENDING\n-    created_time: float = field(default_factory=time.time)\n-    next_token: Optional[int] = field(default_factory=int)\n-\n-\n-@dataclass\n-class RequestState:\n-    \"\"\"Tracks the state of a generation request through its lifecycle.\n-\n-    Attributes:\n-        status (RequestStatus): can be one of PENDING, PREFILLING, PREFILLING_SPLIT,\n-                                SPLIT_PENDING_REMAINDER, DECODING, FINISHED, FAILED\n-    \"\"\"\n-\n-    # Required fields\n-    request_id: str\n-    prompt_ids: Optional[list[int]] = None  # the one being processed\n-    full_prompt_ids: Optional[list[int]] = None  # the full prompt\n-    remaining_prompt_ids: list[int] = field(default_factory=list)  # For split requests\n-    static_outputs: list[int] = field(default_factory=list)\n-    allocated_blocks: list[int] = field(default_factory=list)\n-    position_offset: int = 0  # Current position in the sequence for position_ids\n-    status: RequestStatus = RequestStatus.PENDING\n-    max_new_tokens: int = 20\n-    eos_token_id: int = -1\n-    created_time: float = field(default_factory=time.time)\n-    error: Optional[str] = None\n-    next_token: Optional[str] = None\n-\n-    def current_len(self) -> int:\n-        \"\"\"Get the current length of the sequence (prompt + generated tokens).\"\"\"\n-        return self.position_offset\n-\n-    def generated_len(self) -> int:\n-        \"\"\"Get the number of tokens generated so far.\"\"\"\n-        return len(self.static_outputs)\n-\n-    @traced\n-    def update_with_token(self, token_id: int) -> bool:\n-        \"\"\"Update the request with a newly generated token and check for completion.\n-\n-        Args:\n-            token_id: The token ID to add to the output sequence\n-\n-        Returns:\n-            bool: True if the request is now complete, False otherwise\n-        \"\"\"\n-        # Only update if we're in decoding state\n-        if self.status != RequestStatus.DECODING:\n-            return False\n-\n-        is_eos = token_id == self.eos_token_id and self.eos_token_id != -1\n-        is_max_len = self.generated_len() >= self.max_new_tokens\n-\n-        # Only add the token if we're not finishing due to max length\n-        # (EOS tokens should still be added to the output)\n-        if not (is_max_len and not is_eos):\n-            self.static_outputs.extend([token_id])\n-\n-        if is_eos or is_max_len:\n-            self.status = RequestStatus.FINISHED\n-            return True\n-        return False\n-\n-    def __repr__(self):\n-        return f\"RequestState(\\n\\trequest_id={self.request_id},\\n\\tstatus={self.status},\\n\\tout_tokens={self.generated_len()},\\n\\tquery_length={len(self.prompt_ids)}, \\n\\tremaining_tokens={len(self.remaining_prompt_ids)}, \\n\\tkv_length={self.position_offset}\\n\\tfull_prompt_lenght={len(self.full_prompt_ids)},\\n\\tallocated_blocks={self.allocated_blocks},\\n\\tgenerated_tokens={self.static_outputs}\\n)\"\n-\n-    def to_generation_output(self):\n-        \"\"\"Convert the request state to a GenerationOutput object.\"\"\"\n-        return GenerationOutput(\n-            request_id=self.request_id,\n-            prompt_ids=self.full_prompt_ids,\n-            status=self.status,\n-            generated_tokens=self.static_outputs,\n-            logprobs=[],\n-            error=self.error,\n-            next_token=self.next_token,\n-        )\n-\n-\n-@attach_tracer()\n-class PagedAttentionCache:\n-    def __init__(\n-        self,\n-        config: PretrainedConfig,\n-        generation_config: GenerationConfig,\n-        device: torch.device,\n-        dtype: torch.dtype = torch.float16,\n-        num_requests: int = 100,\n-        layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n-        tp_size: Optional[int] = None,\n-    ) -> None:\n-        \"\"\"Initialize a paged attention cache for efficient memory usage.\n-\n-        Args:\n-            config: Model configuration\n-            generation_config: Generation configuration containing cache parameters\n-            device: Device for the cache tensors\n-            dtype: Data type for the cache tensors\n-            layer_device_map: Optional mapping of layer indices to devices\n-            initial_prompt_shapes: Optional sample prompts to help calculate optimal cache size\n-        \"\"\"\n-        # Extract model dimensions\n-        self.num_key_value_heads = (\n-            config.num_attention_heads\n-            if getattr(config, \"num_key_value_heads\", None) is None\n-            else config.num_key_value_heads\n-        )\n-        num_key_value_heads = self.num_key_value_heads\n-        if tp_size is not None and tp_size > 1:\n-            if num_key_value_heads % tp_size != 0:\n-                raise ValueError(\n-                    f\"Number of key value heads {num_key_value_heads} must be divisible by tensor parallel size {tp_size}.\"\n-                )\n-            # If the model is using tensor parallelism, we need to adjust the number of heads accordingly.\n-            # self.num_key_value_heads //= tp_size\n-\n-        self.head_dim = (\n-            config.head_dim\n-            if hasattr(config, \"head_dim\") and config.head_dim is not None\n-            else config.hidden_size // config.num_attention_heads\n-        )\n-        self.num_hidden_layers = config.num_hidden_layers\n-\n-        # Calculate optimal block size and number if not provided\n-        num_blocks = getattr(generation_config, \"num_blocks\", 1024)\n-        block_size = getattr(generation_config, \"block_size\", 32)\n-        max_memory_percent = getattr(generation_config, \"max_memory\", 0.9)\n-        max_batch_tokens = getattr(generation_config, \"max_batch_tokens\", 256)\n-        if num_blocks is None or max_batch_tokens is None:\n-            num_blocks, max_batch_tokens = compute_optimal_blocks(\n-                generation_config.max_new_tokens,\n-                block_size=block_size,\n-                head_dim=self.head_dim,\n-                num_layers=self.num_hidden_layers,\n-                num_heads=self.num_key_value_heads,\n-                max_memory_percent=max_memory_percent,\n-                dtype=dtype,\n-                num_blocks=num_blocks,\n-            )\n-        logger.warning(\n-            f\"Using calculated num_blocks={num_blocks}, block_size={block_size}, max concurrent requests {max_batch_tokens}\"\n-        )\n-        self.max_batch_tokens = max_batch_tokens\n-        self.block_size = block_size\n-        self.num_blocks = num_blocks\n-        self.cache_shape = (num_key_value_heads, num_blocks, self.block_size, self.head_dim)\n-\n-        self.dtype = dtype\n-        self.device = device\n-\n-        self.key_cache: list[torch.Tensor] = []\n-        self.value_cache: list[torch.Tensor] = []\n-        for idx in range(config.num_hidden_layers):\n-            layer_device = layer_device_map[idx] if layer_device_map is not None else device\n-            new_layer_key_cache = torch.zeros(self.cache_shape, dtype=self.dtype, device=layer_device)\n-            new_layer_value_cache = torch.zeros(self.cache_shape, dtype=self.dtype, device=layer_device)\n-            # Note: `mark_static_address` is used to tag the cache as a fixed data pointer,\n-            # preventing compiled graph breaks when updating the cache.\n-            torch._dynamo.mark_static_address(new_layer_key_cache)\n-            torch._dynamo.mark_static_address(new_layer_value_cache)\n-            self.key_cache.append(new_layer_key_cache)\n-            self.value_cache.append(new_layer_value_cache)\n-\n-        # Block management data structures\n-        self._free_blocks = deque(range(num_blocks))\n-        self._block_tables: dict[str, list[int]] = {}\n-\n-    @traced\n-    def allocate_blocks(self, n_blocks: int, request_id: str) -> list[int]:\n-        \"\"\"Allocates n_blocks for a given request_id.\"\"\"\n-        if len(self._free_blocks) < n_blocks:\n-            return False\n-\n-        allocated = []\n-        for _ in range(n_blocks):\n-            allocated.append(self._free_blocks.popleft())\n-\n-        if request_id not in self._block_tables:\n-            self._block_tables[request_id] = []\n-        self._block_tables[request_id].extend(allocated)\n-        return allocated\n-\n-    @traced\n-    def free_blocks(self, request_id: str) -> None:\n-        \"\"\"Frees all blocks associated with a request_id.\"\"\"\n-        if request_id in self._block_tables:\n-            blocks_to_free = self._block_tables.pop(request_id)\n-            self._free_blocks.extend(blocks_to_free)\n-        else:\n-            logger.info(f\"Attempted to free blocks for non-existent request_id: {request_id}\")\n-\n-    def get_num_free_blocks(self) -> int:\n-        \"\"\"Returns the number of free blocks available.\"\"\"\n-        return len(self._free_blocks)\n-\n-    def get_block_table(self, request_id: str) -> list[int]:\n-        \"\"\"Returns the block table for a request.\"\"\"\n-        return self._block_tables.get(request_id, [])\n-\n-    @traced\n-    def _get_physical_indices(self, state: RequestState, logical_indices: list[int]) -> list[int]:\n-        \"\"\"\n-        Maps logical sequence indices to physical cache indices using the block table, using PyTorch.\n-\n-        Args:\n-            request_id: The request ID.\n-            logical_indices: A list of logical indices.\n-\n-        Returns:\n-            A list of physical indices.\n-\n-        Raises:\n-            ValueError: If no block table is found for the request ID.\n-            IndexError: If a logical index maps to a block index that is out of bounds.\n-        \"\"\"\n-        request_id = state.request_id\n-        block_table = self._block_tables.get(request_id)\n-        if not block_table:\n-            raise ValueError(f\"No block table found for request {request_id}\")\n-\n-        block_size = self.block_size\n-        physical_indices = []\n-\n-        for idx in logical_indices:\n-            block_idx = idx // block_size\n-            block_offset = idx % block_size\n-\n-            if block_idx >= len(block_table):\n-                raise IndexError(\n-                    f\"Logical index {idx} maps to block index {block_idx} which is out of bounds \"\n-                    f\"for request {request_id}\"\n-                )\n-\n-            physical_block_num = block_table[block_idx]\n-            physical_index = physical_block_num * block_size + block_offset\n-            physical_indices.append(physical_index)\n-\n-        return physical_indices\n-\n-    @traced\n-    def update(\n-        self,\n-        key_states: torch.Tensor,\n-        value_states: torch.Tensor,\n-        layer_idx: int,\n-        read_index,\n-        write_index,\n-        **kwargs,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n-        # Reshape cache for easier indexing\n-        total_slots = self.num_blocks * self.block_size\n-        k_cache_flat = self.key_cache[layer_idx].view(self.num_key_value_heads, total_slots, self.head_dim)\n-        v_cache_flat = self.value_cache[layer_idx].view(self.num_key_value_heads, total_slots, self.head_dim)\n-        k_cache_flat[:, write_index, :] = key_states[0]\n-        v_cache_flat[:, write_index, :] = value_states[0]\n-        return k_cache_flat[None, :, read_index, :], v_cache_flat[None, :, read_index, :]\n-\n-\n-class Scheduler(ABC):\n-    \"\"\"\n-    Abstract base class for scheduling requests in the continuous batch processor.\n-    It is expected that cache allocation and scheduling logic will be implemented in subclasses.\n-    \"\"\"\n-\n-    def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = False):\n-        self.active_requests: dict[str, RequestState] = {}\n-        self.waiting_requests: dict[str, RequestState] = {}\n-        self.waiting_requests_order: deque[str] = deque()\n-        self.cache = cache\n-        self.retain_cache_on_finish = retain_cache_on_finish\n-\n-    @abstractmethod\n-    def add_waiting_request(self, state: RequestState):\n-        \"\"\"Add a request to the waiting list.\"\"\"\n-        pass\n-\n-    @abstractmethod\n-    def schedule_batch(self, token_budget: int) -> list[RequestState]:\n-        pass\n-\n-    @traced\n-    def has_pending_requests(self) -> bool:\n-        \"\"\"Check if there are requests ready to be processed.\"\"\"\n-        return len(self.active_requests) or len(self.waiting_requests)\n-\n-    @abstractmethod\n-    def finish_request(self, request_id: str, evict_from_cache: bool = True):\n-        \"\"\"Finish processing a request and free its allocated blocks.\"\"\"\n-        pass\n-\n-    @traced\n-    def get_active_request_static_outputs(self, request_id: str) -> list[int]:\n-        if request_id in self.active_requests:\n-            return self.active_requests[request_id].static_outputs\n-        return []\n-\n-\n-@attach_tracer()\n-class FIFOScheduler(Scheduler):\n-    @traced\n-    def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int):\n-        # 1. we check that the occupancy is less than the requested length\n-        # 2. we allocate enough blocks to cover the requested length\n-        current_len = state.current_len()\n-        occupancy = len(state.allocated_blocks) * self.cache.block_size - current_len\n-        if occupancy < len_next_tokens or (len(state.allocated_blocks) == 0):\n-            blocks_needed = ((len_next_tokens - occupancy + 1) // self.cache.block_size) + 1\n-            allocated = self.cache.allocate_blocks(blocks_needed, state.request_id)\n-            if not allocated:\n-                return False\n-            state.allocated_blocks.extend(allocated)\n-        return True\n-\n-    @traced(span_name=\"prepare_request\")\n-    def _prepare_request_for_processing(\n-        self, state: RequestState, token_budget: int, request_ids_to_remove_from_waiting: set[str]\n-    ):\n-        \"\"\"Prepare a request for processing in the current batch.\"\"\"\n-        request_tokens = (\n-            state.remaining_prompt_ids if state.status == RequestStatus.SPLIT_PENDING_REMAINDER else state.prompt_ids\n-        )\n-        if len(request_tokens) < token_budget:\n-            # Can process the entire prompt/remainder\n-            if state.status == RequestStatus.PENDING:\n-                self.active_requests[state.request_id] = state\n-                state.status = RequestStatus.PREFILLING\n-                request_ids_to_remove_from_waiting.add(state.request_id)\n-            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n-                state.status = RequestStatus.PREFILLING\n-                state.prompt_ids = state.remaining_prompt_ids\n-                state.remaining_prompt_ids = []\n-        else:\n-            # Need to split the request\n-            if state.status == RequestStatus.PENDING:\n-                self.active_requests[state.request_id] = state\n-                state.status = RequestStatus.PREFILLING_SPLIT\n-                request_ids_to_remove_from_waiting.add(state.request_id)\n-            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n-                state.status = RequestStatus.PREFILLING_SPLIT\n-            state.remaining_prompt_ids = request_tokens[token_budget:]\n-            state.prompt_ids = request_tokens[:token_budget]\n-\n-    @traced\n-    def add_waiting_request(self, state: RequestState):\n-        \"\"\"Add a request to the waiting list.\"\"\"\n-        if self.retain_cache_on_finish and state.request_id in self.active_requests:\n-            old_state = self.active_requests.pop(state.request_id)\n-            state.prompt_ids = state.prompt_ids[len(old_state.full_prompt_ids) :]\n-            state.allocated_blocks = old_state.allocated_blocks\n-            state.position_offset = old_state.position_offset\n-        self.waiting_requests[state.request_id] = state\n-        self.waiting_requests_order.append(state.request_id)\n-\n-    @traced\n-    def schedule_batch(self, token_budget: int) -> list[RequestState]:\n-        priority_states: list[RequestState] = []\n-        second_priority_states: list[RequestState] = []\n-        scheduled_requests = []\n-\n-        for state in self.active_requests.values():\n-            if state.status == RequestStatus.DECODING:\n-                priority_states.append(state)\n-            if state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n-                second_priority_states.append(state)\n-\n-        # Add waiting requests to second priority\n-        for req_id in self.waiting_requests_order:\n-            second_priority_states.append(self.waiting_requests[req_id])\n-\n-        candidates = priority_states + second_priority_states\n-        request_ids_to_remove_from_waiting = set()\n-\n-        for state in candidates:\n-            self._prepare_request_for_processing(state, token_budget, request_ids_to_remove_from_waiting)\n-            request_len = len(state.prompt_ids)\n-            if not self._allocate_blocks_if_needed(\n-                state, len(state.prompt_ids)\n-            ):  # don't schedule if we can't allocate blocks\n-                if len(self.cache._free_blocks) == 0:\n-                    break\n-                continue\n-\n-            @traced\n-            def _add_to_scheduled_requests(state: RequestState):\n-                scheduled_requests.append(state)\n-\n-            _add_to_scheduled_requests(state)\n-\n-            token_budget -= request_len\n-\n-            @traced\n-            def _remove_from_waiting_requests(state: RequestState):\n-                req_id = state.request_id\n-                if req_id in self.waiting_requests:\n-                    del self.waiting_requests[req_id]\n-                    request_ids_to_remove_from_waiting.add(req_id)\n-\n-            _remove_from_waiting_requests(state)\n-\n-            if token_budget == 0:\n-                break\n-\n-        self.waiting_requests_order = deque(\n-            [req_id for req_id in self.waiting_requests_order if req_id not in request_ids_to_remove_from_waiting]\n-        )\n-\n-        return scheduled_requests\n-\n-    @traced\n-    def finish_request(self, request_id: str, evict_from_cache: bool = True):\n-        if evict_from_cache:\n-            self.cache.free_blocks(request_id)\n-            if request_id in self.active_requests:\n-                del self.active_requests[request_id]\n-\n-\n-@attach_tracer()\n-class PrefillFirstScheduler(Scheduler):\n-    @traced\n-    def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int):\n-        # 1. we check that the occupancy is less than the requested length\n-        # 2. we allocate enough blocks to cover the requested length\n-        current_len = state.current_len()\n-        occupancy = len(state.allocated_blocks) * self.cache.block_size - current_len\n-        if occupancy < len_next_tokens or (len(state.allocated_blocks) == 0):\n-            blocks_needed = ((len_next_tokens - occupancy + 1) // self.cache.block_size) + 1\n-            allocated = self.cache.allocate_blocks(blocks_needed, state.request_id)\n-            if not allocated:\n-                return False\n-            state.allocated_blocks.extend(allocated)\n-        return True\n-\n-    @traced(span_name=\"prepare_request\")\n-    def _prepare_request_for_processing(\n-        self, state: RequestState, token_budget: int, request_ids_to_remove_from_waiting: set[str]\n-    ):\n-        \"\"\"Prepare a request for processing in the current batch.\"\"\"\n-        request_tokens = (\n-            state.remaining_prompt_ids if state.status == RequestStatus.SPLIT_PENDING_REMAINDER else state.prompt_ids\n-        )\n-        if len(request_tokens) < token_budget:\n-            # Can process the entire prompt/remainder\n-            if state.status == RequestStatus.PENDING:\n-                self.active_requests[state.request_id] = state\n-                state.status = RequestStatus.PREFILLING\n-                request_ids_to_remove_from_waiting.add(state.request_id)\n-            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n-                state.status = RequestStatus.PREFILLING\n-                state.prompt_ids = state.remaining_prompt_ids\n-                state.remaining_prompt_ids = []\n-        else:\n-            # Need to split the request\n-            if state.status == RequestStatus.PENDING:\n-                self.active_requests[state.request_id] = state\n-                state.status = RequestStatus.PREFILLING_SPLIT\n-                request_ids_to_remove_from_waiting.add(state.request_id)\n-            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n-                state.status = RequestStatus.PREFILLING_SPLIT\n-            state.remaining_prompt_ids = request_tokens[token_budget:]\n-            state.prompt_ids = request_tokens[:token_budget]\n-\n-    @traced\n-    def add_waiting_request(self, state: RequestState):\n-        \"\"\"Add a request to the waiting list.\"\"\"\n-        if self.retain_cache_on_finish and state.request_id in self.active_requests:\n-            old_state = self.active_requests.pop(state.request_id)\n-            state.prompt_ids = state.prompt_ids[len(old_state.full_prompt_ids) :]  # XXX: check for indexing error?\n-            state.allocated_blocks = old_state.allocated_blocks\n-            state.position_offset = old_state.position_offset\n-        self.waiting_requests[state.request_id] = state\n-        self.waiting_requests_order.append(state.request_id)\n-\n-    @traced\n-    def schedule_batch(self, token_budget: int) -> list[RequestState]:\n-        priority_states: list[RequestState] = []\n-        second_priority_states: list[RequestState] = []\n-        scheduled_requests = []\n-\n-        for state in self.active_requests.values():\n-            if state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n-                priority_states.append(state)\n-            elif state.status == RequestStatus.DECODING:\n-                second_priority_states.append(state)\n-\n-        for req_id in self.waiting_requests_order:\n-            second_priority_states.append(self.waiting_requests[req_id])\n-\n-        candidates = priority_states + second_priority_states\n-\n-        request_ids_to_remove_from_waiting = set()\n-\n-        for state in candidates:\n-            self._prepare_request_for_processing(state, token_budget, request_ids_to_remove_from_waiting)\n-            request_len = len(state.prompt_ids)\n-            if not self._allocate_blocks_if_needed(\n-                state, len(state.prompt_ids)\n-            ):  # don't schedule if we can't allocate blocks\n-                if len(self.cache._free_blocks) == 0:\n-                    break\n-                continue\n-\n-            @traced\n-            def _add_to_scheduled_requests(state: RequestState):\n-                scheduled_requests.append(state)\n-\n-            _add_to_scheduled_requests(state)\n-\n-            token_budget -= request_len\n-\n-            @traced\n-            def _remove_from_waiting_requests(state: RequestState):\n-                req_id = state.request_id\n-                if req_id in self.waiting_requests:\n-                    del self.waiting_requests[req_id]\n-                    request_ids_to_remove_from_waiting.add(req_id)\n-\n-            _remove_from_waiting_requests(state)\n-\n-            if token_budget == 0:\n-                break\n-\n-        self.waiting_requests_order = deque(\n-            [req_id for req_id in self.waiting_requests_order if req_id not in request_ids_to_remove_from_waiting]\n-        )\n-\n-        return scheduled_requests\n-\n-    @traced\n-    def finish_request(self, request_id: str, evict_from_cache: bool = True):\n-        if evict_from_cache:\n-            self.cache.free_blocks(request_id)\n-            if request_id in self.active_requests:\n-                del self.active_requests[request_id]\n-\n-\n-def get_device_and_memory():\n-    # Select best available device\n-    if torch.cuda.is_available():\n-        device = torch.device(\"cuda\")\n-        total_memory = torch.cuda.get_device_properties(device).total_memory\n-        reserved_memory = torch.cuda.memory_reserved(device)\n-        allocated_memory = torch.cuda.memory_allocated(device)\n-\n-    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n-        device = torch.device(\"mps\")\n-        # MPS memory reporting (PyTorch 2.0+)\n-        total_memory = torch.mps.driver_allocated_memory()\n-        allocated_memory = total_memory - torch.mps.recommended_max_memory()\n-        reserved_memory = 0  # MPS does not track reserved separately\n-\n-    else:\n-        device = torch.device(\"cpu\")\n-        total_memory = None\n-        reserved_memory = 0\n-        allocated_memory = 0\n-\n-    return device, total_memory, reserved_memory, allocated_memory\n-\n-\n-@traced(standalone=True)\n-def compute_optimal_blocks(\n-    max_num_tokens,\n-    block_size,\n-    head_dim,\n-    num_heads,\n-    num_layers,\n-    max_memory_percent=0.9,\n-    num_blocks=None,\n-    dtype=torch.float16,\n-):\n-    device, total, reserved, allocated = get_device_and_memory()\n-    available_memory = int((total - max(allocated, reserved)) * max_memory_percent)\n-\n-    dtype_size = torch.tensor([], dtype=dtype).element_size()\n-    bytes_per_token = 2 * num_heads * head_dim * dtype_size * num_layers\n-    if num_blocks is not None:\n-        # TODO\n-        max_possible_concurrent_requests = num_blocks * bytes_per_token\n-    # FIXME: forgot to add the inintial prompt length in the mix....\n-    max_possible_concurrent_requests = int(\n-        available_memory // (bytes_per_token * max_num_tokens * max_num_tokens // 4)\n-    )\n-    if max_possible_concurrent_requests <= 0:\n-        logger.warning(\"you are trying to generate a bit too many tokens\")\n-        max_possible_concurrent_requests = 32\n-    max_concurrent_tokens = min(64, max_possible_concurrent_requests)\n-    # FIXME: Optimal means uses all memory\n-    optimal_num_blocks = max(((max_concurrent_tokens * max_num_tokens) // block_size) + 1, 64)\n-    return optimal_num_blocks, max_concurrent_tokens\n-\n-\n-@dataclass\n-class PagedAttentionArgs:\n-    input_ids: torch.Tensor\n-    attention_mask: torch.Tensor\n-    position_ids: torch.Tensor\n-    cumulative_seqlens_q: torch.Tensor\n-    cumulative_seqlens_k: torch.Tensor\n-    max_seqlen_q: int\n-    max_seqlen_k: int\n-    write_index: torch.Tensor\n-    read_index: torch.Tensor\n-    logits_indices: torch.Tensor\n-    block_tables: dict[str, list[int]]\n-    cache: PagedAttentionCache\n-    use_cache: bool = False\n-\n-\n-@traced\n-def create_document_mask(cumulative_seqlens_q, cumulative_seqlens_k):\n-    # Number of documents\n-    valid_docs_q = cumulative_seqlens_q[1:] > cumulative_seqlens_q[:-1]\n-    valid_docs_k = cumulative_seqlens_k[1:] > cumulative_seqlens_k[:-1]\n-    num_valid_docs = min(valid_docs_q.sum(), valid_docs_k.sum())\n-\n-    # Trim to valid docs\n-    cumulative_seqlens_q = cumulative_seqlens_q[: num_valid_docs + 1]\n-    cumulative_seqlens_k = cumulative_seqlens_k[: num_valid_docs + 1]\n-\n-    total_q = cumulative_seqlens_q[-1]\n-    total_k = cumulative_seqlens_k[-1]\n-\n-    q_indices = torch.arange(total_q, device=cumulative_seqlens_q.device)\n-    k_indices = torch.arange(total_k, device=cumulative_seqlens_k.device)\n-\n-    q_doc_ids = torch.bucketize(q_indices, cumulative_seqlens_q[1:], right=True)\n-    k_doc_ids = torch.bucketize(k_indices, cumulative_seqlens_k[1:], right=False)\n-    doc_mask = q_doc_ids[:, None] == k_doc_ids[None, :]\n-    # apply causal mask where no decoding (same nb of q than k)\n-\n-    is_causal = ~(cumulative_seqlens_q[1:] - cumulative_seqlens_q[:-1] == 1) * cumulative_seqlens_q[1:]\n-    apply_causal = torch.bucketize(q_indices, is_causal, right=True)[:, None] == k_doc_ids\n-    # TODO don't apply on prefill splitting\n-    causal_mask = torch.triu(torch.ones(total_q, total_k, device=q_doc_ids.device), diagonal=1).bool()\n-    doc_mask.masked_fill_((apply_causal & causal_mask), False)\n-    return doc_mask\n-\n-\n-# Continuous Batch Processor (Internal Logic)\n-@attach_tracer()\n-class ContinuousBatchProcessor:\n-    def __init__(\n-        self,\n-        cache: PagedAttentionCache,\n-        config: PretrainedConfig,\n-        generation_config: GenerationConfig,\n-        input_queue: queue.Queue,\n-        output_queue: queue.Queue,\n-        stop_event: threading.Event,\n-        model_device: torch.device,\n-        model_dtype: torch.dtype,\n-        scheduler: Scheduler,\n-        streaming: bool = False,\n-        manual_eviction: bool = False,\n-    ):\n-        \"\"\"Initialize the continuous batch processor.\n-\n-        Args:\n-            cache: The paged attention cache to use\n-            generation_config: The generation configuration\n-            input_queue: Queue for incoming requests\n-            output_queue: Queue for outgoing results\n-            stop_event: Event to signal processing should stop\n-            model_device: Device for model inputs/outputs\n-            model_dtype: Data type for model inputs/outputs\n-            streaming: Whether to stream tokens as they're generated\n-        \"\"\"\n-        self.cache = cache\n-        self.config = config\n-        self.generation_config = generation_config\n-        self.input_queue = input_queue\n-        self.output_queue = output_queue\n-        self.stop_event = stop_event\n-        self.model_device = model_device\n-        self.model_dtype = model_dtype\n-        self.scheduler = scheduler\n-        self.streaming = streaming\n-        self.manual_eviction = manual_eviction\n-\n-        self.requests_in_batch: list[RequestState] = []\n-\n-        # Set up metrics collector\n-        self.max_batch_tokens = cache.max_batch_tokens\n-        self.metrics = ContinuousBatchProcessorMetrics(cache.max_batch_tokens)\n-\n-        self.setup_static_tensors()\n-\n-        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(self.config._name_or_path)\n-        self.decode_stream = DecodeStream(skip_special_tokens=True)\n-\n-    @traced(standalone=True)\n-    def setup_static_tensors(self):\n-        T = self.max_batch_tokens\n-        max_token_budget = self.cache.num_blocks * self.cache.block_size\n-        tensor_metadata = {\"dtype\": torch.int32, \"device\": self.model_device}\n-        self.tensor_metadata = tensor_metadata\n-        self.input_ids = torch.zeros((1, T), **tensor_metadata)\n-        self.position_ids = torch.zeros((1, T), **tensor_metadata)\n-        self.attention_mask = torch.zeros(\n-            (1, 1, T, max_token_budget), dtype=self.model_dtype, device=self.model_device\n-        )\n-        self.cumulative_seqlens_q = torch.zeros((T + 1,), **tensor_metadata)\n-        self.cumulative_seqlens_k = torch.zeros((T + 1,), **tensor_metadata)\n-        self.write_index = torch.zeros((T,), **tensor_metadata)\n-        self.read_index = torch.zeros((max_token_budget,), **tensor_metadata)\n-        self.logits_indices = torch.full((T,), -1, **tensor_metadata)\n-        self.max_seqlen_q = 0\n-        self.max_seqlen_k = 0\n-        self.output_ids = torch.full((1, T), -1, **tensor_metadata)\n-\n-    @traced\n-    @torch.no_grad()\n-    def reset_static_tensors(self):\n-        \"\"\"Reset static tensors for the next batch.\"\"\"\n-        self.input_ids.zero_()\n-        self.position_ids.zero_()\n-        self.attention_mask.fill_(torch.finfo(self.model_dtype).min)\n-        self.cumulative_seqlens_q.zero_()\n-        self.cumulative_seqlens_k.zero_()\n-        self.write_index.fill_(-1)\n-        self.read_index.fill_(-1)\n-        self.logits_indices.fill_(-1)\n-        self.max_seqlen_q = 0\n-        self.max_seqlen_k = 0\n-        self.output_ids.zero_()\n-\n-    def get_model_kwargs(self) -> PagedAttentionArgs:\n-        \"\"\"Get model keyword arguments for the current batch.\"\"\"\n-        # torch.set_printoptions(threshold=100000,linewidth=10000)\n-        return {\n-            \"input_ids\": self.input_ids,\n-            \"position_ids\": self.position_ids,\n-            \"attention_mask\": self.attention_mask,\n-            \"cu_seq_lens_q\": self.cumulative_seqlens_q,\n-            \"cu_seq_lens_k\": self.cumulative_seqlens_k,\n-            \"write_index\": self.write_index,\n-            \"read_index\": self.read_index,\n-            \"logits_indices\": self.logits_indices,\n-            \"max_seqlen_q\": self.max_seqlen_q,\n-            \"max_seqlen_k\": self.max_seqlen_k,\n-            \"block_tables\": self.cache._block_tables,\n-            \"cache\": self.cache,\n-            \"use_cache\": False,\n-        }\n-\n-    def __repr__(self):\n-        return (\n-            f\"ContinuousBatchProcessor(input_queue={self.input_queue}, output_queue={self.output_queue}, active_requests={self.scheduler.active_requests}, waiting_requests={self.scheduler.waiting_requests})\"\n-            + self.get_model_kwargs().__repr__()\n-        )\n-\n-    @traced\n-    def _get_new_requests(self):\n-        \"\"\"Pull new requests from the input queue and add to waiting list.\"\"\"\n-        while not self.input_queue.empty():\n-            try:\n-                state = self.input_queue.get_nowait()\n-                if state is None:  # Sentinel value\n-                    continue\n-                self.scheduler.add_waiting_request(state)\n-\n-            except queue.Empty:\n-                break\n-            except Exception as e:\n-                logger.error(f\"Error processing new request: {e}\", exc_info=True)\n-                state: RequestState = locals().get(\"state\")\n-                if state is not None:\n-                    self._handle_request_error(e, state)\n-\n-    @traced\n-    def _handle_request_error(self, error, state: RequestState):\n-        \"\"\"Handle general request processing error.\"\"\"\n-        state.status = RequestStatus.FAILED\n-        state.error = str(error)\n-\n-        # Include any generated tokens if this is an active request\n-        if isinstance(state.request_id, str):\n-            state.static_outputs = self.scheduler.get_active_request_static_outputs(state.request_id)\n-        else:\n-            state.static_outputs = []\n-\n-        self.metrics.record_request_completion(state.created_time, state.request_id)\n-        self.output_queue.put(state.to_generation_output())\n-\n-    @traced\n-    def prepare_next_batch(self):\n-        \"\"\"Prepare tensors and metadata for the next model forward pass.\"\"\"\n-        # Get new requests from the queue\n-        self._get_new_requests()\n-        if not self.scheduler.has_pending_requests():\n-            return None\n-\n-        self.metrics.record_queue_metrics(len(self.scheduler.active_requests), len(self.scheduler.waiting_requests))\n-\n-        self.requests_in_batch = self.scheduler.schedule_batch(self.max_batch_tokens)\n-        if not self.requests_in_batch:\n-            return None\n-\n-        # Get the request objects for this batch\n-        self.reset_static_tensors()\n-        position_ids = []\n-        input_ids = []\n-        read_index = []\n-        write_index = []\n-        cumulative_seqlens_q = [0]\n-        cumulative_seqlens_k = [0]\n-        logits_indices = []\n-        self.metrics.record_batch_metrics(self.requests_in_batch)\n-\n-        for state in self.requests_in_batch:\n-            next_input_ids = state.prompt_ids\n-            input_ids.extend(next_input_ids)\n-            past_length = state.position_offset\n-            query_length = len(next_input_ids)\n-            key_length = query_length + past_length\n-            cache_index = list(range(key_length))\n-\n-            positions_to_add = cache_index[past_length:]\n-            read_indices = self.cache._get_physical_indices(state, cache_index)\n-            write_indices = read_indices[-query_length:]\n-\n-            position_ids.extend(positions_to_add)\n-            read_index.extend(read_indices)\n-            write_index.extend(write_indices)\n-            cumulative_seqlens_q.append(cumulative_seqlens_q[-1] + query_length)\n-            cumulative_seqlens_k.append(cumulative_seqlens_k[-1] + key_length)\n-            if len(state.remaining_prompt_ids) == 0:\n-                logits_indices.append(cumulative_seqlens_q[-1] - 1)\n-            self.max_seqlen_q = max(self.max_seqlen_q, query_length)\n-            self.max_seqlen_k = max(self.max_seqlen_k, key_length)\n-            state.position_offset += query_length\n-\n-        logger.info(\n-            f\"Scheduled: {len(self.requests_in_batch)}, Waiting: {len(self.scheduler.waiting_requests)}, Active: {len(self.scheduler.active_requests)}. cum Q: {cumulative_seqlens_q[-1]}. cum KV: {cumulative_seqlens_k[-1]}, free blocks: {self.cache.get_num_free_blocks()}\"\n-        )\n-        self._build_tensors(\n-            input_ids,\n-            position_ids,\n-            read_index,\n-            write_index,\n-            cumulative_seqlens_q,\n-            cumulative_seqlens_k,\n-            logits_indices,\n-        )\n-\n-        self.metrics.record_kv_cache_memory_metrics(self.cache)\n-\n-    @traced\n-    def _build_tensors(\n-        self,\n-        input_ids,\n-        position_ids,\n-        read_index,\n-        write_index,\n-        cumulative_seqlens_q,\n-        cumulative_seqlens_k,\n-        logits_indices,\n-    ):\n-        to_tensor = partial(torch.tensor, **self.tensor_metadata)\n-        self.input_ids[:, : len(input_ids)] = to_tensor(input_ids)\n-        self.position_ids[:, : len(position_ids)] = to_tensor(position_ids)\n-        self.write_index[: len(write_index)] = to_tensor(write_index)\n-        self.read_index[: len(read_index)] = to_tensor(read_index)\n-        self.cumulative_seqlens_q[: len(cumulative_seqlens_q)] = to_tensor(cumulative_seqlens_q)\n-        self.cumulative_seqlens_k[: len(cumulative_seqlens_k)] = to_tensor(cumulative_seqlens_k)\n-        self.logits_indices[: len(logits_indices)] = to_tensor(logits_indices)\n-        min_value = torch.finfo(self.model_dtype).min\n-        if self.config._attn_implementation != \"paged_attention\":  # we set `is_causal` to True in paged call`\n-            for i in range(len(cumulative_seqlens_q) - 1):\n-                if (\n-                    cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i]\n-                    < cumulative_seqlens_k[i + 1] - cumulative_seqlens_k[i]\n-                    and cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i] >= 1\n-                ):\n-                    diagonal = (\n-                        cumulative_seqlens_k[i + 1] - (cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i]) + 1\n-                    )\n-                    diagonal = diagonal - cumulative_seqlens_k[i]\n-                else:\n-                    diagonal = 1\n-                query_range = slice(cumulative_seqlens_q[i], cumulative_seqlens_q[i + 1])\n-                key_range = slice(cumulative_seqlens_k[i], cumulative_seqlens_k[i + 1])\n-\n-                mask = torch.triu(\n-                    torch.full(\n-                        self.attention_mask[..., query_range, key_range].shape,\n-                        min_value,\n-                        dtype=self.model_dtype,\n-                        device=self.model_device,\n-                    ),\n-                    diagonal=diagonal,\n-                )\n-                self.attention_mask[..., query_range, key_range] = mask\n-\n-    @traced\n-    def _sync(self):\n-        if self.output_ids is not None:\n-            try:\n-                out = self.output_ids.tolist()[0]  # should be the only synch we do\n-            except Exception:\n-                out = [0, 1]\n-        else:\n-            out = [0, 0]\n-        return out\n-\n-    @traced\n-    def _maybe_send_output(self, state: RequestState, token: int):\n-        \"\"\"Send output to the queue based on streaming mode and request state.\"\"\"\n-        if self.streaming:\n-            state.next_token = self.decode_stream.step(self.tokenizer, state.static_outputs[-1])\n-            self.output_queue.put(state.to_generation_output())\n-        elif state.status == RequestStatus.FINISHED:\n-            self.output_queue.put(state.to_generation_output())\n-\n-    @traced\n-    def update_batch(self):\n-        \"\"\"Update request states based on generated tokens.\"\"\"\n-        out_tokens = self._sync()\n-        finished_request_ids = []\n-        for i, state in enumerate(self.requests_in_batch):\n-            req_id = state.request_id\n-            if len(state.remaining_prompt_ids) == 0:\n-                self.metrics.record_ttft_metric(state.created_time, state.request_id)\n-                state.status = RequestStatus.DECODING\n-                token = out_tokens[self.logits_indices[i]]\n-                state.prompt_ids = [token]\n-                if state.update_with_token(token):\n-                    self.metrics.record_request_completion(state.created_time, state.request_id)\n-                    self.scheduler.finish_request(state.request_id, evict_from_cache=(not self.manual_eviction))\n-                    finished_request_ids.append(req_id)\n-                self._maybe_send_output(state, token)\n-            elif state.status == RequestStatus.PREFILLING_SPLIT:\n-                state.status = RequestStatus.SPLIT_PENDING_REMAINDER\n-        if self.cache.get_num_free_blocks() == 0:\n-            raise ValueError(\"No more free blocks\")\n-\n-    @traced\n-    def has_pending_requests(self) -> bool:\n-        \"\"\"Check if there are any active or waiting requests.\"\"\"\n-        return self.scheduler.has_pending_requests()\n-\n-    @traced\n-    def handle_batch_error(self, error):\n-        \"\"\"Handle errors during batch processing.\"\"\"\n-        failed_reqs = self.requests_in_batch\n-        for req in failed_reqs:\n-            self._handle_request_error(error, req)\n-            self.scheduler.finish_request(req.request_id)\n-\n-    @traced\n-    def fail_all_requests(self, error):\n-        \"\"\"Fail all active requests with the given error.\n-\n-        Args:\n-            error: The error to report in the failure message\n-        \"\"\"\n-\n-        requests = list(self.scheduler.active_requests.values())\n-        for state in requests:\n-            self._handle_request_error(error, state)\n-            self.scheduler.finish_request(state.request_id)\n-\n-        # Also fail any requests in the waiting queue\n-        for req_id in list(self.scheduler.waiting_requests.keys()):\n-            state = self.scheduler.waiting_requests.pop(req_id)\n-            self._handle_request_error(error, state)\n-\n-        # Clear the ordering queue\n-        self.scheduler.waiting_requests_order.clear()\n-\n-\n-SCHEDULER_MAPPING = {\n-    \"fifo\": FIFOScheduler,\n-    \"prefill_first\": PrefillFirstScheduler,\n-}\n-\n-\n-# Manager Class (User Interface)\n-@attach_tracer()\n-class ContinuousBatchingManager:\n-    \"\"\"Manager for handling continuous batching of generation requests.\n-\n-    This class provides the user interface for submitting generation requests,\n-    retrieving results, and managing the background generation thread.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model,\n-        generation_config: GenerationConfig,\n-        manual_eviction: bool = False,\n-        max_queue_size=0,\n-        streaming: bool = True,\n-    ):\n-        \"\"\"Initialize the continuous batching manager.\n-\n-        Args:\n-            model: The language model for generation\n-            generation_config: Configuration for generation parameters\n-            max_queue_size: Maximum size of the request queue (0 = unlimited)\n-            streaming: Whether to stream tokens as they are generated\n-        \"\"\"\n-        self.model = model.eval()\n-        generation_config = model.generation_config if generation_config is None else generation_config\n-        self.generation_config = generation_config\n-        self.input_queue = queue.Queue(maxsize=max_queue_size)\n-        self.output_queue = queue.Queue()\n-        self.stop_event = threading.Event()\n-        self.streaming = streaming\n-        self.log_prob_generation = getattr(generation_config, \"log_prob_generation\", False)\n-        self._generation_thread = None\n-        self._request_counter = 0\n-        self._request_lock = threading.Lock()\n-        self.model.generation_config.top_p = None\n-        self.do_sample = getattr(generation_config, \"do_sample\", True)\n-        self.logit_processor = self.model._get_logits_processor(generation_config)\n-        self.use_cuda_graph = getattr(generation_config, \"use_cuda_graph\", True)\n-        self.profile = getattr(generation_config, \"profile\", False)\n-        self.manual_eviction = manual_eviction\n-        self.batch_processor: Optional[ContinuousBatchProcessor] = None\n-        self.decode_stream = DecodeStream(skip_special_tokens=True)\n-\n-    @traced\n-    def start(self):\n-        \"\"\"Start the background generation thread.\"\"\"\n-        if self._generation_thread is not None and self._generation_thread.is_alive():\n-            logger.warning(\"Manager thread is already running.\")\n-            return\n-\n-        self._result_queue = queue.Queue()\n-        self._generation_thread = threading.Thread(target=self._run_generation_loop)\n-        self._generation_thread.start()\n-        logger.info(\"Continuous batching manager started.\")\n-\n-    def is_running(self):\n-        \"\"\"Check if the background generation thread is running.\"\"\"\n-        return self._generation_thread is not None and self._generation_thread.is_alive()\n-\n-    def stop(self, block: bool = False, timeout: Optional[float] = None):\n-        \"\"\"Signal the background thread to stop.\n-\n-        Args:\n-            block: Whether to wait for the thread to stop\n-            timeout: Maximum time to wait for the thread to stop\n-        \"\"\"\n-        if self._generation_thread is None:\n-            logger.warning(\"Manager not started.\")\n-            return\n-\n-        if not self.stop_event.is_set():\n-            self.stop_event.set()\n-            logger.info(\"Stopping continuous batching manager...\")\n-\n-        if block:\n-            self.join(timeout)\n-\n-    def join(self, timeout: Optional[float] = None):\n-        \"\"\"Wait for the background thread to finish.\n-\n-        Args:\n-            timeout: Maximum time to wait for the thread to stop\n-        \"\"\"\n-        if self._generation_thread is not None:\n-            self._generation_thread.join(timeout=timeout)\n-            if self._generation_thread.is_alive():\n-                logger.warning(\"Generation thread did not exit after join timeout.\")\n-            else:\n-                logger.info(\"Continuous Batching Manager stopped.\")\n-                self._generation_thread = None\n-\n-    def add_request(\n-        self, input_ids: list[int], request_id: Optional[str] = None, max_new_tokens: Optional[int] = None\n-    ) -> str:\n-        \"\"\"Add a new generation request to the queue.\n-\n-        Args:\n-            input_ids: Input token IDs to use as prompt\n-            request_id: Optional custom request ID (auto-generated if None)\n-            **kwargs: Additional generation parameters\n-\n-        Returns:\n-            str: The request ID\n-        \"\"\"\n-        if request_id is None:\n-            with self._request_lock:\n-                request_id = f\"req_{self._request_counter}\"\n-                self._request_counter += 1\n-\n-        max_new_tokens = self.generation_config.max_new_tokens if max_new_tokens is None else max_new_tokens\n-\n-        state = RequestState(\n-            request_id=request_id,\n-            prompt_ids=list(input_ids),\n-            full_prompt_ids=list(input_ids),\n-            max_new_tokens=max_new_tokens,\n-            eos_token_id=self.generation_config.eos_token_id,\n-        )\n-\n-        # Use block=True with timeout to handle backpressure if queue is full\n-        self.input_queue.put(state, block=True, timeout=10)  # XXX: pass timeout as fn arg?\n-        logger.debug(f\"Added request {request_id} to queue.\")\n-        return request_id\n-\n-    def add_requests(self, inputs: list[list[int]], **kwargs):\n-        for i, input_ids in enumerate(inputs):\n-            # Assign a predictable request ID for ordering results later\n-            req_id = f\"batch_req_{i}\"\n-            self.add_request(input_ids, request_id=req_id, **kwargs)\n-\n-    def get_result(self, timeout=None) -> Optional[GenerationOutput]:\n-        \"\"\"Retrieve one result from the output queue.\n-\n-        Args:\n-            timeout: Maximum time to wait for a result\n-\n-        Returns:\n-            Optional[Dict]: The result data or None if timeout\n-        \"\"\"\n-        if self._generation_thread is None and self.output_queue.empty():\n-            return None\n-        try:\n-            result = self.output_queue.get(block=True, timeout=timeout)\n-            logger.debug(f\"Retrieved result for request {result.request_id}\")\n-            return result\n-        except queue.Empty:\n-            return None\n-\n-    def __iter__(self):\n-        \"\"\"Iterate over results as they become available.\"\"\"\n-        while (\n-            self._generation_thread is not None and self._generation_thread.is_alive() or not self.output_queue.empty()\n-        ):\n-            result = self.get_result(timeout=0.1)  # allow the model to run for 10 seconds\n-            if result is not None:\n-                yield result\n-\n-    @traced\n-    def warmup(self, batch_processor):\n-        stream = torch.cuda.Stream(device=self.model.device)\n-        stream.wait_stream(torch.cuda.current_stream())\n-        with torch.cuda.stream(stream):\n-            # Warmup the model with a dummy forward pass\n-            self._generation_step(batch_processor)\n-        torch.cuda.current_stream().wait_stream(stream)\n-\n-        self.graph = torch.cuda.CUDAGraph()\n-        with torch.cuda.graph(self.graph, stream=stream):\n-            self._generation_step(batch_processor)\n-\n-    @traced\n-    # @torch.compile\n-    def _generation_step(self, batch_processor: ContinuousBatchProcessor):\n-        \"\"\"Perform a single generation step. This is cuda graphed\"\"\"\n-        batch_data = batch_processor.get_model_kwargs()\n-        with torch.no_grad():\n-            logits = self._model_forward(batch_data)\n-            if self.log_prob_generation:\n-                batch_processor.output_probs.copy_(logits)  # TODO\n-            probs = self._process_logit(batch_data, logits)\n-            self._sample(batch_processor, probs)\n-\n-    @traced(span_name=\"model_forward\")\n-    def _model_forward(self, batch_data):\n-        return self.model(**batch_data).logits\n-\n-    @traced(span_name=\"logit_processing\")\n-    def _process_logit(self, batch_data, logits):\n-        # Pass continuous batching context to logits processor if it supports it. TODO we should find a way to make this a little bit cleaner!\n-        if hasattr(self.logit_processor, \"set_continuous_batching_context\"):\n-            self.logit_processor.set_continuous_batching_context(\n-                batch_data[\"logits_indices\"], batch_data[\"cu_seq_lens_q\"]\n-            )\n-        return self.logit_processor(batch_data[\"input_ids\"], logits)\n-\n-    @traced(span_name=\"sampling\")\n-    def _sample(self, batch_processor: ContinuousBatchProcessor, probs):\n-        if self.do_sample:  # sample\n-            probs = nn.functional.softmax(probs, dim=-1)\n-            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(1)\n-        else:\n-            next_tokens = torch.argmax(probs, dim=-1)\n-        batch_processor.output_ids.copy_(next_tokens)\n-\n-    def _run_generation_loop(self):\n-        \"\"\"Main processing loop running in the background thread.\"\"\"\n-        batch_processor = None\n-        try:\n-            paged_attention_cache = PagedAttentionCache(\n-                self.model.config,\n-                self.generation_config,\n-                self.model.device,\n-                self.model.dtype,\n-                num_requests=len(self.input_queue.queue),\n-                tp_size=getattr(self.model, \"_tp_size\", None),  # Use model's actual TP setting\n-            )\n-\n-            scheduler = None\n-            if hasattr(self.generation_config, \"scheduler\"):\n-                scheduler = SCHEDULER_MAPPING.get(self.generation_config.scheduler)\n-                if scheduler is None:\n-                    logger.warning(f\"Scheduler '{scheduler}' not found. Defaulting to FIFO.\")\n-                    scheduler = FIFOScheduler\n-            else:\n-                # Default to fifo\n-                scheduler = FIFOScheduler\n-\n-            batch_processor = ContinuousBatchProcessor(\n-                paged_attention_cache,\n-                self.model.config,\n-                self.generation_config,\n-                self.input_queue,\n-                self.output_queue,\n-                self.stop_event,\n-                self.model.device,\n-                self.model.dtype,\n-                scheduler(paged_attention_cache, self.manual_eviction),\n-                self.streaming,\n-                self.manual_eviction,\n-            )\n-            self.batch_processor = batch_processor\n-            is_first = True\n-            while (not self.stop_event.is_set()) or batch_processor.has_pending_requests():\n-                self._inner_generation_loop(batch_processor, is_first)\n-                if is_first:\n-                    is_first = False\n-\n-        except Exception as e:\n-            logger.error(f\"Error in generation loop: {e}\", exc_info=True)\n-            self._handle_critical_error(e, batch_processor)\n-        finally:\n-            logger.info(\"Generation loop finished.\")\n-\n-    @traced(span_name=\"generation_loop\")\n-    def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor, is_first: bool = False):\n-        if torch.cuda.is_available():\n-            torch.cuda.synchronize()\n-        batch_processor.prepare_next_batch()\n-        device, total, reserved, allocated = get_device_and_memory()\n-        logger.info(f\"[Memory] Device: {device}, Total: {total}, Reserved: {reserved}, Allocated: {allocated}\")\n-        if torch.cuda.is_available() and self.use_cuda_graph:\n-            if is_first:\n-                self.warmup(batch_processor)\n-            elif hasattr(self, \"graph\"):\n-                try:\n-                    self._graph_replay()\n-                except Exception as e:\n-                    logger.error(f\"Model forward pass failed: {e}\", exc_info=True)\n-                    batch_processor.handle_batch_error(e)\n-                    return\n-            else:\n-                self._generation_step(batch_processor)\n-        else:\n-            self._generation_step(batch_processor)\n-        if torch.cuda.is_available():\n-            torch.cuda.synchronize()\n-        batch_processor.update_batch()\n-\n-    @traced(span_name=\"graph_replay\")\n-    def _graph_replay(self):\n-        self.graph.replay()\n-\n-    @traced\n-    def _handle_critical_error(self, error, batch_processor: Optional[ContinuousBatchProcessor]):\n-        \"\"\"Handle critical errors that terminate the generation loop.\"\"\"\n-        # Signal stop\n-        self.stop_event.set()\n-\n-        # Fail pending requests in input queue\n-        try:\n-            while True:\n-                req_data = self.input_queue.get_nowait()\n-                if batch_processor is not None:\n-                    batch_processor._handle_request_error(error, req_data)\n-        except queue.Empty:\n-            pass\n-\n-        # Fail active requests\n-        if batch_processor is not None:\n-            batch_processor.fail_all_requests(error)\n-\n-    @traced\n-    def evict_request_from_cache(self, request_id: str):\n-        \"\"\"Evict a request from the cache. It is assumed that the request is already finished.\"\"\"\n-        if not self.manual_eviction:\n-            raise RuntimeError(\"Manual eviction is not enabled for this manager.\")\n-        if self.batch_processor is not None:\n-            self.batch_processor.scheduler.finish_request(request_id)\n-\n-\n-class ContinuousMixin:\n-    \"\"\"Mixin class for models to add continuous batching capabilities.\"\"\"\n-\n-    def init_continuous_batching(\n-        self,\n-        generation_config: Optional[GenerationConfig] = None,\n-        manual_eviction: bool = False,\n-        max_queue_size: int = 0,\n-        streaming: bool = False,\n-    ) -> ContinuousBatchingManager:\n-        \"\"\"Initialize a manager for continuous batching inference.\n-\n-        Args:\n-            generation_config: Custom generation configuration\n-            max_queue_size: Maximum size of the input request queue\n-            streaming: Whether to stream tokens as they are generated\n-\n-        Returns:\n-            `ContinuousBatchingManager`: The manager instance to add requests and retrieve results.\n-        \"\"\"\n-        if not hasattr(self, \"config\") or not hasattr(self, \"device\") or not hasattr(self, \"dtype\"):\n-            raise AttributeError(\"Model must have 'config', 'device', and 'dtype' attributes.\")\n-\n-        gen_config = generation_config if generation_config is not None else self.generation_config\n-        if gen_config is None:\n-            raise ValueError(\"A GenerationConfig must be provided or set in the model.\")\n-\n-        if gen_config.eos_token_id is None:\n-            logger.warning(\"`eos_token_id` not set in GenerationConfig. Setting to -1 (disabled).\")\n-            gen_config.eos_token_id = -1\n-\n-        # Create and return the manager\n-        return ContinuousBatchingManager(\n-            model=self,\n-            generation_config=gen_config,\n-            manual_eviction=manual_eviction,\n-            max_queue_size=max_queue_size,\n-            streaming=streaming,\n-        )\n-\n-    @traced\n-    @torch.inference_mode()\n-    def generate_batch(\n-        self,\n-        inputs: list[list[int]],\n-        generation_config: Optional[GenerationConfig] = None,\n-        progress_bar: bool = True,\n-        **kwargs,\n-    ) -> list[list[int]]:\n-        \"\"\"Generate sequences for a batch of prompts using continuous batching.\n-\n-        Args:\n-            inputs: List of input token sequences (prompts)\n-            generation_config: Optional generation configuration\n-            **kwargs: Additional generation parameters\n-\n-        Returns:\n-            `list[list[int]]`: A list containing the generated sequences (including prompt tokens\n-                                if not handled otherwise) for each input prompt, in the same order.\n-                                Returns an empty list `[]` for requests that failed.\n-        \"\"\"\n-        if not inputs:\n-            return []\n-\n-        # Initialize manager with the batch inputs\n-        manager = self.init_continuous_batching(generation_config=generation_config)\n-        manager.start()\n-        results = {}\n-        num_requests = len(inputs)\n-        try:\n-            from tqdm.contrib.logging import logging_redirect_tqdm\n-\n-            with logging_redirect_tqdm([logger]):\n-                with tqdm(\n-                    total=num_requests,\n-                    disable=(not progress_bar),\n-                    desc=f\"Solving {num_requests} requests\",\n-                    unit=\"request\",\n-                ) as pbar:\n-                    manager.add_requests(inputs, **kwargs)\n-                    finished_count = 0\n-                    while finished_count < num_requests:\n-                        result = manager.get_result(timeout=1)\n-                        if result:\n-                            req_id = result.request_id\n-                            if result.status == RequestStatus.FINISHED:\n-                                results[req_id] = result\n-                                finished_count += 1\n-                                pbar.update(1)\n-                            logger.debug(manager.batch_processor.tokenizer.decode(result.generated_tokens))\n-                        else:\n-                            if not manager.is_running():\n-                                logger.error(\"Generation thread terminated unexpectedly.\")\n-                                break\n-\n-        except Exception as e:\n-            logger.error(f\"Error during batch generation: {e}\", exc_info=True)\n-        finally:\n-            manager.stop(block=True, timeout=5.0)\n-        return results"
        },
        {
            "sha": "11d15b6468e28558c7fa47ff071d9d4eb963b310",
            "filename": "src/transformers/generation/continuous_batching/__init__.py",
            "status": "added",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/34108a2230c4591c9a20e299ca22edc147c3918f/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34108a2230c4591c9a20e299ca22edc147c3918f/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2F__init__.py?ref=34108a2230c4591c9a20e299ca22edc147c3918f",
            "patch": "@@ -0,0 +1,20 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from .cache import PagedAttentionCache\n+from .classes import RequestState, RequestStatus\n+from .continuous_api import ContinuousBatchingManager, ContinuousMixin\n+\n+\n+__all__ = [\"PagedAttentionCache\", \"RequestState\", \"RequestStatus\", \"ContinuousMixin\", \"ContinuousBatchingManager\"]"
        },
        {
            "sha": "dfc10859b41e9585a7e3897f35b5695ce2701f12",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "added",
            "additions": 396,
            "deletions": 0,
            "changes": 396,
            "blob_url": "https://github.com/huggingface/transformers/blob/34108a2230c4591c9a20e299ca22edc147c3918f/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34108a2230c4591c9a20e299ca22edc147c3918f/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=34108a2230c4591c9a20e299ca22edc147c3918f",
            "patch": "@@ -0,0 +1,396 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from collections import deque\n+from math import floor, sqrt\n+from typing import Optional, Union\n+\n+import torch\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...generation.configuration_utils import GenerationConfig\n+from ...utils.metrics import attach_tracer, traced\n+from .classes import RequestState, get_device_and_memory_breakdown, logger\n+\n+\n+@attach_tracer()\n+class PagedAttentionCache:\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        generation_config: GenerationConfig,\n+        device: torch.device,\n+        dtype: torch.dtype = torch.float16,\n+        num_requests: int = 100,\n+        layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n+        tp_size: Optional[int] = None,\n+    ) -> None:\n+        \"\"\"Initialize a paged attention cache for efficient memory usage.\n+\n+        Args:\n+            config: Model configuration\n+            generation_config: Generation configuration containing cache parameters\n+            device: Device for the cache tensors\n+            dtype: Data type for the cache tensors\n+            layer_device_map: Optional mapping of layer indices to devices\n+            initial_prompt_shapes: Optional sample prompts to help calculate optimal cache size\n+        \"\"\"\n+        self.dtype = dtype\n+        self.device = device\n+\n+        # Extract model dimensions\n+        kv_heads = getattr(config, \"num_key_value_heads\", None)\n+        self.num_key_value_heads: int = kv_heads if kv_heads is not None else config.num_attention_heads\n+        head_dim = getattr(config, \"head_dim\", None)\n+        self.head_dim: int = head_dim if head_dim is not None else config.hidden_size // config.num_attention_heads\n+\n+        self.num_hidden_layers = config.num_hidden_layers\n+        self.block_size = getattr(generation_config, \"block_size\", 32)\n+\n+        # Handle TP\n+        if tp_size is not None and tp_size > 1:\n+            if self.num_key_value_heads % tp_size != 0:\n+                raise ValueError(\n+                    f\"Number of key value heads {self.num_key_value_heads} must be divisible by tensor parallel size {tp_size}.\"\n+                )\n+            # If the model is using tensor parallelism, we need to adjust the number of heads accordingly.\n+            # self.num_key_value_heads //= tp_size # TODO: why is this commented out?\n+\n+        # Infer number of blocks and max batch tokens\n+        memory_handler = PagedAttentionMemoryHandler(\n+            block_size=self.block_size,\n+            head_dim=self.head_dim,\n+            num_heads=self.num_key_value_heads,\n+            num_layers=self.num_hidden_layers,\n+            hidden_size=config.hidden_size,\n+            vocab_size=config.vocab_size,\n+        )\n+        num_blocks, max_batch_tokens = memory_handler.infer_num_blocks_and_max_batch_tokens(\n+            num_blocks=getattr(generation_config, \"num_blocks\", None),\n+            max_batch_tokens=getattr(generation_config, \"max_batch_tokens\", None),\n+            max_memory_percent=getattr(generation_config, \"max_memory\", 0.9),\n+            cache_dtype=self.dtype,\n+        )\n+\n+        # Add the infered attributes to the class\n+        self.num_blocks = num_blocks\n+        self.max_batch_tokens = max_batch_tokens\n+        logger.warning(f\"PagedAttentionCache initialized with {self.num_blocks = } and {self.max_batch_tokens = } \")\n+\n+        # Initialize the cache\n+        self.cache_shape = (self.num_key_value_heads, num_blocks, self.block_size, self.head_dim)\n+        self.key_cache: list[torch.Tensor] = []\n+        self.value_cache: list[torch.Tensor] = []\n+        for idx in range(config.num_hidden_layers):\n+            layer_device = layer_device_map[idx] if layer_device_map is not None else device\n+            new_layer_key_cache = torch.zeros(self.cache_shape, dtype=self.dtype, device=layer_device)\n+            new_layer_value_cache = torch.zeros(self.cache_shape, dtype=self.dtype, device=layer_device)\n+            # Note: `mark_static_address` is used to tag the cache as a fixed data pointer,\n+            # preventing compiled graph breaks when updating the cache.\n+            torch._dynamo.mark_static_address(new_layer_key_cache)\n+            torch._dynamo.mark_static_address(new_layer_value_cache)\n+            self.key_cache.append(new_layer_key_cache)\n+            self.value_cache.append(new_layer_value_cache)\n+\n+        # Block management data structures\n+        self._free_blocks = deque(range(num_blocks))\n+        self._block_tables: dict[str, list[int]] = {}\n+\n+    @traced\n+    def allocate_blocks(self, n_blocks: int, request_id: str) -> list[int]:\n+        \"\"\"Allocates n_blocks for a given request_id.\"\"\"\n+        if len(self._free_blocks) < n_blocks:\n+            return False\n+\n+        allocated = []\n+        for _ in range(n_blocks):\n+            allocated.append(self._free_blocks.popleft())\n+\n+        if request_id not in self._block_tables:\n+            self._block_tables[request_id] = []\n+        self._block_tables[request_id].extend(allocated)\n+        return allocated\n+\n+    @traced\n+    def free_blocks(self, request_id: str) -> None:\n+        \"\"\"Frees all blocks associated with a request_id.\"\"\"\n+        if request_id in self._block_tables:\n+            blocks_to_free = self._block_tables.pop(request_id)\n+            self._free_blocks.extend(blocks_to_free)\n+        else:\n+            logger.info(f\"Attempted to free blocks for non-existent request_id: {request_id}\")\n+\n+    def get_num_free_blocks(self) -> int:\n+        \"\"\"Returns the number of free blocks available.\"\"\"\n+        return len(self._free_blocks)\n+\n+    def get_block_table(self, request_id: str) -> list[int]:\n+        \"\"\"Returns the block table for a request.\"\"\"\n+        return self._block_tables.get(request_id, [])\n+\n+    @traced\n+    def _get_physical_indices(self, state: RequestState, logical_indices: list[int]) -> list[int]:\n+        \"\"\"\n+        Maps logical sequence indices to physical cache indices using the block table, using PyTorch.\n+\n+        Args:\n+            request_id: The request ID.\n+            logical_indices: A list of logical indices.\n+\n+        Returns:\n+            A list of physical indices.\n+\n+        Raises:\n+            ValueError: If no block table is found for the request ID.\n+            IndexError: If a logical index maps to a block index that is out of bounds.\n+        \"\"\"\n+        request_id = state.request_id\n+        block_table = self._block_tables.get(request_id)\n+        if not block_table:\n+            raise ValueError(f\"No block table found for request {request_id}\")\n+\n+        block_size = self.block_size\n+        physical_indices = []\n+\n+        for idx in logical_indices:\n+            block_idx = idx // block_size\n+            block_offset = idx % block_size\n+\n+            if block_idx >= len(block_table):\n+                raise IndexError(\n+                    f\"Logical index {idx} maps to block index {block_idx} which is out of bounds \"\n+                    f\"for request {request_id}\"\n+                )\n+\n+            physical_block_num = block_table[block_idx]\n+            physical_index = physical_block_num * block_size + block_offset\n+            physical_indices.append(physical_index)\n+\n+        return physical_indices\n+\n+    @traced\n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        layer_idx: int,\n+        read_index,\n+        write_index,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        # Reshape cache for easier indexing\n+        total_slots = self.num_blocks * self.block_size\n+        k_cache_flat = self.key_cache[layer_idx].view(self.num_key_value_heads, total_slots, self.head_dim)\n+        v_cache_flat = self.value_cache[layer_idx].view(self.num_key_value_heads, total_slots, self.head_dim)\n+        k_cache_flat[:, write_index, :] = key_states[0]\n+        v_cache_flat[:, write_index, :] = value_states[0]\n+        return k_cache_flat[None, :, read_index, :], v_cache_flat[None, :, read_index, :]\n+\n+\n+class PagedAttentionMemoryHandler:\n+    _activation_dtype = torch.bfloat16\n+    _activation_safety_factor = 2\n+    _input_dtype = torch.int32\n+    _upper_bound_max_batch_tokens = 256\n+    _upper_bound_num_blocks = 4096\n+\n+    def __init__(\n+        self,\n+        block_size: int,\n+        head_dim: int,\n+        num_heads: int,\n+        num_layers: int,\n+        hidden_size: int,\n+        vocab_size: int,\n+    ) -> None:\n+        self.block_size = block_size\n+        self.head_dim = head_dim\n+        self.num_heads = num_heads\n+        self.num_layers = num_layers\n+        self.hidden_size = hidden_size\n+        self.vocab_size = vocab_size\n+\n+    @staticmethod\n+    def get_available_memory(max_memory_percent: float = 1.0) -> int:\n+        _, total, reserved, allocated = get_device_and_memory_breakdown()\n+        available_memory = total - max(allocated, reserved)\n+        available_memory = int(available_memory * max_memory_percent)\n+        return available_memory\n+\n+    def infer_num_blocks_and_max_batch_tokens(\n+        self,\n+        num_blocks: Optional[int] = None,\n+        max_batch_tokens: Optional[int] = None,\n+        max_memory_percent: float = 0.9,\n+        cache_dtype: torch.dtype = torch.float16,\n+    ) -> tuple[int, int]:\n+        \"\"\"\n+        The memory footprint depends on the cache size C and the max batch tokens M in the following way:\n+            Mem = Mem(cache) + Mem(activation) + Mem(static_tensors)\n+        where:\n+            Mem(cache) = 2 * num_heads * head_dim * num_layers * cache_dtype.itemsize * C\n+            Mem(activation) = M * (hidden_size + vocab_size) * activation_dtype.itemsize\n+            Mem(static_tensors) ~= 8M * input_dtype.itemsize + M * C * activation_dtype.itemsize\n+\n+        Depending on if C or M is given, we use different methods to infer the values (C = num_blocks * block_size) and\n+        since block_size is fixed, num_blocks is the true variable to find.\n+        \"\"\"\n+        # If neither num_blocks nor max_batch_tokens are provided, we use a second-order polynomial\n+        if num_blocks is None and max_batch_tokens is None:\n+            num_blocks, max_batch_tokens = self.compute_num_blocks_and_max_batch_tokens(\n+                max_memory_percent, cache_dtype\n+            )\n+        # If only num_blocks is provided, we infer the max_batch_tokens\n+        elif num_blocks is not None and max_batch_tokens is None:\n+            max_batch_tokens = self.compute_max_batch_tokens(num_blocks, max_memory_percent, cache_dtype)\n+        # If only max_batch_tokens is provided, we infer the num_blocks\n+        elif max_batch_tokens is not None and num_blocks is None:\n+            num_blocks = self.compute_num_blocks(max_batch_tokens, max_memory_percent, cache_dtype)\n+\n+        # We check if the memory footprint is too large in all cases\n+        available_memory = self.get_available_memory(max_memory_percent)\n+        memory_footprint = self.compute_memory_footprint(\n+            max_batch_tokens=max_batch_tokens,\n+            num_blocks=num_blocks,\n+            cache_dtype=cache_dtype,\n+        )\n+        if sum(memory_footprint) > available_memory:\n+            raise MemoryError(f\"Memory footprint {memory_footprint} is more than available memory {available_memory}\")\n+        return num_blocks, max_batch_tokens\n+\n+    def compute_num_blocks_and_max_batch_tokens(\n+        self,\n+        max_memory_percent: float = 0.9,\n+        cache_dtype: torch.dtype = torch.float16,\n+        m: float = 0.01,\n+    ) -> tuple[int, int]:\n+        \"\"\"\n+        If neither M nor C is given, we assume M = m*C so we have to solve a second-order polynomial in C:\n+            Mem = C * 2 * self.num_heads * self.head_dim * self.num_layers * cache_dtype.itemsize\n+                + C * m * (hidden_size + vocab_size) * activation_dtype.itemsize\n+                + C * m * 8 * input_dtype.itemsize + C^2 * m * activation_dtype.itemsize\n+\n+        We solve for C and then M = m*C.\n+        \"\"\"\n+        cache_memory = self.get_available_memory(max_memory_percent)\n+        logger.info(f\"Cache memory: {cache_memory}\")\n+\n+        # Compute memory footprints\n+        mem_per_activation_token = m * self._activation_dtype.itemsize * (self.hidden_size + self.vocab_size)\n+        mem_per_cache_token = 2 * self.num_heads * self.head_dim * self.num_layers * cache_dtype.itemsize\n+        mem_per_input_token = 8 * m * self._input_dtype.itemsize\n+        logger.info(f\"Memory per activation token: {mem_per_activation_token}\")\n+        logger.info(f\"Memory per cache token: {mem_per_cache_token}\")\n+        logger.info(f\"Memory per input token: {mem_per_input_token}\")\n+\n+        # Compute second-degree polynomial coefficients\n+        a = m * self._activation_dtype.itemsize\n+        b = mem_per_input_token + mem_per_cache_token + mem_per_activation_token\n+        c = -cache_memory\n+\n+        # Compute discriminant and greatest solution\n+        discriminant = b**2 - 4 * a * c\n+        if discriminant < 0:\n+            raise ValueError(f\"Discriminant is negative: {discriminant = }\")\n+        greatest_solution = (-b + sqrt(discriminant)) / (2 * a)\n+        if greatest_solution < 0:\n+            raise ValueError(f\"Greatest solution is negative: {greatest_solution = }\")\n+\n+        # Infer number of blocks and max batch tokens\n+        num_blocks = int(greatest_solution) // self.block_size\n+        if num_blocks > self._upper_bound_num_blocks:\n+            logger.warning(f\"{num_blocks = } is too large, setting to {self._upper_bound_num_blocks = }\")\n+            num_blocks = self._upper_bound_num_blocks\n+        max_batch_tokens = int(greatest_solution * m)\n+        if max_batch_tokens > self._upper_bound_max_batch_tokens:\n+            logger.warning(f\"{max_batch_tokens = } is too large, setting to {self._upper_bound_max_batch_tokens = }\")\n+            max_batch_tokens = self._upper_bound_max_batch_tokens\n+        return num_blocks, max_batch_tokens\n+\n+    def compute_max_batch_tokens(\n+        self,\n+        num_blocks: int,\n+        max_memory_percent: float = 0.9,\n+        cache_dtype: torch.dtype = torch.float16,\n+    ) -> int:\n+        \"\"\"\n+        If C is given, we have a formula for M:\n+            num = (Mem - C * 2 * num_heads * head_dim * num_layers * cache_dtype.itemsize)\n+            denum = (8 * input_dtype.itemsize + C * activation_dtype.itemsize + (hidden_size + vocab_size) * activation_dtype.itemsize)\n+        M = num / denum\n+        \"\"\"\n+        cache_memory = self.get_available_memory(max_memory_percent)\n+        cache_size = num_blocks * self.block_size\n+        # Compute numerator\n+        num = cache_memory\n+        num -= cache_size * 2 * self.num_heads * self.head_dim * self.num_layers * cache_dtype.itemsize\n+        # Compute denominator\n+        denum = 8 * self._input_dtype.itemsize + cache_size * self._activation_dtype.itemsize\n+        denum += (self.hidden_size + self.vocab_size) * self._activation_dtype.itemsize\n+        # Compute max batch tokens and return\n+        return int(num / denum)\n+\n+    def compute_num_blocks(\n+        self,\n+        max_batch_tokens: int,\n+        max_memory_percent: float = 0.9,\n+        cache_dtype: torch.dtype = torch.float16,\n+    ) -> int:\n+        \"\"\"\n+        If M is given, we have a formula for C:\n+            num = Mem - M * (hidden_size + vocab_size) * activation_dtype.itemsize - 8 * M * input_dtype.itemsize\n+            denum = 2 * num_heads * head_dim * num_layers * cache_dtype.itemsize + M * activation_dtype.itemsize\n+        C = num / denum\n+        \"\"\"\n+        cache_memory = self.get_available_memory(max_memory_percent)\n+        # Compute numerator\n+        num = cache_memory\n+        num -= self._activation_dtype.itemsize * (self.hidden_size + self.vocab_size) * max_batch_tokens\n+        num -= 8 * max_batch_tokens * self._input_dtype.itemsize\n+        # Compute denominator\n+        denum = 2 * self.num_heads * self.head_dim * self.num_layers * cache_dtype.itemsize\n+        denum += max_batch_tokens * self._activation_dtype.itemsize\n+        # Compute cache size and return number of blocks\n+        cache_size = int(num / denum)\n+        return floor(cache_size / self.block_size)\n+\n+    def compute_memory_footprint(\n+        self,\n+        num_blocks: Optional[int] = None,\n+        max_batch_tokens: Optional[int] = None,\n+        cache_dtype: torch.dtype = torch.float16,\n+    ) -> tuple[int, int, int]:\n+        # Compute activation memory footprint\n+        activation_memory_footprint = self._activation_dtype.itemsize * (self.hidden_size + self.vocab_size)\n+        activation_memory_footprint *= max_batch_tokens\n+        # Compute cache memory footprint if num_blocks is provided\n+        if num_blocks is not None:\n+            cache_size = num_blocks * self.block_size\n+            bytes_per_token = 2 * self.num_heads * self.head_dim * self.num_layers * cache_dtype.itemsize\n+            cache_memory_footprint = cache_size * bytes_per_token\n+        else:\n+            cache_memory_footprint = -1\n+        # Compute static tensors memory footprint if num_blocks and max_batch_tokens is provided\n+        if num_blocks is not None and max_batch_tokens is not None:\n+            static_memory_footprint = sum(\n+                [\n+                    3 * max_batch_tokens * self._input_dtype.itemsize,  # input_ids, position_ids, output_ids\n+                    max_batch_tokens * cache_size * self._activation_dtype.itemsize,  # attention_mask\n+                    2 * max_batch_tokens * self._input_dtype.itemsize,  # cumulative_seqlens_qk (we remove the +1 to M)\n+                    3 * max_batch_tokens * self._input_dtype.itemsize,  # write_index, read_index, logits_indices\n+                ]\n+            )\n+        else:\n+            static_memory_footprint = -1\n+        return activation_memory_footprint, cache_memory_footprint, static_memory_footprint"
        },
        {
            "sha": "f2c3a9eda455df872e18a0d9ac7291a44a14ef97",
            "filename": "src/transformers/generation/continuous_batching/classes.py",
            "status": "added",
            "additions": 210,
            "deletions": 0,
            "changes": 210,
            "blob_url": "https://github.com/huggingface/transformers/blob/34108a2230c4591c9a20e299ca22edc147c3918f/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fclasses.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34108a2230c4591c9a20e299ca22edc147c3918f/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fclasses.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fclasses.py?ref=34108a2230c4591c9a20e299ca22edc147c3918f",
            "patch": "@@ -0,0 +1,210 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import time\n+from dataclasses import dataclass, field\n+from enum import Enum\n+from typing import Optional\n+\n+import torch\n+\n+from ...utils.logging import logging\n+from ...utils.metrics import traced\n+\n+\n+# We centralize the logger here to coordinate between logging and progress bar\n+logger = logging.getLogger(\"ContinuousBatchingLogger\")\n+logger.setLevel(logging.INFO)\n+\n+\n+@staticmethod\n+def get_device_and_memory_breakdown() -> tuple[torch.device, int, int, int]:\n+    if torch.cuda.is_available():\n+        device = torch.device(\"cuda\")\n+        torch.cuda.empty_cache()\n+        torch.cuda.synchronize()\n+        total_memory = torch.cuda.get_device_properties(device).total_memory\n+        reserved_memory = torch.cuda.memory_reserved(device)\n+        allocated_memory = torch.cuda.memory_allocated(device)\n+    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n+        device = torch.device(\"mps\")\n+        # MPS memory reporting (PyTorch 2.0+)\n+        total_memory = torch.mps.driver_allocated_memory()\n+        allocated_memory = total_memory - torch.mps.recommended_max_memory()\n+        reserved_memory = 0  # MPS does not track reserved separately\n+    else:\n+        device = torch.device(\"cpu\")\n+        total_memory = None\n+        reserved_memory = 0\n+        allocated_memory = 0\n+    return device, total_memory, reserved_memory, allocated_memory\n+\n+\n+class RequestStatus(Enum):\n+    \"\"\"Status of a generation request through its lifecycle.\"\"\"\n+\n+    PENDING = \"pending\"\n+    PREFILLING = \"prefilling\"\n+    PREFILLING_SPLIT = \"prefilling_split\"\n+    SPLIT_PENDING_REMAINDER = \"split_pending_remainder\"\n+    DECODING = \"decoding\"\n+    FINISHED = \"finished\"\n+    FAILED = \"failed\"\n+\n+\n+@dataclass\n+class GenerationOutput:\n+    \"\"\"Tracks the output of a generation request.\n+\n+    Attributes:\n+        request_id (str): The ID of the generation request.\n+        prompt_ids (list[int]): The IDs of the prompt tokens.\n+        generated_tokens (list[int]): The generated tokens.\n+        logprobs (list[float]): The log probabilities of the generated tokens.\n+        error (Optional[str]): Any error message associated with the request. When None, the request was successful.\n+        status (RequestStatus): The status of the request.\n+        created_time (float): The time the request was created.\n+        next_token (Optional[int]): The next token to be generated.\n+    \"\"\"\n+\n+    request_id: str\n+    prompt_ids: list[int] = field(default_factory=list)\n+    generated_tokens: list[int] = field(default_factory=list)\n+    logprobs: list[float] = field(default_factory=list)\n+    error: Optional[str] = None\n+    status: RequestStatus = RequestStatus.PENDING\n+    created_time: float = field(default_factory=time.time)\n+    next_token: Optional[int] = field(default_factory=int)\n+\n+\n+@dataclass\n+class RequestState:\n+    \"\"\"Tracks the state of a generation request through its lifecycle.\n+\n+    Attributes:\n+        request_id (str): The ID of the generation request.\n+        full_prompt_ids (list[int] | None): The tokens IDs of the full prompt.\n+        prompt_ids (list[int] | None): The tokens IDs currently being processed.\n+        remaining_prompt_ids (list[int]): The tokens IDs remaining to be processed (for split requests).\n+        static_outputs (list[int]): The generated tokens.\n+        allocated_blocks (list[int]): The identifiers of the allocated blocks to the request.\n+        position_offset (int): The current position in the sequence for position_ids.\n+        status (RequestStatus): The status of the request: can be one of PENDING, PREFILLING, PREFILLING_SPLIT,\n+                                SPLIT_PENDING_REMAINDER, DECODING, FINISHED, FAILED\n+        max_new_tokens (int): The maximum number of new tokens to generate.\n+        eos_token_id (int): The ID of the end-of-sequence token.\n+        created_time (float): The time the request was created.\n+        error (Optional[str]): Any error message associated with the request. When None, has had no error yet.\n+        next_token (Optional[str]): The next token to be generated.\n+    \"\"\"\n+\n+    # Required fields\n+    request_id: str\n+    full_prompt_ids: Optional[list[int]] = None  # Full initial prompt\n+    prompt_ids: Optional[list[int]] = None  # Tokens IDs currently being processed (initial + generated)\n+    remaining_prompt_ids: list[int] = field(default_factory=list)  # For split requests, prefill left to process\n+    static_outputs: list[int] = field(default_factory=list)  # Generated tokens\n+    allocated_blocks: list[int] = field(default_factory=list)  # Block IDs allocated to the request\n+    position_offset: int = 0  # Current position in the sequence for position_ids\n+    _status: RequestStatus = RequestStatus.PENDING  # Status of the request, hidden behind a property\n+    max_new_tokens: int = 20  # Maximum number of new tokens to generate\n+    eos_token_id: int = -1  # ID of the end-of-sequence token\n+    created_time: float = field(default_factory=time.time)  # Time the request was created\n+    error: Optional[str] = None  # Error message if the request failed\n+    next_token: Optional[str] = None  # Next token to be generated\n+    lifespan: tuple[float, float] = (-1, -1)  # (time request was no longer pending, time request finished)\n+\n+    @property\n+    def status(self) -> RequestStatus:\n+        return self._status\n+\n+    @status.setter\n+    def status(self, value: RequestStatus):\n+        if self._status == RequestStatus.PENDING:\n+            self.lifespan = (time.time(), -1)\n+        elif value == RequestStatus.FINISHED:\n+            self.lifespan = (self.lifespan[0], time.time())\n+            self.log_end_of_request()\n+        self._status = value\n+\n+    def log_end_of_request(self):\n+        prefill_len = len(self.full_prompt_ids)\n+        decode_len = self.generated_len()\n+        start_time = self.lifespan[0] - self.created_time\n+        end_time = self.lifespan[1] - self.created_time\n+        logger.info(\n+            f\"Request {self.request_id} finished: {prefill_len = } {decode_len = } {start_time = } {end_time = }\"\n+        )\n+\n+    def current_len(self) -> int:\n+        \"\"\"Get the current length of the sequence (prompt + generated tokens).\"\"\"\n+        return self.position_offset\n+\n+    def generated_len(self) -> int:\n+        \"\"\"Get the number of tokens generated so far.\"\"\"\n+        return len(self.static_outputs)\n+\n+    # TODO: this logic seems one token off, check it out\n+    @traced\n+    def update_with_token(self, token_id: int) -> bool:\n+        \"\"\"Update the request with a newly generated token and check for completion.\n+\n+        Args:\n+            token_id: The token ID to add to the output sequence\n+\n+        Returns:\n+            bool: True if the request is now complete, False otherwise\n+        \"\"\"\n+        # Only update if we're in decoding state\n+        if self.status != RequestStatus.DECODING:\n+            return False\n+\n+        is_eos = token_id == self.eos_token_id and self.eos_token_id != -1\n+        is_max_len = self.generated_len() >= self.max_new_tokens\n+\n+        # Only add the token if we're not finishing due to max length\n+        # (EOS tokens should still be added to the output)\n+        if not (is_max_len and not is_eos):\n+            self.static_outputs.extend([token_id])\n+\n+        if is_eos or is_max_len:\n+            self.status = RequestStatus.FINISHED\n+            return True\n+        return False\n+\n+    def __repr__(self):\n+        msg = [\n+            f\"request_id={self.request_id}\",\n+            f\"status={self._status}\",\n+            f\"out_tokens={self.generated_len()}\",\n+            f\"query_length={len(self.prompt_ids)}\",\n+            f\"remaining_tokens={len(self.remaining_prompt_ids)}\",\n+            f\"kv_length={self.position_offset}\",\n+            f\"full_prompt_lenght={len(self.full_prompt_ids)}\",\n+            f\"allocated_blocks={self.allocated_blocks}\",\n+            f\"generated_tokens={self.static_outputs}\",\n+        ]\n+        return \"RequestState(\\n\\t\" + \",\\n\\t\".join(msg) + \"\\n)\"\n+\n+    def to_generation_output(self):\n+        \"\"\"Convert the request state to a GenerationOutput object.\"\"\"\n+        return GenerationOutput(\n+            request_id=self.request_id,\n+            prompt_ids=self.full_prompt_ids,\n+            status=self.status,\n+            generated_tokens=self.static_outputs,\n+            logprobs=[],\n+            error=self.error,\n+            next_token=self.next_token,\n+        )"
        },
        {
            "sha": "4b6775141362eeaa96e3d99d9bd97ca04538ca87",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "added",
            "additions": 842,
            "deletions": 0,
            "changes": 842,
            "blob_url": "https://github.com/huggingface/transformers/blob/34108a2230c4591c9a20e299ca22edc147c3918f/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34108a2230c4591c9a20e299ca22edc147c3918f/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=34108a2230c4591c9a20e299ca22edc147c3918f",
            "patch": "@@ -0,0 +1,842 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team.\n+# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import queue\n+import threading\n+from dataclasses import dataclass\n+from functools import partial\n+from typing import Optional\n+\n+import torch\n+from tokenizers.decoders import DecodeStream\n+from torch import nn\n+from tqdm import tqdm\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...generation.configuration_utils import GenerationConfig\n+from ...tokenization_utils_fast import PreTrainedTokenizerFast\n+from ...utils.logging import logging\n+from ...utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n+from .cache import PagedAttentionCache\n+from .classes import GenerationOutput, RequestState, RequestStatus, get_device_and_memory_breakdown, logger\n+from .scheduler import SCHEDULER_MAPPING, FIFOScheduler, Scheduler\n+\n+\n+@dataclass\n+class PagedAttentionArgs:\n+    input_ids: torch.Tensor\n+    attention_mask: Optional[torch.Tensor]\n+    position_ids: torch.Tensor\n+    cumulative_seqlens_q: torch.Tensor\n+    cumulative_seqlens_k: torch.Tensor\n+    max_seqlen_q: int\n+    max_seqlen_k: int\n+    write_index: torch.Tensor\n+    read_index: torch.Tensor\n+    logits_indices: torch.Tensor\n+    block_tables: dict[str, list[int]]\n+    cache: PagedAttentionCache\n+    use_cache: bool = False\n+\n+\n+# Continuous Batch Processor (Internal Logic)\n+@attach_tracer()\n+class ContinuousBatchProcessor:\n+    def __init__(\n+        self,\n+        cache: PagedAttentionCache,\n+        config: PretrainedConfig,\n+        generation_config: GenerationConfig,\n+        input_queue: queue.Queue,\n+        output_queue: queue.Queue,\n+        stop_event: threading.Event,\n+        model_device: torch.device,\n+        model_dtype: torch.dtype,\n+        scheduler: Scheduler,\n+        streaming: bool = False,\n+        manual_eviction: bool = False,\n+        slice_inputs: bool = True,  # TODO: remove this once parity is ensured\n+    ):\n+        \"\"\"Initialize the continuous batch processor.\n+\n+        Args:\n+            cache: The paged attention cache to use\n+            generation_config: The generation configuration\n+            input_queue: Queue for incoming requests\n+            output_queue: Queue for outgoing results\n+            stop_event: Event to signal processing should stop\n+            model_device: Device for model inputs/outputs\n+            model_dtype: Data type for model inputs/outputs\n+            streaming: Whether to stream tokens as they're generated\n+        \"\"\"\n+        self.cache = cache\n+        self.config = config\n+        self.generation_config = generation_config\n+        self.input_queue = input_queue\n+        self.output_queue = output_queue\n+        self.stop_event = stop_event\n+        self.model_device = model_device\n+        self.model_dtype = model_dtype\n+        self.scheduler = scheduler\n+        self.streaming = streaming\n+        self.manual_eviction = manual_eviction\n+        self.slice_inputs = slice_inputs\n+\n+        self.requests_in_batch: list[RequestState] = []\n+\n+        # Set up metrics collector\n+        self.max_batch_tokens = cache.max_batch_tokens\n+        self.metrics = ContinuousBatchProcessorMetrics(cache.max_batch_tokens)\n+\n+        self.setup_static_tensors()\n+\n+        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(self.config._name_or_path)\n+        self.decode_stream = DecodeStream(skip_special_tokens=True)\n+\n+    def return_attention_mask(self) -> bool:\n+        return self.config._attn_implementation != \"paged_attention\"  # we set `is_causal` to True in paged call\n+\n+    @traced(standalone=True)\n+    def setup_static_tensors(self):\n+        T = self.max_batch_tokens\n+        max_token_budget = self.cache.num_blocks * self.cache.block_size\n+        tensor_metadata = {\"dtype\": torch.int32, \"device\": self.model_device}\n+        # Prepare empty tensors\n+        self.tensor_metadata = tensor_metadata\n+        self.input_ids = torch.empty((1, T), **tensor_metadata)\n+        self.position_ids = torch.empty((1, T), **tensor_metadata)\n+        self.cumulative_seqlens_q = torch.empty((T + 1,), **tensor_metadata)\n+        self.cumulative_seqlens_k = torch.empty((T + 1,), **tensor_metadata)\n+        self.write_index = torch.empty((T,), **tensor_metadata)\n+        self.read_index = torch.empty((max_token_budget,), **tensor_metadata)\n+        self.logits_indices = torch.empty((T,), **tensor_metadata)\n+        self.max_seqlen_q = 0\n+        self.max_seqlen_k = 0\n+        self.output_ids = torch.empty((1, T), **tensor_metadata)\n+        # Since attenention_mask is not always needed, we only allocate it if it is needed\n+        if self.return_attention_mask():\n+            self.attention_mask = torch.empty(\n+                (1, 1, T, max_token_budget), dtype=self.model_dtype, device=self.model_device\n+            )\n+        else:\n+            self.attention_mask = None\n+        # Initialize the tensors by pretending they are in full use\n+        self.actual_tokens = T\n+        self.cache_used = max_token_budget\n+        self.reset_static_tensors()\n+        # Reset stats to 0\n+        self.actual_tokens = 0\n+        self.cache_used = 0\n+\n+    @traced\n+    @torch.no_grad()\n+    def reset_static_tensors(self):\n+        \"\"\"Reset static tensors for the next batch.\"\"\"\n+        # Compute the slice to reset\n+        t = self.actual_tokens if self.slice_inputs else self.write_index.size(0)\n+        c = self.cache_used if self.slice_inputs else self.read_index.size(0)\n+        # Reset the tensors\n+        self.input_ids[:, :t].zero_()\n+        self.position_ids[:, :t].zero_()\n+        self.cumulative_seqlens_q[: t + 1].zero_()\n+        self.cumulative_seqlens_k[: t + 1].zero_()\n+        self.write_index[:t].fill_(-1)\n+        self.read_index[:c].fill_(-1)\n+        self.logits_indices[:t].fill_(-1)\n+        self.max_seqlen_q = 0\n+        self.max_seqlen_k = 0\n+        self.output_ids[:, :t].fill_(-1)\n+        if self.attention_mask is not None:\n+            self.attention_mask[:, :, :t, :c].fill_(torch.finfo(self.model_dtype).min)\n+\n+    def get_model_kwargs(self) -> PagedAttentionArgs:\n+        \"\"\"Get model keyword arguments for the current batch.\"\"\"\n+        # Compute the slice to return\n+        t = self.actual_tokens if self.slice_inputs else self.write_index.size(0)\n+        c = self.cache_used if self.slice_inputs else self.read_index.size(0)\n+        # Prepare the kwargs\n+        kwargs = {\n+            \"input_ids\": self.input_ids[:, :t],\n+            \"attention_mask\": self.attention_mask,\n+            \"position_ids\": self.position_ids[:, :t],\n+            \"cu_seq_lens_q\": self.cumulative_seqlens_q[: t + 1],\n+            \"cu_seq_lens_k\": self.cumulative_seqlens_k[: t + 1],\n+            \"write_index\": self.write_index[:t],\n+            \"read_index\": self.read_index[:c],\n+            \"logits_indices\": self.logits_indices[:t],\n+            \"max_seqlen_q\": self.max_seqlen_q,\n+            \"max_seqlen_k\": self.max_seqlen_k,\n+            \"block_tables\": self.cache._block_tables,\n+            \"cache\": self.cache,\n+            \"use_cache\": False,\n+        }\n+        # If the attention mask is not None, we slice it as the others\n+        if self.attention_mask is not None:\n+            kwargs[\"attention_mask\"] = self.attention_mask[:, :, :t, :c]\n+        return kwargs\n+\n+    def __repr__(self):\n+        return (\n+            f\"ContinuousBatchProcessor(input_queue={self.input_queue}, output_queue={self.output_queue}, active_requests={self.scheduler.active_requests}, waiting_requests={self.scheduler.waiting_requests})\"\n+            + self.get_model_kwargs().__repr__()\n+        )\n+\n+    @traced\n+    def _get_new_requests(self):\n+        \"\"\"Pull new requests from the input queue and add to waiting list.\"\"\"\n+        while not self.input_queue.empty():\n+            try:\n+                state = self.input_queue.get_nowait()\n+                if state is None:  # Sentinel value\n+                    continue\n+                self.scheduler.add_waiting_request(state)\n+\n+            except queue.Empty:\n+                break\n+            except Exception as e:\n+                logger.error(f\"Error processing new request: {e}\", exc_info=True)\n+                state: RequestState = locals().get(\"state\")\n+                if state is not None:\n+                    self._handle_request_error(e, state)\n+\n+    @traced\n+    def _handle_request_error(self, error, state: RequestState):\n+        \"\"\"Handle general request processing error.\"\"\"\n+        state.status = RequestStatus.FAILED\n+        state.error = str(error)\n+\n+        # Include any generated tokens if this is an active request\n+        if isinstance(state.request_id, str):\n+            state.static_outputs = self.scheduler.get_active_request_static_outputs(state.request_id)\n+        else:\n+            state.static_outputs = []\n+\n+        self.metrics.record_request_completion(state.created_time, state.request_id)\n+        self.output_queue.put(state.to_generation_output())\n+\n+    @traced\n+    def prepare_next_batch(self):\n+        \"\"\"Prepare tensors and metadata for the next model forward pass.\"\"\"\n+        # Get new requests from the queue\n+        self._get_new_requests()\n+        if not self.scheduler.has_pending_requests():\n+            return None\n+\n+        self.metrics.record_queue_metrics(len(self.scheduler.active_requests), len(self.scheduler.waiting_requests))\n+\n+        self.requests_in_batch = self.scheduler.schedule_batch(self.max_batch_tokens)\n+        if not self.requests_in_batch:\n+            return None\n+\n+        # Get the request objects for this batch\n+        self.reset_static_tensors()\n+        position_ids = []\n+        input_ids = []\n+        read_index = []\n+        write_index = []\n+        cumulative_seqlens_q = [0]\n+        cumulative_seqlens_k = [0]\n+        logits_indices = []\n+        self.metrics.record_batch_metrics(self.requests_in_batch)\n+\n+        for state in self.requests_in_batch:\n+            next_input_ids = state.prompt_ids\n+            input_ids.extend(next_input_ids)\n+            past_length = state.position_offset\n+            query_length = len(next_input_ids)\n+            key_length = query_length + past_length\n+            cache_index = list(range(key_length))\n+\n+            positions_to_add = cache_index[past_length:]\n+            read_indices = self.cache._get_physical_indices(state, cache_index)\n+            write_indices = read_indices[-query_length:]\n+\n+            position_ids.extend(positions_to_add)\n+            read_index.extend(read_indices)\n+            write_index.extend(write_indices)\n+            cumulative_seqlens_q.append(cumulative_seqlens_q[-1] + query_length)\n+            cumulative_seqlens_k.append(cumulative_seqlens_k[-1] + key_length)\n+            if len(state.remaining_prompt_ids) == 0:\n+                logits_indices.append(cumulative_seqlens_q[-1] - 1)\n+            self.max_seqlen_q = max(self.max_seqlen_q, query_length)\n+            self.max_seqlen_k = max(self.max_seqlen_k, key_length)\n+            state.position_offset += query_length\n+\n+        logger.debug(\n+            f\"Scheduled: {len(self.requests_in_batch)}, Waiting: {len(self.scheduler.waiting_requests)}, \"\n+            f\"Active: {len(self.scheduler.active_requests)}. cum Q: {cumulative_seqlens_q[-1]}. \"\n+            f\"cum KV: {cumulative_seqlens_k[-1]}, free blocks: {self.cache.get_num_free_blocks()}\"\n+        )\n+        self._build_tensors(\n+            input_ids,\n+            position_ids,\n+            read_index,\n+            write_index,\n+            cumulative_seqlens_q,\n+            cumulative_seqlens_k,\n+            logits_indices,\n+        )\n+\n+        self.metrics.record_kv_cache_memory_metrics(self.cache)\n+\n+    @traced\n+    def _build_tensors(\n+        self,\n+        input_ids,\n+        position_ids,\n+        read_index,\n+        write_index,\n+        cumulative_seqlens_q,\n+        cumulative_seqlens_k,\n+        logits_indices,\n+    ):\n+        to_tensor = partial(torch.tensor, **self.tensor_metadata)\n+        self.input_ids[:, : len(input_ids)] = to_tensor(input_ids)\n+        self.position_ids[:, : len(position_ids)] = to_tensor(position_ids)\n+        self.write_index[: len(write_index)] = to_tensor(write_index)\n+        self.read_index[: len(read_index)] = to_tensor(read_index)\n+        self.cumulative_seqlens_q[: len(cumulative_seqlens_q)] = to_tensor(cumulative_seqlens_q)\n+        self.cumulative_seqlens_k[: len(cumulative_seqlens_k)] = to_tensor(cumulative_seqlens_k)\n+        self.logits_indices[: len(logits_indices)] = to_tensor(logits_indices)\n+\n+        self.actual_tokens = len(input_ids)\n+        self.cache_used = len(read_index)\n+\n+        min_value = torch.finfo(self.model_dtype).min\n+        if self.attention_mask is not None:\n+            for i in range(len(cumulative_seqlens_q) - 1):\n+                if (\n+                    cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i]\n+                    < cumulative_seqlens_k[i + 1] - cumulative_seqlens_k[i]\n+                    and cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i] >= 1\n+                ):\n+                    diagonal = (\n+                        cumulative_seqlens_k[i + 1] - (cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i]) + 1\n+                    )\n+                    diagonal = diagonal - cumulative_seqlens_k[i]\n+                else:\n+                    diagonal = 1\n+                query_range = slice(cumulative_seqlens_q[i], cumulative_seqlens_q[i + 1])\n+                key_range = slice(cumulative_seqlens_k[i], cumulative_seqlens_k[i + 1])\n+\n+                mask = torch.triu(\n+                    torch.full(\n+                        self.attention_mask[..., query_range, key_range].shape,\n+                        min_value,\n+                        dtype=self.model_dtype,\n+                        device=self.model_device,\n+                    ),\n+                    diagonal=diagonal,\n+                )\n+                self.attention_mask[..., query_range, key_range] = mask\n+\n+    @traced\n+    def _sync(self):\n+        if self.output_ids is not None:\n+            try:\n+                out = self.output_ids.tolist()[0]  # should be the only synch we do\n+            except Exception:\n+                out = [0, 1]\n+        else:\n+            out = [0, 0]\n+        return out\n+\n+    @traced\n+    def _maybe_send_output(self, state: RequestState, token: int):\n+        \"\"\"Send output to the queue based on streaming mode and request state.\"\"\"\n+        if self.streaming:\n+            state.next_token = self.decode_stream.step(self.tokenizer, state.static_outputs[-1])\n+            self.output_queue.put(state.to_generation_output())\n+        elif state.status == RequestStatus.FINISHED:\n+            self.output_queue.put(state.to_generation_output())\n+\n+    @traced\n+    def update_batch(self):\n+        \"\"\"Update request states based on generated tokens.\"\"\"\n+        out_tokens = self._sync()\n+        finished_request_ids = []\n+        for i, state in enumerate(self.requests_in_batch):\n+            req_id = state.request_id\n+            if len(state.remaining_prompt_ids) == 0:\n+                self.metrics.record_ttft_metric(state.created_time, state.request_id)\n+                state.status = RequestStatus.DECODING\n+                token = out_tokens[self.logits_indices[i]]\n+                state.prompt_ids = [token]\n+                if state.update_with_token(token):\n+                    self.metrics.record_request_completion(state.created_time, state.request_id)\n+                    self.scheduler.finish_request(state.request_id, evict_from_cache=(not self.manual_eviction))\n+                    finished_request_ids.append(req_id)\n+                self._maybe_send_output(state, token)\n+            elif state.status == RequestStatus.PREFILLING_SPLIT:\n+                state.status = RequestStatus.SPLIT_PENDING_REMAINDER\n+        if self.cache.get_num_free_blocks() == 0:\n+            raise ValueError(\"No more free blocks\")\n+\n+    @traced\n+    def has_pending_requests(self) -> bool:\n+        \"\"\"Check if there are any active or waiting requests.\"\"\"\n+        return self.scheduler.has_pending_requests()\n+\n+    @traced\n+    def handle_batch_error(self, error):\n+        \"\"\"Handle errors during batch processing.\"\"\"\n+        failed_reqs = self.requests_in_batch\n+        for req in failed_reqs:\n+            self._handle_request_error(error, req)\n+            self.scheduler.finish_request(req.request_id)\n+\n+    @traced\n+    def fail_all_requests(self, error):\n+        \"\"\"Fail all active requests with the given error.\n+\n+        Args:\n+            error: The error to report in the failure message\n+        \"\"\"\n+\n+        requests = list(self.scheduler.active_requests.values())\n+        for state in requests:\n+            self._handle_request_error(error, state)\n+            self.scheduler.finish_request(state.request_id)\n+\n+        # Also fail any requests in the waiting queue\n+        for req_id in list(self.scheduler.waiting_requests.keys()):\n+            state = self.scheduler.waiting_requests.pop(req_id)\n+            self._handle_request_error(error, state)\n+\n+        # Clear the ordering queue\n+        self.scheduler.waiting_requests_order.clear()\n+\n+\n+# Manager Class (User Interface)\n+@attach_tracer()\n+class ContinuousBatchingManager:\n+    \"\"\"Manager for handling continuous batching of generation requests.\n+\n+    This class provides the user interface for submitting generation requests,\n+    retrieving results, and managing the background generation thread.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        model,\n+        generation_config: GenerationConfig,\n+        manual_eviction: bool = False,\n+        max_queue_size=0,\n+        streaming: bool = True,\n+        slice_inputs: bool = True,\n+    ):\n+        \"\"\"Initialize the continuous batching manager.\n+\n+        Args:\n+            model: The language model for generation\n+            generation_config: Configuration for generation parameters\n+            max_queue_size: Maximum size of the request queue (0 = unlimited)\n+            streaming: Whether to stream tokens as they are generated\n+        \"\"\"\n+        self.model = model.eval()\n+        generation_config = model.generation_config if generation_config is None else generation_config\n+        self.generation_config = generation_config\n+        self.input_queue = queue.Queue(maxsize=max_queue_size)\n+        self.output_queue = queue.Queue()\n+        self.stop_event = threading.Event()\n+        self.streaming = streaming\n+        self.log_prob_generation = getattr(generation_config, \"log_prob_generation\", False)\n+        self._generation_thread = None\n+        self._request_counter = 0\n+        self._request_lock = threading.Lock()\n+        self.model.generation_config.top_p = None\n+        self.do_sample = getattr(generation_config, \"do_sample\", True)\n+        self.logit_processor = self.model._get_logits_processor(generation_config)\n+        self.use_cuda_graph = getattr(generation_config, \"use_cuda_graph\", True)\n+        self.profile = getattr(generation_config, \"profile\", False)\n+        self.manual_eviction = manual_eviction\n+        self.batch_processor: Optional[ContinuousBatchProcessor] = None\n+        self.decode_stream = DecodeStream(skip_special_tokens=True)\n+        self.slice_inputs = slice_inputs\n+\n+    @traced\n+    def start(self):\n+        \"\"\"Start the background generation thread.\"\"\"\n+        if self._generation_thread is not None and self._generation_thread.is_alive():\n+            logger.warning(\"Manager thread is already running.\")\n+            return\n+\n+        self._result_queue = queue.Queue()\n+        self._generation_thread = threading.Thread(target=self._run_generation_loop)\n+        self._generation_thread.start()\n+        logger.info(\"Continuous batching manager started.\")\n+\n+    def is_running(self):\n+        \"\"\"Check if the background generation thread is running.\"\"\"\n+        return self._generation_thread is not None and self._generation_thread.is_alive()\n+\n+    def stop(self, block: bool = False, timeout: Optional[float] = None):\n+        \"\"\"Signal the background thread to stop.\n+\n+        Args:\n+            block: Whether to wait for the thread to stop\n+            timeout: Maximum time to wait for the thread to stop\n+        \"\"\"\n+        if self._generation_thread is None:\n+            logger.warning(\"Manager not started.\")\n+            return\n+\n+        if not self.stop_event.is_set():\n+            self.stop_event.set()\n+            logger.info(\"Stopping continuous batching manager...\")\n+\n+        if block:\n+            self.join(timeout)\n+\n+    def join(self, timeout: Optional[float] = None):\n+        \"\"\"Wait for the background thread to finish.\n+\n+        Args:\n+            timeout: Maximum time to wait for the thread to stop\n+        \"\"\"\n+        if self._generation_thread is not None:\n+            self._generation_thread.join(timeout=timeout)\n+            if self._generation_thread.is_alive():\n+                logger.warning(\"Generation thread did not exit after join timeout.\")\n+            else:\n+                logger.info(\"Continuous Batching Manager stopped.\")\n+                self._generation_thread = None\n+\n+    def add_request(\n+        self, input_ids: list[int], request_id: Optional[str] = None, max_new_tokens: Optional[int] = None\n+    ) -> str:\n+        \"\"\"Add a new generation request to the queue.\n+\n+        Args:\n+            input_ids: Input token IDs to use as prompt\n+            request_id: Optional custom request ID (auto-generated if None)\n+            **kwargs: Additional generation parameters\n+\n+        Returns:\n+            str: The request ID\n+        \"\"\"\n+        if request_id is None:\n+            with self._request_lock:\n+                request_id = f\"req_{self._request_counter}\"\n+                self._request_counter += 1\n+\n+        max_new_tokens = self.generation_config.max_new_tokens if max_new_tokens is None else max_new_tokens\n+\n+        state = RequestState(\n+            request_id=request_id,\n+            prompt_ids=list(input_ids),\n+            full_prompt_ids=list(input_ids),\n+            max_new_tokens=max_new_tokens,\n+            eos_token_id=self.generation_config.eos_token_id,\n+        )\n+\n+        # Use block=True with timeout to handle backpressure if queue is full\n+        self.input_queue.put(state, block=True, timeout=10)  # XXX: pass timeout as fn arg?\n+        logger.debug(f\"Added request {request_id} to queue.\")\n+        return request_id\n+\n+    def add_requests(self, inputs: list[list[int]], **kwargs):\n+        for i, input_ids in enumerate(inputs):\n+            # Assign a predictable request ID for ordering results later\n+            req_id = f\"batch_req_{i}\"\n+            self.add_request(input_ids, request_id=req_id, **kwargs)\n+\n+    def get_result(self, timeout=None) -> Optional[GenerationOutput]:\n+        \"\"\"Retrieve one result from the output queue.\n+\n+        Args:\n+            timeout: Maximum time to wait for a result\n+\n+        Returns:\n+            Optional[Dict]: The result data or None if timeout\n+        \"\"\"\n+        if self._generation_thread is None and self.output_queue.empty():\n+            return None\n+        try:\n+            result = self.output_queue.get(block=True, timeout=timeout)\n+            logger.debug(f\"Retrieved result for request {result.request_id}\")\n+            return result\n+        except queue.Empty:\n+            return None\n+\n+    def __iter__(self):\n+        \"\"\"Iterate over results as they become available.\"\"\"\n+        while (\n+            self._generation_thread is not None and self._generation_thread.is_alive() or not self.output_queue.empty()\n+        ):\n+            result = self.get_result(timeout=0.1)  # allow the model to run for 10 seconds\n+            if result is not None:\n+                yield result\n+\n+    @traced\n+    def warmup(self, batch_processor):\n+        stream = torch.cuda.Stream(device=self.model.device)\n+        stream.wait_stream(torch.cuda.current_stream())\n+        with torch.cuda.stream(stream):\n+            # Warmup the model with a dummy forward pass\n+            self._generation_step(batch_processor)\n+        torch.cuda.current_stream().wait_stream(stream)\n+\n+        self.graph = torch.cuda.CUDAGraph()\n+        with torch.cuda.graph(self.graph, stream=stream):\n+            self._generation_step(batch_processor)\n+\n+    @traced\n+    # @torch.compile\n+    def _generation_step(self, batch_processor: ContinuousBatchProcessor):\n+        \"\"\"Perform a single generation step. This is cuda graphed\"\"\"\n+        batch_data = batch_processor.get_model_kwargs()\n+        with torch.no_grad():\n+            logits = self._model_forward(batch_data)\n+            if self.log_prob_generation:\n+                batch_processor.output_probs.copy_(logits)  # TODO\n+            probs = self._process_logit(batch_data, logits)\n+            self._sample(batch_processor, probs)\n+\n+    @traced(span_name=\"model_forward\")\n+    def _model_forward(self, batch_data):\n+        return self.model(**batch_data).logits\n+\n+    @traced(span_name=\"logit_processing\")\n+    def _process_logit(self, batch_data, logits):\n+        # Pass continuous batching context to logits processor if it supports it. TODO we should find a way to make this a little bit cleaner!\n+        if hasattr(self.logit_processor, \"set_continuous_batching_context\"):\n+            self.logit_processor.set_continuous_batching_context(\n+                batch_data[\"logits_indices\"], batch_data[\"cu_seq_lens_q\"]\n+            )\n+        return self.logit_processor(batch_data[\"input_ids\"], logits)\n+\n+    @traced(span_name=\"sampling\")\n+    def _sample(self, batch_processor: ContinuousBatchProcessor, probs):\n+        if self.do_sample:  # sample\n+            probs = nn.functional.softmax(probs, dim=-1)\n+            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(1)\n+        else:\n+            next_tokens = torch.argmax(probs, dim=-1)\n+        tokens = next_tokens.size(1)\n+        batch_processor.output_ids[:, :tokens].copy_(next_tokens)\n+\n+    def _run_generation_loop(self):\n+        \"\"\"Main processing loop running in the background thread.\"\"\"\n+        batch_processor = None\n+        try:\n+            paged_attention_cache = PagedAttentionCache(\n+                self.model.config,\n+                self.generation_config,\n+                self.model.device,\n+                self.model.dtype,\n+                num_requests=len(self.input_queue.queue),\n+                tp_size=getattr(self.model, \"_tp_size\", None),  # Use model's actual TP setting\n+            )\n+\n+            scheduler = None\n+            if hasattr(self.generation_config, \"scheduler\"):\n+                scheduler = SCHEDULER_MAPPING.get(self.generation_config.scheduler, None)\n+                if scheduler is None:\n+                    logger.warning(f\"Scheduler '{scheduler}' not found. Defaulting to FIFO.\")\n+                    scheduler = FIFOScheduler\n+            else:\n+                # Default to fifo\n+                scheduler = FIFOScheduler\n+\n+            batch_processor = ContinuousBatchProcessor(\n+                paged_attention_cache,\n+                self.model.config,\n+                self.generation_config,\n+                self.input_queue,\n+                self.output_queue,\n+                self.stop_event,\n+                self.model.device,\n+                self.model.dtype,\n+                scheduler(paged_attention_cache, self.manual_eviction),\n+                self.streaming,\n+                self.manual_eviction,\n+                slice_inputs=self.slice_inputs,\n+            )\n+            self.batch_processor = batch_processor\n+            self.current_batch = 0\n+            while (not self.stop_event.is_set()) or batch_processor.has_pending_requests():\n+                self._inner_generation_loop(batch_processor)\n+                self.current_batch += 1\n+\n+        except Exception as e:\n+            logger.error(f\"Error in generation loop: {e}\", exc_info=True)\n+            self._handle_critical_error(e, batch_processor)\n+        finally:\n+            logger.info(\"Generation loop finished.\")\n+\n+    @traced(span_name=\"generation_loop\")\n+    def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor):\n+        if torch.cuda.is_available():\n+            torch.cuda.synchronize()\n+        batch_processor.prepare_next_batch()\n+        device, total, reserved, allocated = get_device_and_memory_breakdown()\n+        logger.debug(f\"[Memory] Device: {device}, Total: {total}, Reserved: {reserved}, Allocated: {allocated}\")\n+        if torch.cuda.is_available() and self.use_cuda_graph:\n+            if self.current_batch == 0:\n+                self.warmup(batch_processor)\n+            elif hasattr(self, \"graph\"):\n+                try:\n+                    self._graph_replay()\n+                except Exception as e:\n+                    logger.error(f\"Model forward pass failed: {e}\", exc_info=True)\n+                    batch_processor.handle_batch_error(e)\n+                    return\n+            else:\n+                self._generation_step(batch_processor)\n+        else:\n+            self._generation_step(batch_processor)\n+        if torch.cuda.is_available():\n+            torch.cuda.synchronize()\n+        batch_processor.update_batch()\n+\n+    @traced(span_name=\"graph_replay\")\n+    def _graph_replay(self):\n+        self.graph.replay()\n+\n+    @traced\n+    def _handle_critical_error(self, error, batch_processor: Optional[ContinuousBatchProcessor]):\n+        \"\"\"Handle critical errors that terminate the generation loop.\"\"\"\n+        # Signal stop\n+        self.stop_event.set()\n+\n+        # Fail pending requests in input queue\n+        try:\n+            while True:\n+                req_data = self.input_queue.get_nowait()\n+                if batch_processor is not None:\n+                    batch_processor._handle_request_error(error, req_data)\n+        except queue.Empty:\n+            pass\n+\n+        # Fail active requests\n+        if batch_processor is not None:\n+            batch_processor.fail_all_requests(error)\n+\n+    @traced\n+    def evict_request_from_cache(self, request_id: str):\n+        \"\"\"Evict a request from the cache. It is assumed that the request is already finished.\"\"\"\n+        if not self.manual_eviction:\n+            raise RuntimeError(\"Manual eviction is not enabled for this manager.\")\n+        if self.batch_processor is not None:\n+            self.batch_processor.scheduler.finish_request(request_id)\n+\n+\n+class ContinuousMixin:\n+    \"\"\"Mixin class for models to add continuous batching capabilities.\"\"\"\n+\n+    def init_continuous_batching(\n+        self,\n+        generation_config: Optional[GenerationConfig] = None,\n+        manual_eviction: bool = False,\n+        max_queue_size: int = 0,\n+        streaming: bool = False,\n+        slice_inputs: bool = True,\n+    ) -> ContinuousBatchingManager:\n+        \"\"\"Initialize a manager for continuous batching inference.\n+\n+        Args:\n+            generation_config: Custom generation configuration\n+            max_queue_size: Maximum size of the input request queue\n+            streaming: Whether to stream tokens as they are generated\n+\n+        Returns:\n+            `ContinuousBatchingManager`: The manager instance to add requests and retrieve results.\n+        \"\"\"\n+        if not hasattr(self, \"config\") or not hasattr(self, \"device\") or not hasattr(self, \"dtype\"):\n+            raise AttributeError(\"Model must have 'config', 'device', and 'dtype' attributes.\")\n+\n+        gen_config = generation_config if generation_config is not None else self.generation_config\n+        if gen_config is None:\n+            raise ValueError(\"A GenerationConfig must be provided or set in the model.\")\n+\n+        if gen_config.eos_token_id is None:\n+            logger.warning(\"`eos_token_id` not set in GenerationConfig. Setting to -1 (disabled).\")\n+            gen_config.eos_token_id = -1\n+\n+        # Create and return the manager\n+        return ContinuousBatchingManager(\n+            model=self,\n+            generation_config=gen_config,\n+            manual_eviction=manual_eviction,\n+            max_queue_size=max_queue_size,\n+            streaming=streaming,\n+            slice_inputs=slice_inputs,\n+        )\n+\n+    @traced\n+    @torch.inference_mode()\n+    def generate_batch(\n+        self,\n+        inputs: list[list[int]],\n+        generation_config: Optional[GenerationConfig] = None,\n+        progress_bar: bool = True,\n+        slice_inputs: bool = True,\n+        **kwargs,\n+    ) -> list[list[int]]:\n+        \"\"\"Generate sequences for a batch of prompts using continuous batching.\n+\n+        Args:\n+            inputs: List of input token sequences (prompts)\n+            generation_config: Optional generation configuration\n+            **kwargs: Additional generation parameters\n+\n+        Returns:\n+            `list[list[int]]`: A list containing the generated sequences (including prompt tokens\n+                                if not handled otherwise) for each input prompt, in the same order.\n+                                Returns an empty list `[]` for requests that failed.\n+        \"\"\"\n+        if not inputs:\n+            return []\n+        if logger.getEffectiveLevel() <= logging.DEBUG:\n+            logger.warning(\"Progress bar is disabled when logger level is less than DEBUG\")\n+            progress_bar = False\n+\n+        # Initialize manager with the batch inputs\n+        manager = self.init_continuous_batching(generation_config=generation_config, slice_inputs=slice_inputs)\n+        manager.start()\n+        results = {}\n+        num_requests = len(inputs)\n+        try:\n+            from tqdm.contrib.logging import logging_redirect_tqdm\n+\n+            with logging_redirect_tqdm([logger]):\n+                with tqdm(\n+                    total=num_requests,\n+                    disable=(not progress_bar),\n+                    desc=f\"Solving {num_requests} requests\",\n+                    unit=\"request\",\n+                ) as pbar:\n+                    manager.add_requests(inputs, **kwargs)\n+                    finished_count = 0\n+                    while finished_count < num_requests:\n+                        result = manager.get_result(timeout=1)\n+                        if result:\n+                            req_id = result.request_id\n+                            if result.status == RequestStatus.FINISHED:\n+                                results[req_id] = result\n+                                finished_count += 1\n+                                pbar.update(1)\n+                            logger.debug(manager.batch_processor.tokenizer.decode(result.generated_tokens))\n+                        else:\n+                            if not manager.is_running():\n+                                logger.error(\"Generation thread terminated unexpectedly.\")\n+                                break\n+\n+        except Exception as e:\n+            logger.error(f\"Error during batch generation: {e}\", exc_info=True)\n+        finally:\n+            manager.stop(block=True, timeout=5.0)\n+        return results"
        },
        {
            "sha": "9f612c9380ff22389a86ec14bdde799e1d74c8a7",
            "filename": "src/transformers/generation/continuous_batching/scheduler.py",
            "status": "added",
            "additions": 314,
            "deletions": 0,
            "changes": 314,
            "blob_url": "https://github.com/huggingface/transformers/blob/34108a2230c4591c9a20e299ca22edc147c3918f/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34108a2230c4591c9a20e299ca22edc147c3918f/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py?ref=34108a2230c4591c9a20e299ca22edc147c3918f",
            "patch": "@@ -0,0 +1,314 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from abc import ABC, abstractmethod\n+from collections import deque\n+\n+from ...utils.metrics import attach_tracer, traced\n+from .cache import PagedAttentionCache\n+from .classes import RequestState, RequestStatus\n+\n+\n+class Scheduler(ABC):\n+    \"\"\"\n+    Abstract base class for scheduling requests in the continuous batch processor.\n+    It is expected that cache allocation and scheduling logic will be implemented in subclasses.\n+    \"\"\"\n+\n+    def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = False):\n+        self.active_requests: dict[str, RequestState] = {}\n+        self.waiting_requests: dict[str, RequestState] = {}\n+        self.waiting_requests_order: deque[str] = deque()\n+        self.cache = cache\n+        self.retain_cache_on_finish = retain_cache_on_finish\n+\n+    @abstractmethod\n+    def add_waiting_request(self, state: RequestState):\n+        \"\"\"Add a request to the waiting list.\"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def schedule_batch(self, token_budget: int) -> list[RequestState]:\n+        pass\n+\n+    @traced\n+    def has_pending_requests(self) -> bool:\n+        \"\"\"Check if there are requests ready to be processed.\"\"\"\n+        return len(self.active_requests) or len(self.waiting_requests)\n+\n+    @abstractmethod\n+    def finish_request(self, request_id: str, evict_from_cache: bool = True):\n+        \"\"\"Finish processing a request and free its allocated blocks.\"\"\"\n+        pass\n+\n+    @traced\n+    def get_active_request_static_outputs(self, request_id: str) -> list[int]:\n+        if request_id in self.active_requests:\n+            return self.active_requests[request_id].static_outputs\n+        return []\n+\n+\n+@attach_tracer()\n+class FIFOScheduler(Scheduler):\n+    def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = False, safety_margin: float = 0.0):\n+        super().__init__(cache, retain_cache_on_finish)\n+        self.safety_margin = safety_margin\n+\n+    @traced\n+    def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int):\n+        # 1. we check that the occupancy is less than the requested length\n+        # 2. we allocate enough blocks to cover the requested length\n+        current_len = state.current_len()\n+        occupancy = len(state.allocated_blocks) * self.cache.block_size - current_len\n+        if occupancy < len_next_tokens or (len(state.allocated_blocks) == 0):\n+            blocks_needed = ((len_next_tokens - occupancy + 1) // self.cache.block_size) + 1\n+            allocated = self.cache.allocate_blocks(blocks_needed, state.request_id)\n+            if not allocated:\n+                return False\n+            state.allocated_blocks.extend(allocated)\n+        return True\n+\n+    @traced(span_name=\"prepare_request\")\n+    def _prepare_request_for_processing(\n+        self, state: RequestState, token_budget: int, request_ids_to_remove_from_waiting: set[str]\n+    ):\n+        \"\"\"Prepare a request for processing in the current batch.\"\"\"\n+        request_tokens = (\n+            state.remaining_prompt_ids if state.status == RequestStatus.SPLIT_PENDING_REMAINDER else state.prompt_ids\n+        )\n+        if len(request_tokens) < token_budget:\n+            # Can process the entire prompt/remainder\n+            if state.status == RequestStatus.PENDING:\n+                self.active_requests[state.request_id] = state\n+                state.status = RequestStatus.PREFILLING\n+                request_ids_to_remove_from_waiting.add(state.request_id)\n+            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+                state.status = RequestStatus.PREFILLING\n+                state.prompt_ids = state.remaining_prompt_ids\n+                state.remaining_prompt_ids = []\n+        else:\n+            # Need to split the request\n+            if state.status == RequestStatus.PENDING:\n+                self.active_requests[state.request_id] = state\n+                state.status = RequestStatus.PREFILLING_SPLIT\n+                request_ids_to_remove_from_waiting.add(state.request_id)\n+            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+                state.status = RequestStatus.PREFILLING_SPLIT\n+            state.remaining_prompt_ids = request_tokens[token_budget:]\n+            state.prompt_ids = request_tokens[:token_budget]\n+\n+    @traced\n+    def add_waiting_request(self, state: RequestState):\n+        \"\"\"Add a request to the waiting list.\"\"\"\n+        if self.retain_cache_on_finish and state.request_id in self.active_requests:\n+            old_state = self.active_requests.pop(state.request_id)\n+            state.prompt_ids = state.prompt_ids[len(old_state.full_prompt_ids) :]\n+            state.allocated_blocks = old_state.allocated_blocks\n+            state.position_offset = old_state.position_offset\n+        self.waiting_requests[state.request_id] = state\n+        self.waiting_requests_order.append(state.request_id)\n+\n+    @traced\n+    def schedule_batch(self, token_budget: int) -> list[RequestState]:\n+        priority_states: list[RequestState] = []\n+        second_priority_states: list[RequestState] = []\n+        scheduled_requests = []\n+\n+        for state in self.active_requests.values():\n+            if state.status == RequestStatus.DECODING:\n+                priority_states.append(state)\n+            if state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+                second_priority_states.append(state)\n+\n+        # Add waiting requests to second priority\n+        for req_id in self.waiting_requests_order:\n+            second_priority_states.append(self.waiting_requests[req_id])\n+\n+        candidates = priority_states + second_priority_states\n+        request_ids_to_remove_from_waiting = set()\n+        safety_margins = self.safety_margin * self.cache.num_blocks\n+\n+        for state in candidates:\n+            # If we are out the safety margin, we only accept decoding requests or the first prefill request\n+            num_free_blocks = self.cache.get_num_free_blocks()\n+            outside_safety_margin = num_free_blocks < safety_margins\n+            if outside_safety_margin and scheduled_requests and state.status != RequestStatus.DECODING:\n+                break\n+\n+            self._prepare_request_for_processing(state, token_budget, request_ids_to_remove_from_waiting)\n+            request_len = len(state.prompt_ids)\n+            if not self._allocate_blocks_if_needed(\n+                state, len(state.prompt_ids)\n+            ):  # don't schedule if we can't allocate blocks\n+                if len(self.cache._free_blocks) == 0:\n+                    break\n+                continue\n+\n+            @traced\n+            def _add_to_scheduled_requests(state: RequestState):\n+                scheduled_requests.append(state)\n+\n+            _add_to_scheduled_requests(state)\n+\n+            token_budget -= request_len\n+\n+            @traced\n+            def _remove_from_waiting_requests(state: RequestState):\n+                req_id = state.request_id\n+                if req_id in self.waiting_requests:\n+                    del self.waiting_requests[req_id]\n+                    request_ids_to_remove_from_waiting.add(req_id)\n+\n+            _remove_from_waiting_requests(state)\n+\n+            if token_budget == 0:\n+                break\n+\n+        self.waiting_requests_order = deque(\n+            [req_id for req_id in self.waiting_requests_order if req_id not in request_ids_to_remove_from_waiting]\n+        )\n+\n+        return scheduled_requests\n+\n+    @traced\n+    def finish_request(self, request_id: str, evict_from_cache: bool = True):\n+        if evict_from_cache:\n+            self.cache.free_blocks(request_id)\n+            if request_id in self.active_requests:\n+                del self.active_requests[request_id]\n+\n+\n+@attach_tracer()\n+class PrefillFirstScheduler(Scheduler):\n+    @traced\n+    def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int):\n+        # 1. we check that the occupancy is less than the requested length\n+        # 2. we allocate enough blocks to cover the requested length\n+        current_len = state.current_len()\n+        occupancy = len(state.allocated_blocks) * self.cache.block_size - current_len\n+        if occupancy < len_next_tokens or (len(state.allocated_blocks) == 0):\n+            blocks_needed = ((len_next_tokens - occupancy + 1) // self.cache.block_size) + 1\n+            allocated = self.cache.allocate_blocks(blocks_needed, state.request_id)\n+            if not allocated:\n+                return False\n+            state.allocated_blocks.extend(allocated)\n+        return True\n+\n+    @traced(span_name=\"prepare_request\")\n+    def _prepare_request_for_processing(\n+        self, state: RequestState, token_budget: int, request_ids_to_remove_from_waiting: set[str]\n+    ):\n+        \"\"\"Prepare a request for processing in the current batch.\"\"\"\n+        request_tokens = (\n+            state.remaining_prompt_ids if state.status == RequestStatus.SPLIT_PENDING_REMAINDER else state.prompt_ids\n+        )\n+        if len(request_tokens) < token_budget:\n+            # Can process the entire prompt/remainder\n+            if state.status == RequestStatus.PENDING:\n+                self.active_requests[state.request_id] = state\n+                state.status = RequestStatus.PREFILLING\n+                request_ids_to_remove_from_waiting.add(state.request_id)\n+            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+                state.status = RequestStatus.PREFILLING\n+                state.prompt_ids = state.remaining_prompt_ids\n+                state.remaining_prompt_ids = []\n+        else:\n+            # Need to split the request\n+            if state.status == RequestStatus.PENDING:\n+                self.active_requests[state.request_id] = state\n+                state.status = RequestStatus.PREFILLING_SPLIT\n+                request_ids_to_remove_from_waiting.add(state.request_id)\n+            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+                state.status = RequestStatus.PREFILLING_SPLIT\n+            state.remaining_prompt_ids = request_tokens[token_budget:]\n+            state.prompt_ids = request_tokens[:token_budget]\n+\n+    @traced\n+    def add_waiting_request(self, state: RequestState):\n+        \"\"\"Add a request to the waiting list.\"\"\"\n+        if self.retain_cache_on_finish and state.request_id in self.active_requests:\n+            old_state = self.active_requests.pop(state.request_id)\n+            state.prompt_ids = state.prompt_ids[len(old_state.full_prompt_ids) :]  # XXX: check for indexing error?\n+            state.allocated_blocks = old_state.allocated_blocks\n+            state.position_offset = old_state.position_offset\n+        self.waiting_requests[state.request_id] = state\n+        self.waiting_requests_order.append(state.request_id)\n+\n+    @traced\n+    def schedule_batch(self, token_budget: int) -> list[RequestState]:\n+        priority_states: list[RequestState] = []\n+        second_priority_states: list[RequestState] = []\n+        scheduled_requests = []\n+\n+        for state in self.active_requests.values():\n+            if state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+                priority_states.append(state)\n+            elif state.status == RequestStatus.DECODING:\n+                second_priority_states.append(state)\n+\n+        for req_id in self.waiting_requests_order:\n+            second_priority_states.append(self.waiting_requests[req_id])\n+\n+        candidates = priority_states + second_priority_states\n+\n+        request_ids_to_remove_from_waiting = set()\n+\n+        for state in candidates:\n+            self._prepare_request_for_processing(state, token_budget, request_ids_to_remove_from_waiting)\n+            request_len = len(state.prompt_ids)\n+            if not self._allocate_blocks_if_needed(\n+                state, len(state.prompt_ids)\n+            ):  # don't schedule if we can't allocate blocks\n+                if len(self.cache._free_blocks) == 0:\n+                    break\n+                continue\n+\n+            @traced\n+            def _add_to_scheduled_requests(state: RequestState):\n+                scheduled_requests.append(state)\n+\n+            _add_to_scheduled_requests(state)\n+\n+            token_budget -= request_len\n+\n+            @traced\n+            def _remove_from_waiting_requests(state: RequestState):\n+                req_id = state.request_id\n+                if req_id in self.waiting_requests:\n+                    del self.waiting_requests[req_id]\n+                    request_ids_to_remove_from_waiting.add(req_id)\n+\n+            _remove_from_waiting_requests(state)\n+\n+            if token_budget == 0:\n+                break\n+\n+        self.waiting_requests_order = deque(\n+            [req_id for req_id in self.waiting_requests_order if req_id not in request_ids_to_remove_from_waiting]\n+        )\n+\n+        return scheduled_requests\n+\n+    @traced\n+    def finish_request(self, request_id: str, evict_from_cache: bool = True):\n+        if evict_from_cache:\n+            self.cache.free_blocks(request_id)\n+            if request_id in self.active_requests:\n+                del self.active_requests[request_id]\n+\n+\n+SCHEDULER_MAPPING = {\n+    \"fifo\": FIFOScheduler,\n+    \"prefill_first\": PrefillFirstScheduler,\n+}"
        }
    ],
    "stats": {
        "total": 3721,
        "additions": 2174,
        "deletions": 1547
    }
}