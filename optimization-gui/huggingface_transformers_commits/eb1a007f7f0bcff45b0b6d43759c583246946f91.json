{
    "author": "zucchini-nlp",
    "message": "Rename `supports_static_cache` to `can_compile_fullgraph` (#39505)\n\n* update all\n\n* Apply suggestions from code review\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* apply suggestions\n\n* fix copies\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "eb1a007f7f0bcff45b0b6d43759c583246946f91",
    "files": [
        {
            "sha": "e56eeec7d75bb375cff071056afa0a931df5d6f2",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -294,7 +294,7 @@ class MyNewModel2PreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": MyNewModel2DecoderLayer,"
        },
        {
            "sha": "2a3df8e9c1d0bb48d97ce3d269e97252d9bd78a8",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -94,7 +94,7 @@ class NewTaskModelPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "ee90750cac255165bc516dc675575336ee5391ee",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -293,7 +293,7 @@ class SuperPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": SuperDecoderLayer,"
        },
        {
            "sha": "6f4adcfeb14e0dfdef20f1fb4ee4bcd9469f0c8c",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -2059,7 +2059,7 @@ def _prepare_cache_for_generation(\n         )\n         if generation_config.cache_implementation is not None:\n             if generation_config.cache_implementation in NEED_SETUP_CACHE_CLASSES_MAPPING:\n-                if generation_config.cache_implementation == \"static\" and not self._supports_static_cache:\n+                if generation_config.cache_implementation == \"static\" and not self._can_compile_fullgraph:\n                     raise ValueError(\n                         \"This model does not support `cache_implementation='static'`. Please check the following \"\n                         \"issue: https://github.com/huggingface/transformers/issues/28981\"\n@@ -2215,7 +2215,8 @@ def _valid_auto_compile_criteria(self, model_kwargs: dict, generation_config: Ge\n         using_compilable_cache = (\n             isinstance(model_kwargs.get(\"past_key_values\"), Cache) and model_kwargs[\"past_key_values\"].is_compileable\n         )\n-        can_compile = valid_hardware and using_compilable_cache and self._supports_static_cache\n+        # TODO @raushan `self._can_compile_fullgraph` can be removed and inferred from model arch (e.g. MoE doesn't support compile)\n+        can_compile = valid_hardware and using_compilable_cache and self._can_compile_fullgraph\n \n         # Exception 1: Some quantization methods do not support compilation\n         if getattr(self, \"hf_quantizer\", None) is not None:"
        },
        {
            "sha": "dc4e997fef20d54a6fc43ded22df7c80bcf6a3ca",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -2063,8 +2063,7 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n     # Flex Attention support\n     _supports_flex_attn = False\n \n-    # Has support `torch.compile(fullgraph=True)`\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n \n     # A tensor parallel plan to be applied to the model when TP is enabled. For\n     # top-level models, this attribute is currently defined in respective model"
        },
        {
            "sha": "e288f63d71b5f0a312e7373959b3ffaf3d72378e",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -313,7 +313,7 @@ class ArceePreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": ArceeDecoderLayer,"
        },
        {
            "sha": "9144cc6bdd3b4f66eba17615cf3f9d7af1ad1dd3",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -654,7 +654,7 @@ class AriaPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_static_cache = False  # MoE models don't work with torch.compile (dynamic slicing)\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (dynamic slicing)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": AriaTextDecoderLayer,"
        },
        {
            "sha": "d980898460736ed00e3073758e9fe214e95e796e",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -1302,7 +1302,7 @@ def _init_weights(self, module):\n class AriaPreTrainedModel(LlamaPreTrainedModel):\n     config: AriaConfig\n     base_model_prefix = \"\"\n-    _supports_static_cache = False  # MoE models don't work with torch.compile (dynamic slicing)\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (dynamic slicing)\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "df45633cc70c75a3ba4db28d6d9bd39431cadf4c",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -96,7 +96,7 @@ class AyaVisionPreTrainedModel(PreTrainedModel):\n \n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "8e7776291764753eca3eb0d080f777fa3362f23d",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -90,7 +90,7 @@ def pixel_shuffle(self, image_features):  # B, S, D\n \n \n class AyaVisionPreTrainedModel(LlavaPreTrainedModel):\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n \n \n class AyaVisionCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):"
        },
        {
            "sha": "236a2f6471d91f8507a543f0a599985137c92321",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -493,7 +493,7 @@ class BartPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "4eeecb5577fe743a6408bdbd08e6e712d2afc6f5",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -1565,7 +1565,7 @@ class BigBirdPegasusPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_param_buffer_assignment = False\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "d6ed401cd6930c2a482483d5793c06d280aab616",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -347,7 +347,7 @@ class BioGptPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n     def _update_causal_mask("
        },
        {
            "sha": "db5ad5dbbc51b8e90d97bf583843e10c9c7e52a8",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -172,7 +172,7 @@ class BioGptPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n     def _update_causal_mask("
        },
        {
            "sha": "7bbfab8cdd39b87f2b7b829fed6b31376642406e",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -312,7 +312,7 @@ class BitNetPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": BitNetDecoderLayer,"
        },
        {
            "sha": "b4ec543b3e0dc1d82c975605333887a3f6c7067f",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -458,7 +458,7 @@ class BlenderbotPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "b248b2f0da1a0560f8a5542cb019a427179d9e04",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -451,7 +451,7 @@ class BlenderbotSmallPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "b19ae2f8dc44fbaebd716554f44e1b5fd856457b",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -1831,7 +1831,7 @@ class Blip2ForConditionalGeneration(Blip2PreTrainedModel, GenerationMixin):\n     config: Blip2Config\n     main_input_name = \"pixel_values\"\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n     _supports_flash_attn = False  # because self.qformer does not support FA2\n "
        },
        {
            "sha": "cc8cd4eae90b58848873b4bd001fd0486a3f54e9",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -434,7 +434,7 @@ class BloomPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"BloomBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)"
        },
        {
            "sha": "b70387d595558e7de3a39447b3d4026583925c36",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -815,7 +815,7 @@ class ChameleonPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_param_buffer_assignment = False\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "3dbb6f5ecce3184706871df510ba4d09b21c6377",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -287,7 +287,7 @@ class CodeGenPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"CodeGenBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)"
        },
        {
            "sha": "fc4314386b87761bd64da220b244617bb52de1cb",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -345,7 +345,7 @@ class CoherePreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": CohereDecoderLayer,"
        },
        {
            "sha": "88c3afe60764d5c0cd5bb82565d371586e41b8d1",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -322,7 +322,7 @@ class Cohere2PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Cohere2DecoderLayer,"
        },
        {
            "sha": "f36b81f886e555b1cfa9fbab656f4ba369ff7e6f",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -371,7 +371,7 @@ class CsmPreTrainedModel(PreTrainedModel):\n     # does not because of Mimi codec model\n     # _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": CsmDecoderLayer,"
        },
        {
            "sha": "ad11589283b8a9fc7d9f80900d66ae3d9375dc25",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -134,7 +134,7 @@ class CsmPreTrainedModel(PreTrainedModel):\n     # does not because of Mimi codec model\n     # _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": CsmDecoderLayer,"
        },
        {
            "sha": "ee5ec65f86bc9b560fac2860df9f4e0a5abdd51c",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -810,7 +810,7 @@ class DbrxPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module: nn.Module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "6a492e937a51b67b6ae4946df59ac3d1bd345bca",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -453,7 +453,7 @@ class DecisionTransformerGPT2PreTrainedModel(PreTrainedModel):\n     is_parallelizable = True\n     supports_gradient_checkpointing = True\n \n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n \n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)"
        },
        {
            "sha": "bffef42464142cf16b8dbb483cf6c7e4994b789f",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -456,7 +456,7 @@ class DeepseekV2PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": DeepseekV2DecoderLayer,"
        },
        {
            "sha": "75cf1077567b2c4be08fcdd0dbc9233333c73392",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -498,7 +498,7 @@ class DeepseekV3PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": DeepseekV3DecoderLayer,"
        },
        {
            "sha": "13e375e15e1d8e0d9d4142e845a14f248c1931ff",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -67,7 +67,7 @@ class DiaPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     main_input_name = \"input_ids\"\n     _no_split_modules = [\"DiaEncoderLayer\", \"DiaDecoderLayer\"]\n "
        },
        {
            "sha": "1bf4cbd5ef394596539febeff34a7b4a5c337dde",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -62,7 +62,7 @@ class DiaPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     main_input_name = \"input_ids\"\n     _no_split_modules = [\"DiaEncoderLayer\", \"DiaDecoderLayer\"]\n "
        },
        {
            "sha": "5deec876f612bad4d27a84d5e451debf00bd0276",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -533,7 +533,7 @@ class DiffLlamaPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = False\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = False\n     _can_record_outputs = {\n         \"hidden_states\": DiffLlamaDecoderLayer,"
        },
        {
            "sha": "60e640c5f162398ab437ed23d53f12cc89323c58",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -486,7 +486,7 @@ class DogePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = False\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(DogeCDMoE, index=1),"
        },
        {
            "sha": "f9b8154ab1891101e3b43d2f6b198c6f1354af36",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -564,7 +564,7 @@ def forward(\n \n class DogePreTrainedModel(LlamaPreTrainedModel):\n     _supports_flash_attn = False\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(DogeCDMoE, index=1),\n         \"hidden_states\": DogeDecoderLayer,"
        },
        {
            "sha": "26fdc9f76ce46f1492a3cc46b50b25fbba824c3e",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -418,7 +418,7 @@ class Dots1PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Dots1DecoderLayer,"
        },
        {
            "sha": "182afe6b90f767c934786ed80a948b130e11a713",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -1098,7 +1098,7 @@ class Emu3PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_param_buffer_assignment = False\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n@@ -1307,7 +1307,6 @@ def forward(\n \n class Emu3Model(Emu3PreTrainedModel):\n     _checkpoint_conversion_mapping = {\"text_model.model\": \"text_model\"}\n-    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compileable\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -1450,7 +1449,6 @@ class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n         \"^vqmodel\": \"model.vqmodel\",\n         \"^text_model.lm_head\": \"lm_head\",\n     }\n-    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compileable\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "7bd59d1fae946d3a348afde5744bbc31a11722ab",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -889,7 +889,6 @@ def forward(**super_kwargs):\n \n class Emu3Model(Emu3PreTrainedModel):\n     _checkpoint_conversion_mapping = {\"text_model.model\": \"text_model\"}\n-    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compileable\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -1032,7 +1031,6 @@ class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n         \"^vqmodel\": \"model.vqmodel\",\n         \"^text_model.lm_head\": \"lm_head\",\n     }\n-    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compileable\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "de575baf7970b7d89e7cdec45aa1837cad14ee89",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -311,7 +311,7 @@ class Ernie4_5PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Ernie4_5DecoderLayer,"
        },
        {
            "sha": "74671bb33fead21b25ecff86222b43f06b5d2509",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -473,7 +473,7 @@ class Ernie4_5_MoEPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(Ernie4_5_MoESparseMoeBlock, index=1),"
        },
        {
            "sha": "5cd2bd505865e0eda393ce443e9ac9b72920adab",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -643,7 +643,7 @@ class FalconPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)"
        },
        {
            "sha": "4b1fefd2cd93cbba90ceb714a06d0f2413c4713d",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -310,7 +310,7 @@ class GemmaPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": GemmaDecoderLayer,"
        },
        {
            "sha": "2ce51042ed90d305e3fcac3a087cea8c3a56531d",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -343,7 +343,7 @@ class Gemma2PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Gemma2DecoderLayer,"
        },
        {
            "sha": "307b67184480db96c1b6b8dfe1d45937cb0b5396",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -434,7 +434,7 @@ class Gemma3PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Gemma3DecoderLayer,"
        },
        {
            "sha": "3c304bbcf66daf475e0785a1f9de91ef916e34f5",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -1490,7 +1490,7 @@ class Gemma3nPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Gemma3nTextDecoderLayer,"
        },
        {
            "sha": "6dd31884e20ce2902a62cfe50aa41225be998bba",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -327,7 +327,7 @@ class GlmPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": GlmDecoderLayer,"
        },
        {
            "sha": "55b3ecfd7abfc613bfb93289049d93d0435e0fed",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -331,7 +331,7 @@ class Glm4PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Glm4DecoderLayer,"
        },
        {
            "sha": "19f0a6b2e5b7f3302d164ac61d15d0b486d8c01d",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -403,7 +403,7 @@ class Glm4MoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Glm4MoeDecoderLayer,"
        },
        {
            "sha": "1f6628d938daf3fe8f709ece97c3e1bcdf66564a",
            "filename": "src/transformers/models/glm4_moe/modular_glm4_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -310,7 +310,7 @@ class Glm4MoeDecoderLayer(DeepseekV3DecoderLayer):\n \n \n class Glm4MoePreTrainedModel(DeepseekV3PreTrainedModel):\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n \n \n class Glm4MoeModel(DeepseekV3Model):"
        },
        {
            "sha": "25ccd9f10ff6d12038ca9f69db9b5ea8257cfc8f",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -407,7 +407,7 @@ class Glm4vPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n \n "
        },
        {
            "sha": "464d54f819814567a671781698c04b8a8a37ee4f",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -283,7 +283,7 @@ class GotOcr2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = False\n     _supports_sdpa = False\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_flex_attn = False\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "80442af9110d31b92f9928593236e04a9384c3f7",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -563,7 +563,7 @@ class GPT2PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_attention_backend = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)"
        },
        {
            "sha": "89e6f7182a759500ed18666f5d1a32e973780f1e",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -477,7 +477,7 @@ class GPTNeoPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GPTNeoBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n-    _supports_static_cache = False  # TODO: needs a HybridCache\n+    _can_compile_fullgraph = False  # TODO: needs a HybridCache\n \n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)"
        },
        {
            "sha": "216ba439ca15117708ea8ba9a667575769e19c1a",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -364,7 +364,7 @@ class GPTNeoXPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": GPTNeoXDecoderLayer,"
        },
        {
            "sha": "e80f7880239f8da51993d549fc520a145fed82b1",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -48,7 +48,7 @@ class GPTNeoXJapanesePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GPTNeoXJapaneseLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "0622bf5ed0a5d3aeca0559b5f6dce79d4cb4e589",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -472,7 +472,7 @@ class GPTJPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GPTJBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_param_buffer_assignment = False\n \n     def __init__(self, *inputs, **kwargs):"
        },
        {
            "sha": "5d76b63b81a9eac4ce160e5810efd26fff4558a0",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -309,7 +309,7 @@ class GranitePreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": GraniteDecoderLayer,"
        },
        {
            "sha": "5ea293e5bfffcf96c547015d47e8be14f823c686",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -592,7 +592,7 @@ class GraniteMoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module):\n         super()._init_weights(module)"
        },
        {
            "sha": "ab31709f3d5f83b11f8ff847ea14610e2fb4e7f8",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -1212,7 +1212,7 @@ class GraniteMoeHybridPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _is_stateful = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "b10369e767f392d77f2b71c706c397755a54cf32",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -510,7 +510,7 @@ class GraniteMoeSharedPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module):\n         super()._init_weights(module)"
        },
        {
            "sha": "3aac2621e8456ebb9e76ac7661d3f3530ab78636",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -312,7 +312,7 @@ class HeliumPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": HeliumDecoderLayer,"
        },
        {
            "sha": "ac8b7776c564a2f21e08295a55046ff841cb9365",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -880,7 +880,7 @@ class IdeficsPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n \n     _supports_flash_attn = True\n-    _supports_static_cache = False  # IDEFICS cannot compile due to dynamic control flow when checking inputs\n+    _can_compile_fullgraph = False  # IDEFICS cannot compile due to dynamic control flow when checking inputs\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "bcafeeec1e730a1dbe033cb34d5d1c05d81727d0",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -340,7 +340,7 @@ class InstructBlipPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     _no_split_modules = [\n         \"InstructBlipQFormerEmbeddings\",\n@@ -1354,7 +1354,7 @@ class InstructBlipForConditionalGeneration(InstructBlipPreTrainedModel, Generati\n     config: InstructBlipConfig\n     main_input_name = \"pixel_values\"\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n \n     def __init__(self, config: InstructBlipConfig):"
        },
        {
            "sha": "8e098183e274d614f14f9d2b3fd27f0450e69f3d",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -827,7 +827,7 @@ class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     _no_split_modules = [\n         \"InstructBlipVideoQFormerEmbeddings\",\n@@ -1360,7 +1360,7 @@ class InstructBlipVideoForConditionalGeneration(InstructBlipVideoPreTrainedModel\n     config: InstructBlipVideoConfig\n     main_input_name = \"pixel_values\"\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n \n     def __init__(self, config: InstructBlipVideoConfig):"
        },
        {
            "sha": "8e1c6167003be87e751b9bb9692e27be5d8d64ed",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -512,7 +512,7 @@ class InternVLPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "b93e1a8b67e57b242221382d50b30959d52aff22",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -63,7 +63,7 @@ class JanusPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_param_buffer_assignment = False\n \n \n@@ -1105,7 +1105,7 @@ def forward(\n \n class JanusForConditionalGeneration(JanusPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def __init__(self, config: JanusConfig):\n         super().__init__(config)"
        },
        {
            "sha": "29accd88e51eb5308359aa6c30d764165a8d528e",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -391,7 +391,7 @@ class JanusPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_param_buffer_assignment = False\n \n \n@@ -965,7 +965,7 @@ def forward(\n \n class JanusForConditionalGeneration(JanusPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def __init__(self, config: JanusConfig):\n         super().__init__(config)"
        },
        {
            "sha": "5a60fed7eb279b6936c53c127ffdf85a3a2a0b2c",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -578,7 +578,7 @@ class Lfm2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Lfm2DecoderLayer,"
        },
        {
            "sha": "c3c39e46776ffef68a9ac6602b6a2bb727fda117",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -437,7 +437,7 @@ def forward(\n \n \n class Lfm2PreTrainedModel(LlamaPreTrainedModel):\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n \n \n class Lfm2Model(LlamaModel):"
        },
        {
            "sha": "11bcb93f8666af74bcab7c304f174c181eb4634b",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -317,7 +317,7 @@ class LlamaPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": LlamaDecoderLayer,"
        },
        {
            "sha": "53d9367b7c189790b197422687029fe4d368468b",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -437,7 +437,7 @@ class Llama4PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "92199c9505fda0dcf7e9eb8183dbe79ce2a70b69",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -121,7 +121,7 @@ class LlavaPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "94f03925b8b03fd59ee66cd6ea30adfb83235772",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -232,7 +232,7 @@ class LlavaNextPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "dce37a4dd9851e1b98ac4a4acd2bf1d6423b5674",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -173,7 +173,7 @@ class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "41c39d26f38b76bf588a5b00615dd98b7803eacc",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -286,7 +286,7 @@ class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "87badf1ad9b9fe0f567d6f8d8f5b34d5cefa2df6",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -1250,7 +1250,7 @@ class LongT5PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LongT5Block\"]\n \n-    _supports_static_cache = False  # TODO: @raushan more involved due to local/global attn\n+    _can_compile_fullgraph = False  # TODO: @raushan more involved due to local/global attn\n \n     @property\n     # Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel.dummy_inputs"
        },
        {
            "sha": "ccd7c000ace40bcf0d776e6fbbd2c236d18242a7",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -525,7 +525,7 @@ class M2M100PreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n \n     # Doesn't support `compile` (dynamic control flow). Can be fixed but low usage model\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "80ad44407514b269c445462eff080a740b42e727",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -467,7 +467,7 @@ class MarianPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Embedding, MarianSinusoidalPositionalEmbedding]):\n         std = self.config.init_std"
        },
        {
            "sha": "2f6b5c20efb46cfdbd090b2f53671afc7489379d",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -492,7 +492,7 @@ class MBartPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "260ea6f7ce2ef79d414c5e26ff6b9cb518768faf",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -1376,7 +1376,7 @@ class MimiPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "1469b6d242f4296149a31d7871ae6072fe4e127e",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -581,8 +581,7 @@ class MiniMaxPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    # Note: only supports MiniMaxCache\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(MiniMaxSparseMoeBlock, index=1),"
        },
        {
            "sha": "99be8f7fb5e64b8b53e257df1345bbb051029216",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -468,8 +468,7 @@ def forward(\n \n \n class MiniMaxPreTrainedModel(MixtralPreTrainedModel):\n-    # Note: only supports MiniMaxCache\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(MiniMaxSparseMoeBlock, index=1),\n         \"hidden_states\": MiniMaxDecoderLayer,"
        },
        {
            "sha": "caf3681f14546667ee4c82ee8bf10f41354d91f6",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -256,7 +256,7 @@ class MistralPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": MistralDecoderLayer,"
        },
        {
            "sha": "bc61bc55b1cd9f604972cae3a329483e794a11a6",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -186,7 +186,7 @@ class Mistral3PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "043862a3a2c0ab2047d3def0f8456f65653f7bfa",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -383,7 +383,7 @@ class MixtralPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(MixtralSparseMoeBlock, index=1),"
        },
        {
            "sha": "c4a7b5b2df6cfb1906018cd1637ce0daa92dba45",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -277,7 +277,7 @@ class MixtralRotaryEmbedding(MistralRotaryEmbedding):\n \n \n class MixtralPreTrainedModel(MistralPreTrainedModel):\n-    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(MixtralSparseMoeBlock, index=1),\n         \"hidden_states\": MixtralDecoderLayer,"
        },
        {
            "sha": "266a916cef63d9d96adfd5dba87c3d7635aaa29f",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -850,7 +850,7 @@ class MllamaPreTrainedModel(PreTrainedModel):\n         \"MllamaSelfAttentionDecoderLayer\",\n     ]\n \n-    _supports_static_cache = False  # static cache cannot have different shapes for each layer\n+    _can_compile_fullgraph = False  # static cache cannot have different shapes for each layer\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n@@ -1449,7 +1449,7 @@ def forward(\n )\n class MllamaForCausalLM(MllamaPreTrainedModel, GenerationMixin):\n     config: MllamaTextConfig\n-    _supports_static_cache = True  # only the LLM without cross attn can do compile\n+    _can_compile_fullgraph = True  # only the LLM without cross attn can do compile\n     base_model_prefix = \"language_model\"\n     _tied_weights_keys = [\"lm_head.weight\"]\n "
        },
        {
            "sha": "0b1d572a1f79696d703c5b08e5a2964501a1befb",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -224,7 +224,7 @@ class ModernBertDecoderPreTrainedModel(ModernBertPreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = False\n     _supports_gradient_checkpointing = True\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": ModernBertDecoderLayer,"
        },
        {
            "sha": "3b6b936e15e7a844505cfe153d840582283175ef",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -401,7 +401,7 @@ class ModernBertDecoderPreTrainedModel(ModernBertPreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = False\n     _supports_gradient_checkpointing = True\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": ModernBertDecoderLayer,"
        },
        {
            "sha": "15598ccb3293872052807adb6a025c15b79a5c9b",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -462,7 +462,7 @@ class MoonshinePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     # TODO arthur, how do we separate when it cross / self coming from different layer?\n \n     def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):"
        },
        {
            "sha": "326cd743cead36a5d2be432d0fbda26407fe3722",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -497,7 +497,7 @@ class MoonshinePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     # TODO arthur, how do we separate when it cross / self coming from different layer?\n \n     def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):"
        },
        {
            "sha": "071010abf1fc2ce3e286647dbc1bf3006378cd56",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -757,7 +757,7 @@ class MT5PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     is_parallelizable = True\n     supports_gradient_checkpointing = True\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     _no_split_modules = [\"MT5Block\"]\n     _keep_in_fp32_modules = [\"wo\"]"
        },
        {
            "sha": "714b71f6bb0481e097ad3e1a990da98e191eea17",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -589,7 +589,7 @@ class NemotronPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "987611bca25b4262bd298d68267dc256545bfbf5",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -294,7 +294,7 @@ class OlmoPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": OlmoDecoderLayer,"
        },
        {
            "sha": "d113a26462ab2d5326ca9a4d6bc80c1813e9bc79",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -299,7 +299,7 @@ class Olmo2PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Olmo2DecoderLayer,"
        },
        {
            "sha": "38b538fdf3013b63d91c690faf6b0052e59283fa",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -706,7 +706,7 @@ class OlmoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "ff5e8dfa010deea96be6e6624f2831a315e23795",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -313,7 +313,7 @@ class OPTPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "2d82dccc18667862cc8e2921ee615978f9c30807",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -114,7 +114,7 @@ class PaliGemmaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"PaliGemmaMultiModalProjector\"]\n     _skip_keys_device_placement = \"past_key_values\"\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "33fc066d89b3db0715d5b4ce0ad6e6d745361cc9",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -458,7 +458,7 @@ class PegasusPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "029ad0a2e42b253fbbc4b58a93eef719676bb274",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -758,7 +758,7 @@ class PegasusXPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = False\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "65f35c7951b01fe143ebe30b0128db811e97892c",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -95,7 +95,7 @@ class PerceptionLMPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "2779b2a504950ef801d287ff1807600d6a1a96ba",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -390,7 +390,7 @@ class PersimmonPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"PersimmonDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "4cf3d54a65526e312b4657f15a3fd97f6dfd0ccb",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -298,7 +298,7 @@ class PhiPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": PhiDecoderLayer,"
        },
        {
            "sha": "399e207a428f3f597d910bfe20560abe615596ab",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -287,7 +287,7 @@ class Phi3PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Phi3DecoderLayer,"
        },
        {
            "sha": "c0376fc0e27cd8938d2dcb4f228e3a8e720ca9ed",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -1582,7 +1582,7 @@ class Phi4MultimodalPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Phi4MultimodalDecoderLayer,"
        },
        {
            "sha": "3b1369a92449e8f817afe3daa06b6b6122b68f34",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -887,7 +887,7 @@ class PhimoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "8ef1ea8090e9f9aed3c4c1f1c2cbb0cbe13bab8a",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -351,7 +351,7 @@ def forward(\n class Pix2StructPreTrainedModel(PreTrainedModel):\n     config: Pix2StructConfig\n \n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n \n     @property\n     def dummy_inputs(self):"
        },
        {
            "sha": "71a1397cc3b478fbfcb9ee7c78ffc33225e6d319",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -577,7 +577,7 @@ class Pop2PianoPreTrainedModel(PreTrainedModel):\n     is_parallelizable = False\n     supports_gradient_checkpointing = True\n \n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n     _no_split_modules = [\"Pop2PianoBlock\"]\n     _keep_in_fp32_modules = [\"wo\"]\n "
        },
        {
            "sha": "5aaaae52fc8f481781d9ceb80580aca736158c45",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -259,7 +259,7 @@ class Qwen2PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Qwen2DecoderLayer,"
        },
        {
            "sha": "f690ca5108a28c8a984b0d68b5452e0db681a45b",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -67,7 +67,7 @@ class Qwen2_5OmniPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n     _supports_attention_backend = True\n \n "
        },
        {
            "sha": "b8e5bc6216ec79beba0596aac79f8306b56aff0d",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -1133,7 +1133,7 @@ def get_text_config(self, decoder=False):\n \n class Qwen2_5OmniPreTrainedModel(Qwen2_5_VLPreTrainedModel):\n     config: Qwen2_5OmniConfig\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n \n \n class Qwen2_5OmniPreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModel):"
        },
        {
            "sha": "c270e2714cdc5ace6db22aa8c6f2cb910aacd35f",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -326,7 +326,7 @@ class Qwen2_5_VLPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n \n "
        },
        {
            "sha": "068199e6d9b2f93ac84f42047e939cece5af4ebc",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -660,7 +660,7 @@ class Qwen2VLPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n \n "
        },
        {
            "sha": "695df11f37e8ef9a735eb6500e90eb343d56c10e",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -285,7 +285,7 @@ class Qwen3PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Qwen3DecoderLayer,"
        },
        {
            "sha": "e7080f9d1bb9247ca9e8b88ee38e56c0943da131",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -406,7 +406,7 @@ class Qwen3MoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(Qwen3MoeSparseMoeBlock, index=1),"
        },
        {
            "sha": "082cc792e058c0afb3c63dd298a82bf79645a723",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -289,7 +289,7 @@ class SmolLM3PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": SmolLM3DecoderLayer,"
        },
        {
            "sha": "b70eaf1a0951e2a7e895a5184f4969e2b459f0c6",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -621,7 +621,7 @@ class StableLmPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "cce7b5cebb24029973bba35ca8798d231c70d9bd",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -293,7 +293,7 @@ class Starcoder2PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Starcoder2DecoderLayer,"
        },
        {
            "sha": "d6241ff134b97c5be66e7a388f4c5875e75d7bb2",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -766,7 +766,7 @@ class SwitchTransformersPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"switch_transformers\"\n     supports_gradient_checkpointing = True\n \n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n     _no_split_modules = [\"SwitchTransformersBlock\"]\n \n     @property"
        },
        {
            "sha": "e39c4b2f998cb40def069956bbbc960485f7e4c0",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -771,7 +771,7 @@ class T5PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     is_parallelizable = True\n     supports_gradient_checkpointing = True\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     _no_split_modules = [\"T5Block\"]\n     _keep_in_fp32_modules = [\"wo\"]"
        },
        {
            "sha": "e2b563cbaca9212c4ba400587508d32bfc220ef4",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -585,7 +585,7 @@ class T5GemmaPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": T5GemmaDecoderLayer,"
        },
        {
            "sha": "a84d15051b1dd8f02f66a24ee2be53e90b3538e5",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -255,7 +255,7 @@ class UdopPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     supports_gradient_checkpointing = True\n \n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n     _keep_in_fp32_modules = [\"wo\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "e171be26fe12417d6bbe62f3f9def910e5db3f6f",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -508,7 +508,7 @@ class UMT5PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     supports_gradient_checkpointing = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _no_split_modules = [\"UMT5Block\"]\n     _keep_in_fp32_modules = [\"wo\"]\n "
        },
        {
            "sha": "befa350b907c05fdfae268040848c9b07dd7269c",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -135,7 +135,7 @@ class VideoLlavaPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "df3635b690bcb9c4f99f842f4b2810af8a6f6b37",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -122,7 +122,7 @@ class VipLlavaPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "ae949a457969474ff3aeb18635fc7dc55b306171",
            "filename": "src/transformers/models/voxtral/modeling_voxtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -236,7 +236,7 @@ class VoxtralPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_attention_backend = True\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         # important: this ported version of Voxtral isn't meant for training from scratch - only"
        },
        {
            "sha": "a3cb8c3ed00dba1dbe9875aadbbefb71f8e3a2bc",
            "filename": "src/transformers/models/voxtral/modular_voxtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -47,7 +47,7 @@ class VoxtralPreTrainedModel(Qwen2AudioPreTrainedModel):\n     _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_attention_backend = True\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n     _supports_attention_backend = True\n \n "
        },
        {
            "sha": "ad8ac7cee348f58dd25948ba63e5e23bb1798795",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -553,7 +553,7 @@ class WhisperPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "c1e2f8e1ebc548a08e2693a9dbcf876b139b690d",
            "filename": "src/transformers/utils/auto_docstring.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fauto_docstring.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -965,8 +965,9 @@ class ClassAttrs:\n     _supports_flex_attn = r\"\"\"\n     Whether the model's attention implementation supports FlexAttention.\n     \"\"\"\n-    _supports_static_cache = r\"\"\"\n-    Whether the model supports a `StaticCache` instance as `past_key_values`.\n+    _can_compile_fullgraph = r\"\"\"\n+    Whether the model can `torch.compile` fullgraph without graph breaks. Models will auto-compile if this flag is set to `True`\n+    in inference, if a compilable cache is used.\n     \"\"\"\n     _supports_attention_backend = r\"\"\"\n     Whether the model supports attention interface functions. This flag signal that the model can be used as an efficient backend in TGI and vLLM."
        },
        {
            "sha": "5f5f33de3502ccdd49dda9994240946ee5fd4d6a",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -1764,7 +1764,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n         to verify that the cache length is indeed set correctly and we don't run out of index when slicing the cache.\n         \"\"\"\n         for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_static_cache:\n+            if not model_class._can_compile_fullgraph:\n                 self.skipTest(reason=\"This model does not support the static cache format\")\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n@@ -1984,7 +1984,7 @@ def test_generate_with_static_cache(self):\n         \"\"\"\n         set_model_tester_for_less_flaky_test(self)\n         for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_static_cache:\n+            if not model_class._can_compile_fullgraph:\n                 self.skipTest(reason=\"This model does not support the static cache format\")\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n@@ -2087,8 +2087,8 @@ def test_generate_compile_model_forward(self):\n         set_model_tester_for_less_flaky_test(self)\n         for model_class in self.all_generative_model_classes:\n             # 1. Test exclusion criteria\n-            if not model_class._supports_static_cache:\n-                self.skipTest(\"This model doesn't support static cache (= no expectations of compilation support)\")\n+            if not model_class._can_compile_fullgraph:\n+                self.skipTest(\"This model doesn't support compilation without graph breaks\")\n \n             # 2. Prepares two sets of inputs\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=4)\n@@ -2201,8 +2201,8 @@ def test_generate_compilation_all_outputs(self):\n         In essence, it's the same as `test_greedy_generate_dict_outputs`, but with automatic compilation triggered.\n         \"\"\"\n         for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_static_cache:\n-                self.skipTest(\"This model doesn't support static cache (= no expectations of compilation support)\")\n+            if not model_class._can_compile_fullgraph:\n+                self.skipTest(\"This model doesn't support compilation without graph breaks\")\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n             if self.has_attentions:"
        },
        {
            "sha": "f490108817c6b25a000393c9f9e6ff50fbd8a6a2",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb1a007f7f0bcff45b0b6d43759c583246946f91/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb1a007f7f0bcff45b0b6d43759c583246946f91/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=eb1a007f7f0bcff45b0b6d43759c583246946f91",
            "patch": "@@ -4415,7 +4415,7 @@ def test_custom_4d_attention_mask(self):\n         set_model_tester_for_less_flaky_test(self)\n \n         for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_static_cache:\n+            if not model_class._can_compile_fullgraph:\n                 self.skipTest(f\"{model_class.__name__} is not guaranteed to work with custom 4D attention masks\")\n             config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n             set_config_for_less_flaky_test(config)"
        }
    ],
    "stats": {
        "total": 291,
        "additions": 143,
        "deletions": 148
    }
}