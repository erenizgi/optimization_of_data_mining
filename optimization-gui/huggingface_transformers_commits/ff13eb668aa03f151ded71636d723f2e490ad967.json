{
    "author": "eustlb",
    "message": "Lasr model (#42648)\n\n* nit on dac!\n\n* first commit\n\n* fixes\n\n* passing all API tests\n\n* nit\n\n* update\n\n* test update\n\n* make style\n\n* make\n\n* fixes\n\n* fix\n\n* test update\n\n* return attention mask by default\n\n* doc update\n\n* make\n\n* make\n\n* fix",
    "sha": "ff13eb668aa03f151ded71636d723f2e490ad967",
    "files": [
        {
            "sha": "5d95fc368285baa8f90655e459c164ecc5ec12d3",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -901,6 +901,8 @@\n         title: Hubert\n       - local: model_doc/kyutai_speech_to_text\n         title: Kyutai Speech-To-Text\n+      - local: model_doc/lasr\n+        title: LASR\n       - local: model_doc/mimi\n         title: Mimi\n       - local: model_doc/mms"
        },
        {
            "sha": "35292826c10c307cf7547fc5695d8d1acee7375c",
            "filename": "docs/source/en/model_doc/lasr.md",
            "status": "added",
            "additions": 108,
            "deletions": 0,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/docs%2Fsource%2Fen%2Fmodel_doc%2Flasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/docs%2Fsource%2Fen%2Fmodel_doc%2Flasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flasr.md?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -0,0 +1,108 @@\n+<!--Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+# LASR\n+\n+## Overview\n+\n+TODO\n+\n+## Usage\n+\n+### Basic usage\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+from transformers import pipeline\n+\n+pipe = pipeline(\"automatic-speech-recognition\", model=\"path/to/lasr-model\")\n+out = pipe(\"path/to/audio.mp3\")\n+print(out)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+from transformers import AutoModelForCTC, AutoProcessor\n+from datasets import load_dataset, Audio\n+import torch\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+processor = AutoProcessor.from_pretrained(\"path/to/lasr-model\")\n+model = AutoModelForCTC.from_pretrained(\"path/to/lasr-model\", dtype=\"auto\", device_map=device)\n+\n+ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+speech_samples = [el['array'] for el in ds[\"audio\"][:5]]\n+\n+inputs = processor(speech_samples, sampling_rate=processor.feature_extractor.sampling_rate)\n+inputs.to(model.device, dtype=model.dtype)\n+outputs = model.generate(**inputs)\n+print(processor.batch_decode(outputs))\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+### Making The Model Go Brrr\n+\n+TODO\n+\n+### Training\n+\n+TODO\n+\n+## LasrTokenizer\n+\n+[[autodoc]] LasrTokenizer\n+\n+## LasrFeatureExtractor\n+\n+[[autodoc]] LasrFeatureExtractor\n+    - __call__\n+\n+## LasrProcessor\n+\n+[[autodoc]] LasrProcessor\n+    - __call__\n+    - batch_decode\n+    - decode\n+\n+## LasrEncoderConfig\n+\n+[[autodoc]] LasrEncoderConfig\n+\n+## LasrCTCConfig\n+\n+[[autodoc]] LasrCTCConfig\n+\n+## LasrEncoder\n+\n+[[autodoc]] LasrEncoder\n+\n+## LasrForCTC\n+\n+[[autodoc]] LasrForCTC\n+"
        },
        {
            "sha": "71b2155e9bc50caad45e073d3b41260e7ae2da7b",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -186,6 +186,7 @@\n     from .jetmoe import *\n     from .kosmos2 import *\n     from .kyutai_speech_to_text import *\n+    from .lasr import *\n     from .layoutlm import *\n     from .layoutlmv2 import *\n     from .layoutlmv3 import *"
        },
        {
            "sha": "38a0abb9e2d7f3ea38e1dedbee6f93d7060a4271",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -221,6 +221,8 @@\n         (\"kosmos-2\", \"Kosmos2Config\"),\n         (\"kosmos-2.5\", \"Kosmos2_5Config\"),\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToTextConfig\"),\n+        (\"lasr_ctc\", \"LasrCTCConfig\"),\n+        (\"lasr_encoder\", \"LasrEncoderConfig\"),\n         (\"layoutlm\", \"LayoutLMConfig\"),\n         (\"layoutlmv2\", \"LayoutLMv2Config\"),\n         (\"layoutlmv3\", \"LayoutLMv3Config\"),\n@@ -662,6 +664,9 @@\n         (\"kosmos-2\", \"KOSMOS-2\"),\n         (\"kosmos-2.5\", \"KOSMOS-2.5\"),\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToText\"),\n+        (\"lasr\", \"Lasr\"),\n+        (\"lasr_ctc\", \"Lasr\"),\n+        (\"lasr_encoder\", \"LasrEncoder\"),\n         (\"layoutlm\", \"LayoutLM\"),\n         (\"layoutlmv2\", \"LayoutLMv2\"),\n         (\"layoutlmv3\", \"LayoutLMv3\"),\n@@ -977,6 +982,8 @@\n         (\"video_llama_3_vision\", \"video_llama_3\"),\n         (\"parakeet_encoder\", \"parakeet\"),\n         (\"parakeet_ctc\", \"parakeet\"),\n+        (\"lasr_encoder\", \"lasr\"),\n+        (\"lasr_ctc\", \"lasr\"),\n         (\"wav2vec2-bert\", \"wav2vec2_bert\"),\n     ]\n )"
        },
        {
            "sha": "a9008af06ab616df8c689aad439bcc177a42e0d1",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -49,6 +49,8 @@\n         (\"granite_speech\", \"GraniteSpeechFeatureExtractor\"),\n         (\"hubert\", \"Wav2Vec2FeatureExtractor\"),\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToTextFeatureExtractor\"),\n+        (\"lasr_ctc\", \"LasrFeatureExtractor\"),\n+        (\"lasr_encoder\", \"LasrFeatureExtractor\"),\n         (\"markuplm\", \"MarkupLMFeatureExtractor\"),\n         (\"mimi\", \"EncodecFeatureExtractor\"),\n         (\"moonshine\", \"Wav2Vec2FeatureExtractor\"),"
        },
        {
            "sha": "ddd29ad96d5b35ba3045e6a38b9abfa5ea7962d4",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -222,6 +222,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"kosmos-2\", \"Kosmos2Model\"),\n         (\"kosmos-2.5\", \"Kosmos2_5Model\"),\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToTextModel\"),\n+        (\"lasr_ctc\", \"LasrForCTC\"),\n+        (\"lasr_encoder\", \"LasrEncoder\"),\n         (\"layoutlm\", \"LayoutLMModel\"),\n         (\"layoutlmv2\", \"LayoutLMv2Model\"),\n         (\"layoutlmv3\", \"LayoutLMv3Model\"),\n@@ -1583,6 +1585,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         # Model for Connectionist temporal classification (CTC) mapping\n         (\"data2vec-audio\", \"Data2VecAudioForCTC\"),\n         (\"hubert\", \"HubertForCTC\"),\n+        (\"lasr_ctc\", \"LasrForCTC\"),\n         (\"parakeet_ctc\", \"ParakeetForCTC\"),\n         (\"sew\", \"SEWForCTC\"),\n         (\"sew-d\", \"SEWDForCTC\"),"
        },
        {
            "sha": "c52630c13a185bbf2e138b042eefc787dcbbaa33",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -264,7 +264,7 @@ def forward(self, hidden_state):\n         return hidden_state\n \n \n-class DacResidualVectorQuantize(nn.Module):\n+class DacResidualVectorQuantizer(nn.Module):\n     \"\"\"\n     ResidualVectorQuantize block - Introduced in SoundStream: An end2end neural audio codec (https://huggingface.co/papers/2107.03312)\n     \"\"\"\n@@ -568,7 +568,7 @@ def __init__(self, config: DacConfig):\n         self.encoder = DacEncoder(config)\n         self.decoder = DacDecoder(config)\n \n-        self.quantizer = DacResidualVectorQuantize(config)\n+        self.quantizer = DacResidualVectorQuantizer(config)\n \n         self.bits_per_codebook = int(math.log2(self.config.codebook_size))\n         if 2**self.bits_per_codebook != self.config.codebook_size:"
        },
        {
            "sha": "643eb76ff4552e92abf4672894508d09852da16f",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -514,7 +514,7 @@ def forward(self, hidden_states, attention_mask=None):\n \n         Args:\n             hidden_states (`torch.Tensor` of shape `(batch, time, channels)`): Input tensor.\n-            attention_mask (`torch.Tensor` of shape `(batch, 1, time)`): Attention mask.\n+            attention_mask (`torch.Tensor` of shape `(batch, 1, time, time)`): Attention mask.\n \n         Returns:\n             `torch.Tensor`: Output tensor of shape `(batch, time, channels)`.\n@@ -530,7 +530,10 @@ def forward(self, hidden_states, attention_mask=None):\n \n         # Apply padding mask before convolution\n         if attention_mask is not None:\n-            all_masked_rows = torch.all(~attention_mask, dim=-1)\n+            if attention_mask.dtype == torch.bool:\n+                all_masked_rows = torch.all(~attention_mask, dim=2)\n+            else:\n+                all_masked_rows = torch.all(~(attention_mask == 0.0), dim=2)\n             hidden_states = hidden_states.masked_fill(all_masked_rows, 0.0)\n \n         # 1D Depthwise Conv"
        },
        {
            "sha": "f4c7c98261a1c11dcb0046bb3964999484ad2c0e",
            "filename": "src/transformers/models/lasr/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2F__init__.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_lasr import *\n+    from .feature_extraction_lasr import *\n+    from .modeling_lasr import *\n+    from .tokenization_lasr import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "28051469be589c3fa3ccec12aac6d68410bbc1d3",
            "filename": "src/transformers/models/lasr/configuration_lasr.py",
            "status": "added",
            "additions": 244,
            "deletions": 0,
            "changes": 244,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2Fconfiguration_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2Fconfiguration_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Fconfiguration_lasr.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -0,0 +1,244 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/lasr/modular_lasr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_lasr.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team and Google LLC. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Union\n+\n+from ...configuration_utils import PreTrainedConfig\n+\n+\n+class LasrEncoderConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`LasrEncoder`]. It is used to instantiate a\n+    `LasrEncoder` model according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+            hidden_size (`int`, *optional*, defaults to 512):\n+                Dimension of the layers and the hidden states.\n+            num_hidden_layers (`int`, *optional*, defaults to 17):\n+                Number of hidden layers in the Transformer encoder.\n+            num_attention_heads (`int`, *optional*, defaults to 8):\n+                Number of attention heads for each attention layer in the Transformer encoder.\n+            intermediate_size (`int`, *optional*, defaults to 2048):\n+                Dimension of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n+            hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+                The non-linear activation function (function or string) in the encoder and pooler.\n+            attention_bias (`bool`, *optional*, defaults to `False`):\n+                Whether to use bias in the attention layers.\n+            convolution_bias (`bool`, *optional*, defaults to `False`):\n+                Whether to use bias in convolutions of the conformer's convolution module.\n+            conv_kernel_size (`int`, *optional*, defaults to 32):\n+                The kernel size of the convolution layers in the Conformer block.\n+            subsampling_conv_channels (`int`, *optional*, defaults to 256):\n+                The number of channels in the subsampling convolution layers.\n+            subsampling_conv_kernel_size (`int`, *optional*, defaults to 5):\n+                The kernel size of the subsampling convolution layers.\n+            subsampling_conv_stride (`int`, *optional*, defaults to 2):\n+                The stride of the subsampling convolution layers.\n+            num_mel_bins (`int`, *optional*, defaults to 128):\n+                Number of mel features.\n+            dropout (`float`, *optional*, defaults to 0.1):\n+                The dropout ratio for all fully connected layers in the embeddings, encoder, and pooler.\n+            dropout_positions (`float`, *optional*, defaults to 0.0):\n+                The dropout ratio for the positions in the input sequence.\n+            layerdrop (`float`, *optional*, defaults to 0.1):\n+                The dropout ratio for the layers in the encoder.\n+            activation_dropout (`float`, *optional*, defaults to 0.1):\n+                The dropout ratio for activations inside the fully connected layer.\n+            attention_dropout (`float`, *optional*, defaults to 0.1):\n+                The dropout ratio for the attention layers.\n+            max_position_embeddings (`int`, *optional*, defaults to 10000):\n+                The maximum sequence length that this model might ever be used with.\n+            initializer_range (`float`, *optional*, defaults to 0.02):\n+                The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+            layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+                The epsilon used by the layer normalization layers.\n+            feed_forward_residual_weights (`tuple[float, float]`, *optional*, defaults to `[1.5, 0.5]`):\n+                The residual weights for the feed forward layers.\n+            conv_residual_weights (`tuple[float, float]`, *optional*, defaults to `[2.0, 1.0]`):\n+                The residual weights for the convolution layers.\n+            batch_norm_momentum (`float`, *optional*, defaults to 0.01):\n+                The momentum for the batch normalization layers.\n+            rope_parameters (`RopeParameters`, *optional*):\n+                Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n+                a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+                with longer `max_position_embeddings`.\n+\n+    Example:\n+        ```python\n+        >>> from transformers import LasrEncoderModel, LasrEncoderConfig\n+\n+        >>> # Initializing a `LasrEncoder` configuration\n+        >>> configuration = LasrEncoderConfig()\n+\n+        >>> # Initializing a model from the configuration\n+        >>> model = LasrEncoderModel(configuration)\n+\n+        >>> # Accessing the model configuration\n+        >>> configuration = model.config\n+        ```\n+\n+    This configuration class is based on the LasrEncoder architecture from Google Health AI. You can find more details\n+    and pre-trained models at [TODO/TODO](https://huggingface.co/TODO/TODO).\n+    \"\"\"\n+\n+    model_type = \"lasr_encoder\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        hidden_size=512,\n+        num_hidden_layers=17,\n+        num_attention_heads=8,\n+        intermediate_size=2048,\n+        hidden_act=\"silu\",\n+        attention_bias=False,\n+        convolution_bias=False,\n+        conv_kernel_size=32,\n+        subsampling_conv_channels=256,\n+        subsampling_conv_kernel_size=5,\n+        subsampling_conv_stride=2,\n+        num_mel_bins=128,\n+        dropout=0.1,\n+        dropout_positions=0.0,\n+        layerdrop=0.1,\n+        activation_dropout=0.1,\n+        attention_dropout=0.1,\n+        max_position_embeddings=10000,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-6,\n+        feed_forward_residual_weights=[1.5, 0.5],\n+        conv_residual_weights=[2.0, 1.0],\n+        batch_norm_momentum=0.01,\n+        rope_parameters=None,\n+        **kwargs,\n+    ):\n+        self.rope_parameters = rope_parameters\n+        self.layer_norm_eps = layer_norm_eps\n+        self.feed_forward_residual_weights = feed_forward_residual_weights\n+        self.conv_residual_weights = conv_residual_weights\n+        self.batch_norm_momentum = batch_norm_momentum\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_attention_heads  # LlamaAttention compatibility\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.attention_bias = attention_bias\n+        self.convolution_bias = convolution_bias\n+\n+        self.conv_kernel_size = conv_kernel_size\n+        self.subsampling_conv_kernel_size = subsampling_conv_kernel_size\n+        self.subsampling_conv_stride = subsampling_conv_stride\n+        self.subsampling_conv_channels = subsampling_conv_channels\n+        self.num_mel_bins = num_mel_bins\n+\n+        self.dropout = dropout\n+        self.dropout_positions = dropout_positions\n+        self.layerdrop = layerdrop\n+        self.activation_dropout = activation_dropout\n+        self.attention_dropout = attention_dropout\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+\n+        super().__init__(\n+            **kwargs,\n+        )\n+\n+\n+class LasrCTCConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`LasrForCTC`]. It is used to instantiate a\n+    Lasr CTC model according to the specified arguments, defining the model architecture.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+    Args:\n+            vocab_size (`int`, *optional*, defaults to 512):\n+                Vocabulary size of the model.\n+            ctc_loss_reduction (`str`, *optional*, defaults to `\"mean\"`):\n+                Specifies the reduction to apply to the output of `torch.nn.CTCLoss`. Only relevant when training an\n+                instance of [`LasrForCTC`].\n+            ctc_zero_infinity (`bool`, *optional*, defaults to `True`):\n+                Whether to zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite losses mainly\n+                occur when the inputs are too short to be aligned to the targets. Only relevant when training an instance\n+                of [`LasrForCTC`].\n+            encoder_config (`Union[dict, LasrEncoderConfig]`, *optional*):\n+                The config object or dictionary of the encoder.\n+            pad_token_id (`int`, *optional*, defaults to 0):\n+                Padding token id. Also used as blank token id.\n+    Example:\n+        ```python\n+        >>> from transformers import LasrForCTC, LasrCTCConfig\n+        >>> # Initializing a Lasr configuration\n+        >>> configuration = LasrCTCConfig()\n+        >>> # Initializing a model from the configuration\n+        >>> model = LasrForCTC(configuration)\n+        >>> # Accessing the model configuration\n+        >>> configuration = model.config\n+        ```\n+    This configuration class is based on the Lasr CTC architecture from Google Health AI. You can find more details\n+    and pre-trained models at [TODO/TODO](https://huggingface.co/TODO/TODO).\n+    \"\"\"\n+\n+    model_type = \"lasr_ctc\"\n+    sub_configs = {\"encoder_config\": LasrEncoderConfig}\n+\n+    def __init__(\n+        self,\n+        vocab_size=512,\n+        ctc_loss_reduction=\"mean\",\n+        ctc_zero_infinity=True,\n+        encoder_config: Union[dict, LasrEncoderConfig] = None,\n+        pad_token_id=0,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.ctc_loss_reduction = ctc_loss_reduction\n+        self.ctc_zero_infinity = ctc_zero_infinity\n+\n+        if isinstance(encoder_config, dict):\n+            self.encoder_config = LasrEncoderConfig(**encoder_config)\n+        elif encoder_config is None:\n+            self.encoder_config = LasrEncoderConfig()\n+\n+        self.encoder_config = self.encoder_config\n+        self.initializer_range = self.encoder_config.initializer_range\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            **kwargs,\n+        )\n+\n+    @classmethod\n+    def from_encoder_config(cls, encoder_config: LasrEncoderConfig, **kwargs):\n+        r\"\"\"\n+        Instantiate a [`LasrCTCConfig`] (or a derived class) from lasr encoder model configuration.\n+\n+        Returns:\n+            [`LasrCTCConfig`]: An instance of a configuration object\n+        \"\"\"\n+\n+        return cls(encoder_config=encoder_config.to_dict(), **kwargs)\n+\n+\n+__all__ = [\"LasrEncoderConfig\", \"LasrCTCConfig\"]"
        },
        {
            "sha": "50a0229838aac0d32952f4931ccb27cfd1caeca2",
            "filename": "src/transformers/models/lasr/feature_extraction_lasr.py",
            "status": "added",
            "additions": 277,
            "deletions": 0,
            "changes": 277,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2Ffeature_extraction_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2Ffeature_extraction_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Ffeature_extraction_lasr.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -0,0 +1,277 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team and Google LLC. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional, Union\n+\n+import numpy as np\n+import torch\n+\n+from ...audio_utils import hertz_to_mel\n+from ...feature_extraction_sequence_utils import SequenceFeatureExtractor\n+from ...feature_extraction_utils import BatchFeature\n+from ...utils import TensorType, logging\n+from ...utils.import_utils import requires\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# TODO: @eustlb, we should be able to remove this and use mel_filter_bank from audio_utils\n+def linear_to_mel_weight_matrix(\n+    num_mel_bins: int,\n+    num_spectrogram_bins: int,\n+    sample_rate: float,\n+    lower_edge_hertz: float,\n+    upper_edge_hertz: float,\n+    dtype,\n+) -> np.ndarray:\n+    \"\"\"NumPy-port of the JAX mel weight matrix logic.\"\"\"\n+    # We use float64 for precision, matching the JAX implementation.\n+    internal_dtype = np.float64\n+\n+    # HTK excludes the spectrogram DC bin.\n+    bands_to_zero = 1\n+    nyquist_hertz = sample_rate / 2.0\n+    linear_frequencies = np.linspace(0.0, nyquist_hertz, num_spectrogram_bins, dtype=internal_dtype)[bands_to_zero:]\n+    spectrogram_bins_mel = hertz_to_mel(linear_frequencies, mel_scale=\"kaldi\")[:, np.newaxis]\n+\n+    edges = np.linspace(\n+        hertz_to_mel(lower_edge_hertz, mel_scale=\"kaldi\"),\n+        hertz_to_mel(upper_edge_hertz, mel_scale=\"kaldi\"),\n+        num_mel_bins + 2,\n+        dtype=internal_dtype,\n+    )\n+\n+    lower_edge_mel, center_mel, upper_edge_mel = (\n+        edges[:-2][np.newaxis, :],\n+        edges[1:-1][np.newaxis, :],\n+        edges[2:][np.newaxis, :],\n+    )\n+\n+    lower_slopes = (spectrogram_bins_mel - lower_edge_mel) / (center_mel - lower_edge_mel)\n+    upper_slopes = (upper_edge_mel - spectrogram_bins_mel) / (upper_edge_mel - center_mel)\n+    mel_weights_matrix = np.maximum(0.0, np.minimum(lower_slopes, upper_slopes))\n+    return np.pad(mel_weights_matrix, [[bands_to_zero, 0], [0, 0]]).astype(dtype)\n+\n+\n+@requires(backends=(\"torch\",))\n+class LasrFeatureExtractor(SequenceFeatureExtractor):\n+    r\"\"\"\n+    Constructs a LASR feature extractor.\n+\n+    This feature extractor inherits from [`~feature_extraction_sequence_utils.SequenceFeatureExtractor`] which contains\n+    most of the main methods. Users should refer to this superclass for more information regarding those methods.\n+\n+    This class extracts mel-filter bank features from raw speech using a custom numpy implementation of the `Short Time\n+    Fourier Transform` which should match pytorch's `torch.stft` equivalent.\n+\n+    Args:\n+            feature_size (`int`, *optional*, defaults to 128):\n+                The feature dimension of the extracted features.\n+            sampling_rate (`int`, *optional*, defaults to 16000):\n+                The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).\n+            hop_length (`int`, *optional*, defaults to 160):\n+                Length of the overlapping windows for the STFT used to obtain the Mel Frequency coefficients.\n+            n_fft (`int`, *optional*, defaults to 512):\n+                Size of the Fourier transform.\n+            win_length (`int`, *optional*, defaults to 400):\n+                The window length for the STFT computation.\n+            padding_value (`float`, *optional*, defaults to 0.0):\n+                Padding value used to pad the audio. Should correspond to silences.\n+    \"\"\"\n+\n+    model_input_names = [\"input_features\", \"attention_mask\"]\n+\n+    def __init__(\n+        self,\n+        feature_size=128,\n+        sampling_rate=16000,\n+        hop_length=160,\n+        n_fft=512,\n+        win_length=400,\n+        padding_value=0.0,\n+        **kwargs,\n+    ):\n+        super().__init__(feature_size=feature_size, sampling_rate=sampling_rate, padding_value=padding_value, **kwargs)\n+\n+        self.hop_length = hop_length\n+        self.n_fft = n_fft\n+        self.win_length = win_length\n+        self.mel_filters = torch.from_numpy(\n+            linear_to_mel_weight_matrix(\n+                num_mel_bins=feature_size,\n+                num_spectrogram_bins=n_fft // 2 + 1,\n+                sample_rate=sampling_rate,\n+                lower_edge_hertz=125.0,\n+                upper_edge_hertz=7500.0,\n+                dtype=np.float64,\n+            )\n+        )\n+\n+    def _torch_extract_fbank_features(self, waveform, device=\"cpu\"):\n+        # spectrogram\n+        window = torch.hann_window(self.win_length, periodic=False, device=device, dtype=torch.float64)\n+        waveform = waveform.to(torch.float64)\n+\n+        # TODO: @eustlb, to be standardized\n+        # here we cannot use directly torch.stft because every fft frame is padded with zeros\n+        # due to unfold then rfft, while torch.stft unfolds with the number of fft points\n+        frames = waveform.unfold(-1, self.win_length, self.hop_length)\n+        stft = torch.fft.rfft(window * frames, n=self.n_fft)\n+        power_spec = torch.abs(stft) ** 2\n+\n+        # log mel spectrogram\n+        mel_filters = self.mel_filters.to(device)\n+        mel_spec = torch.clamp(power_spec @ mel_filters, min=1e-5)\n+        mel_spec = torch.log(mel_spec)\n+\n+        return mel_spec\n+\n+    def __call__(\n+        self,\n+        raw_speech: Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]],\n+        truncation: bool = False,\n+        pad_to_multiple_of: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_attention_mask: Optional[bool] = None,\n+        padding: Optional[str] = \"longest\",\n+        max_length: Optional[int] = None,\n+        sampling_rate: Optional[int] = None,\n+        do_normalize: Optional[bool] = None,\n+        device: Optional[str] = \"cpu\",\n+        return_token_timestamps: Optional[bool] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to featurize and prepare for the model one or several sequence(s). Implementation uses PyTorch for\n+        the STFT computation if available, otherwise a slower NumPy based one.\n+\n+        Args:\n+            raw_speech (`np.ndarray`, `list[float]`, `list[np.ndarray]`, `list[list[float]]`):\n+                The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float\n+                values, a list of numpy arrays or a list of list of float values. Must be mono channel audio, not\n+                stereo, i.e. single float per timestep.\n+            truncation (`bool`, *optional*, default to `True`):\n+                Activates truncation to cut input sequences longer than *max_length* to *max_length*.\n+            pad_to_multiple_of (`int`, *optional*, defaults to None):\n+                If set will pad the sequence to a multiple of the provided value.\n+\n+                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n+                `>= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.\n+            return_attention_mask (`bool`, *optional*):\n+                Whether to return the attention mask. If left to the default, will return the attention mask according\n+                to the specific feature_extractor's default.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+\n+                <Tip>\n+\n+                For Parakeet models, `attention_mask` should always be passed for batched inference, to avoid subtle\n+                bugs.\n+\n+                </Tip>\n+\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors instead of list of python integers. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return Numpy `np.ndarray` objects.\n+            sampling_rate (`int`, *optional*):\n+                The sampling rate at which the `raw_speech` input was sampled. It is strongly recommended to pass\n+                `sampling_rate` at the forward call to prevent silent errors and allow automatic speech recognition\n+                pipeline.\n+            padding_value (`float`, *optional*, defaults to 0.0):\n+                The value that is used to fill the padding values / vectors.\n+            do_normalize (`bool`, *optional*, defaults to `False`):\n+                Whether or not to zero-mean unit-variance normalize the input. Normalizing can help to significantly\n+                improve the performance of the model.\n+            device (`str`, *optional*, defaults to `'cpu'`):\n+                Specifies the device for computation of the log-mel spectrogram of audio signals in the\n+                `_torch_extract_fbank_features` method. (e.g., \"cpu\", \"cuda\")\n+            return_token_timestamps (`bool`, *optional*, defaults to `None`):\n+                Deprecated. Use `return_attention_mask` instead from which the number of frames can be inferred.\n+\n+                Whether or not to return the number of frames of the input raw_speech.\n+                These num_frames can be used by the model to compute word level timestamps.\n+        \"\"\"\n+        if sampling_rate is not None:\n+            if sampling_rate != self.sampling_rate:\n+                raise ValueError(\n+                    f\"The model corresponding to this feature extractor: {self.__class__.__name__} was trained using a\"\n+                    f\" sampling rate of {self.sampling_rate}. Please make sure that the provided `raw_speech` input\"\n+                    f\" was sampled with {self.sampling_rate} and not {sampling_rate}.\"\n+                )\n+        else:\n+            logger.warning(\n+                f\"It is strongly recommended to pass the `sampling_rate` argument to `{self.__class__.__name__}()`. \"\n+                \"Failing to do so can result in silent errors that might be hard to debug.\"\n+            )\n+\n+        # Convert to torch tensor\n+        if isinstance(raw_speech, np.ndarray):\n+            raw_speech = torch.tensor(raw_speech)\n+        elif isinstance(raw_speech, (list, tuple)):\n+            if isinstance(raw_speech[0], (list, np.ndarray)):\n+                raw_speech = [torch.tensor(speech) for speech in raw_speech]\n+            else:  # list[float]\n+                raw_speech = torch.tensor(raw_speech)\n+\n+        is_batched_torch = isinstance(raw_speech, torch.Tensor) and len(raw_speech.shape) > 1\n+        if is_batched_torch and len(raw_speech.shape) > 2:\n+            logger.warning(\n+                f\"Only mono-channel audio is supported for input to {self.__class__.__name__}. \"\n+                \"We will take the mean of the channels to convert to mono.\"\n+            )\n+            raw_speech = raw_speech.mean(-1)\n+\n+        is_batched_sequence = isinstance(raw_speech, (list, tuple))\n+        if is_batched_sequence:\n+            for speech in raw_speech:\n+                if len(speech.shape) > 1:\n+                    logger.warning(\n+                        f\"Only mono-channel audio is supported for input to {self.__class__.__name__}. \"\n+                        \"We will take the mean of the channels to convert to mono.\"\n+                    )\n+                    speech = speech.mean(-1)\n+\n+        if is_batched_torch or is_batched_sequence:\n+            raw_speech = [speech[:, None].to(torch.float32) for speech in raw_speech]\n+        else:\n+            raw_speech = [raw_speech[:, None].to(torch.float32)]\n+\n+        batched_speech = BatchFeature({\"input_features\": raw_speech})\n+        padded_inputs = self.pad(\n+            batched_speech,\n+            padding=padding,\n+            max_length=max_length,\n+            truncation=truncation,\n+            pad_to_multiple_of=pad_to_multiple_of,\n+            return_attention_mask=return_attention_mask,\n+            return_tensors=\"pt\",\n+        )\n+        input_features = padded_inputs.input_features.squeeze(-1)\n+        input_features = self._torch_extract_fbank_features(input_features, device)\n+        data = {\n+            \"input_features\": input_features.to(torch.float32),\n+        }\n+\n+        if return_attention_mask:\n+            attention_mask = padded_inputs.attention_mask[:, self.win_length - 1 :: self.hop_length]\n+            data[\"attention_mask\"] = attention_mask.to(torch.bool)\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"LasrFeatureExtractor\"]"
        },
        {
            "sha": "802ff3ea9ad580de66f1c7a632a024cc3d591e0e",
            "filename": "src/transformers/models/lasr/modeling_lasr.py",
            "status": "added",
            "additions": 729,
            "deletions": 0,
            "changes": 729,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2Fmodeling_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2Fmodeling_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Fmodeling_lasr.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -0,0 +1,729 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/lasr/modular_lasr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_lasr.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team and Google LLC. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from collections.abc import Callable\n+from dataclasses import dataclass\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...integrations import use_kernel_func_from_hub\n+from ...masking_utils import create_bidirectional_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutput, CausalLMOutput\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import check_model_inputs\n+from .configuration_lasr import LasrCTCConfig, LasrEncoderConfig\n+\n+\n+class LasrEncoderSubsampling(nn.Module):\n+    def __init__(self, config: LasrEncoderConfig):\n+        super().__init__()\n+        self.dense_0 = nn.Linear(config.num_mel_bins, config.hidden_size)\n+        self.conv_0 = nn.Conv1d(\n+            config.hidden_size,\n+            config.hidden_size,\n+            kernel_size=config.subsampling_conv_kernel_size,\n+            stride=config.subsampling_conv_stride,\n+        )\n+        self.conv_1 = nn.Conv1d(\n+            config.hidden_size,\n+            config.subsampling_conv_channels,\n+            kernel_size=config.subsampling_conv_kernel_size,\n+            stride=config.subsampling_conv_stride,\n+        )\n+        self.dense_1 = nn.Linear(config.subsampling_conv_channels, config.hidden_size)\n+        self.act_fn = nn.ReLU()\n+\n+    def forward(self, input_features: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.act_fn(self.dense_0(input_features))\n+        hidden_states = hidden_states.transpose(1, 2)\n+        hidden_states = self.act_fn(self.conv_0(hidden_states))\n+        hidden_states = self.act_fn(self.conv_1(hidden_states))\n+        hidden_states = hidden_states.transpose(1, 2)\n+        return self.dense_1(hidden_states)\n+\n+\n+class LasrEncoderRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: LasrEncoderConfig, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[LasrEncoderConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class LasrEncoderAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: LasrEncoderConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = False\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.rotary_fn = apply_rotary_pos_emb\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class LasrEncoderConvolutionModule(nn.Module):\n+    def __init__(self, config: LasrEncoderConfig, module_config=None):\n+        \"\"\"\n+        Args:\n+            config (LasrEncoderConfig): Configuration for the model.\n+            module_config (dict): Configuration for the module (e.g., encoder or decoder).\n+        \"\"\"\n+        super().__init__()\n+        channels = config.hidden_size\n+        # kernel_size should be an odd number for 'SAME' padding\n+        if module_config is None:\n+            # e.g. using `LasrEncoderEncoderConfig` in src/transformers/models/lasr_encoder/configuration_lasr_encoder.py\n+            kernel_size = config.conv_kernel_size\n+            self.activation = ACT2FN[getattr(config, \"hidden_act\", \"silu\")]\n+        else:\n+            kernel_size = module_config[\"kernel_size\"]\n+            self.activation = ACT2FN[module_config.get(\"activation\", \"silu\")]\n+        self.padding = \"same\"\n+        self.pointwise_conv1 = nn.Conv1d(\n+            channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n+        self.depthwise_conv = nn.Conv1d(\n+            channels,\n+            channels,\n+            kernel_size,\n+            stride=1,\n+            padding=self.padding,\n+            groups=channels,\n+            bias=config.convolution_bias,\n+        )\n+        self.norm = nn.BatchNorm1d(config.hidden_size, momentum=config.batch_norm_momentum)\n+        self.pointwise_conv2 = nn.Conv1d(\n+            channels, channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n+\n+    def forward(self, hidden_states, attention_mask=None):\n+        \"\"\"\n+        Compute convolution module.\n+\n+        Args:\n+            hidden_states (`torch.Tensor` of shape `(batch, time, channels)`): Input tensor.\n+            attention_mask (`torch.Tensor` of shape `(batch, 1, time, time)`): Attention mask.\n+\n+        Returns:\n+            `torch.Tensor`: Output tensor of shape `(batch, time, channels)`.\n+\n+        \"\"\"\n+        # exchange the temporal dimension and the feature dimension\n+        hidden_states = hidden_states.transpose(1, 2)\n+\n+        # GLU mechanism, (batch_size, 2*channel, dim)\n+        hidden_states = self.pointwise_conv1(hidden_states)\n+        # (batch_size, channel, dim)\n+        hidden_states = nn.functional.glu(hidden_states, dim=1)\n+\n+        # Apply padding mask before convolution\n+        if attention_mask is not None:\n+            if attention_mask.dtype == torch.bool:\n+                all_masked_rows = torch.all(~attention_mask, dim=2)\n+            else:\n+                all_masked_rows = torch.all(~(attention_mask == 0.0), dim=2)\n+            hidden_states = hidden_states.masked_fill(all_masked_rows, 0.0)\n+\n+        # 1D Depthwise Conv\n+        hidden_states = self.depthwise_conv(hidden_states)\n+        hidden_states = self.norm(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        hidden_states = self.pointwise_conv2(hidden_states)\n+\n+        return hidden_states.transpose(1, 2)\n+\n+\n+class LasrEncoderFeedForward(nn.Module):\n+    def __init__(self, config: LasrEncoderConfig):\n+        super().__init__()\n+        self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size, bias=config.attention_bias)\n+        self.activation = ACT2FN[config.hidden_act]\n+        self.linear2 = nn.Linear(config.intermediate_size, config.hidden_size, bias=config.attention_bias)\n+        self.activation_dropout = config.activation_dropout\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.activation(self.linear1(hidden_states))\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n+        hidden_states = self.linear2(hidden_states)\n+        return hidden_states\n+\n+\n+class LasrEncoderBlock(GradientCheckpointingLayer):\n+    def __init__(self, config: LasrEncoderConfig, layer_idx: int):\n+        super().__init__()\n+        self.gradient_checkpointing = False\n+\n+        self.feed_forward1 = LasrEncoderFeedForward(config)\n+        self.self_attn = LasrEncoderAttention(config, layer_idx)\n+        self.conv = LasrEncoderConvolutionModule(config)\n+        self.feed_forward2 = LasrEncoderFeedForward(config)\n+\n+        self.norm_feed_forward1 = nn.LayerNorm(config.hidden_size, config.layer_norm_eps, bias=False)\n+        self.norm_self_att = nn.LayerNorm(config.hidden_size, config.layer_norm_eps, bias=False)\n+        self.norm_conv = nn.LayerNorm(config.hidden_size, config.layer_norm_eps, bias=False)\n+        self.norm_feed_forward2 = nn.LayerNorm(config.hidden_size, config.layer_norm_eps, bias=False)\n+        self.norm_out = nn.LayerNorm(config.hidden_size, config.layer_norm_eps, bias=False)\n+\n+        self.feed_forward_residual_weights = config.feed_forward_residual_weights\n+        self.conv_residual_weights = config.conv_residual_weights\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.feed_forward1(self.norm_feed_forward1(hidden_states))\n+        hidden_states = (\n+            self.feed_forward_residual_weights[0] * residual + self.feed_forward_residual_weights[1] * hidden_states\n+        )\n+\n+        normalized_hidden_states = self.norm_self_att(hidden_states)\n+        attn_output, _ = self.self_attn(\n+            hidden_states=normalized_hidden_states,\n+            attention_mask=attention_mask,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = hidden_states + attn_output\n+\n+        conv_output = self.conv(self.norm_conv(hidden_states), attention_mask=attention_mask)\n+        hidden_states = self.conv_residual_weights[0] * hidden_states + self.conv_residual_weights[1] * conv_output\n+\n+        residual = hidden_states\n+        hidden_states = self.feed_forward2(self.norm_feed_forward2(hidden_states))\n+        hidden_states = (\n+            self.feed_forward_residual_weights[0] * residual + self.feed_forward_residual_weights[1] * hidden_states\n+        )\n+\n+        hidden_states = self.norm_out(hidden_states)\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class LasrPreTrainedModel(PreTrainedModel):\n+    config: LasrCTCConfig\n+    base_model_prefix = \"model\"\n+    main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"LasrEncoderBlock\"]\n+    _supports_flat_attention_mask = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    # TODO: @eustlb, add support when flash attention supports custom attention bias\n+    _supports_flash_attn = False\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": LasrEncoderBlock,\n+        \"attentions\": LasrEncoderAttention,\n+    }\n+\n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+\n+    def _get_subsampling_output_length(self, input_lengths: torch.Tensor):\n+        encoder_config = self.config.encoder_config if isinstance(self.config, LasrCTCConfig) else self.config\n+        kernel_size = encoder_config.subsampling_conv_kernel_size\n+        stride = encoder_config.subsampling_conv_stride\n+\n+        num_layers = 2\n+        for _ in range(num_layers):\n+            input_lengths = (input_lengths - kernel_size) // stride + 1\n+\n+        return input_lengths\n+\n+    def _get_output_attention_mask(self, attention_mask: torch.Tensor, target_length: Optional[int] = None):\n+        \"\"\"\n+        Convert the input attention mask to its subsampled form. `target_length` sets the desired output length, useful\n+        when the attention mask length differs from `sum(-1).max()` (i.e., when the longest sequence in the batch is padded)\n+        \"\"\"\n+        output_lengths = self._get_subsampling_output_length(attention_mask.sum(-1))\n+        # Use target_length if provided, otherwise use max length in batch\n+        max_length = target_length if target_length is not None else output_lengths.max()\n+        attention_mask = torch.arange(max_length, device=attention_mask.device) < output_lengths[:, None]\n+        return attention_mask\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The LasrEncoder model, based on the Conformer architecture](https://arxiv.org/abs/2005.08100).\n+    \"\"\"\n+)\n+class LasrEncoder(LasrPreTrainedModel):\n+    config: LasrEncoderConfig\n+    base_model_prefix = \"encoder\"\n+\n+    def __init__(self, config: LasrEncoderConfig):\n+        super().__init__(config)\n+        self.gradient_checkpointing = False\n+\n+        self.dropout = config.dropout\n+        self.dropout_positions = config.dropout_positions\n+        self.layerdrop = config.layerdrop\n+\n+        self.subsampler = LasrEncoderSubsampling(config)\n+        self.rotary_emb = LasrEncoderRotaryEmbedding(config)\n+        self.layers = nn.ModuleList(\n+            [LasrEncoderBlock(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.out_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps, bias=False)\n+\n+        self.post_init()\n+\n+    @auto_docstring\n+    @check_model_inputs()\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_features: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutput:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, LasrEncoder\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = TODO\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> encoder = ParakeetEncoder.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+\n+        >>> inputs = processor(ds[0][\"audio\"][\"array\"])\n+        >>> encoder_outputs = encoder(**inputs)\n+\n+        >>> print(encoder_outputs.last_hidden_state.shape)\n+        ```\n+        \"\"\"\n+\n+        hidden_states = self.subsampler(input_features)\n+        cos, sin = self.rotary_emb(\n+            hidden_states, torch.arange(hidden_states.shape[1], device=hidden_states.device).unsqueeze(0)\n+        )\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        cos = nn.functional.dropout(cos, p=self.dropout_positions, training=self.training)\n+        sin = nn.functional.dropout(sin, p=self.dropout_positions, training=self.training)\n+\n+        if attention_mask is not None:\n+            attention_mask = self._get_output_attention_mask(attention_mask, target_length=hidden_states.shape[1])\n+\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+        )\n+\n+        for encoder_layer in self.layers:\n+            # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n+            to_drop = False\n+            if self.training:\n+                dropout_probability = torch.rand([])\n+                if dropout_probability < self.layerdrop:  # skip the layer\n+                    to_drop = True\n+\n+            if not to_drop:\n+                hidden_states = encoder_layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    position_embeddings=(cos, sin),\n+                    **kwargs,\n+                )\n+\n+        hidden_states = self.out_norm(hidden_states)\n+\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n+\n+\n+@dataclass\n+class LasrGenerateOutput(ModelOutput):\n+    \"\"\"\n+    Outputs of Lasr models.\n+\n+    Args:\n+        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n+            if all batches finished early due to the `eos_token_id`.\n+        logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True`):\n+            Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n+            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n+            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n+        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True`):\n+            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n+            `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n+        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n+            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n+            `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n+    \"\"\"\n+\n+    sequences: torch.LongTensor\n+    logits: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Lasr Encoder with a Connectionist Temporal Classification (CTC) head.\n+    \"\"\"\n+)\n+class LasrForCTC(LasrPreTrainedModel):\n+    config: LasrCTCConfig\n+\n+    def __init__(self, config: LasrCTCConfig):\n+        super().__init__(config)\n+        self.encoder = LasrEncoder(config.encoder_config)\n+        # Conv rather than linear to be consistent with NeMO decoding layer\n+        self.ctc_head = nn.Conv1d(config.encoder_config.hidden_size, config.vocab_size, kernel_size=1)\n+\n+        self.post_init()\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_features: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutput:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, LasrForCTC\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = \"nvidia/lasr-ctc-1.1b\"\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> model = LasrForCTC.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+\n+        >>> inputs = processor(ds[0][\"audio\"][\"array\"], text=ds[0][\"text\"])\n+        >>> outputs = model(**inputs)\n+\n+        >>> print(outputs.loss)\n+        ```\"\"\"\n+\n+        encoder_outputs = self.encoder(\n+            input_features=input_features,\n+            attention_mask=attention_mask,\n+            **kwargs,\n+        )\n+\n+        hidden_states = encoder_outputs.last_hidden_state\n+        logits = self.ctc_head(hidden_states.transpose(1, 2)).transpose(1, 2)\n+\n+        loss = None\n+        if labels is not None:\n+            # retrieve loss input_lengths from attention_mask\n+            attention_mask = (\n+                attention_mask if attention_mask is not None else torch.ones_like(input_features, dtype=torch.long)\n+            )\n+            input_lengths = self._get_subsampling_output_length(attention_mask.sum(-1))\n+\n+            # assuming that padded tokens are filled with -100\n+            # when not being attended to\n+            labels_mask = labels != self.config.pad_token_id\n+            target_lengths = labels_mask.sum(-1)\n+            flattened_targets = labels.masked_select(labels_mask)\n+\n+            # ctc_loss doesn't support fp16\n+            log_probs = nn.functional.log_softmax(logits, dim=-1, dtype=torch.float32).transpose(0, 1)\n+\n+            with torch.backends.cudnn.flags(enabled=False):\n+                loss = nn.functional.ctc_loss(\n+                    log_probs,\n+                    flattened_targets,\n+                    input_lengths,\n+                    target_lengths,\n+                    blank=self.config.pad_token_id,\n+                    reduction=self.config.ctc_loss_reduction,\n+                    zero_infinity=self.config.ctc_zero_infinity,\n+                )\n+\n+        return CausalLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+    @torch.no_grad()\n+    def generate(\n+        self,\n+        input_features: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        return_dict_in_generate: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[LasrGenerateOutput, torch.LongTensor]:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, LasrForCTC\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = TODO\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> model = LasrForCTC.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+\n+        >>> inputs = processor(ds[0][\"audio\"][\"array\"], text=ds[0][\"text\"])\n+        >>> predicted_ids = model.generate(**inputs)\n+        >>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n+\n+        >>> print(transcription)\n+        ```\n+        \"\"\"\n+        kwargs[\"return_dict\"] = True\n+        outputs: CausalLMOutput = self.forward(\n+            input_features=input_features,\n+            attention_mask=attention_mask,\n+            **kwargs,\n+        )\n+\n+        # greedy decoding\n+        sequences = outputs.logits.argmax(dim=-1)\n+\n+        # mask out padded tokens\n+        if attention_mask is not None:\n+            attention_mask = self._get_output_attention_mask(attention_mask, target_length=sequences.shape[1])\n+            sequences[~attention_mask] = self.config.pad_token_id\n+\n+        if return_dict_in_generate:\n+            return LasrGenerateOutput(\n+                sequences=sequences,\n+                logits=outputs.logits,\n+                attentions=outputs.attentions,\n+                hidden_states=outputs.hidden_states,\n+            )\n+\n+        return sequences\n+\n+\n+__all__ = [\"LasrForCTC\", \"LasrEncoder\", \"LasrPreTrainedModel\"]"
        },
        {
            "sha": "c02b2ae0f1c371e79e6c73c9358984c28b5f284b",
            "filename": "src/transformers/models/lasr/modular_lasr.py",
            "status": "added",
            "additions": 569,
            "deletions": 0,
            "changes": 569,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2Fmodular_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2Fmodular_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Fmodular_lasr.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -0,0 +1,569 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team and Google LLC. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import itertools\n+from collections.abc import Callable\n+from typing import Optional, Union\n+\n+import torch\n+from tokenizers import Tokenizer\n+from tokenizers.models import Unigram\n+from torch import nn\n+\n+from ...masking_utils import create_bidirectional_mask\n+from ...modeling_outputs import BaseModelOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import check_model_inputs\n+from ..llama.modeling_llama import LlamaAttention, LlamaRotaryEmbedding, apply_rotary_pos_emb, eager_attention_forward\n+from ..parakeet.configuration_parakeet import ParakeetCTCConfig, ParakeetEncoderConfig\n+from ..parakeet.modeling_parakeet import (\n+    ParakeetEncoderBlock,\n+    ParakeetEncoderConvolutionModule,\n+    ParakeetForCTC,\n+    ParakeetPreTrainedModel,\n+)\n+from ..parakeet.processing_parakeet import ParakeetProcessor\n+from ..t5.tokenization_t5 import T5Tokenizer\n+\n+\n+class LasrTokenizer(T5Tokenizer, TokenizersBackend):\n+    def __init__(\n+        self,\n+        eos_token=\"</s>\",\n+        unk_token=\"<unk>\",\n+        pad_token=\"<pad>\",\n+        extra_ids=100,\n+        additional_special_tokens=None,\n+        vocab=None,\n+        vocab_file=None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            eos_token=eos_token,\n+            unk_token=unk_token,\n+            pad_token=pad_token,\n+            extra_ids=extra_ids,\n+            additional_special_tokens=additional_special_tokens,\n+            vocab=vocab,\n+            vocab_file=vocab_file,\n+            **kwargs,\n+        )\n+        self._tokenizer = Tokenizer(\n+            Unigram(\n+                self._vocab_scores,\n+                unk_id=3,\n+                byte_fallback=False,\n+            )\n+        )\n+\n+    def _decode(\n+        self,\n+        token_ids: Union[int, list[int]],\n+        skip_special_tokens: bool = False,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n+        group_tokens: bool = True,\n+        **kwargs,\n+    ) -> str:\n+        if isinstance(token_ids, int):\n+            token_ids = [token_ids]\n+        if group_tokens:\n+            token_ids = [token_group[0] for token_group in itertools.groupby(token_ids)]\n+\n+        # for CTC we filter out the blank token, which is the pad token\n+        token_ids = [token for token in token_ids if token != self.pad_token_id]\n+\n+        return TokenizersBackend._decode(\n+            self,\n+            token_ids=token_ids,\n+            skip_special_tokens=skip_special_tokens,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+            **kwargs,\n+        )\n+\n+\n+class LasrProcessor(ParakeetProcessor):\n+    tokenizer_class = \"ParakeetTokenizerFast\"\n+\n+\n+class LasrEncoderConfig(ParakeetEncoderConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`LasrEncoder`]. It is used to instantiate a\n+    `LasrEncoder` model according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+            hidden_size (`int`, *optional*, defaults to 512):\n+                Dimension of the layers and the hidden states.\n+            num_hidden_layers (`int`, *optional*, defaults to 17):\n+                Number of hidden layers in the Transformer encoder.\n+            num_attention_heads (`int`, *optional*, defaults to 8):\n+                Number of attention heads for each attention layer in the Transformer encoder.\n+            intermediate_size (`int`, *optional*, defaults to 2048):\n+                Dimension of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n+            hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+                The non-linear activation function (function or string) in the encoder and pooler.\n+            attention_bias (`bool`, *optional*, defaults to `False`):\n+                Whether to use bias in the attention layers.\n+            convolution_bias (`bool`, *optional*, defaults to `False`):\n+                Whether to use bias in convolutions of the conformer's convolution module.\n+            conv_kernel_size (`int`, *optional*, defaults to 32):\n+                The kernel size of the convolution layers in the Conformer block.\n+            subsampling_conv_channels (`int`, *optional*, defaults to 256):\n+                The number of channels in the subsampling convolution layers.\n+            subsampling_conv_kernel_size (`int`, *optional*, defaults to 5):\n+                The kernel size of the subsampling convolution layers.\n+            subsampling_conv_stride (`int`, *optional*, defaults to 2):\n+                The stride of the subsampling convolution layers.\n+            num_mel_bins (`int`, *optional*, defaults to 128):\n+                Number of mel features.\n+            dropout (`float`, *optional*, defaults to 0.1):\n+                The dropout ratio for all fully connected layers in the embeddings, encoder, and pooler.\n+            dropout_positions (`float`, *optional*, defaults to 0.0):\n+                The dropout ratio for the positions in the input sequence.\n+            layerdrop (`float`, *optional*, defaults to 0.1):\n+                The dropout ratio for the layers in the encoder.\n+            activation_dropout (`float`, *optional*, defaults to 0.1):\n+                The dropout ratio for activations inside the fully connected layer.\n+            attention_dropout (`float`, *optional*, defaults to 0.1):\n+                The dropout ratio for the attention layers.\n+            max_position_embeddings (`int`, *optional*, defaults to 10000):\n+                The maximum sequence length that this model might ever be used with.\n+            initializer_range (`float`, *optional*, defaults to 0.02):\n+                The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+            layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+                The epsilon used by the layer normalization layers.\n+            feed_forward_residual_weights (`tuple[float, float]`, *optional*, defaults to `[1.5, 0.5]`):\n+                The residual weights for the feed forward layers.\n+            conv_residual_weights (`tuple[float, float]`, *optional*, defaults to `[2.0, 1.0]`):\n+                The residual weights for the convolution layers.\n+            batch_norm_momentum (`float`, *optional*, defaults to 0.01):\n+                The momentum for the batch normalization layers.\n+            rope_parameters (`RopeParameters`, *optional*):\n+                Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n+                a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+                with longer `max_position_embeddings`.\n+\n+    Example:\n+        ```python\n+        >>> from transformers import LasrEncoderModel, LasrEncoderConfig\n+\n+        >>> # Initializing a `LasrEncoder` configuration\n+        >>> configuration = LasrEncoderConfig()\n+\n+        >>> # Initializing a model from the configuration\n+        >>> model = LasrEncoderModel(configuration)\n+\n+        >>> # Accessing the model configuration\n+        >>> configuration = model.config\n+        ```\n+\n+    This configuration class is based on the LasrEncoder architecture from Google Health AI. You can find more details\n+    and pre-trained models at [TODO/TODO](https://huggingface.co/TODO/TODO).\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=512,\n+        num_hidden_layers=17,\n+        num_attention_heads=8,\n+        intermediate_size=2048,\n+        hidden_act=\"silu\",\n+        attention_bias=False,\n+        convolution_bias=False,\n+        conv_kernel_size=32,\n+        subsampling_conv_channels=256,\n+        subsampling_conv_kernel_size=5,\n+        subsampling_conv_stride=2,\n+        num_mel_bins=128,\n+        dropout=0.1,\n+        dropout_positions=0.0,\n+        layerdrop=0.1,\n+        activation_dropout=0.1,\n+        attention_dropout=0.1,\n+        max_position_embeddings=10000,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-6,\n+        feed_forward_residual_weights=[1.5, 0.5],\n+        conv_residual_weights=[2.0, 1.0],\n+        batch_norm_momentum=0.01,\n+        rope_parameters=None,\n+        **kwargs,\n+    ):\n+        self.rope_parameters = rope_parameters\n+        self.layer_norm_eps = layer_norm_eps\n+        self.feed_forward_residual_weights = feed_forward_residual_weights\n+        self.conv_residual_weights = conv_residual_weights\n+        self.batch_norm_momentum = batch_norm_momentum\n+\n+        super().__init__(\n+            hidden_size=hidden_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            intermediate_size=intermediate_size,\n+            hidden_act=hidden_act,\n+            attention_bias=attention_bias,\n+            convolution_bias=convolution_bias,\n+            conv_kernel_size=conv_kernel_size,\n+            subsampling_conv_channels=subsampling_conv_channels,\n+            num_mel_bins=num_mel_bins,\n+            subsampling_conv_kernel_size=subsampling_conv_kernel_size,\n+            subsampling_conv_stride=subsampling_conv_stride,\n+            dropout=dropout,\n+            dropout_positions=dropout_positions,\n+            layerdrop=layerdrop,\n+            activation_dropout=activation_dropout,\n+            attention_dropout=attention_dropout,\n+            max_position_embeddings=max_position_embeddings,\n+            initializer_range=initializer_range,\n+            **kwargs,\n+        )\n+\n+        del self.subsampling_factor\n+        del self.scale_input\n+\n+\n+class LasrCTCConfig(ParakeetCTCConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`LasrForCTC`]. It is used to instantiate a\n+    Lasr CTC model according to the specified arguments, defining the model architecture.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+    Args:\n+            vocab_size (`int`, *optional*, defaults to 512):\n+                Vocabulary size of the model.\n+            ctc_loss_reduction (`str`, *optional*, defaults to `\"mean\"`):\n+                Specifies the reduction to apply to the output of `torch.nn.CTCLoss`. Only relevant when training an\n+                instance of [`LasrForCTC`].\n+            ctc_zero_infinity (`bool`, *optional*, defaults to `True`):\n+                Whether to zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite losses mainly\n+                occur when the inputs are too short to be aligned to the targets. Only relevant when training an instance\n+                of [`LasrForCTC`].\n+            encoder_config (`Union[dict, LasrEncoderConfig]`, *optional*):\n+                The config object or dictionary of the encoder.\n+            pad_token_id (`int`, *optional*, defaults to 0):\n+                Padding token id. Also used as blank token id.\n+    Example:\n+        ```python\n+        >>> from transformers import LasrForCTC, LasrCTCConfig\n+        >>> # Initializing a Lasr configuration\n+        >>> configuration = LasrCTCConfig()\n+        >>> # Initializing a model from the configuration\n+        >>> model = LasrForCTC(configuration)\n+        >>> # Accessing the model configuration\n+        >>> configuration = model.config\n+        ```\n+    This configuration class is based on the Lasr CTC architecture from Google Health AI. You can find more details\n+    and pre-trained models at [TODO/TODO](https://huggingface.co/TODO/TODO).\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        vocab_size=512,\n+        ctc_loss_reduction=\"mean\",\n+        ctc_zero_infinity=True,\n+        encoder_config: Union[dict, LasrEncoderConfig] = None,\n+        pad_token_id=0,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            vocab_size=vocab_size,\n+            ctc_loss_reduction=ctc_loss_reduction,\n+            ctc_zero_infinity=ctc_zero_infinity,\n+            encoder_config=encoder_config,\n+            pad_token_id=pad_token_id,\n+            **kwargs,\n+        )\n+\n+\n+class LasrEncoderSubsampling(nn.Module):\n+    def __init__(self, config: LasrEncoderConfig):\n+        super().__init__()\n+        self.dense_0 = nn.Linear(config.num_mel_bins, config.hidden_size)\n+        self.conv_0 = nn.Conv1d(\n+            config.hidden_size,\n+            config.hidden_size,\n+            kernel_size=config.subsampling_conv_kernel_size,\n+            stride=config.subsampling_conv_stride,\n+        )\n+        self.conv_1 = nn.Conv1d(\n+            config.hidden_size,\n+            config.subsampling_conv_channels,\n+            kernel_size=config.subsampling_conv_kernel_size,\n+            stride=config.subsampling_conv_stride,\n+        )\n+        self.dense_1 = nn.Linear(config.subsampling_conv_channels, config.hidden_size)\n+        self.act_fn = nn.ReLU()\n+\n+    def forward(self, input_features: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.act_fn(self.dense_0(input_features))\n+        hidden_states = hidden_states.transpose(1, 2)\n+        hidden_states = self.act_fn(self.conv_0(hidden_states))\n+        hidden_states = self.act_fn(self.conv_1(hidden_states))\n+        hidden_states = hidden_states.transpose(1, 2)\n+        return self.dense_1(hidden_states)\n+\n+\n+class LasrEncoderRotaryEmbedding(LlamaRotaryEmbedding): ...\n+\n+\n+class LasrEncoderAttention(LlamaAttention):\n+    def __init__(self, config: LasrEncoderConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.is_causal = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class LasrEncoderConvolutionModule(ParakeetEncoderConvolutionModule):\n+    def __init__(self, config: LasrEncoderConfig, module_config=None):\n+        super().__init__(config, module_config)\n+        self.padding = \"same\"\n+        self.norm = nn.BatchNorm1d(config.hidden_size, momentum=config.batch_norm_momentum)\n+\n+\n+class LasrEncoderBlock(ParakeetEncoderBlock):\n+    def __init__(self, config: LasrEncoderConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+\n+        self.feed_forward_residual_weights = config.feed_forward_residual_weights\n+        self.conv_residual_weights = config.conv_residual_weights\n+\n+        self.norm_feed_forward1 = nn.LayerNorm(config.hidden_size, config.layer_norm_eps, bias=False)\n+        self.norm_self_att = nn.LayerNorm(config.hidden_size, config.layer_norm_eps, bias=False)\n+        self.norm_conv = nn.LayerNorm(config.hidden_size, config.layer_norm_eps, bias=False)\n+        self.norm_feed_forward2 = nn.LayerNorm(config.hidden_size, config.layer_norm_eps, bias=False)\n+        self.norm_out = nn.LayerNorm(config.hidden_size, config.layer_norm_eps, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.feed_forward1(self.norm_feed_forward1(hidden_states))\n+        hidden_states = (\n+            self.feed_forward_residual_weights[0] * residual + self.feed_forward_residual_weights[1] * hidden_states\n+        )\n+\n+        normalized_hidden_states = self.norm_self_att(hidden_states)\n+        attn_output, _ = self.self_attn(\n+            hidden_states=normalized_hidden_states,\n+            attention_mask=attention_mask,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = hidden_states + attn_output\n+\n+        conv_output = self.conv(self.norm_conv(hidden_states), attention_mask=attention_mask)\n+        hidden_states = self.conv_residual_weights[0] * hidden_states + self.conv_residual_weights[1] * conv_output\n+\n+        residual = hidden_states\n+        hidden_states = self.feed_forward2(self.norm_feed_forward2(hidden_states))\n+        hidden_states = (\n+            self.feed_forward_residual_weights[0] * residual + self.feed_forward_residual_weights[1] * hidden_states\n+        )\n+\n+        hidden_states = self.norm_out(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class LasrPreTrainedModel(ParakeetPreTrainedModel):\n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(module)\n+\n+    def _get_subsampling_output_length(self, input_lengths: torch.Tensor):\n+        encoder_config = self.config.encoder_config if isinstance(self.config, LasrCTCConfig) else self.config\n+        kernel_size = encoder_config.subsampling_conv_kernel_size\n+        stride = encoder_config.subsampling_conv_stride\n+\n+        num_layers = 2\n+        for _ in range(num_layers):\n+            input_lengths = (input_lengths - kernel_size) // stride + 1\n+\n+        return input_lengths\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The LasrEncoder model, based on the Conformer architecture](https://arxiv.org/abs/2005.08100).\n+    \"\"\"\n+)\n+class LasrEncoder(LasrPreTrainedModel):\n+    config: LasrEncoderConfig\n+    base_model_prefix = \"encoder\"\n+\n+    def __init__(self, config: LasrEncoderConfig):\n+        super().__init__(config)\n+        self.gradient_checkpointing = False\n+\n+        self.dropout = config.dropout\n+        self.dropout_positions = config.dropout_positions\n+        self.layerdrop = config.layerdrop\n+\n+        self.subsampler = LasrEncoderSubsampling(config)\n+        self.rotary_emb = LasrEncoderRotaryEmbedding(config)\n+        self.layers = nn.ModuleList(\n+            [LasrEncoderBlock(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.out_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps, bias=False)\n+\n+        self.post_init()\n+\n+    @auto_docstring\n+    @check_model_inputs()\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_features: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutput:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, LasrEncoder\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = TODO\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> encoder = ParakeetEncoder.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+\n+        >>> inputs = processor(ds[0][\"audio\"][\"array\"])\n+        >>> encoder_outputs = encoder(**inputs)\n+\n+        >>> print(encoder_outputs.last_hidden_state.shape)\n+        ```\n+        \"\"\"\n+\n+        hidden_states = self.subsampler(input_features)\n+        cos, sin = self.rotary_emb(\n+            hidden_states, torch.arange(hidden_states.shape[1], device=hidden_states.device).unsqueeze(0)\n+        )\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        cos = nn.functional.dropout(cos, p=self.dropout_positions, training=self.training)\n+        sin = nn.functional.dropout(sin, p=self.dropout_positions, training=self.training)\n+\n+        if attention_mask is not None:\n+            attention_mask = self._get_output_attention_mask(attention_mask, target_length=hidden_states.shape[1])\n+\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+        )\n+\n+        for encoder_layer in self.layers:\n+            # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n+            to_drop = False\n+            if self.training:\n+                dropout_probability = torch.rand([])\n+                if dropout_probability < self.layerdrop:  # skip the layer\n+                    to_drop = True\n+\n+            if not to_drop:\n+                hidden_states = encoder_layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    position_embeddings=(cos, sin),\n+                    **kwargs,\n+                )\n+\n+        hidden_states = self.out_norm(hidden_states)\n+\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n+\n+\n+class LasrForCTC(ParakeetForCTC):\n+    def generate(**super_kwargs):\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, LasrForCTC\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = TODO\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> model = LasrForCTC.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+\n+        >>> inputs = processor(ds[0][\"audio\"][\"array\"], text=ds[0][\"text\"])\n+        >>> predicted_ids = model.generate(**inputs)\n+        >>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n+\n+        >>> print(transcription)\n+        ```\n+        \"\"\"\n+        return super().generate(**super_kwargs)\n+\n+\n+__all__ = [\n+    \"LasrForCTC\",\n+    \"LasrEncoder\",\n+    \"LasrPreTrainedModel\",\n+    \"LasrProcessor\",\n+    \"LasrEncoderConfig\",\n+    \"LasrCTCConfig\",\n+    \"LasrTokenizer\",\n+]"
        },
        {
            "sha": "3396986866e285a1a0e7d42defcf016959b8739d",
            "filename": "src/transformers/models/lasr/processing_lasr.py",
            "status": "added",
            "additions": 96,
            "deletions": 0,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2Fprocessing_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2Fprocessing_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Fprocessing_lasr.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -0,0 +1,96 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/lasr/modular_lasr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_lasr.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team and Google LLC. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+from ...audio_utils import AudioInput, make_list_of_audio\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class LasrProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"audio_kwargs\": {\n+            \"sampling_rate\": 16000,\n+            \"padding\": \"longest\",\n+            \"return_attention_mask\": True,\n+        },\n+        \"text_kwargs\": {\n+            \"padding\": True,\n+            \"padding_side\": \"right\",\n+            \"add_special_tokens\": False,\n+        },\n+        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+    }\n+\n+\n+class LasrProcessor(ProcessorMixin):\n+    tokenizer_class = \"ParakeetTokenizerFast\"\n+\n+    def __init__(self, feature_extractor, tokenizer):\n+        super().__init__(feature_extractor, tokenizer)\n+\n+    def __call__(\n+        self,\n+        audio: AudioInput,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput], None] = None,\n+        sampling_rate: Optional[int] = None,\n+        **kwargs: Unpack[LasrProcessorKwargs],\n+    ):\n+        audio = make_list_of_audio(audio)\n+\n+        output_kwargs = self._merge_kwargs(\n+            LasrProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        if sampling_rate is None:\n+            logger.warning_once(\n+                f\"You've provided audio without specifying the sampling rate. It will be assumed to be {output_kwargs['audio_kwargs']['sampling_rate']}, which can result in silent errors.\"\n+            )\n+        elif sampling_rate != output_kwargs[\"audio_kwargs\"][\"sampling_rate\"]:\n+            raise ValueError(\n+                f\"The sampling rate of the audio ({sampling_rate}) does not match the sampling rate of the processor ({output_kwargs['audio_kwargs']['sampling_rate']}). Please provide resampled the audio to the expected sampling rate.\"\n+            )\n+\n+        if audio is not None:\n+            inputs = self.feature_extractor(audio, **output_kwargs[\"audio_kwargs\"])\n+        if text is not None:\n+            encodings = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+\n+        if text is None:\n+            return inputs\n+        else:\n+            inputs[\"labels\"] = encodings[\"input_ids\"]\n+            return inputs\n+\n+    @property\n+    def model_input_names(self):\n+        feature_extractor_input_names = self.feature_extractor.model_input_names\n+        return feature_extractor_input_names + [\"labels\"]\n+\n+\n+__all__ = [\"LasrProcessor\"]"
        },
        {
            "sha": "b88a3a0f9b57fcd83a8a555b3e18c655ab022786",
            "filename": "src/transformers/models/lasr/tokenization_lasr.py",
            "status": "added",
            "additions": 190,
            "deletions": 0,
            "changes": 190,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2Ftokenization_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Flasr%2Ftokenization_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Ftokenization_lasr.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -0,0 +1,190 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/lasr/modular_lasr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_lasr.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team and Google LLC. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import itertools\n+import re\n+from typing import Optional, Union\n+\n+from tokenizers import Tokenizer, decoders, pre_tokenizers, processors\n+from tokenizers.models import Unigram\n+\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n+\n+\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\", \"tokenizer_file\": \"tokenizer.json\"}\n+\n+\n+class LasrTokenizer(TokenizersBackend):\n+    \"\"\"\n+    Construct a LASR tokenizer (backed by HuggingFace's *tokenizers* library). Based on\n+    [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models).\n+\n+    This tokenizer inherits from [`TokenizersBackend`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n+\n+    Args:\n+        vocab_file (`str`, *optional*):\n+            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n+            contains the vocabulary necessary to instantiate a tokenizer.\n+        eos_token (`str`, *optional*, defaults to `\"</s>\"`):\n+            The end of sequence token.\n+\n+            <Tip>\n+\n+            When building a sequence using special tokens, this is not the token that is used for the end of sequence.\n+            The token used is the `sep_token`.\n+\n+            </Tip>\n+\n+        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n+            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n+            token instead.\n+        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n+            The token used for padding, for example when batching sequences of different lengths.\n+        extra_ids (`int`, *optional*, defaults to 100):\n+            Add a number of extra ids added to the vocabulary for use as sentinels. These tokens are accessible as\n+            \"<extra_id_{%d}>\" where \"{%d}\" is a number between 0 and extra_ids-1. These tokens can be retrieved by\n+            calling get_sentinel_tokens method and token ids can be by calling get_sentinel_token_ids method\n+        additional_special_tokens (`list[str]`, *optional*):\n+            Additional special tokens used by the tokenizer.\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dict. If not provided, a minimal vocabulary is created using the special tokens.\n+    \"\"\"\n+\n+    vocab_files_names = VOCAB_FILES_NAMES\n+    model_input_names = [\"input_ids\", \"attention_mask\"]\n+    slow_tokenizer_class = None\n+\n+    def __init__(\n+        self,\n+        eos_token=\"</s>\",\n+        unk_token=\"<unk>\",\n+        pad_token=\"<pad>\",\n+        extra_ids=100,\n+        additional_special_tokens=None,\n+        vocab=None,\n+        vocab_file=None,\n+        **kwargs,\n+    ):\n+        self.vocab_file = vocab_file\n+        self._extra_ids = extra_ids\n+\n+        # Handle extra_ids and additional_special_tokens\n+        if additional_special_tokens is not None:\n+            extra_tokens = [x for x in additional_special_tokens if \"<extra_id_\" in str(x)]\n+            if len(extra_tokens) < 1:\n+                additional_special_tokens += [f\"<extra_id_{i}>\" for i in range(extra_ids)]\n+            elif extra_ids > 0 and extra_ids != len(extra_tokens):\n+                raise ValueError(\n+                    f\"Both extra_ids ({extra_ids}) and additional_special_tokens ({additional_special_tokens}) are\"\n+                    \" provided to LasrTokenizer. In this case the additional_special_tokens must include the extra_ids\"\n+                    \" tokens\"\n+                )\n+        else:\n+            extra_tokens = [f\"<extra_id_{i}>\" for i in range(extra_ids)]\n+            additional_special_tokens = extra_tokens\n+\n+        # LASR vocab structure: <pad>=0, </s>=1, <unk>=2, then regular vocab, then extra_ids in reverse\n+        if vocab is not None:\n+            self._vocab_scores = vocab\n+        else:\n+            self._vocab_scores = [\n+                (str(pad_token), 0.0),\n+                (str(eos_token), 0.0),\n+                (str(unk_token), 0.0),\n+                (\"â–\", -2.0),  # Space token\n+            ]\n+            for i in range(extra_ids - 1, -1, -1):\n+                self._vocab_scores.append((f\"<extra_id_{i}>\", 0.0))\n+        self._tokenizer = Tokenizer(\n+            Unigram(\n+                self._vocab_scores,\n+                unk_id=3,\n+                byte_fallback=False,\n+            )\n+        )\n+\n+        self._tokenizer.normalizer = None\n+\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n+            [\n+                pre_tokenizers.WhitespaceSplit(),\n+                pre_tokenizers.Metaspace(replacement=\"â–\", prepend_scheme=\"always\", split=True),\n+            ]\n+        )\n+\n+        self._tokenizer.decoder = decoders.Metaspace(replacement=\"â–\", prepend_scheme=\"always\", split=True)\n+\n+        tokenizer_object = self._tokenizer\n+\n+        super().__init__(\n+            tokenizer_object=tokenizer_object,\n+            eos_token=eos_token,\n+            unk_token=unk_token,\n+            pad_token=pad_token,\n+            extra_ids=extra_ids,\n+            additional_special_tokens=additional_special_tokens,\n+            **kwargs,\n+        )\n+\n+        self._tokenizer.post_processor = processors.TemplateProcessing(\n+            single=[\"$A\", \"</s>\"],\n+            pair=[\"$A\", \"</s>\", \"$B\", \"</s>\"],\n+            special_tokens=[\n+                (\"</s>\", self.eos_token_id),\n+            ],\n+        )\n+\n+    def get_sentinel_tokens(self):\n+        \"\"\"Get the list of sentinel tokens (extra_id tokens) from additional_special_tokens.\"\"\"\n+        return list(\n+            set(filter(lambda x: bool(re.search(r\"<extra_id_\\d+>\", x)) is not None, self.additional_special_tokens))\n+        )\n+\n+    def get_sentinel_token_ids(self):\n+        \"\"\"Get the token IDs for sentinel tokens.\"\"\"\n+        return [self.convert_tokens_to_ids(token) for token in self.get_sentinel_tokens()]\n+\n+    def _decode(\n+        self,\n+        token_ids: Union[int, list[int]],\n+        skip_special_tokens: bool = False,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n+        group_tokens: bool = True,\n+        **kwargs,\n+    ) -> str:\n+        if isinstance(token_ids, int):\n+            token_ids = [token_ids]\n+        if group_tokens:\n+            token_ids = [token_group[0] for token_group in itertools.groupby(token_ids)]\n+\n+        # for CTC we filter out the blank token, which is the pad token\n+        token_ids = [token for token in token_ids if token != self.pad_token_id]\n+\n+        return super()._decode(\n+            token_ids=token_ids,\n+            skip_special_tokens=skip_special_tokens,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"LasrTokenizer\"]"
        },
        {
            "sha": "48ece84b7b84bee18adc266fad555b7a12354751",
            "filename": "src/transformers/models/parakeet/configuration_parakeet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fparakeet%2Fconfiguration_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fparakeet%2Fconfiguration_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fconfiguration_parakeet.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -121,9 +121,6 @@ def __init__(\n         initializer_range=0.02,\n         **kwargs,\n     ):\n-        super().__init__(\n-            **kwargs,\n-        )\n         self.hidden_size = hidden_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n@@ -133,10 +130,7 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.convolution_bias = convolution_bias\n \n-        if (conv_kernel_size - 1) % 2 != 0:\n-            raise ValueError(f\"conv_kernel_size must be odd, got {conv_kernel_size}\")\n         self.conv_kernel_size = conv_kernel_size\n-\n         self.subsampling_conv_kernel_size = subsampling_conv_kernel_size\n         self.subsampling_conv_stride = subsampling_conv_stride\n \n@@ -153,6 +147,10 @@ def __init__(\n         self.scale_input = scale_input\n         self.initializer_range = initializer_range\n \n+        super().__init__(\n+            **kwargs,\n+        )\n+\n \n class ParakeetCTCConfig(PreTrainedConfig):\n     r\"\"\""
        },
        {
            "sha": "d5e8d8fdf8bfbc1c81723a6a407d5b1d16b43679",
            "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -155,7 +155,7 @@ def forward(self, hidden_states, attention_mask=None):\n \n         Args:\n             hidden_states (`torch.Tensor` of shape `(batch, time, channels)`): Input tensor.\n-            attention_mask (`torch.Tensor` of shape `(batch, 1, time)`): Attention mask.\n+            attention_mask (`torch.Tensor` of shape `(batch, 1, time, time)`): Attention mask.\n \n         Returns:\n             `torch.Tensor`: Output tensor of shape `(batch, time, channels)`.\n@@ -171,7 +171,10 @@ def forward(self, hidden_states, attention_mask=None):\n \n         # Apply padding mask before convolution\n         if attention_mask is not None:\n-            all_masked_rows = torch.all(~attention_mask, dim=-1)\n+            if attention_mask.dtype == torch.bool:\n+                all_masked_rows = torch.all(~attention_mask, dim=2)\n+            else:\n+                all_masked_rows = torch.all(~(attention_mask == 0.0), dim=2)\n             hidden_states = hidden_states.masked_fill(all_masked_rows, 0.0)\n \n         # 1D Depthwise Conv"
        },
        {
            "sha": "1ac54ba7555266f49dbdb85938269c85533b2941",
            "filename": "src/transformers/models/parakeet/processing_parakeet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -28,6 +28,7 @@ class ParakeetProcessorKwargs(ProcessingKwargs, total=False):\n         \"audio_kwargs\": {\n             \"sampling_rate\": 16000,\n             \"padding\": \"longest\",\n+            \"return_attention_mask\": True,\n         },\n         \"text_kwargs\": {\n             \"padding\": True,"
        },
        {
            "sha": "133ac8726cd2f58ddbc4b518d4d9f31b55b59843",
            "filename": "src/transformers/utils/auto_docstring.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fauto_docstring.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -67,6 +67,7 @@\n     \"donut\": \"DonutSwinConfig\",\n     \"esmfold\": \"EsmConfig\",\n     \"parakeet\": \"ParakeetCTCConfig\",\n+    \"lasr\": \"LasrCTCConfig\",\n }\n \n _re_checkpoint = re.compile(r\"\\[(.+?)\\]\\((https://huggingface\\.co/.+?)\\)\")"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/lasr/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/tests%2Fmodels%2Flasr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/tests%2Fmodels%2Flasr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flasr%2F__init__.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967"
        },
        {
            "sha": "4b723e71539052d1392e6c6ad6688a2b7532313b",
            "filename": "tests/models/lasr/test_modeling_lasr.py",
            "status": "added",
            "additions": 390,
            "deletions": 0,
            "changes": 390,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/tests%2Fmodels%2Flasr%2Ftest_modeling_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/tests%2Fmodels%2Flasr%2Ftest_modeling_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flasr%2Ftest_modeling_lasr.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -0,0 +1,390 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Lasr model.\"\"\"\n+\n+import tempfile\n+import unittest\n+\n+from transformers import is_datasets_available, is_torch_available\n+from transformers.testing_utils import cleanup, require_torch, slow, torch_device\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+\n+\n+if is_datasets_available():\n+    from datasets import Audio, load_dataset\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        AutoProcessor,\n+        LasrCTCConfig,\n+        LasrEncoder,\n+        LasrEncoderConfig,\n+        LasrForCTC,\n+    )\n+\n+\n+class LasrEncoderModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        seq_length=1024,\n+        is_training=True,\n+        hidden_size=64,\n+        num_hidden_layers=2,\n+        num_mel_bins=80,\n+        num_attention_heads=4,\n+        intermediate_size=256,\n+        conv_kernel_size=8,\n+        subsampling_conv_channels=32,\n+        subsampling_conv_kernel_size=5,\n+        subsampling_conv_stride=2,\n+    ):\n+        # testing suite parameters\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.num_mel_bins = num_mel_bins\n+        self.is_training = is_training\n+\n+        # config parameters\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.conv_kernel_size = conv_kernel_size\n+        self.subsampling_conv_channels = subsampling_conv_channels\n+        self.subsampling_conv_kernel_size = subsampling_conv_kernel_size\n+        self.subsampling_conv_stride = subsampling_conv_stride\n+\n+        self.num_mel_bins = num_mel_bins\n+\n+        # output sequence length after subsampling\n+        self.output_seq_length = self._get_output_seq_length(self.seq_length)\n+        self.encoder_seq_length = self.output_seq_length\n+        self.key_length = self.output_seq_length\n+\n+    def _get_output_seq_length(self, seq_length):\n+        kernel_size = self.subsampling_conv_kernel_size\n+        stride = self.subsampling_conv_stride\n+        num_layers = 2\n+\n+        input_length = seq_length\n+        for _ in range(num_layers):\n+            input_length = (input_length - kernel_size) // stride + 1\n+\n+        return input_length\n+\n+    def prepare_config_and_inputs(self):\n+        input_features = floats_tensor([self.batch_size, self.seq_length, self.num_mel_bins])\n+        attention_mask = random_attention_mask([self.batch_size, self.seq_length])\n+        config = self.get_config()\n+\n+        return config, input_features, attention_mask\n+\n+    def get_config(self):\n+        return LasrEncoderConfig(\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            conv_kernel_size=self.conv_kernel_size,\n+            subsampling_conv_channels=self.subsampling_conv_channels,\n+            subsampling_conv_kernel_size=self.subsampling_conv_kernel_size,\n+            subsampling_conv_stride=self.subsampling_conv_stride,\n+            num_mel_bins=self.num_mel_bins,\n+        )\n+\n+    def create_and_check_model(self, config, input_features, attention_mask):\n+        model = LasrEncoder(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(input_features, attention_mask=attention_mask)\n+\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape, (self.batch_size, self.output_seq_length, config.hidden_size)\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, input_features, attention_mask = self.prepare_config_and_inputs()\n+        inputs_dict = {\n+            \"input_features\": input_features,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+    def check_ctc_loss(self, config, input_values, *args):\n+        model = LasrForCTC(config=config)\n+        model.to(torch_device)\n+\n+        # make sure that dropout is disabled\n+        model.eval()\n+\n+        input_values = input_values[:3]\n+        attention_mask = torch.ones(input_values.shape, device=torch_device, dtype=torch.long)\n+\n+        input_lengths = [input_values.shape[-1] // i for i in [4, 2, 1]]\n+        max_length_labels = model._get_feat_extract_output_lengths(torch.tensor(input_lengths))\n+        labels = ids_tensor((input_values.shape[0], min(max_length_labels) - 1), model.config.vocab_size)\n+\n+        # pad input\n+        for i in range(len(input_lengths)):\n+            input_values[i, input_lengths[i] :] = 0.0\n+            attention_mask[i, input_lengths[i] :] = 0\n+\n+        model.config.ctc_loss_reduction = \"sum\"\n+        sum_loss = model(input_values, attention_mask=attention_mask, labels=labels).loss.item()\n+\n+        model.config.ctc_loss_reduction = \"mean\"\n+        mean_loss = model(input_values, attention_mask=attention_mask, labels=labels).loss.item()\n+\n+        self.parent.assertTrue(isinstance(sum_loss, float))\n+        self.parent.assertTrue(isinstance(mean_loss, float))\n+\n+\n+@require_torch\n+class LasrEncoderModelTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (LasrEncoder,) if is_torch_available() else ()\n+\n+    test_resize_embeddings = False\n+    test_torch_exportable = True\n+\n+    def setUp(self):\n+        self.model_tester = LasrEncoderModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=LasrEncoderConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"LasrEncoder does not use inputs_embeds\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+\n+class LasrForCTCModelTester:\n+    def __init__(self, parent, encoder_kwargs=None, is_training=True, vocab_size=128, pad_token_id=0):\n+        if encoder_kwargs is None:\n+            encoder_kwargs = {}\n+\n+        self.parent = parent\n+        self.encoder_model_tester = LasrEncoderModelTester(parent, **encoder_kwargs)\n+        self.is_training = is_training\n+\n+        self.batch_size = self.encoder_model_tester.batch_size\n+        self.output_seq_length = self.encoder_model_tester.output_seq_length\n+        self.num_hidden_layers = self.encoder_model_tester.num_hidden_layers\n+        self.seq_length = vocab_size\n+        self.hidden_size = self.encoder_model_tester.hidden_size\n+\n+        self.vocab_size = vocab_size\n+        self.pad_token_id = pad_token_id\n+        self.encoder_seq_length = self.encoder_model_tester.encoder_seq_length\n+\n+    def prepare_config_and_inputs(self):\n+        _, input_features, attention_mask = self.encoder_model_tester.prepare_config_and_inputs()\n+        config = self.get_config()\n+        return config, input_features, attention_mask\n+\n+    def get_config(self):\n+        return LasrCTCConfig.from_encoder_config(\n+            encoder_config=self.encoder_model_tester.get_config(),\n+            vocab_size=self.vocab_size,\n+            pad_token_id=self.pad_token_id,\n+        )\n+\n+    def create_and_check_model(self, config, input_features, attention_mask):\n+        model = LasrForCTC(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(input_features, attention_mask=attention_mask)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.output_seq_length, self.vocab_size))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, input_features, attention_mask = self.prepare_config_and_inputs()\n+        inputs_dict = {\n+            \"input_features\": input_features,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+    def test_ctc_loss_inference(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.encoder_model_tester.check_ctc_loss(*config_and_inputs)\n+\n+\n+@require_torch\n+class LasrForCTCModelTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (LasrForCTC,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": LasrEncoder,\n+            \"automatic-speech-recognition\": LasrForCTC,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    test_attention_outputs = False\n+\n+    test_resize_embeddings = False\n+    test_torch_exportable = True\n+\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = LasrForCTCModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=LasrCTCConfig)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"LasrEncoder does not use inputs_embeds\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    # Original function assumes vision+text model, so overwrite since Lasr is audio+text\n+    # Below is modified from `tests/models/granite_speech/test_modeling_granite_speech.py`\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+\n+class LasrForCTCIntegrationTest(unittest.TestCase):\n+    _dataset = None\n+\n+    @classmethod\n+    def setUp(cls):\n+        cls.checkpoint_name = \"eustlb/lasr\"\n+        cls.dtype = torch.bfloat16\n+        cls.processor = AutoProcessor.from_pretrained(cls.checkpoint_name)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @classmethod\n+    def _load_dataset(cls):\n+        # Lazy loading of the dataset. Because it is a class method, it will only be loaded once per pytest process.\n+        if cls._dataset is None:\n+            cls._dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+            cls._dataset = cls._dataset.cast_column(\n+                \"audio\", Audio(sampling_rate=cls.processor.feature_extractor.sampling_rate)\n+            )\n+\n+    def _load_datasamples(self, num_samples):\n+        self._load_dataset()\n+        ds = self._dataset\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n+        return [x[\"array\"] for x in speech_samples]\n+\n+    @slow\n+    @unittest.skip(reason=\"TODO when checkpoint\")\n+    def test_model_integration(self):\n+        # fmt: off\n+        EXPECTED_TOKENS = torch.tensor([\n+            [0,0,0,0,0,0,0,0,0,0,0,0,315,0,0,9,0,0,4,0,382,28,0,0,0,0,31,0,0,0,57,57,0,0,7,0,0,14,0,0,0,27,0,0,0,35,0,46,0,0,0,0,16,0,0,7,0,0,192,15,0,15,15,46,0,0,54,100,5,5,0,5,5,71,0,0,0,0,0,0,0,19,19,0,0,0,150,0,142,0,0,0,106,100,100,15,15,0,0,0,18,18,0,0,50,50,121,121,30,30,279,279,0,0,0,63,63,0,0,0,0,188,0,5,5,0,0,0,27,29,0,0,0,0,0,0,0,0,9,0,0,2,2]\n+        ])\n+        # fmt: on\n+\n+        # fmt: off\n+        EXPECTED_TRANSCRIPTIONS = [\n+            \"Mr. Kuer is the apstle of the middle classes and we are glad to welcome his gospal.\"\n+        ]\n+        # fmt: on\n+\n+        samples = self._load_datasamples(1)\n+        model = LasrForCTC.from_pretrained(self.checkpoint_name, torch_dtype=self.dtype, device_map=torch_device)\n+        model.eval()\n+        model.to(torch_device)\n+\n+        # -- apply\n+        inputs = self.processor(samples)\n+        inputs.to(torch_device, dtype=self.dtype)\n+        predicted_ids = model.generate(**inputs)\n+        torch.testing.assert_close(predicted_ids.cpu(), EXPECTED_TOKENS)\n+        predicted_transcripts = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)\n+        self.assertListEqual(predicted_transcripts, EXPECTED_TRANSCRIPTIONS)\n+\n+    @slow\n+    @unittest.skip(reason=\"TODO when checkpoint\")\n+    def test_model_integration_batched(self):\n+        # fmt: off\n+        EXPECTED_TOKENS = torch.tensor([\n+            [0,0,0,0,0,0,0,0,0,0,0,0,315,0,0,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,57,0,0,7,0,0,0,0,0,0,167,0,0,0,35,0,46,0,0,0,0,16,0,0,7,0,0,192,15,0,15,15,46,0,0,54,100,5,5,0,5,5,71,71,0,0,0,0,0,0,19,19,0,0,0,150,0,142,0,0,0,106,100,100,15,15,0,0,0,0,18,0,0,50,50,121,121,30,30,279,279,0,0,0,63,63,0,0,0,0,188,0,5,5,0,0,27,0,29,0,0,0,0,0,0,0,0,0,0,0,0,9,9,0,156,156,0,229,0,90,0,13,13,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2],\n+            [0,0,0,0,0,0,0,0,0,0,0,0,0,0,117,25,25,0,0,0,57,0,0,0,0,0,315,0,0,9,9,0,0,0,382,0,0,0,0,65,0,34,34,5,0,0,0,179,0,17,17,31,0,0,0,0,0,4,0,343,0,0,0,0,0,24,24,0,0,65,65,0,228,228,0,0,22,0,0,0,0,0,304,0,0,0,0,0,63,63,0,0,0,0,0,0,0,0,113,0,8,0,65,0,0,0,0,0,0,0,0,0,0,0,0,0,9,0,0,156,156,229,229,90,90,90,13,13,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2],\n+            [0,0,0,0,0,0,0,0,0,0,0,0,0,144,0,0,0,450,450,0,5,5,0,0,294,0,0,0,0,0,0,0,0,48,0,0,0,0,0,102,102,0,0,0,149,149,0,0,0,0,0,0,91,0,35,0,0,0,198,0,0,0,0,0,136,136,11,11,5,5,56,56,0,0,0,16,16,0,0,7,0,0,0,286,286,26,26,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,64,0,0,0,0,0,0,398,68,68,35,35,21,21,11,11,5,5,0,0,19,0,0,0,4,4,74,0,86,86,0,0,0,44,49,0,10,10,39,0,0,0,0,305,0,13,21,21,22,0,0,0,0,0,0,0,360,360,0,0,0,294,294,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4,5,5,178,178,95,95,0,41,0,0,57,0,0,0,290,290,11,62,17,17,0,0,137,0,0,0,0,0,89,0,99,0,22,22,0,0,0,0,19,0,0,53,0,5,0,0,58,58,5,5,147,147,8,8,5,0,0,4,4,13,13,30,0,0,30,61,61,0,0,0,0,110,0,35,35,0,0,0,58,58,101,0,23,23,41,41,0,0,0,18,0,0,7,7,0,0,192,192,0,82,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,0,0,0,0,229,0,90,0,13,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2],\n+            [0,0,0,0,0,0,0,0,0,0,0,0,0,144,0,0,0,299,0,0,0,0,0,391,391,0,76,0,0,0,0,0,104,0,0,8,0,5,0,0,0,0,0,50,222,222,130,130,0,0,0,0,0,0,0,54,0,0,0,39,0,0,12,0,25,84,0,0,0,138,0,0,199,0,252,0,5,5,0,0,0,0,424,0,0,0,0,0,0,57,57,0,0,0,0,0,58,58,29,29,41,41,0,0,0,0,0,0,0,106,33,33,10,10,52,0,0,0,0,0,351,0,0,0,0,0,0,0,0,134,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,19,19,0,0,0,265,265,0,0,0,212,212,0,0,207,0,0,112,0,0,0,0,24,0,0,0,53,0,0,0,0,0,127,0,0,0,0,0,317,0,0,0,0,0,0,0,16,16,0,0,0,0,0,0,0,0,0,4,4,74,0,153,153,0,20,0,0,0,0,89,0,60,60,0,84,84,11,11,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,0,0,156,0,229,0,90,90,0,13,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2],\n+        ])\n+        # fmt: on\n+\n+        # fmt: off\n+        EXPECTED_TRANSCRIPTIONS = [\n+            \"Mr. is the postle of the middle classes and we are glad to welcome his gospal. [Echo\",\n+            \"nor is Mr.Kter's manner less interesting than his matter. [Echo\",\n+            \"He tells us that at thisvestive season of the year with Christmas and roseb beef looming before us similly is drawn from eating and its results occur most readily to the mind.Echo\",\n+            \"He has grav dots whether cfedric laatetens work is really greek after all and can discover in it but little of rocky ethica. [Echo\",\n+            \"Linel's pictures are sort of upgards and item paintings and maisons exquisite iteddles are as national as a Gingo palm. Mr. Bintckigible] fosters landscapes smile at one much in the same way that Mr. Carcker used to flash his teeth and Mr.J gives his sitter a cheerful slap in the back before he says like a shampoo in a turkeish bath next man\"\n+        ]\n+        # fmt: on\n+\n+        samples = self._load_datasamples(5)\n+        model = LasrForCTC.from_pretrained(\n+            self.checkpoint_name,\n+            torch_dtype=self.dtype,\n+            device_map=torch_device,\n+        )\n+        model.eval()\n+        model.to(torch_device)\n+\n+        # -- apply\n+        inputs = self.processor(samples)\n+        inputs.to(torch_device, dtype=self.dtype)\n+        predicted_ids = model.generate(**inputs)\n+        torch.testing.assert_close(predicted_ids.cpu(), EXPECTED_TOKENS)\n+        predicted_transcripts = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)\n+        self.assertListEqual(predicted_transcripts, EXPECTED_TRANSCRIPTIONS)"
        },
        {
            "sha": "f345bb9aad81bd1a495dcd3942f75271a58377b2",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -289,6 +289,9 @@ class DecoratedItem:\n     \"JukeboxTokenizer\",\n     \"LEDConfig\",\n     \"LEDTokenizerFast\",\n+    \"LasrEncoderConfig\",\n+    \"LasrFeatureExtractor\",\n+    \"LasrTokenizer\",\n     \"LayoutLMForQuestionAnswering\",\n     \"LayoutLMTokenizerFast\",\n     \"LayoutLMv2Config\",\n@@ -367,6 +370,7 @@ class DecoratedItem:\n     \"OpenLlamaConfig\",\n     \"PLBartConfig\",\n     \"ParakeetCTCConfig\",\n+    \"LasrCTCConfig\",\n     \"PegasusConfig\",\n     \"PegasusTokenizer\",\n     \"PegasusTokenizerFast\","
        },
        {
            "sha": "24313f65419d61f19d40bd29fde8574d02bbaee8",
            "filename": "utils/models_to_deprecate.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff13eb668aa03f151ded71636d723f2e490ad967/utils%2Fmodels_to_deprecate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff13eb668aa03f151ded71636d723f2e490ad967/utils%2Fmodels_to_deprecate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodels_to_deprecate.py?ref=ff13eb668aa03f151ded71636d723f2e490ad967",
            "patch": "@@ -105,6 +105,7 @@\n     \"maskformer\": [\"maskformer-swin\"],\n     \"mbart\": [\"mbart50\"],\n     \"parakeet\": [\"parakeet_ctc\", \"parakeet_encoder\"],\n+    \"lasr\": [\"lasr_ctc\", \"lasr_encoder\"],\n     \"perception_lm\": [\"perception_encoder\"],\n     \"pix2struct\": [\"deplot\", \"matcha\"],\n     \"qwen2_5_vl\": [\"qwen2_5_vl_text\"],"
        }
    ],
    "stats": {
        "total": 2682,
        "additions": 2670,
        "deletions": 12
    }
}