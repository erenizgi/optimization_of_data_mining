{
    "author": "zucchini-nlp",
    "message": "VLM: enable skipped tests (#35746)\n\n* fix cached tests\r\n\r\n* fix some tests\r\n\r\n* fix pix2struct\r\n\r\n* fix",
    "sha": "8fc6ecba4f09a738e02d8cb08736ac924f504c08",
    "files": [
        {
            "sha": "84f0356cecb2be6864824e013c9a0c3a014ec68e",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8fc6ecba4f09a738e02d8cb08736ac924f504c08/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8fc6ecba4f09a738e02d8cb08736ac924f504c08/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=8fc6ecba4f09a738e02d8cb08736ac924f504c08",
            "patch": "@@ -579,6 +579,9 @@ def _init_weights(self, module):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n             Whether to interpolate the pre-trained position encodings.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n \"\"\"\n \n BLIP2_IMAGE_TEXT_RETRIEVAL_INPUTS_DOCSTRING = r\"\"\"\n@@ -2094,6 +2097,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        use_cache: Optional[bool] = None,\n     ) -> Union[Tuple, Blip2ForConditionalGenerationModelOutput]:\n         r\"\"\"\n         Returns:\n@@ -2217,6 +2221,7 @@ def forward(\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n+                use_cache=use_cache,\n             )\n             logits = outputs.logits if return_dict else outputs[0]\n             loss = None\n@@ -2242,6 +2247,7 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=True,  # toggle for easier access to loss/logits below\n                 labels=labels,\n+                use_cache=use_cache,\n             )\n             loss = outputs.loss\n             logits = outputs.logits"
        },
        {
            "sha": "a04a27b0188c4394060eb2c33ae7b4be2ae98edf",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8fc6ecba4f09a738e02d8cb08736ac924f504c08/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8fc6ecba4f09a738e02d8cb08736ac924f504c08/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=8fc6ecba4f09a738e02d8cb08736ac924f504c08",
            "patch": "@@ -441,6 +441,9 @@ def _init_weights(self, module):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n             Whether to interpolate the pre-trained position encodings.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n \"\"\"\n \n \n@@ -1375,6 +1378,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        use_cache: Optional[bool] = None,\n     ) -> Union[Tuple, InstructBlipForConditionalGenerationModelOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1485,6 +1489,7 @@ def forward(\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n+                use_cache=use_cache,\n             )\n             logits = outputs.logits if return_dict else outputs[0]\n             loss = None\n@@ -1510,6 +1515,7 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 labels=labels,\n+                use_cache=use_cache,\n             )\n             loss = outputs.loss if return_dict else outputs[0]\n             logits = outputs.logits if return_dict else outputs[1]"
        },
        {
            "sha": "18aed639204be13e61936841397f33408d88b4aa",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8fc6ecba4f09a738e02d8cb08736ac924f504c08/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8fc6ecba4f09a738e02d8cb08736ac924f504c08/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=8fc6ecba4f09a738e02d8cb08736ac924f504c08",
            "patch": "@@ -1265,6 +1265,9 @@ def forward(\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n             Whether to interpolate the pre-trained position encodings.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n \"\"\"\n \n \n@@ -1369,6 +1372,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        use_cache: Optional[bool] = None,\n     ) -> Union[Tuple, InstructBlipVideoForConditionalGenerationModelOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1512,6 +1516,7 @@ def forward(\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n+                use_cache=use_cache,\n             )\n             logits = outputs.logits if return_dict else outputs[0]\n             loss = None\n@@ -1537,6 +1542,7 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 labels=labels,\n+                use_cache=use_cache,\n             )\n             loss = outputs.loss if return_dict else outputs[0]\n             logits = outputs.logits if return_dict else outputs[1]"
        },
        {
            "sha": "7409581358cb7ca11971f97d28cedd5baf1b7586",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8fc6ecba4f09a738e02d8cb08736ac924f504c08/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8fc6ecba4f09a738e02d8cb08736ac924f504c08/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=8fc6ecba4f09a738e02d8cb08736ac924f504c08",
            "patch": "@@ -188,6 +188,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        use_cache: Optional[bool] = None,\n     ) -> Union[Tuple, InstructBlipVideoForConditionalGenerationModelOutput]:\n         r\"\"\"\n         ```python\n@@ -322,6 +323,7 @@ def forward(\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n+                use_cache=use_cache,\n             )\n             logits = outputs.logits if return_dict else outputs[0]\n             loss = None\n@@ -347,6 +349,7 @@ def forward(\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n                 labels=labels,\n+                use_cache=use_cache,\n             )\n             loss = outputs.loss if return_dict else outputs[0]\n             logits = outputs.logits if return_dict else outputs[1]"
        },
        {
            "sha": "55277cd5a19354d00b1743b0825a1a1932943103",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8fc6ecba4f09a738e02d8cb08736ac924f504c08/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8fc6ecba4f09a738e02d8cb08736ac924f504c08/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=8fc6ecba4f09a738e02d8cb08736ac924f504c08",
            "patch": "@@ -1694,6 +1694,7 @@ def prepare_inputs_for_generation(\n         past_key_values=None,\n         attention_mask=None,\n         use_cache=None,\n+        cache_position=None,\n         **model_kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1704,17 +1705,21 @@ def prepare_inputs_for_generation(\n             attention_mask = input_ids.new_ones(input_shape)\n \n         position_ids = None\n+        if cache_position is None:\n+            past_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+            cache_position = torch.arange(past_length, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n \n-        # cut input_ids if past_key_values is used\n         if past_key_values is not None:\n             position_ids = create_position_ids_from_input_ids(\n                 input_ids,\n                 padding_idx=self.config.pad_token_id,\n                 past_key_values_length=0,\n-            )[:, -1:]\n+            )\n+\n+            if input_ids.shape[1] != cache_position.shape[0]:\n+                input_ids = input_ids[:, cache_position]\n+                position_ids = position_ids[:, -input_ids.shape[1] :]\n \n-            input_ids = input_ids[:, -1:]\n-            # the image info. is already encoded into the past keys/values\n             image_embeds = None\n             image_embeds_position_mask = None\n         elif image_embeds_position_mask is not None:"
        },
        {
            "sha": "a953221eb4dbebc3ac7ed27047eb2207d3e92a69",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 32,
            "deletions": 13,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/8fc6ecba4f09a738e02d8cb08736ac924f504c08/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8fc6ecba4f09a738e02d8cb08736ac924f504c08/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=8fc6ecba4f09a738e02d8cb08736ac924f504c08",
            "patch": "@@ -516,7 +516,7 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n             if self.has_attentions:\n                 config._attn_implementation = \"eager\"  # can't output attentions otherwise\n \n-            if not hasattr(config, \"use_cache\"):\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n             if any(model_name in model_class.__name__.lower() for model_name in [\"rwkv\"]):\n                 self.skipTest(reason=\"Won't fix: model with non-standard dictionary output shapes\")\n@@ -651,7 +651,7 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n-            if not hasattr(config, \"use_cache\"):\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n             if any(model_name in model_class.__name__.lower() for model_name in [\"rwkv\"]):\n                 self.skipTest(reason=\"Won't fix: model with non-standard dictionary output shapes\")\n@@ -989,7 +989,7 @@ def test_contrastive_generate(self):\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n             # NOTE: contrastive search only works with cache on at the moment.\n-            if not hasattr(config, \"use_cache\"):\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n             config.is_decoder = True\n \n@@ -1018,7 +1018,7 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n             # NOTE: contrastive search only works with cache on at the moment.\n-            if not hasattr(config, \"use_cache\"):\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n             config.is_decoder = True\n             if self.has_attentions:\n@@ -1060,7 +1060,7 @@ def test_contrastive_generate_low_memory(self):\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n \n             # NOTE: contrastive search only works with cache on at the moment.\n-            if not hasattr(config, \"use_cache\"):\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n \n             config.is_decoder = True\n@@ -1179,6 +1179,10 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n                     \"prophetnet\",\n                     \"seamlessm4t\",\n                     \"clvp\",\n+                    \"mllama\",  # special cache sizes\n+                    \"blip2\",  # overridden `generate()`\n+                    \"instructblip\",\n+                    \"instructblipvideo\",\n                 ]\n             ):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n@@ -1187,7 +1191,7 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n \n             # NOTE: assisted generation only works with cache on at the moment.\n-            if not hasattr(config, \"use_cache\"):\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n \n             config.is_decoder = True\n@@ -1254,6 +1258,10 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n                     \"seamlessm4t\",\n                     \"clvp\",\n                     \"fuyu\",\n+                    \"mllama\",  # special cache sizes\n+                    \"blip2\",  # overridden `generate()`\n+                    \"instructblip\",\n+                    \"instructblipvideo\",\n                 ]\n             ):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n@@ -1262,7 +1270,7 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n \n             # NOTE: assisted generation only works with cache on at the moment.\n-            if not hasattr(config, \"use_cache\"):\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n \n             config.is_decoder = True\n@@ -1368,6 +1376,10 @@ def test_assisted_decoding_sample(self):\n                     \"prophetnet\",\n                     \"seamlessm4t\",\n                     \"clvp\",\n+                    \"mllama\",  # special cache sizes\n+                    \"blip2\",  # overridden `generate()`\n+                    \"instructblip\",\n+                    \"instructblipvideo\",\n                 ]\n             ):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n@@ -1376,7 +1388,7 @@ def test_assisted_decoding_sample(self):\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n \n             # NOTE: assisted generation only works with cache on at the moment.\n-            if not hasattr(config, \"use_cache\"):\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n \n             config.is_decoder = True\n@@ -1570,7 +1582,7 @@ def test_past_key_values_format(self):\n             config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n \n             # If it doesn't support cache, pass the test\n-            if not hasattr(config, \"use_cache\"):\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n \n             model = model_class(config).to(torch_device)\n@@ -1605,7 +1617,14 @@ def test_past_key_values_format(self):\n \n             # Encoder-Decoder checks\n             if config.is_encoder_decoder:\n-                encoder_num_attention_heads = config.encoder_attention_heads\n+                # encoder-decoder models usually don't have text config\n+                # below is needed only for Pix2Struct which we cannot modify now due to BC\n+                config = config.get_text_config()\n+                encoder_num_attention_heads = (\n+                    config.encoder_attention_heads\n+                    if hasattr(config, \"encoder_attention_heads\")\n+                    else config.num_attention_heads\n+                )\n                 encoder_per_head_embed_dim = embed_dim // encoder_num_attention_heads\n                 batch_size, seq_length = inputs[\"decoder_input_ids\"].shape\n                 for i in range(num_hidden_layers):\n@@ -1804,14 +1823,14 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_generate_continue_from_past_key_values(self):\n         # Tests that we can continue generating from past key values, returned from a previous `generate` call\n         for model_class in self.all_generative_model_classes:\n-            if any(model_name in model_class.__name__.lower() for model_name in [\"imagegpt\"]):\n+            if any(model_name in model_class.__name__.lower() for model_name in [\"imagegpt\", \"mllama\"]):\n                 self.skipTest(reason=\"Won't fix: old model with unique inputs/caches/other\")\n             if any(model_name in model_class.__name__.lower() for model_name in [\"umt5\"]):\n                 self.skipTest(reason=\"TODO: needs modeling or test input preparation fixes for compatibility\")\n \n             config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n \n-            if not hasattr(config, \"use_cache\"):\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n \n             # Let's make it always:\n@@ -2251,7 +2270,7 @@ def test_assisted_decoding_with_logits_to_keep(self):\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n             # NOTE: assisted generation only works with cache on at the moment.\n-            if not hasattr(config, \"use_cache\"):\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n             config.use_cache = True\n             config.is_decoder = True"
        },
        {
            "sha": "9fb57eeec90f1aaf5087a9fa6de023e82ca3747e",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8fc6ecba4f09a738e02d8cb08736ac924f504c08/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8fc6ecba4f09a738e02d8cb08736ac924f504c08/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=8fc6ecba4f09a738e02d8cb08736ac924f504c08",
            "patch": "@@ -82,14 +82,14 @@ def __init__(\n             moe_intermediate_size=4,\n             moe_num_experts=4,\n             moe_topk=2,\n-            num_attention_heads=20,\n+            num_attention_heads=8,\n             num_experts_per_tok=3,\n             num_hidden_layers=2,\n-            num_key_value_heads=20,\n+            num_key_value_heads=8,\n             rope_theta=5000000,\n             vocab_size=99,\n             eos_token_id=2,\n-            head_dim=2,\n+            head_dim=4,\n         ),\n         is_training=True,\n         vision_config=Idefics3VisionConfig("
        },
        {
            "sha": "40541fc8277df131cb4f1beae9e91f264c221ad5",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 100,
            "deletions": 0,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/8fc6ecba4f09a738e02d8cb08736ac924f504c08/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8fc6ecba4f09a738e02d8cb08736ac924f504c08/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=8fc6ecba4f09a738e02d8cb08736ac924f504c08",
            "patch": "@@ -29,6 +29,7 @@\n     is_torch_available,\n     is_vision_available,\n )\n+from transformers.cache_utils import Cache\n from transformers.models.mllama.configuration_mllama import MllamaTextConfig\n from transformers.testing_utils import (\n     cleanup,\n@@ -378,6 +379,105 @@ def test_model_parallelism(self):\n     def test_offloaded_cache_implementation(self, cache_implementation):\n         pass\n \n+    @unittest.skip(\n+        reason=\"Mllama cache type doesn't allow correct check on output `past_key_values` due to `Cache.crop()`\"\n+    )\n+    def test_contrastive_generate_dict_outputs_use_cache(self, assistant_type):\n+        pass\n+\n+    @unittest.skip(reason=\"Mllama can't do low memory due to `Cache.crop()`\")\n+    def test_contrastive_generate_low_memory(self, assistant_type):\n+        pass\n+\n+    @unittest.skip(reason=\"Mllama can't assisted decoding due to cache format and `Cache.crop()`\")\n+    def test_assisted_decoding_with_num_logits_to_keep(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    # overriden because mllama has special cache for self and cross attentions\n+    def test_past_key_values_format(self):\n+        # Test that the KV cache is formatted correctly. Exceptions need to explicitly overwrite this test. Having a\n+        # standard KV cache format is important for a consistent API (and for advanced generation methods).\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            model = model_class(config).to(torch_device)\n+            if \"use_cache\" not in inputs:\n+                inputs[\"use_cache\"] = True\n+            outputs = model(**inputs)\n+\n+            text_config = config.get_text_config()\n+            num_hidden_layers = (\n+                getattr(text_config, \"decoder_layers\", None)\n+                or getattr(text_config, \"num_decoder_layers\", None)\n+                or text_config.num_hidden_layers\n+            )\n+            num_attention_heads = getattr(text_config, \"decoder_attention_heads\", text_config.num_attention_heads)\n+            embed_dim = getattr(text_config, \"d_model\", text_config.hidden_size)\n+            per_head_embed_dim = embed_dim // num_attention_heads\n+\n+            # some models have diffent num-head for query vs key/value so we need to assign correct value\n+            # BUT only after `per_head_embed_dim` is set\n+            num_attention_heads = (\n+                text_config.num_key_value_heads\n+                if getattr(text_config, \"num_key_value_heads\", None) is not None\n+                else num_attention_heads\n+            )\n+\n+            past_kv = outputs[\"past_key_values\"]\n+            self.assertEqual(len(past_kv), num_hidden_layers)\n+            batch_size, seq_length = inputs[\"input_ids\"].shape\n+            for i in range(num_hidden_layers):\n+                self.assertEqual(len(past_kv[0]), 2)  # K V for the decoder = 2\n+                if i in self.model_tester.text_config[\"cross_attention_layers\"]:\n+                    self.assertEqual(\n+                        past_kv[i][0].shape,\n+                        (batch_size, num_attention_heads, self.model_tester.image_length, per_head_embed_dim),\n+                    )\n+                    self.assertEqual(\n+                        past_kv[i][1].shape,\n+                        (batch_size, num_attention_heads, self.model_tester.image_length, per_head_embed_dim),\n+                    )\n+                else:\n+                    self.assertEqual(\n+                        past_kv[i][0].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n+                    )\n+                    self.assertEqual(\n+                        past_kv[i][1].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n+                    )\n+\n+    # overriden because mllama has special cache for self and cross attentions\n+    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n+        self.assertIsInstance(decoder_past_key_values, Cache)\n+        self.assertListEqual(\n+            [isinstance(iter_past_key_values, tuple) for iter_past_key_values in decoder_past_key_values],\n+            [True] * len(decoder_past_key_values),\n+        )\n+\n+        for layer_idx, layer_past_key_values in enumerate(decoder_past_key_values):\n+            if layer_idx in self.model_tester.text_config[\"cross_attention_layers\"]:\n+                expected_shape = (\n+                    batch_size,\n+                    config.num_key_value_heads\n+                    if hasattr(config, \"num_key_value_heads\")\n+                    else config.num_attention_heads,\n+                    self.model_tester.image_length,\n+                    config.hidden_size // config.num_attention_heads,\n+                )\n+            else:\n+                # (batch, head, cache_length, head_features)\n+                expected_shape = (\n+                    batch_size,\n+                    config.num_key_value_heads\n+                    if hasattr(config, \"num_key_value_heads\")\n+                    else config.num_attention_heads,\n+                    cache_length,\n+                    config.hidden_size // config.num_attention_heads,\n+                )\n+            # check shape key, value\n+            self.assertListEqual([layer_past_key_values[0].shape], [expected_shape])\n+            self.assertListEqual([layer_past_key_values[1].shape], [expected_shape])\n+\n     def test_generate_text_only_with_cache(self):\n         \"\"\"\n         Tests that our cached generation with text-only inputs works. When mllama was introduced, this feature"
        },
        {
            "sha": "f637fb9efa32b44650884cf90185c34eaca50c23",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8fc6ecba4f09a738e02d8cb08736ac924f504c08/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8fc6ecba4f09a738e02d8cb08736ac924f504c08/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=8fc6ecba4f09a738e02d8cb08736ac924f504c08",
            "patch": "@@ -612,6 +612,18 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n     def test_contrastive_generate_low_memory(self):\n         pass\n \n+    @unittest.skip(\n+        \"Moshi either needs deafult generation config or fix for fullgraph compile because it hardcodes SlidingWindowCache in custom generation loop.\"\n+    )\n+    def test_greedy_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"Moshi either needs deafult generation config or fix for fullgraph compile because it hardcodes SlidingWindowCache in custom generation loop.\"\n+    )\n+    def test_beam_search_generate_dict_outputs_use_cache(self):\n+        pass\n+\n     @unittest.skip(\"Adapting this test is costly. `test_eager_matches_sdpa_generate` tests this already.\")\n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @require_torch_sdpa"
        },
        {
            "sha": "451d4cc17a0630e5611e0b1b390dc0fbaeb06c05",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8fc6ecba4f09a738e02d8cb08736ac924f504c08/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8fc6ecba4f09a738e02d8cb08736ac924f504c08/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=8fc6ecba4f09a738e02d8cb08736ac924f504c08",
            "patch": "@@ -16,6 +16,8 @@\n \n import unittest\n \n+from parameterized import parameterized\n+\n from transformers import (\n     PaliGemmaConfig,\n     PaliGemmaForConditionalGeneration,\n@@ -348,3 +350,40 @@ def test_generate_compile_model_forward(self):\n     @unittest.skip(\"Low memory will be removed soon so no need to fix it\")\n     def test_beam_search_low_memory(self):\n         pass\n+\n+    @parameterized.expand([(\"random\",), (\"same\",)])\n+    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with assisted decoding\")\n+    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with assisted decoding\")\n+    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with assisted decoding\")\n+    def test_assisted_decoding_sample(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with dola decoding\")\n+    def test_dola_decoding_sample(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support continue from past kv\")\n+    def test_generate_continue_from_past_key_values(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support StaticCache\")\n+    def test_generate_with_static_cache(self):\n+        pass"
        }
    ],
    "stats": {
        "total": 236,
        "additions": 216,
        "deletions": 20
    }
}