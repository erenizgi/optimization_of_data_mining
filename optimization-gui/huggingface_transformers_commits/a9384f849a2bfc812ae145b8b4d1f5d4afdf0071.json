{
    "author": "gante",
    "message": "[offload] respect `max_memory` argument when factoring in unused reserved memory (#37982)",
    "sha": "a9384f849a2bfc812ae145b8b4d1f5d4afdf0071",
    "files": [
        {
            "sha": "be84fdee54b6a5eda381271d134b5e2750454ff3",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9384f849a2bfc812ae145b8b4d1f5d4afdf0071/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9384f849a2bfc812ae145b8b4d1f5d4afdf0071/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=a9384f849a2bfc812ae145b8b4d1f5d4afdf0071",
            "patch": "@@ -1275,28 +1275,31 @@ def _get_device_map(\n             )\n \n         if device_map != \"sequential\":\n-            max_memory = get_balanced_memory(\n+            inferred_max_memory = get_balanced_memory(\n                 model,\n                 dtype=target_dtype,\n                 low_zero=(device_map == \"balanced_low_0\"),\n                 max_memory=max_memory,\n                 **device_map_kwargs,\n             )\n         else:\n-            max_memory = get_max_memory(max_memory)\n+            inferred_max_memory = get_max_memory(max_memory)\n         if hf_quantizer is not None:\n-            max_memory = hf_quantizer.adjust_max_memory(max_memory)\n+            inferred_max_memory = hf_quantizer.adjust_max_memory(inferred_max_memory)\n \n-        # `max_memory` contains non-reserved memory. There may be *unused* reserved memory in the GPU, which we\n-        # can use to allocate parameters.\n-        for device_name in max_memory.keys():\n+        # `inferred_max_memory` contains non-reserved memory. There may be *unused* reserved memory in the GPU,\n+        # which we can use to allocate parameters.\n+        for device_name in inferred_max_memory.keys():\n             if isinstance(device_name, int):  # it's a GPU device\n                 if is_torch_xpu_available():\n                     unused_memory = torch.xpu.memory_reserved(device_name) - torch.xpu.memory_allocated(device_name)\n                 else:\n                     unused_memory = torch.cuda.memory_reserved(device_name) - torch.cuda.memory_allocated(device_name)\n-                max_memory[device_name] += unused_memory\n-        device_map_kwargs[\"max_memory\"] = max_memory\n+                inferred_max_memory[device_name] += unused_memory\n+            # respect the `max_memory` passed by the user\n+            if max_memory is not None and device_name in max_memory:\n+                inferred_max_memory[device_name] = min(inferred_max_memory[device_name], max_memory[device_name])\n+        device_map_kwargs[\"max_memory\"] = inferred_max_memory\n \n         device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n "
        }
    ],
    "stats": {
        "total": 19,
        "additions": 11,
        "deletions": 8
    }
}