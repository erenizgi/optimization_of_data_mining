{
    "author": "pkuderov",
    "message": "Fix `TrainingArguments.torch_empty_cache_steps` post_init check (#36734)\n\nMistaken use of De Morgan's law. Fixed \"not (X or Y)\"\nto correct \"not (X and Y)\" check to raise a ValueError.\n\nAdded corresponding test to check \"positive int or None\" condition.\n\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "c8a2b25f915a7745d57c92635415e2517b739bc8",
    "files": [
        {
            "sha": "a1f172801ad472a74c9096f2121289f0c9b4c570",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8a2b25f915a7745d57c92635415e2517b739bc8/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8a2b25f915a7745d57c92635415e2517b739bc8/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=c8a2b25f915a7745d57c92635415e2517b739bc8",
            "patch": "@@ -1641,7 +1641,7 @@ def __post_init__(self):\n             self.do_eval = True\n \n         if self.torch_empty_cache_steps is not None:\n-            if not (isinstance(self.torch_empty_cache_steps, int) or self.torch_empty_cache_steps > 0):\n+            if not (isinstance(self.torch_empty_cache_steps, int) and self.torch_empty_cache_steps > 0):\n                 raise ValueError(\n                     f\"`torch_empty_cache_steps` must be an integer bigger than 0, got {self.torch_empty_cache_steps}.\"\n                 )"
        },
        {
            "sha": "c207196abc8c8d042c854ea0bd05acb7c21d7aca",
            "filename": "tests/test_training_args.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8a2b25f915a7745d57c92635415e2517b739bc8/tests%2Ftest_training_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8a2b25f915a7745d57c92635415e2517b739bc8/tests%2Ftest_training_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_training_args.py?ref=c8a2b25f915a7745d57c92635415e2517b739bc8",
            "patch": "@@ -40,3 +40,28 @@ def test_output_dir_creation(self):\n             self.assertFalse(os.path.exists(output_dir))  # Still shouldn't exist\n \n             # Directory should be created when actually needed (e.g. in Trainer)\n+\n+    def test_torch_empty_cache_steps_requirements(self):\n+        \"\"\"Test that torch_empty_cache_steps is a positive integer or None.\"\"\"\n+\n+        # None is acceptable (feature is disabled):\n+        args = TrainingArguments(torch_empty_cache_steps=None)\n+        self.assertIsNone(args.torch_empty_cache_steps)\n+\n+        # non-int is unacceptable:\n+        with self.assertRaises(ValueError):\n+            TrainingArguments(torch_empty_cache_steps=1.0)\n+        with self.assertRaises(ValueError):\n+            TrainingArguments(torch_empty_cache_steps=\"none\")\n+\n+        # negative int is unacceptable:\n+        with self.assertRaises(ValueError):\n+            TrainingArguments(torch_empty_cache_steps=-1)\n+\n+        # zero is unacceptable:\n+        with self.assertRaises(ValueError):\n+            TrainingArguments(torch_empty_cache_steps=0)\n+\n+        # positive int is acceptable:\n+        args = TrainingArguments(torch_empty_cache_steps=1)\n+        self.assertEqual(args.torch_empty_cache_steps, 1)"
        }
    ],
    "stats": {
        "total": 27,
        "additions": 26,
        "deletions": 1
    }
}