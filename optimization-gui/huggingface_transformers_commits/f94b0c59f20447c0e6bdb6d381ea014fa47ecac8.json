{
    "author": "danieldk",
    "message": "Use `deformable_detr` kernel from the Hub (#36853)\n\n* Use `deformable_detr` kernel from the Hub\n\nRemove the `deformable_detr` kernel from `kernels/` and use the\npre-built kernel from the Hub instead.\n\n* Add license header\n\n* Add `kernels` as an extra `hub-kernels`\n\nAlso add it to `testing`, so that the kernel replacement gets tested\nwhen using CUDA in CI.",
    "sha": "f94b0c59f20447c0e6bdb6d381ea014fa47ecac8",
    "files": [
        {
            "sha": "8731f8187abb84bd19f81588650115b99230e15a",
            "filename": "setup.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=f94b0c59f20447c0e6bdb6d381ea014fa47ecac8",
            "patch": "@@ -129,6 +129,7 @@\n     # Keras pin - this is to make sure Keras 3 doesn't destroy us. Remove or change when we have proper support.\n     \"keras>2.9,<2.16\",\n     \"keras-nlp>=0.3.1,<0.14.0\",  # keras-nlp 0.14 doesn't support keras 2, see pin on keras.\n+    \"kernels>=0.3.2,<0.4\",\n     \"librosa\",\n     \"natten>=0.14.6,<0.15.0\",\n     \"nltk<=3.8.1\",\n@@ -301,8 +302,9 @@ def run(self):\n extras[\"optuna\"] = deps_list(\"optuna\")\n extras[\"ray\"] = deps_list(\"ray[tune]\")\n extras[\"sigopt\"] = deps_list(\"sigopt\")\n+extras[\"hub-kernels\"] = deps_list(\"kernels\")\n \n-extras[\"integrations\"] = extras[\"optuna\"] + extras[\"ray\"] + extras[\"sigopt\"]\n+extras[\"integrations\"] = extras[\"hub-kernels\"] + extras[\"optuna\"] + extras[\"ray\"] + extras[\"sigopt\"]\n \n extras[\"serving\"] = deps_list(\"pydantic\", \"uvicorn\", \"fastapi\", \"starlette\")\n extras[\"audio\"] = deps_list(\"librosa\", \"pyctcdecode\", \"phonemizer\", \"kenlm\")"
        },
        {
            "sha": "51682b6a706f815b2f8187659dcd2969b49f486d",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=f94b0c59f20447c0e6bdb6d381ea014fa47ecac8",
            "patch": "@@ -35,6 +35,7 @@\n     \"kenlm\": \"kenlm\",\n     \"keras\": \"keras>2.9,<2.16\",\n     \"keras-nlp\": \"keras-nlp>=0.3.1,<0.14.0\",\n+    \"kernels\": \"kernels>=0.3.2,<0.4\",\n     \"librosa\": \"librosa\",\n     \"natten\": \"natten>=0.14.6,<0.15.0\",\n     \"nltk\": \"nltk<=3.8.1\","
        },
        {
            "sha": "da8c9cd4c6e4a8608042ec2f8f28f82a674a8c84",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=f94b0c59f20447c0e6bdb6d381ea014fa47ecac8",
            "patch": "@@ -70,6 +70,12 @@\n         \"replace_with_higgs_linear\",\n     ],\n     \"hqq\": [\"prepare_for_hqq_linear\"],\n+    \"hub_kernels\": [\n+        \"LayerRepository\",\n+        \"register_kernel_mapping\",\n+        \"replace_kernel_forward_from_hub\",\n+        \"use_kernel_forward_from_hub\",\n+    ],\n     \"integration_utils\": [\n         \"INTEGRATION_TO_CALLBACK\",\n         \"AzureMLCallback\",\n@@ -198,6 +204,12 @@\n     )\n     from .higgs import HiggsLinear, dequantize_higgs, quantize_with_higgs, replace_with_higgs_linear\n     from .hqq import prepare_for_hqq_linear\n+    from .hub_kernels import (\n+        LayerRepository,\n+        register_kernel_mapping,\n+        replace_kernel_forward_from_hub,\n+        use_kernel_forward_from_hub,\n+    )\n     from .integration_utils import (\n         INTEGRATION_TO_CALLBACK,\n         AzureMLCallback,"
        },
        {
            "sha": "b2ec6b53715aca2068db6e18ac4753d2720b9b09",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "added",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=f94b0c59f20447c0e6bdb6d381ea014fa47ecac8",
            "patch": "@@ -0,0 +1,73 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Dict, Union\n+\n+\n+try:\n+    from kernels import (\n+        Device,\n+        LayerRepository,\n+        register_kernel_mapping,\n+        replace_kernel_forward_from_hub,\n+        use_kernel_forward_from_hub,\n+    )\n+\n+    _hub_kernels_available = True\n+\n+    _KERNEL_MAPPING: Dict[str, Dict[Union[Device, str], LayerRepository]] = {\n+        \"MultiScaleDeformableAttention\": {\n+            \"cuda\": LayerRepository(\n+                repo_id=\"kernels-community/deformable-detr\",\n+                layer_name=\"MultiScaleDeformableAttention\",\n+            )\n+        }\n+    }\n+\n+    register_kernel_mapping(_KERNEL_MAPPING)\n+\n+except ImportError:\n+    # Stub to make decorators int transformers work when `kernels`\n+    # is not installed.\n+    def use_kernel_forward_from_hub(*args, **kwargs):\n+        def decorator(cls):\n+            return cls\n+\n+        return decorator\n+\n+    class LayerRepository:\n+        def __init__(self, *args, **kwargs):\n+            raise RuntimeError(\"LayerRepository requires `kernels` to be installed. Run `pip install kernels`.\")\n+\n+    def replace_kernel_forward_from_hub(*args, **kwargs):\n+        raise RuntimeError(\n+            \"replace_kernel_forward_from_hub requires `kernels` to be installed. Run `pip install kernels`.\"\n+        )\n+\n+    def register_kernel_mapping(*args, **kwargs):\n+        raise RuntimeError(\"register_kernel_mapping requires `kernels` to be installed. Run `pip install kernels`.\")\n+\n+    _hub_kernels_available = False\n+\n+\n+def is_hub_kernels_available():\n+    return _hub_kernels_available\n+\n+\n+__all__ = [\n+    \"LayerRepository\",\n+    \"is_hub_kernels_available\",\n+    \"use_kernel_forward_from_hub\",\n+    \"register_kernel_mapping\",\n+    \"replace_kernel_forward_from_hub\",\n+]"
        },
        {
            "sha": "388a73d22d4c9b561e2a887b50a1897b8cf2def9",
            "filename": "src/transformers/kernels/deformable_detr/cpu/ms_deform_attn_cpu.cpp",
            "status": "removed",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcpu%2Fms_deform_attn_cpu.cpp",
            "raw_url": "https://github.com/huggingface/transformers/raw/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcpu%2Fms_deform_attn_cpu.cpp",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcpu%2Fms_deform_attn_cpu.cpp?ref=2638d54e7851f1323dc78a8b513b041835aba27b",
            "patch": "@@ -1,40 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-\n-at::Tensor\n-ms_deform_attn_cpu_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ERROR(\"Not implement on cpu\");\n-}\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_cpu_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-    AT_ERROR(\"Not implement on cpu\");\n-}"
        },
        {
            "sha": "7eac8c8bcd1bf529bb9c13d54d2d4215c9e4c89f",
            "filename": "src/transformers/kernels/deformable_detr/cpu/ms_deform_attn_cpu.h",
            "status": "removed",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcpu%2Fms_deform_attn_cpu.h",
            "raw_url": "https://github.com/huggingface/transformers/raw/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcpu%2Fms_deform_attn_cpu.h",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcpu%2Fms_deform_attn_cpu.h?ref=2638d54e7851f1323dc78a8b513b041835aba27b",
            "patch": "@@ -1,32 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-at::Tensor\n-ms_deform_attn_cpu_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step);\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_cpu_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step);\n-"
        },
        {
            "sha": "c2b3a462a1a06338d4a04c33a6867fcad42248e6",
            "filename": "src/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu",
            "status": "removed",
            "additions": 0,
            "deletions": 159,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcuda%2Fms_deform_attn_cuda.cu",
            "raw_url": "https://github.com/huggingface/transformers/raw/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcuda%2Fms_deform_attn_cuda.cu",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcuda%2Fms_deform_attn_cuda.cu?ref=2638d54e7851f1323dc78a8b513b041835aba27b",
            "patch": "@@ -1,159 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-#include \"cuda/ms_deform_im2col_cuda.cuh\"\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-#include <cuda.h>\n-#include <cuda_runtime.h>\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    at::DeviceGuard guard(value.device());\n-\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.is_cuda(), \"attn_weight must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-    \n-    auto output = at::zeros({batch, num_query, num_heads, channels}, value.options());\n-\n-    const int batch_n = im2col_step_;\n-    auto output_n = output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto columns = output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n-            ms_deformable_im2col_cuda(at::cuda::getCurrentCUDAStream(),\n-                value.data_ptr<scalar_t>() + n * im2col_step_ * per_value_size,\n-                spatial_shapes.data_ptr<int64_t>(),\n-                level_start_index.data_ptr<int64_t>(),\n-                sampling_loc.data_ptr<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                attn_weight.data_ptr<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                columns.data_ptr<scalar_t>());\n-\n-        }));\n-    }\n-\n-    output = output.view({batch, num_query, num_heads*channels});\n-\n-    return output;\n-}\n-\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-    at::DeviceGuard guard(value.device());\n-\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-    AT_ASSERTM(grad_output.is_contiguous(), \"grad_output tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.is_cuda(), \"attn_weight must be a CUDA tensor\");\n-    AT_ASSERTM(grad_output.is_cuda(), \"grad_output must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-\n-    auto grad_value = at::zeros_like(value);\n-    auto grad_sampling_loc = at::zeros_like(sampling_loc);\n-    auto grad_attn_weight = at::zeros_like(attn_weight);\n-\n-    const int batch_n = im2col_step_;\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    auto grad_output_n = grad_output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    \n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto grad_output_g = grad_output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n-            ms_deformable_col2im_cuda(at::cuda::getCurrentCUDAStream(),\n-                                    grad_output_g.data_ptr<scalar_t>(),\n-                                    value.data_ptr<scalar_t>() + n * im2col_step_ * per_value_size,\n-                                    spatial_shapes.data_ptr<int64_t>(),\n-                                    level_start_index.data_ptr<int64_t>(),\n-                                    sampling_loc.data_ptr<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    attn_weight.data_ptr<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                                    batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                                    grad_value.data_ptr<scalar_t>() +  n * im2col_step_ * per_value_size,\n-                                    grad_sampling_loc.data_ptr<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    grad_attn_weight.data_ptr<scalar_t>() + n * im2col_step_ * per_attn_weight_size);\n-\n-        }));\n-    }\n-\n-    return {\n-        grad_value, grad_sampling_loc, grad_attn_weight\n-    };\n-}"
        },
        {
            "sha": "20ae6892e4b9881578a72aae27ddc4ec9f68ae1c",
            "filename": "src/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cuh",
            "status": "removed",
            "additions": 0,
            "deletions": 1467,
            "changes": 1467,
            "blob_url": "https://github.com/huggingface/transformers/blob/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcuda%2Fms_deform_attn_cuda.cuh",
            "raw_url": "https://github.com/huggingface/transformers/raw/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcuda%2Fms_deform_attn_cuda.cuh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcuda%2Fms_deform_attn_cuda.cuh?ref=2638d54e7851f1323dc78a8b513b041835aba27b",
            "patch": "@@ -1,1467 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-\n-#include <cuda.h>\n-#include <cuda_runtime.h>\n-\n-#include <cstdio>\n-#include <algorithm>\n-#include <cstring>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-#include <THC/THCAtomics.cuh>\n-\n-#define CUDA_KERNEL_LOOP(i, n)                          \\\n-  for (int i = blockIdx.x * blockDim.x + threadIdx.x;   \\\n-      i < (n);                                          \\\n-      i += blockDim.x * gridDim.x)\n-\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.is_cuda(), \"attn_weight must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-    \n-    auto output = at::zeros({batch, num_query, num_heads, channels}, value.options());\n-\n-    const int batch_n = im2col_step_;\n-    auto output_n = output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto columns = output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n-            ms_deformable_im2col_cuda(at::cuda::getCurrentCUDAStream(),\n-                value.data_ptr<scalar_t>() + n * im2col_step_ * per_value_size,\n-                spatial_shapes.data_ptr<int64_t>(),\n-                level_start_index.data_ptr<int64_t>(),\n-                sampling_loc.data_ptr<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                attn_weight.data_ptr<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                columns.data_ptr<scalar_t>());\n-\n-        }));\n-    }\n-\n-    output = output.view({batch, num_query, num_heads*channels});\n-\n-    return output;\n-}\n-\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-    AT_ASSERTM(grad_output.is_contiguous(), \"grad_output tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.is_cuda(), \"attn_weight must be a CUDA tensor\");\n-    AT_ASSERTM(grad_output.is_cuda(), \"grad_output must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-\n-    auto grad_value = at::zeros_like(value);\n-    auto grad_sampling_loc = at::zeros_like(sampling_loc);\n-    auto grad_attn_weight = at::zeros_like(attn_weight);\n-\n-    const int batch_n = im2col_step_;\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    auto grad_output_n = grad_output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    \n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto grad_output_g = grad_output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n-            ms_deformable_col2im_cuda(at::cuda::getCurrentCUDAStream(),\n-                                    grad_output_g.data_ptr<scalar_t>(),\n-                                    value.data_ptr<scalar_t>() + n * im2col_step_ * per_value_size,\n-                                    spatial_shapes.data_ptr<int64_t>(),\n-                                    level_start_index.data_ptr<int64_t>(),\n-                                    sampling_loc.data_ptr<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    attn_weight.data_ptr<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                                    batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                                    grad_value.data_ptr<scalar_t>() +  n * im2col_step_ * per_value_size,\n-                                    grad_sampling_loc.data_ptr<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    grad_attn_weight.data_ptr<scalar_t>() + n * im2col_step_ * per_attn_weight_size);\n-\n-        }));\n-    }\n-\n-    return {\n-        grad_value, grad_sampling_loc, grad_attn_weight\n-    };\n-}\n-\n-const int CUDA_NUM_THREADS = 1024;\n-inline int GET_BLOCKS(const int N, const int num_threads)\n-{\n-  return (N + num_threads - 1) / num_threads;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ scalar_t ms_deform_attn_im2col_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-  }\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  return val;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  *grad_attn_weight = top_grad * val;\n-  *grad_sampling_loc = width * grad_w_weight * top_grad_value;\n-  *(grad_sampling_loc + 1) = height * grad_h_weight * top_grad_value;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear_gm(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  atomicAdd(grad_attn_weight, top_grad * val); \n-  atomicAdd(grad_sampling_loc, width * grad_w_weight * top_grad_value);\n-  atomicAdd(grad_sampling_loc + 1, height * grad_h_weight * top_grad_value);\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_im2col_gpu_kernel(const int n,\n-                                                const scalar_t *data_value, \n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *data_col)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    scalar_t *data_col_ptr = data_col + index;\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-    scalar_t col = 0;\n-    \n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const scalar_t *data_value_ptr = data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          col += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col) * weight;\n-        }\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-      }\n-    }\n-    *data_col_ptr = col;\n-  }\n-}\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockSize; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockSize/2; s>0; s>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        { \n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockDim.x; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            } \n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            }\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          atomicAdd(grad_sampling_loc, cache_grad_sampling_loc[0]);\n-          atomicAdd(grad_sampling_loc + 1, cache_grad_sampling_loc[1]);\n-          atomicAdd(grad_attn_weight, cache_grad_attn_weight[0]);\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_gm(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear_gm(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            grad_sampling_loc, grad_attn_weight);\n-        }\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-void ms_deformable_im2col_cuda(cudaStream_t stream,\n-                              const scalar_t* data_value,\n-                              const int64_t* data_spatial_shapes, \n-                              const int64_t* data_level_start_index, \n-                              const scalar_t* data_sampling_loc,\n-                              const scalar_t* data_attn_weight,\n-                              const int batch_size,\n-                              const int spatial_size, \n-                              const int num_heads, \n-                              const int channels, \n-                              const int num_levels, \n-                              const int num_query,\n-                              const int num_point,\n-                              scalar_t* data_col)\n-{\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_threads = CUDA_NUM_THREADS;\n-  ms_deformable_im2col_gpu_kernel<scalar_t>\n-      <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-          0, stream>>>(\n-      num_kernels, data_value, data_spatial_shapes, data_level_start_index, data_sampling_loc, data_attn_weight, \n-      batch_size, spatial_size, num_heads, channels, num_levels, num_query, num_point, data_col);\n-  \n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_im2col_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}\n-\n-template <typename scalar_t>\n-void ms_deformable_col2im_cuda(cudaStream_t stream,\n-                              const scalar_t* grad_col,\n-                              const scalar_t* data_value,\n-                              const int64_t * data_spatial_shapes,\n-                              const int64_t * data_level_start_index,\n-                              const scalar_t * data_sampling_loc,\n-                              const scalar_t * data_attn_weight,\n-                              const int batch_size, \n-                              const int spatial_size, \n-                              const int num_heads,\n-                              const int channels, \n-                              const int num_levels,\n-                              const int num_query,\n-                              const int num_point, \n-                              scalar_t* grad_value,\n-                              scalar_t* grad_sampling_loc,\n-                              scalar_t* grad_attn_weight)\n-{\n-  const int num_threads = (channels > CUDA_NUM_THREADS)?CUDA_NUM_THREADS:channels;\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  if (channels > 1024)\n-  {\n-    if ((channels & 1023) == 0)\n-    {\n-      ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-    }\n-    else\n-    {\n-      ms_deformable_col2im_gpu_kernel_gm<scalar_t>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-    }\n-  }\n-  else{\n-    switch(channels)\n-    {\n-      case 1:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 1>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 2:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 2>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 4:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 4>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 8:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 8>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 16:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 16>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 32:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 32>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 64:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 64>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 128:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 128>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 256:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 256>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 512:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 512>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 1024:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 1024>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      default:\n-        if (channels < 64)\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v1<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-        else\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v2<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-    }\n-  }\n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_col2im_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}"
        },
        {
            "sha": "d8c21b4e54dcd7071f5fd27f62410326ac069a94",
            "filename": "src/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.h",
            "status": "removed",
            "additions": 0,
            "deletions": 46,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcuda%2Fms_deform_attn_cuda.h",
            "raw_url": "https://github.com/huggingface/transformers/raw/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcuda%2Fms_deform_attn_cuda.h",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcuda%2Fms_deform_attn_cuda.h?ref=2638d54e7851f1323dc78a8b513b041835aba27b",
            "patch": "@@ -1,46 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step);\n-\n-at::Tensor ms_deform_attn_cuda_forward_bf16(\n-    const at::Tensor &value,\n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step);\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step);\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward_bf16(\n-    const at::Tensor &value,\n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step);"
        },
        {
            "sha": "4fb544bf791ddae79238924df591b7a33f3cccdd",
            "filename": "src/transformers/kernels/deformable_detr/cuda/ms_deform_im2col_cuda.cuh",
            "status": "removed",
            "additions": 0,
            "deletions": 1327,
            "changes": 1327,
            "blob_url": "https://github.com/huggingface/transformers/blob/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcuda%2Fms_deform_im2col_cuda.cuh",
            "raw_url": "https://github.com/huggingface/transformers/raw/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcuda%2Fms_deform_im2col_cuda.cuh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fcuda%2Fms_deform_im2col_cuda.cuh?ref=2638d54e7851f1323dc78a8b513b041835aba27b",
            "patch": "@@ -1,1327 +0,0 @@\n-/*!\n-**************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************\n-* Modified from DCN (https://github.com/msracver/Deformable-ConvNets)\n-* Copyright (c) 2018 Microsoft\n-**************************************************************************\n-*/\n-\n-#include <cstdio>\n-#include <algorithm>\n-#include <cstring>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-#include <THC/THCAtomics.cuh>\n-\n-#define CUDA_KERNEL_LOOP(i, n)                          \\\n-  for (int i = blockIdx.x * blockDim.x + threadIdx.x;   \\\n-      i < (n);                                          \\\n-      i += blockDim.x * gridDim.x)\n-\n-const int CUDA_NUM_THREADS = 1024;\n-inline int GET_BLOCKS(const int N, const int num_threads)\n-{\n-  return (N + num_threads - 1) / num_threads;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ scalar_t ms_deform_attn_im2col_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-  }\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  return val;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  *grad_attn_weight = top_grad * val;\n-  *grad_sampling_loc = width * grad_w_weight * top_grad_value;\n-  *(grad_sampling_loc + 1) = height * grad_h_weight * top_grad_value;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear_gm(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  atomicAdd(grad_attn_weight, top_grad * val); \n-  atomicAdd(grad_sampling_loc, width * grad_w_weight * top_grad_value);\n-  atomicAdd(grad_sampling_loc + 1, height * grad_h_weight * top_grad_value);\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_im2col_gpu_kernel(const int n,\n-                                                const scalar_t *data_value, \n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *data_col)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    scalar_t *data_col_ptr = data_col + index;\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-    scalar_t col = 0;\n-    \n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const scalar_t *data_value_ptr = data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          col += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col) * weight;\n-        }\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-      }\n-    }\n-    *data_col_ptr = col;\n-  }\n-}\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockSize; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockSize/2; s>0; s>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        { \n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockDim.x; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            } \n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            }\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          atomicAdd(grad_sampling_loc, cache_grad_sampling_loc[0]);\n-          atomicAdd(grad_sampling_loc + 1, cache_grad_sampling_loc[1]);\n-          atomicAdd(grad_attn_weight, cache_grad_attn_weight[0]);\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_gm(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    [[maybe_unused]] const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear_gm(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            grad_sampling_loc, grad_attn_weight);\n-        }\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-void ms_deformable_im2col_cuda(cudaStream_t stream,\n-                              const scalar_t* data_value,\n-                              const int64_t* data_spatial_shapes, \n-                              const int64_t* data_level_start_index, \n-                              const scalar_t* data_sampling_loc,\n-                              const scalar_t* data_attn_weight,\n-                              const int batch_size,\n-                              const int spatial_size, \n-                              const int num_heads, \n-                              const int channels, \n-                              const int num_levels, \n-                              const int num_query,\n-                              const int num_point,\n-                              scalar_t* data_col)\n-{\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_threads = CUDA_NUM_THREADS;\n-  ms_deformable_im2col_gpu_kernel<scalar_t>\n-      <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-          0, stream>>>(\n-      num_kernels, data_value, data_spatial_shapes, data_level_start_index, data_sampling_loc, data_attn_weight, \n-      batch_size, spatial_size, num_heads, channels, num_levels, num_query, num_point, data_col);\n-  \n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_im2col_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}\n-\n-template <typename scalar_t>\n-void ms_deformable_col2im_cuda(cudaStream_t stream,\n-                              const scalar_t* grad_col,\n-                              const scalar_t* data_value,\n-                              const int64_t * data_spatial_shapes,\n-                              const int64_t * data_level_start_index,\n-                              const scalar_t * data_sampling_loc,\n-                              const scalar_t * data_attn_weight,\n-                              const int batch_size, \n-                              const int spatial_size, \n-                              const int num_heads,\n-                              const int channels, \n-                              const int num_levels,\n-                              const int num_query,\n-                              const int num_point, \n-                              scalar_t* grad_value,\n-                              scalar_t* grad_sampling_loc,\n-                              scalar_t* grad_attn_weight)\n-{\n-  const int num_threads = (channels > CUDA_NUM_THREADS)?CUDA_NUM_THREADS:channels;\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  if (channels > 1024)\n-  {\n-    if ((channels & 1023) == 0)\n-    {\n-      ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-    }\n-    else\n-    {\n-      ms_deformable_col2im_gpu_kernel_gm<scalar_t>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-    }\n-  }\n-  else{\n-    switch(channels)\n-    {\n-      case 1:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 1>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 2:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 2>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 4:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 4>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 8:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 8>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 16:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 16>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 32:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 32>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 64:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 64>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 128:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 128>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 256:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 256>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 512:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 512>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 1024:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 1024>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      default:\n-        if (channels < 64)\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v1<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-        else\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v2<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-    }\n-  }\n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_col2im_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}"
        },
        {
            "sha": "e649e65e37ae58d789a4cfb396e715ff2e33477f",
            "filename": "src/transformers/kernels/deformable_detr/ms_deform_attn.h",
            "status": "removed",
            "additions": 0,
            "deletions": 61,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fms_deform_attn.h",
            "raw_url": "https://github.com/huggingface/transformers/raw/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fms_deform_attn.h",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fms_deform_attn.h?ref=2638d54e7851f1323dc78a8b513b041835aba27b",
            "patch": "@@ -1,61 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-\n-#include \"cpu/ms_deform_attn_cpu.h\"\n-\n-#ifdef WITH_CUDA\n-#include \"cuda/ms_deform_attn_cuda.h\"\n-#endif\n-\n-\n-at::Tensor\n-ms_deform_attn_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    if (value.is_cuda())\n-    {\n-#ifdef WITH_CUDA\n-        return ms_deform_attn_cuda_forward(\n-            value, spatial_shapes, level_start_index, sampling_loc, attn_weight, im2col_step);\n-#else\n-        AT_ERROR(\"Not compiled with GPU support\");\n-#endif\n-    }\n-    AT_ERROR(\"Not implemented on the CPU\");\n-}\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-    if (value.is_cuda())\n-    {\n-#ifdef WITH_CUDA\n-        return ms_deform_attn_cuda_backward(\n-            value, spatial_shapes, level_start_index, sampling_loc, attn_weight, grad_output, im2col_step);\n-#else\n-        AT_ERROR(\"Not compiled with GPU support\");\n-#endif\n-    }\n-    AT_ERROR(\"Not implemented on the CPU\");\n-}"
        },
        {
            "sha": "6ce3875568b9ba8d660c90acc805077cca98f891",
            "filename": "src/transformers/kernels/deformable_detr/vision.cpp",
            "status": "removed",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fvision.cpp",
            "raw_url": "https://github.com/huggingface/transformers/raw/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fvision.cpp",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fdeformable_detr%2Fvision.cpp?ref=2638d54e7851f1323dc78a8b513b041835aba27b",
            "patch": "@@ -1,16 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include \"ms_deform_attn.h\"\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-  m.def(\"ms_deform_attn_forward\", &ms_deform_attn_forward, \"ms_deform_attn_forward\");\n-  m.def(\"ms_deform_attn_backward\", &ms_deform_attn_backward, \"ms_deform_attn_backward\");\n-}\n\\ No newline at end of file"
        },
        {
            "sha": "3c0b3a432be127d1ecfe0f0060b5905d32e320e0",
            "filename": "src/transformers/models/deformable_detr/load_custom.py",
            "status": "removed",
            "additions": 0,
            "deletions": 50,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fload_custom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2638d54e7851f1323dc78a8b513b041835aba27b/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fload_custom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fload_custom.py?ref=2638d54e7851f1323dc78a8b513b041835aba27b",
            "patch": "@@ -1,50 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Loading of Deformable DETR's CUDA kernels\"\"\"\n-\n-import os\n-from pathlib import Path\n-\n-\n-def load_cuda_kernels():\n-    from torch.utils.cpp_extension import load\n-\n-    root = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"deformable_detr\"\n-    src_files = [\n-        root / filename\n-        for filename in [\n-            \"vision.cpp\",\n-            os.path.join(\"cpu\", \"ms_deform_attn_cpu.cpp\"),\n-            os.path.join(\"cuda\", \"ms_deform_attn_cuda.cu\"),\n-        ]\n-    ]\n-\n-    load(\n-        \"MultiScaleDeformableAttention\",\n-        src_files,\n-        with_cuda=True,\n-        extra_include_paths=[str(root)],\n-        extra_cflags=[\"-DWITH_CUDA=1\"],\n-        extra_cuda_cflags=[\n-            \"-DCUDA_HAS_FP16=1\",\n-            \"-D__CUDA_NO_HALF_OPERATORS__\",\n-            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n-            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n-        ],\n-    )\n-\n-    import MultiScaleDeformableAttention as MSDA\n-\n-    return MSDA"
        },
        {
            "sha": "6ffebca32dcfe8cefa12229f3194d41a05f3adfa",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 117,
            "deletions": 161,
            "changes": 278,
            "blob_url": "https://github.com/huggingface/transformers/blob/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=f94b0c59f20447c0e6bdb6d381ea014fa47ecac8",
            "patch": "@@ -16,19 +16,16 @@\n \n import copy\n import math\n-import os\n import warnings\n from dataclasses import dataclass\n-from pathlib import Path\n from typing import Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn.functional as F\n from torch import Tensor, nn\n-from torch.autograd import Function\n-from torch.autograd.function import once_differentiable\n \n from ...activations import ACT2FN\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n@@ -37,10 +34,7 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_ninja_available,\n     is_timm_available,\n-    is_torch_cuda_available,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n     requires_backends,\n@@ -51,38 +45,6 @@\n \n logger = logging.get_logger(__name__)\n \n-MultiScaleDeformableAttention = None\n-\n-\n-def load_cuda_kernels():\n-    from torch.utils.cpp_extension import load\n-\n-    global MultiScaleDeformableAttention\n-\n-    root = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"deformable_detr\"\n-    src_files = [\n-        root / filename\n-        for filename in [\n-            \"vision.cpp\",\n-            os.path.join(\"cpu\", \"ms_deform_attn_cpu.cpp\"),\n-            os.path.join(\"cuda\", \"ms_deform_attn_cuda.cu\"),\n-        ]\n-    ]\n-\n-    MultiScaleDeformableAttention = load(\n-        \"MultiScaleDeformableAttention\",\n-        src_files,\n-        with_cuda=True,\n-        extra_include_paths=[str(root)],\n-        extra_cflags=[\"-DWITH_CUDA=1\"],\n-        extra_cuda_cflags=[\n-            \"-DCUDA_HAS_FP16=1\",\n-            \"-D__CUDA_NO_HALF_OPERATORS__\",\n-            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n-            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n-        ],\n-    )\n-\n \n if is_timm_available():\n     from timm import create_model\n@@ -94,52 +56,59 @@ def load_cuda_kernels():\n _CHECKPOINT_FOR_DOC = \"sensetime/deformable-detr\"\n \n \n-class MultiScaleDeformableAttentionFunction(Function):\n-    @staticmethod\n+@use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n+class MultiScaleDeformableAttention(nn.Module):\n     def forward(\n-        context,\n-        value,\n-        value_spatial_shapes,\n-        value_level_start_index,\n-        sampling_locations,\n-        attention_weights,\n-        im2col_step,\n+        self,\n+        value: Tensor,\n+        value_spatial_shapes: Tensor,\n+        value_spatial_shapes_list: List[Tuple],\n+        level_start_index: Tensor,\n+        sampling_locations: Tensor,\n+        attention_weights: Tensor,\n+        im2col_step: int,\n     ):\n-        context.im2col_step = im2col_step\n-        output = MultiScaleDeformableAttention.ms_deform_attn_forward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            context.im2col_step,\n-        )\n-        context.save_for_backward(\n-            value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights\n+        batch_size, _, num_heads, hidden_dim = value.shape\n+        _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n+        value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+        sampling_grids = 2 * sampling_locations - 1\n+        sampling_value_list = []\n+        for level_id, (height, width) in enumerate(value_spatial_shapes):\n+            # batch_size, height*width, num_heads, hidden_dim\n+            # -> batch_size, height*width, num_heads*hidden_dim\n+            # -> batch_size, num_heads*hidden_dim, height*width\n+            # -> batch_size*num_heads, hidden_dim, height, width\n+            value_l_ = (\n+                value_list[level_id]\n+                .flatten(2)\n+                .transpose(1, 2)\n+                .reshape(batch_size * num_heads, hidden_dim, height, width)\n+            )\n+            # batch_size, num_queries, num_heads, num_points, 2\n+            # -> batch_size, num_heads, num_queries, num_points, 2\n+            # -> batch_size*num_heads, num_queries, num_points, 2\n+            sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n+            # batch_size*num_heads, hidden_dim, num_queries, num_points\n+            sampling_value_l_ = nn.functional.grid_sample(\n+                value_l_,\n+                sampling_grid_l_,\n+                mode=\"bilinear\",\n+                padding_mode=\"zeros\",\n+                align_corners=False,\n+            )\n+            sampling_value_list.append(sampling_value_l_)\n+        # (batch_size, num_queries, num_heads, num_levels, num_points)\n+        # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n+        # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n+        attention_weights = attention_weights.transpose(1, 2).reshape(\n+            batch_size * num_heads, 1, num_queries, num_levels * num_points\n         )\n-        return output\n-\n-    @staticmethod\n-    @once_differentiable\n-    def backward(context, grad_output):\n-        (\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-        ) = context.saved_tensors\n-        grad_value, grad_sampling_loc, grad_attn_weight = MultiScaleDeformableAttention.ms_deform_attn_backward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            grad_output,\n-            context.im2col_step,\n+        output = (\n+            (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n+            .sum(-1)\n+            .view(batch_size, num_heads * hidden_dim, num_queries)\n         )\n-\n-        return grad_value, None, None, grad_sampling_loc, grad_attn_weight, None\n+        return output.transpose(1, 2).contiguous()\n \n \n @dataclass\n@@ -564,48 +533,6 @@ def build_position_encoding(config):\n     return position_embedding\n \n \n-def multi_scale_deformable_attention(\n-    value: Tensor,\n-    value_spatial_shapes: Union[Tensor, List[Tuple]],\n-    sampling_locations: Tensor,\n-    attention_weights: Tensor,\n-) -> Tensor:\n-    batch_size, _, num_heads, hidden_dim = value.shape\n-    _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-    value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n-    sampling_grids = 2 * sampling_locations - 1\n-    sampling_value_list = []\n-    for level_id, (height, width) in enumerate(value_spatial_shapes):\n-        # batch_size, height*width, num_heads, hidden_dim\n-        # -> batch_size, height*width, num_heads*hidden_dim\n-        # -> batch_size, num_heads*hidden_dim, height*width\n-        # -> batch_size*num_heads, hidden_dim, height, width\n-        value_l_ = (\n-            value_list[level_id].flatten(2).transpose(1, 2).reshape(batch_size * num_heads, hidden_dim, height, width)\n-        )\n-        # batch_size, num_queries, num_heads, num_points, 2\n-        # -> batch_size, num_heads, num_queries, num_points, 2\n-        # -> batch_size*num_heads, num_queries, num_points, 2\n-        sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n-        # batch_size*num_heads, hidden_dim, num_queries, num_points\n-        sampling_value_l_ = nn.functional.grid_sample(\n-            value_l_, sampling_grid_l_, mode=\"bilinear\", padding_mode=\"zeros\", align_corners=False\n-        )\n-        sampling_value_list.append(sampling_value_l_)\n-    # (batch_size, num_queries, num_heads, num_levels, num_points)\n-    # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n-    # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n-    attention_weights = attention_weights.transpose(1, 2).reshape(\n-        batch_size * num_heads, 1, num_queries, num_levels * num_points\n-    )\n-    output = (\n-        (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n-        .sum(-1)\n-        .view(batch_size, num_heads * hidden_dim, num_queries)\n-    )\n-    return output.transpose(1, 2).contiguous()\n-\n-\n class DeformableDetrMultiscaleDeformableAttention(nn.Module):\n     \"\"\"\n     Multiscale deformable attention as proposed in Deformable DETR.\n@@ -614,12 +541,7 @@ class DeformableDetrMultiscaleDeformableAttention(nn.Module):\n     def __init__(self, config: DeformableDetrConfig, num_heads: int, n_points: int):\n         super().__init__()\n \n-        kernel_loaded = MultiScaleDeformableAttention is not None\n-        if is_torch_cuda_available() and is_ninja_available() and not kernel_loaded:\n-            try:\n-                load_cuda_kernels()\n-            except Exception as e:\n-                logger.warning(f\"Could not load the custom kernel for multi-scale deformable attention: {e}\")\n+        self.attn = MultiScaleDeformableAttention()\n \n         if config.d_model % num_heads != 0:\n             raise ValueError(\n@@ -706,27 +628,16 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        if self.disable_custom_kernels or MultiScaleDeformableAttention is None or is_torchdynamo_compiling():\n-            # PyTorch implementation\n-            output = multi_scale_deformable_attention(\n-                value, spatial_shapes_list, sampling_locations, attention_weights\n-            )\n-        else:\n-            try:\n-                # custom kernel\n-                output = MultiScaleDeformableAttentionFunction.apply(\n-                    value,\n-                    spatial_shapes,\n-                    level_start_index,\n-                    sampling_locations,\n-                    attention_weights,\n-                    self.im2col_step,\n-                )\n-            except Exception:\n-                # PyTorch implementation\n-                output = multi_scale_deformable_attention(\n-                    value, spatial_shapes_list, sampling_locations, attention_weights\n-                )\n+        output = self.attn(\n+            value,\n+            spatial_shapes,\n+            spatial_shapes_list,\n+            level_start_index,\n+            sampling_locations,\n+            attention_weights,\n+            self.im2col_step,\n+        )\n+\n         output = self.output_proj(output)\n \n         return output, attention_weights\n@@ -834,7 +745,11 @@ def forward(\n \n         attn_output = torch.bmm(attn_probs, value_states)\n \n-        if attn_output.size() != (batch_size * self.num_heads, target_len, self.head_dim):\n+        if attn_output.size() != (\n+            batch_size * self.num_heads,\n+            target_len,\n+            self.head_dim,\n+        ):\n             raise ValueError(\n                 f\"`attn_output` should be of size {(batch_size, self.num_heads, target_len, self.head_dim)}, but is\"\n                 f\" {attn_output.size()}\"\n@@ -854,7 +769,9 @@ def __init__(self, config: DeformableDetrConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n         self.self_attn = DeformableDetrMultiscaleDeformableAttention(\n-            config, num_heads=config.encoder_attention_heads, n_points=config.encoder_n_points\n+            config,\n+            num_heads=config.encoder_attention_heads,\n+            n_points=config.encoder_n_points,\n         )\n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.dropout = config.dropout\n@@ -1054,7 +971,11 @@ class DeformableDetrPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [r\"DeformableDetrConvEncoder\", r\"DeformableDetrEncoderLayer\", r\"DeformableDetrDecoderLayer\"]\n+    _no_split_modules = [\n+        r\"DeformableDetrConvEncoder\",\n+        r\"DeformableDetrEncoderLayer\",\n+        r\"DeformableDetrDecoderLayer\",\n+    ]\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -1299,7 +1220,9 @@ def forward(\n         if not return_dict:\n             return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n         )\n \n \n@@ -1525,7 +1448,13 @@ def __init__(self, config: DeformableDetrConfig):\n             for _ in range(config.num_feature_levels - num_backbone_outs):\n                 input_proj_list.append(\n                     nn.Sequential(\n-                        nn.Conv2d(in_channels, config.d_model, kernel_size=3, stride=2, padding=1),\n+                        nn.Conv2d(\n+                            in_channels,\n+                            config.d_model,\n+                            kernel_size=3,\n+                            stride=2,\n+                            padding=1,\n+                        ),\n                         nn.GroupNorm(32, config.d_model),\n                     )\n                 )\n@@ -1535,7 +1464,11 @@ def __init__(self, config: DeformableDetrConfig):\n             self.input_proj = nn.ModuleList(\n                 [\n                     nn.Sequential(\n-                        nn.Conv2d(backbone.intermediate_channel_sizes[-1], config.d_model, kernel_size=1),\n+                        nn.Conv2d(\n+                            backbone.intermediate_channel_sizes[-1],\n+                            config.d_model,\n+                            kernel_size=1,\n+                        ),\n                         nn.GroupNorm(32, config.d_model),\n                     )\n                 ]\n@@ -1625,8 +1558,20 @@ def gen_encoder_output_proposals(self, enc_output, padding_mask, spatial_shapes)\n             valid_width = torch.sum(~mask_flatten_[:, 0, :, 0], 1)\n \n             grid_y, grid_x = meshgrid(\n-                torch.linspace(0, height - 1, height, dtype=enc_output.dtype, device=enc_output.device),\n-                torch.linspace(0, width - 1, width, dtype=enc_output.dtype, device=enc_output.device),\n+                torch.linspace(\n+                    0,\n+                    height - 1,\n+                    height,\n+                    dtype=enc_output.dtype,\n+                    device=enc_output.device,\n+                ),\n+                torch.linspace(\n+                    0,\n+                    width - 1,\n+                    width,\n+                    dtype=enc_output.dtype,\n+                    device=enc_output.device,\n+                ),\n                 indexing=\"ij\",\n             )\n             grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1)\n@@ -1802,7 +1747,9 @@ def forward(\n             topk = self.config.two_stage_num_proposals\n             topk_proposals = torch.topk(enc_outputs_class[..., 0], topk, dim=1)[1]\n             topk_coords_logits = torch.gather(\n-                enc_outputs_coord_logits, 1, topk_proposals.unsqueeze(-1).repeat(1, 1, 4)\n+                enc_outputs_coord_logits,\n+                1,\n+                topk_proposals.unsqueeze(-1).repeat(1, 1, 4),\n             )\n \n             topk_coords_logits = topk_coords_logits.detach()\n@@ -1897,7 +1844,10 @@ def __init__(self, config: DeformableDetrConfig):\n         # Detection heads on top\n         self.class_embed = nn.Linear(config.d_model, config.num_labels)\n         self.bbox_embed = DeformableDetrMLPPredictionHead(\n-            input_dim=config.d_model, hidden_dim=config.d_model, output_dim=4, num_layers=3\n+            input_dim=config.d_model,\n+            hidden_dim=config.d_model,\n+            output_dim=4,\n+            num_layers=3,\n         )\n \n         prior_prob = 0.01\n@@ -2033,7 +1983,13 @@ def forward(\n         loss, loss_dict, auxiliary_outputs = None, None, None\n         if labels is not None:\n             loss, loss_dict, auxiliary_outputs = self.loss_function(\n-                logits, labels, self.device, pred_boxes, self.config, outputs_class, outputs_coord\n+                logits,\n+                labels,\n+                self.device,\n+                pred_boxes,\n+                self.config,\n+                outputs_class,\n+                outputs_coord,\n             )\n         if not return_dict:\n             if auxiliary_outputs is not None:"
        },
        {
            "sha": "a8b244be5f6a6324fbc3f49bf888c5b7628164fe",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 67,
            "deletions": 155,
            "changes": 222,
            "blob_url": "https://github.com/huggingface/transformers/blob/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=f94b0c59f20447c0e6bdb6d381ea014fa47ecac8",
            "patch": "@@ -15,31 +15,27 @@\n \"\"\"PyTorch Grounding DINO model.\"\"\"\n \n import math\n-import os\n import warnings\n from dataclasses import dataclass\n-from pathlib import Path\n from typing import Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn.functional as F\n from torch import Tensor, nn\n-from torch.autograd import Function\n-from torch.autograd.function import once_differentiable\n \n from ...activations import ACT2FN\n from ...file_utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_timm_available,\n-    is_torch_cuda_available,\n     replace_return_docstrings,\n     requires_backends,\n )\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import meshgrid\n-from ...utils import is_ninja_available, logging\n+from ...utils import logging\n from ...utils.backbone_utils import load_backbone\n from ..auto import AutoModel\n from .configuration_grounding_dino import GroundingDinoConfig\n@@ -51,93 +47,64 @@\n \n logger = logging.get_logger(__name__)\n \n-MultiScaleDeformableAttention = None\n-\n-\n-# Copied from models.deformable_detr.load_cuda_kernels\n-def load_cuda_kernels():\n-    from torch.utils.cpp_extension import load\n-\n-    global MultiScaleDeformableAttention\n-\n-    root = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"deformable_detr\"\n-    src_files = [\n-        root / filename\n-        for filename in [\n-            \"vision.cpp\",\n-            os.path.join(\"cpu\", \"ms_deform_attn_cpu.cpp\"),\n-            os.path.join(\"cuda\", \"ms_deform_attn_cuda.cu\"),\n-        ]\n-    ]\n-\n-    MultiScaleDeformableAttention = load(\n-        \"MultiScaleDeformableAttention\",\n-        src_files,\n-        with_cuda=True,\n-        extra_include_paths=[str(root)],\n-        extra_cflags=[\"-DWITH_CUDA=1\"],\n-        extra_cuda_cflags=[\n-            \"-DCUDA_HAS_FP16=1\",\n-            \"-D__CUDA_NO_HALF_OPERATORS__\",\n-            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n-            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n-        ],\n-    )\n-\n-\n-# Copied from transformers.models.deformable_detr.modeling_deformable_detr.MultiScaleDeformableAttentionFunction\n-class MultiScaleDeformableAttentionFunction(Function):\n-    @staticmethod\n+_CONFIG_FOR_DOC = \"GroundingDinoConfig\"\n+_CHECKPOINT_FOR_DOC = \"IDEA-Research/grounding-dino-tiny\"\n+\n+\n+# Copied from models.deformable_detr.MultiScaleDeformableAttention\n+@use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n+class MultiScaleDeformableAttention(nn.Module):\n     def forward(\n-        context,\n-        value,\n-        value_spatial_shapes,\n-        value_level_start_index,\n-        sampling_locations,\n-        attention_weights,\n-        im2col_step,\n+        self,\n+        value: Tensor,\n+        value_spatial_shapes: Tensor,\n+        value_spatial_shapes_list: List[Tuple],\n+        level_start_index: Tensor,\n+        sampling_locations: Tensor,\n+        attention_weights: Tensor,\n+        im2col_step: int,\n     ):\n-        context.im2col_step = im2col_step\n-        output = MultiScaleDeformableAttention.ms_deform_attn_forward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            context.im2col_step,\n-        )\n-        context.save_for_backward(\n-            value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights\n+        batch_size, _, num_heads, hidden_dim = value.shape\n+        _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n+        value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+        sampling_grids = 2 * sampling_locations - 1\n+        sampling_value_list = []\n+        for level_id, (height, width) in enumerate(value_spatial_shapes):\n+            # batch_size, height*width, num_heads, hidden_dim\n+            # -> batch_size, height*width, num_heads*hidden_dim\n+            # -> batch_size, num_heads*hidden_dim, height*width\n+            # -> batch_size*num_heads, hidden_dim, height, width\n+            value_l_ = (\n+                value_list[level_id]\n+                .flatten(2)\n+                .transpose(1, 2)\n+                .reshape(batch_size * num_heads, hidden_dim, height, width)\n+            )\n+            # batch_size, num_queries, num_heads, num_points, 2\n+            # -> batch_size, num_heads, num_queries, num_points, 2\n+            # -> batch_size*num_heads, num_queries, num_points, 2\n+            sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n+            # batch_size*num_heads, hidden_dim, num_queries, num_points\n+            sampling_value_l_ = nn.functional.grid_sample(\n+                value_l_,\n+                sampling_grid_l_,\n+                mode=\"bilinear\",\n+                padding_mode=\"zeros\",\n+                align_corners=False,\n+            )\n+            sampling_value_list.append(sampling_value_l_)\n+        # (batch_size, num_queries, num_heads, num_levels, num_points)\n+        # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n+        # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n+        attention_weights = attention_weights.transpose(1, 2).reshape(\n+            batch_size * num_heads, 1, num_queries, num_levels * num_points\n         )\n-        return output\n-\n-    @staticmethod\n-    @once_differentiable\n-    def backward(context, grad_output):\n-        (\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-        ) = context.saved_tensors\n-        grad_value, grad_sampling_loc, grad_attn_weight = MultiScaleDeformableAttention.ms_deform_attn_backward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            grad_output,\n-            context.im2col_step,\n+        output = (\n+            (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n+            .sum(-1)\n+            .view(batch_size, num_heads * hidden_dim, num_queries)\n         )\n-\n-        return grad_value, None, None, grad_sampling_loc, grad_attn_weight, None\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-_CONFIG_FOR_DOC = \"GroundingDinoConfig\"\n-_CHECKPOINT_FOR_DOC = \"IDEA-Research/grounding-dino-tiny\"\n+        return output.transpose(1, 2).contiguous()\n \n \n @dataclass\n@@ -583,49 +550,6 @@ def build_position_encoding(config):\n     return position_embedding\n \n \n-# Copied from transformers.models.deformable_detr.modeling_deformable_detr.multi_scale_deformable_attention\n-def multi_scale_deformable_attention(\n-    value: Tensor,\n-    value_spatial_shapes: Union[Tensor, List[Tuple]],\n-    sampling_locations: Tensor,\n-    attention_weights: Tensor,\n-) -> Tensor:\n-    batch_size, _, num_heads, hidden_dim = value.shape\n-    _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-    value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n-    sampling_grids = 2 * sampling_locations - 1\n-    sampling_value_list = []\n-    for level_id, (height, width) in enumerate(value_spatial_shapes):\n-        # batch_size, height*width, num_heads, hidden_dim\n-        # -> batch_size, height*width, num_heads*hidden_dim\n-        # -> batch_size, num_heads*hidden_dim, height*width\n-        # -> batch_size*num_heads, hidden_dim, height, width\n-        value_l_ = (\n-            value_list[level_id].flatten(2).transpose(1, 2).reshape(batch_size * num_heads, hidden_dim, height, width)\n-        )\n-        # batch_size, num_queries, num_heads, num_points, 2\n-        # -> batch_size, num_heads, num_queries, num_points, 2\n-        # -> batch_size*num_heads, num_queries, num_points, 2\n-        sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n-        # batch_size*num_heads, hidden_dim, num_queries, num_points\n-        sampling_value_l_ = nn.functional.grid_sample(\n-            value_l_, sampling_grid_l_, mode=\"bilinear\", padding_mode=\"zeros\", align_corners=False\n-        )\n-        sampling_value_list.append(sampling_value_l_)\n-    # (batch_size, num_queries, num_heads, num_levels, num_points)\n-    # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n-    # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n-    attention_weights = attention_weights.transpose(1, 2).reshape(\n-        batch_size * num_heads, 1, num_queries, num_levels * num_points\n-    )\n-    output = (\n-        (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n-        .sum(-1)\n-        .view(batch_size, num_heads * hidden_dim, num_queries)\n-    )\n-    return output.transpose(1, 2).contiguous()\n-\n-\n # Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrMultiscaleDeformableAttention with DeformableDetr->GroundingDino, Deformable DETR->Grounding DINO\n class GroundingDinoMultiscaleDeformableAttention(nn.Module):\n     \"\"\"\n@@ -635,12 +559,7 @@ class GroundingDinoMultiscaleDeformableAttention(nn.Module):\n     def __init__(self, config: GroundingDinoConfig, num_heads: int, n_points: int):\n         super().__init__()\n \n-        kernel_loaded = MultiScaleDeformableAttention is not None\n-        if is_torch_cuda_available() and is_ninja_available() and not kernel_loaded:\n-            try:\n-                load_cuda_kernels()\n-            except Exception as e:\n-                logger.warning(f\"Could not load the custom kernel for multi-scale deformable attention: {e}\")\n+        self.attn = MultiScaleDeformableAttention()\n \n         if config.d_model % num_heads != 0:\n             raise ValueError(\n@@ -727,23 +646,16 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        if self.disable_custom_kernels or MultiScaleDeformableAttention is None:\n-            # PyTorch implementation\n-            output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n-        else:\n-            try:\n-                # custom kernel\n-                output = MultiScaleDeformableAttentionFunction.apply(\n-                    value,\n-                    spatial_shapes,\n-                    level_start_index,\n-                    sampling_locations,\n-                    attention_weights,\n-                    self.im2col_step,\n-                )\n-            except Exception:\n-                # PyTorch implementation\n-                output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n+        output = self.attn(\n+            value,\n+            spatial_shapes,\n+            spatial_shapes_list,\n+            level_start_index,\n+            sampling_locations,\n+            attention_weights,\n+            self.im2col_step,\n+        )\n+\n         output = self.output_proj(output)\n \n         return output, attention_weights"
        },
        {
            "sha": "ca9377e2827244f9b8b49e10952a0934a1e65d8c",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=f94b0c59f20447c0e6bdb6d381ea014fa47ecac8",
            "patch": "@@ -799,7 +799,7 @@ def get_num_masks(self, class_labels: torch.Tensor, device: torch.device) -> tor\n         return num_masks\n \n \n-# Copied from transformers.models.deformable_detr.modeling_deformable_detr.multi_scale_deformable_attention\n+# Copied from transformers.models.oneformer.modeling_oneformer.multi_scale_deformable_attention\n def multi_scale_deformable_attention(\n     value: Tensor,\n     value_spatial_shapes: Union[Tensor, List[Tuple]],"
        },
        {
            "sha": "61cc747ca752c190ae37cd6f2a0ed3f4fc6d60e0",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 65,
            "deletions": 155,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=f94b0c59f20447c0e6bdb6d381ea014fa47ecac8",
            "patch": "@@ -15,38 +15,32 @@\n \"\"\"PyTorch OmDet-Turbo model.\"\"\"\n \n import math\n-import os\n import warnings\n from collections import OrderedDict\n from dataclasses import dataclass\n from functools import lru_cache\n-from pathlib import Path\n from typing import List, Optional, Tuple, Union\n \n import torch\n import torch.nn.functional as F\n from torch import Tensor, nn\n-from torch.autograd import Function\n-from torch.autograd.function import once_differentiable\n \n from ...activations import ACT2CLS, ACT2FN\n from ...file_utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_torch_cuda_available,\n     replace_return_docstrings,\n )\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_utils import PreTrainedModel\n-from ...utils import is_ninja_available, logging\n+from ...utils import logging\n from ...utils.backbone_utils import load_backbone\n from ..auto import AutoModel\n from .configuration_omdet_turbo import OmDetTurboConfig\n \n \n-MultiScaleDeformableAttention = None\n-\n logger = logging.get_logger(__name__)\n _CONFIG_FOR_DOC = \"OmDetTurboConfig\"\n \n@@ -178,79 +172,60 @@ class OmDetTurboObjectDetectionOutput(ModelOutput):\n     classes_structure: Optional[torch.LongTensor] = None\n \n \n-# Copied from models.deformable_detr.load_cuda_kernels\n-def load_cuda_kernels():\n-    from torch.utils.cpp_extension import load\n-\n-    global MultiScaleDeformableAttention\n-\n-    root = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"deformable_detr\"\n-    src_files = [\n-        root / filename\n-        for filename in [\n-            \"vision.cpp\",\n-            os.path.join(\"cpu\", \"ms_deform_attn_cpu.cpp\"),\n-            os.path.join(\"cuda\", \"ms_deform_attn_cuda.cu\"),\n-        ]\n-    ]\n-\n-    MultiScaleDeformableAttention = load(\n-        \"MultiScaleDeformableAttention\",\n-        src_files,\n-        with_cuda=True,\n-        extra_include_paths=[str(root)],\n-        extra_cflags=[\"-DWITH_CUDA=1\"],\n-        extra_cuda_cflags=[\n-            \"-DCUDA_HAS_FP16=1\",\n-            \"-D__CUDA_NO_HALF_OPERATORS__\",\n-            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n-            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n-        ],\n-    )\n-\n-\n-# Copied from transformers.models.deformable_detr.modeling_deformable_detr.multi_scale_deformable_attention\n-def multi_scale_deformable_attention(\n-    value: Tensor,\n-    value_spatial_shapes: Union[Tensor, List[Tuple]],\n-    sampling_locations: Tensor,\n-    attention_weights: Tensor,\n-) -> Tensor:\n-    batch_size, _, num_heads, hidden_dim = value.shape\n-    _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-    # Ignore copy\n-    value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n-    sampling_grids = 2 * sampling_locations - 1\n-    sampling_value_list = []\n-    for level_id, (height, width) in enumerate(value_spatial_shapes):\n-        # batch_size, height*width, num_heads, hidden_dim\n-        # -> batch_size, height*width, num_heads*hidden_dim\n-        # -> batch_size, num_heads*hidden_dim, height*width\n-        # -> batch_size*num_heads, hidden_dim, height, width\n-        value_l_ = (\n-            value_list[level_id].flatten(2).transpose(1, 2).reshape(batch_size * num_heads, hidden_dim, height, width)\n+# Copied from models.deformable_detr.MultiScaleDeformableAttention\n+@use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n+class MultiScaleDeformableAttention(nn.Module):\n+    def forward(\n+        self,\n+        value: Tensor,\n+        value_spatial_shapes: Tensor,\n+        value_spatial_shapes_list: List[Tuple],\n+        level_start_index: Tensor,\n+        sampling_locations: Tensor,\n+        attention_weights: Tensor,\n+        im2col_step: int,\n+    ):\n+        batch_size, _, num_heads, hidden_dim = value.shape\n+        _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n+        value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+        sampling_grids = 2 * sampling_locations - 1\n+        sampling_value_list = []\n+        for level_id, (height, width) in enumerate(value_spatial_shapes):\n+            # batch_size, height*width, num_heads, hidden_dim\n+            # -> batch_size, height*width, num_heads*hidden_dim\n+            # -> batch_size, num_heads*hidden_dim, height*width\n+            # -> batch_size*num_heads, hidden_dim, height, width\n+            value_l_ = (\n+                value_list[level_id]\n+                .flatten(2)\n+                .transpose(1, 2)\n+                .reshape(batch_size * num_heads, hidden_dim, height, width)\n+            )\n+            # batch_size, num_queries, num_heads, num_points, 2\n+            # -> batch_size, num_heads, num_queries, num_points, 2\n+            # -> batch_size*num_heads, num_queries, num_points, 2\n+            sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n+            # batch_size*num_heads, hidden_dim, num_queries, num_points\n+            sampling_value_l_ = nn.functional.grid_sample(\n+                value_l_,\n+                sampling_grid_l_,\n+                mode=\"bilinear\",\n+                padding_mode=\"zeros\",\n+                align_corners=False,\n+            )\n+            sampling_value_list.append(sampling_value_l_)\n+        # (batch_size, num_queries, num_heads, num_levels, num_points)\n+        # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n+        # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n+        attention_weights = attention_weights.transpose(1, 2).reshape(\n+            batch_size * num_heads, 1, num_queries, num_levels * num_points\n         )\n-        # batch_size, num_queries, num_heads, num_points, 2\n-        # -> batch_size, num_heads, num_queries, num_points, 2\n-        # -> batch_size*num_heads, num_queries, num_points, 2\n-        sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n-        # batch_size*num_heads, hidden_dim, num_queries, num_points\n-        sampling_value_l_ = nn.functional.grid_sample(\n-            value_l_, sampling_grid_l_, mode=\"bilinear\", padding_mode=\"zeros\", align_corners=False\n+        output = (\n+            (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n+            .sum(-1)\n+            .view(batch_size, num_heads * hidden_dim, num_queries)\n         )\n-        sampling_value_list.append(sampling_value_l_)\n-    # (batch_size, num_queries, num_heads, num_levels, num_points)\n-    # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n-    # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n-    attention_weights = attention_weights.transpose(1, 2).reshape(\n-        batch_size * num_heads, 1, num_queries, num_levels * num_points\n-    )\n-    output = (\n-        (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n-        .sum(-1)\n-        .view(batch_size, num_heads * hidden_dim, num_queries)\n-    )\n-    return output.transpose(1, 2).contiguous()\n+        return output.transpose(1, 2).contiguous()\n \n \n class OmDetTurboLRUCache:\n@@ -332,55 +307,6 @@ def forward(self, pixel_values):\n         return outputs\n \n \n-# Copied from transformers.models.deformable_detr.modeling_deformable_detr.MultiScaleDeformableAttentionFunction\n-class MultiScaleDeformableAttentionFunction(Function):\n-    @staticmethod\n-    def forward(\n-        context,\n-        value,\n-        value_spatial_shapes,\n-        value_level_start_index,\n-        sampling_locations,\n-        attention_weights,\n-        im2col_step,\n-    ):\n-        context.im2col_step = im2col_step\n-        output = MultiScaleDeformableAttention.ms_deform_attn_forward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            context.im2col_step,\n-        )\n-        context.save_for_backward(\n-            value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights\n-        )\n-        return output\n-\n-    @staticmethod\n-    @once_differentiable\n-    def backward(context, grad_output):\n-        (\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-        ) = context.saved_tensors\n-        grad_value, grad_sampling_loc, grad_attn_weight = MultiScaleDeformableAttention.ms_deform_attn_backward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            grad_output,\n-            context.im2col_step,\n-        )\n-\n-        return grad_value, None, None, grad_sampling_loc, grad_attn_weight, None\n-\n-\n # Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrMultiscaleDeformableAttention with DeformableDetr->OmDetTurbo, Deformable DETR->OmDet-Turbo\n class OmDetTurboMultiscaleDeformableAttention(nn.Module):\n     \"\"\"\n@@ -390,12 +316,7 @@ class OmDetTurboMultiscaleDeformableAttention(nn.Module):\n     def __init__(self, config: OmDetTurboConfig, num_heads: int, n_points: int):\n         super().__init__()\n \n-        kernel_loaded = MultiScaleDeformableAttention is not None\n-        if is_torch_cuda_available() and is_ninja_available() and not kernel_loaded:\n-            try:\n-                load_cuda_kernels()\n-            except Exception as e:\n-                logger.warning(f\"Could not load the custom kernel for multi-scale deformable attention: {e}\")\n+        self.attn = MultiScaleDeformableAttention()\n \n         if config.d_model % num_heads != 0:\n             raise ValueError(\n@@ -483,27 +404,16 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        if self.disable_custom_kernels:\n-            # PyTorch implementation\n-            output = multi_scale_deformable_attention(\n-                value, spatial_shapes_list, sampling_locations, attention_weights\n-            )\n-        else:\n-            try:\n-                # custom kernel\n-                output = MultiScaleDeformableAttentionFunction.apply(\n-                    value,\n-                    spatial_shapes,\n-                    level_start_index,\n-                    sampling_locations,\n-                    attention_weights,\n-                    self.im2col_step,\n-                )\n-            except Exception:\n-                # PyTorch implementation\n-                output = multi_scale_deformable_attention(\n-                    value, spatial_shapes_list, sampling_locations, attention_weights\n-                )\n+        output = self.attn(\n+            value,\n+            spatial_shapes,\n+            spatial_shapes_list,\n+            level_start_index,\n+            sampling_locations,\n+            attention_weights,\n+            self.im2col_step,\n+        )\n+\n         output = self.output_proj(output)\n \n         return output, attention_weights"
        },
        {
            "sha": "eef4b6a3c2d324ee9d832535d610c56fb3379163",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=f94b0c59f20447c0e6bdb6d381ea014fa47ecac8",
            "patch": "@@ -61,7 +61,6 @@ def _get_clones(module, N):\n     return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n \n \n-# Copied from transformers.models.deformable_detr.modeling_deformable_detr.multi_scale_deformable_attention\n def multi_scale_deformable_attention(\n     value: Tensor,\n     value_spatial_shapes: Union[Tensor, List[Tuple]],"
        },
        {
            "sha": "54dfc43bf867a0bfabaf4143d9959407c691876d",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 66,
            "deletions": 159,
            "changes": 225,
            "blob_url": "https://github.com/huggingface/transformers/blob/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=f94b0c59f20447c0e6bdb6d381ea014fa47ecac8",
            "patch": "@@ -15,31 +15,25 @@\n \"\"\"PyTorch RT-DETR model.\"\"\"\n \n import math\n-import os\n import warnings\n from dataclasses import dataclass\n from functools import partial\n-from pathlib import Path\n from typing import Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn.functional as F\n from torch import Tensor, nn\n-from torch.autograd import Function\n-from torch.autograd.function import once_differentiable\n \n from ...activations import ACT2CLS, ACT2FN\n from ...image_transforms import center_to_corners_format, corners_to_center_format\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import compile_compatible_method_lru_cache\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_ninja_available,\n-    is_torch_cuda_available,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n     torch_int,\n@@ -50,94 +44,66 @@\n \n logger = logging.get_logger(__name__)\n \n-MultiScaleDeformableAttention = None\n-\n-\n-# Copied from transformers.models.deformable_detr.modeling_deformable_detr.load_cuda_kernels\n-def load_cuda_kernels():\n-    from torch.utils.cpp_extension import load\n-\n-    global MultiScaleDeformableAttention\n-\n-    root = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"deformable_detr\"\n-    src_files = [\n-        root / filename\n-        for filename in [\n-            \"vision.cpp\",\n-            os.path.join(\"cpu\", \"ms_deform_attn_cpu.cpp\"),\n-            os.path.join(\"cuda\", \"ms_deform_attn_cuda.cu\"),\n-        ]\n-    ]\n-\n-    MultiScaleDeformableAttention = load(\n-        \"MultiScaleDeformableAttention\",\n-        src_files,\n-        with_cuda=True,\n-        extra_include_paths=[str(root)],\n-        extra_cflags=[\"-DWITH_CUDA=1\"],\n-        extra_cuda_cflags=[\n-            \"-DCUDA_HAS_FP16=1\",\n-            \"-D__CUDA_NO_HALF_OPERATORS__\",\n-            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n-            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n-        ],\n-    )\n+\n+_CONFIG_FOR_DOC = \"RTDetrConfig\"\n+# TODO: Replace all occurrences of the checkpoint with the final one\n+_CHECKPOINT_FOR_DOC = \"PekingU/rtdetr_r50vd\"\n \n \n-# Copied from transformers.models.deformable_detr.modeling_deformable_detr.MultiScaleDeformableAttentionFunction\n-class MultiScaleDeformableAttentionFunction(Function):\n-    @staticmethod\n+# Copied from models.deformable_detr.MultiScaleDeformableAttention\n+@use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n+class MultiScaleDeformableAttention(nn.Module):\n     def forward(\n-        context,\n-        value,\n-        value_spatial_shapes,\n-        value_level_start_index,\n-        sampling_locations,\n-        attention_weights,\n-        im2col_step,\n+        self,\n+        value: Tensor,\n+        value_spatial_shapes: Tensor,\n+        value_spatial_shapes_list: List[Tuple],\n+        level_start_index: Tensor,\n+        sampling_locations: Tensor,\n+        attention_weights: Tensor,\n+        im2col_step: int,\n     ):\n-        context.im2col_step = im2col_step\n-        output = MultiScaleDeformableAttention.ms_deform_attn_forward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            context.im2col_step,\n-        )\n-        context.save_for_backward(\n-            value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights\n+        batch_size, _, num_heads, hidden_dim = value.shape\n+        _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n+        value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+        sampling_grids = 2 * sampling_locations - 1\n+        sampling_value_list = []\n+        for level_id, (height, width) in enumerate(value_spatial_shapes):\n+            # batch_size, height*width, num_heads, hidden_dim\n+            # -> batch_size, height*width, num_heads*hidden_dim\n+            # -> batch_size, num_heads*hidden_dim, height*width\n+            # -> batch_size*num_heads, hidden_dim, height, width\n+            value_l_ = (\n+                value_list[level_id]\n+                .flatten(2)\n+                .transpose(1, 2)\n+                .reshape(batch_size * num_heads, hidden_dim, height, width)\n+            )\n+            # batch_size, num_queries, num_heads, num_points, 2\n+            # -> batch_size, num_heads, num_queries, num_points, 2\n+            # -> batch_size*num_heads, num_queries, num_points, 2\n+            sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n+            # batch_size*num_heads, hidden_dim, num_queries, num_points\n+            sampling_value_l_ = nn.functional.grid_sample(\n+                value_l_,\n+                sampling_grid_l_,\n+                mode=\"bilinear\",\n+                padding_mode=\"zeros\",\n+                align_corners=False,\n+            )\n+            sampling_value_list.append(sampling_value_l_)\n+        # (batch_size, num_queries, num_heads, num_levels, num_points)\n+        # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n+        # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n+        attention_weights = attention_weights.transpose(1, 2).reshape(\n+            batch_size * num_heads, 1, num_queries, num_levels * num_points\n         )\n-        return output\n-\n-    @staticmethod\n-    @once_differentiable\n-    def backward(context, grad_output):\n-        (\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-        ) = context.saved_tensors\n-        grad_value, grad_sampling_loc, grad_attn_weight = MultiScaleDeformableAttention.ms_deform_attn_backward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            grad_output,\n-            context.im2col_step,\n+        output = (\n+            (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n+            .sum(-1)\n+            .view(batch_size, num_heads * hidden_dim, num_queries)\n         )\n-\n-        return grad_value, None, None, grad_sampling_loc, grad_attn_weight, None\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-_CONFIG_FOR_DOC = \"RTDetrConfig\"\n-# TODO: Replace all occurrences of the checkpoint with the final one\n-_CHECKPOINT_FOR_DOC = \"PekingU/rtdetr_r50vd\"\n+        return output.transpose(1, 2).contiguous()\n \n \n @dataclass\n@@ -728,49 +694,6 @@ def forward(self, hidden_state):\n         return self.conv3(hidden_state_1 + hidden_state_2)\n \n \n-# Copied from transformers.models.deformable_detr.modeling_deformable_detr.multi_scale_deformable_attention\n-def multi_scale_deformable_attention(\n-    value: Tensor,\n-    value_spatial_shapes: Union[Tensor, List[Tuple]],\n-    sampling_locations: Tensor,\n-    attention_weights: Tensor,\n-) -> Tensor:\n-    batch_size, _, num_heads, hidden_dim = value.shape\n-    _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n-    value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n-    sampling_grids = 2 * sampling_locations - 1\n-    sampling_value_list = []\n-    for level_id, (height, width) in enumerate(value_spatial_shapes):\n-        # batch_size, height*width, num_heads, hidden_dim\n-        # -> batch_size, height*width, num_heads*hidden_dim\n-        # -> batch_size, num_heads*hidden_dim, height*width\n-        # -> batch_size*num_heads, hidden_dim, height, width\n-        value_l_ = (\n-            value_list[level_id].flatten(2).transpose(1, 2).reshape(batch_size * num_heads, hidden_dim, height, width)\n-        )\n-        # batch_size, num_queries, num_heads, num_points, 2\n-        # -> batch_size, num_heads, num_queries, num_points, 2\n-        # -> batch_size*num_heads, num_queries, num_points, 2\n-        sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n-        # batch_size*num_heads, hidden_dim, num_queries, num_points\n-        sampling_value_l_ = nn.functional.grid_sample(\n-            value_l_, sampling_grid_l_, mode=\"bilinear\", padding_mode=\"zeros\", align_corners=False\n-        )\n-        sampling_value_list.append(sampling_value_l_)\n-    # (batch_size, num_queries, num_heads, num_levels, num_points)\n-    # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n-    # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n-    attention_weights = attention_weights.transpose(1, 2).reshape(\n-        batch_size * num_heads, 1, num_queries, num_levels * num_points\n-    )\n-    output = (\n-        (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n-        .sum(-1)\n-        .view(batch_size, num_heads * hidden_dim, num_queries)\n-    )\n-    return output.transpose(1, 2).contiguous()\n-\n-\n # Copied from transformers.models.deformable_detr.modeling_deformable_detr.DeformableDetrMultiscaleDeformableAttention with DeformableDetr->RTDetr\n class RTDetrMultiscaleDeformableAttention(nn.Module):\n     \"\"\"\n@@ -780,12 +703,7 @@ class RTDetrMultiscaleDeformableAttention(nn.Module):\n     def __init__(self, config: RTDetrConfig, num_heads: int, n_points: int):\n         super().__init__()\n \n-        kernel_loaded = MultiScaleDeformableAttention is not None\n-        if is_torch_cuda_available() and is_ninja_available() and not kernel_loaded:\n-            try:\n-                load_cuda_kernels()\n-            except Exception as e:\n-                logger.warning(f\"Could not load the custom kernel for multi-scale deformable attention: {e}\")\n+        self.attn = MultiScaleDeformableAttention()\n \n         if config.d_model % num_heads != 0:\n             raise ValueError(\n@@ -872,27 +790,16 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        if self.disable_custom_kernels or MultiScaleDeformableAttention is None or is_torchdynamo_compiling():\n-            # PyTorch implementation\n-            output = multi_scale_deformable_attention(\n-                value, spatial_shapes_list, sampling_locations, attention_weights\n-            )\n-        else:\n-            try:\n-                # custom kernel\n-                output = MultiScaleDeformableAttentionFunction.apply(\n-                    value,\n-                    spatial_shapes,\n-                    level_start_index,\n-                    sampling_locations,\n-                    attention_weights,\n-                    self.im2col_step,\n-                )\n-            except Exception:\n-                # PyTorch implementation\n-                output = multi_scale_deformable_attention(\n-                    value, spatial_shapes_list, sampling_locations, attention_weights\n-                )\n+        output = self.attn(\n+            value,\n+            spatial_shapes,\n+            spatial_shapes_list,\n+            level_start_index,\n+            sampling_locations,\n+            attention_weights,\n+            self.im2col_step,\n+        )\n+\n         output = self.output_proj(output)\n \n         return output, attention_weights"
        },
        {
            "sha": "929caeb7fa0653bf3e9469ad1b23091707f43012",
            "filename": "utils/check_build.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/utils%2Fcheck_build.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/utils%2Fcheck_build.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_build.py?ref=f94b0c59f20447c0e6bdb6d381ea014fa47ecac8",
            "patch": "@@ -21,8 +21,6 @@\n FILES_TO_FIND = [\n     \"kernels/rwkv/wkv_cuda.cu\",\n     \"kernels/rwkv/wkv_op.cpp\",\n-    \"kernels/deformable_detr/ms_deform_attn.h\",\n-    \"kernels/deformable_detr/cuda/ms_deform_im2col_cuda.cuh\",\n     \"kernels/falcon_mamba/selective_scan_with_ln_interface.py\",\n     \"kernels/falcon_mamba/__init__.py\",\n     \"kernels/__init__.py\","
        },
        {
            "sha": "61d452464c6842888a3bc97abad12653f10707e7",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/f94b0c59f20447c0e6bdb6d381ea014fa47ecac8/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=f94b0c59f20447c0e6bdb6d381ea014fa47ecac8",
            "patch": "@@ -475,7 +475,6 @@ src/transformers/models/deberta/modeling_tf_deberta.py\n src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py\n src/transformers/models/decision_transformer/modeling_decision_transformer.py\n src/transformers/models/deformable_detr/convert_deformable_detr_to_pytorch.py\n-src/transformers/models/deformable_detr/load_custom.py\n src/transformers/models/deit/convert_deit_timm_to_pytorch.py\n src/transformers/models/deprecated/bort/convert_bort_original_gluonnlp_checkpoint_to_pytorch.py\n src/transformers/models/deprecated/mctct/configuration_mctct.py"
        }
    ],
    "stats": {
        "total": 4239,
        "additions": 405,
        "deletions": 3834
    }
}