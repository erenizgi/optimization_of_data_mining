{
    "author": "remi-or",
    "message": "Small changes to benchmarking script (#41662)",
    "sha": "f7c33abab3a6233a51d7d4fd116625be14df68ff",
    "files": [
        {
            "sha": "28f5824ea6ae943891ad5535fbee10d687d79947",
            "filename": "benchmark_v2/framework/benchmark_config.py",
            "status": "modified",
            "additions": 15,
            "deletions": 18,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/f7c33abab3a6233a51d7d4fd116625be14df68ff/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f7c33abab3a6233a51d7d4fd116625be14df68ff/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_config.py?ref=f7c33abab3a6233a51d7d4fd116625be14df68ff",
            "patch": "@@ -104,7 +104,7 @@ def to_dict(self) -> dict[str, Any]:\n             \"attn_implementation\": self.attn_implementation,\n             \"sdpa_backend\": self.sdpa_backend,\n             \"compile_mode\": self.compile_mode,\n-            \"compile_options\": self.compile_options,\n+            \"compile_options\": self.compile_options | {},  # to avoid inplace modification of the original dict\n             \"kernelize\": self.kernelize,\n         }\n \n@@ -191,28 +191,25 @@ def generate_all_configs(\n     )\n \n \n-def generate_default_configs(\n+def generate_main_configs(\n     warmup_iterations: int = 5,\n     measurement_iterations: int = 20,\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n     gpu_monitoring: bool = False,\n ) -> list[BenchmarkConfig]:\n-    all_attn_implementations = [\n-        (\"flash_attention_2\", None),\n-        (\"eager\", None),\n-        (\"sdpa\", \"math\"),\n-        (\"sdpa\", \"flash_attention\"),  # note: this one can fail with compile because of attn mask\n+    # Create kwargs common to all configs\n+    kwargs = {\n+        \"warmup_iterations\": warmup_iterations,\n+        \"measurement_iterations\": measurement_iterations,\n+        \"batch_size\": batch_size,\n+        \"sequence_length\": sequence_length,\n+        \"num_tokens_to_generate\": num_tokens_to_generate,\n+        \"gpu_monitoring\": gpu_monitoring,\n+    }\n+    return [  # TODO: test max-autotune instead of default\n+        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", **kwargs),\n+        BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\", **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flash_attention_2\", **kwargs),\n     ]\n-    return cross_generate_configs(\n-        attn_impl_and_sdpa_backend=all_attn_implementations,\n-        compiled_mode=[None, \"max-autotune\"],\n-        kernelized=[False, KERNELIZATION_AVAILABLE],\n-        warmup_iterations=warmup_iterations,\n-        measurement_iterations=measurement_iterations,\n-        batch_size=batch_size,\n-        sequence_length=sequence_length,\n-        num_tokens_to_generate=num_tokens_to_generate,\n-        gpu_monitoring=gpu_monitoring,\n-    )"
        },
        {
            "sha": "913533b9d50e4aa75e94533cc1e1749264da3bc2",
            "filename": "benchmark_v2/framework/benchmark_runner.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/f7c33abab3a6233a51d7d4fd116625be14df68ff/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f7c33abab3a6233a51d7d4fd116625be14df68ff/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_runner.py?ref=f7c33abab3a6233a51d7d4fd116625be14df68ff",
            "patch": "@@ -144,11 +144,11 @@ def __next__(self):\n class BenchmarkRunner:\n     \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n \n-    def __init__(\n-        self, logger: logging.Logger, output_dir: str = \"benchmark_results\", commit_id: str | None = None\n-    ) -> None:\n+    def __init__(self, logger: logging.Logger, output_dir: str | None = None, commit_id: str | None = None) -> None:\n         # Those stay constant for the whole run\n         self.logger = logger\n+        if output_dir is None:\n+            output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"benchmark_results\")\n         self.output_dir = output_dir\n         self.commit_id = get_git_revision() if commit_id is None else commit_id\n         os.makedirs(self.output_dir, exist_ok=True)\n@@ -214,7 +214,7 @@ def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_t\n \n             # Quick validation: try one measurement first to see if this scenario works\n             flush_memory()\n-            e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+            e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics = self.time_generate(\n                 max_new_tokens=1, gpu_monitor=None\n             )\n             if e2e_latency < 0:\n@@ -231,11 +231,11 @@ def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_t\n             result = BenchmarkResult()\n             self.logger.info(f\"Benchmarking with {config.measurement_iterations} iterations.\")\n             for _ in trange(config.measurement_iterations):\n-                e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+                e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics = self.time_generate(\n                     max_new_tokens=config.num_tokens_to_generate,\n                     gpu_monitor=(GPUMonitor(logger=self.logger) if config.gpu_monitoring else None),\n                 )\n-                result.accumulate(e2e_latency, token_generation_times, decoded_output, gpu_metrics)\n+                result.accumulate(e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics)\n             self.logger.info(\"Benchmarking done. Cleaning up.\")\n \n             # Profile if needed\n@@ -277,10 +277,11 @@ def time_generate(\n             raise RuntimeError(f\"Generated {new_tokens} tokens, expected {max_new_tokens}\")\n         # Decode outputs\n         decoded_output = self.tokenizer.decode(outputs[0, input_tokens:], skip_special_tokens=True)\n+        shape_and_decoded_output = f\"{tuple(outputs.shape)} | {decoded_output}\"\n         # Compute intermediate quantities\n         e2e_latency = wall_time_1 - wall_time_0\n         token_generation_times = [t - wall_time_0 for t in streamer.timestamps[1:]]\n-        return e2e_latency, token_generation_times, decoded_output, gpu_metrics\n+        return e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics\n \n     def profile_generate(self, num_tokens_to_profile: int, config_name: str) -> None:\n         \"\"\"Profile the latency of a call to model.generate() with the given (inputs) and (max_new_tokens).\"\"\"\n@@ -351,10 +352,10 @@ def run_benchmarks(\n                 first_metadata = all_results[first_key][\"metadata\"].to_dict()\n                 hardware_info = first_metadata.pop(\"hardware_info\")\n                 pretty_print_dict(first_metadata | hardware_info, tabs=1)\n-            for value in all_results.values():\n+            for result in all_results.values():\n                 print(\"=\" * 100)\n-                print(f\"Config: {value['config'].infer_name(compact=False)}\\n\")\n-                value[\"measurements\"].pprint(tabs=1)\n+                print(f\"Config: {result['config'].infer_name(compact=False)}\\n\")\n+                result[\"measurements\"].pprint(batch_size=result[\"config\"].batch_size, tabs=1)\n             print(\"=\" * 100)\n \n         return all_results"
        },
        {
            "sha": "149b3d9c91a7a78c59e80a45e5c6113e1d147022",
            "filename": "benchmark_v2/framework/data_classes.py",
            "status": "modified",
            "additions": 29,
            "deletions": 21,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/f7c33abab3a6233a51d7d4fd116625be14df68ff/benchmark_v2%2Fframework%2Fdata_classes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f7c33abab3a6233a51d7d4fd116625be14df68ff/benchmark_v2%2Fframework%2Fdata_classes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fdata_classes.py?ref=f7c33abab3a6233a51d7d4fd116625be14df68ff",
            "patch": "@@ -82,19 +82,19 @@ class BenchmarkResult:\n     def __init__(self) -> None:\n         self.e2e_latency = []\n         self.token_generation_times = []  # time at which each token was generated (relative to start of the generation)\n-        self.decoded_outputs = []\n+        self.shape_and_decoded_outputs = []\n         self.gpu_metrics = []\n \n     def accumulate(\n         self,\n         e2e_latency: float,\n         token_generation_times: list[float],\n-        decoded_output: str,\n+        shape_and_decoded_output: str,\n         gpu_metrics: GPURawMetrics | None,\n     ) -> None:\n         self.e2e_latency.append(e2e_latency)\n         self.token_generation_times.append(token_generation_times)\n-        self.decoded_outputs.append(decoded_output)\n+        self.shape_and_decoded_outputs.append(shape_and_decoded_output)\n         self.gpu_metrics.append(gpu_metrics)\n \n     def to_dict(self) -> dict[str, None | int | float]:\n@@ -106,7 +106,7 @@ def to_dict(self) -> dict[str, None | int | float]:\n         return {\n             \"e2e_latency\": self.e2e_latency,\n             \"token_generation_times\": self.token_generation_times,\n-            \"decoded_outputs\": self.decoded_outputs,\n+            \"shape_and_decoded_outputs\": self.shape_and_decoded_outputs,\n             \"gpu_metrics\": gpu_metrics,\n         }\n \n@@ -123,7 +123,7 @@ def from_dict(cls, data: dict[str, None | int | float]) -> \"BenchmarkResult\":\n             new_instance.accumulate(\n                 e2e_latency=data[\"e2e_latency\"][i],\n                 token_generation_times=data[\"token_generation_times\"][i],\n-                decoded_output=data[\"decoded_output\"][i],\n+                shape_and_decoded_output=data[\"shape_and_decoded_outputs\"][i],\n                 gpu_metrics=gpu_metrics[i],\n             )\n         return new_instance\n@@ -134,19 +134,27 @@ def get_measured_ttft(self) -> list[float]:\n     def get_measured_itl(self) -> list[float]:\n         return [(dt[-1] - dt[0]) / (len(dt) - 1) for dt in self.token_generation_times if len(dt) > 1]\n \n-    def pprint(self, tabs: int = 0) -> None:\n-        collated_stats = equalize_lengths_and_collate(\n-            [\n-                add_unit_to_duration(compute_basic_statistics(self.e2e_latency)),\n-                add_unit_to_duration(compute_basic_statistics(self.get_measured_ttft())),\n-                add_unit_to_duration(compute_basic_statistics(self.get_measured_itl())),\n-            ]\n-        )\n-        pretty_print_dict(\n-            {\n-                \"E2E Latency\": collated_stats[0],\n-                \"Time to First Token\": collated_stats[1],\n-                \"Inter-Token Latency\": collated_stats[2],\n-            },\n-            tabs=tabs,\n-        )\n+    def get_throughput(self, batch_size: int) -> float:\n+        return [\n+            batch_size * len(dt) / e2e_latency\n+            for e2e_latency, dt in zip(self.e2e_latency, self.token_generation_times)\n+        ]\n+\n+    def pprint(self, batch_size: int = 0, tabs: int = 0) -> None:\n+        stats_to_collate = [\n+            add_unit_to_duration(compute_basic_statistics(self.e2e_latency)),\n+            add_unit_to_duration(compute_basic_statistics(self.get_measured_ttft())),\n+            add_unit_to_duration(compute_basic_statistics(self.get_measured_itl())),\n+        ]\n+        if batch_size > 0:\n+            throughput_stats = compute_basic_statistics(self.get_throughput(batch_size))\n+            stats_to_collate.append({key: f\"{value:.2f}tok/s\" for key, value in throughput_stats.items()})\n+        collated_stats = equalize_lengths_and_collate(stats_to_collate)\n+        dict_to_pprint = {\n+            \"E2E Latency\": collated_stats[0],\n+            \"Time to First Token\": collated_stats[1],\n+            \"Inter-Token Latency\": collated_stats[2],\n+        }\n+        if batch_size > 0:\n+            dict_to_pprint[\"Throughput\"] = collated_stats[3]\n+        pretty_print_dict(dict_to_pprint, tabs=tabs)"
        },
        {
            "sha": "ea811b423eff742b8ccf8d37e89ace4732aa2ad6",
            "filename": "benchmark_v2/run_benchmarks.py",
            "status": "modified",
            "additions": 33,
            "deletions": 28,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/f7c33abab3a6233a51d7d4fd116625be14df68ff/benchmark_v2%2Frun_benchmarks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f7c33abab3a6233a51d7d4fd116625be14df68ff/benchmark_v2%2Frun_benchmarks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Frun_benchmarks.py?ref=f7c33abab3a6233a51d7d4fd116625be14df68ff",
            "patch": "@@ -20,28 +20,28 @@\n \n import argparse\n import logging\n-import random\n import sys\n import uuid\n \n-from framework.benchmark_config import BenchmarkConfig, generate_all_configs\n+from framework.benchmark_config import BenchmarkConfig, generate_all_configs, generate_main_configs\n from framework.benchmark_runner import BenchmarkRunner\n \n \n if __name__ == \"__main__\":\n     # Parse arguments\n     parser = argparse.ArgumentParser()\n-    parser.add_argument(\"--output-dir\", type=str, default=\"benchmark_results\", help=\"Output dir for benchmark results\")\n+    parser.add_argument(\"--output-dir\", type=str, default=None, help=\"Output dir for benchmark results\")\n     parser.add_argument(\"--log-level\", type=str, choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"], default=\"INFO\")\n     parser.add_argument(\"--model-id\", type=str, help=\"Specific model ID to benchmark (if supported by benchmarks)\")\n \n-    parser.add_argument(\"--warmup\", type=int, default=5, help=\"Number of warmup iterations\")\n-    parser.add_argument(\"--iterations\", type=int, default=20, help=\"Number of measurement iterations\")\n+    parser.add_argument(\"--warmup\", type=int, default=3, help=\"Number of warmup iterations\")\n+    parser.add_argument(\"--iterations\", type=int, default=10, help=\"Number of measurement iterations\")\n \n     parser.add_argument(\"--batch-size\", \"-b\", type=int, nargs=\"+\", help=\"Batch size\")\n     parser.add_argument(\"--sequence-length\", \"-s\", type=int, nargs=\"+\", help=\"Sequence length\")\n     parser.add_argument(\"--num-tokens-to-generate\", \"-n\", type=int, nargs=\"+\", help=\"Number of tokens to generate\")\n \n+    parser.add_argument(\"--cross-generate\", action=\"store_true\", help=\"Cross-generate all combinations of configs\")\n     parser.add_argument(\"--num-tokens-to-profile\", \"-p\", type=int, default=0, help=\"Number of tokens to profile\")\n \n     parser.add_argument(\"--commit-id\", type=str, help=\"Git commit ID (if not provided, will auto-detect from git)\")\n@@ -69,42 +69,47 @@\n \n     # If there is only one (batch_size, sequence_length, num_tokens_to_generate), we benchmark across configs\n     elif len(args.batch_size) * len(args.sequence_length) * len(args.num_tokens_to_generate) == 1:\n-        benchmark_configs = generate_all_configs(\n+        if args.cross_generate:\n+            benchmark_configs = generate_all_configs(\n+                warmup_iterations=args.warmup,\n+                measurement_iterations=args.iterations,\n+                batch_size=args.batch_size[0],\n+                sequence_length=args.sequence_length[0],\n+                num_tokens_to_generate=args.num_tokens_to_generate[0],\n+            )\n+        else:\n+            benchmark_configs = generate_main_configs(\n+                warmup_iterations=args.warmup,\n+                measurement_iterations=args.iterations,\n+                batch_size=args.batch_size[0],\n+                sequence_length=args.sequence_length[0],\n+                num_tokens_to_generate=args.num_tokens_to_generate[0],\n+            )\n+\n+    # Otherwise, we benchmark across all combinations of dimensions\n+    else:\n+        main_config = generate_main_configs(\n             warmup_iterations=args.warmup,\n             measurement_iterations=args.iterations,\n             batch_size=args.batch_size[0],\n             sequence_length=args.sequence_length[0],\n             num_tokens_to_generate=args.num_tokens_to_generate[0],\n-        )\n-        random.shuffle(benchmark_configs)\n-\n-    # Otherwise, we benchmark across all combinations of dimensions\n-    else:\n-        kwargs = {\n-            \"warmup_iterations\": args.warmup,\n-            \"measurement_iterations\": args.iterations,\n-            \"gpu_monitoring\": False,\n-            \"batch_size\": args.batch_size[0],\n-            \"sequence_length\": args.sequence_length[0],\n-            \"num_tokens_to_generate\": args.num_tokens_to_generate[0],\n-            \"attn_implementation\": \"flex_attention\",\n-            \"sdpa_backend\": None,\n-            \"compile_mode\": \"default\",\n-            \"kernelize\": False,\n-        }\n+        )[0]\n         benchmark_configs = []\n         for num_tokens_to_generate in args.num_tokens_to_generate:\n             for sequence_length in args.sequence_length:\n                 for batch_size in args.batch_size:\n-                    kwargs[\"batch_size\"] = batch_size\n-                    kwargs[\"sequence_length\"] = sequence_length\n-                    kwargs[\"num_tokens_to_generate\"] = num_tokens_to_generate\n-                    benchmark_configs.append(BenchmarkConfig(**kwargs))\n+                    cfg_dict = main_config.to_dict()\n+                    cfg_dict[\"batch_size\"] = batch_size\n+                    cfg_dict[\"sequence_length\"] = sequence_length\n+                    cfg_dict[\"num_tokens_to_generate\"] = num_tokens_to_generate\n+                    cfg_dict.pop(\"name\")\n+                    benchmark_configs.append(BenchmarkConfig.from_dict(cfg_dict))\n \n     runner = BenchmarkRunner(logger, args.output_dir, args.commit_id)\n     results = runner.run_benchmarks(\n         args.model_id,\n-        benchmark_configs[:3],\n+        benchmark_configs,\n         args.num_tokens_to_profile,\n         pretty_print_summary=True,\n     )"
        }
    ],
    "stats": {
        "total": 165,
        "additions": 88,
        "deletions": 77
    }
}