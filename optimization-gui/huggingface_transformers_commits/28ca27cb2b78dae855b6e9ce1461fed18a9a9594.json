{
    "author": "qgallouedec",
    "message": "HF papers in doc (#40381)\n\n* HF papers\n\n* clean\n\n* Update src/transformers/models/gemma3n/configuration_gemma3n.py\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* style\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
    "files": [
        {
            "sha": "94ff4ae6d73287647c0ec8e902cf60852b73d993",
            "filename": "docs/source/en/model_doc/florence2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n # Florence-2\n \n-[Florence-2](https://arxiv.org/abs/2311.06242) is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks. Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages the FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model.\n+[Florence-2](https://huggingface.co/papers/2311.06242) is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks. Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages the FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model.\n \n You can find all the original Florence-2 checkpoints under the [Florence-2](https://huggingface.co/models?other=florence-2) collection.\n \n@@ -154,7 +154,7 @@ print(parsed_answer)\n \n ## Resources\n \n-- [Florence-2 technical report](https://arxiv.org/abs/2311.06242)\n+- [Florence-2 technical report](https://huggingface.co/papers/2311.06242)\n - [Jupyter Notebook for inference and visualization of Florence-2-large model](https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb)\n \n ## Florence2VisionConfig"
        },
        {
            "sha": "b43379cf3fd4c32bb69d3c853a63b2cf368b45bd",
            "filename": "docs/source/en/model_doc/gemma3n.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -202,5 +202,5 @@ echo -e \"Plants create energy through a process known as\" | transformers run --t\n [gemma3n-collection]: https://huggingface.co/collections/google/gemma-3n\n [laurel]: https://huggingface.co/papers/2411.07501\n [matformer]: https://huggingface.co/papers/2310.07707\n-[spark-transformer]: https://arxiv.org/abs/2506.06644\n+[spark-transformer]: https://huggingface.co/papers/2506.06644\n [usm]: https://huggingface.co/papers/2303.01037"
        },
        {
            "sha": "ab1d761f19ed91c3d25bfc8e2533cad9fae9f78b",
            "filename": "docs/source/en/model_doc/ovis2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The [Ovis2](https://github.com/AIDC-AI/Ovis) is an updated version of the [Ovis](https://arxiv.org/abs/2405.20797) model developed by the AIDC-AI team at Alibaba International Digital Commerce Group. \n+The [Ovis2](https://github.com/AIDC-AI/Ovis) is an updated version of the [Ovis](https://huggingface.co/papers/2405.20797) model developed by the AIDC-AI team at Alibaba International Digital Commerce Group. \n \n Ovis2 is the latest advancement in multi-modal large language models (MLLMs), succeeding Ovis1.6. It retains the architectural design of the Ovis series, which focuses on aligning visual and textual embeddings, and introduces major improvements in data curation and training methods.\n "
        },
        {
            "sha": "f94134609d2babebcb8c24c77acba1259b513616",
            "filename": "docs/source/en/model_doc/reformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Reformer model was proposed in the paper [Reformer: The Efficient Transformer](https://huggingface.co/papers/2001.04451.pdf) by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.\n+The Reformer model was proposed in the paper [Reformer: The Efficient Transformer](https://huggingface.co/papers/2001.04451) by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "49a58254630a6019a949071a08633ad8589be83e",
            "filename": "docs/source/en/model_doc/sam.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-SAM (Segment Anything Model) was proposed in [Segment Anything](https://huggingface.co/papers/2304.02643v1.pdf) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.\n+SAM (Segment Anything Model) was proposed in [Segment Anything](https://huggingface.co/papers/2304.02643) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.\n \n The model can be used to predict segmentation masks of any object of interest given an input image.\n "
        },
        {
            "sha": "349dcecf03ccb472171a8675c194e0c6cb0518df",
            "filename": "docs/source/en/model_doc/umt5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Fumt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Fumt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fumt5.md?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The UMT5 model was proposed in [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://arxiv.org/pdf/2304.09151) by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.\n+The UMT5 model was proposed in [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://huggingface.co/papers/2304.09151) by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "0593e9940bd68413bae0977228d986d02aa62dab",
            "filename": "docs/source/en/model_doc/xmod.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Fxmod.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Fmodel_doc%2Fxmod.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxmod.md?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The X-MOD model was proposed in [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](https://arxiv.org/abs/2205.06266) by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe.\n+The X-MOD model was proposed in [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](https://huggingface.co/papers/2205.06266) by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe.\n X-MOD extends multilingual masked language models like [XLM-R](xlm-roberta) to include language-specific modular components (_language adapters_) during pre-training. For fine-tuning, the language adapters in each transformer layer are frozen.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "873b09349feb148f8584a2bbf2cfa1596eabbb78",
            "filename": "docs/source/en/optimizers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Foptimizers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fen%2Foptimizers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Foptimizers.md?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -180,7 +180,7 @@ args = TrainingArguments(\n pip install torch-optimi\n ```\n \n-[StableAdamW](https://arxiv.org/pdf/2304.13013) is a hybrid between AdamW and AdaFactor. It ports AdaFactor's update clipping into AdamW, which removes the need for gradient clipping. Otherwise, it behaves as a drop-in replacement for AdamW.\n+[StableAdamW](https://huggingface.co/papers/2304.13013) is a hybrid between AdamW and AdaFactor. It ports AdaFactor's update clipping into AdamW, which removes the need for gradient clipping. Otherwise, it behaves as a drop-in replacement for AdamW.\n \n > [!TIP]\n > If training on large batch sizes or still observing training loss spikes, consider reducing beta_2 between [0.95, 0.99]."
        },
        {
            "sha": "6f9043b1cabaa7739e7920cfde7c9d63186703d7",
            "filename": "docs/source/ko/glossary.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fko%2Fglossary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fko%2Fglossary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fglossary.md?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -150,7 +150,7 @@ DataParallel 방식에 대해 더 알아보려면 [여기](perf_train_gpu_many#d\n \n 입력 크기가 `[batch_size, sequence_length]`일 경우, 중간 Feed Forward 임베딩\n `[batch_size, sequence_length, config.intermediate_size]`을 저장하는 데 필요한 메모리는 전체 메모리 사용량의 큰 부분을 차지할 수 있습니다.\n-[Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) 논문의 저자들은 이 연산이 `sequence_length` 차원에 대해 독립적이기 때문에,토큰마다 Feed Forward Layer의 출력 임베딩을 각 토큰별로 `[batch_size, config.hidden_size]`을 개별적으로 계산한 뒤, 이를 이어 붙여 `[batch_size, sequence_length, config.hidden_size]` 형태로 만들 수 있습니다.`n = sequence_length`. 이 방식은 계산 시간은 늘어나지만, 메모리 사용량은 줄어들게 됩니다.\n+[Reformer: The Efficient Transformer](https://huggingface.co/papers/2001.04451) 논문의 저자들은 이 연산이 `sequence_length` 차원에 대해 독립적이기 때문에,토큰마다 Feed Forward Layer의 출력 임베딩을 각 토큰별로 `[batch_size, config.hidden_size]`을 개별적으로 계산한 뒤, 이를 이어 붙여 `[batch_size, sequence_length, config.hidden_size]` 형태로 만들 수 있습니다.`n = sequence_length`. 이 방식은 계산 시간은 늘어나지만, 메모리 사용량은 줄어들게 됩니다.\n \n [`apply_chunking_to_forward`] 함수를 사용하는 모델의 경우, `chunk_size`는 병렬로 계산되는 출력 임베딩의 개수를 정의하며, 이는 메모리 사용량과 계산 시간 간의 트레이드오프를 결정합니다.\n `chunk_size`가 0으로 설정되면, 피드 포워드 청킹(Feed Forward Chunking)은 수행되지 않습니다.\n@@ -187,7 +187,7 @@ DataParallel 방식에 대해 더 알아보려면 [여기](perf_train_gpu_many#d\n \n <Youtube id=\"VFp38yj8h3A\"/>\n \n-토크나이저마다 작동 방식은 다르지만, 기본 메커니즘은 동일합니다. 다음은 [WordPiece](https://arxiv.org/pdf/1609.08144.pdf) 토크나이저인 BERT 토크나이저를 사용한 예시입니다:\n+토크나이저마다 작동 방식은 다르지만, 기본 메커니즘은 동일합니다. 다음은 [WordPiece](https://huggingface.co/papers/1609.08144) 토크나이저인 BERT 토크나이저를 사용한 예시입니다:\n \n ```python\n >>> from transformers import BertTokenizer"
        },
        {
            "sha": "2076901ee0fec40ac505ff62826bc9cbcd4944ae",
            "filename": "docs/source/ko/model_doc/exaone4.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fko%2Fmodel_doc%2Fexaone4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fko%2Fmodel_doc%2Fexaone4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fexaone4.md?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -27,7 +27,7 @@ EXAONE 4.0의 모델 구조는 이전 EXAONE 모델들과 다른 아키텍처 \n 1. **Hybrid Attention**: 32B 모델은 *Local attention (sliding window attention)*과 *Global attention (full attention)*을 3:1 비율로 연결한 hybrid attention 구조를 채택했습니다. 또한 전체 문맥을 더 잘 이해할 수 있도록 global attention에서 RoPE를 사용하지 않았습니다.\n 2. **QK-Reorder-Norm**: 더 나은 downstream tasks 성능을 위해 연산량의 증가를 감수하며 전통적으로 사용되고 있던 Pre-LN 방식을 변경했습니다. LayerNorm의 위치를 attention과 MLP의 출력에 적용되도록 재배치했고, Q와 K projection 직후에도 RMS normalization을 추가했습니다. \n \n-더 자세한 정보는 [기술 보고서](https://arxiv.org/abs/2507.11407), [HuggingFace 논문](https://huggingface.co/papers/2507.11407), [블로그](https://www.lgresearch.ai/blog/view?seq=576), [공식 GitHub](https://github.com/LG-AI-EXAONE/EXAONE-4.0) 페이지를 참고해주시길 바랍니다.\n+더 자세한 정보는 [기술 보고서](https://huggingface.co/papers/2507.11407), [HuggingFace 논문](https://huggingface.co/papers/2507.11407), [블로그](https://www.lgresearch.ai/blog/view?seq=576), [공식 GitHub](https://github.com/LG-AI-EXAONE/EXAONE-4.0) 페이지를 참고해주시길 바랍니다.\n \n 공개된 모든 모델 체크포인트는 [HuggingFace 콜렉션](https://huggingface.co/collections/LGAI-EXAONE/exaone-40-686b2e0069800c835ed48375)에서 확인할 수 있습니다.\n "
        },
        {
            "sha": "7b6fcc7b10165aacffe27353868985cb9f28048b",
            "filename": "docs/source/ko/optimizers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fko%2Foptimizers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fko%2Foptimizers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Foptimizers.md?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -180,7 +180,7 @@ args = TrainingArguments(\n pip install torch-optimi\n ```\n \n-[StableAdamW](https://arxiv.org/pdf/2304.13013)는 AdamW와 AdaFactor를 결합한 하이브리드 옵티마이저입니다. AdaFactor의 업데이트 클리핑(update clipping)이 AdamW에 도입되어 별도의 그래디언트 클리핑(gradient clipping)이 필요 없습니다. 그 외의 동작에서는 AdamW와 완벽히 호환되는 대체제로 사용할 수 있습니다.\n+[StableAdamW](https://huggingface.co/papers/2304.13013)는 AdamW와 AdaFactor를 결합한 하이브리드 옵티마이저입니다. AdaFactor의 업데이트 클리핑(update clipping)이 AdamW에 도입되어 별도의 그래디언트 클리핑(gradient clipping)이 필요 없습니다. 그 외의 동작에서는 AdamW와 완벽히 호환되는 대체제로 사용할 수 있습니다.\n \n > [!TIP]\n > 배치(batch) 크기가 크거나 훈련 손실(training loss)이 계속해서 급격하게 변동한다면, beta_2 값을 [0.95, 0.99] 사이로 줄여보세요."
        },
        {
            "sha": "e54b6aa574c6445d0cb2140e61cc09eb6c9e7ad0",
            "filename": "docs/source/ms/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fms%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/docs%2Fsource%2Fms%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fms%2Findex.md?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -204,7 +204,7 @@ Dokumentasi disusun kepada lima bahagian:\n 1. **[RoFormer](model_doc/roformer)** (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://huggingface.co/papers/2104.09864) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\n 1. **[RWKV](model_doc/rwkv)** (from Bo Peng), released on [this repo](https://github.com/BlinkDL/RWKV-LM) by Bo Peng.\n 1. **[SegFormer](model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://huggingface.co/papers/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\n-1. **[Segment Anything](model_doc/sam)** (from Meta AI) released with the paper [Segment Anything](https://huggingface.co/papers/2304.02643v1.pdf) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.\n+1. **[Segment Anything](model_doc/sam)** (from Meta AI) released with the paper [Segment Anything](https://huggingface.co/papers/2304.02643) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.\n 1. **[SEW](model_doc/sew)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://huggingface.co/papers/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n 1. **[SEW-D](model_doc/sew_d)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://huggingface.co/papers/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n 1. **[SpeechT5](model_doc/speecht5)** (from Microsoft Research) released with the paper [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://huggingface.co/papers/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei."
        },
        {
            "sha": "b603b69787aa9f46d1344f0749fc8a13e59f5ef5",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -1144,7 +1144,7 @@ def __init__(\n class QuantizedCache(Cache):\n     \"\"\"\n     A quantizer cache similar to what is described in the\n-    [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache paper](https://arxiv.org/abs/2402.02750).\n+    [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache paper](https://huggingface.co/papers/2402.02750).\n     It allows the model to generate longer sequence length without allocating too much memory for keys and values\n     by applying quantization.\n     The cache has two types of storage, one for original precision and one for the"
        },
        {
            "sha": "cdb0c3dcd03256b5a04fcbe31697ce9a6d232d28",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -972,7 +972,7 @@ def forward(\n     all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n     Kaiser and Illia Polosukhin.\n \n-    .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n+    .. _*Attention is all you need*: https://huggingface.co/papers/1706.03762\n     \"\"\"\n )\n class AltRobertaModel(AltCLIPPreTrainedModel):"
        },
        {
            "sha": "5793697311bdc5906836811387c8a47f9db48dd9",
            "filename": "src/transformers/models/arcee/configuration_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -54,7 +54,7 @@ class ArceeConfig(PretrainedConfig):\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"relu2\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "3a35ee8a1373e7df25ce0b2dea73856fd95ec3ec",
            "filename": "src/transformers/models/arcee/modular_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -60,7 +60,7 @@ class ArceeConfig(LlamaConfig):\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"relu2\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "f3a93fa198f2073ca0e34e613a7d9bed01f892d0",
            "filename": "src/transformers/models/doge/configuration_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -98,7 +98,7 @@ class DogeConfig(PretrainedConfig):\n             If `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used.\n             When converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed by meanpooling all the original heads within that group.\n-            For more details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf).\n+            For more details checkout [this paper](https://huggingface.co/papers/2305.13245).\n             If it is not specified, will default to `num_attention_heads`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention."
        },
        {
            "sha": "3281dd05638f718a837fdefffe2cccc24bf94cc6",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -605,7 +605,7 @@ def load_balancing_loss_func(\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n-    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n     function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n     experts is too unbalanced.\n "
        },
        {
            "sha": "c6db98a4745748090f3605cb5fcca09dcc0089f7",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -126,7 +126,7 @@ class DogeConfig(PretrainedConfig):\n             If `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used.\n             When converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed by meanpooling all the original heads within that group.\n-            For more details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf).\n+            For more details checkout [this paper](https://huggingface.co/papers/2305.13245).\n             If it is not specified, will default to `num_attention_heads`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n@@ -601,7 +601,7 @@ def load_balancing_loss_func(\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n-    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n     function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n     experts is too unbalanced.\n "
        },
        {
            "sha": "a24d1cd5bdeb05c20ea4ca5c7eb964212709f841",
            "filename": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -889,7 +889,7 @@ class EfficientLoFTRForKeypointMatching(EfficientLoFTRPreTrainedModel):\n \n     Yifan Wang, Xingyi He, Sida Peng, Dongli Tan and Xiaowei Zhou.\n     Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed\n-    In CVPR, 2024. https://arxiv.org/abs/2403.04765\n+    In CVPR, 2024. https://huggingface.co/papers/2403.04765\n     \"\"\"\n \n     def __init__(self, config: EfficientLoFTRConfig):"
        },
        {
            "sha": "d80d7d0ca381e7d3f9270ff3fc2d6eda66f51b14",
            "filename": "src/transformers/models/exaone4/configuration_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -50,7 +50,7 @@ class Exaone4Config(PretrainedConfig):\n             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "064a288b3b238a41084dfb30a6f1a409b32d438d",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -85,7 +85,7 @@ class Exaone4Config(PretrainedConfig):\n             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "b5f144bcc0ee9e6868ce614d04850ae04c8148e5",
            "filename": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -68,7 +68,7 @@ class Gemma3nTextConfig(PretrainedConfig):\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout this\n-            [paper](https://arxiv.org/pdf/2305.13245.pdf). If not specified, will default to `num_attention_heads`.\n+            [paper](https://huggingface.co/papers/2305.13245). If not specified, will default to `num_attention_heads`.\n         head_dim (`int`, *optional*, defaults to 256):\n             The attention head dimension.\n         hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):"
        },
        {
            "sha": "4acab69cb1bb4d2144d49bac347542a5159d1a61",
            "filename": "src/transformers/models/gemma3n/feature_extraction_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fgemma3n%2Ffeature_extraction_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fgemma3n%2Ffeature_extraction_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Ffeature_extraction_gemma3n.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -108,7 +108,7 @@ def _unfold(array: np.ndarray, dimension: int, size: int, step: int) -> np.ndarr\n \n \n class Gemma3nAudioFeatureExtractor(SequenceFeatureExtractor):\n-    \"\"\"An audio feature extractor Universal Speech Models https://arxiv.org/abs/2303.01037.\n+    \"\"\"An audio feature extractor Universal Speech Models https://huggingface.co/papers/2303.01037.\n \n     Args:\n         feature_size (`int`, *optional*, defaults to 128):"
        },
        {
            "sha": "765343f0d0ec05bac4513e77cc105188e432503a",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -907,7 +907,9 @@ def forward(self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTenso\n \n \n class Gemma3nAudioEncoder(PreTrainedModel):\n-    \"\"\"An audio encoder based on the [Universal Speech Model](https://arxiv.org/abs/2303.01037) architecture.\"\"\"\n+    \"\"\"\n+    An audio encoder based on the [Universal Speech Model](https://huggingface.co/papers/2303.01037) architecture.\n+    \"\"\"\n \n     config: Gemma3nAudioConfig\n "
        },
        {
            "sha": "c5e66dbf855462a7aff06419d43629f1c0db37e6",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -96,7 +96,7 @@ class Gemma3nTextConfig(Gemma2Config, PretrainedConfig):\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout this\n-            [paper](https://arxiv.org/pdf/2305.13245.pdf). If not specified, will default to `num_attention_heads`.\n+            [paper](https://huggingface.co/papers/2305.13245). If not specified, will default to `num_attention_heads`.\n         head_dim (`int`, *optional*, defaults to 256):\n             The attention head dimension.\n         hidden_activation (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n@@ -1476,7 +1476,9 @@ def forward(self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTenso\n \n \n class Gemma3nAudioEncoder(PreTrainedModel):\n-    \"\"\"An audio encoder based on the [Universal Speech Model](https://arxiv.org/abs/2303.01037) architecture.\"\"\"\n+    \"\"\"\n+    An audio encoder based on the [Universal Speech Model](https://huggingface.co/papers/2303.01037) architecture.\n+    \"\"\"\n \n     config: Gemma3nAudioConfig\n "
        },
        {
            "sha": "e311cd246c8e4e2c18f1032d1b24e91302dd47fe",
            "filename": "src/transformers/models/glm4v/configuration_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -147,7 +147,7 @@ class Glm4vTextConfig(PretrainedConfig):\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `32`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder.\n         max_position_embeddings (`int`, *optional*, defaults to 32768):"
        },
        {
            "sha": "f76e04fc89c241adeadfd32faebf92d1443f775f",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -188,7 +188,7 @@ class Glm4vTextConfig(PretrainedConfig):\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `32`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder.\n         max_position_embeddings (`int`, *optional*, defaults to 32768):"
        },
        {
            "sha": "367155e7dd216bbafa2f54137d86b57cccf259d8",
            "filename": "src/transformers/models/glm4v_moe/configuration_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -148,7 +148,7 @@ class Glm4vMoeTextConfig(PretrainedConfig):\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `32`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder.\n         max_position_embeddings (`int`, *optional*, defaults to 65536):"
        },
        {
            "sha": "ad51eb4fcc926f238c23b51eb9c0b85f6def6c11",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -77,7 +77,7 @@ class Glm4vMoeTextConfig(Glm4MoeConfig, nn.Module):\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `32`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder.\n         max_position_embeddings (`int`, *optional*, defaults to 65536):"
        },
        {
            "sha": "064b0a9702ccc1a1bd38dfe889bd0eb88291fac0",
            "filename": "src/transformers/models/hunyuan_v1_dense/configuration_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -50,7 +50,7 @@ class HunYuanDenseV1Config(PretrainedConfig):\n             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "386ddac1d3ebb37de330a3940ad5ac556be5bcf6",
            "filename": "src/transformers/models/hunyuan_v1_moe/configuration_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -52,7 +52,7 @@ class HunYuanMoEV1Config(PretrainedConfig):\n             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n             `num_attention_heads`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder."
        },
        {
            "sha": "9fc3bd586959a0cb4d9c147c0623f55d58d09037",
            "filename": "src/transformers/models/kosmos2_5/configuration_kosmos2_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fconfiguration_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fconfiguration_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fconfiguration_kosmos2_5.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -56,8 +56,8 @@ class Kosmos2_5TextConfig(PretrainedConfig):\n         activation_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for activations inside the fully connected layer.\n         layerdrop (`float`, *optional*, defaults to 0.0):\n-            The LayerDrop probability for the decoder. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)\n-            for more details.\n+            The LayerDrop probability for the decoder. See the [LayerDrop paper](see\n+            https://huggingface.co/papers/1909.11556) for more details.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-5):\n             The epsilon used by the layer normalization layers.\n         init_std (`float`, *optional*, defaults to 0.02):"
        },
        {
            "sha": "e679050d337f251d4cdd7db199f1d53c6af686c7",
            "filename": "src/transformers/models/kosmos2_5/image_processing_kosmos2_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -88,7 +88,7 @@ class Kosmos2_5ImageProcessor(BaseImageProcessor):\n             The patch size to use for the image. According to Kosmos2_5 paper and code, the patch size is 16x16.\n         max_patches (`int`, *optional*, defaults to 4096):\n             The maximum number of patches to extract from the image as per the\n-            [KOSMOS 2.5 paper](https://arxiv.org/pdf/2309.11419).\n+            [KOSMOS 2.5 paper](https://huggingface.co/papers/2309.11419).\n     \"\"\"\n \n     model_input_names = [\"flattened_patches\"]"
        },
        {
            "sha": "7b9613ed0074f52d27ee6f437feab39b666b6ffe",
            "filename": "src/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -65,7 +65,7 @@ class Kosmos2_5FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         The patch size to use for the image. According to Kosmos2_5 paper and code, the patch size is 16x16.\n     max_patches (`int`, *optional*, defaults to 4096):\n         The maximum number of patches to extract from the image as per the\n-        [KOSMOS 2.5 paper](https://arxiv.org/pdf/2309.11419).\n+        [KOSMOS 2.5 paper](https://huggingface.co/papers/2309.11419).\n     \"\"\"\n \n     patch_size: Optional[dict[str, int]]\n@@ -93,7 +93,7 @@ def preprocess(self, images: ImageInput, **kwargs: Unpack[Kosmos2_5FastImageProc\n             The patch size to use for the image. According to Kosmos2_5 paper and code, the patch size is 16x16.\n         max_patches (`int`, *optional*, defaults to 4096):\n             The maximum number of patches to extract from the image as per the\n-            [KOSMOS 2.5 paper](https://arxiv.org/pdf/2309.11419).\n+            [KOSMOS 2.5 paper](https://huggingface.co/papers/2309.11419).\n         \"\"\"\n         # return super().preprocess(images, **kwargs)\n         # TODO: revert once the issue is fixed: https://huggingface.slack.com/archives/C02TXKQQLE5/p1743411133979019"
        },
        {
            "sha": "8693fd66679d6e6a747eaf30627d65d8f48a0ca7",
            "filename": "src/transformers/models/kyutai_speech_to_text/configuration_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -51,7 +51,8 @@ class KyutaiSpeechToTextConfig(PretrainedConfig):\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `num_attention_heads`.\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n         max_position_embeddings (`int`, *optional*, defaults to 750):\n             The maximum sequence length that this model might ever be used with. Typically, set this to something large\n             just in case (e.g., 512 or 1024 or 2048)."
        },
        {
            "sha": "a42e97631be0626dc96f015e94241b431efea3fc",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -143,7 +143,7 @@ def _init_weights(self, module):\n class KyutaiSpeechToTextConv1dPaddingCache:\n     \"\"\"\n     Padding cache for KyutaiSpeechToTextConv1d causal convolutions in order to support streaming via cache padding.\n-    See: https://arxiv.org/pdf/2005.06720 & https://arxiv.org/pdf/2204.07064\n+    See: https://huggingface.co/papers/2005.06720 & https://huggingface.co/papers/2204.07064\n \n     A padding cache is a list of cached partial hidden states for each convolution layer.\n     Hidden states are cached from the previous call to the KyutaiSpeechToTextConv1d forward pass, given the padding size."
        },
        {
            "sha": "90e8d41b45156932278872ddd61432e18330db7c",
            "filename": "src/transformers/models/lightglue/configuration_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -49,7 +49,7 @@ class LightGlueConfig(PretrainedConfig):\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n             `num_attention_heads`.\n         depth_confidence (`float`, *optional*, defaults to 0.95):\n             The confidence threshold used to perform early stopping"
        },
        {
            "sha": "2a94acf1c0b22e4497eafd8fcf831151e901af9c",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -500,7 +500,7 @@ class LightGlueForKeypointMatching(LightGluePreTrainedModel):\n     The correspondence ids use -1 to indicate non-matching points.\n \n     Philipp Lindenberger, Paul-Edouard Sarlin and Marc Pollefeys. LightGlue: Local Feature Matching at Light Speed.\n-    In ICCV 2023. https://arxiv.org/pdf/2306.13643.pdf\n+    In ICCV 2023. https://huggingface.co/papers/2306.13643\n     \"\"\"\n \n     def __init__(self, config: LightGlueConfig):"
        },
        {
            "sha": "ce18ff0d6f504a09b8323c4d22cf44fde76d6b34",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -68,7 +68,7 @@ class LightGlueConfig(PretrainedConfig):\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n             `num_attention_heads`.\n         depth_confidence (`float`, *optional*, defaults to 0.95):\n             The confidence threshold used to perform early stopping\n@@ -658,7 +658,7 @@ class LightGlueForKeypointMatching(LightGluePreTrainedModel):\n     The correspondence ids use -1 to indicate non-matching points.\n \n     Philipp Lindenberger, Paul-Edouard Sarlin and Marc Pollefeys. LightGlue: Local Feature Matching at Light Speed.\n-    In ICCV 2023. https://arxiv.org/pdf/2306.13643.pdf\n+    In ICCV 2023. https://huggingface.co/papers/2306.13643\n     \"\"\"\n \n     def __init__(self, config: LightGlueConfig):"
        },
        {
            "sha": "c39a7639bdda471d2526a555ede69701699771d2",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -77,7 +77,7 @@ class MimiOutput(ModelOutput):\n class MimiConv1dPaddingCache:\n     \"\"\"\n     Padding cache for MimiConv1d causal convolutions in order to support streaming via cache padding.\n-    See: https://arxiv.org/pdf/2005.06720 & https://arxiv.org/pdf/2204.07064\n+    See: https://huggingface.co/papers/2005.06720 & https://huggingface.co/papers/2204.07064\n \n     A padding cache is a list of cached partial hidden states for each convolution layer.\n     Hidden states are cached from the previous call to the MimiConv1d forward pass, given the padding size."
        },
        {
            "sha": "2aeeea04c6f315d6460a3eddedef178457849125",
            "filename": "src/transformers/models/sam2/modeling_sam2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -591,7 +591,7 @@ def __init__(self, config: Sam2HieraDetConfig):\n         super().__init__(config)\n \n         self.patch_embed = Sam2PatchEmbeddings(config)\n-        # Windowed positional embedding (https://arxiv.org/abs/2311.05613)\n+        # Windowed positional embedding (https://huggingface.co/papers/2311.05613)\n         self.pos_embed = nn.Parameter(\n             torch.zeros(1, config.hidden_size, *config.window_positional_embedding_background_size)\n         )"
        },
        {
            "sha": "debaf3cb40cf3f5e106fffd5078917dca7f19569",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -706,7 +706,7 @@ def __init__(self, config: Sam2HieraDetConfig):\n         super().__init__(config)\n \n         self.patch_embed = Sam2PatchEmbeddings(config)\n-        # Windowed positional embedding (https://arxiv.org/abs/2311.05613)\n+        # Windowed positional embedding (https://huggingface.co/papers/2311.05613)\n         self.pos_embed = nn.Parameter(\n             torch.zeros(1, config.hidden_size, *config.window_positional_embedding_background_size)\n         )"
        },
        {
            "sha": "fd1861a589f953b38982053235f79ceba7b9cc30",
            "filename": "src/transformers/models/smollm3/configuration_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -51,7 +51,7 @@ class SmolLM3Config(PretrainedConfig):\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `16`.\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `16`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder.\n         max_position_embeddings (`int`, *optional*, defaults to 32768):"
        },
        {
            "sha": "66d58f07a372df2f720f9940aba9902c23e74e66",
            "filename": "src/transformers/models/smollm3/modular_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -70,7 +70,7 @@ class SmolLM3Config(PretrainedConfig):\n             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n             by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `16`.\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `16`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n             The non-linear activation function (function or string) in the decoder.\n         max_position_embeddings (`int`, *optional*, defaults to 32768):"
        },
        {
            "sha": "233653a36b39c58d3edcc984cefec9f8e44961be",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -65,7 +65,7 @@ class UniSpeechForPreTrainingOutput(ModelOutput):\n     r\"\"\"\n     loss (*optional*, returned when model is in train mode, `torch.FloatTensor` of shape `(1,)`):\n         Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the [official\n-        paper](https://arxiv.org/pdf/2006.11477.pdf) . (classification) loss.\n+        paper](https://huggingface.co/papers/2006.11477).\n     projected_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n         Hidden-states of the model projected to *config.proj_codevector_dim* that can be used to predict the masked\n         projected quantized states."
        },
        {
            "sha": "5ab58ff187485e1bfcacd01fbab527bd75089573",
            "filename": "src/transformers/models/unispeech/modular_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -52,7 +52,7 @@ class UniSpeechForPreTrainingOutput(ModelOutput):\n     r\"\"\"\n     loss (*optional*, returned when model is in train mode, `torch.FloatTensor` of shape `(1,)`):\n         Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the [official\n-        paper](https://arxiv.org/pdf/2006.11477.pdf) . (classification) loss.\n+        paper](https://huggingface.co/papers/2006.11477).\n     projected_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n         Hidden-states of the model projected to *config.proj_codevector_dim* that can be used to predict the masked\n         projected quantized states."
        },
        {
            "sha": "c1281f7955afa8ee7c93b1248500ee17815269de",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -67,7 +67,7 @@ class UniSpeechSatForPreTrainingOutput(ModelOutput):\n     r\"\"\"\n     loss (*optional*, returned when model is in train mode, `torch.FloatTensor` of shape `(1,)`):\n         Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the [official\n-        paper](https://arxiv.org/pdf/2006.11477.pdf) . (classification) loss.\n+        paper](https://huggingface.co/papers/2006.11477).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`, *optional*):\n         Prediction scores of the contrastive loss model, i.e. the output of the model before the final softmax.\n     projected_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):"
        },
        {
            "sha": "ff1a2fefe4e5344d35a7e27df572263e1d42f3ac",
            "filename": "src/transformers/models/unispeech_sat/modular_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -57,7 +57,7 @@ class UniSpeechSatForPreTrainingOutput(ModelOutput):\n     r\"\"\"\n     loss (*optional*, returned when model is in train mode, `torch.FloatTensor` of shape `(1,)`):\n         Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the [official\n-        paper](https://arxiv.org/pdf/2006.11477.pdf) . (classification) loss.\n+        paper](https://huggingface.co/papers/2006.11477).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`, *optional*):\n         Prediction scores of the contrastive loss model, i.e. the output of the model before the final softmax.\n     projected_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):"
        },
        {
            "sha": "99db1d37fa55ddc9eaa795e6cc019a00717309f4",
            "filename": "src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -79,7 +79,7 @@ class FlaxWav2Vec2ForPreTrainingOutput(ModelOutput):\n     Args:\n         loss (*optional*, returned when model is in train mode, `jnp.ndarray` of shape `(1,)`):\n             Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the [official\n-            paper](https://huggingface.co/papers/2006.11477) . (classification) loss.\n+            paper](https://huggingface.co/papers/2006.11477).\n         projected_states (`jnp.ndarray` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n             Hidden-states of the model projected to *config.proj_codevector_dim* that can be used to predict the masked\n             projected quantized states."
        },
        {
            "sha": "90760d290e825e374186295eb2de59a5999f61f8",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -85,7 +85,7 @@ class Wav2Vec2ForPreTrainingOutput(ModelOutput):\n     r\"\"\"\n     loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n         Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the [official\n-        paper](https://arxiv.org/pdf/2006.11477.pdf) . (classification) loss.\n+        paper](https://huggingface.co/papers/2006.11477).\n     projected_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n         Hidden-states of the model projected to *config.proj_codevector_dim* that can be used to predict the masked\n         projected quantized states.\n@@ -95,9 +95,9 @@ class Wav2Vec2ForPreTrainingOutput(ModelOutput):\n     codevector_perplexity (`torch.FloatTensor` of shape `(1,)`):\n         The perplexity of the codevector distribution, used to measure the diversity of the codebook.\n     contrastive_loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n-        The contrastive loss (L_m) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .\n+        The contrastive loss (L_m) as stated in the [official paper](https://huggingface.co/papers/2006.11477).\n     diversity_loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n-        The diversity loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .\n+        The diversity loss (L_d) as stated in the [official paper](https://huggingface.co/papers/2006.11477).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "b786e415546e8dd61ce7824ab2e4627621c51d8c",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -45,7 +45,7 @@ class Wav2Vec2ConformerForPreTrainingOutput(ModelOutput):\n     r\"\"\"\n     loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n         Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the [official\n-        paper](https://arxiv.org/pdf/2006.11477.pdf) . (classification) loss.\n+        paper](https://huggingface.co/papers/2006.11477).\n     projected_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n         Hidden-states of the model projected to *config.proj_codevector_dim* that can be used to predict the masked\n         projected quantized states.\n@@ -55,9 +55,9 @@ class Wav2Vec2ConformerForPreTrainingOutput(ModelOutput):\n     codevector_perplexity (`torch.FloatTensor` of shape `(1,)`):\n         The perplexity of the codevector distribution, used to measure the diversity of the codebook.\n     contrastive_loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n-        The contrastive loss (L_m) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .\n+        The contrastive loss (L_m) as stated in the [official paper](https://huggingface.co/papers/2006.11477).\n     diversity_loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n-        The diversity loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .\n+        The diversity loss (L_d) as stated in the [official paper](https://huggingface.co/papers/2006.11477).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "dc3a937e56ba55d094d029e1adb7ee524e9e6337",
            "filename": "src/transformers/models/wav2vec2_conformer/modular_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -45,7 +45,7 @@ class Wav2Vec2ConformerForPreTrainingOutput(ModelOutput):\n     r\"\"\"\n     loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n         Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the [official\n-        paper](https://arxiv.org/pdf/2006.11477.pdf) . (classification) loss.\n+        paper](https://huggingface.co/papers/2006.11477).\n     projected_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n         Hidden-states of the model projected to *config.proj_codevector_dim* that can be used to predict the masked\n         projected quantized states.\n@@ -55,9 +55,9 @@ class Wav2Vec2ConformerForPreTrainingOutput(ModelOutput):\n     codevector_perplexity (`torch.FloatTensor` of shape `(1,)`):\n         The perplexity of the codevector distribution, used to measure the diversity of the codebook.\n     contrastive_loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n-        The contrastive loss (L_m) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .\n+        The contrastive loss (L_m) as stated in the [official paper](https://huggingface.co/papers/2006.11477).\n     diversity_loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n-        The diversity loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .\n+        The diversity loss (L_d) as stated in the [official paper](https://huggingface.co/papers/2006.11477).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "8909162db7240a9cea6c67c53a69ce3b781b9a96",
            "filename": "src/transformers/models/xcodec/modeling_xcodec.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -267,7 +267,7 @@ def decode(self, embed_ind):\n \n class XcodecResidualVectorQuantization(nn.Module):\n     \"\"\"\n-    Residual vector quantization implementation. Follows Algorithm 1 in https://arxiv.org/pdf/2107.03312.pdf\n+    Residual vector quantization implementation. Follows Algorithm 1 in https://huggingface.co/papers/2107.03312\n     \"\"\"\n \n     def __init__(self, config: XcodecConfig):"
        },
        {
            "sha": "820baf06b5a9ed607838cac9c8b45aedc1fe0582",
            "filename": "src/transformers/models/xlstm/modeling_xlstm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28ca27cb2b78dae855b6e9ce1461fed18a9a9594/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py?ref=28ca27cb2b78dae855b6e9ce1461fed18a9a9594",
            "patch": "@@ -51,7 +51,7 @@ def soft_cap(values: torch.Tensor, cap_value: Optional[Union[float, torch.Tensor\n \n         Performs a tanh operation on the logits and scales the result to the cap value. Common technique in attention\n         and output language heads to prevent large logits from dominating the softmax. See for example Gemma2:\n-        https://arxiv.org/abs/2408.00118\n+        https://huggingface.co/papers/2408.00118\n \n         Args:\n             values: The tensor to cap."
        }
    ],
    "stats": {
        "total": 141,
        "additions": 73,
        "deletions": 68
    }
}