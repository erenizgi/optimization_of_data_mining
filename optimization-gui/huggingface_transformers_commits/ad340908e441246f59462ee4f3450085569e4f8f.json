{
    "author": "falconlee236",
    "message": "Fix warning message for PEFT models in text-generation pipeline #36783 (#36887)\n\n* add peft model in constant\n\n* add test\n\n* fix formating\n\n* make fixup execute\n\n* change code\n\n* check by self.task\n\n* add test\n\n* fixup test code\n\n* fix minor typo\n\n* fix pipeline test\n\n* apply maintainers reqests",
    "sha": "ad340908e441246f59462ee4f3450085569e4f8f",
    "files": [
        {
            "sha": "459fd283ca8bed5ab517d08568d4ca70c9791e53",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad340908e441246f59462ee4f3450085569e4f8f/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad340908e441246f59462ee4f3450085569e4f8f/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=ad340908e441246f59462ee4f3450085569e4f8f",
            "patch": "@@ -849,6 +849,22 @@ def build_pipeline_init_args(\n     supports_binary_output=True,\n )\n \n+SUPPORTED_PEFT_TASKS = {\n+    \"document-question-answering\": [\"PeftModelForQuestionAnswering\"],\n+    \"feature-extraction\": [\"PeftModelForFeatureExtraction\", \"PeftModel\"],\n+    \"question-answering\": [\"PeftModelForQuestionAnswering\"],\n+    \"summarization\": [\"PeftModelForSeq2SeqLM\"],\n+    \"table-question-answering\": [\"PeftModelForQuestionAnswering\"],\n+    \"text2text-generation\": [\"PeftModelForSeq2SeqLM\"],\n+    \"text-classification\": [\"PeftModelForSequenceClassification\"],\n+    \"sentiment-analysis\": [\"PeftModelForSequenceClassification\"],\n+    \"text-generation\": [\"PeftModelForCausalLM\"],\n+    \"token-classification\": [\"PeftModelForTokenClassification\"],\n+    \"ner\": [\"PeftModelForTokenClassification\"],\n+    \"translation\": [\"PeftModelForSeq2SeqLM\"],\n+    \"translation_xx_to_yy\": [\"PeftModelForSeq2SeqLM\"],\n+    \"zero-shot-classification\": [\"PeftModelForSequenceClassification\"],\n+}\n \n if is_torch_available():\n     from transformers.pipelines.pt_utils import (\n@@ -1209,6 +1225,9 @@ def check_model_type(self, supported_models: Union[List[str], dict]):\n         \"\"\"\n         if not isinstance(supported_models, list):  # Create from a model mapping\n             supported_models_names = []\n+            if self.task in SUPPORTED_PEFT_TASKS:\n+                supported_models_names.extend(SUPPORTED_PEFT_TASKS[self.task])\n+\n             for _, model_name in supported_models.items():\n                 # Mapping can now contain tuples of models for the same configuration.\n                 if isinstance(model_name, tuple):"
        },
        {
            "sha": "203124439ddfbe0da206822cf3e3a7321389c739",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad340908e441246f59462ee4f3450085569e4f8f/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad340908e441246f59462ee4f3450085569e4f8f/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=ad340908e441246f59462ee4f3450085569e4f8f",
            "patch": "@@ -814,3 +814,50 @@ def tokenize_function(examples):\n                     msg = \"When using prompt learning PEFT methods such as PREFIX_TUNING\"\n                     with self.assertRaisesRegex(RuntimeError, msg):\n                         trainer.train()\n+\n+    def test_peft_pipeline_no_warning(self):\n+        \"\"\"\n+        Test to verify that the warning message \"The model 'PeftModel' is not supported for text-generation\"\n+        does not appear when using PeftModel with text-generation pipeline.\n+        \"\"\"\n+        from peft import PeftModel\n+\n+        from transformers import pipeline\n+\n+        ADAPTER_PATH = \"peft-internal-testing/tiny-OPTForCausalLM-lora\"\n+        BASE_PATH = \"hf-internal-testing/tiny-random-OPTForCausalLM\"\n+\n+        # Input text for testing\n+        text = \"Who is a Elon Musk?\"\n+        expected_error_msg = \"The model 'PeftModel' is not supported for text-generation\"\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            BASE_PATH,\n+            device_map=\"auto\",\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(BASE_PATH)\n+\n+        lora_model = PeftModel.from_pretrained(\n+            model,\n+            ADAPTER_PATH,\n+            device_map=\"auto\",\n+        )\n+\n+        # Create pipeline with PEFT model while capturing log output\n+        # Check that the warning message is not present in the logs\n+        pipeline_logger = logging.get_logger(\"transformers.pipelines.base\")\n+        with self.assertNoLogs(pipeline_logger, logging.ERROR) as cl:\n+            lora_generator = pipeline(\n+                task=\"text-generation\",\n+                model=lora_model,\n+                tokenizer=tokenizer,\n+                max_length=10,\n+            )\n+\n+            # Generate text to verify pipeline works\n+            _ = lora_generator(text)\n+\n+        # Check that the warning message is not present in the logs\n+        self.assertNotIn(\n+            expected_error_msg, cl.out, f\"Error message '{expected_error_msg}' should not appear when using PeftModel\"\n+        )"
        }
    ],
    "stats": {
        "total": 66,
        "additions": 66,
        "deletions": 0
    }
}