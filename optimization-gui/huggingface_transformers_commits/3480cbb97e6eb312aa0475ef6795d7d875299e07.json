{
    "author": "xenova",
    "message": "Only cast `cu_seqlens` when tracing (#35016)\n\n* Only cast `cu_seqlens` when tracing\n\n* Formatting",
    "sha": "3480cbb97e6eb312aa0475ef6795d7d875299e07",
    "files": [
        {
            "sha": "f7648f4a53d1afb94cb1debf22d9f7b28dbf8b33",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3480cbb97e6eb312aa0475ef6795d7d875299e07/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3480cbb97e6eb312aa0475ef6795d7d875299e07/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=3480cbb97e6eb312aa0475ef6795d7d875299e07",
            "patch": "@@ -1025,7 +1025,12 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         rotary_pos_emb = self.rot_pos_emb(grid_thw)\n \n         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(\n-            dim=0, dtype=grid_thw.dtype\n+            dim=0,\n+            # Select dtype based on the following factors:\n+            #  - FA2 requires that cu_seqlens_q must have dtype int32\n+            #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw\n+            # See https://github.com/huggingface/transformers/pull/34852 for more information\n+            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n         )\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n "
        }
    ],
    "stats": {
        "total": 7,
        "additions": 6,
        "deletions": 1
    }
}