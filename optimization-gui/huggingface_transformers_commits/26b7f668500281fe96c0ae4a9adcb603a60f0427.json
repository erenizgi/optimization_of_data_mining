{
    "author": "philiproeleveld",
    "message": "Add `logits_to_keep` to many older CausalLM models (#41335)\n\n* Add logits_to_keep to CausalLM models\n\n* Skip failing test for git model\n\n* Remove unused return_dict from kosmos2 signature\n\n* Revert BlipForQuestionAnswering",
    "sha": "26b7f668500281fe96c0ae4a9adcb603a60f0427",
    "files": [
        {
            "sha": "6e0a9b51ea691e8a7267fe3ba253722f54ea2f93",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1553,6 +1553,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1598,7 +1599,10 @@ def forward(\n             cache_position=cache_position,\n         )\n \n-        logits = self.lm_head(outputs[0])\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "444753bef63e9fda30d4ef960dc8918668ad8be8",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -900,6 +900,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -911,7 +912,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.bert(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.bert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -926,16 +927,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.cls(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.cls(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(prediction_scores, labels, self.config.vocab_size, **kwargs)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "5967774905a13ad639a23ff74bb385f76f1648a0",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -685,6 +685,7 @@ def forward(\n         past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -714,7 +715,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.bert(\n+        outputs: BaseModelOutputWithPastAndCrossAttentions = self.bert(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -728,21 +729,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.lm_head(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "3b2d5fcf797ac17ff7b053c9419304210205cdd7",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -2179,6 +2179,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[CausalLMOutputWithCrossAttentions, tuple[torch.FloatTensor]]:\n         r\"\"\"\n@@ -2206,25 +2207,22 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.cls(sequence_output)\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.cls(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n+            output = (logits,) + outputs[2:]\n+            return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "3aad7fb0167b091a3a4a28466d8459d5613cf434",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -2661,6 +2661,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -2705,7 +2706,10 @@ def forward(\n             cache_position=cache_position,\n         )\n \n-        logits = self.lm_head(outputs[0])\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "67bca4bae7ed97e433bcf3f306c666eb7e1d7620",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -541,6 +541,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -565,25 +566,22 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.output_projection(sequence_output)\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.output_projection(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n-            output = (prediction_scores,) + outputs[1:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n+            output = (logits,) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "f267d9fc10ca3f33d4b2d7e940c45cc879568bc6",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -363,6 +363,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -387,25 +388,22 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.output_projection(sequence_output)\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.output_projection(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n-            output = (prediction_scores,) + outputs[1:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n+            output = (logits,) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "8faa86b1fd2bf5adbaffdce9df5f456296145e20",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1224,6 +1224,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1269,7 +1270,10 @@ def forward(\n             cache_position=cache_position,\n         )\n \n-        logits = self.lm_head(outputs[0])\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "675df2cd49ebe25c58cc38e36f3e1211ea7166bb",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1184,6 +1184,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1229,7 +1230,10 @@ def forward(\n             cache_position=cache_position,\n         )\n \n-        logits = self.lm_head(outputs[0])\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "ab302663401fd9011c11b79b4a4b42b353c7a6c2",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -826,6 +826,7 @@ def forward(\n         attention_mask: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         interpolate_pos_encoding: bool = False,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BlipForConditionalGenerationModelOutput]:\n         r\"\"\"\n@@ -862,6 +863,7 @@ def forward(\n             encoder_hidden_states=image_embeds,\n             labels=labels,\n             reduction=\"mean\",\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "ee67f77d52414a897f9eb28abceaa7c5cdfaca6e",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -784,6 +784,7 @@ def forward(\n         is_decoder: Optional[bool] = True,\n         reduction: Optional[str] = \"mean\",\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states (`torch.FloatTensor`, *optional*): Sequence of\n@@ -827,8 +828,10 @@ def forward(\n             cache_position=cache_position,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.cls(sequence_output)\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        prediction_scores = self.cls(hidden_states[:, slice_indices, :])\n \n         if return_logits:\n             return prediction_scores[:, :-1, :].contiguous()"
        },
        {
            "sha": "d5f0ef26610ea5471c2050e8b378132f262f659e",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -823,6 +823,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **deprecated_arguments,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -867,29 +868,28 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n         )\n-        hidden_states = transformer_outputs[0]\n \n-        lm_logits = self.lm_head(hidden_states)\n+        hidden_states = transformer_outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(lm_logits.device)\n-            # Flatten the tokens\n             loss = self.loss_function(\n-                lm_logits,\n+                logits,\n                 labels,\n                 vocab_size=self.config.vocab_size,\n                 num_items_in_batch=num_items_in_batch,\n             )\n \n         if not return_dict:\n-            output = (lm_logits,) + transformer_outputs[1:]\n+            output = (logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithCrossAttentions(\n             loss=loss,\n-            logits=lm_logits,\n+            logits=logits,\n             past_key_values=transformer_outputs.past_key_values,\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,"
        },
        {
            "sha": "26897520a2c71f9099a054f9f056839b4de7d581",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 10,
            "deletions": 14,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1226,6 +1226,7 @@ def forward(\n         past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -1262,7 +1263,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.roberta(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -1277,23 +1278,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.lm_head(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(prediction_scores.device)\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "eb83629ccc4e29a861c7f7245e00d1fe64232dfc",
            "filename": "src/transformers/models/camembert/modular_camembert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 14,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -21,6 +21,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...modeling_outputs import (\n+    BaseModelOutputWithPoolingAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n     MaskedLMOutput,\n     MultipleChoiceModelOutput,\n@@ -445,6 +446,7 @@ def forward(\n         past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -481,7 +483,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.roberta(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -496,23 +498,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.lm_head(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(prediction_scores.device)\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "845e170325ce12255bf60c7be2132d5df41d9158",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1061,6 +1061,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1111,7 +1112,9 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         # Disallow image tokens which does not include special begin-image and end-image tokens\n         image_tokens = self.model.vocabulary_mapping.image_tokens"
        },
        {
            "sha": "8bb5bc9bda9537f201f92ab413e9eed7ebd96dd1",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 8,
            "deletions": 18,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -585,6 +585,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -612,34 +613,23 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n         )\n-        hidden_states = transformer_outputs[0]\n \n-        # make sure sampling in fp16 works correctly and\n-        # compute loss in fp32 to match with mesh-tf version\n-        # https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179\n-        lm_logits = self.lm_head(hidden_states).to(torch.float32)\n+        hidden_states = transformer_outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(lm_logits.device)\n-            # Flatten the tokens\n-            loss = self.loss_function(\n-                lm_logits,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n-\n-            loss = loss.to(hidden_states.dtype)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n-            output = (lm_logits,) + transformer_outputs[1:]\n+            output = (logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithPast(\n             loss=loss,\n-            logits=lm_logits,\n+            logits=logits,\n             past_key_values=transformer_outputs.past_key_values,\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,"
        },
        {
            "sha": "fbc64d4b141f280ea0ee17643f037fe7af6620d4",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -722,6 +722,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         attention_mask: Optional[torch.Tensor] = None,  # dummy parameter for text-generation pipeline\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -763,8 +764,9 @@ def forward(\n             cache_position,\n         )\n         hidden_states = model_output.last_hidden_state if return_dict else model_output[0]\n-\n-        logits = self.lm_head(hidden_states)\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "945ba0431c25cd8999a336511c0dc8ebd548b3ac",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -409,6 +409,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -459,25 +460,26 @@ def forward(\n         )\n \n         hidden_states = transformer_outputs[0]\n-\n-        lm_logits = self.lm_head(hidden_states)\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(\n-                lm_logits,\n+                logits,\n                 labels,\n                 vocab_size=self.config.vocab_size,\n                 **kwargs,\n             )\n \n         if not return_dict:\n-            output = (lm_logits,) + transformer_outputs[1:]\n+            output = (logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithPast(\n             loss=loss,\n-            logits=lm_logits,\n+            logits=logits,\n             past_key_values=transformer_outputs.past_key_values,\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,"
        },
        {
            "sha": "1ef12699360cf6ab9eb291066edd1ee371cbb74c",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -797,6 +797,7 @@ def forward(\n         past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -824,7 +825,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.data2vec_text(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.data2vec_text(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -839,21 +840,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.lm_head(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "1c91e50db8c7db4d961915a1eeeff5de59a66d13",
            "filename": "src/transformers/models/data2vec/modular_data2vec_text.py",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -22,6 +22,7 @@\n \n from ...generation import GenerationMixin\n from ...modeling_outputs import (\n+    BaseModelOutputWithPoolingAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n     MaskedLMOutput,\n     MultipleChoiceModelOutput,\n@@ -153,6 +154,7 @@ def forward(\n         past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -180,7 +182,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.data2vec_text(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.data2vec_text(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -195,21 +197,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.lm_head(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "cb915277f6bb2747a8b844ea1f33a71bbed0c29d",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1339,6 +1339,7 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -1366,7 +1367,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.electra(\n+        outputs: BaseModelOutputWithPastAndCrossAttentions = self.electra(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -1381,21 +1382,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.generator_lm_head(self.generator_predictions(sequence_output))\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.generator_lm_head(self.generator_predictions(hidden_states[:, slice_indices, :]))\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "b45e56d587c058ecdffa6f637b0f3ccc1eb0e550",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -936,6 +936,7 @@ def forward(\n         past_key_values: Optional[list[torch.Tensor]] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -952,7 +953,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.ernie(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.ernie(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -968,21 +969,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.cls(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.cls(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "491ce971e24b975bae770662abc464c824f7866d",
            "filename": "src/transformers/models/ernie/modular_ernie.py",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -433,6 +433,7 @@ def forward(\n         past_key_values: Optional[list[torch.Tensor]] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -449,7 +450,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.ernie(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.ernie(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -465,21 +466,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.cls(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.cls(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "a92a5758bfdff194e38cc0559da33624e570fa31",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1407,6 +1407,7 @@ def forward(\n         protein_input_ids: Optional[torch.LongTensor] = None,\n         protein_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ):\n         r\"\"\"\n@@ -1437,8 +1438,7 @@ def forward(\n \n         >>> print(processor.batch_decode(outputs, skip_special_tokens=True))\n         ```\"\"\"\n-\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             inputs_embeds=inputs_embeds,\n@@ -1447,8 +1447,11 @@ def forward(\n             use_cache=use_cache,\n             **kwargs,\n         )\n-        hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "3b1ceeee8238381a491e76b6f10910eaf1e5f094",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -900,6 +900,7 @@ def forward(\n         protein_input_ids: Optional[torch.LongTensor] = None,\n         protein_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ):\n         r\"\"\"\n@@ -930,8 +931,7 @@ def forward(\n \n         >>> print(processor.batch_decode(outputs, skip_special_tokens=True))\n         ```\"\"\"\n-\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             inputs_embeds=inputs_embeds,\n@@ -940,8 +940,11 @@ def forward(\n             use_cache=use_cache,\n             **kwargs,\n         )\n-        hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "f9c8611c019951d6ed319d15473c566d9448eb5c",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -887,6 +887,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,  # for now we need this for generation\n     ) -> Union[tuple, FalconMambaCausalLMOutput]:\n         r\"\"\"\n@@ -912,9 +913,11 @@ def forward(\n             cache_position=cache_position,\n             attention_mask=attention_mask,\n         )\n-        hidden_states = falcon_mamba_outputs[0]\n \n-        logits = self.lm_head(hidden_states.to(self.lm_head.weight.dtype)).float()\n+        hidden_states = falcon_mamba_outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :].to(self.lm_head.weight.dtype)).float()\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "17f9cbfdbfcaefe4028375cffd38e92c2820579e",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1149,6 +1149,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1300,8 +1301,10 @@ def forward(\n             return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n-        logits = self.output(sequence_output)\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.output(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "fbbad2c6082588ad9881be5cdddafe85916c6b2d",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -603,6 +603,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -640,26 +641,23 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n         )\n-        hidden_states = transformer_outputs[0]\n \n-        lm_logits = self.lm_head(hidden_states)\n+        hidden_states = transformer_outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(\n-                lm_logits,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n-            output = (lm_logits,) + transformer_outputs[1:]\n+            output = (logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithCrossAttentions(\n             loss=loss,\n-            logits=lm_logits,\n+            logits=logits,\n             past_key_values=transformer_outputs.past_key_values,\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,"
        },
        {
            "sha": "a046938c52e58360e0bb0c9e920d9fd5b76807f2",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 18,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -697,6 +697,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -732,34 +733,23 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n         )\n-        hidden_states = transformer_outputs[0]\n \n-        lm_logits = self.lm_head(hidden_states)\n+        hidden_states = transformer_outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(lm_logits.device)\n-            lm_logits = lm_logits.to(torch.float32)\n-\n-            # Flatten the tokens\n-            loss = self.loss_function(\n-                lm_logits,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n-\n-            lm_logits = lm_logits.to(hidden_states.dtype)\n-            loss = loss.to(hidden_states.dtype)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n-            output = (lm_logits,) + transformer_outputs[1:]\n+            output = (logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithPast(\n             loss=loss,\n-            logits=lm_logits,\n+            logits=logits,\n             past_key_values=transformer_outputs.past_key_values,\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,"
        },
        {
            "sha": "c87da821c958fdab595bdbcb55d9cb702a379cf3",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 10,
            "deletions": 15,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -662,6 +662,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -703,27 +704,21 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        lm_logits = self.embed_out(hidden_states)\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.embed_out(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(lm_logits.device)\n-\n-            lm_loss = self.loss_function(\n-                lm_logits,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n-            output = (lm_logits,) + outputs[1:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n+            output = (logits,) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithPast(\n-            loss=lm_loss,\n-            logits=lm_logits,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "ae04d2435fc48b8ce6bd2c3317fe3dec90f5d903",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 8,
            "deletions": 18,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -764,6 +764,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -791,34 +792,23 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n         )\n-        hidden_states = transformer_outputs[0]\n \n-        # make sure sampling in fp16 works correctly and\n-        # compute loss in fp32 to match with mesh-tf version\n-        # https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179\n-        lm_logits = self.lm_head(hidden_states).to(torch.float32)\n+        hidden_states = transformer_outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(lm_logits.device)\n-            # Flatten the tokens\n-            loss = self.loss_function(\n-                lm_logits,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n-\n-            loss = loss.to(hidden_states.dtype)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n-            output = (lm_logits,) + transformer_outputs[1:]\n+            output = (logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithPast(\n             loss=loss,\n-            logits=lm_logits,\n+            logits=logits,\n             past_key_values=transformer_outputs.past_key_values,\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,"
        },
        {
            "sha": "4f04d4e44bb438f089bf80854a672770b235472a",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1159,6 +1159,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, IdeficsCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1198,8 +1199,7 @@ def forward(\n         >>> generate_ids = model.generate(**inputs, max_new_tokens=6)\n         >>> processor.batch_decode(generate_ids, skip_special_tokens=True)\n         ```\"\"\"\n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: IdeficsBaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1216,8 +1216,10 @@ def forward(\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "09fae200a2643d95288b825390fd178cee150151",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1307,8 +1307,8 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -1325,14 +1325,12 @@ def forward(\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n             ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if labels is not None:\n             if use_cache:\n                 logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n             use_cache = False\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPastAndCrossAttentions = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             image_embeds=image_embeds,\n@@ -1345,19 +1343,22 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=True,\n             cache_position=cache_position,\n             **kwargs,\n         )\n-        lm_logits = self.lm_head(outputs[0])\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=lm_logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n             loss=loss,\n-            logits=lm_logits,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n@@ -1653,6 +1654,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Kosmos2ForConditionalGenerationModelOutput]:\n         r\"\"\"\n@@ -1729,7 +1731,7 @@ def forward(\n             image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n             image_embeds, projection_attentions = self.image_to_text_projection(image_embeds)\n \n-        lm_outputs = self.text_model(\n+        lm_outputs: CausalLMOutputWithCrossAttentions = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             image_embeds=image_embeds,\n@@ -1741,7 +1743,7 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=True,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "15080a1ee8b28cfcd2470368dd8cb777fcd61234",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1537,6 +1537,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithCrossAttentions:\n         r\"\"\"\n@@ -1553,7 +1554,7 @@ def forward(\n                 logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n             use_cache = False\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPastAndCrossAttentions = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             image_embeds=image_embeds,\n@@ -1566,22 +1567,19 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             **kwargs,\n         )\n-        lm_logits = self.lm_head(outputs.last_hidden_state)\n \n-        lm_loss = None\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(lm_logits.device)\n-            lm_loss = self.loss_function(\n-                lm_logits,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=lm_logits,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n@@ -1702,6 +1700,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Kosmos2_5ForConditionalGenerationModelOutput:\n         r\"\"\"\n@@ -1761,7 +1760,7 @@ def forward(\n                 image_embeds = nn.functional.normalize(vision_model_output.last_hidden_state, dim=-1)\n                 image_embeds, projection_attentions = self.image_to_text_projection(image_embeds)\n \n-        lm_outputs = self.text_model(\n+        lm_outputs: CausalLMOutputWithCrossAttentions = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             image_embeds=image_embeds,\n@@ -1773,6 +1772,7 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            logits_to_keep=logits_to_keep,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "5f918fdd0a70aa828dbaea4b7b1f6cd85ce8f088",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -827,6 +827,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,  # for now we need this for generation\n     ) -> Union[tuple, MambaCausalLMOutput]:\n         r\"\"\"\n@@ -852,9 +853,11 @@ def forward(\n             cache_position=cache_position,\n             attention_mask=attention_mask,\n         )\n-        hidden_states = mamba_outputs[0]\n \n-        logits = self.lm_head(hidden_states.to(self.lm_head.weight.dtype)).float()\n+        hidden_states = mamba_outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :].to(self.lm_head.weight.dtype)).float()\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "5148a57d25e35c3ea33454a8b19661ac69184899",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1008,6 +1008,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,  # for now we need this for generation and loss_function\n     ) -> Union[tuple, Mamba2CausalLMOutput]:\n         r\"\"\"\n@@ -1036,9 +1037,11 @@ def forward(\n             cache_position=cache_position,\n             attention_mask=attention_mask,\n         )\n-        hidden_states = mamba2_outputs[0]\n \n-        logits = self.lm_head(hidden_states.to(self.lm_head.weight.dtype)).float()\n+        hidden_states = mamba2_outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :].to(self.lm_head.weight.dtype)).float()\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "9356e03fc5caaac7a58bfccf8e626fee60765f9b",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1336,6 +1336,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1381,7 +1382,10 @@ def forward(\n             cache_position=cache_position,\n         )\n \n-        logits = self.lm_head(outputs[0])\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "ea1e8f5b04aae02582a76161ced7f5aec33df786",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1519,6 +1519,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1564,7 +1565,10 @@ def forward(\n             cache_position=cache_position,\n         )\n \n-        logits = self.lm_head(outputs[0])\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "6f0a035eca9522bb971e71bd92003d4e1394405e",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -851,6 +851,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -893,25 +894,22 @@ def forward(\n             cache_position=cache_position,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.cls(sequence_output)\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.cls(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n+            output = (logits,) + outputs[2:]\n+            return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "df1acd0e04d5b6e5fe1151ae7a4375b0b39be725",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -543,6 +543,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -572,8 +573,7 @@ def forward(\n         \"The capital of France is Paris\"\n         ```\n         \"\"\"\n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -583,8 +583,10 @@ def forward(\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n-        logits = self.decoder(self.lm_head(hidden_states))\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.decoder(self.lm_head(hidden_states[:, slice_indices, :]))\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "8e588599aac3a8d8cf7fffd667679dc182422ae9",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -614,6 +614,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -643,8 +644,7 @@ def forward(\n         \"The capital of France is Paris\"\n         ```\n         \"\"\"\n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -654,8 +654,10 @@ def forward(\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n-        logits = self.decoder(self.lm_head(hidden_states))\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.decoder(self.lm_head(hidden_states[:, slice_indices, :]))\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "00cdac508d64068af55d408081df5b88750bdf44",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 8,
            "deletions": 13,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -422,6 +422,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -454,29 +455,23 @@ def forward(\n             return_dict=return_dict,\n             cache_position=cache_position,\n         )\n-        hidden_states = transformer_outputs[0]\n \n-        lm_logits = self.lm_head(hidden_states)\n+        hidden_states = transformer_outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(lm_logits.device)\n-            # Flatten the tokens\n-            loss = self.loss_function(\n-                lm_logits,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n-            output = (lm_logits,) + transformer_outputs[1:]\n+            output = (logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithCrossAttentions(\n             loss=loss,\n-            logits=lm_logits,\n+            logits=logits,\n             past_key_values=transformer_outputs.past_key_values,\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,"
        },
        {
            "sha": "6f2bf620cfe4f1ffabf01e6e766797b8b0ebac82",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1581,6 +1581,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1624,7 +1625,10 @@ def forward(\n             return_dict=return_dict,\n         )\n \n-        logits = self.lm_head(outputs[0])\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "aebe5074c706c57504df0c3aa49a464e972fe24b",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -438,6 +438,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutput]:\n         r\"\"\"\n@@ -458,26 +459,23 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n         )\n+\n         hidden_states = transformer_outputs[0]\n-        lm_logits = self.lm_head(hidden_states)\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Flatten the tokens\n-            loss = self.loss_function(\n-                lm_logits,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n-            output = (lm_logits,) + transformer_outputs[1:]\n+            output = (logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutput(\n             loss=loss,\n-            logits=lm_logits,\n+            logits=logits,\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,\n         )"
        },
        {
            "sha": "9de23d596f3ab51dddc32c41d947673ee86b4012",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -756,6 +756,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -787,8 +788,7 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model.decoder(\n+        outputs: BaseModelOutputWithPast = self.model.decoder(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -802,18 +802,14 @@ def forward(\n             **kwargs,\n         )\n \n-        logits = self.lm_head(outputs[0]).contiguous()\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :]).contiguous()\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(logits.device)\n-            loss = self.loss_function(\n-                logits,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "a23f45bf8437e028f985891494fad2125c7ee18a",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1306,6 +1306,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1351,7 +1352,10 @@ def forward(\n             cache_position=cache_position,\n         )\n \n-        logits = self.lm_head(outputs[0])\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "6e81632af963e2655bc9b0482ca3ad3367a99f04",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1336,6 +1336,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1381,7 +1382,10 @@ def forward(\n             cache_position=cache_position,\n         )\n \n-        logits = self.lm_head(outputs[0])\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "3489728b62eb0f23e410532ade9d2d78bbb9f868",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1301,6 +1301,7 @@ def forward(\n         video_grid_thw: Optional[torch.LongTensor] = None,\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Qwen2VLCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1351,7 +1352,7 @@ def forward(\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n \n-        outputs = self.model(\n+        outputs: Qwen2VLModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,\n             pixel_values_videos=pixel_values_videos,\n@@ -1369,8 +1370,10 @@ def forward(\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "9abff584c78931553ef344f976376bb3fb896d2e",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -712,6 +712,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n@@ -753,7 +754,9 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         # Soft-cap the logits TODO remove if always done.\n         # if self.config.logits_soft_cap is not None:\n@@ -762,14 +765,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            loss = self.loss_function(\n-                logits,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "a880837004be2069e04b17943f72d223abc2011b",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -2190,6 +2190,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n@@ -2234,17 +2235,14 @@ def forward(\n             return_dict=return_dict,\n         )\n \n-        sequence_output = reformer_outputs[0]\n-        logits = self.lm_head(sequence_output)\n+        hidden_states = reformer_outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(\n-                logits,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + reformer_outputs[1:]"
        },
        {
            "sha": "a8e4a29e806f79db587e0eafd244b0216ee836cc",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -781,6 +781,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -822,25 +823,22 @@ def forward(\n             return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.cls(sequence_output)\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.cls(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n+            output = (logits,) + outputs[2:]\n+            return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "a718c3528805422a0c17e71c652c7689f08d2132",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 10,
            "deletions": 14,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -754,6 +754,7 @@ def forward(\n         past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -790,7 +791,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.roberta(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -805,23 +806,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.lm_head(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(prediction_scores.device)\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "5884e893027d433e65e4b100a7a6cd6036f6079f",
            "filename": "src/transformers/models/roberta/modular_roberta.py",
            "status": "modified",
            "additions": 11,
            "deletions": 14,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -24,6 +24,7 @@\n from ...activations import gelu\n from ...generation import GenerationMixin\n from ...modeling_outputs import (\n+    BaseModelOutputWithPoolingAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n     MaskedLMOutput,\n     MultipleChoiceModelOutput,\n@@ -228,6 +229,7 @@ def forward(\n         past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -264,7 +266,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.roberta(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -279,23 +281,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.lm_head(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(prediction_scores.device)\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "17cc0ad9e3aece554c6567a7f69c98856f4b1d41",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 14,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -784,6 +784,7 @@ def forward(\n         past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -820,7 +821,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.roberta_prelayernorm(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.roberta_prelayernorm(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -835,23 +836,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.lm_head(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(prediction_scores.device)\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "b7ae250bd297c698e8555396b29a6209549ecb20",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1216,6 +1216,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -1255,7 +1256,7 @@ def forward(\n         >>> prediction_logits = outputs.logits\n         ```\n         \"\"\"\n-        outputs = self.roc_bert(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.roc_bert(\n             input_ids,\n             input_shape_ids=input_shape_ids,\n             input_pronunciation_ids=input_pronunciation_ids,\n@@ -1272,21 +1273,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.cls(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.cls(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "b7c5afa017225114313c5f16eb0982e24c4c311f",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -931,6 +931,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[CausalLMOutputWithCrossAttentions, tuple[torch.Tensor]]:\n         r\"\"\"\n@@ -972,25 +973,22 @@ def forward(\n             cache_position=cache_position,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.cls(sequence_output)\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.cls(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n-            output = (prediction_scores,) + outputs[1:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n+            output = (logits,) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "895abd9812288743c174bbdd5afdaf6175808f65",
            "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -717,6 +717,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, RwkvCausalLMOutput]:\n         r\"\"\"\n@@ -753,18 +754,15 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n         )\n-        hidden_states = rwkv_outputs[0]\n \n-        logits = self.head(hidden_states)\n+        hidden_states = rwkv_outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(\n-                logits,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + rwkv_outputs[1:]"
        },
        {
            "sha": "32f3cdab33632b6668c9b8c262f3817fd0b917cf",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -586,6 +586,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -628,7 +629,10 @@ def forward(\n             cache_position=cache_position,\n         )\n \n-        logits = self.lm_head(outputs[0])\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "856a84c760078b1801d59f2f3de2fe65803e08b1",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -979,6 +979,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n@@ -1020,8 +1021,13 @@ def forward(\n             **kwargs,\n         )\n \n-        output = transformer_outputs[0]\n-        outputs = self.pred_layer(output, labels)  # (loss, logits) or (logits,) depending on if labels are provided.\n+        hidden_states = transformer_outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        outputs = self.pred_layer(\n+            hidden_states[:, slice_indices, :],\n+            labels,\n+        )  # (loss, logits) or (logits,) depending on if labels are provided.\n \n         if not return_dict:\n             return outputs + transformer_outputs[1:]"
        },
        {
            "sha": "074755d68362d663e792e93329a35a4d9f8b20a6",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 10,
            "deletions": 14,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -772,6 +772,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -808,7 +809,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.roberta(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -822,23 +823,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.lm_head(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(prediction_scores.device)\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "4b61a30f7190eb6838191eb4094c959749a2cc19",
            "filename": "src/transformers/models/xlm_roberta/modular_xlm_roberta.py",
            "status": "modified",
            "additions": 11,
            "deletions": 14,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodular_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodular_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodular_xlm_roberta.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -21,6 +21,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...modeling_outputs import (\n+    BaseModelOutputWithPoolingAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n     MaskedLMOutput,\n     MultipleChoiceModelOutput,\n@@ -79,6 +80,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -115,7 +117,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.roberta(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -129,23 +131,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.lm_head(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            # move labels to correct device\n-            labels = labels.to(prediction_scores.device)\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "a0f13d505d6ee21a1867eab825beef7717a2a15c",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 13,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -813,6 +813,7 @@ def forward(\n         past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -839,7 +840,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.roberta(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -850,25 +851,21 @@ def forward(\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n-            return_dict=True,\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.lm_head(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "bca175a6934e0a62b20bd88fcd93af1470ff7fce",
            "filename": "src/transformers/models/xlm_roberta_xl/modular_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -31,6 +31,7 @@\n from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n+    BaseModelOutputWithPoolingAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n     MaskedLMOutput,\n     MultipleChoiceModelOutput,\n@@ -309,6 +310,7 @@ def forward(\n         past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -335,7 +337,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.roberta(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n@@ -346,25 +348,21 @@ def forward(\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             cache_position=cache_position,\n-            return_dict=True,\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.lm_head(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "67f9f1bf7874fbaa144ba7c979f276bbef3dbe0e",
            "filename": "src/transformers/models/xlnet/modeling_xlnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -1321,6 +1321,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,  # delete when `use_cache` is removed in XLNetModel\n     ) -> Union[tuple, XLNetLMHeadModelOutput]:\n         r\"\"\"\n@@ -1439,7 +1440,10 @@ def forward(\n             **kwargs,\n         )\n \n-        logits = self.lm_loss(transformer_outputs[0])\n+        hidden_states = transformer_outputs[0]\n+        # Only compute necessary logits\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_loss(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "fc9cfca7359df82aea355e6a55cbcf99986764cc",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -891,6 +891,7 @@ def forward(\n         past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -922,7 +923,7 @@ def forward(\n         if labels is not None:\n             use_cache = False\n \n-        outputs = self.roberta(\n+        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.roberta(\n             input_ids,\n             lang_ids=lang_ids,\n             attention_mask=attention_mask,\n@@ -938,21 +939,18 @@ def forward(\n             **kwargs,\n         )\n \n-        sequence_output = outputs[0]\n-        prediction_scores = self.lm_head(sequence_output)\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n-        lm_loss = None\n+        loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(\n-                prediction_scores,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n-            loss=lm_loss,\n-            logits=prediction_scores,\n+            loss=loss,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,"
        },
        {
            "sha": "750ed95aa83fc22e66eeb83045a1e4d21f89919f",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/26b7f668500281fe96c0ae4a9adcb603a60f0427/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26b7f668500281fe96c0ae4a9adcb603a60f0427/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=26b7f668500281fe96c0ae4a9adcb603a60f0427",
            "patch": "@@ -452,6 +452,10 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n     def test_greedy_generate_dict_outputs_use_cache(self):\n         pass\n \n+    @unittest.skip(reason=\"GIT input and output sequence lengths are not equal due to pixel values additional input\")\n+    def test_forward_with_logits_to_keep(self):\n+        pass\n+\n \n @require_torch\n @require_vision"
        }
    ],
    "stats": {
        "total": 1043,
        "additions": 514,
        "deletions": 529
    }
}