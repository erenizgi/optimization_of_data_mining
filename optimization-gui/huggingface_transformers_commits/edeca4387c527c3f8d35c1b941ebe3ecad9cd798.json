{
    "author": "Isotr0py",
    "message": "ðŸš¨ Support dequantization for most GGML types (#32625)\n\n* use gguf internal dequantize\r\n\r\n* add Q5_0 test\r\n\r\n* add iq1 test\r\n\r\n* add remained test\r\n\r\n* remove duplicated test\r\n\r\n* update docs\r\n\r\n* add gguf version limit\r\n\r\n* make style\r\n\r\n* update gguf import catch\r\n\r\n* revert vocab_size patch\r\n\r\n* make style\r\n\r\n* use GGUF_MIN_VERSION everywhere",
    "sha": "edeca4387c527c3f8d35c1b941ebe3ecad9cd798",
    "files": [
        {
            "sha": "1b7515498e44c6a6cab99347382229337dba35e0",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 18,
            "deletions": 4,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=edeca4387c527c3f8d35c1b941ebe3ecad9cd798",
            "patch": "@@ -46,16 +46,30 @@ The initial supported quantization types are decided according to the popular qu\n on the Hub.\n \n - F32\n+- F16\n+- BF16\n+- Q4_0\n+- Q4_1\n+- Q5_0\n+- Q5_1\n+- Q8_0\n - Q2_K\n - Q3_K\n-- Q4_0\n - Q4_K\n - Q5_K\n - Q6_K\n-- Q8_0\n+- IQ1_S\n+- IQ1_M\n+- IQ2_XXS\n+- IQ2_XS\n+- IQ2_S\n+- IQ3_XXS\n+- IQ3_S\n+- IQ4_XS\n+- IQ4_NL\n \n-We take example from the excellent [99991/pygguf](https://github.com/99991/pygguf) Python parser to dequantize the \n-weights.\n+> [!NOTE]\n+> To support gguf dequantization, `gguf>=0.10.0` installation is required.\n \n ### Supported model architectures\n "
        },
        {
            "sha": "fe5b71b7d613c86026697efd92630958a9194176",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 0,
            "deletions": 335,
            "changes": 335,
            "blob_url": "https://github.com/huggingface/transformers/blob/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=edeca4387c527c3f8d35c1b941ebe3ecad9cd798",
            "patch": "@@ -33,44 +33,6 @@\n logger = logging.get_logger(__name__)\n \n \n-# Listed here: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md\n-GGML_TYPES = {\n-    \"F32\": 0,\n-    \"F16\": 1,\n-    \"Q4_0\": 2,\n-    \"Q8_0\": 8,\n-    \"Q2_K\": 10,\n-    \"Q3_K\": 11,\n-    \"Q4_K\": 12,\n-    \"Q5_K\": 13,\n-    \"Q6_K\": 14,\n-}\n-\n-# The Blocksizes are reported in bytes\n-# Check out: https://github.com/ggerganov/llama.cpp/blob/8a56075b07a8b571bf95a912ffdce4c928c2b414/gguf-py/gguf/constants.py#L801\n-GGML_BLOCK_SIZES = {\n-    \"Q8_0\": 2 + 32,  # Q8_0 uses a blocksize of 32 (int8 tensors) + 2 bytes allocated for the scales\n-    \"Q4_K\": 144,\n-    # Q4_0 uses a blocksize of 32 but the 4-bit tensors are packed into 8-bit tensors + 2 bytes for the scales\n-    \"Q4_0\": 2 + 16,\n-    \"Q6_K\": 210,\n-    # See: https://github.com/99991/pygguf/commit/a417edbfc029a1bc270f984a694f9128c5afa8b9\n-    \"Q2_K\": 256 // 16 + 256 // 4 + 2 + 2,\n-    \"Q3_K\": 256 // 8 + 256 // 4 + 12 + 2,\n-    \"Q5_K\": 2 + 2 + 12 + 256 // 8 + 256 // 2,\n-}\n-\n-# Listed here: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md\n-DATA_TYPES = {\n-    \"uint32\": 4,\n-    \"int32\": 5,\n-    \"float32\": 6,\n-    \"bool\": 7,\n-    \"string\": 8,\n-    \"array\": 9,\n-    \"uint64\": 10,\n-}\n-\n GGUF_TENSOR_MAPPING = {\n     \"llama\": {\n         \"token_embd\": \"model.embed_tokens\",\n@@ -217,303 +179,6 @@ def _gguf_parse_value(_value, data_type):\n     return _value\n \n \n-def dequantize_q4_k(data, n_bytes: int):\n-    # C implementation\n-    # https://github.com/ggerganov/ggml/blob/fca1caafea7de9fbd7efc733b9818f9cf2da3050/src/ggml-quants.c#L1929\n-    # C struct definition\n-    # https://github.com/ggerganov/ggml/blob/fca1caafea7de9fbd7efc733b9818f9cf2da3050/src/ggml-quants.h#L116\n-    block_size = GGML_BLOCK_SIZES[\"Q4_K\"]\n-    num_blocks = n_bytes // block_size\n-\n-    data_f16 = np.frombuffer(data, dtype=np.float16).reshape(num_blocks, block_size // 2)\n-    data_u8 = np.frombuffer(data, dtype=np.uint8).reshape(num_blocks, block_size)\n-\n-    # Casting to float32 because float16 is very slow on CPU\n-    scale_factors = data_f16[:, 0].reshape(num_blocks, 1, 1).astype(np.float32)\n-    scale_offsets = data_f16[:, 1].reshape(num_blocks, 1, 1).astype(np.float32)\n-    qs1 = data_u8[:, 4:16].reshape(num_blocks, 12, 1)\n-    qs2 = data_u8[:, 16:].reshape(num_blocks, 4, 32)\n-\n-    # Dequantize scales and offsets (6 bits and 4 + 2 bits)\n-    factors = scale_factors * np.concatenate(\n-        [qs1[:, 0:4] & 0b111111, (qs1[:, 8:] & 15) | ((qs1[:, 0:4] >> 6) << 4)], axis=1\n-    )\n-    offsets = scale_offsets * np.concatenate(\n-        [qs1[:, 4:8] & 0b111111, (qs1[:, 8:] >> 4) | ((qs1[:, 4:8] >> 6) << 4)], axis=1\n-    )\n-\n-    # Interleave low and high quantized bits\n-    qs2 = np.stack([qs2 & 0xF, qs2 >> 4], axis=2).reshape(num_blocks, 8, 32)\n-    # Dequantize final weights using scales and offsets\n-    return factors * qs2 - offsets\n-\n-\n-def dequantize_q4_0(data, n_bytes: int):\n-    # C implementation\n-    # https://github.com/ggerganov/ggml/blob/fca1caafea7de9fbd7efc733b9818f9cf2da3050/src/ggml-quants.c#L1086\n-    # C struct definition\n-    # https://github.com/ggerganov/ggml/blob/fca1caafea7de9fbd7efc733b9818f9cf2da3050/src/ggml-quants.h#L11\n-    block_size = GGML_BLOCK_SIZES[\"Q4_0\"]\n-    num_blocks = n_bytes // block_size\n-\n-    data_f16 = np.frombuffer(data, dtype=np.float16).reshape(num_blocks, block_size // 2)\n-    data_u8 = np.frombuffer(data, dtype=np.uint8).reshape(num_blocks, block_size)\n-\n-    # The scales are stored on the first 2 bytes and the rest corresponds to the quants\n-    scales = data_f16[:, 0].reshape(num_blocks, 1).astype(np.float32)\n-    # scales = np.nan_to_num(scales)\n-    # the rest of the bytes corresponds to the quants - we discard the first two bytes\n-    quants = data_u8[:, 2:]\n-\n-    ql = (quants[:, :] & 0xF).astype(np.int8) - 8\n-    qr = (quants[:, :] >> 4).astype(np.int8) - 8\n-\n-    # Use hstack\n-    quants = np.hstack([ql, qr])\n-\n-    return (scales * quants).astype(np.float32)\n-\n-\n-def dequantize_q6_k(data, n_bytes: int):\n-    # C implementation\n-    # https://github.com/ggerganov/ggml/blob/fca1caafea7de9fbd7efc733b9818f9cf2da3050/src/ggml-quants.c#L2275\n-    # C struct definition\n-    # https://github.com/ggerganov/ggml/blob/fca1caafea7de9fbd7efc733b9818f9cf2da3050/src/ggml-quants.h#L152\n-    block_size = GGML_BLOCK_SIZES[\"Q6_K\"]\n-    num_blocks = n_bytes // block_size\n-\n-    data_f16 = np.frombuffer(data, dtype=np.float16).reshape(num_blocks, block_size // 2)\n-    data_u8 = np.frombuffer(data, dtype=np.uint8).reshape(num_blocks, block_size)\n-    data_i8 = np.frombuffer(data, dtype=np.int8).reshape(num_blocks, block_size)\n-\n-    scales = data_f16[:, -1].reshape(num_blocks, 1).astype(np.float32)\n-\n-    # TODO use uint8 and cast later?\n-    ql = data_u8[:, :128].astype(np.int16)\n-    qh = data_u8[:, 128:192].astype(np.int16)\n-    sc = data_i8[:, 192:208, np.newaxis].astype(np.float32)\n-\n-    # Unpack bits, subtraction requires signed data type\n-    q1 = (ql[:, :32] & 0xF) | (((qh[:, :32] >> 0) & 3) << 4) - 32\n-    q2 = (ql[:, 32:64] & 0xF) | (((qh[:, :32] >> 2) & 3) << 4) - 32\n-    q3 = (ql[:, :32] >> 4) | (((qh[:, :32] >> 4) & 3) << 4) - 32\n-    q4 = (ql[:, 32:64] >> 4) | (((qh[:, :32] >> 6) & 3) << 4) - 32\n-    q5 = (ql[:, 64:96] & 0xF) | (((qh[:, 32:] >> 0) & 3) << 4) - 32\n-    q6 = (ql[:, 96:128] & 0xF) | (((qh[:, 32:] >> 2) & 3) << 4) - 32\n-    q7 = (ql[:, 64:96] >> 4) | (((qh[:, 32:] >> 4) & 3) << 4) - 32\n-    q8 = (ql[:, 96:128] >> 4) | (((qh[:, 32:] >> 6) & 3) << 4) - 32\n-\n-    # Dequantize\n-    return scales * np.concatenate(\n-        [\n-            sc[:, 0] * q1[:, :16],\n-            sc[:, 1] * q1[:, 16:],\n-            sc[:, 2] * q2[:, :16],\n-            sc[:, 3] * q2[:, 16:],\n-            sc[:, 4] * q3[:, :16],\n-            sc[:, 5] * q3[:, 16:],\n-            sc[:, 6] * q4[:, :16],\n-            sc[:, 7] * q4[:, 16:],\n-            sc[:, 8] * q5[:, :16],\n-            sc[:, 9] * q5[:, 16:],\n-            sc[:, 10] * q6[:, :16],\n-            sc[:, 11] * q6[:, 16:],\n-            sc[:, 12] * q7[:, :16],\n-            sc[:, 13] * q7[:, 16:],\n-            sc[:, 14] * q8[:, :16],\n-            sc[:, 15] * q8[:, 16:],\n-        ],\n-        axis=1,\n-    )\n-\n-\n-def dequantize_q8_0(data, n_bytes: int):\n-    # C struct definition\n-    # https://github.com/ggerganov/ggml/blob/fca1caafea7de9fbd7efc733b9818f9cf2da3050/src/ggml-quants.h#L43\n-    block_size = GGML_BLOCK_SIZES[\"Q8_0\"]\n-    num_blocks = n_bytes // block_size\n-\n-    scales = np.frombuffer(data, dtype=np.float16).reshape(num_blocks, 1 + 16)[:, :1].astype(np.float32)\n-    qs = np.frombuffer(data, dtype=np.int8).reshape(num_blocks, 2 + 32)[:, 2:]\n-\n-    return scales * qs\n-\n-\n-def dequantize_q2_k(data, n_bytes: int):\n-    # C implementation\n-    # https://github.com/ggerganov/ggml/blob/fca1caafea7de9fbd7efc733b9818f9cf2da3050/src/ggml-quants.c#L1547\n-    # C struct definition\n-    # https://github.com/ggerganov/ggml/blob/fca1caafea7de9fbd7efc733b9818f9cf2da3050/src/ggml-quants.h#L74\n-    num_blocks = n_bytes // GGML_BLOCK_SIZES[\"Q2_K\"]\n-\n-    data_f16 = np.frombuffer(data, dtype=np.float16).reshape(num_blocks, GGML_BLOCK_SIZES[\"Q2_K\"] // 2)\n-    data_u8 = np.frombuffer(data, dtype=np.uint8).reshape(num_blocks, GGML_BLOCK_SIZES[\"Q2_K\"])\n-\n-    dmin = data_f16[:, -1].reshape(num_blocks, 1, 1).astype(np.float32)\n-    d = data_f16[:, -2].reshape(num_blocks, 1, 1).astype(np.float32)\n-    scales = data_u8[:, :16].reshape(num_blocks, 16, 1)\n-    qs = data_u8[:, 16:80].reshape(num_blocks, 64)\n-\n-    tmp = np.stack(\n-        [\n-            qs[:, 00:16] >> 0,\n-            qs[:, 16:32] >> 0,\n-            qs[:, 00:16] >> 2,\n-            qs[:, 16:32] >> 2,\n-            qs[:, 00:16] >> 4,\n-            qs[:, 16:32] >> 4,\n-            qs[:, 00:16] >> 6,\n-            qs[:, 16:32] >> 6,\n-            qs[:, 32:48] >> 0,\n-            qs[:, 48:64] >> 0,\n-            qs[:, 32:48] >> 2,\n-            qs[:, 48:64] >> 2,\n-            qs[:, 32:48] >> 4,\n-            qs[:, 48:64] >> 4,\n-            qs[:, 32:48] >> 6,\n-            qs[:, 48:64] >> 6,\n-        ],\n-        axis=1,\n-    )\n-\n-    return d * (scales & 15) * (tmp & 3) - dmin * (scales >> 4)\n-\n-\n-def dequantize_q3_k(data, n_bytes: int):\n-    # C implementation\n-    # https://github.com/ggerganov/ggml/blob/fca1caafea7de9fbd7efc733b9818f9cf2da3050/src/ggml-quants.c#L1723C32-L1723C42\n-    # C struct definition\n-    # https://github.com/ggerganov/ggml/blob/fca1caafea7de9fbd7efc733b9818f9cf2da3050/src/ggml-quants.h#L95\n-    num_blocks = n_bytes // GGML_BLOCK_SIZES[\"Q3_K\"]\n-\n-    data_f16 = np.frombuffer(data, dtype=np.float16).reshape(num_blocks, GGML_BLOCK_SIZES[\"Q3_K\"] // 2)\n-    data_u8 = np.frombuffer(data, dtype=np.uint8).reshape(num_blocks, GGML_BLOCK_SIZES[\"Q3_K\"])\n-\n-    d = data_f16[:, -1].reshape(num_blocks, 1, 1).astype(np.float32)\n-    bits = np.unpackbits(data_u8[:, :32].reshape(num_blocks, 32, 1), axis=-1, bitorder=\"little\")\n-    bits = 4 ^ (bits << 2)\n-    qs = data_u8[:, 32 : 32 + 64].astype(np.int16)\n-    a, b, c = data_u8[:, 96 : 96 + 12].reshape(num_blocks, 3, 4).transpose(1, 0, 2)\n-    scales = np.zeros((num_blocks, 4, 4), dtype=np.uint8)\n-    scales[:, 0] = (a & 15) | ((c & 3) << 4)\n-    scales[:, 1] = (b & 15) | (((c >> 2) & 3) << 4)\n-    scales[:, 2] = (a >> 4) | (((c >> 4) & 3) << 4)\n-    scales[:, 3] = (b >> 4) | ((c >> 6) << 4)\n-    scales = scales.reshape(num_blocks, 16, 1).astype(np.int16)\n-\n-    return (\n-        d\n-        * (scales - 32)\n-        * np.stack(\n-            [\n-                (((qs[:, 00:16] >> 0) & 3) - bits[:, :16, 0]),\n-                (((qs[:, 16:32] >> 0) & 3) - bits[:, 16:, 0]),\n-                (((qs[:, 00:16] >> 2) & 3) - bits[:, :16, 1]),\n-                (((qs[:, 16:32] >> 2) & 3) - bits[:, 16:, 1]),\n-                (((qs[:, 00:16] >> 4) & 3) - bits[:, :16, 2]),\n-                (((qs[:, 16:32] >> 4) & 3) - bits[:, 16:, 2]),\n-                (((qs[:, 00:16] >> 6) & 3) - bits[:, :16, 3]),\n-                (((qs[:, 16:32] >> 6) & 3) - bits[:, 16:, 3]),\n-                (((qs[:, 32:48] >> 0) & 3) - bits[:, :16, 4]),\n-                (((qs[:, 48:64] >> 0) & 3) - bits[:, 16:, 4]),\n-                (((qs[:, 32:48] >> 2) & 3) - bits[:, :16, 5]),\n-                (((qs[:, 48:64] >> 2) & 3) - bits[:, 16:, 5]),\n-                (((qs[:, 32:48] >> 4) & 3) - bits[:, :16, 6]),\n-                (((qs[:, 48:64] >> 4) & 3) - bits[:, 16:, 6]),\n-                (((qs[:, 32:48] >> 6) & 3) - bits[:, :16, 7]),\n-                (((qs[:, 48:64] >> 6) & 3) - bits[:, 16:, 7]),\n-            ],\n-            axis=1,\n-        )\n-    )\n-\n-\n-def dequantize_q5_k(data, n_bytes: int):\n-    # C implementation\n-    # https://github.com/ggerganov/ggml/blob/fca1caafea7de9fbd7efc733b9818f9cf2da3050/src/ggml-quants.c#L2129\n-    # C struct definition\n-    # https://github.com/ggerganov/ggml/blob/fca1caafea7de9fbd7efc733b9818f9cf2da3050/src/ggml-quants.h#L138\n-    num_blocks = n_bytes // GGML_BLOCK_SIZES[\"Q5_K\"]\n-\n-    data_f16 = np.frombuffer(data, dtype=np.float16).reshape(num_blocks, GGML_BLOCK_SIZES[\"Q5_K\"] // 2)\n-    data_u8 = np.frombuffer(data, dtype=np.uint8).reshape(num_blocks, GGML_BLOCK_SIZES[\"Q5_K\"])\n-\n-    d = data_f16[:, 0].reshape(num_blocks, 1).astype(np.float32)\n-    dmin = data_f16[:, 1].reshape(num_blocks, 1).astype(np.float32)\n-    scales = data_u8[:, 4:16].reshape(num_blocks, 12, 1)\n-    qh = data_u8[:, 16 : 16 + 32].reshape(num_blocks, 32, 1)\n-    qs = data_u8[:, 48 : 48 + 128].reshape(num_blocks, 4, 32)\n-\n-    bits = np.unpackbits(qh, axis=-1, bitorder=\"little\")\n-\n-    qs_hi_4 = qs >> 4\n-    qs_lo_4 = qs & 15\n-\n-    scales_lo_6 = scales[:, :8] & 63\n-    scales_hi_6 = scales[:, :8] >> 6\n-    scales_lo_4 = scales[:, 8:] & 15\n-    scales_hi_4 = scales[:, 8:] >> 4\n-\n-    m1 = dmin * scales_lo_6[:, 4]\n-    m2 = dmin * scales_lo_6[:, 5]\n-    m3 = dmin * scales_lo_6[:, 6]\n-    m4 = dmin * scales_lo_6[:, 7]\n-    m5 = dmin * (scales_hi_4[:, 0] | (scales_hi_6[:, 4] << 4))\n-    m6 = dmin * (scales_hi_4[:, 1] | (scales_hi_6[:, 5] << 4))\n-    m7 = dmin * (scales_hi_4[:, 2] | (scales_hi_6[:, 6] << 4))\n-    m8 = dmin * (scales_hi_4[:, 3] | (scales_hi_6[:, 7] << 4))\n-\n-    d1 = d * scales_lo_6[:, 0]\n-    d2 = d * scales_lo_6[:, 1]\n-    d3 = d * scales_lo_6[:, 2]\n-    d4 = d * scales_lo_6[:, 3]\n-    d5 = d * (scales_lo_4[:, 0] | (scales_hi_6[:, 0] << 4))\n-    d6 = d * (scales_lo_4[:, 1] | (scales_hi_6[:, 1] << 4))\n-    d7 = d * (scales_lo_4[:, 2] | (scales_hi_6[:, 2] << 4))\n-    d8 = d * (scales_lo_4[:, 3] | (scales_hi_6[:, 3] << 4))\n-\n-    return np.concatenate(\n-        [\n-            d1 * (qs_lo_4[:, 0] + (bits[:, :, 0] << 4)) - m1,\n-            d2 * (qs_hi_4[:, 0] + (bits[:, :, 1] << 4)) - m2,\n-            d3 * (qs_lo_4[:, 1] + (bits[:, :, 2] << 4)) - m3,\n-            d4 * (qs_hi_4[:, 1] + (bits[:, :, 3] << 4)) - m4,\n-            d5 * (qs_lo_4[:, 2] + (bits[:, :, 4] << 4)) - m5,\n-            d6 * (qs_hi_4[:, 2] + (bits[:, :, 5] << 4)) - m6,\n-            d7 * (qs_lo_4[:, 3] + (bits[:, :, 6] << 4)) - m7,\n-            d8 * (qs_hi_4[:, 3] + (bits[:, :, 7] << 4)) - m8,\n-        ],\n-        axis=1,\n-    )\n-\n-\n-def load_dequant_gguf_tensor(shape, ggml_type, data, n_bytes):\n-    if ggml_type == GGML_TYPES[\"F32\"]:\n-        values = data\n-    elif ggml_type == GGML_TYPES[\"F16\"]:\n-        values = data\n-    elif ggml_type == GGML_TYPES[\"Q8_0\"]:\n-        values = dequantize_q8_0(data, n_bytes)\n-    elif ggml_type == GGML_TYPES[\"Q4_0\"]:\n-        values = dequantize_q4_0(data, n_bytes)\n-    elif ggml_type == GGML_TYPES[\"Q4_K\"]:\n-        values = dequantize_q4_k(data, n_bytes)\n-    elif ggml_type == GGML_TYPES[\"Q6_K\"]:\n-        values = dequantize_q6_k(data, n_bytes)\n-    elif ggml_type == GGML_TYPES[\"Q2_K\"]:\n-        values = dequantize_q2_k(data, n_bytes)\n-    elif ggml_type == GGML_TYPES[\"Q3_K\"]:\n-        values = dequantize_q3_k(data, n_bytes)\n-    elif ggml_type == GGML_TYPES[\"Q5_K\"]:\n-        values = dequantize_q5_k(data, n_bytes)\n-    else:\n-        raise NotImplementedError(\n-            f\"ggml_type {ggml_type} not implemented - please raise an issue on huggingface transformers: https://github.com/huggingface/transformers/issues/new/choose\"\n-        )\n-\n-    return values.reshape(shape[::-1])\n-\n-\n class GGUFTokenizerSkeleton:\n     def __init__(self, dict_):\n         for k, v in dict_.items():"
        },
        {
            "sha": "e5fa0ff7b5097dfe60f052c2648d8435ef660c32",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=edeca4387c527c3f8d35c1b941ebe3ecad9cd798",
            "patch": "@@ -24,9 +24,9 @@\n     GGUF_TENSOR_MAPPING,\n     GGUF_TOKENIZER_MAPPING,\n     _gguf_parse_value,\n-    load_dequant_gguf_tensor,\n )\n from .utils import is_torch_available\n+from .utils.import_utils import is_gguf_available\n from .utils.logging import get_logger\n \n \n@@ -71,14 +71,14 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n             Whether to read the tensors from the file and return them. Not doing so is faster\n             and only loads the metadata in memory.\n     \"\"\"\n-    try:\n-        from gguf import GGUFReader\n-    except (ImportError, ModuleNotFoundError):\n+    if is_gguf_available() and is_torch_available():\n+        from gguf import GGUFReader, dequantize\n+    else:\n         logger.error(\n-            \"Loading a GGUF checkpoint in PyTorch, requires both PyTorch and GGUF to be installed. Please see \"\n+            \"Loading a GGUF checkpoint in PyTorch, requires both PyTorch and GGUF>=0.10.0 to be installed. Please see \"\n             \"https://pytorch.org/ and https://github.com/ggerganov/llama.cpp/tree/master/gguf-py for installation instructions.\"\n         )\n-        raise\n+        raise ImportError(\"Please install torch and gguf>=0.10.0 to load a GGUF checkpoint in PyTorch.\")\n \n     reader = GGUFReader(gguf_checkpoint_path)\n     fields = reader.fields\n@@ -154,12 +154,9 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n                         tensor_name_mapping, GGUF_TO_TRANSFORMERS_MAPPING[\"tensors\"][tensor_name_mapping]\n                     )\n \n-            shape = tensor.shape\n             name = tensor.name\n \n-            weights = load_dequant_gguf_tensor(\n-                shape=shape, ggml_type=tensor.tensor_type, data=tensor.data, n_bytes=int(tensor.n_bytes)\n-            )\n+            weights = dequantize(tensor.data, tensor.tensor_type)\n \n             if architecture == \"llama\" and (\".attn_k.\" in name or \".attn_q.\" in name):\n                 num_heads = parsed_parameters[\"config\"][\"num_attention_heads\"]"
        },
        {
            "sha": "b6dfa85b1d20277d4bc9661f738ae712857a3688",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=edeca4387c527c3f8d35c1b941ebe3ecad9cd798",
            "patch": "@@ -53,6 +53,7 @@\n from .integrations.deepspeed import is_deepspeed_available\n from .utils import (\n     ACCELERATE_MIN_VERSION,\n+    GGUF_MIN_VERSION,\n     is_accelerate_available,\n     is_apex_available,\n     is_aqlm_available,\n@@ -407,11 +408,13 @@ def require_accelerate(test_case, min_version: str = ACCELERATE_MIN_VERSION):\n     )(test_case)\n \n \n-def require_gguf(test_case):\n+def require_gguf(test_case, min_version: str = GGUF_MIN_VERSION):\n     \"\"\"\n     Decorator marking a test that requires ggguf. These tests are skipped when gguf isn't installed.\n     \"\"\"\n-    return unittest.skipUnless(is_gguf_available(), \"test requires gguf\")(test_case)\n+    return unittest.skipUnless(is_gguf_available(min_version), f\"test requires gguf version >= {min_version}\")(\n+        test_case\n+    )\n \n \n def require_fsdp(test_case, min_version: str = \"1.12.0\"):"
        },
        {
            "sha": "27d102a9fd128c506c03a62db798cc4e2fcd964b",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=edeca4387c527c3f8d35c1b941ebe3ecad9cd798",
            "patch": "@@ -99,6 +99,7 @@\n     ACCELERATE_MIN_VERSION,\n     ENV_VARS_TRUE_AND_AUTO_VALUES,\n     ENV_VARS_TRUE_VALUES,\n+    GGUF_MIN_VERSION,\n     TORCH_FX_REQUIRED_VERSION,\n     USE_JAX,\n     USE_TF,"
        },
        {
            "sha": "8ae133d0ffe0f9144793f97de1c14024637c044b",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=edeca4387c527c3f8d35c1b941ebe3ecad9cd798",
            "patch": "@@ -89,6 +89,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n \n ACCELERATE_MIN_VERSION = \"0.26.0\"\n FSDP_MIN_VERSION = \"1.12.0\"\n+GGUF_MIN_VERSION = \"0.10.0\"\n XLA_FSDPV2_MIN_VERSION = \"2.2.0\"\n \n \n@@ -156,7 +157,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _scipy_available = _is_package_available(\"scipy\")\n _sentencepiece_available = _is_package_available(\"sentencepiece\")\n _is_seqio_available = _is_package_available(\"seqio\")\n-_is_gguf_available = _is_package_available(\"gguf\")\n+_is_gguf_available, _gguf_version = _is_package_available(\"gguf\", return_version=True)\n _sklearn_available = importlib.util.find_spec(\"sklearn\") is not None\n if _sklearn_available:\n     try:\n@@ -914,8 +915,8 @@ def is_seqio_available():\n     return _is_seqio_available\n \n \n-def is_gguf_available():\n-    return _is_gguf_available\n+def is_gguf_available(min_version: str = GGUF_MIN_VERSION):\n+    return _is_gguf_available and version.parse(_gguf_version) >= version.parse(min_version)\n \n \n def is_protobuf_available():"
        },
        {
            "sha": "c81df1910eb68bdf3d517aa7aaca51e8bc060bb7",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 134,
            "deletions": 2,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/edeca4387c527c3f8d35c1b941ebe3ecad9cd798/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=edeca4387c527c3f8d35c1b941ebe3ecad9cd798",
            "patch": "@@ -30,18 +30,32 @@\n class GgufIntegrationTests(unittest.TestCase):\n     original_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n     model_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n+    imatrix_model_id = \"duyntnet/TinyLlama-1.1B-Chat-v1.0-imatrix-GGUF\"\n     mistral_model_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n     qwen2_model_id = \"Qwen/Qwen1.5-0.5B-Chat-GGUF\"\n     llama3_model_id = \"NousResearch/Meta-Llama-3-8B-GGUF\"\n     tinyllama_model_id = \"PenutChen/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n \n+    # standard quants\n     q4_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_0.gguf\"\n-    q4_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n+    q5_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q5_0.gguf\"\n+    q8_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q8_0.gguf\"\n+    # k-quants\n     q2_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q2_K.gguf\"\n     q3_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q3_K_L.gguf\"\n+    q4_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n     q5_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf\"\n     q6_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"\n-    q8_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q8_0.gguf\"\n+    # imatrix\n+    iq1_m_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ1_M.gguf\"\n+    iq1_s_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ1_S.gguf\"\n+    iq2_s_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ2_S.gguf\"\n+    iq2_xs_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ2_XS.gguf\"\n+    iq2_xxs_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ2_XXS.gguf\"\n+    iq3_s_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ3_S.gguf\"\n+    iq3_xxs_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ3_XXS.gguf\"\n+    iq4_xs_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ4_XS.gguf\"\n+    iq4_nl_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ4_NL.gguf\"\n \n     q4_0_mistral_model_id = \"mistral-7b-instruct-v0.2.Q4_0.gguf\"\n     q4_0_qwen2_model_id = \"qwen1_5-0_5b-chat-q4_0.gguf\"\n@@ -87,6 +101,16 @@ def test_q3_k(self):\n         EXPECTED_TEXT = \"Hello, World!\\n\\n```\\n<|user\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n+    def test_q5_0(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q5_0_gguf_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(self.model_id, gguf_file=self.q5_0_gguf_model_id).to(torch_device)\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, World!\\n\\n5. Use a library\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n     def test_q5_k(self):\n         tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q5_k_gguf_model_id)\n         model = AutoModelForCausalLM.from_pretrained(self.model_id, gguf_file=self.q5_k_gguf_model_id).to(torch_device)\n@@ -151,6 +175,114 @@ def test_q8_0(self):\n         EXPECTED_TEXT = \"Hello, World!\\n\\n5. Use a library\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n+    def test_iq1_s(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq1_s_gguf_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq1_s_gguf_model_id).to(\n+            torch_device\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, I'm a friend of mine, I\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_iq1_m(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq1_m_gguf_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq1_m_gguf_model_id).to(\n+            torch_device\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, I am interested in purching a copy of\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_iq2_s(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq2_s_gguf_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq2_s_gguf_model_id).to(\n+            torch_device\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello World!\\n\\n```\\n<|user|\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_iq2_xs(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq2_xs_gguf_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq2_xs_gguf_model_id).to(\n+            torch_device\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello World!\\n\\n```\\n<|user|\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_iq2_xxs(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq2_xxs_gguf_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq2_xxs_gguf_model_id).to(\n+            torch_device\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, I'm a software engineer. I'\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_iq3_s(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq3_s_gguf_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq3_s_gguf_model_id).to(\n+            torch_device\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, World!\\n\\n5. Python:\\n\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_iq3_xxs(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq3_xxs_gguf_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq3_xxs_gguf_model_id).to(\n+            torch_device\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, I am interested in your product. Can you\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_iq4_xs(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq4_xs_gguf_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq4_xs_gguf_model_id).to(\n+            torch_device\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, world!\\n\\n5. Using a loop\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_iq4_nl(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq4_nl_gguf_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq4_nl_gguf_model_id).to(\n+            torch_device\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, world!\\n\\n5. Using a loop\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n     def test_f16(self):\n         tokenizer = AutoTokenizer.from_pretrained(self.tinyllama_model_id, gguf_file=self.f16_tinyllama_model_id)\n         model = AutoModelForCausalLM.from_pretrained("
        }
    ],
    "stats": {
        "total": 525,
        "additions": 169,
        "deletions": 356
    }
}