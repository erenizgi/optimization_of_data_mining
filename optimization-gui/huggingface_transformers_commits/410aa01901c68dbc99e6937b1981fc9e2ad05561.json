{
    "author": "gante",
    "message": "[chat] clean code and add base help (#37892)",
    "sha": "410aa01901c68dbc99e6937b1981fc9e2ad05561",
    "files": [
        {
            "sha": "f276368ce8bd8ade61f6c3ff8edf7f8c40bbe2b5",
            "filename": "src/transformers/commands/chat.py",
            "status": "modified",
            "additions": 246,
            "deletions": 238,
            "changes": 484,
            "blob_url": "https://github.com/huggingface/transformers/blob/410aa01901c68dbc99e6937b1981fc9e2ad05561/src%2Ftransformers%2Fcommands%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/410aa01901c68dbc99e6937b1981fc9e2ad05561/src%2Ftransformers%2Fcommands%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fchat.py?ref=410aa01901c68dbc99e6937b1981fc9e2ad05561",
            "patch": "@@ -44,190 +44,69 @@\n \n     from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextIteratorStreamer\n \n+\n ALLOWED_KEY_CHARS = set(string.ascii_letters + string.whitespace)\n ALLOWED_VALUE_CHARS = set(\n     string.ascii_letters + string.digits + string.whitespace + r\".!\\\"#$%&'()*+,\\-/:<=>?@[]^_`{|}~\"\n )\n \n-HELP_STRING = \"\"\"\\\n-\n-**TRANSFORMERS CHAT INTERFACE**\n-\n-The chat interface is a simple tool to try out a chat model.\n-\n-Besides talking to the model there are several commands:\n-- **help**: show this help message\n-- **clear**: clears the current conversation and start a new one\n-- **example {NAME}**: load example named `{NAME}` from the config and use it as the user input\n-- **set {SETTING_NAME}={SETTING_VALUE};**: change the system prompt or generation settings (multiple settings are separated by a ';').\n-- **reset**: same as clear but also resets the generation configs to defaults if they have been changed by **set**\n-- **save {SAVE_NAME} (optional)**: save the current chat and settings to file by default to `./chat_history/{MODEL_NAME}/chat_{DATETIME}.yaml` or `{SAVE_NAME}` if provided\n-- **exit**: closes the interface\n-\"\"\"\n-\n-SUPPORTED_GENERATION_KWARGS = [\n-    \"max_new_tokens\",\n-    \"do_sample\",\n-    \"num_beams\",\n-    \"temperature\",\n-    \"top_p\",\n-    \"top_k\",\n-    \"repetition_penalty\",\n-]\n-\n DEFAULT_EXAMPLES = {\n     \"llama\": {\"text\": \"There is a Llama in my lawn, how can I get rid of it?\"},\n     \"code\": {\n-        \"text\": \"Write a Python function that integrates any Python function f(x) numerically over an arbitrary interval [x_start, x_end].\"\n+        \"text\": (\n+            \"Write a Python function that integrates any Python function f(x) numerically over an arbitrary \"\n+            \"interval [x_start, x_end].\"\n+        ),\n     },\n     \"helicopter\": {\"text\": \"How many helicopters can a human eat in one sitting?\"},\n     \"numbers\": {\"text\": \"Count to 10 but skip every number ending with an 'e'\"},\n     \"birds\": {\"text\": \"Why aren't birds real?\"},\n     \"socks\": {\"text\": \"Why is it important to eat socks after meditating?\"},\n }\n \n+SUPPORTED_GENERATION_KWARGS = [\n+    \"max_new_tokens\",\n+    \"do_sample\",\n+    \"num_beams\",\n+    \"temperature\",\n+    \"top_p\",\n+    \"top_k\",\n+    \"repetition_penalty\",\n+]\n \n-def get_username():\n-    if platform.system() == \"Windows\":\n-        return os.getlogin()\n-    else:\n-        return pwd.getpwuid(os.getuid()).pw_name\n-\n-\n-def create_default_filename(model_name):\n-    time_str = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n-    return f\"{model_name}/chat_{time_str}.json\"\n-\n-\n-def save_chat(chat, args, filename):\n-    output_dict = {}\n-    output_dict[\"settings\"] = vars(args)\n-    output_dict[\"chat_history\"] = chat\n-\n-    folder = args.save_folder\n-\n-    if filename is None:\n-        filename = create_default_filename(args.model_name_or_path)\n-        filename = os.path.join(folder, filename)\n-    os.makedirs(os.path.dirname(filename), exist_ok=True)\n-\n-    with open(filename, \"w\") as f:\n-        json.dump(output_dict, f, indent=4)\n-    return os.path.abspath(filename)\n-\n-\n-def clear_chat_history(system_prompt):\n-    if system_prompt is None:\n-        chat = []\n-    else:\n-        chat = [{\"role\": \"system\", \"content\": system_prompt}]\n-    return chat\n-\n-\n-def parse_settings(user_input, current_args, interface):\n-    settings = user_input[4:].strip().split(\";\")\n-    settings = [(setting.split(\"=\")[0], setting[len(setting.split(\"=\")[0]) + 1 :]) for setting in settings]\n-    settings = dict(settings)\n-    error = False\n-\n-    for name in settings:\n-        if hasattr(current_args, name):\n-            try:\n-                if isinstance(getattr(current_args, name), bool):\n-                    if settings[name] == \"True\":\n-                        settings[name] = True\n-                    elif settings[name] == \"False\":\n-                        settings[name] = False\n-                    else:\n-                        raise ValueError\n-                else:\n-                    settings[name] = type(getattr(current_args, name))(settings[name])\n-            except ValueError:\n-                interface.print_red(\n-                    f\"Cannot cast setting {name} (={settings[name]}) to {type(getattr(current_args, name))}.\"\n-                )\n-        else:\n-            interface.print_red(f\"There is no '{name}' setting.\")\n-\n-    if error:\n-        interface.print_red(\"There was an issue parsing the settings. No settings have been changed.\")\n-        return current_args, False\n-    else:\n-        for name in settings:\n-            setattr(current_args, name, settings[name])\n-            interface.print_green(f\"Set {name} to {settings[name]}.\")\n-\n-        time.sleep(1.5)  # so the user has time to read the changes\n-        return current_args, True\n-\n-\n-def get_quantization_config(model_args) -> Optional[\"BitsAndBytesConfig\"]:\n-    if model_args.load_in_4bit:\n-        quantization_config = BitsAndBytesConfig(\n-            load_in_4bit=True,\n-            bnb_4bit_compute_dtype=model_args.torch_dtype,  # For consistency with model weights, we use the same value as `torch_dtype`\n-            bnb_4bit_quant_type=model_args.bnb_4bit_quant_type,\n-            bnb_4bit_use_double_quant=model_args.use_bnb_nested_quant,\n-            bnb_4bit_quant_storage=model_args.torch_dtype,\n-        )\n-    elif model_args.load_in_8bit:\n-        quantization_config = BitsAndBytesConfig(\n-            load_in_8bit=True,\n-        )\n-    else:\n-        quantization_config = None\n-\n-    return quantization_config\n-\n-\n-def load_model_and_tokenizer(args):\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        args.model_name_or_path,\n-        revision=args.model_revision,\n-        trust_remote_code=args.trust_remote_code,\n-    )\n-\n-    torch_dtype = args.torch_dtype if args.torch_dtype in [\"auto\", None] else getattr(torch, args.torch_dtype)\n-    quantization_config = get_quantization_config(args)\n-    model_kwargs = {\n-        \"revision\": args.model_revision,\n-        \"attn_implementation\": args.attn_implementation,\n-        \"torch_dtype\": torch_dtype,\n-        \"device_map\": \"auto\",\n-        \"quantization_config\": quantization_config,\n-    }\n-    model = AutoModelForCausalLM.from_pretrained(\n-        args.model_name_or_path, trust_remote_code=args.trust_remote_code, **model_kwargs\n-    )\n-\n-    if getattr(model, \"hf_device_map\", None) is None:\n-        model = model.to(args.device)\n-\n-    return model, tokenizer\n-\n+# Printed at the start of a chat session\n+HELP_STRING_MINIMAL = \"\"\"\n \n-def parse_eos_tokens(tokenizer, eos_tokens, eos_token_ids):\n-    if tokenizer.pad_token_id is None:\n-        pad_token_id = tokenizer.eos_token_id\n-    else:\n-        pad_token_id = tokenizer.pad_token_id\n+**TRANSFORMERS CHAT INTERFACE**\n \n-    all_eos_token_ids = []\n+Chat interface to try out a model. Besides chatting with the model, here are some basic commands:\n+- **help**: shows all available commands\n+- **clear**: clears the current conversation and starts a new one\n+- **exit**: closes the interface\n+\"\"\"\n \n-    if eos_tokens is not None:\n-        all_eos_token_ids.extend(tokenizer.convert_tokens_to_ids(eos_tokens.split(\",\")))\n \n-    if eos_token_ids is not None:\n-        all_eos_token_ids.extend([int(token_id) for token_id in eos_token_ids.split(\",\")])\n+# Printed when the user types `help` in the chat session\n+HELP_STRING = f\"\"\"\n \n-    if len(all_eos_token_ids) == 0:\n-        all_eos_token_ids.append(tokenizer.eos_token_id)\n+**TRANSFORMERS CHAT INTERFACE HELP**\n \n-    return pad_token_id, all_eos_token_ids\n+Full command list:\n+- **help**: shows this help message\n+- **clear**: clears the current conversation and starts a new one\n+- **example {{NAME}}**: loads example named `{{NAME}}` from the config and uses it as the user input. Available example\n+names: `{\"`, `\".join(DEFAULT_EXAMPLES.keys())}`\n+- **set {{SETTING_NAME}}={{SETTING_VALUE}};**: changes the system prompt or generation settings (multiple settings are\n+separated by a ';'). Available settings: `{\"`, `\".join(SUPPORTED_GENERATION_KWARGS)}`\n+- **reset**: same as clear but also resets the generation configs to defaults if they have been changed by **set**\n+- **save {{SAVE_NAME}} (optional)**: saves the current chat and settings to file by default to\n+`./chat_history/{{MODEL_NAME}}/chat_{{DATETIME}}.yaml` or `{{SAVE_NAME}}` if provided\n+- **exit**: closes the interface\n+\"\"\"\n \n \n class RichInterface:\n-    def __init__(self, model_name=None, user_name=None):\n+    def __init__(self, model_name: Optional[str] = None, user_name: Optional[str] = None):\n         self._console = Console()\n         if model_name is None:\n             self.model_name = \"assistant\"\n@@ -238,9 +117,10 @@ def __init__(self, model_name=None, user_name=None):\n         else:\n             self.user_name = user_name\n \n-    def stream_output(self, output_stream):\n-        \"\"\"Stream output from a role.\"\"\"\n-        # This method is originally from the FastChat CLI: https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/cli.py\n+    def stream_output(self, output_stream: TextIteratorStreamer) -> str:\n+        \"\"\"Stream output from a role, and return the generated text after it's done steaming.\"\"\"\n+        # This method is originally from the FastChat CLI:\n+        # https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/cli.py\n         # Create a Live context for updating the console output\n         text = \"\"\n         self._console.print(f\"[bold blue]<{self.model_name}>:\")\n@@ -276,93 +156,54 @@ def stream_output(self, output_stream):\n         self._console.print()\n         return text\n \n-    def input(self):\n+    def input(self) -> str:\n+        \"\"\"Gets user input from the console.\"\"\"\n         input = self._console.input(f\"[bold red]<{self.user_name}>:\\n\")\n         self._console.print()\n         return input\n \n     def clear(self):\n+        \"\"\"Clears the console.\"\"\"\n         self._console.clear()\n \n-    def print_user_message(self, text):\n+    def print_user_message(self, text: str):\n+        \"\"\"Prints a user message to the console.\"\"\"\n         self._console.print(f\"[bold red]<{self.user_name}>:[/ bold red]\\n{text}\")\n         self._console.print()\n \n-    def print_green(self, text):\n-        self._console.print(f\"[bold green]{text}\")\n+    def print_color(self, text: str, color: str):\n+        \"\"\"Prints text in a given color to the console.\"\"\"\n+        self._console.print(f\"[bold {color}]{text}\")\n         self._console.print()\n \n-    def print_red(self, text):\n-        self._console.print(f\"[bold red]{text}\")\n-        self._console.print()\n-\n-    def print_help(self):\n-        self._console.print(Markdown(HELP_STRING))\n+    def print_help(self, minimal: bool = False):\n+        \"\"\"Prints the help message to the console.\"\"\"\n+        self._console.print(Markdown(HELP_STRING_MINIMAL if minimal else HELP_STRING))\n         self._console.print()\n \n \n @dataclass\n class ChatArguments:\n     r\"\"\"\n-    Arguments for the chat script.\n-\n-    Args:\n-        model_name_or_path (`str`):\n-            Name of the pre-trained model.\n-        user (`str` or `None`, *optional*, defaults to `None`):\n-            Username to display in chat interface.\n-        system_prompt (`str` or `None`, *optional*, defaults to `None`):\n-            System prompt.\n-        save_folder (`str`, *optional*, defaults to `\"./chat_history/\"`):\n-            Folder to save chat history.\n-        device (`str`, *optional*, defaults to `\"cpu\"`):\n-            Device to use for inference.\n-        examples_path (`str` or `None`, *optional*, defaults to `None`):\n-            Path to a yaml file with examples.\n-        max_new_tokens (`int`, *optional*, defaults to `256`):\n-            Maximum number of tokens to generate.\n-        do_sample (`bool`, *optional*, defaults to `True`):\n-            Whether to sample outputs during generation.\n-        num_beams (`int`, *optional*, defaults to `1`):\n-            Number of beams for beam search.\n-        temperature (`float`, *optional*, defaults to `1.0`):\n-            Temperature parameter for generation.\n-        top_k (`int`, *optional*, defaults to `50`):\n-            Value of k for top-k sampling.\n-        top_p (`float`, *optional*, defaults to `1.0`):\n-            Value of p for nucleus sampling.\n-        repetition_penalty (`float`, *optional*, defaults to `1.0`):\n-            Repetition penalty.\n-        eos_tokens (`str` or `None`, *optional*, defaults to `None`):\n-            EOS tokens to stop the generation. If multiple they should be comma separated.\n-        eos_token_ids (`str` or `None`, *optional*, defaults to `None`):\n-            EOS token IDs to stop the generation. If multiple they should be comma separated.\n-        model_revision (`str`, *optional*, defaults to `\"main\"`):\n-            Specific model version to use (can be a branch name, tag name or commit id).\n-        torch_dtype (`str` or `None`, *optional*, defaults to `None`):\n-            Override the default `torch.dtype` and load the model under this dtype. If `'auto'` is passed, the dtype\n-            will be automatically derived from the model's weights.\n-        trust_remote_code (`bool`, *optional*, defaults to `False`):\n-            Whether to trust remote code when loading a model.\n-        attn_implementation (`str` or `None`, *optional*, defaults to `None`):\n-            Which attention implementation to use; you can run --attn_implementation=flash_attention_2, in which case\n-            you must install this manually by running `pip install flash-attn --no-build-isolation`.\n-        load_in_8bit (`bool`, *optional*, defaults to `False`):\n-            Whether to use 8 bit precision for the base model - works only with LoRA.\n-        load_in_4bit (`bool`, *optional*, defaults to `False`):\n-            Whether to use 4 bit precision for the base model - works only with LoRA.\n-        bnb_4bit_quant_type (`str`, *optional*, defaults to `\"nf4\"`):\n-            Quantization type.\n-        use_bnb_nested_quant (`bool`, *optional*, defaults to `False`):\n-            Whether to use nested quantization.\n+    Arguments for the chat CLI.\n+\n+    See the metadata arg for each argument's description -- the medatata will be printed with\n+    `transformers chat --help`\n     \"\"\"\n \n     # General settings\n-    model_name_or_path: Optional[str] = field(default=None, metadata={\"help\": \"Name of the pre-trained model.\"})\n-    user: Optional[str] = field(default=None, metadata={\"help\": \"Username to display in chat interface.\"})\n+    model_name_or_path: Optional[str] = field(\n+        default=None,\n+        metadata={\n+            \"help\": \"Name of the pre-trained model. The positional argument will take precedence if both are passed.\"\n+        },\n+    )\n+    user: Optional[str] = field(\n+        default=None,\n+        metadata={\"help\": \"Username to display in chat interface. Defaults to the current user's name.\"},\n+    )\n     system_prompt: Optional[str] = field(default=None, metadata={\"help\": \"System prompt.\"})\n     save_folder: str = field(default=\"./chat_history/\", metadata={\"help\": \"Folder to save chat history.\"})\n-    device: str = field(default=\"cpu\", metadata={\"help\": \"Device to use for inference.\"})\n     examples_path: Optional[str] = field(default=None, metadata={\"help\": \"Path to a yaml file with examples.\"})\n \n     # Generation settings\n@@ -387,6 +228,7 @@ class ChatArguments:\n         default=\"main\",\n         metadata={\"help\": \"Specific model version to use (can be a branch name, tag name or commit id).\"},\n     )\n+    device: str = field(default=\"cpu\", metadata={\"help\": \"Device to use for inference.\"})\n     torch_dtype: Optional[str] = field(\n         default=\"auto\",\n         metadata={\n@@ -434,7 +276,7 @@ def register_subcommand(parser: ArgumentParser):\n             parser: Root parser to register command-specific arguments\n         \"\"\"\n         dataclass_types = (ChatArguments,)\n-        chat_parser = parser.add_parser(\"chat\", help=HELP_STRING, dataclass_types=dataclass_types)\n+        chat_parser = parser.add_parser(\"chat\", dataclass_types=dataclass_types)\n \n         group = chat_parser.add_argument_group(\"Positional arguments\")\n         group.add_argument(\n@@ -447,10 +289,123 @@ def __init__(self, args):\n         args.model_name_or_path = args.model_name_or_path_positional or args.model_name_or_path\n \n         if args.model_name_or_path is None:\n-            raise ValueError(\"--model_name_or_path required for chat command.\")\n+            raise ValueError(\n+                \"One of the following must be provided:\"\n+                \"\\n- The positional argument containing the model repo;\"\n+                \"\\n- the optional --model_name_or_path argument, containing the model repo\"\n+                \"\\ne.g. transformers chat <model_repo> or transformers chat --model_name_or_path <model_repo>\"\n+            )\n \n         self.args = args\n \n+    # -----------------------------------------------------------------------------------------------------------------\n+    # Chat session methods\n+    @staticmethod\n+    def get_username() -> str:\n+        \"\"\"Returns the username of the current user.\"\"\"\n+        if platform.system() == \"Windows\":\n+            return os.getlogin()\n+        else:\n+            return pwd.getpwuid(os.getuid()).pw_name\n+\n+    @staticmethod\n+    def save_chat(chat, args: ChatArguments, filename: Optional[str] = None) -> str:\n+        \"\"\"Saves the chat history to a file.\"\"\"\n+        output_dict = {}\n+        output_dict[\"settings\"] = vars(args)\n+        output_dict[\"chat_history\"] = chat\n+\n+        folder = args.save_folder\n+\n+        if filename is None:\n+            time_str = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n+            filename = f\"{args.model_name_or_path}/chat_{time_str}.json\"\n+            filename = os.path.join(folder, filename)\n+\n+        os.makedirs(os.path.dirname(filename), exist_ok=True)\n+        with open(filename, \"w\") as f:\n+            json.dump(output_dict, f, indent=4)\n+        return os.path.abspath(filename)\n+\n+    @staticmethod\n+    def clear_chat_history(system_prompt: Optional[str] = None) -> list[dict]:\n+        \"\"\"Clears the chat history.\"\"\"\n+        if system_prompt is None:\n+            chat = []\n+        else:\n+            chat = [{\"role\": \"system\", \"content\": system_prompt}]\n+        return chat\n+\n+    # -----------------------------------------------------------------------------------------------------------------\n+    # Input parsing methods\n+    @staticmethod\n+    def parse_settings(\n+        user_input: str, current_args: ChatArguments, interface: RichInterface\n+    ) -> tuple[ChatArguments, bool]:\n+        \"\"\"Parses the settings from the user input into the CLI arguments.\"\"\"\n+        settings = user_input[4:].strip().split(\";\")\n+        settings = [(setting.split(\"=\")[0], setting[len(setting.split(\"=\")[0]) + 1 :]) for setting in settings]\n+        settings = dict(settings)\n+        error = False\n+\n+        for name in settings:\n+            if hasattr(current_args, name):\n+                try:\n+                    if isinstance(getattr(current_args, name), bool):\n+                        if settings[name] == \"True\":\n+                            settings[name] = True\n+                        elif settings[name] == \"False\":\n+                            settings[name] = False\n+                        else:\n+                            raise ValueError\n+                    else:\n+                        settings[name] = type(getattr(current_args, name))(settings[name])\n+                except ValueError:\n+                    error = True\n+                    interface.print_color(\n+                        text=f\"Cannot cast setting {name} (={settings[name]}) to {type(getattr(current_args, name))}.\",\n+                        color=\"red\",\n+                    )\n+            else:\n+                interface.print_color(text=f\"There is no '{name}' setting.\", color=\"red\")\n+\n+        if error:\n+            interface.print_color(\n+                text=\"There was an issue parsing the settings. No settings have been changed.\",\n+                color=\"red\",\n+            )\n+        else:\n+            for name in settings:\n+                setattr(current_args, name, settings[name])\n+                interface.print_color(text=f\"Set {name} to {settings[name]}.\", color=\"green\")\n+\n+            time.sleep(1.5)  # so the user has time to read the changes\n+\n+        return current_args, not error\n+\n+    @staticmethod\n+    def parse_eos_tokens(\n+        tokenizer: AutoTokenizer, eos_tokens: Optional[str], eos_token_ids: Optional[str]\n+    ) -> tuple[int, list[int]]:\n+        \"\"\"Retrieves the pad token ID and all possible EOS token IDs.\"\"\"\n+        if tokenizer.pad_token_id is None:\n+            pad_token_id = tokenizer.eos_token_id\n+        else:\n+            pad_token_id = tokenizer.pad_token_id\n+\n+        all_eos_token_ids = []\n+\n+        if eos_tokens is not None:\n+            all_eos_token_ids.extend(tokenizer.convert_tokens_to_ids(eos_tokens.split(\",\")))\n+\n+        if eos_token_ids is not None:\n+            all_eos_token_ids.extend([int(token_id) for token_id in eos_token_ids.split(\",\")])\n+\n+        if len(all_eos_token_ids) == 0:\n+            all_eos_token_ids.append(tokenizer.eos_token_id)\n+\n+        return pad_token_id, all_eos_token_ids\n+\n     @staticmethod\n     def is_valid_setting_command(s: str) -> bool:\n         # First check the basic structure\n@@ -481,6 +436,55 @@ def is_valid_setting_command(s: str) -> bool:\n \n         return True\n \n+    # -----------------------------------------------------------------------------------------------------------------\n+    # Model loading and performance automation methods\n+    @staticmethod\n+    def get_quantization_config(model_args: ChatArguments) -> Optional[\"BitsAndBytesConfig\"]:\n+        if model_args.load_in_4bit:\n+            quantization_config = BitsAndBytesConfig(\n+                load_in_4bit=True,\n+                # For consistency with model weights, we use the same value as `torch_dtype`\n+                bnb_4bit_compute_dtype=model_args.torch_dtype,\n+                bnb_4bit_quant_type=model_args.bnb_4bit_quant_type,\n+                bnb_4bit_use_double_quant=model_args.use_bnb_nested_quant,\n+                bnb_4bit_quant_storage=model_args.torch_dtype,\n+            )\n+        elif model_args.load_in_8bit:\n+            quantization_config = BitsAndBytesConfig(\n+                load_in_8bit=True,\n+            )\n+        else:\n+            quantization_config = None\n+\n+        return quantization_config\n+\n+    def load_model_and_tokenizer(self, args: ChatArguments) -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n+        tokenizer = AutoTokenizer.from_pretrained(\n+            args.model_name_or_path,\n+            revision=args.model_revision,\n+            trust_remote_code=args.trust_remote_code,\n+        )\n+\n+        torch_dtype = args.torch_dtype if args.torch_dtype in [\"auto\", None] else getattr(torch, args.torch_dtype)\n+        quantization_config = self.get_quantization_config(args)\n+        model_kwargs = {\n+            \"revision\": args.model_revision,\n+            \"attn_implementation\": args.attn_implementation,\n+            \"torch_dtype\": torch_dtype,\n+            \"device_map\": \"auto\",\n+            \"quantization_config\": quantization_config,\n+        }\n+        model = AutoModelForCausalLM.from_pretrained(\n+            args.model_name_or_path, trust_remote_code=args.trust_remote_code, **model_kwargs\n+        )\n+\n+        if getattr(model, \"hf_device_map\", None) is None:\n+            model = model.to(args.device)\n+\n+        return model, tokenizer\n+\n+    # -----------------------------------------------------------------------------------------------------------------\n+    # Main logic\n     def run(self):\n         if not is_rich_available():\n             raise ImportError(\"You need to install rich to use the chat interface. (`pip install rich`)\")\n@@ -497,24 +501,27 @@ def run(self):\n         current_args = copy.deepcopy(args)\n \n         if args.user is None:\n-            user = get_username()\n+            user = self.get_username()\n         else:\n             user = args.user\n \n-        model, tokenizer = load_model_and_tokenizer(args)\n+        model, tokenizer = self.load_model_and_tokenizer(args)\n         generation_streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n \n-        pad_token_id, eos_token_ids = parse_eos_tokens(tokenizer, args.eos_tokens, args.eos_token_ids)\n+        pad_token_id, eos_token_ids = self.parse_eos_tokens(tokenizer, args.eos_tokens, args.eos_token_ids)\n \n         interface = RichInterface(model_name=args.model_name_or_path, user_name=user)\n         interface.clear()\n-        chat = clear_chat_history(current_args.system_prompt)\n+        chat = self.clear_chat_history(current_args.system_prompt)\n+\n+        # Starts the session with a minimal help message at the top, so that a user doesn't get stuck\n+        interface.print_help(minimal=True)\n         while True:\n             try:\n                 user_input = interface.input()\n \n                 if user_input == \"clear\":\n-                    chat = clear_chat_history(current_args.system_prompt)\n+                    chat = self.clear_chat_history(current_args.system_prompt)\n                     interface.clear()\n                     continue\n \n@@ -528,7 +535,7 @@ def run(self):\n                 if user_input == \"reset\":\n                     interface.clear()\n                     current_args = copy.deepcopy(args)\n-                    chat = clear_chat_history(current_args.system_prompt)\n+                    chat = self.clear_chat_history(current_args.system_prompt)\n                     continue\n \n                 if user_input.startswith(\"save\") and len(user_input.split()) < 2:\n@@ -538,12 +545,12 @@ def run(self):\n                         filename = split_input[1]\n                     else:\n                         filename = None\n-                    filename = save_chat(chat, current_args, filename)\n-                    interface.print_green(f\"Chat saved in {filename}!\")\n+                    filename = self.save_chat(chat, current_args, filename)\n+                    interface.print_color(text=f\"Chat saved in {filename}!\", color=\"green\")\n                     continue\n \n                 if self.is_valid_setting_command(user_input):\n-                    current_args, success = parse_settings(user_input, current_args, interface)\n+                    current_args, success = self.parse_settings(user_input, current_args, interface)\n                     if success:\n                         chat = []\n                         interface.clear()\n@@ -557,9 +564,10 @@ def run(self):\n                         interface.print_user_message(examples[example_name][\"text\"])\n                         user_input = examples[example_name][\"text\"]\n                     else:\n-                        interface.print_red(\n+                        example_error = (\n                             f\"Example {example_name} not found in list of available examples: {list(examples.keys())}.\"\n                         )\n+                        interface.print_color(text=example_error, color=\"red\")\n                         continue\n \n                 chat.append({\"role\": \"user\", \"content\": user_input})"
        }
    ],
    "stats": {
        "total": 484,
        "additions": 246,
        "deletions": 238
    }
}