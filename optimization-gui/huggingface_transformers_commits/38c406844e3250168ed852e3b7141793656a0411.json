{
    "author": "MekkCyber",
    "message": "Fixing quantization tests (#37650)\n\n* fix\n\n* style\n\n* add capability check",
    "sha": "38c406844e3250168ed852e3b7141793656a0411",
    "files": [
        {
            "sha": "055f736e12fb7ea7d7460d22821c516ca5de0bed",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/38c406844e3250168ed852e3b7141793656a0411/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38c406844e3250168ed852e3b7141793656a0411/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=38c406844e3250168ed852e3b7141793656a0411",
            "patch": "@@ -110,7 +110,7 @@ class AwqTest(unittest.TestCase):\n     input_text = \"Hello my name is\"\n \n     EXPECTED_OUTPUT = \"Hello my name is Katie and I am a 20 year old student at the University of North Carolina at Chapel Hill. I am a junior and I am majoring in Journalism and minoring in Spanish\"\n-    EXPECTED_OUTPUT_BF16 = \"Hello my name is Katie and I am a 20 year old student at the University of North Carolina at Chapel Hill. I am a junior and I am majoring in Exercise and Sport Science with a\"\n+    EXPECTED_OUTPUT_BF16 = \"Hello my name is Katie and I am a 20 year old student at the University of North Carolina at Chapel Hill. I am a junior and I am majoring in Journalism and minoring in Spanish\"\n \n     EXPECTED_OUTPUT_EXLLAMA = [\n         \"Hello my name is Katie and I am a 20 year old student from the UK. I am currently studying for a degree in English Literature and History at the University of York. I am a very out\",\n@@ -299,7 +299,7 @@ class AwqFusedTest(unittest.TestCase):\n         \"You end up exactly where you started. Where are you?\"\n     )\n \n-    EXPECTED_GENERATION = prompt + \"\\n\\nThis is a classic puzzle that has been around for\"\n+    EXPECTED_GENERATION = prompt + \"\\n\\nYou're at the center of a square.\"\n     EXPECTED_GENERATION_CUSTOM_MODEL = \"Hello,\\n\\nI have a problem with my 20\"\n     EXPECTED_GENERATION_MIXTRAL = prompt + \" You're on the North Pole.\\n\\nThe\"\n \n@@ -355,6 +355,10 @@ def test_fused_modules_to_not_convert(self):\n         # Checks if the modules_to_not_convert (here gate layer) is a Linear\n         self.assertTrue(isinstance(model.model.layers[0].block_sparse_moe.gate, torch.nn.Linear))\n \n+    @unittest.skipIf(\n+        torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8,\n+        \"Skipping because RuntimeError: FlashAttention only supports Ampere GPUs or newer, so not supported on GPU with capability < 8.0\",\n+    )\n     def test_generation_fused(self):\n         \"\"\"\n         Test generation quality for fused models - single batch case\n@@ -378,6 +382,10 @@ def test_generation_fused(self):\n \n         self.assertEqual(tokenizer.decode(outputs[0], skip_special_tokens=True), self.EXPECTED_GENERATION)\n \n+    @unittest.skipIf(\n+        torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8,\n+        \"Skipping because RuntimeError: FlashAttention only supports Ampere GPUs or newer, so not supported on GPU with capability < 8.0\",\n+    )\n     def test_generation_fused_batched(self):\n         \"\"\"\n         Test generation quality for fused models - multi batch case\n@@ -426,6 +434,10 @@ def test_generation_llava_fused(self):\n         self.assertEqual(outputs[0][\"generated_text\"], EXPECTED_OUTPUT)\n \n     @require_torch_multi_gpu\n+    @unittest.skipIf(\n+        torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8,\n+        \"Skipping because RuntimeError: FlashAttention only supports Ampere GPUs or newer, so not supported on GPU with capability < 8.0\",\n+    )\n     def test_generation_custom_model(self):\n         \"\"\"\n         Test generation quality for fused models using custom fused map."
        },
        {
            "sha": "b80de8d45d967da62a4ce9e1422ad74dd180340a",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/38c406844e3250168ed852e3b7141793656a0411/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38c406844e3250168ed852e3b7141793656a0411/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=38c406844e3250168ed852e3b7141793656a0411",
            "patch": "@@ -904,6 +904,7 @@ def test_gemma3_qat_q4_0(self):\n         out = model.generate(text, max_new_tokens=10)\n \n         EXPECTED_TEXT = 'Hello with the prompt, \"What is the best way'\n+\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n     @require_read_token"
        },
        {
            "sha": "766faafbbfaf6c4f4c5efaf531cfb0f951b6f61f",
            "filename": "tests/quantization/quanto_integration/test_quanto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/38c406844e3250168ed852e3b7141793656a0411/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38c406844e3250168ed852e3b7141793656a0411/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py?ref=38c406844e3250168ed852e3b7141793656a0411",
            "patch": "@@ -30,7 +30,7 @@\n if is_torch_available():\n     import torch\n \n-    from transformers import LlamaForCausalLM, LlamaTokenizer\n+    from transformers import LlamaForCausalLM\n \n if is_accelerate_available():\n     from accelerate import init_empty_weights\n@@ -455,7 +455,7 @@ def test_quantized_cache(self):\n             \"Simply put, the theory of relativity states that \",\n             \"My favorite all time favorite condiment is ketchup.\",\n         ]\n-        tokenizer = LlamaTokenizer.from_pretrained(\n+        tokenizer = AutoTokenizer.from_pretrained(\n             \"unsloth/Llama-3.2-1B-Instruct\", pad_token=\"</s>\", padding_side=\"left\"\n         )\n         model = LlamaForCausalLM.from_pretrained("
        },
        {
            "sha": "22d0eb5293a9b78402f8bb42cfd876a0087a2808",
            "filename": "tests/quantization/quark_integration/test_quark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/38c406844e3250168ed852e3b7141793656a0411/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38c406844e3250168ed852e3b7141793656a0411/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py?ref=38c406844e3250168ed852e3b7141793656a0411",
            "patch": "@@ -54,6 +54,7 @@ class QuarkTest(unittest.TestCase):\n     EXPECTED_OUTPUTS.add(\"Today I am in Paris and I am enjoying the city of light. I am not just any ordinary Paris\")\n     EXPECTED_OUTPUTS.add(\"Today I am in Paris and I am enjoying my day off! The sun is shining, the birds are\")\n     EXPECTED_OUTPUTS.add(\"Today I am in Paris and I'm here to tell you about it. It's a beautiful day,\")\n+    EXPECTED_OUTPUTS.add(\"Today I am in Paris and I am not in Paris at all! I am not in Paris, but\")\n \n     EXPECTED_RELATIVE_DIFFERENCE = 1.66\n     device_map = None"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 18,
        "deletions": 4
    }
}