{
    "author": "winglian",
    "message": "FSDP grad accum fix (#34645)\n\n* add gradient accumulation steps tests for fsdp\r\n\r\n* invert no_sync context to fix training for fsdp",
    "sha": "b0c0ba7b4dd53635e6401e207c0d33231d273551",
    "files": [
        {
            "sha": "fec4bc4d6b283ceae0c385dd98f4fefce1e2caaf",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c0ba7b4dd53635e6401e207c0d33231d273551/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c0ba7b4dd53635e6401e207c0d33231d273551/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=b0c0ba7b4dd53635e6401e207c0d33231d273551",
            "patch": "@@ -2488,7 +2488,7 @@ def _inner_training_loop(\n                     # We explicitly want to avoid relying on `accelerator.accumulate` for generation training\n                     context = (\n                         functools.partial(self.accelerator.no_sync, model=model)\n-                        if i == len(batch_samples) - 1\n+                        if i != len(batch_samples) - 1\n                         else contextlib.nullcontext\n                     )\n                     with context():"
        },
        {
            "sha": "74a3bfe04b7506aa37b3bd45f0386d2d9fb09f7e",
            "filename": "tests/fsdp/test_fsdp.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0c0ba7b4dd53635e6401e207c0d33231d273551/tests%2Ffsdp%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0c0ba7b4dd53635e6401e207c0d33231d273551/tests%2Ffsdp%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffsdp%2Ftest_fsdp.py?ref=b0c0ba7b4dd53635e6401e207c0d33231d273551",
            "patch": "@@ -224,6 +224,18 @@ def test_basic_run(self, sharding_strategy, dtype):\n         cmd = launcher + script + args + fsdp_args\n         execute_subprocess_async(cmd, env=self.get_env())\n \n+    @parameterized.expand(params, name_func=_parameterized_custom_name_func)\n+    @require_torch_multi_accelerator\n+    @slow\n+    def test_basic_run_with_gradient_accumulation(self, sharding_strategy, dtype):\n+        launcher = get_launcher(distributed=True, use_accelerate=False)\n+        output_dir = self.get_auto_remove_tmp_dir()\n+        args = self.get_base_args(output_dir, 1, 50).split() + [f\"--{dtype}\", \"--gradient_accumulation_steps\", \"2\"]\n+        fsdp_args = [\"--fsdp\", f\"{sharding_strategy} auto_wrap\", \"--fsdp_transformer_layer_cls_to_wrap\", \"BertLayer\"]\n+        script = [f\"{self.examples_dir_str}/pytorch/text-classification/run_glue.py\"]\n+        cmd = launcher + script + args + fsdp_args\n+        execute_subprocess_async(cmd, env=self.get_env())\n+\n     @parameterized.expand(dtypes)\n     @require_torch_multi_accelerator\n     @slow"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 13,
        "deletions": 1
    }
}