{
    "author": "ArthurZucker",
    "message": "use `TokenizersBackend` (#42894)\n\n* us `TokenizersBackend`\n\n* fixes\n\n* pioritize mapping\n\n* pioritize mapping\n\n* only use mapping for some models\n\n* fix fallback\n\n* undo debug thing\n\n* add case to tokenizersbackend init\n\n* add default bos eos token to tok backend\n\n* set bos eos\n\n* fix more models\n\n* mistrla idefics\n\n* fix stopping criteria test\n\n* fix stopping criteria test\n\n* try stopping criteria fix\n\n* rebase\n\n* update tokenizer model for stopping criteria test\n\n* fix tuple mapping for ministral\n\n* ignore `tokenizer_class` as it is always wrong\n\n* up\n\n* try to fix idefics\n\n* fix unispeech and maybe other: fallback if conversion was not possible to the saveclass\n\n* nits\n\n* fixup\n\n* TIL that it was ALSO saved in config.json...\n\n* arf\n\n* fallback to tok config if no config json\n\n* people who map to Llama probably don't even want llama either..\n\n* processors to load tokbackend\n\n* auto fix order\n\n* try diff order\n\n* mistral fix for weird chars\n\n* reorder\n\n* random fix attempt for failing tests that are failing locally so idk how to check these\n\n* trying an older commit\n\n* fix mistral\n\n* map unispeech\n\n* try something out\n\n* update\n\n* nits\n\n* trying to be a little bit more restrictive\n\n* token type ids for tokenizers should be explicits... let's see which test fail this and we'll add to the specific classes?\n\n* Nit\n\n* idefics 1-2 are actually the only ones that should map to llama force\n\n* small fixes\n\n* fix layout\n\n* fixup\n\n* fix some tests\n\n* 1 nit\n\n* aria fix\n\n* style\n\n* canine\n\n* fixup\n\n* very small test\n\n* style\n\n* update to tokenizersbackend\n\n---------\n\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-164-45.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-168-52.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-174-196.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-167-217.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-167-111.ec2.internal>\nCo-authored-by: itazap <ita.zaporozhets@huggingface.co>\nCo-authored-by: Ita Zaporozhets <31893021+itazap@users.noreply.github.com>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-164-75.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-160-100.ec2.internal>",
    "sha": "9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
    "files": [
        {
            "sha": "b075e6d5ccf7e920e6416e27c3dd71a860fec47d",
            "filename": "docs/source/en/model_doc/parakeet.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/docs%2Fsource%2Fen%2Fmodel_doc%2Fparakeet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/docs%2Fsource%2Fen%2Fmodel_doc%2Fparakeet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fparakeet.md?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -188,9 +188,9 @@ outputs = model(**inputs)\n outputs.loss.backward()\n ```\n \n-## ParakeetTokenizerFast\n+## ParakeetTokenizer\n \n-[[autodoc]] ParakeetTokenizerFast\n+[[autodoc]] ParakeetTokenizer\n \n ## ParakeetFeatureExtractor\n "
        },
        {
            "sha": "a885c380bc5800cdf86316fd2154f437b00c808c",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -293,7 +293,7 @@ def _get_device_map(\n         # especially if the model uses WeightConverter (because there will be some uncontrollable cpu memory spikes during\n         # the conversions before we resave the weights). In those cases, it's better to offload to disk a bit more\n         # if we were in-between, as otherwise we blow-up cpu memory\n-        if max_memory is None:\n+        if max_memory is None and \"cpu\" in inferred_max_memory:\n             inferred_max_memory[\"cpu\"] *= 0.90\n \n         if hf_quantizer is not None:"
        },
        {
            "sha": "8a263666303d042a3606674865995fc2dd4b3d6d",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 126,
            "deletions": 155,
            "changes": 281,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -63,11 +63,9 @@\n \n TOKENIZER_MAPPING_NAMES = OrderedDict[str, Optional[str]](\n     [\n-        (\"aimv2\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"aimv2\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n         (\"albert\", \"AlbertTokenizer\" if is_tokenizers_available() else None),\n         (\"align\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"arcee\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"aria\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"audioflamingo3\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n         (\"aya_vision\", \"CohereTokenizer\" if is_tokenizers_available() else None),\n         (\"bark\", \"BertTokenizer\" if is_tokenizers_available() else None),\n@@ -81,19 +79,15 @@\n         (\"big_bird\", \"BigBirdTokenizer\" if is_tokenizers_available() else None),\n         (\"bigbird_pegasus\", \"PegasusTokenizer\" if is_tokenizers_available() else None),\n         (\"biogpt\", \"BioGptTokenizer\"),\n-        (\"bitnet\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"blenderbot\", \"BlenderbotTokenizer\" if is_tokenizers_available() else None),\n         (\"blenderbot-small\", \"BlenderbotSmallTokenizer\"),\n         (\"blip\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"blip-2\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n-        (\"bloom\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"blt\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"bridgetower\", \"RobertaTokenizer\"),\n         (\"bros\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"byt5\", \"ByT5Tokenizer\"),\n         (\"camembert\", \"CamembertTokenizer\" if is_tokenizers_available() else None),\n         (\"canine\", \"CanineTokenizer\"),\n-        (\"chameleon\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"chinese_clip\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"clap\", \"RobertaTokenizer\"),\n         (\"clip\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n@@ -103,193 +97,141 @@\n         (\"codegen\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"cohere\", \"CohereTokenizer\" if is_tokenizers_available() else None),\n         (\"cohere2\", \"CohereTokenizer\" if is_tokenizers_available() else None),\n-        (\"colpali\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"colqwen2\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"colqwen2\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n         (\"convbert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"cpm\", \"CpmTokenizer\" if is_tokenizers_available() else None),\n         (\"cpmant\", \"CpmAntTokenizer\"),\n-        (\"csm\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"ctrl\", \"CTRLTokenizer\"),\n         (\"data2vec-audio\", \"Wav2Vec2CTCTokenizer\"),\n         (\"data2vec-text\", \"RobertaTokenizer\"),\n         (\"dbrx\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"deberta\", \"DebertaTokenizer\" if is_tokenizers_available() else None),\n         (\"deberta-v2\", \"DebertaV2Tokenizer\" if is_tokenizers_available() else None),\n-        (\"deepseek_v2\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"deepseek_v3\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"deepseek_vl\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"deepseek_vl_hybrid\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"dia\", \"DiaTokenizer\"),\n-        (\"diffllama\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"distilbert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"dpr\", \"DPRQuestionEncoderTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"dpr\", \"DPRQuestionEncoderTokenizer\" if is_tokenizers_available() else None),\n         (\"electra\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"emu3\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"ernie\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"ernie4_5\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"ernie4_5_moe\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"ernie4_5_vl_moe\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"esm\", \"EsmTokenizer\"),\n         (\"exaone4\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n-        (\"falcon\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"falcon_mamba\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"falcon_mamba\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n         (\"fastspeech2_conformer\", \"FastSpeech2ConformerTokenizer\" if is_g2p_en_available() else None),\n         (\"flaubert\", \"FlaubertTokenizer\"),\n         (\"flava\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"flex_olmo\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"florence2\", \"BartTokenizer\" if is_tokenizers_available() else None),\n-        (\"fnet\", \"FNetTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"fnet\", \"FNetTokenizer\" if is_tokenizers_available() else None),\n         (\"fsmt\", \"FSMTTokenizer\"),\n         (\"funnel\", \"FunnelTokenizer\" if is_tokenizers_available() else None),\n-        (\"fuyu\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"gemma\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"gemma2\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"gemma3\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"gemma3_text\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"gemma3n\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"gemma3n_text\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"gemma\", \"GemmaTokenizer\" if is_tokenizers_available() else None),\n+        (\"gemma2\", \"GemmaTokenizer\" if is_tokenizers_available() else None),\n+        (\"gemma3\", \"GemmaTokenizer\" if is_tokenizers_available() else None),\n+        (\"gemma3_text\", \"GemmaTokenizer\" if is_tokenizers_available() else None),\n+        (\"gemma3n\", \"GemmaTokenizer\" if is_tokenizers_available() else None),\n+        (\"gemma3n_text\", \"GemmaTokenizer\" if is_tokenizers_available() else None),\n         (\"git\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"glm\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"glm4\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"glm4_moe\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"glm4v\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"glm4v_moe\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"glmasr\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"got_ocr2\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"gpt-sw3\", \"GPTSw3Tokenizer\" if is_sentencepiece_available() else None),\n         (\"gpt2\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"gpt_bigcode\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"gpt_neo\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"gpt_neox\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n         (\"gpt_neox_japanese\", \"GPTNeoXJapaneseTokenizer\"),\n-        (\"gpt_oss\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"gptj\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"granite\", \"GPT2Tokenizer\"),\n         (\"granitemoe\", \"GPT2Tokenizer\"),\n         (\"granitemoehybrid\", \"GPT2Tokenizer\"),\n         (\"granitemoeshared\", \"GPT2Tokenizer\"),\n         (\"grounding-dino\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"groupvit\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"helium\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n+        (\"groupvit\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n         (\"herbert\", \"HerbertTokenizer\" if is_tokenizers_available() else None),\n         (\"hubert\", \"Wav2Vec2CTCTokenizer\"),\n         (\"ibert\", \"RobertaTokenizer\"),\n         (\"idefics\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"idefics2\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"idefics3\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"instructblip\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"instructblipvideo\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n-        (\"internvl\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"internvl\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n         (\"jais2\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n-        (\"jamba\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"janus\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"jetmoe\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"kosmos-2\", \"XLMRobertaTokenizer\" if is_tokenizers_available() else None),\n-        (\"kosmos-2.5\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"lasr_ctc\", \"ParakeetTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"lasr_encoder\", \"ParakeetTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"lasr_ctc\", \"ParakeetTokenizer\" if is_tokenizers_available() else None),\n+        (\"lasr_encoder\", \"ParakeetTokenizer\" if is_tokenizers_available() else None),\n         (\"layoutlm\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"layoutlmv2\", \"LayoutLMv2Tokenizer\" if is_tokenizers_available() else None),\n         (\"layoutlmv3\", \"LayoutLMv3Tokenizer\" if is_tokenizers_available() else None),\n         (\"layoutxlm\", \"LayoutXLMTokenizer\" if is_tokenizers_available() else None),\n         (\"led\", \"LEDTokenizer\" if is_tokenizers_available() else None),\n-        (\"lfm2_vl\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"lilt\", \"RobertaTokenizer\" if is_tokenizers_available() else None),\n-        (\"llama\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"llama4\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"llama4_text\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"llava\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"llava_next\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"llava_next_video\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"llava_onevision\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"longformer\", \"RobertaTokenizer\" if is_tokenizers_available() else None),\n         (\"longt5\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n         (\"luke\", \"LukeTokenizer\"),\n         (\"lxmert\", \"LxmertTokenizer\" if is_tokenizers_available() else None),\n         (\"m2m_100\", \"M2M100Tokenizer\" if is_sentencepiece_available() else None),\n-        (\"mamba\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"mamba2\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"mamba\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n+        (\"mamba2\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n         (\"marian\", \"MarianTokenizer\" if is_sentencepiece_available() else None),\n+        (\"markuplm\", \"MarkupLMTokenizer\" if is_tokenizers_available() else None),\n         (\"mbart\", \"MBartTokenizer\" if is_tokenizers_available() else None),\n         (\"mbart50\", \"MBart50Tokenizer\" if is_tokenizers_available() else None),\n         (\"mega\", \"RobertaTokenizer\"),\n         (\"megatron-bert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"metaclip_2\", \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"metaclip_2\", \"XLMRobertaTokenizer\" if is_tokenizers_available() else None),\n         (\"mgp-str\", \"MgpstrTokenizer\"),\n-        (\"minimax\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\n             \"ministral3\",\n             \"MistralCommonBackend\"\n             if is_mistral_common_available()\n-            else (\n-                \"LlamaTokenizer\"\n-                if is_tokenizers_available()\n-                else (\"LlamaTokenizer\" if is_sentencepiece_available() else None)\n-            ),\n+            else (\"TokenizersBackend\" if is_tokenizers_available() else None),\n         ),\n         (\n             \"mistral\",\n             \"MistralCommonBackend\"\n             if is_mistral_common_available()\n-            else (\"LlamaTokenizer\" if is_tokenizers_available() else None),\n+            else (\"TokenizersBackend\" if is_tokenizers_available() else None),\n         ),\n         (\n             \"mistral3\",\n             \"MistralCommonBackend\"\n             if is_mistral_common_available()\n-            else (\n-                \"LlamaTokenizer\"\n-                if is_tokenizers_available()\n-                else (\"LlamaTokenizer\" if is_sentencepiece_available() else None)\n-            ),\n+            else (\"TokenizersBackend\" if is_tokenizers_available() else None),\n         ),\n         (\n             \"mixtral\",\n             \"MistralCommonBackend\"\n             if is_mistral_common_available()\n-            else (\"LlamaTokenizer\" if is_tokenizers_available() else None),\n+            else (\"TokenizersBackend\" if is_tokenizers_available() else None),\n         ),\n-        (\"mllama\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"mluke\", \"MLukeTokenizer\" if is_sentencepiece_available() else None),\n         (\"mm-grounding-dino\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"mobilebert\", \"MobileBertTokenizer\" if is_tokenizers_available() else None),\n-        (\"modernbert\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"moonshine\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"moshi\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"mpnet\", \"MPNetTokenizer\" if is_tokenizers_available() else None),\n-        (\"mpt\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"mpt\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n         (\"mra\", \"RobertaTokenizer\"),\n         (\"mt5\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n         (\"musicgen\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n         (\"musicgen_melody\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n         (\"mvp\", \"MvpTokenizer\" if is_tokenizers_available() else None),\n         (\"myt5\", \"MyT5Tokenizer\"),\n-        (\"nemotron\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"nezha\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"nllb\", \"NllbTokenizer\" if is_tokenizers_available() else None),\n         (\"nllb-moe\", \"NllbTokenizer\" if is_tokenizers_available() else None),\n         (\"nougat\", \"NougatTokenizer\" if is_tokenizers_available() else None),\n-        (\"nystromformer\", \"AlbertTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"olmo\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"olmo2\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"nystromformer\", \"AlbertTokenizer\" if is_tokenizers_available() else None),\n+        (\"olmo\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n+        (\"olmo2\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n         (\"olmo3\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n-        (\"olmoe\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"omdet-turbo\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"oneformer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"olmoe\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n+        (\"omdet-turbo\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n+        (\"oneformer\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n         (\"openai-gpt\", \"OpenAIGPTTokenizer\" if is_tokenizers_available() else None),\n         (\"opt\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n-        (\"ovis2\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n-        (\"owlv2\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"owlvit\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"paddleocr_vl\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n-        (\"paligemma\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"ovis2\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"owlv2\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n+        (\"owlvit\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n         (\"pegasus\", \"PegasusTokenizer\" if is_tokenizers_available() else None),\n         (\"pegasus_x\", \"PegasusTokenizer\" if is_tokenizers_available() else None),\n         (\"perceiver\", \"PerceiverTokenizer\"),\n-        (\"persimmon\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"phi\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n-        (\"phi3\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"phimoe\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"phobert\", \"PhobertTokenizer\"),\n         (\"pix2struct\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n         (\n@@ -301,76 +243,73 @@\n         (\"plbart\", \"PLBartTokenizer\" if is_tokenizers_available() else None),\n         (\"prophetnet\", \"ProphetNetTokenizer\"),\n         (\"qdqbert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"qwen2\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n-        (\"qwen2_5_omni\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n-        (\"qwen2_5_vl\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n-        (\"qwen2_audio\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n-        (\"qwen2_moe\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n-        (\"qwen2_vl\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n-        (\"qwen3\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n-        (\"qwen3_moe\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n-        (\"qwen3_next\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n-        (\"qwen3_omni_moe\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n-        (\"qwen3_vl\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n-        (\"qwen3_vl_moe\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"qwen2\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"qwen2_5_omni\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"qwen2_5_vl\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"qwen2_audio\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"qwen2_moe\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"qwen2_vl\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"qwen3\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"qwen3_moe\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"qwen3_next\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"qwen3_omni_moe\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"qwen3_vl\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"qwen3_vl_moe\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n         (\"rag\", \"RagTokenizer\"),\n         (\"realm\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"recurrent_gemma\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"recurrent_gemma\", \"GemmaTokenizer\" if is_tokenizers_available() else None),\n         (\"reformer\", \"ReformerTokenizer\" if is_tokenizers_available() else None),\n         (\"rembert\", \"RemBertTokenizer\" if is_tokenizers_available() else None),\n         (\"retribert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"roberta\", \"RobertaTokenizer\"),\n         (\"roberta-prelayernorm\", \"RobertaTokenizer\"),\n         (\"roc_bert\", \"RoCBertTokenizer\"),\n         (\"roformer\", \"RoFormerTokenizer\" if is_tokenizers_available() else None),\n-        (\"rwkv\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"rwkv\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n         (\"seamless_m4t\", \"SeamlessM4TTokenizer\" if is_tokenizers_available() else None),\n         (\"seamless_m4t_v2\", \"SeamlessM4TTokenizer\" if is_tokenizers_available() else None),\n-        (\"shieldgemma2\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"shieldgemma2\", \"GemmaTokenizer\" if is_tokenizers_available() else None),\n         (\"siglip\", \"SiglipTokenizer\" if is_sentencepiece_available() else None),\n-        (\"siglip2\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"smollm3\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n+        (\"siglip2\", \"GemmaTokenizer\" if is_tokenizers_available() else None),\n         (\"speech_to_text\", \"Speech2TextTokenizer\" if is_sentencepiece_available() else None),\n         (\"speecht5\", \"SpeechT5Tokenizer\" if is_sentencepiece_available() else None),\n         (\"splinter\", \"SplinterTokenizer\"),\n         (\"squeezebert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"stablelm\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"stablelm\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n         (\"starcoder2\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"switch_transformers\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n         (\"t5\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n-        (\"t5gemma\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"t5gemma\", \"GemmaTokenizer\" if is_tokenizers_available() else None),\n         (\"tapas\", \"TapasTokenizer\"),\n         (\"trocr\", \"XLMRobertaTokenizer\" if is_tokenizers_available() else None),\n         (\"tvp\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"udop\", \"UdopTokenizer\" if is_tokenizers_available() else None),\n         (\"umt5\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n-        (\"video_llava\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"unispeech\", \"Wav2Vec2CTCTokenizer\"),\n+        (\"unispeech-sat\", \"Wav2Vec2CTCTokenizer\"),\n         (\"vilt\", \"BertTokenizer\" if is_tokenizers_available() else None),\n-        (\"vipllava\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"visual_bert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"vits\", \"VitsTokenizer\"),\n         (\n             \"voxtral\",\n             \"MistralCommonBackend\"\n             if is_mistral_common_available()\n-            else (\"LlamaTokenizer\" if is_tokenizers_available() else None),\n+            else (\"TokenizersBackend\" if is_tokenizers_available() else None),\n         ),\n         (\"wav2vec2\", \"Wav2Vec2CTCTokenizer\"),\n         (\"wav2vec2-bert\", \"Wav2Vec2CTCTokenizer\"),\n         (\"wav2vec2-conformer\", \"Wav2Vec2CTCTokenizer\"),\n         (\"wav2vec2_phoneme\", \"Wav2Vec2PhonemeCTCTokenizer\"),\n         (\"whisper\", \"WhisperTokenizer\" if is_tokenizers_available() else None),\n-        (\"xclip\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"xclip\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n         (\"xglm\", \"XGLMTokenizer\" if is_tokenizers_available() else None),\n         (\"xlm\", \"XLMTokenizer\"),\n         (\"xlm-roberta\", \"XLMRobertaTokenizer\" if is_tokenizers_available() else None),\n         (\"xlm-roberta-xl\", \"XLMRobertaTokenizer\" if is_tokenizers_available() else None),\n         (\"xlnet\", \"XLNetTokenizer\" if is_tokenizers_available() else None),\n-        (\"xlstm\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n-        (\"xmod\", \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"xlstm\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n+        (\"xmod\", \"XLMRobertaTokenizer\" if is_tokenizers_available() else None),\n         (\"yoso\", \"AlbertTokenizer\" if is_tokenizers_available() else None),\n-        (\"zamba\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n-        (\"zamba2\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n     ]\n )\n \n@@ -670,11 +609,43 @@ def from_pretrained(\n \n             return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n \n+        if gguf_file:\n+            gguf_path = cached_file(pretrained_model_name_or_path, gguf_file, **kwargs)\n+            config_dict = load_gguf_checkpoint(gguf_path, return_tensors=False)[\"config\"]\n+            config = AutoConfig.for_model(**config_dict)\n+        elif config is None:\n+            try:\n+                config = AutoConfig.from_pretrained(\n+                    pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n+                )\n+            except Exception:\n+                config = PreTrainedConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n+\n+        config_model_type = config.model_type\n+\n         # Next, let's try to use the tokenizer_config file to get the tokenizer class.\n         tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n+        tokenizer_config_class = tokenizer_config.get(\"tokenizer_class\", None)\n+        # if there is a config, we can check that the tokenizer class != than model class and can thus assume we need to use `TokenizersBackend`\n+        if (\n+            tokenizer_config_class is not None\n+            and config_model_type is not None\n+            and config_model_type != \"\"\n+            and TOKENIZER_MAPPING_NAMES.get(config_model_type, \"\").replace(\"Fast\", \"\")\n+            != tokenizer_config_class.replace(\"Fast\", \"\")\n+        ):\n+            # new model, but we ignore it unless the model type is the same\n+            try:\n+                return TokenizersBackend.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n+            except Exception:\n+                return tokenizer_class_from_name(tokenizer_config_class).from_pretrained(\n+                    pretrained_model_name_or_path, *inputs, **kwargs\n+                )\n+\n         if \"_commit_hash\" in tokenizer_config:\n             kwargs[\"_commit_hash\"] = tokenizer_config[\"_commit_hash\"]\n-        config_tokenizer_class = tokenizer_config.get(\"tokenizer_class\")\n+\n+        # Check for auto_map early to handle dynamic tokenizers properly\n         tokenizer_auto_map = None\n         if \"auto_map\" in tokenizer_config:\n             if isinstance(tokenizer_config[\"auto_map\"], (tuple, list)):\n@@ -683,34 +654,15 @@ def from_pretrained(\n             else:\n                 tokenizer_auto_map = tokenizer_config[\"auto_map\"].get(\"AutoTokenizer\", None)\n \n-        # If that did not work, let's try to use the config.\n-        if config_tokenizer_class is None:\n-            if not isinstance(config, PreTrainedConfig):\n-                if gguf_file:\n-                    gguf_path = cached_file(pretrained_model_name_or_path, gguf_file, **kwargs)\n-                    config_dict = load_gguf_checkpoint(gguf_path, return_tensors=False)[\"config\"]\n-                    config = AutoConfig.for_model(**config_dict)\n-                else:\n-                    config = AutoConfig.from_pretrained(\n-                        pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n-                    )\n-            config_tokenizer_class = config.tokenizer_class\n-            if hasattr(config, \"auto_map\") and \"AutoTokenizer\" in config.auto_map:\n-                tokenizer_auto_map = config.auto_map[\"AutoTokenizer\"]\n-\n-        if (\n-            config_tokenizer_class is not None\n-            and config_tokenizer_class != \"TokenizersBackend\"\n-            and \"Fast\" in config_tokenizer_class\n-        ):\n-            config_tokenizer_class = config_tokenizer_class[:-4]\n+        if tokenizer_config_class:\n+            tokenizer_config_class = tokenizer_config_class.replace(\"Fast\", \"\")\n \n         has_remote_code = tokenizer_auto_map is not None\n         has_local_code = type(config) in TOKENIZER_MAPPING or (\n-            config_tokenizer_class is not None\n+            tokenizer_config_class is not None\n             and (\n-                tokenizer_class_from_name(config_tokenizer_class) is not None\n-                or tokenizer_class_from_name(config_tokenizer_class + \"Fast\") is not None\n+                tokenizer_class_from_name(tokenizer_config_class) is not None\n+                or tokenizer_class_from_name(tokenizer_config_class + \"Fast\") is not None\n             )\n         )\n         if has_remote_code:\n@@ -734,19 +686,24 @@ def from_pretrained(\n             return tokenizer_class.from_pretrained(\n                 pretrained_model_name_or_path, *inputs, trust_remote_code=trust_remote_code, **kwargs\n             )\n-        elif config_tokenizer_class is not None:\n-            fast_tokenizer_class = None\n-            if fast_tokenizer_class is None:\n-                tokenizer_class_candidate = config_tokenizer_class\n-                tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)\n-                if tokenizer_class is None and not tokenizer_class_candidate.endswith(\"Fast\"):\n-                    tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate + \"Fast\")\n-                if tokenizer_class.__name__ == \"PythonBackend\":  # unless you inherit from it?\n-                    tokenizer_class = TokenizersBackend\n-            else:\n-                tokenizer_class = fast_tokenizer_class\n+        elif tokenizer_config_class is not None:\n+            tokenizer_class_candidate = tokenizer_config_class\n+            tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)\n+            if tokenizer_class is None and not tokenizer_class_candidate.endswith(\"Fast\"):\n+                tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate + \"Fast\")\n+            if tokenizer_class is not None and tokenizer_class.__name__ == \"PythonBackend\":\n+                tokenizer_class = TokenizersBackend\n+            # Fallback to TokenizersBackend if the class wasn't found\n+            if tokenizer_class is None:\n+                tokenizer_class = TokenizersBackend\n \n             return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n+        elif getattr(config, \"tokenizer_class\"):\n+            _class = config.tokenizer_class\n+            if \"PreTrainedTokenizerFast\" not in _class:\n+                _class = _class.replace(\"Fast\", \"\")\n+            tokenizer_class = tokenizer_class_from_name(_class)\n+            return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n \n         # Otherwise we have to be creative.\n         # if model is an encoder decoder, the encoder tokenizer class is used by default\n@@ -760,12 +717,26 @@ def from_pretrained(\n                 )\n             config = config.encoder\n \n-        model_type = config_class_to_model_type(type(config).__name__)\n+        model_type = config_class_to_model_type(type(config).__name__) or config.get(\"model_type\", None)\n         if model_type is not None:\n             tokenizer_class = TOKENIZER_MAPPING.get(type(config), TokenizersBackend)\n             if tokenizer_class is not None:\n                 return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n \n+        # Fallback: try tokenizer_class from tokenizer_config.json\n+        tokenizer_config_class = tokenizer_config.get(\"tokenizer_class\", None)\n+        if tokenizer_config_class is not None:\n+            if tokenizer_config_class != \"TokenizersBackend\" and \"Fast\" in tokenizer_config_class:\n+                tokenizer_config_class = tokenizer_config_class[:-4]\n+            tokenizer_class = tokenizer_class_from_name(tokenizer_config_class)\n+            if tokenizer_class is None and not tokenizer_config_class.endswith(\"Fast\"):\n+                tokenizer_class = tokenizer_class_from_name(tokenizer_config_class + \"Fast\")\n+            if tokenizer_class is not None and tokenizer_class.__name__ == \"PythonBackend\":\n+                tokenizer_class = TokenizersBackend\n+            if tokenizer_class is None:\n+                tokenizer_class = TokenizersBackend\n+            return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n+\n         raise ValueError(\n             f\"Unrecognized configuration class {config.__class__} to build an AutoTokenizer.\\n\"\n             f\"Model type should be one of {', '.join(c.__name__ for c in TOKENIZER_MAPPING)}.\""
        },
        {
            "sha": "e26523b8a8f947fac563c85b981b9305e2adc3e5",
            "filename": "src/transformers/models/blenderbot/tokenization_blenderbot.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -160,13 +160,6 @@ def __init__(\n \n         self._tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n         self._tokenizer.decoder = decoders.ByteLevel()\n-        self._tokenizer.post_processor = processors.RobertaProcessing(\n-            sep=(str(eos_token), self._vocab.get(str(eos_token), 2)),\n-            cls=(str(bos_token), self._vocab.get(str(bos_token), 0)),\n-            add_prefix_space=add_prefix_space,\n-            trim_offsets=True,\n-        )\n-\n         super().__init__(\n             bos_token=bos_token,\n             eos_token=eos_token,\n@@ -178,6 +171,12 @@ def __init__(\n             add_prefix_space=add_prefix_space,\n             **kwargs,\n         )\n+        self._tokenizer.post_processor = processors.RobertaProcessing(\n+            sep=(str(eos_token), self.eos_token_id),\n+            cls=(str(bos_token), self.bos_token_id),\n+            add_prefix_space=add_prefix_space,\n+            trim_offsets=True,\n+        )\n \n \n __all__ = [\"BlenderbotTokenizer\"]"
        },
        {
            "sha": "bdbd7749915c8b812455eb42bfbe613547781de4",
            "filename": "src/transformers/models/canine/tokenization_canine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -67,6 +67,8 @@ class CanineTokenizer(PreTrainedTokenizer):\n                 The maximum sentence length the model accepts.\n     \"\"\"\n \n+    model_input_names = [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n+\n     def __init__(\n         self,\n         bos_token=chr(CLS),"
        },
        {
            "sha": "452ae7b8e08726609b0797e1a4bfff2e7c54c23d",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -158,7 +158,7 @@ def __init__(\n                 unk_token=str(unk_token),\n             )\n         )\n-        prepend_scheme = \"first\" if self.add_prefix_space else \"none\"\n+        prepend_scheme = \"first\" if self.add_prefix_space else \"never\"\n         self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(\n             replacement=\"â–\", prepend_scheme=prepend_scheme, split=False\n         )"
        },
        {
            "sha": "7484a61b53ef081d247b16c011a9e99be668de52",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -160,6 +160,7 @@ class LayoutLMv2Tokenizer(TokenizersBackend):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model = models.WordPiece\n+    model_input_names = [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n \n     def __init__(\n         self,"
        },
        {
            "sha": "527ed48ccfecabb97311310aa0538eeb57bec5c0",
            "filename": "src/transformers/models/nougat/tokenization_nougat.py",
            "status": "modified",
            "additions": 11,
            "deletions": 16,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -441,31 +441,26 @@ def __init__(\n         )\n         self._tokenizer.decoder = decoders.ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True)\n \n-        # Set up post processor with bos and eos tokens\n-        bos_token_id = self._vocab.get(str(bos_token), 0)\n-        eos_token_id = self._vocab.get(str(eos_token), 2)\n-        pad_token_id = self._vocab.get(str(pad_token), 1)\n+        super().__init__(\n+            errors=errors,\n+            unk_token=unk_token,\n+            bos_token=bos_token,\n+            eos_token=eos_token,\n+            pad_token=pad_token,\n+            **kwargs,\n+        )\n         self._tokenizer.post_processor = processors.TemplateProcessing(\n             single=f\"{bos_token}:0 $A:0 {eos_token}:0\",\n             pair=\"$A:0 $B:1\",\n             special_tokens=[\n-                (str(eos_token), eos_token_id),\n-                (str(bos_token), bos_token_id),\n+                (str(eos_token), self.eos_token_id),\n+                (str(bos_token), self.bos_token_id),\n             ],\n         )\n \n         # Enable truncation and padding\n         self._tokenizer.enable_truncation(max_length=4096)\n-        self._tokenizer.enable_padding(length=4096, pad_id=pad_token_id, pad_token=str(pad_token))\n-\n-        super().__init__(\n-            errors=errors,\n-            unk_token=unk_token,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            pad_token=pad_token,\n-            **kwargs,\n-        )\n+        self._tokenizer.enable_padding(length=4096, pad_id=self.pad_token_id, pad_token=str(pad_token))\n \n     def remove_hallucinated_references(self, text: str) -> str:\n         \"\"\""
        },
        {
            "sha": "baecf59928f2380d77614aa8bfb0402e1652a122",
            "filename": "src/transformers/models/parakeet/convert_nemo_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fparakeet%2Fconvert_nemo_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fparakeet%2Fconvert_nemo_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fconvert_nemo_to_hf.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -30,7 +30,7 @@\n     ParakeetFeatureExtractor,\n     ParakeetForCTC,\n     ParakeetProcessor,\n-    ParakeetTokenizerFast,\n+    ParakeetTokenizer,\n )\n from transformers.convert_slow_tokenizer import ParakeetConverter\n from transformers.utils.hub import cached_file\n@@ -151,7 +151,7 @@ def extract_nemo_archive(nemo_file_path: str, extract_dir: str) -> dict[str, str\n \n def write_processor(nemo_config: dict, model_files, output_dir, push_to_repo_id=None):\n     tokenizer_converted = ParakeetConverter(model_files[\"tokenizer_model_file\"]).converted()\n-    tokenizer_converted_fast = ParakeetTokenizerFast(\n+    tokenizer_converted_fast = ParakeetTokenizer(\n         tokenizer_object=tokenizer_converted,\n         clean_up_tokenization_spaces=False,\n     )"
        },
        {
            "sha": "85cf7d4ba126ebf9bca425a8c48a336ee47643db",
            "filename": "src/transformers/models/parakeet/tokenization_parakeet.py",
            "status": "renamed",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fparakeet%2Ftokenization_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fparakeet%2Ftokenization_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Ftokenization_parakeet.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -16,10 +16,10 @@\n import itertools\n from typing import Optional, Union\n \n-from ...tokenization_utils_tokenizers import PreTrainedTokenizerFast\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n \n \n-class ParakeetTokenizerFast(PreTrainedTokenizerFast):\n+class ParakeetTokenizer(TokenizersBackend):\n     \"\"\"\n     Inherits all methods from [`PreTrainedTokenizerFast`]. Users should refer to this superclass for more information regarding those methods,\n     except for `_decode` which is overridden to adapt it to CTC decoding:\n@@ -51,4 +51,4 @@ def _decode(\n         )\n \n \n-__all__ = [\"ParakeetTokenizerFast\"]\n+__all__ = [\"ParakeetTokenizer\"]",
            "previous_filename": "src/transformers/models/parakeet/tokenization_parakeet_fast.py"
        },
        {
            "sha": "c3b6d297f0640526780ccb238d99283d85210c70",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 20,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -87,25 +87,6 @@ class PixtralProcessor(ProcessorMixin):\n             Special token used to denote the end of an image input.\n     \"\"\"\n \n-    @classmethod\n-    def _load_tokenizer_from_pretrained(\n-        cls, sub_processor_type, pretrained_model_name_or_path, subfolder=\"\", **kwargs\n-    ):\n-        \"\"\"\n-        Override for BC. Pixtral requires a modified pre_tokenizer with ByteLevel prepended to handle\n-        the specific tokenization format expected by pretrained Pixtral models.\n-        \"\"\"\n-        from tokenizers import pre_tokenizers\n-\n-        from ...models.llama import LlamaTokenizer\n-\n-        tokenizer = LlamaTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)\n-        # Add ByteLevel pre_tokenizer before the existing one\n-        tokenizer._tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n-            [pre_tokenizers.ByteLevel(False), tokenizer._tokenizer.pre_tokenizer]\n-        )\n-        return tokenizer\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -169,7 +150,7 @@ def __call__(\n \n         output_kwargs = self._merge_kwargs(\n             PixtralProcessorKwargs,\n-            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            tokenizer_init_kwargs=getattr(self.tokenizer, \"init_kwargs\", {}),\n             **kwargs,\n         )\n \n@@ -216,6 +197,8 @@ def __call__(\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n         return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        # Remove return_token_type_ids as MistralCommonBackend doesn't support it\n+        output_kwargs[\"text_kwargs\"].pop(\"return_token_type_ids\", None)\n         text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n         self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\"])\n "
        },
        {
            "sha": "77539f97102304712e969d51c185dc756c714ac4",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -1513,10 +1513,18 @@ def _get_arguments_from_pretrained(cls, pretrained_model_name_or_path, processor\n             modality = _get_modality_for_attribute(sub_processor_type)\n             is_primary = sub_processor_type == modality\n \n-            if \"tokenizer\" in sub_processor_type:\n-                tokenizer = cls._load_tokenizer_from_pretrained(\n-                    sub_processor_type, pretrained_model_name_or_path, subfolder=subfolder, **kwargs\n-                )\n+            if (\n+                \"tokenizer\" in sub_processor_type\n+            ):  # This is only necessary for the checkpoing in test_procesing_mistral3.py which has no config.json and\n+                # the tokenizer_config.json references LlamaTokenizerFast. TODO: update the config on the hub.\n+                if \"PixtralProcessor\" in cls.__name__:\n+                    from .tokenization_utils_tokenizers import TokenizersBackend\n+\n+                    tokenizer = TokenizersBackend.from_pretrained(pretrained_model_name_or_path, **kwargs)\n+                else:\n+                    tokenizer = cls._load_tokenizer_from_pretrained(\n+                        sub_processor_type, pretrained_model_name_or_path, subfolder=subfolder, **kwargs\n+                    )\n                 args.append(tokenizer)\n             elif is_primary:\n                 # Primary non-tokenizer sub-processor: load via Auto class"
        },
        {
            "sha": "5b997c045a76f4d12daa07c9be65868ba7da864e",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -1114,7 +1114,7 @@ def _get_padding_truncation_strategies(\n                     max_length = self.model_max_length\n \n         # Test if we have a padding token\n-        if padding_strategy != PaddingStrategy.DO_NOT_PAD and (self.pad_token is None or self.pad_token_id < 0):\n+        if padding_strategy != PaddingStrategy.DO_NOT_PAD and (self.pad_token_id is None or self.pad_token_id < 0):\n             raise ValueError(\n                 \"Asking to pad but the tokenizer does not have a padding token. \"\n                 \"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\n@@ -1851,8 +1851,9 @@ def from_pretrained(\n             raise ValueError(\"`init_inputs` are not supported by `MistralCommonBackend.from_pretrained`.\")\n \n         # Handle kwargs and AutoTokenizer/AutoProcessor case\n+        # These kwargs are passed by AutoTokenizer/AutoProcessor but are not used by MistralCommonBackend\n         if kwargs and not set(kwargs.keys()).issubset(\n-            {\"trust_remote_code\", \"_from_pipeline\", \"_commit_hash\", \"dtype\", \"_from_auto\"}\n+            {\"trust_remote_code\", \"_from_pipeline\", \"_commit_hash\", \"dtype\", \"_from_auto\", \"subfolder\"}\n         ):\n             raise ValueError(f\"Some kwargs in {kwargs} are not supported by `MistralCommonBackend.from_pretrained`.\")\n "
        },
        {
            "sha": "00c903cdc06b6e7a086e7a019ce0f4a2a492c037",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -972,7 +972,7 @@ class PreTrainedTokenizerBase(PushToHubMixin):\n \n     # first name has to correspond to main model input name\n     # to make sure `tokenizer.pad(...)` works correctly\n-    model_input_names: list[str] = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n+    model_input_names: list[str] = [\"input_ids\", \"attention_mask\"]\n     padding_side: str = \"right\"\n     truncation_side: str = \"right\"\n     slow_tokenizer_class = None"
        },
        {
            "sha": "f4dbaa1d48480483366a4b1093cb1e30ccbc61a7",
            "filename": "src/transformers/tokenization_utils_tokenizers.py",
            "status": "modified",
            "additions": 21,
            "deletions": 1,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Ftokenization_utils_tokenizers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Ftokenization_utils_tokenizers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_tokenizers.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -30,6 +30,7 @@\n from tokenizers import Encoding as EncodingFast\n from tokenizers import Tokenizer as TokenizerFast\n from tokenizers.decoders import Decoder as DecoderFast\n+from tokenizers.models import BPE, Unigram\n from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordLevelTrainer, WordPieceTrainer\n \n from .integrations.ggml import convert_gguf_tokenizer\n@@ -121,7 +122,8 @@ def convert_to_native_format(cls, trust_remote_code=False, **kwargs):\n                 if isinstance(vocab, list):\n                     vocab = list(map(tuple, vocab))  # TODO just for now\n             elif cls.model.__name__ == \"Unigram\":\n-                vocab = list(map(tuple, vocab))\n+                if vocab and isinstance(vocab[0], (list, tuple)):\n+                    vocab = [tuple(item) for item in vocab]\n             elif cls.model.__name__ == \"WordLevel\":\n                 vocab = {token: i for i, token in enumerate(vocab)}\n             elif cls.model.__name__ == \"BPE\" or cls.model.__name__ == \"WordPiece\":\n@@ -237,6 +239,9 @@ def __init__(self, *args, **kwargs):\n         add_prefix_space = kwargs.get(\"add_prefix_space\", False)\n         vocab_file = kwargs.get(\"vocab_file\")\n \n+        vocab = kwargs.get(\"vocab\")\n+        merges = kwargs.get(\"merges\")\n+\n         fast_tokenizer = None\n         if tokenizer_object is not None:\n             fast_tokenizer = copy.deepcopy(tokenizer_object)\n@@ -253,6 +258,15 @@ def __init__(self, *args, **kwargs):\n             kwargs.update(tokenizer_config)\n             if len(additional_kwargs) > 0:\n                 kwargs.update(additional_kwargs)\n+        elif self._tokenizer is None and vocab is not None:\n+            # Build from vocab/merges extracted by convert_to_native_format\n+            if merges is not None:\n+                vocab_dict = vocab if isinstance(vocab, dict) else {w: i for i, (w, _) in enumerate(vocab)}\n+                fast_tokenizer = TokenizerFast(BPE(vocab=vocab_dict, merges=merges, fuse_unk=True, dropout=None))\n+            elif isinstance(vocab, dict):\n+                fast_tokenizer = TokenizerFast(BPE(vocab=vocab, merges=[], fuse_unk=True, dropout=None))\n+            elif isinstance(vocab, list) and vocab and isinstance(vocab[0], (tuple, list)):\n+                fast_tokenizer = TokenizerFast(Unigram(vocab=vocab, unk_id=kwargs.get(\"unk_id\", 0)))\n         elif self._tokenizer is None:\n             raise ValueError(\n                 \"Couldn't instantiate the backend tokenizer from one of: \\n\"\n@@ -261,6 +275,11 @@ def __init__(self, *args, **kwargs):\n                 \"(3) an equivalent slow tokenizer class to instantiate and convert. \\n\"\n                 \"You need to have sentencepiece or tiktoken installed to convert a slow tokenizer to a fast one.\"\n             )\n+        # Only set defaults when creating TokenizersBackend from scratch\n+        if fast_tokenizer_file is None and tokenizer_object is None and self._tokenizer is None:\n+            kwargs.setdefault(\"bos_token\", \"<s>\")\n+            kwargs.setdefault(\"eos_token\", \"</s>\")\n+\n         if fast_tokenizer is not None:\n             self._tokenizer = fast_tokenizer\n \n@@ -290,6 +309,7 @@ def __init__(self, *args, **kwargs):\n         # Set backend to \"tokenizers\" if not already set\n         if \"backend\" not in kwargs:\n             kwargs[\"backend\"] = \"tokenizers\"\n+\n         explicit_bos_eos_in_kwargs = \"add_bos_token\" in kwargs or \"add_eos_token\" in kwargs\n         self._add_bos_token = kwargs.get(\"add_bos_token\", False)\n         self._add_eos_token = kwargs.get(\"add_eos_token\", False)"
        },
        {
            "sha": "88b7492f2fc8dac3a5fd774344de453460d46ad2",
            "filename": "src/transformers/utils/attention_visualizer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fattention_visualizer.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -21,7 +21,7 @@\n from ..models.auto.configuration_auto import AutoConfig\n from ..models.auto.modeling_auto import MODEL_FOR_PRETRAINING_MAPPING, MODEL_MAPPING\n from ..models.auto.processing_auto import PROCESSOR_MAPPING_NAMES, AutoProcessor\n-from ..models.auto.tokenization_auto import TOKENIZER_MAPPING_NAMES, AutoTokenizer\n+from ..models.auto.tokenization_auto import AutoTokenizer\n from .import_utils import is_torch_available\n \n \n@@ -199,12 +199,12 @@ def visualize_attention_mask(self, input_sentence: str, suffix=\"\"):\n             if \"token_type_ids\" in inputs:  # TODO inspect signature of update causal mask\n                 kwargs[\"token_type_ids\"] = inputs[\"token_type_ids\"]\n             tokens = processor.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n-        elif self.config.model_type in TOKENIZER_MAPPING_NAMES:\n+        else:\n             tokenizer = AutoTokenizer.from_pretrained(self.repo_id)\n             tokens = tokenizer.tokenize(input_sentence)\n             attention_mask = tokenizer(input_sentence, return_tensors=\"pt\")[\"attention_mask\"]\n-        else:\n-            raise ValueError(f\"Model type {model.config.model_type} does not support attention visualization\")\n+            if attention_mask is None:\n+                raise ValueError(f\"Model type {self.config.model_type} does not support attention visualization\")\n \n         model.config._attn_implementation = \"eager\"\n         model.train()"
        },
        {
            "sha": "c120fe77882c90682337a2e599c4c37b54acd746",
            "filename": "tests/generation/test_stopping_criteria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fgeneration%2Ftest_stopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fgeneration%2Ftest_stopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_stopping_criteria.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -164,7 +164,7 @@ def test_stop_string_criteria(self):\n             self.assertFalse(criteria(false_input_ids[\"input_ids\"][i : i + 1], scores))\n \n         # Now try it with a tokenizer where those are actually special tokens\n-        tokenizer = AutoTokenizer.from_pretrained(\"cognitivecomputations/dolphin-2.5-mixtral-8x7b\")\n+        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n         tokenizer.padding_side = \"left\"\n         true_input_ids = tokenizer(true_strings, return_tensors=\"pt\", padding=\"longest\", add_special_tokens=False)\n         false_input_ids = tokenizer(false_strings, return_tensors=\"pt\", padding=\"longest\", add_special_tokens=False)"
        },
        {
            "sha": "1dbe7a6a7946c8447b9103ee8e144deb14787395",
            "filename": "tests/models/aria/test_processing_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Faria%2Ftest_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Faria%2Ftest_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_processing_aria.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -149,7 +149,7 @@ def test_process_interleaved_images_prompts_no_image_splitting(self):\n         # Pad the first input to match the second input\n         pad_len = len(expected_input_ids_2) - len(expected_input_ids_1)\n \n-        expected_attention_mask = [[0] * pad_len + [1] * len(expected_input_ids_1), [1] * (len(expected_input_ids_2))]\n+        expected_attention_mask = [ [1] * len(expected_input_ids_1) + [0] * pad_len, [1] * (len(expected_input_ids_2))]\n \n         self.assertEqual(\n             inputs[\"attention_mask\"],"
        },
        {
            "sha": "0f2c05cf686f130aa4582329e2ebeaf1b6b42d7c",
            "filename": "tests/models/auto/test_tokenization_auto.py",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -32,6 +32,7 @@\n     BertTokenizerFast,\n     CTRLTokenizer,\n     GPT2Tokenizer,\n+    HerbertTokenizer,\n     PreTrainedTokenizerFast,\n     Qwen2Tokenizer,\n     Qwen2TokenizerFast,\n@@ -593,3 +594,25 @@ def __init__(self, **kwargs):\n                     pass\n             finally:\n                 os.chdir(prev_dir)\n+\n+    def test_tokenization_class_priority(self):\n+        from transformers import AutoProcessor\n+\n+        tok = AutoTokenizer.from_pretrained(\"mlx-community/MiniMax-M2.1-4bit\")\n+        self.assertTrue(tok.__class__ == TokenizersBackend)\n+\n+        tok = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n+        self.assertTrue(tok.__class__ == HerbertTokenizer)\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            tok.save_pretrained(tmp_dir)\n+            tok2 = AutoTokenizer.from_pretrained(tmp_dir)\n+            self.assertTrue(tok2.__class__ == HerbertTokenizer)\n+\n+        tok = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n+        self.assertTrue(tok.__class__ == TokenizersBackend)\n+\n+        tok = AutoProcessor.from_pretrained(\"mistralai/Ministral-3-8B-Instruct-2512-BF16\").tokenizer\n+        self.assertTrue(tok.__class__ == TokenizersBackend)\n+\n+        tok = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n+        self.assertTrue(tok.__class__ == TokenizersBackend)"
        },
        {
            "sha": "882b8c63509444fc65d1e711bbc94411cbf1c97c",
            "filename": "tests/models/chameleon/test_processing_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fchameleon%2Ftest_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fchameleon%2Ftest_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_processing_chameleon.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -34,7 +34,7 @@ def _setup_test_attributes(cls, processor):\n     @classmethod\n     def _setup_tokenizer(cls):\n         tokenizer_class = cls._get_component_class_from_processor(\"tokenizer\")\n-        tokenizer = tokenizer_class(vocab_file=SAMPLE_VOCAB)\n+        tokenizer = tokenizer_class.from_pretrained(SAMPLE_VOCAB)\n         tokenizer.pad_token_id = 0\n         tokenizer.sep_token_id = 1\n         tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\"]})"
        },
        {
            "sha": "15ae819a91dd34a58f394cc66f545e21a3513814",
            "filename": "tests/models/chinese_clip/test_processing_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fchinese_clip%2Ftest_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fchinese_clip%2Ftest_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_processing_chinese_clip.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -56,7 +56,7 @@ def _setup_tokenizer(cls):\n         vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n         with open(vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n-        return tokenizer_class(vocab_file=vocab_file)\n+        return tokenizer_class.from_pretrained(cls.tmpdirname)\n \n     @classmethod\n     def _setup_image_processor(cls):"
        },
        {
            "sha": "9a0521847bf8b144077e7a931d478bfb6593ab14",
            "filename": "tests/models/deepseek_vl/test_processing_deepseek_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fdeepseek_vl%2Ftest_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fdeepseek_vl%2Ftest_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl%2Ftest_processing_deepseek_vl.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -29,8 +29,8 @@ class DeepseekVLProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     @classmethod\n     def _setup_tokenizer(cls):\n         tokenizer_class = cls._get_component_class_from_processor(\"tokenizer\")\n-        return tokenizer_class(\n-            vocab_file=SAMPLE_VOCAB,\n+        return tokenizer_class.from_pretrained(\n+            SAMPLE_VOCAB,\n             extra_special_tokens={\n                 \"pad_token\": \"<ï½œendâ–ofâ–sentenceï½œ>\",\n                 \"image_token\": \"<image_placeholder>\","
        },
        {
            "sha": "dfe3e70aaa3e308928cb1fd0224a5ed288a4f94d",
            "filename": "tests/models/deepseek_vl_hybrid/test_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_processing_deepseek_vl_hybrid.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -29,8 +29,8 @@ class DeepseekVLHybridProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     @classmethod\n     def _setup_tokenizer(cls):\n         tokenizer_class = cls._get_component_class_from_processor(\"tokenizer\")\n-        return tokenizer_class(\n-            vocab_file=SAMPLE_VOCAB,\n+        return tokenizer_class.from_pretrained(\n+            SAMPLE_VOCAB,\n             extra_special_tokens={\n                 \"pad_token\": \"<ï½œendâ–ofâ–sentenceï½œ>\",\n                 \"image_token\": \"<image_placeholder>\","
        },
        {
            "sha": "8e4341727612a948e0840cfe2943bc35ccc9c563",
            "filename": "tests/models/ernie4_5_vl_moe/test_processing_ernie4_5_vl_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fernie4_5_vl_moe%2Ftest_processing_ernie4_5_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fernie4_5_vl_moe%2Ftest_processing_ernie4_5_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_vl_moe%2Ftest_processing_ernie4_5_vl_moe.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -20,7 +20,7 @@\n import numpy as np\n import pytest\n \n-from transformers import AutoProcessor, LlamaTokenizerFast\n+from transformers import AutoProcessor, TokenizersBackend\n from transformers.testing_utils import require_av, require_torch, require_torchvision, require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n@@ -93,7 +93,7 @@ def test_save_load_pretrained_default(self):\n \n         self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n         self.assertEqual(processor.image_processor.to_json_string(), image_processor.to_json_string())\n-        self.assertIsInstance(processor.tokenizer, LlamaTokenizerFast)\n+        self.assertIsInstance(processor.tokenizer, TokenizersBackend)\n         self.assertIsInstance(processor.image_processor, Ernie4_5_VL_MoeImageProcessorFast)\n \n     def test_image_processor(self):"
        },
        {
            "sha": "e2791f8252b49885d1e002c5a0b77850415fab7a",
            "filename": "tests/models/granite_speech/test_processing_granite_speech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fgranite_speech%2Ftest_processing_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fgranite_speech%2Ftest_processing_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2Ftest_processing_granite_speech.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -20,7 +20,7 @@\n import torch\n from parameterized import parameterized\n \n-from transformers import AutoTokenizer, GPT2TokenizerFast\n+from transformers import AutoTokenizer, TokenizersBackend\n from transformers.testing_utils import (\n     require_torch,\n     require_torch_accelerator,\n@@ -65,7 +65,7 @@ def test_save_load_pretrained_default(self):\n         processor = GraniteSpeechProcessor.from_pretrained(self.tmpdirname)\n \n         self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n-        self.assertIsInstance(processor.tokenizer, GPT2TokenizerFast)\n+        self.assertIsInstance(processor.tokenizer, TokenizersBackend)\n \n         self.assertEqual(processor.audio_processor.to_json_string(), audio_processor.to_json_string())\n         self.assertIsInstance(processor.audio_processor, GraniteSpeechFeatureExtractor)"
        },
        {
            "sha": "75fc11fdbe61737a7b773a25f7fbfbcf19d10e44",
            "filename": "tests/models/parakeet/test_tokenization_parakeet.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fparakeet%2Ftest_tokenization_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Fmodels%2Fparakeet%2Ftest_tokenization_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fparakeet%2Ftest_tokenization_parakeet.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -15,31 +15,29 @@\n \n import unittest\n \n-from transformers.models.parakeet import ParakeetTokenizerFast\n+from transformers.models.parakeet import ParakeetTokenizer\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n \n class ParakeetTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     slow_tokenizer_class = None\n-    rust_tokenizer_class = ParakeetTokenizerFast\n-    tokenizer_class = ParakeetTokenizerFast\n+    rust_tokenizer_class = ParakeetTokenizer\n+    tokenizer_class = ParakeetTokenizer\n     test_slow_tokenizer = False\n     test_rust_tokenizer = True\n     from_pretrained_id = \"nvidia/parakeet-ctc-1.1b\"\n \n     @classmethod\n     def setUpClass(cls):\n         super().setUpClass()\n-        tokenizer = ParakeetTokenizerFast.from_pretrained(\"nvidia/parakeet-ctc-1.1b\")\n+        tokenizer = ParakeetTokenizer.from_pretrained(\"nvidia/parakeet-ctc-1.1b\")\n         tokenizer.save_pretrained(cls.tmpdirname)\n \n-    @unittest.skip(\n-        reason=\"This test does not apply to ParakeetTokenizerFast. More details in the test docstring itself.\"\n-    )\n+    @unittest.skip(reason=\"This test does not apply to ParakeetTokenizer. More details in the test docstring itself.\")\n     def test_added_tokens_do_lower_case(self):\n         \"\"\"\n-        Precompiled normalization from sentencepiece is `nmt_nfkc_cf` that includes lowercasing. Yet, ParakeetTokenizerFast does not have a do_lower_case attribute.\n+        Precompiled normalization from sentencepiece is `nmt_nfkc_cf` that includes lowercasing. Yet, ParakeetTokenizer does not have a do_lower_case attribute.\n         This result in the test failing.\n         \"\"\"\n         pass\n@@ -48,6 +46,6 @@ def test_added_tokens_do_lower_case(self):\n     def test_encode_decode_with_spaces(self):\n         return\n \n-    @unittest.skip(reason=\"ParakeetTokenizerFast doesn't have tokenizer_file in its signature.\")\n+    @unittest.skip(reason=\"ParakeetTokenizer doesn't have tokenizer_file in its signature.\")\n     def test_rust_tokenizer_signature(self):\n         pass"
        },
        {
            "sha": "dbf5b7156c5a940c2b7760d4baf1e830622752ef",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -277,8 +277,13 @@ def _get_component_class_from_processor(cls, attribute, use_fast: bool = True):\n         # Get the appropriate Auto mapping for this component type\n         if mapping_name == \"tokenizer\":\n             from transformers.models.auto.tokenization_auto import TOKENIZER_MAPPING\n+            from transformers.utils import is_tokenizers_available\n \n             component_class = TOKENIZER_MAPPING.get(config_class, None)\n+            if component_class is None and is_tokenizers_available():\n+                from transformers.tokenization_utils_tokenizers import TokenizersBackend\n+\n+                component_class = TokenizersBackend\n         elif mapping_name == \"image_processor\":\n             from transformers.models.auto.image_processing_auto import IMAGE_PROCESSOR_MAPPING\n "
        },
        {
            "sha": "86ef47f85e27fac067866c42c6e61e2b10c215ec",
            "filename": "tests/tokenization/test_tokenization_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Ftokenization%2Ftest_tokenization_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Ftokenization%2Ftest_tokenization_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_fast.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -180,7 +180,7 @@ def test_init_from_tokenizers_model(self):\n             self.assertEqual(tok.pad_token, \"<pad>\")\n             self.assertEqual(tok.init_kwargs[\"max_length\"], 512)\n             self.assertEqual(tok.init_kwargs[\"pad_to_multiple_of\"], 8)\n-            self.assertEqual(tok(sentences, padding = True), {'input_ids': [[8774, 6, 3, 63, 31, 1748, 55, 1, 0, 0, 0, 0,0, 0, 0, 0],[ 571, 33, 25, 3, 2, 3, 58, 290, 225, 59, 36, 136, 962, 269, 58, 1]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]})  # fmt: skip\n+            self.assertEqual(tok(sentences, padding = True, return_token_type_ids=True), {'input_ids': [[8774, 6, 3, 63, 31, 1748, 55, 1, 0, 0, 0, 0,0, 0, 0, 0],[ 571, 33, 25, 3, 2, 3, 58, 290, 225, 59, 36, 136, 962, 269, 58, 1]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]})  # fmt: skip\n \n         tokenizer.enable_truncation(8, stride=0, strategy=\"longest_first\", direction=\"right\")\n         self.assertEqual(\n@@ -197,7 +197,7 @@ def test_init_from_tokenizers_model(self):\n             self.assertEqual(tok.init_kwargs[\"stride\"], 0)\n             # NOTE even if the model has a default max_length, it is not used...\n             # thus tok(sentences, truncation = True) does nothing and does not warn either\n-            self.assertEqual(tok(sentences, truncation = True, max_length = 8), {'input_ids': [[8774, 6, 3, 63, 31, 1748, 55, 1],[ 571, 33, 25, 3, 2, 3, 58, 1]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],[0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],[1, 1, 1, 1, 1, 1, 1, 1]]})  # fmt: skip\n+            self.assertEqual(tok(sentences, truncation = True, max_length = 8, return_token_type_ids=True), {'input_ids': [[8774, 6, 3, 63, 31, 1748, 55, 1],[ 571, 33, 25, 3, 2, 3, 58, 1]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],[0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],[1, 1, 1, 1, 1, 1, 1, 1]]})  # fmt: skip\n \n     def test_class_after_save_and_reload(self):\n         model_id = self.model_paths[0]"
        },
        {
            "sha": "843cb78f73d43424dd37a63fca3f6de4700e6ae2",
            "filename": "tests/tokenization/test_tokenization_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_utils.py?ref=9daee2e8bd0b5f0b07b577145267fe8d32b0d8fc",
            "patch": "@@ -335,7 +335,7 @@ def test_encode_message_raises_on_add_generation_prompt(self):\n     def test_special_tokens_overwrite(self):\n         text_with_nonspecial_tokens = \"there are 2 cats\"  # '2' is originally special\n \n-        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/Ernie4_5_Tokenizer\")\n+        tokenizer = LlamaTokenizer.from_pretrained(\"hf-internal-testing/Ernie4_5_Tokenizer\")\n         # Overwrite special tokens 0-9 to non-special\n         tokenizer.add_tokens([AddedToken(f\"{i}\", normalized=False, special=False) for i in range(10)])\n         self.assertTrue("
        }
    ],
    "stats": {
        "total": 492,
        "additions": 249,
        "deletions": 243
    }
}