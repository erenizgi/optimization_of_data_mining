{
    "author": "molbap",
    "message": "Fix DPT /Dinov2 sdpa regression on main (#33660)\n\n* fallback to eager if output attentions.\r\n\r\n* fix copies",
    "sha": "b7c381f01176b27013a44769b9a5b9b271613f07",
    "files": [
        {
            "sha": "bae21dacb95b0fe8db8b90e49005a5418c6b73db",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7c381f01176b27013a44769b9a5b9b271613f07/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7c381f01176b27013a44769b9a5b9b271613f07/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=b7c381f01176b27013a44769b9a5b9b271613f07",
            "patch": "@@ -231,7 +231,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaSelfAttention with ViT->Dinov2\n class Dinov2SdpaSelfAttention(Dinov2SelfAttention):\n     def __init__(self, config: Dinov2Config) -> None:\n         super().__init__(config)\n@@ -240,6 +239,16 @@ def __init__(self, config: Dinov2Config) -> None:\n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"Dinov2Model is using Dinov2SdpaSelfAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states, head_mask=head_mask, output_attentions=output_attentions\n+            )\n+\n         mixed_query_layer = self.query(hidden_states)\n \n         key_layer = self.transpose_for_scores(self.key(hidden_states))"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 10,
        "deletions": 1
    }
}