{
    "author": "zucchini-nlp",
    "message": "[docs] update attention implementation and cache docs (#39547)\n\n* update docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* applu suggestions\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "cd98c1fee3167c484944580516edea03fb03f784",
    "files": [
        {
            "sha": "407a47a7d3538f597f31d1033fd6e2e84404f3fe",
            "filename": "docs/source/en/attention_interface.md",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd98c1fee3167c484944580516edea03fb03f784/docs%2Fsource%2Fen%2Fattention_interface.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd98c1fee3167c484944580516edea03fb03f784/docs%2Fsource%2Fen%2Fattention_interface.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fattention_interface.md?ref=cd98c1fee3167c484944580516edea03fb03f784",
            "patch": "@@ -72,6 +72,34 @@ model(torch.ones(1, 5, dtype=int))\n and it will stop printing the statements, as it now uses the `sdpa` attention.  \n This allows to quickly change an attention function, without needing to reload the model!\n \n+## Different attention per backbone in multimodal models\n+\n+For multimodal models different attention functions may work better for each backbone module. For example, some vision backbones perform better in fp32, but are incompatible with FlashAttention. To continue using FlashAttention while keeping the vision encoder in fp32, create a dict and map each config to an attention implementation as shown below.\n+\n+```python\n+from transformers import AutoModelForImageTextToText\n+\n+model_id = \"facebook/chameleon-7b\"\n+\n+attention_implementation_per_backbone = {\"vision_config\": \"sdpa\", \"text_config\": \"flash_attention_2\"}\n+model = AutoModelForImageTextToText.from_pretrained(model_id, attn_implementation=attention_implementation_per_backbone)\n+\n+# NOTE: keys in the attention implementation have to be the same as the sub-config names\n+for key in attention_implementation_per_backbone:\n+    assert key in model.config.sub_configs, f\"Invalid key in `attention_implementation`\"\n+\n+# You can omit certain backbones - the default attention function (SDPA) will be used\n+# This is equivalent to the previous example\n+model = AutoModelForImageTextToText.from_pretrained(model_id, attn_implementation={\"text_config\": \"flash_attention_2\"})\n+\n+\n+# Set the same attention implementation for all backbones with single string, same as in non-multimodal models\n+model = AutoModelForImageTextToText.from_pretrained(model_id, attn_implementation=\"eager\")\n+\n+# Alternatively use a dict with an empty key for global configuration\n+model = AutoModelForImageTextToText.from_pretrained(model_id, attn_implementation={\"\": \"eager\"})\n+```\n+\n ## What about new args needed in my custom attention function?\n \n But indeed, what if the new function requires a new arg to be properly used? It's no issue! Models supporting the"
        },
        {
            "sha": "17d35c33ab4c17b6f7a0647a2ad6aac88c599f82",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 29,
            "deletions": 1,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd98c1fee3167c484944580516edea03fb03f784/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd98c1fee3167c484944580516edea03fb03f784/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=cd98c1fee3167c484944580516edea03fb03f784",
            "patch": "@@ -132,6 +132,34 @@ for _ in range(max_new_tokens):\n print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\n \"[INST] Hello, what's your name. [/INST]  Hello! My name is LLaMA,\"\n ```\n+\n+## Cache position\n+\n+The cache position tracks where to insert new tokens in the attention cache. It represents the *absolute* position of each token in the context, independent of padding or batch structure. Suppose you already cached `N` tokens and are now processing `K` new tokens. The cache position for the new tokens will range from `N` to `N + K - 1`. In other words, you're processing tokens at positions - `[N, N + 1, N + 2, ..., N + K - 1]`.\n+\n+Cache position is used internally for two purposes:\n+\n+1. Selecting new tokens to process in the input sequence and ensuring only tokens that havenâ€™t been cached yet are passed to the model's `forward`.\n+2. Storing key/value pairs at the correct positions in the cache. This is especially important for fixed-size caches, like [`StaticCache`], that pre-allocates a specific cache length.\n+\n+The generation loop usually takes care of the cache position, but if you're writing a custom generation method, it is important that cache positions are accurate since they are used to write and read key/value states into fixed slots.\n+\n+\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+\n+model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+messages = [{\"role\": \"user\", \"content\": \"You are a helpful assistant.\"}]\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(\"cuda:0\")\n+generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=10)\n+\n+```\n+\n+\n ## Legacy cache format\n \n Before the [`Cache`] class, the cache used to be stored as a tuple of tuples of tensors. This format is dynamic because it grows as text is generated, similar to [`DynamicCache`].\n@@ -157,4 +185,4 @@ generation_outputs = model.generate(**inputs, return_dict_in_generate=True, retu\n \n cache = DynamicCache.from_legacy_cache(generation_outputs.past_key_values)\n legacy_format_cache = cache.to_legacy_cache()\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "0295a5bf1b342f4dc5cd216a4b420bceca34aaa1",
            "filename": "docs/source/en/llm_optims.md",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd98c1fee3167c484944580516edea03fb03f784/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd98c1fee3167c484944580516edea03fb03f784/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=cd98c1fee3167c484944580516edea03fb03f784",
            "patch": "@@ -341,7 +341,7 @@ A known issue with transformer models is that the self-attention mechanism grows\n \n FlashAttention and [FlashAttention-2](./perf_infer_gpu_one#flashattention-2) break up the attention computation into smaller chunks and reduces the number of intermediate read/write operations to the GPU memory to speed up inference. FlashAttention-2 improves on the original FlashAttention algorithm by also parallelizing over sequence length dimension and better partitioning work on the hardware to reduce synchronization and communication overhead.\n \n-To use FlashAttention-2, set [attn_implementation](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.PreTrainedModel.from_pretrained.attn_implementation) to `\"flash_attention_2\"` in [`~PreTrainedModel.from_pretrained`].\n+To use FlashAttention-2, set [attn_implementation](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.PreTrainedModel.from_pretrained.attn_implementation) to `\"flash_attention_2\"` in [`~PreTrainedModel.from_pretrained`] or set with `model.set_attention_implementation(\"flash_attention_2\")` to dynamically update the [attention interface](./attention_interface) after the model is loaded.\n \n ```py\n from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n@@ -353,14 +353,22 @@ model = AutoModelForCausalLM.from_pretrained(\n     torch_dtype=torch.bfloat16,\n     attn_implementation=\"flash_attention_2\",\n )\n+\n+# Change the model's attention dynamically after loading\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"google/gemma-2b\",\n+    quantization_config=quant_config,\n+    torch_dtype=torch.bfloat16\n+)\n+model.set_attention_implementation(\"flash_attention_2\")\n ```\n \n ### PyTorch scaled dot product attention\n \n Scaled dot product attention (SDPA) is automatically enabled in PyTorch 2.0 and it supports FlashAttention, xFormers, and PyTorch's C++ implementation. SDPA chooses the most performant attention algorithm if you're using a CUDA backend. For other backends, SDPA defaults to the PyTorch C++ implementation.\n \n > [!TIP]\n-> SDPA automaticallysupports FlashAttention-2 as long as you have the latest PyTorch version installed.\n+> SDPA automatically supports FlashAttention-2 as long as you have the latest PyTorch version installed.\n \n Use the [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) context manager to explicitly enable or disable any of the four attention algorithms. For example, use `SDPBackend.FLASH_ATTENTION` to enable FlashAttention.\n "
        },
        {
            "sha": "fa726e1f98b4c68866f4437e1677b88253a449c9",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd98c1fee3167c484944580516edea03fb03f784/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd98c1fee3167c484944580516edea03fb03f784/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=cd98c1fee3167c484944580516edea03fb03f784",
            "patch": "@@ -177,10 +177,16 @@ There are three supported implementations available.\n \n SDPA is used by default for PyTorch v2.1.1. and greater when an implementation is available. You could explicitly enable SDPA by setting `attn_implementation=\"sdpa\"` in [`~PreTrainedModel.from_pretrained`] though. Certain attention parameters, such as `head_mask` and `output_attentions=True`, are unsupported and returns a warning that Transformers will fall back to the (slower) eager implementation.\n \n+Refer to the [AttentionInterface](./attention_interface) guide to learn how to change the attention implementation after loading a model.\n+\n ```py\n from transformers import AutoModelForCausalLM\n \n model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", attn_implementation=\"sdpa\")\n+\n+# Change the model's attention dynamically after loading it\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\")\n+model.set_attention_implementation(\"sdpa\")\n ```\n \n SDPA selects the most performant implementation available, but you can also explicitly select an implementation with [torch.nn.attention.sdpa_kernel](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel) as a context manager. The example below shows how to enable the FlashAttention2 implementation with `enable_flash=True`.\n@@ -234,7 +240,7 @@ FlashAttention2 support is currently limited to Instinct MI210, Instinct MI250 a\n </hfoption>\n </hfoptions>\n \n-Enable FlashAttention2 by setting `attn_implementation=\"flash_attention_2\"` in [`~PreTrainedModel.from_pretrained`]. FlashAttention2 is only supported for models with the fp16 or bf16 torch type. Make sure to cast your model to the appropriate data type first.\n+Enable FlashAttention2 by setting `attn_implementation=\"flash_attention_2\"` in [`~PreTrainedModel.from_pretrained`] or by setting `model.set_attention_implementation(\"flash_attention_2\")` to dynamically update the [attention interface](./attention_interface). FlashAttention2 is only supported for models with the fp16 or bf16 torch type. Make sure to cast your model to the appropriate data type first.\n \n ```py\n from transformers import AutoModelForCausalLM"
        }
    ],
    "stats": {
        "total": 78,
        "additions": 74,
        "deletions": 4
    }
}