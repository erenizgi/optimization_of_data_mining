{
    "author": "cyyever",
    "message": "Use torch.expm1 (#36995)",
    "sha": "78afa1c5375240f927ec4ddc5c932fa5a28f1e52",
    "files": [
        {
            "sha": "372010a4287f4f5ba83b451e84059691280785db",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/78afa1c5375240f927ec4ddc5c932fa5a28f1e52/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78afa1c5375240f927ec4ddc5c932fa5a28f1e52/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=78afa1c5375240f927ec4ddc5c932fa5a28f1e52",
            "patch": "@@ -2578,7 +2578,7 @@ def forward(\n         lang = self.language_embedding(lang_id).transpose(1, 2)\n \n         log_dur_pred = self.dur_predictor(hidden_states.transpose(1, 2))\n-        dur_out = torch.clamp(torch.round((torch.exp(log_dur_pred) - 1)).long(), min=1)\n+        dur_out = torch.clamp(torch.round((torch.expm1(log_dur_pred))).long(), min=1)\n         # B x C x T\n         if hidden_states.size(0) == 1:\n             hidden_states = torch.repeat_interleave(hidden_states, dur_out.view(-1), dim=2)"
        },
        {
            "sha": "ae191b311e9369ab64dc021fcbb16484caee8168",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/78afa1c5375240f927ec4ddc5c932fa5a28f1e52/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78afa1c5375240f927ec4ddc5c932fa5a28f1e52/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=78afa1c5375240f927ec4ddc5c932fa5a28f1e52",
            "patch": "@@ -2292,7 +2292,7 @@ def forward(\n \n         # predict duration\n         log_dur_pred = self.duration_predictor(char_hidden_states, padding_mask=char_padding_mask)\n-        dur_out = torch.clamp(torch.round((torch.exp(log_dur_pred) - 1)).long(), min=1)\n+        dur_out = torch.clamp(torch.round((torch.expm1(log_dur_pred))).long(), min=1)\n         dur_out = dur_out.masked_fill(~char_padding_mask.bool(), 0.0)\n \n         # upsample char hidden states according to predicted duration\n@@ -2854,7 +2854,7 @@ def forward(\n         lang = self.language_embedding(lang_id).transpose(1, 2)\n \n         log_dur_pred = self.dur_predictor(hidden_states.transpose(1, 2))\n-        dur_out = torch.clamp(torch.round((torch.exp(log_dur_pred) - 1)).long(), min=1)\n+        dur_out = torch.clamp(torch.round((torch.expm1(log_dur_pred))).long(), min=1)\n         # B x C x T\n         if hidden_states.size(0) == 1:\n             hidden_states = torch.repeat_interleave(hidden_states, dur_out.view(-1), dim=2)"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}