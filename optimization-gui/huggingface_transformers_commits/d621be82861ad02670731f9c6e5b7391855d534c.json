{
    "author": "gante",
    "message": "ðŸš¨ [v5] `generate` delegates default cache initialization to the model (#41505)",
    "sha": "d621be82861ad02670731f9c6e5b7391855d534c",
    "files": [
        {
            "sha": "ea54566577537826bd315f589e3321647f38deb7",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -2167,7 +2167,10 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n \n \n class WhisperNoSpeechDetection(LogitsProcessor):\n-    r\"\"\"This processor can be used to detect silence when using Whisper. It should take as input unprocessed logits to follow the original implementation\"\"\"\n+    \"\"\"\n+    This processor can be used to detect silence when using Whisper. It should take as input unprocessed logits\n+    to follow the original implementation\n+    \"\"\"\n \n     def __init__(self, no_speech_token: int, begin_index: int, scores_is_logprobs: bool = False):\n         self.no_speech_token = no_speech_token\n@@ -2188,6 +2191,10 @@ def set_model(self, model):\n         self.model = model\n \n     def set_inputs(self, inputs):\n+        # build `cache_position` on the fly\n+        seq_length = inputs[\"input_ids\"].shape[1]\n+        inputs = self.model._get_initial_cache_position(seq_length, self.model.device, inputs)\n+        # prepare other inputs\n         self.inputs = {**self.model.prepare_inputs_for_generation(**inputs), **inputs}\n         self.inputs[\"input_features\"] = self.inputs.pop(\"inputs\")\n "
        },
        {
            "sha": "deb3361edc37cbd3afe102decc0bdba8e8326802",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 11,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -548,6 +548,12 @@ def prepare_inputs_for_generation(\n         # 2. Generic cache-dependent input preparation\n         if past_key_values is not None:\n             model_inputs[\"past_key_values\"] = past_key_values\n+        # We check `use_cache` below because some stateful models (like `recurrent_gemma`) expect input slicing if\n+        # their caching mechanism is used. To define `use_cache`, the user-defined argument takes precedence.\n+        use_cache = kwargs.get(\"use_cache\")\n+        if use_cache is None:\n+            use_cache = getattr(self.config, \"use_cache\", False)\n+        if past_key_values is None or use_cache:\n             # TODO (joao): handle the case where cache length == input_ids length. The function below results in an\n             # exception because we get empty input_ids after slicing. In essence, we need to roll back the cache 1\n             # token to recompute the logits for the first token to be generated (but not all caches support roll backs)\n@@ -589,7 +595,7 @@ def prepare_inputs_for_generation(\n         for model_input_name in [\"position_ids\", \"token_type_ids\", \"decoder_position_ids\"]:\n             model_input = kwargs.get(model_input_name)\n             if model_input is not None:\n-                if past_key_values is not None:\n+                if past_key_values is not None or use_cache:\n                     current_input_length = (\n                         model_inputs[\"inputs_embeds\"].shape[1]\n                         if model_inputs.get(\"inputs_embeds\") is not None\n@@ -1999,17 +2005,15 @@ def _prepare_cache_for_generation(\n             elif \"dynamic\" in generation_config.cache_implementation:\n                 model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n \n-        # TODO (joao): remove this `else` when we remove the last traces of the legacy cache format (v4.58.0, search\n-        # for `instance(past_key_values, Cache)` as well). In general, if `cache_implementation` is unset, cache\n-        # initialization should happen inside the model at prefill time.\n-        else:\n-            model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n-\n         # TODO (joao): this logic is incomplete, e.g. `offloaded` should apply to both caches. Refactor this function\n         # to correctly pass parameterization to both caches.\n-        if requires_cross_attention_cache and not isinstance(model_kwargs[cache_name], EncoderDecoderCache):\n-            model_kwargs[cache_name] = EncoderDecoderCache(\n-                model_kwargs[cache_name],  # self-attention cache\n+        if (\n+            requires_cross_attention_cache\n+            and \"past_key_values\" in model_kwargs\n+            and not isinstance(model_kwargs[\"past_key_values\"], EncoderDecoderCache)\n+        ):\n+            model_kwargs[\"past_key_values\"] = EncoderDecoderCache(\n+                model_kwargs[\"past_key_values\"],  # self-attention cache\n                 DynamicCache(**dynamic_cache_kwargs),  # cross-attention cache\n             )\n \n@@ -3335,7 +3339,7 @@ def _beam_search(\n \n             # pluck the cache from the beam indices that will be used in the next iteration\n             # NOTE: we need to check if `self._reorder_cache` exists for special models like RAG, RecurrentGemma etc.\n-            if model_kwargs.get(\"past_key_values\", None) is not None:\n+            if model_kwargs.get(\"past_key_values\") is not None:\n                 beam_idx = self._flatten_beam_dim(running_beam_indices[..., cur_len - decoder_prompt_len])\n                 if hasattr(self, \"_reorder_cache\"):\n                     model_kwargs[\"past_key_values\"] = self._reorder_cache(model_kwargs[\"past_key_values\"], beam_idx)"
        },
        {
            "sha": "d4d24d39c86c40a08c942243c0afea2681687f6d",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -802,7 +802,7 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = (\n                 EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-                if encoder_hidden_states is not None\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n                 else DynamicCache(config=self.config)\n             )\n "
        },
        {
            "sha": "afa6cfe28dc28cdfcaa0e6aadfe6360321f9b60f",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -671,7 +671,11 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")"
        },
        {
            "sha": "85d8191ac70a63011c6f2ce6e55c3150bf86dc34",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -533,7 +533,11 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")"
        },
        {
            "sha": "c91227b0ea3f565b785d22a08f3496817cddbc82",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -1976,7 +1976,7 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = (\n                 EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-                if encoder_hidden_states is not None\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n                 else DynamicCache(config=self.config)\n             )\n "
        },
        {
            "sha": "e00ee87f505103d726514a92aabc86a131b73aef",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -752,7 +752,7 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = (\n                 EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-                if encoder_hidden_states is not None\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n                 else DynamicCache(config=self.config)\n             )\n "
        },
        {
            "sha": "d6d6aadbf744491d1c2dd47faf87a78343f01960",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -739,7 +739,7 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = (\n                 EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-                if encoder_hidden_states is not None\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n                 else DynamicCache(config=self.config)\n             )\n "
        },
        {
            "sha": "c085e86f38c94b6100427072c8f6a3a97da69b9d",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -651,7 +651,11 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")"
        },
        {
            "sha": "fcf7ecb0b30e82de94239b998ea37074712d374a",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -611,7 +611,11 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")"
        },
        {
            "sha": "633d69c6f8c0224f636f46614a45b0dd3fb4c787",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -610,7 +610,11 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")"
        },
        {
            "sha": "f7ceb8f1fbdd084b1be66cbc97c9097a61f05892",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -647,7 +647,11 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")"
        },
        {
            "sha": "a523127e98c984c245b2215a7380c736667066fa",
            "filename": "src/transformers/models/ernie/modular_ernie.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -234,7 +234,11 @@ def forward(\n                 use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")"
        },
        {
            "sha": "6a4fb184771886c23791e6495b38a56ef67dc45d",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -616,10 +616,6 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # initialize `past_key_values`\n-        if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-\n         x += positions\n         x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n \n@@ -912,6 +908,9 @@ def forward(\n         if decoder_input_ids is None and decoder_inputs_embeds is None:\n             raise ValueError(\"Make sure that `decoder_input_ids` or `decoder_inputs_embeds` are passed.\")\n \n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+\n         if encoder_outputs is None:\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,"
        },
        {
            "sha": "ea9144bfe6d37132067f93a30ba1e54af781fea6",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -1029,7 +1029,7 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = (\n                 EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-                if encoder_hidden_states is not None\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n                 else DynamicCache(config=self.config)\n             )\n "
        },
        {
            "sha": "1392ea2d5d7ca06685aca4a6cf0bdf2a8c8e1240",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -750,7 +750,7 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = (\n                 EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-                if encoder_hidden_states is not None\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n                 else DynamicCache(config=self.config)\n             )\n "
        },
        {
            "sha": "cca5894ba4ca613dd256ef432d674377c0907848",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -800,7 +800,7 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = (\n                 EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-                if encoder_hidden_states is not None\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n                 else DynamicCache(config=self.config)\n             )\n "
        },
        {
            "sha": "914c68b9ec5a0514d2d0b2f9817de9d61ba7d480",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -802,7 +802,7 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = (\n                 EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-                if encoder_hidden_states is not None\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n                 else DynamicCache(config=self.config)\n             )\n "
        },
        {
            "sha": "c5f78f7fdc79f6517705b85858fa98c98feab353",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -802,7 +802,7 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = (\n                 EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-                if encoder_hidden_states is not None\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n                 else DynamicCache(config=self.config)\n             )\n "
        },
        {
            "sha": "5e8151bc447a9753e0d078fba6539595d865f083",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -715,7 +715,7 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = (\n                 EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-                if encoder_hidden_states is not None\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n                 else DynamicCache(config=self.config)\n             )\n "
        },
        {
            "sha": "55aadc696fbe71f2766e65bdf3eaf0e5d54686a2",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -1182,7 +1182,7 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = (\n                 EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-                if encoder_hidden_states is not None\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n                 else DynamicCache(config=self.config)\n             )\n "
        },
        {
            "sha": "0f324d91c0e8f812629412eae0cd463dd482fdfe",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -621,7 +621,11 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")"
        },
        {
            "sha": "7affa0a60db01f07fbf3d426ce68da0cb7b10427",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -645,7 +645,11 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")"
        },
        {
            "sha": "d62a27329e5b8aa29d8e1c40d5caa39d5d7bc574",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -430,9 +430,6 @@ def forward(\n                 )\n                 use_cache = False\n \n-        if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -736,6 +733,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n+\n         past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:"
        },
        {
            "sha": "9caecd7ada72e5fb1555a5b09b996cb8443b68d6",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -550,7 +550,7 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = (\n                 EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-                if encoder_hidden_states is not None\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n                 else DynamicCache(config=self.config)\n             )\n "
        },
        {
            "sha": "492a22e79feaf0b7e0bc4f746cbd9289260c85ca",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -808,12 +808,11 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            if self.config.is_encoder_decoder:\n-                past_key_values = EncoderDecoderCache(\n-                    DynamicCache(config=self.config), DynamicCache(config=self.config)\n-                )\n-            else:\n-                past_key_values = DynamicCache(config=self.config)\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         past_key_values_length = 0\n         if cache_position is not None:"
        },
        {
            "sha": "e957e78592a4a3dc321bfe1c64397c57d10259c5",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -469,7 +469,7 @@ def forward(\n         if use_cache and past_key_values is None:\n             past_key_values = (\n                 EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-                if encoder_hidden_states is not None\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n                 else DynamicCache(config=self.config)\n             )\n "
        },
        {
            "sha": "6f799207fa6168b0c5e0838541b1f6697677dd10",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -640,7 +640,11 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")"
        },
        {
            "sha": "a1d0eed401d1239daf607e8dfa0250241ad6e532",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -627,7 +627,11 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")"
        },
        {
            "sha": "9ccfb2e0166a0537b0ad3f45fa7c2916b1558f63",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -742,7 +742,11 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")"
        },
        {
            "sha": "f8eeb27e3286a67e22886e918ac94fab03eef55f",
            "filename": "tests/models/roformer/test_modeling_roformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d621be82861ad02670731f9c6e5b7391855d534c/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d621be82861ad02670731f9c6e5b7391855d534c/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py?ref=d621be82861ad02670731f9c6e5b7391855d534c",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch RoFormer model.\"\"\"\n \n+import copy\n import unittest\n \n from transformers import RoFormerConfig, is_torch_available\n@@ -209,6 +210,8 @@ def create_and_check_for_generate_causal_lm(\n         token_labels,\n         choice_labels,\n     ):\n+        config = copy.deepcopy(config)\n+        config.is_decoder = True\n         model = RoFormerForCausalLM(config=config).to(torch_device).eval()\n         torch.manual_seed(0)\n         output_without_past_cache = model.generate("
        }
    ],
    "stats": {
        "total": 164,
        "additions": 114,
        "deletions": 50
    }
}