{
    "author": "ydshieh",
    "message": "Fix flaky `SwitchTransformersModelTest::test_training_gradient` (#35587)\n\n* fix\r\n\r\n* Update tests/models/switch_transformers/test_modeling_switch_transformers.py\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "82dd6c14bb811379d126acb801b8f7fd857eb2cc",
    "files": [
        {
            "sha": "32597f8ce2869e86972c5fd87d30b78cb3f7309c",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/82dd6c14bb811379d126acb801b8f7fd857eb2cc/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82dd6c14bb811379d126acb801b8f7fd857eb2cc/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=82dd6c14bb811379d126acb801b8f7fd857eb2cc",
            "patch": "@@ -576,6 +576,8 @@ class SwitchTransformersModelTest(ModelTesterMixin, GenerationTesterMixin, Pipel\n     test_torchscript = False\n     # The small SWITCH_TRANSFORMERS model needs higher percentages for CPU/MP tests\n     model_split_percents = [0.5, 0.8, 0.9]\n+    # `SwitchTransformers` is a MOE in which not all experts will get gradients because they are not all used in a single forward pass\n+    test_all_params_have_gradient = False\n \n     def setUp(self):\n         self.model_tester = SwitchTransformersModelTester(self)"
        },
        {
            "sha": "c29a15efd33342f5e2ccb2d357ed2edeac53f6b4",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/82dd6c14bb811379d126acb801b8f7fd857eb2cc/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82dd6c14bb811379d126acb801b8f7fd857eb2cc/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=82dd6c14bb811379d126acb801b8f7fd857eb2cc",
            "patch": "@@ -221,6 +221,8 @@ class ModelTesterMixin:\n     test_mismatched_shapes = True\n     test_missing_keys = True\n     test_model_parallel = False\n+    # Used in `check_training_gradient_checkpointing` to NOT check all params having gradient (e.g. for some MOE models)\n+    test_all_params_have_gradient = True\n     is_encoder_decoder = False\n     has_attentions = True\n     _is_composite = False\n@@ -895,9 +897,10 @@ def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=No\n                 loss.backward()\n                 optimizer.step()\n \n-                for k, v in model.named_parameters():\n-                    if v.requires_grad:\n-                        self.assertTrue(v.grad is not None, f\"{k} in {model_class.__name__} has no gradient!\")\n+                if self.test_all_params_have_gradient:\n+                    for k, v in model.named_parameters():\n+                        if v.requires_grad:\n+                            self.assertTrue(v.grad is not None, f\"{k} in {model_class.__name__} has no gradient!\")\n \n     def test_training(self):\n         if not self.model_tester.is_training:"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 8,
        "deletions": 3
    }
}