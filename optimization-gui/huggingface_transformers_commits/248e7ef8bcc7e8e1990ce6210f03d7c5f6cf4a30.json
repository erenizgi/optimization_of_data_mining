{
    "author": "gante",
    "message": "[docs] remove references to recently deleted classes in non-`en` docs (onnx, feature processors) (#41286)\n\nremove references to old classes",
    "sha": "248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
    "files": [
        {
            "sha": "81a7a500b783ede75b602aa56c9da7655069d81c",
            "filename": "docs/source/es/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fes%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fes%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2F_toctree.yml?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -64,8 +64,6 @@\n     title: Entrenador\n   - local: sagemaker\n     title: Ejecutar el entrenamiento en Amazon SageMaker\n-  - local: serialization\n-    title: Exportar a ONNX\n   - local: torchscript\n     title: Exportar a TorchScript\n   - local: community"
        },
        {
            "sha": "9c29ed6f0406a096256058dfc5fedba49fd1f0b6",
            "filename": "docs/source/es/serialization.md",
            "status": "removed",
            "additions": 0,
            "deletions": 651,
            "changes": 651,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/docs%2Fsource%2Fes%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/docs%2Fsource%2Fes%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fserialization.md?ref=bc33fd3fc21f9b69e2b48ab8db890435752ac5fd",
            "patch": "@@ -1,651 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-锔 Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Exportar modelos  Transformers\n-\n-Si necesitas implementar modelos  Transformers en entornos de producci贸n, te \n-recomendamos exportarlos a un formato serializado que se pueda cargar y ejecutar \n-en tiempos de ejecuci贸n y hardware especializados. En esta gu铆a, te mostraremos c贸mo \n-exportar modelos  Transformers en dos formatos ampliamente utilizados: ONNX y TorchScript.\n-\n-Una vez exportado, un modelo puede optimizarse para la inferencia a trav茅s de t茅cnicas \n-como la cuantizaci贸n y _pruning_. Si est谩s interesado en optimizar tus modelos para\n-que funcionen con la m谩xima eficiencia, consulta la \n-[biblioteca de  Optimum](https://github.com/huggingface/optimum).\n-\n-## ONNX\n-\n-El proyecto [ONNX (Open Neural Network eXchange)](http://onnx.ai) es un \n-est谩ndar abierto que define un conjunto com煤n de operadores y un formato \n-de archivo com煤n para representar modelos de aprendizaje profundo en una \n-amplia variedad de _frameworks_, incluidos PyTorch y TensorFlow. Cuando un modelo \n-se exporta al formato ONNX, estos operadores se usan para construir un \n-grafo computacional (a menudo llamado _representaci贸n intermedia_) que \n-representa el flujo de datos a trav茅s de la red neuronal.\n-\n-Al exponer un grafo con operadores y tipos de datos estandarizados, ONNX facilita \n-el cambio entre frameworks. Por ejemplo, un modelo entrenado en PyTorch se puede \n-exportar a formato ONNX y luego importar en TensorFlow (y viceversa).\n-\n- Transformers proporciona un paquete llamado `transformers.onnx`, el cual permite convertir \n-los checkpoints de un modelo en un grafo ONNX aprovechando los objetos de configuraci贸n. \n-Estos objetos de configuraci贸n est谩n hechos a la medida de diferentes arquitecturas de modelos\n-y est谩n dise帽ados para ser f谩cilmente extensibles a otras arquitecturas.\n-\n-Las configuraciones a la medida incluyen las siguientes arquitecturas:\n-\n-<!--This table is automatically generated by `make fix-copies`, do not fill manually!-->\n-\n-- ALBERT\n-- BART\n-- BEiT\n-- BERT\n-- BigBird\n-- BigBird-Pegasus\n-- Blenderbot\n-- BlenderbotSmall\n-- BLOOM\n-- CamemBERT\n-- CLIP\n-- CodeGen\n-- ConvBERT\n-- ConvNeXT\n-- ConvNeXTV2\n-- Data2VecText\n-- Data2VecVision\n-- DeBERTa\n-- DeBERTa-v2\n-- DeiT\n-- DETR\n-- DistilBERT\n-- ELECTRA\n-- FlauBERT\n-- GPT Neo\n-- GPT-J\n-- I-BERT\n-- LayoutLM\n-- LayoutLMv3\n-- LeViT\n-- LongT5\n-- M2M100\n-- Marian\n-- mBART\n-- MobileBERT\n-- MobileViT\n-- MT5\n-- OpenAI GPT-2\n-- Perceiver\n-- PLBart\n-- ResNet\n-- RoBERTa\n-- RoFormer\n-- SqueezeBERT\n-- T5\n-- ViT\n-- XLM\n-- XLM-RoBERTa\n-- XLM-RoBERTa-XL\n-- YOLOS\n-\n-En las pr贸ximas dos secciones, te mostraremos c贸mo:\n-\n-* Exportar un modelo compatible utilizando el paquete `transformers.onnx`.\n-* Exportar un modelo personalizado para una arquitectura no compatible.\n-\n-### Exportar un model a ONNX\n-\n-Para exportar un modelo  Transformers a ONNX, tienes que instalar primero algunas\n-dependencias extra:\n-\n-```bash\n-pip install transformers[onnx]\n-```\n-\n-El paquete `transformers.onnx` puede ser usado luego como un m贸dulo de Python:\n-\n-```bash\n-python -m transformers.onnx --help\n-\n-usage: Hugging Face Transformers ONNX exporter [-h] -m MODEL [--feature {causal-lm, ...}] [--opset OPSET] [--atol ATOL] output\n-\n-positional arguments:\n-  output                Path indicating where to store generated ONNX model.\n-\n-optional arguments:\n-  -h, --help            show this help message and exit\n-  -m MODEL, --model MODEL\n-                        Model ID on huggingface.co or path on disk to load model from.\n-  --feature {causal-lm, ...}\n-                        The type of features to export the model with.\n-  --opset OPSET         ONNX opset version to export the model with.\n-  --atol ATOL           Absolute difference tolerence when validating the model.\n-```\n-\n-Exportar un checkpoint usando una configuraci贸n a la medida se puede hacer de la siguiente manera:\n-\n-```bash\n-python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/\n-```\n-\n-que deber铆a mostrar los siguientes registros:\n-\n-```bash\n-Validating ONNX model...\n-        -[] ONNX model output names match reference model ({'last_hidden_state'})\n-        - Validating ONNX Model output \"last_hidden_state\":\n-                -[] (2, 8, 768) matches (2, 8, 768)\n-                -[] all values close (atol: 1e-05)\n-All good, model saved at: onnx/model.onnx\n-```\n-\n-Esto exporta un grafo ONNX del checkpoint definido por el argumento `--model`. \n-En este ejemplo, es un modelo `distilbert/distilbert-base-uncased`, pero puede ser cualquier\n-checkpoint en Hugging Face Hub o que est茅 almacenado localmente.\n-\n-El archivo `model.onnx` resultante se puede ejecutar en uno de los \n-[muchos aceleradores](https://onnx.ai/supported-tools.html#deployModel) \n-que admiten el est谩ndar ONNX. Por ejemplo, podemos cargar y ejecutar el \n-modelo con [ONNX Runtime](https://onnxruntime.ai/) de la siguiente manera:\n-\n-```python\n->>> from transformers import AutoTokenizer\n->>> from onnxruntime import InferenceSession\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> session = InferenceSession(\"onnx/model.onnx\")\n->>> # ONNX Runtime expects NumPy arrays as input\n->>> inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n->>> outputs = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n-```\n-\n-Los nombres necesarios de salida (es decir, `[\"last_hidden_state\"]`) se pueden obtener \n-echando un vistazo a la configuraci贸n ONNX de cada modelo. Por ejemplo, para DistilBERT tenemos:\n-\n-```python\n->>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig\n-\n->>> config = DistilBertConfig()\n->>> onnx_config = DistilBertOnnxConfig(config)\n->>> print(list(onnx_config.outputs.keys()))\n-[\"last_hidden_state\"]s\n-```\n-\n-El proceso es id茅ntico para los checkpoints de TensorFlow en Hub. \n-Por ejemplo, podemos exportar un checkpoint puro de TensorFlow desde \n-[Keras](https://huggingface.co/keras-io) de la siguiente manera:\n-\n-```bash\n-python -m transformers.onnx --model=keras-io/transformers-qa onnx/\n-```\n-\n-Para exportar un modelo que est谩 almacenado localmente, deber谩s tener los pesos \n-y tokenizadores del modelo almacenados en un directorio. Por ejemplo, podemos cargar \n-y guardar un checkpoint de la siguiente manera:\n-\n-```python\n->>> from transformers import AutoTokenizer, AutoModelForSequenceClassification\n-\n->>> # Load tokenizer and PyTorch weights form the Hub\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> pt_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> # Save to disk\n->>> tokenizer.save_pretrained(\"local-pt-checkpoint\")\n->>> pt_model.save_pretrained(\"local-pt-checkpoint\")\n-```\n-\n-Una vez que se guarda el checkpoint, podemos exportarlo a ONNX usando el argumento `--model` \n-del paquete `transformers.onnx` al directorio deseado:\n-\n-```bash\n-python -m transformers.onnx --model=local-pt-checkpoint onnx/\n-```\n-\n-### Seleccionar caracter铆sticas para diferentes topolog铆as de un modelo\n-\n-Cada configuraci贸n a la medida viene con un conjunto de _caracter铆sticas_ que te permiten exportar \n-modelos para diferentes tipos de topolog铆as o tareas. Como se muestra en la siguiente tabla, cada \n-funci贸n est谩 asociada con una auto-clase de autom贸vil diferente:\n-\n-| Feature                              | Auto Class                           |\n-| ------------------------------------ | ------------------------------------ |\n-| `causal-lm`, `causal-lm-with-past`   | `AutoModelForCausalLM`               |\n-| `default`, `default-with-past`       | `AutoModel`                          |\n-| `masked-lm`                          | `AutoModelForMaskedLM`               |\n-| `question-answering`                 | `AutoModelForQuestionAnswering`      |\n-| `seq2seq-lm`, `seq2seq-lm-with-past` | `AutoModelForSeq2SeqLM`              |\n-| `sequence-classification`            | `AutoModelForSequenceClassification` |\n-| `token-classification`               | `AutoModelForTokenClassification`    |\n-\n-Para cada configuraci贸n, puedes encontrar la lista de funciones admitidas a trav茅s de `FeaturesManager`. \n-Por ejemplo, para DistilBERT tenemos:\n-\n-```python\n->>> from transformers.onnx.features import FeaturesManager\n-\n->>> distilbert_features = list(FeaturesManager.get_supported_features_for_model_type(\"distilbert\").keys())\n->>> print(distilbert_features)\n-[\"default\", \"masked-lm\", \"causal-lm\", \"sequence-classification\", \"token-classification\", \"question-answering\"]\n-```\n-\n-Le puedes pasar una de estas caracter铆sticas al argumento `--feature` en el paquete `transformers.onnx`. \n-Por ejemplo, para exportar un modelo de clasificaci贸n de texto, podemos elegir un modelo ya ajustado del Hub y ejecutar:\n-\n-```bash\n-python -m transformers.onnx --model=distilbert/distilbert-base-uncased-finetuned-sst-2-english \\\n-                            --feature=sequence-classification onnx/\n-```\n-\n-que mostrar谩 los siguientes registros:\n-\n-```bash\n-Validating ONNX model...\n-        -[] ONNX model output names match reference model ({'logits'})\n-        - Validating ONNX Model output \"logits\":\n-                -[] (2, 2) matches (2, 2)\n-                -[] all values close (atol: 1e-05)\n-All good, model saved at: onnx/model.onnx\n-```\n-\n-Ten en cuenta que, en este caso, los nombres de salida del modelo ajustado son `logits` en lugar de `last_hidden_state` \n-que vimos anteriormente con el checkpoint `distilbert/distilbert-base-uncased`. Esto es de esperarse ya que el modelo ajustado \n-tiene un cabezal de clasificaci贸n secuencial.\n-\n-<Tip>\n-\n-Las caracter铆sticas que tienen un sufijo 'with-past' (por ejemplo, 'causal-lm-with-past') corresponden a topolog铆as \n-de modelo con estados ocultos precalculados (clave y valores en los bloques de atenci贸n) que se pueden usar para una \n-decodificaci贸n autorregresiva m谩s r谩pida.\n-\n-</Tip>\n-\n-\n-### Exportar un modelo para una arquitectura no compatible\n-\n-Si deseas exportar un modelo cuya arquitectura no es compatible de forma nativa \n-con la biblioteca, debes seguir tres pasos principales:\n-\n-1. Implementa una configuraci贸n personalizada en ONNX.\n-2. Exporta el modelo a ONNX.\n-3. Valide los resultados de PyTorch y los modelos exportados.\n-\n-En esta secci贸n, veremos c贸mo se implement贸 la serializaci贸n de DistilBERT \n-para mostrar lo que implica cada paso.\n-\n-#### Implementar una configuraci贸n personalizada en ONNX\n-\n-Comencemos con el objeto de configuraci贸n de ONNX. Proporcionamos tres clases abstractas \n-de las que debe heredar, seg煤n el tipo de arquitectura del modelo que quieras exportar:\n-\n-* Modelos basados en el _Encoder_ inherente de [`~onnx.config.OnnxConfig`]\n-* Modelos basados en el _Decoder_ inherente de [`~onnx.config.OnnxConfigWithPast`]\n-* Modelos _Encoder-decoder_ inherente de [`~onnx.config.OnnxSeq2SeqConfigWithPast`]\n-\n-<Tip>\n-\n-Una buena manera de implementar una configuraci贸n personalizada en ONNX es observar la implementaci贸n \n-existente en el archivo `configuration_<model_name>.py` de una arquitectura similar.\n-\n-</Tip>\n-\n-Dado que DistilBERT es un modelo de tipo _encoder_, su configuraci贸n se hereda de `OnnxConfig`:\n-\n-```python\n->>> from typing import Mapping, OrderedDict\n->>> from transformers.onnx import OnnxConfig\n-\n-\n->>> class DistilBertOnnxConfig(OnnxConfig):\n-...     @property\n-...     def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-...         return OrderedDict(\n-...             [\n-...                 (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n-...                 (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n-...             ]\n-...         )\n-```\n-\n-Cada objeto de configuraci贸n debe implementar la propiedad `inputs` y devolver un mapeo, \n-donde cada llave corresponde a una entrada esperada y cada valor indica el eje de esa entrada. \n-Para DistilBERT, podemos ver que se requieren dos entradas: `input_ids` y `attention_mask`. \n-Estas entradas tienen la misma forma de `(batch_size, sequence_length)`, es por lo que vemos \n-los mismos ejes utilizados en la configuraci贸n.\n-\n-<Tip>\n-\n-Observa que la propiedad `inputs` para `DistilBertOnnxConfig` devuelve un `OrderedDict`.\n-Esto nos asegura que las entradas coincidan con su posici贸n relativa dentro del m茅todo \n-`PreTrainedModel.forward()` al rastrear el grafo. Recomendamos usar un `OrderedDict` \n-para las propiedades `inputs` y `outputs` al implementar configuraciones ONNX personalizadas.\n-\n-</Tip>\n-\n-Una vez que hayas implementado una configuraci贸n ONNX, puedes crear una \n-instancia proporcionando la configuraci贸n del modelo base de la siguiente manera:\n-\n-```python\n->>> from transformers import AutoConfig\n-\n->>> config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> onnx_config = DistilBertOnnxConfig(config)\n-```\n-\n-El objeto resultante tiene varias propiedades 煤tiles. Por ejemplo, puedes ver el conjunto de operadores ONNX que se \n-utilizar谩 durante la exportaci贸n:\n-\n-```python\n->>> print(onnx_config.default_onnx_opset)\n-11\n-```\n-\n-Tambi茅n puedes ver los resultados asociados con el modelo de la siguiente manera:\n-\n-```python\n->>> print(onnx_config.outputs)\n-OrderedDict([(\"last_hidden_state\", {0: \"batch\", 1: \"sequence\"})])\n-```\n-\n-Observa que la propiedad de salidas sigue la misma estructura que las entradas; \n-devuelve un objecto `OrderedDict` de salidas nombradas y sus formas. La estructura \n-de salida est谩 vinculada a la elecci贸n de la funci贸n con la que se inicializa la configuraci贸n.\n-Por defecto, la configuraci贸n de ONNX se inicializa con la funci贸n `default` que \n-corresponde a exportar un modelo cargado con la clase `AutoModel`. Si quieres exportar \n-una topolog铆a de modelo diferente, simplemente proporciona una caracter铆stica diferente \n-al argumento `task` cuando inicialices la configuraci贸n de ONNX. Por ejemplo, si quisi茅ramos \n-exportar DistilBERT con un cabezal de clasificaci贸n de secuencias, podr铆amos usar:\n-\n-```python\n->>> from transformers import AutoConfig\n-\n->>> config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> onnx_config_for_seq_clf = DistilBertOnnxConfig(config, task=\"sequence-classification\")\n->>> print(onnx_config_for_seq_clf.outputs)\n-OrderedDict([('logits', {0: 'batch'})])\n-```\n-\n-<Tip>\n-\n-Todas las propiedades base y m茅todos asociados con [`~onnx.config.OnnxConfig`] y las \n-otras clases de configuraci贸n se pueden sobreescribir si es necesario.\n-Consulte [`BartOnnxConfig`] para ver un ejemplo avanzado.\n-\n-</Tip>\n-\n-#### Exportar el modelo\n-\n-Una vez que hayas implementado la configuraci贸n de ONNX, el siguiente paso es exportar el modelo.\n-Aqu铆 podemos usar la funci贸n `export()` proporcionada por el paquete `transformers.onnx`.\n-Esta funci贸n espera la configuraci贸n de ONNX, junto con el modelo base y el tokenizador, \n-y la ruta para guardar el archivo exportado:\n-\n-```python\n->>> from pathlib import Path\n->>> from transformers.onnx import export\n->>> from transformers import AutoTokenizer, AutoModel\n-\n->>> onnx_path = Path(\"model.onnx\")\n->>> model_ckpt = \"distilbert/distilbert-base-uncased\"\n->>> base_model = AutoModel.from_pretrained(model_ckpt)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n-\n->>> onnx_inputs, onnx_outputs = export(tokenizer, base_model, onnx_config, onnx_config.default_onnx_opset, onnx_path)\n-```\n-\n-Los objetos `onnx_inputs` y `onnx_outputs` devueltos por la funci贸n `export()` \n-son listas de llaves definidas en las propiedades `inputs` y `outputs` de la configuraci贸n.\n-Una vez exportado el modelo, puedes probar que el modelo est谩 bien formado de la siguiente manera:\n-\n-```python\n->>> import onnx\n-\n->>> onnx_model = onnx.load(\"model.onnx\")\n->>> onnx.checker.check_model(onnx_model)\n-```\n-\n-<Tip>\n-\n-Si tu modelo tiene m谩s de 2GB, ver谩s que se crean muchos archivos adicionales durante la exportaci贸n.\n-Esto es _esperado_ porque ONNX usa [B煤feres de protocolo](https://developers.google.com/protocol-buffers/) \n-para almacenar el modelo y 茅stos tienen un l铆mite de tama帽o de 2 GB. Consulta la \n-[documentaci贸n de ONNX](https://github.com/onnx/onnx/blob/master/docs/ExternalData.md) para obtener \n-instrucciones sobre c贸mo cargar modelos con datos externos.\n-\n-</Tip>\n-\n-#### Validar los resultados del modelo\n-\n-El paso final es validar que los resultados del modelo base y exportado coincidan dentro \n-de cierta tolerancia absoluta. Aqu铆 podemos usar la funci贸n `validate_model_outputs()` \n-proporcionada por el paquete `transformers.onnx` de la siguiente manera:\n-\n-```python\n->>> from transformers.onnx import validate_model_outputs\n-\n->>> validate_model_outputs(\n-...     onnx_config, tokenizer, base_model, onnx_path, onnx_outputs, onnx_config.atol_for_validation\n-... )\n-```\n-\n-Esta funci贸n usa el m茅todo `OnnxConfig.generate_dummy_inputs()` para generar entradas para el modelo base \n-y exportado, y la tolerancia absoluta se puede definir en la configuraci贸n. En general, encontramos una \n-concordancia num茅rica en el rango de 1e-6 a 1e-4, aunque es probable que cualquier valor menor que 1e-3 est茅 bien.\n-\n-### Contribuir con una nueva configuraci贸n a  Transformers\n-\n-隆Estamos buscando expandir el conjunto de configuraciones a la medida para usar y agradecemos las contribuciones de la comunidad! \n-Si deseas contribuir con su colaboraci贸n a la biblioteca, deber谩s:\n-\n-* Implementa la configuraci贸n de ONNX en el archivo `configuration_<model_name>.py` correspondiente\n-* Incluye la arquitectura del modelo y las caracter铆sticas correspondientes en [`~onnx.features.FeatureManager`]\n-* Agrega tu arquitectura de modelo a las pruebas en `test_onnx_v2.py`\n-\n-Revisa c贸mo fue la contribuci贸n para la [configuraci贸n de IBERT](https://github.com/huggingface/transformers/pull/14868/files) \n-y as铆 tener una idea de lo que necesito.\n-\n-## TorchScript\n-\n-<Tip>\n-\n-Este es el comienzo de nuestros experimentos con TorchScript y todav铆a estamos explorando sus capacidades con modelos de \n-tama帽o de entrada variable. Es un tema de inter茅s y profundizaremos nuestro an谩lisis en las pr贸ximas \n-versiones,  con m谩s ejemplos de c贸digo, una implementaci贸n m谩s flexible y puntos de referencia que comparen c贸digos \n-basados en Python con TorchScript compilado.\n-\n-</Tip>\n-\n-Seg煤n la documentaci贸n de PyTorch: \"TorchScript es una forma de crear modelos serializables y optimizables a partir del \n-c贸digo de PyTorch\". Los dos m贸dulos de Pytorch [JIT y TRACE](https://pytorch.org/docs/stable/jit.html) permiten al \n-desarrollador exportar su modelo para reutilizarlo  en otros programas, como los programas C++ orientados a la eficiencia.\n-\n-Hemos proporcionado una interfaz que permite exportar modelos de  Transformers a TorchScript para que puedan reutilizarse \n-en un entorno diferente  al de un programa Python basado en PyTorch. Aqu铆 explicamos c贸mo exportar y usar nuestros modelos \n-usando TorchScript.\n-\n-Exportar un modelo requiere de dos cosas:\n-\n-- un pase hacia adelante con entradas ficticias.\n-- instanciaci贸n del modelo con la indicador `torchscript`.\n-\n-Estas necesidades implican varias cosas con las que los desarrolladores deben tener cuidado. stas se detallan a continuaci贸n.\n-\n-### Indicador de TorchScript y pesos atados\n-\n-Este indicador es necesario porque la mayor铆a de los modelos de lenguaje en este repositorio tienen pesos vinculados entre su capa \n-de `Embedding` y su capa de `Decoding`. TorchScript no permite la exportaci贸n de modelos que tengan pesos atados, por lo que es \n-necesario desvincular y clonar los pesos previamente.\n-\n-Esto implica que los modelos instanciados con el indicador `torchscript` tienen su capa `Embedding` y `Decoding` separadas, \n-lo que significa que no deben entrenarse m谩s adelante. El entrenamiento desincronizar铆a las dos capas, lo que generar铆a \n-resultados inesperados.\n-\n-Este no es el caso de los modelos que no tienen un cabezal de modelo de lenguaje, ya que no tienen pesos atados.\n-Estos modelos se pueden exportar de forma segura sin el indicador `torchscript`.\n-\n-### Entradas ficticias y longitudes est谩ndar\n-\n-Las entradas ficticias se utilizan para crear un modelo de pase hacia adelante. Mientras los valores de las entradas se \n-propagan a trav茅s de las capas, PyTorch realiza un seguimiento de las diferentes operaciones ejecutadas en cada tensor.\n-Estas operaciones registradas se utilizan luego para crear el \"rastro\" del modelo.\n-\n-El rastro se crea en relaci贸n con las dimensiones de las entradas. Por lo tanto, est谩 limitado por las dimensiones de la \n-entrada ficticia y no funcionar谩 para ninguna otra longitud de secuencia o tama帽o de lote. Al intentar con un tama帽o diferente, \n-un error como:\n-\n-`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`\n-\n-aparecer谩. Por lo tanto, se recomienda rastrear el modelo con un tama帽o de entrada ficticia al menos tan grande como la \n-entrada m谩s  grande que se alimentar谩 al modelo durante la inferencia. El _padding_ se puede realizar para completar los \n-valores que faltan.  Sin embargo, como el modelo se habr谩 rastreado con un tama帽o de entrada grande, las dimensiones de \n-las diferentes matrices tambi茅n ser谩n grandes, lo que dar谩 como resultado m谩s c谩lculos.\n-\n-Se recomienda tener cuidado con el n煤mero total de operaciones realizadas en cada entrada y seguir de cerca el rendimiento \n-al exportar modelos de longitud de secuencia variable.\n-\n-### Usar TorchScript en Python\n-\n-A continuaci贸n se muestra un ejemplo que muestra c贸mo guardar, cargar modelos y c贸mo usar el rastreo para la inferencia.\n-\n-#### Guardando un modelo\n-\n-Este fragmento muestra c贸mo usar TorchScript para exportar un `BertModel`. Aqu铆, el `BertModel` se instancia de acuerdo \n-con la clase `BertConfig` y luego se guarda en el disco con el nombre de archivo `traced_bert.pt`\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-\n-enc = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-\n-# Tokenizing input text\n-text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n-tokenized_text = enc.tokenize(text)\n-\n-# Masking one of the input tokens\n-masked_index = 8\n-tokenized_text[masked_index] = \"[MASK]\"\n-indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)\n-segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n-\n-# Creating a dummy input\n-tokens_tensor = torch.tensor([indexed_tokens])\n-segments_tensors = torch.tensor([segments_ids])\n-dummy_input = [tokens_tensor, segments_tensors]\n-\n-# Initializing the model with the torchscript flag\n-# Flag set to True even though it is not necessary as this model does not have an LM Head.\n-config = BertConfig(\n-    vocab_size_or_config_json_file=32000,\n-    hidden_size=768,\n-    num_hidden_layers=12,\n-    num_attention_heads=12,\n-    intermediate_size=3072,\n-    torchscript=True,\n-)\n-\n-# Instantiating the model\n-model = BertModel(config)\n-\n-# The model needs to be in evaluation mode\n-model.eval()\n-\n-# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag\n-model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", torchscript=True)\n-\n-# Creating the trace\n-traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-torch.jit.save(traced_model, \"traced_bert.pt\")\n-```\n-\n-#### Cargar un modelo\n-\n-Este fragmento muestra c贸mo cargar el `BertModel` que se guard贸 previamente en el disco con el nombre `traced_bert.pt`.\n-Estamos reutilizando el `dummy_input` previamente inicializado.\n-\n-```python\n-loaded_model = torch.jit.load(\"traced_bert.pt\")\n-loaded_model.eval()\n-\n-all_encoder_layers, pooled_output = loaded_model(*dummy_input)\n-```\n-\n-#### Usar un modelo rastreado para la inferencia\n-\n-Usar el modelo rastreado para la inferencia es tan simple como usar su m茅todo `__call__`:\n-\n-```python\n-traced_model(tokens_tensor, segments_tensors)\n-```\n-\n-### Implementar los modelos HuggingFace TorchScript en AWS mediante Neuron SDK\n-\n-AWS present贸 la familia de instancias [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) para la inferencia \n-de aprendizaje autom谩tico de bajo costo y  alto rendimiento en la nube. Las instancias Inf1 funcionan con el chip AWS \n-Inferentia, un acelerador de hardware personalizado,  que se especializa en cargas de trabajo de inferencia de aprendizaje \n-profundo. [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) es el kit de desarrollo para  Inferentia \n-que admite el rastreo y la optimizaci贸n de modelos de  transformers para su implementaci贸n en Inf1. El SDK de Neuron proporciona:\n-\n-\n-1. API f谩cil de usar con una l铆nea de cambio de c贸digo para rastrear y optimizar un modelo de TorchScript para la inferencia en la nube.\n-2. Optimizaciones de rendimiento listas para usar con un [costo-rendimiento mejorado](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/>)\n-3. Soporte para modelos HuggingFace Transformers construidos con [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html) \n-o [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html).\n-\n-#### Implicaciones\n-\n-Los modelos Transformers basados en la arquitectura \n-[BERT (Representaciones de _Enconder_ bidireccional de Transformers)](https://huggingface.co/docs/transformers/main/model_doc/bert), \n-o sus variantes, como [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert) y \n-[roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta), se ejecutar谩n mejor en Inf1 para tareas no \n-generativas, como la respuesta extractiva de preguntas, la clasificaci贸n de secuencias y la clasificaci贸n de tokens.\n-Como alternativa, las tareas de generaci贸n de texto se pueden adaptar para ejecutarse en Inf1, seg煤n este \n-[tutorial de AWS Neuron MarianMT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html).\n-Puedes encontrar m谩s informaci贸n sobre los modelos que est谩n listos para usarse en Inferentia en la \n-[secci贸n _Model Architecture Fit_ de la documentaci贸n de Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia).\n-\n-#### Dependencias\n-\n-Usar AWS Neuron para convertir modelos requiere las siguientes dependencias y entornos:\n-\n-* Un [entorno Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide), \n-que viene preconfigurado en [AWS Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html).\n-\n-#### Convertir un modelo a AWS Neuron\n-\n-Con el mismo script usado en [Uso de TorchScript en Python](https://huggingface.co/docs/transformers/main/es/serialization#using-torchscript-in-python)\n-para rastrear un \"BertModel\", puedes importar la extensi贸n del _framework_ `torch.neuron` para acceder a los componentes \n-del SDK de Neuron a trav茅s de una API de Python.\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-import torch.neuron\n-```\n-Y modificando la l铆nea de c贸digo de rastreo de:\n-\n-```python\n-torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-```\n-\n-con lo siguiente:\n-\n-```python\n-torch.neuron.trace(model, [token_tensor, segments_tensors])\n-```\n-\n-Este cambio permite a Neuron SDK rastrear el modelo y optimizarlo para ejecutarse en instancias Inf1.\n-\n-Para obtener m谩s informaci贸n sobre las funciones, las herramientas, los tutoriales de ejemplo y las 煤ltimas actualizaciones \n-de AWS Neuron SDK, consulte la [documentaci贸n de AWS NeuronSDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)."
        },
        {
            "sha": "6e0371fb24708ca33362f03de397f5d62a29eee3",
            "filename": "docs/source/it/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fit%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fit%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2F_toctree.yml?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -29,8 +29,6 @@\n     title: Addestramento con script\r\n   - local: multilingual\r\n     title: Modelli multilingua per l'inferenza\r\n-  - local: serialization\r\n-    title: Esporta modelli Transformers\r\n   - local: perf_train_cpu\r\n     title: Addestramento efficiente su CPU\r\n   - local: perf_train_cpu_many\r\n@@ -66,4 +64,3 @@\n   - local: pr_checks\r\n     title: Controlli su una Pull Request\r\n   title: Guide How-to\r\n-  \r"
        },
        {
            "sha": "53e16d927eb97833a1e4c8975dd8e4501afea1d4",
            "filename": "docs/source/it/serialization.md",
            "status": "removed",
            "additions": 0,
            "deletions": 653,
            "changes": 653,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/docs%2Fsource%2Fit%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/docs%2Fsource%2Fit%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fserialization.md?ref=bc33fd3fc21f9b69e2b48ab8db890435752ac5fd",
            "patch": "@@ -1,653 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-锔 Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Esporta modelli  Transformers \n-\n-Se devi implementare  modelli Transformers in ambienti di produzione, noi\n-consigliamo di esportarli in un formato serializzato che pu貌 essere caricato ed eseguito\n-su runtime e hardware specializzati. In questa guida ti mostreremo come farlo\n-esporta  Modelli Transformers in due formati ampiamente utilizzati: ONNX e TorchScript.\n-\n-Una volta esportato, un modello pu貌 essere ottimizato per l'inferenza tramite tecniche come \n-la quantizzazione e soppressione. Se sei interessato a ottimizzare i tuoi modelli per l'esecuzione\n-con la massima efficienza, dai un'occhiata a [ Optimum\n-library](https://github.com/huggingface/optimum).\n-\n-## ONNX\n-\n-Il progetto [ONNX (Open Neural Network eXchange)](http://onnx.ai) Il progetto onnx 猫 un open\n-standard che definisce un insieme comune di operatori e un formato di file comune a\n-rappresentano modelli di deep learning in un'ampia variet di framework, tra cui\n-PyTorch e TensorFlow. Quando un modello viene esportato nel formato ONNX, questi\n-operatori sono usati per costruire un grafico computazionale (often called an\n-_intermediate representation_) che rappresenta il flusso di dati attraverso la\n-rete neurale.\n-\n-Esponendo un grafico con operatori e tipi di dati standardizzati, ONNX rende\n-pi霉 facile passare da un framework all'altro. Ad esempio, un modello allenato in PyTorch pu貌\n-essere esportato in formato ONNX e quindi importato in TensorFlow (e viceversa).\n-\n- Transformers fornisce un pacchetto `transformers.onnx` che ti consente di\n-convertire i checkpoint del modello in un grafico ONNX sfruttando gli oggetti di configurazione.\n-Questi oggetti di configurazione sono gi pronti per una serie di architetture di modelli,\n-e sono progettati per essere facilmente estensibili ad altre architetture.\n-\n-Le configurazioni pronte includono le seguenti architetture:\n-\n-<!--This table is automatically generated by `make fix-copies`, do not fill manually!-->\n-\n-- ALBERT\n-- BART\n-- BEiT\n-- BERT\n-- BigBird\n-- BigBird-Pegasus\n-- Blenderbot\n-- BlenderbotSmall\n-- CamemBERT\n-- ConvBERT\n-- Data2VecText\n-- Data2VecVision\n-- DeiT\n-- DistilBERT\n-- ELECTRA\n-- FlauBERT\n-- GPT Neo\n-- GPT-J\n-- I-BERT\n-- LayoutLM\n-- M2M100\n-- Marian\n-- mBART\n-- MobileBERT\n-- OpenAI GPT-2\n-- Perceiver\n-- PLBart\n-- RoBERTa\n-- RoFormer\n-- SqueezeBERT\n-- T5\n-- ViT\n-- XLM\n-- XLM-RoBERTa\n-- XLM-RoBERTa-XL\n-\n-Nelle prossime due sezioni, ti mostreremo come:\n-\n-* Esporta un modello supportato usando il pacchetto `transformers.onnx`.\n-* Esporta un modello personalizzato per un'architettura non supportata.\n-\n-### Esportazione di un modello in ONNX\n-\n-Per esportare un modello  Transformers in ONNX, dovrai prima installarne alcune\n-dipendenze extra:\n-\n-```bash\n-pip install transformers[onnx]\n-```\n-\n-Il pacchetto `transformers.onnx` pu貌 essere usato come modulo Python:\n-\n-```bash\n-python -m transformers.onnx --help\n-\n-usage: Hugging Face Transformers ONNX exporter [-h] -m MODEL [--feature {causal-lm, ...}] [--opset OPSET] [--atol ATOL] output\n-\n-positional arguments:\n-  output                Path indicating where to store generated ONNX model.\n-\n-optional arguments:\n-  -h, --help            show this help message and exit\n-  -m MODEL, --model MODEL\n-                        Model ID on huggingface.co or path on disk to load model from.\n-  --feature {causal-lm, ...}\n-                        The type of features to export the model with.\n-  --opset OPSET         ONNX opset version to export the model with.\n-  --atol ATOL           Absolute difference tolerance when validating the model.\n-```\n-\n-L'esportazione di un checkpoint utilizzando una configurazione gi pronta pu貌 essere eseguita come segue:\n-\n-```bash\n-python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/\n-```\n-\n-che dovrebbe mostrare i seguenti log:\n-\n-```bash\n-Validating ONNX model...\n-        -[] ONNX model output names match reference model ({'last_hidden_state'})\n-        - Validating ONNX Model output \"last_hidden_state\":\n-                -[] (2, 8, 768) matches (2, 8, 768)\n-                -[] all values close (atol: 1e-05)\n-All good, model saved at: onnx/model.onnx\n-```\n-\n-Questo esporta un grafico ONNX del checkpoint definito dall'argomento `--model`.\n-In questo esempio 猫 `distilbert/distilbert-base-uncased`, ma pu貌 essere qualsiasi checkpoint\n-Hugging Face Hub o uno memorizzato localmente.\n-\n-Il file risultante `model.onnx` pu貌 quindi essere eseguito su uno dei [tanti\n-acceleratori](https://onnx.ai/supported-tools.html#deployModel) che supportano il\n-lo standard ONNX. Ad esempio, possiamo caricare ed eseguire il modello con [ONNX\n-Runtime](https://onnxruntime.ai/) come segue:\n-\n-```python\n->>> from transformers import AutoTokenizer\n->>> from onnxruntime import InferenceSession\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> session = InferenceSession(\"onnx/model.onnx\")\n->>> # ONNX Runtime expects NumPy arrays as input\n->>> inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n->>> outputs = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n-```\n-\n-I nomi di output richiesti (cio猫 `[\"last_hidden_state\"]`) possono essere ottenuti\n-dando un'occhiata alla configurazione ONNX di ogni modello. Ad esempio, per\n-DistilBERT abbiamo:\n-\n-```python\n->>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig\n-\n->>> config = DistilBertConfig()\n->>> onnx_config = DistilBertOnnxConfig(config)\n->>> print(list(onnx_config.outputs.keys()))\n-[\"last_hidden_state\"]\n-```\n-\n-Il processo 猫 identico per i checkpoint TensorFlow sull'hub. Ad esempio, noi\n-possiamo esportare un checkpoint TensorFlow puro da [Keras\n-organizzazione](https://huggingface.co/keras-io) come segue:\n-\n-```bash\n-python -m transformers.onnx --model=keras-io/transformers-qa onnx/\n-```\n-\n-Per esportare un modello memorizzato localmente, devi disporre dei pesi del modello\n-e file tokenizer memorizzati in una directory. Ad esempio, possiamo caricare e salvare un\n-checkpoint come segue:\n-\n-```python\n->>> from transformers import AutoTokenizer, AutoModelForSequenceClassification\n-\n->>> # Load tokenizer and PyTorch weights form the Hub\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> pt_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> # Save to disk\n->>> tokenizer.save_pretrained(\"local-pt-checkpoint\")\n->>> pt_model.save_pretrained(\"local-pt-checkpoint\")\n-```\n-\n-Una volta salvato il checkpoint, possiamo esportarlo su ONNX puntando l'argomento `--model`\n-del pacchetto `transformers.onnx` nella directory desiderata:\n-\n-```bash\n-python -m transformers.onnx --model=local-pt-checkpoint onnx/\n-```\n-\n-### Selezione delle caratteristiche per diverse topologie di modello\n-\n-Ogni configurazione gi pronta viene fornita con una serie di _caratteristiche_ che ti consentono di\n-esportare modelli per diversi tipi di topologie o attivit. Come mostrato nella tabella\n-di seguito, ogni caratteristica 猫 associata a una diversa Auto Class:\n-\n-| Caratteristica                              | Auto Class                           |\n-| ------------------------------------ | ------------------------------------ |\n-| `causal-lm`, `causal-lm-with-past`   | `AutoModelForCausalLM`               |\n-| `default`, `default-with-past`       | `AutoModel`                          |\n-| `masked-lm`                          | `AutoModelForMaskedLM`               |\n-| `question-answering`                 | `AutoModelForQuestionAnswering`      |\n-| `seq2seq-lm`, `seq2seq-lm-with-past` | `AutoModelForSeq2SeqLM`              |\n-| `sequence-classification`            | `AutoModelForSequenceClassification` |\n-| `token-classification`               | `AutoModelForTokenClassification`    |\n-\n-Per ciascuna configurazione, puoi trovare l'elenco delle funzionalit supportate tramite il\n-`FeaturesManager`. Ad esempio, per DistilBERT abbiamo:\n-\n-```python\n->>> from transformers.onnx.features import FeaturesManager\n-\n->>> distilbert_features = list(FeaturesManager.get_supported_features_for_model_type(\"distilbert\").keys())\n->>> print(distilbert_features)\n-[\"default\", \"masked-lm\", \"causal-lm\", \"sequence-classification\", \"token-classification\", \"question-answering\"]\n-```\n-\n-Puoi quindi passare una di queste funzionalit all'argomento `--feature` nel\n-pacchetto `transformers.onnx`. Ad esempio, per esportare un modello di classificazione del testo\n-possiamo scegliere un modello ottimizzato dall'Hub ed eseguire:\n-\n-```bash\n-python -m transformers.onnx --model=distilbert/distilbert-base-uncased-finetuned-sst-2-english \\\n-                            --feature=sequence-classification onnx/\n-```\n-\n-che visualizzer i seguenti registri:\n-\n-```bash\n-Validating ONNX model...\n-        -[] ONNX model output names match reference model ({'logits'})\n-        - Validating ONNX Model output \"logits\":\n-                -[] (2, 2) matches (2, 2)\n-                -[] all values close (atol: 1e-05)\n-All good, model saved at: onnx/model.onnx\n-```\n-\n-Puoi notare che in questo caso, i nomi di output del modello ottimizzato sono\n-`logits` invece di `last_hidden_state` che abbiamo visto con il\n-checkpoint `distilbert/distilbert-base-uncased` precedente. Questo 猫 previsto dal\n-modello ottimizato visto che ha una testa di e.\n-\n-<Tip>\n-\n-Le caratteristiche che hanno un suffisso `wtih-past` (ad es. `causal-lm-with-past`)\n-corrispondono a topologie di modello con stati nascosti precalcolati (chiave e valori\n-nei blocchi di attenzione) che possono essere utilizzati per la decodifica autoregressiva veloce.\n-\n-</Tip>\n-\n-\n-### Esportazione di un modello per un'architettura non supportata\n-\n-Se desideri esportare un modello la cui architettura non 猫 nativamente supportata dalla\n-libreria, ci sono tre passaggi principali da seguire:\n-\n-1. Implementare una configurazione ONNX personalizzata.\n-2. Esportare il modello in ONNX.\n-3. Convalidare gli output di PyTorch e dei modelli esportati.\n-\n-In questa sezione, vedremo come DistilBERT 猫 stato implementato per mostrare cosa 猫\n-coinvolto in ogni passaggio.\n-\n-#### Implementazione di una configurazione ONNX personalizzata\n-\n-Iniziamo con l'oggetto di configurazione ONNX. Forniamo tre classi\n-astratte da cui ereditare, a seconda del tipo di archittettura\n-del modello che desideri esportare:\n-\n-* I modelli basati su encoder ereditano da [`~onnx.config.OnnxConfig`]\n-* I modelli basati su decoder ereditano da [`~onnx.config.OnnxConfigWithPast`]\n-* I modelli encoder-decoder ereditano da[`~onnx.config.OnnxSeq2SeqConfigWithPast`]\n-\n-<Tip>\n-\n-Un buon modo per implementare una configurazione ONNX personalizzata 猫 guardare l'implementazione\n-esistente nel file `configuration_<model_name>.py` di un'architettura simile.\n-\n-</Tip>\n-\n-Poich茅 DistilBERT 猫 un modello basato su encoder, la sua configurazione eredita da\n-`OnnxConfig`:\n-\n-```python\n->>> from typing import Mapping, OrderedDict\n->>> from transformers.onnx import OnnxConfig\n-\n-\n->>> class DistilBertOnnxConfig(OnnxConfig):\n-...     @property\n-...     def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-...         return OrderedDict(\n-...             [\n-...                 (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n-...                 (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n-...             ]\n-...         )\n-```\n-\n-Ogni oggetto di configurazione deve implementare la propriet `inputs` e restituire una\n-mappatura, dove ogni chiave corrisponde a un input previsto e ogni valore\n-indica l'asse di quell'input. Per DistilBERT, possiamo vedere che sono richiesti\n-due input: `input_ids` e `attention_mask`. Questi inputs hanno la stessa forma di\n-`(batch_size, sequence_length)` per questo motivo vediamo gli stessi assi usati nella\n-configurazione.\n-\n-<Tip>\n-\n-Puoi notare che la propriet `inputs` per `DistilBertOnnxConfig` restituisce un\n-`OrdinatoDict`. Ci貌 garantisce che gli input corrispondano alla loro posizione\n-relativa all'interno del metodo `PreTrainedModel.forward()` durante il tracciamento del grafico.\n-Raccomandiamo di usare un `OrderedDict` per le propriet `inputs` e `outputs`\n-quando si implementano configurazioni ONNX personalizzate.\n-\n-</Tip>\n-\n-Dopo aver implementato una configurazione ONNX, 猫 possibile istanziarla\n-fornendo alla configurazione del modello base come segue:\n-\n-```python\n->>> from transformers import AutoConfig\n-\n->>> config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> onnx_config = DistilBertOnnxConfig(config)\n-```\n-\n-L'oggetto risultante ha diverse propriet utili. Ad esempio 猫 possibile visualizzare il\n-Set operatore ONNX che verr utilizzato durante l'esportazione:\n-\n-```python\n->>> print(onnx_config.default_onnx_opset)\n-11\n-```\n-\n- inoltre possibile visualizzare gli output associati al modello come segue:\n-\n-```python\n->>> print(onnx_config.outputs)\n-OrderedDict([(\"last_hidden_state\", {0: \"batch\", 1: \"sequence\"})])\n-```\n-\n-Puoi notare che la propriet degli output segue la stessa struttura degli input; esso\n-restituisce un `OrderedDict` di output con nome e le loro forme. La struttura di output\n-猫 legato alla scelta della funzione con cui viene inizializzata la configurazione.\n-Per impostazione predefinita, la configurazione ONNX viene inizializzata con la funzione 'predefinita'\n-che corrisponde all'esportazione di un modello caricato con la classe `AutoModel`. Se tu\n-desideri esportare una topologia di modello diversa, 猫 sufficiente fornire una funzionalit diversa a\n-l'argomento `task` quando inizializzi la configurazione ONNX. Ad esempio, se\n-volevamo esportare DistilBERT con una testa di classificazione per sequenze, potremmo\n-usare:\n-\n-```python\n->>> from transformers import AutoConfig\n-\n->>> config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> onnx_config_for_seq_clf = DistilBertOnnxConfig(config, task=\"sequence-classification\")\n->>> print(onnx_config_for_seq_clf.outputs)\n-OrderedDict([('logits', {0: 'batch'})])\n-```\n-\n-<Tip>\n-\n-Tutte le propriet e i metodi di base associati a [`~onnx.config.OnnxConfig`] e le\n-altre classi di configurazione possono essere sovrascritte se necessario. Guarda\n-[`BartOnnxConfig`] per un esempio avanzato.\n-\n-</Tip>\n-\n-#### Esportazione del modello\n-\n-Una volta implementata la configurazione ONNX, il passaggio successivo consiste nell'esportare il\n-modello. Qui possiamo usare la funzione `export()` fornita dal\n-pacchetto `transformers.onnx`. Questa funzione prevede la configurazione ONNX, insieme\n-con il modello base e il tokenizer e il percorso per salvare il file esportato:\n-\n-```python\n->>> from pathlib import Path\n->>> from transformers.onnx import export\n->>> from transformers import AutoTokenizer, AutoModel\n-\n->>> onnx_path = Path(\"model.onnx\")\n->>> model_ckpt = \"distilbert/distilbert-base-uncased\"\n->>> base_model = AutoModel.from_pretrained(model_ckpt)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n-\n->>> onnx_inputs, onnx_outputs = export(tokenizer, base_model, onnx_config, onnx_config.default_onnx_opset, onnx_path)\n-```\n-\n-Gli `onnx_inputs` e `onnx_outputs` restituiti dalla funzione `export()` sono\n-liste di chiavi definite nelle propriet di `input` e `output` della\n-configurazione. Una volta esportato il modello, puoi verificare che il modello sia ben\n-formato come segue:\n-\n-```python\n->>> import onnx\n-\n->>> onnx_model = onnx.load(\"model.onnx\")\n->>> onnx.checker.check_model(onnx_model)\n-```\n-\n-<Tip>\n-\n-Se il tuo modello 猫 pi霉 largo di 2 GB, vedrai che molti file aggiuntivi sono\n-creati durante l'esportazione. Questo 猫 _previsto_ perch茅 ONNX utilizza [Protocol\n-Buffer](https://developers.google.com/protocol-buffers/) per memorizzare il modello e\n-questi hanno un limite di dimensione 2 GB. Vedi la [Documentazione\n-ONNX](https://github.com/onnx/onnx/blob/master/docs/ExternalData.md)\n-per istruzioni su come caricare modelli con dati esterni.\n-\n-</Tip>\n-\n-#### Convalida degli output del modello\n-\n-Il passaggio finale consiste nel convalidare gli output dal modello di base e quello esportato\n-corrispondere entro una soglia di tolleranza assoluta. Qui possiamo usare la\n-Funzione `validate_model_outputs()` fornita dal pacchetto `transformers.onnx`\n-come segue:\n-\n-```python\n->>> from transformers.onnx import validate_model_outputs\n-\n->>> validate_model_outputs(\n-...     onnx_config, tokenizer, base_model, onnx_path, onnx_outputs, onnx_config.atol_for_validation\n-... )\n-```\n-\n-Questa funzione usa il metodo `OnnxConfig.generate_dummy_inputs()` per generare\n-input per il modello di base e quello esportato e la tolleranza assoluta pu貌 essere\n-definita nella configurazione. Generalmente troviamo una corrispondenza numerica nell'intervallo da 1e-6\n-a 1e-4, anche se 猫 probabile che qualsiasi cosa inferiore a 1e-3 vada bene.\n-\n-### Contribuire con una nuova configurazione a  Transformers\n-\n-Stiamo cercando di espandere l'insieme di configurazioni gi pronte e di accettare\n-contributi della community! Se vuoi contribuire con la tua aggiunta\n-nella libreria, dovrai:\n-\n-* Implementare la configurazione ONNX nella corrispondente `configuration file\n-_<model_name>.py`\n-* Includere l'architettura del modello e le funzioni corrispondenti in [`~onnx.features.FeatureManager`]\n-* Aggiungere la tua architettura del modello ai test in `test_onnx_v2.py`\n-\n-Scopri come stato contribuito la configurazione per [IBERT](https://github.com/huggingface/transformers/pull/14868/files) per\n-avere un'idea di cosa 猫 coinvolto.\n-\n-## TorchScript\n-\n-<Tip>\n-\n-Questo 猫 l'inizio dei nostri esperimenti con TorchScript e stiamo ancora esplorando le sue capacit con\n-modelli con variable-input-size.  una nostra priorit e approfondiremo le nostre analisi nelle prossime versioni,\n-con pi霉 esempi di codici, un'implementazione pi霉 flessibile e benchmark che confrontano i codici basati su Python con quelli compilati con\n-TorchScript.\n-\n-</Tip>\n-\n-Secondo la documentazione di Pytorch: \"TorchScript 猫 un modo per creare modelli serializzabili e ottimizzabili da codice\n-Pytorch\". I due moduli di Pytorch [JIT e TRACE](https://pytorch.org/docs/stable/jit.html) consentono allo sviluppatore di esportare\n-il loro modello da riutilizzare in altri programmi, come i programmi C++ orientati all'efficienza.\n-\n-Abbiamo fornito un'interfaccia che consente l'esportazione di modelli  Transformers in TorchScript in modo che possano essere riutilizzati\n-in un ambiente diverso rispetto a un programma Python basato su Pytorch. Qui spieghiamo come esportare e utilizzare i nostri modelli utilizzando\n-TorchScript.\n-\n-Esportare un modello richiede due cose:\n-\n-- Un passaggio in avanti con input fittizzi.\n-- Istanziazione del modello con flag `torchscript`.\n-\n-Queste necessit implicano diverse cose a cui gli sviluppatori dovrebbero prestare attenzione. Questi dettagli mostrati sotto.\n-\n-### Flag TorchScript e pesi legati\n-\n-Questo flag 猫 necessario perch茅 la maggior parte dei modelli linguistici in questo repository hanno pesi legati tra il loro\n-strato \"Embedding\" e lo strato \"Decoding\". TorchScript non consente l'esportazione di modelli che hanno pesi\n-legati, quindi 猫 necessario prima slegare e clonare i pesi.\n-\n-Ci貌 implica che i modelli istanziati con il flag `torchscript` hanno il loro strato `Embedding` e strato `Decoding`\n-separato, il che significa che non dovrebbero essere addestrati in futuro. L'allenamento de-sincronizza i due\n-strati, portando a risultati inaspettati.\n-\n-Questo non 猫 il caso per i modelli che non hanno una testa del modello linguistico, poich茅 quelli non hanno pesi legati. Questi modelli\n-pu貌 essere esportato in sicurezza senza il flag `torchscript`.\n-\n-### Input fittizi e standard lengths\n-\n-Gli input fittizzi sono usati per fare un modello passaggio in avanti . Mentre i valori degli input si propagano attraverso i strati,\n-Pytorch tiene traccia delle diverse operazioni eseguite su ciascun tensore. Queste operazioni registrate vengono quindi utilizzate per\n-creare la \"traccia\" del modello.\n-\n-La traccia viene creata relativamente alle dimensioni degli input.  quindi vincolato dalle dimensioni dell'input\n-fittizio e non funzioner per altre lunghezze di sequenza o dimensioni batch. Quando si prover con una dimensione diversa, ci sar errore\n-come:\n-\n-`La dimensione espansa del tensore (3) deve corrispondere alla dimensione esistente (7) nella dimensione non singleton 2`\n-\n-will be raised. Si consiglia pertanto di tracciare il modello con una dimensione di input fittizia grande almeno quanto il pi霉 grande\n-input che verr fornito al modello durante l'inferenza.  possibile eseguire il padding per riempire i valori mancanti. Il modello\n-sar tracciato con una grande dimensione di input, tuttavia, anche le dimensioni della diverse matrici saranno grandi,\n-risultando in pi霉 calcoli.\n-\n-Si raccomanda di prestare attenzione al numero totale di operazioni eseguite su ciascun input e di seguire da vicino le prestazioni\n-durante l'esportazione di modelli di sequenza-lunghezza variabili.\n-\n-### Usare TorchSscript in Python\n-\n-Di seguito 猫 riportato un esempio, che mostra come salvare, caricare modelli e come utilizzare la traccia per l'inferenza.\n-\n-#### Salvare un modello\n-\n-Questo frammento di codice mostra come usare TorchScript per esportare un `BertModel`. Qui il `BertModel` 猫 istanziato secondo\n-una classe `BertConfig` e quindi salvato su disco con il nome del file `traced_bert.pt`\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-\n-enc = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-\n-# Tokenizing input text\n-text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n-tokenized_text = enc.tokenize(text)\n-\n-# Masking one of the input tokens\n-masked_index = 8\n-tokenized_text[masked_index] = \"[MASK]\"\n-indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)\n-segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n-\n-# Creating a dummy input\n-tokens_tensor = torch.tensor([indexed_tokens])\n-segments_tensors = torch.tensor([segments_ids])\n-dummy_input = [tokens_tensor, segments_tensors]\n-\n-# Initializing the model with the torchscript flag\n-# Flag set to True even though it is not necessary as this model does not have an LM Head.\n-config = BertConfig(\n-    vocab_size_or_config_json_file=32000,\n-    hidden_size=768,\n-    num_hidden_layers=12,\n-    num_attention_heads=12,\n-    intermediate_size=3072,\n-    torchscript=True,\n-)\n-\n-# Instantiating the model\n-model = BertModel(config)\n-\n-# The model needs to be in evaluation mode\n-model.eval()\n-\n-# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag\n-model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", torchscript=True)\n-\n-# Creating the trace\n-traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-torch.jit.save(traced_model, \"traced_bert.pt\")\n-```\n-\n-#### Caricare un modello\n-\n-Questo frammento di codice mostra come caricare il `BertModel` che era stato precedentemente salvato su disco con il nome `traced_bert.pt`.\n-Stiamo riutilizzando il `dummy_input` precedentemente inizializzato.\n-\n-```python\n-loaded_model = torch.jit.load(\"traced_bert.pt\")\n-loaded_model.eval()\n-\n-all_encoder_layers, pooled_output = loaded_model(*dummy_input)\n-```\n-\n-#### Utilizzare un modello tracciato per l'inferenza\n-\n-Usare il modello tracciato per l'inferenza 猫 semplice come usare il suo metodo dunder `__call__`:\n-\n-```python\n-traced_model(tokens_tensor, segments_tensors)\n-```\n-\n-### Implementare modelli HuggingFace TorchScript su AWS utilizzando Neuron SDK\n-\n-AWS ha introdotto [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/)\n-famiglia di istanze per l'inferenza di machine learning a basso costo e ad alte prestazioni nel cloud.\n-Le istanze Inf1 sono alimentate dal chip AWS Inferentia, un acceleratore hardware personalizzato,\n-specializzato in carichi di lavoro di inferenza di deep learning.\n-[AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#)\n-猫 l'SDK per Inferentia che supporta il tracciamento e l'ottimizzazione dei modelli transformers per\n-distribuzione su Inf1. L'SDK Neuron fornisce:\n-\n-\n-1. API di facile utilizzo con una riga di modifica del codice per tracciare e ottimizzare un modello TorchScript per l'inferenza nel cloud.\n-2. Ottimizzazioni delle prestazioni pronte all'uso per [miglioramento dei costi-prestazioni](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/>)\n-3. Supporto per i modelli di trasformatori HuggingFace costruiti con [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html)\n-    o [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html).\n-\n-#### Implicazioni\n-\n-Modelli Transformers basati su architettura [BERT (Bidirectional Encoder Representations from Transformers)](https://huggingface.co/docs/transformers/main/model_doc/bert),\n-o sue varianti come [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert)\n-e [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta)\n-funzioneranno meglio su Inf1 per attivit non generative come la question answering estrattive,\n-Classificazione della sequenza, Classificazione dei token. In alternativa, generazione di testo\n-le attivit possono essere adattate per essere eseguite su Inf1, secondo questo [tutorial AWS Neuron MarianMT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html).\n-Ulteriori informazioni sui modelli che possono essere convertiti fuori dagli schemi su Inferentia possono essere\n-trovati nella [sezione Model Architecture Fit della documentazione Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia).\n-\n-#### Dipendenze\n-\n-L'utilizzo di AWS Neuron per convertire i modelli richiede le seguenti dipendenze e l'ambiente:\n-\n-* A [Neuron SDK environment](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide),\n-  which comes pre-configured on [AWS Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html).\n-\n-#### Convertire un modello per AWS Neuron\n-\n-Usando lo stesso script come in [Usando TorchScipt in Python](https://huggingface.co/docs/transformers/main/en/serialization#using-torchscript-in-python)\n-per tracciare un \"BertModel\", importi l'estensione del framework `torch.neuron` per accedere\n-i componenti di Neuron SDK tramite un'API Python.\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-import torch.neuron\n-```\n-E modificare solo la riga di codice di traccia\n-\n-Da:\n-\n-```python\n-torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-```\n-\n-A:\n-\n-```python\n-torch.neuron.trace(model, [token_tensor, segments_tensors])\n-```\n-\n-Questa modifica consente a Neuron SDK di tracciare il modello e ottimizzarlo per l'esecuzione nelle istanze Inf1.\n-\n-Per ulteriori informazioni sulle funzionalit, gli strumenti, i tutorial di esempi e gli ultimi aggiornamenti di AWS Neuron SDK,\n-consultare la [documentazione AWS NeuronSDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)."
        },
        {
            "sha": "aa5538a6971cbb255e0c3931d6e067f227a84039",
            "filename": "docs/source/ja/main_classes/onnx.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmain_classes%2Fonnx.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmain_classes%2Fonnx.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fonnx.md?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -48,8 +48,3 @@ rendered properly in your Markdown viewer.\n \n  ONNX 妲娆°ㄣ姐涓ｃ _姗_ ㈤ｄ俱\n 俱俱裤ゃ搞俱裤广ㄣ广笺俱\n-\n-### FeaturesManager\n-\n-[[autodoc]] onnx.features.FeaturesManager\n-"
        },
        {
            "sha": "e58543b023a16c0c342ea7a70d4dd260c1ee44c3",
            "filename": "docs/source/ja/model_doc/beit.md",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fbeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fbeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbeit.md?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -93,12 +93,6 @@ BEiT 浣跨ㄣ濮褰圭ゅ寮 Hugging Face 炽炽\n \n [[autodoc]] BeitConfig\n \n-## BeitFeatureExtractor\n-\n-[[autodoc]] BeitFeatureExtractor\n-    - __call__\n-    - post_process_semantic_segmentation\n-\n ## BeitImageProcessor\n \n [[autodoc]] BeitImageProcessor"
        },
        {
            "sha": "c5a258c4962cc00408e486a9748f88277a0503b1",
            "filename": "docs/source/ja/model_doc/chinese_clip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fchinese_clip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fchinese_clip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fchinese_clip.md?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -91,10 +91,6 @@ Chinese-CLIP [OFA-Sys](https://huggingface.co/OFA-Sys) \n [[autodoc]] ChineseCLIPImageProcessorFast\n     - preprocess\n \n-## ChineseCLIPFeatureExtractor\n-\n-[[autodoc]] ChineseCLIPFeatureExtractor\n-\n ## ChineseCLIPProcessor\n \n [[autodoc]] ChineseCLIPProcessor\n@@ -114,4 +110,4 @@ Chinese-CLIP [OFA-Sys](https://huggingface.co/OFA-Sys) \n ## ChineseCLIPVisionModel\n \n [[autodoc]] ChineseCLIPVisionModel\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "acfab70139f17a8430d78b25d70dad81c38383b9",
            "filename": "docs/source/ja/model_doc/clip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fclip.md?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -138,10 +138,6 @@ CLIP 浣裤濮褰圭ゅ寮 Hugging Face 炽炽ャ\n [[autodoc]] CLIPImageProcessorFast\n     - preprocess\n \n-## CLIPFeatureExtractor\n-\n-[[autodoc]] CLIPFeatureExtractor\n-\n ## CLIPProcessor\n \n [[autodoc]] CLIPProcessor\n@@ -173,4 +169,3 @@ CLIP 浣裤濮褰圭ゅ寮 Hugging Face 炽炽ャ\n \n [[autodoc]] CLIPVisionModel\n     - forward\n-"
        },
        {
            "sha": "5dad8dab09a9c0ed6bc3679acff51df1ff27dc01",
            "filename": "docs/source/ja/model_doc/conditional_detr.md",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fconditional_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fconditional_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fconditional_detr.md?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -53,15 +53,6 @@ alt=\"\" width=\"600\"/>\n     - post_process_semantic_segmentation\n     - post_process_panoptic_segmentation\n \n-## ConditionalDetrFeatureExtractor\n-\n-[[autodoc]] ConditionalDetrFeatureExtractor\n-    - __call__\n-    - post_process_object_detection\n-    - post_process_instance_segmentation\n-    - post_process_semantic_segmentation\n-    - post_process_panoptic_segmentation\n-\n ## ConditionalDetrModel\n \n [[autodoc]] ConditionalDetrModel\n@@ -76,5 +67,3 @@ alt=\"\" width=\"600\"/>\n \n [[autodoc]] ConditionalDetrForSegmentation\n     - forward\n-\n-    \n\\ No newline at end of file"
        },
        {
            "sha": "93c9edb4b2b385b192c283e5fec51ee6426af05f",
            "filename": "docs/source/ja/model_doc/convnext.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fconvnext.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fconvnext.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fconvnext.md?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -55,10 +55,6 @@ ConvNeXT 浣跨ㄣ濮褰圭ゅ寮 Hugging Face 炽\n \n [[autodoc]] ConvNextConfig\n \n-## ConvNextFeatureExtractor\n-\n-[[autodoc]] ConvNextFeatureExtractor\n-\n ## ConvNextImageProcessor\n \n [[autodoc]] ConvNextImageProcessor\n@@ -79,4 +75,3 @@ ConvNeXT 浣跨ㄣ濮褰圭ゅ寮 Hugging Face 炽\n \n [[autodoc]] ConvNextForImageClassification\n     - forward\n-"
        },
        {
            "sha": "251acfe971d6278e2a08c79c54e9e23058658e51",
            "filename": "docs/source/ja/model_doc/deformable_detr.md",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeformable_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeformable_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeformable_detr.md?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -54,12 +54,6 @@ Deformable DETR 浣跨ㄣ濮褰圭ゅ寮 Hugging Face \n     - preprocess\n     - post_process_object_detection\n \n-## DeformableDetrFeatureExtractor\n-\n-[[autodoc]] DeformableDetrFeatureExtractor\n-    - __call__\n-    - post_process_object_detection\n-\n ## DeformableDetrConfig\n \n [[autodoc]] DeformableDetrConfig"
        },
        {
            "sha": "18ec1737f40490d511db7a67e03a55a2864957a5",
            "filename": "docs/source/ja/model_doc/deit.md",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeit.md?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -88,11 +88,6 @@ DeiT 濮褰圭ゅ寮 Hugging Face 炽炽ャ\n \n [[autodoc]] DeiTConfig\n \n-## DeiTFeatureExtractor\n-\n-[[autodoc]] DeiTFeatureExtractor\n-    - __call__\n-\n ## DeiTImageProcessor\n \n [[autodoc]] DeiTImageProcessor\n@@ -123,4 +118,3 @@ DeiT 濮褰圭ゅ寮 Hugging Face 炽炽ャ\n \n [[autodoc]] DeiTForImageClassificationWithTeacher\n     - forward\n-"
        },
        {
            "sha": "d1adb5f838b12c1332528c3c40cea3c18900b311",
            "filename": "docs/source/ja/model_doc/detr.md",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fdetr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fja%2Fmodel_doc%2Fdetr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fdetr.md?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -193,15 +193,6 @@ DETR 浣跨ㄣ濮褰圭ゅ寮 Hugging Face 炽炽\n     - post_process_instance_segmentation\n     - post_process_panoptic_segmentation\n \n-## DetrFeatureExtractor\n-\n-[[autodoc]] DetrFeatureExtractor\n-    - __call__\n-    - post_process_object_detection\n-    - post_process_semantic_segmentation\n-    - post_process_instance_segmentation\n-    - post_process_panoptic_segmentation\n-\n ## DETR specific outputs\n \n [[autodoc]] models.detr.modeling_detr.DetrModelOutput"
        },
        {
            "sha": "ba8faca5f943d3a7520339d4a53a09f9b596b891",
            "filename": "docs/source/ko/main_classes/onnx.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fko%2Fmain_classes%2Fonnx.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fko%2Fmain_classes%2Fonnx.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmain_classes%2Fonnx.md?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -43,8 +43,3 @@ rendered properly in your Markdown viewer.\n ## ONNX 轨[[onnx-features]]\n \n 臧 ONNX れ れ  措歆   氇胳 措炒  瓴(exporting) 挫＜ _features_ 疙胳 瓣 惦.\n-\n-### FeaturesManager[[transformers.onnx.FeaturesManager]]\n-\n-[[autodoc]] onnx.features.FeaturesManager\n-"
        },
        {
            "sha": "4929408e6e136b90a346b73d23112dd6a98f94f7",
            "filename": "docs/source/ko/model_doc/clip.md",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fko%2Fmodel_doc%2Fclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fko%2Fmodel_doc%2Fclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fclip.md?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -25,7 +25,7 @@ Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gret\n \n *斓 旎错 牍 ろ 氙鸽Μ 挫 瓿 臧觳 旃错瓿毽 歆╈ 旄№搿 ╇. 措   歆 るジ 臧 臧 歆旮  於臧 茧波毵 办错瓣 氙搿 攴 茧标臣 ╈膘 ╇. 措胳  ろ胳 歆 淀 瓴   甏氩 歆 るゼ ╉ 欤 膦 . 措胳 旌§ 毵於 臧   , 疙半缝 歆 4  措胳-ろ 办错办 SOTA 欷 措胳  觳攵 搓  臧ロ瓴 淀 氚╇ 疙  惦.   , 办措 惦 臧 臧 彀胳“瓯半 搿 臧 る  ╇ 氇胳  茧 搿 措ゼ 臧ロ瓴 ╇. 措 茧胳 OCR, 牍   胳, 歆毽 旃 , 攴鸽Μ瓿 毵 膦毳 鸽 臧觳 攵毳  30臧 挫 れ 旮办〈 旎错 牍 办错办  氩れ毵轨 淀  攴 氚╈ 彪レ 瓣惮╇.  氇胳 攵攵   氙 瓴 措氅, 膦膦 办错办氤  措  歆  旮办瓿 瓴届  彪レ 氤挫. 毳 れ, ImageNet  ResNet-50 毳 搿缝茧 检る, 措 ResNet-50  128毵 臧  毳  ╉ 臧 惦. 旖 氚   氇 臧欷旃  https URL 瓿店╇.*\n \n- 氇胳 [valhalla](https://huggingface.co/valhalla)  旮办惦. \n+ 氇胳 [valhalla](https://huggingface.co/valhalla)  旮办惦.\n 氤 旖 [搓吵](https://github.com/openai/CLIP) 疙  惦.\n \n ##  瓿 [[usage-tips-and-example]]\n@@ -34,7 +34,7 @@ CLIP 氅半 牍 氚 胳 氇胳. 措胳-ろ \n \n 鸽ろ毹 胳 措胳毳 ロ旮 , 臧 措胳 瓿 旮办 瓴轨歆  旃れ る 攵瓿, 错  氩╇╇. [CLS]办 觳 措胳 茧 於臧╇. れ   旃 氩╈ 於臧瓿, 瓴瓣臣搿  氩№ るゼ 欷 鸽ろ毹 疙 ロ╇. [`CLIPImageProcessor`] 氇胳  措胳毳 毽挫( れ茧)瓿 攴 ╇  惦.\n \n-[`CLIPTokenizer`] ろ鸽ゼ 胳╉ ╇╇. [`CLIPProcessor`] [`CLIPImageProcessor`] [`CLIPTokenizer`]毳  胳ろ挫る 臧胳 ろ鸽ゼ 胳╉瓿 措胳毳 欷牍 氇 ╇╇. \n+[`CLIPTokenizer`] ろ鸽ゼ 胳╉ ╇╇. [`CLIPProcessor`] [`CLIPImageProcessor`] [`CLIPTokenizer`]毳  胳ろ挫る 臧胳 ろ鸽ゼ 胳╉瓿 措胳毳 欷牍 氇 ╇╇.\n \n れ  [`CLIPProcessor`] [`CLIPModel`] ╉ 措胳-ろ  毳 浑 氚╇ 氤挫欷.\n \n@@ -175,9 +175,9 @@ model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", dtype=torch.fl\n \n CLIP     Hugging Face community 耄 氇╇(搿 ) .\n \n-- [瓴 检 (戈车) 措胳 旌§ 臧歆瓿 CLIP 氙胳胳“旮](https://huggingface.co/blog/fine-tune-clip-rsicd): \n+- [瓴 检 (戈车) 措胳 旌§ 臧歆瓿 CLIP 氙胳胳“旮](https://huggingface.co/blog/fine-tune-clip-rsicd):\n [RSICD dataset](https://github.com/201528014227051/RSICD_optimal) 臧歆瓿 CLIP 氙胳胳“  氚╇瓿 办错 歃臧  彪 牍甑  敫搿攴 ろ\n--  [ ろ毽巾](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text) [COCO dataset](https://cocodataset.org/#home)毳 挫╉ 惦 牍瓿 ろ胳 胳毳 ╉挫 CLIP臧 牍-ろ  氇胳 措魂 奠る歆 氤挫欷. \n+-  [ ろ毽巾](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text) [COCO dataset](https://cocodataset.org/#home)毳 挫╉ 惦 牍瓿 ろ胳 胳毳 ╉挫 CLIP臧 牍-ろ  氇胳 措魂 奠る歆 氤挫欷.\n \n <PipelineTag pipeline=\"image-to-text\"/>\n \n@@ -187,7 +187,7 @@ CLIP     Hugging Face community 耄 氇\n \n - 惦 CLIP氇戈臣 MRR(Mean Reciprocal Rank)  办办 ╉ 措胳 瓴  [疙鸽](https://colab.research.google.com/drive/1bLVwVKpAndpEDHqjzxVPr_9nGrSbuOQd?usp=sharing). \n - 措胳 瓴瓿    氤挫欤茧 [疙鸽](https://colab.research.google.com/github/deep-diver/image_search_with_natural_language/blob/main/notebooks/Image_Search_CLIP.ipynb). \n-- Multilingual CLIP毳 ╉挫 措胳 ろ鸽ゼ 措魂 臧 氩№ 瓿店 毵ろ る歆  [疙鸽](https://colab.research.google.com/drive/1xO-wC_m_GNzgjIBQ4a4znvQkvDoZJvH4?usp=sharing).  \n+- Multilingual CLIP毳 ╉挫 措胳 ろ鸽ゼ 措魂 臧 氩№ 瓿店 毵ろ る歆  [疙鸽](https://colab.research.google.com/drive/1xO-wC_m_GNzgjIBQ4a4znvQkvDoZJvH4?usp=sharing). \n - [Unsplash](https://unsplash.com) [TMDB](https://www.themoviedb.org/) 办错办 ╉ 氙鸽(semantic) 措胳 瓴 CLIP 甑 氚╇  [疙鸽](https://colab.research.google.com/github/vivien000/clip-demo/blob/master/clip.ipynb#scrollTo=uzdFhRGqiWkR). \n \n **る 臧レ**\n@@ -226,10 +226,6 @@ CLIP     Hugging Face community 耄 氇\n [[autodoc]] CLIPImageProcessor\n     - preprocess\n \n-## CLIPFeatureExtractor[[transformers.CLIPFeatureExtractor]]\n-\n-[[autodoc]] CLIPFeatureExtractor\n-\n ## CLIPProcessor[[transformers.CLIPProcessor]]\n \n [[autodoc]] CLIPProcessor\n@@ -266,4 +262,3 @@ CLIP     Hugging Face community 耄 氇\n \n [[autodoc]] CLIPForImageClassification\n     - forward\n-"
        },
        {
            "sha": "473020a33f7aa25d86f05d1d8e8cca32aa3dd874",
            "filename": "docs/source/ko/model_doc/vit.md",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fko%2Fmodel_doc%2Fvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fko%2Fmodel_doc%2Fvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fvit.md?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -35,7 +35,7 @@ alt=\"drawing\" width=\"600\"/>\n - [DeiT](deit) (Data-efficient Image Transformers) (Facebook AI 臧氚). DeiT 氇胳 distilled vision transformers.\n   DeiT れ  茧  ViT 氇鸽 瓿店茧┌, 措 [`ViTModel`]  [`ViTForImageClassification`] 氚搿 ╉  惦. 旮办 3臧歆 旮半 4臧 氤 瓿惦╇: *facebook/deit-tiny-patch16-224*, *facebook/deit-small-patch16-224*, *facebook/deit-base-patch16-224* and *facebook/deit-base-patch16-384*. 攴鸽Μ瓿 氇胳 措胳毳 欷牍る┐ [`DeiTImageProcessor`]毳 ╉挫 る  .\n \n-- [BEiT](beit) (BERT pre-training of Image Transformers) (Microsoft Research 臧氚). BEiT 氇胳 BERT (masked image modeling) 臧  氚瓿 VQ-VAE 旮半 self-supervised 氚╇ 挫╉ supervised pre-trained vision transformers氤措  办 彪レ 氤挫. \n+- [BEiT](beit) (BERT pre-training of Image Transformers) (Microsoft Research 臧氚). BEiT 氇胳 BERT (masked image modeling) 臧  氚瓿 VQ-VAE 旮半 self-supervised 氚╇ 挫╉ supervised pre-trained vision transformers氤措  办 彪レ 氤挫.\n \n - DINO (Vision Transformers self-supervised   氚╇) (Facebook AI 臧氚). DINO 氚╇茧  Vision Transformer 惦歆   臧觳措ゼ 攵   ╈标潮 瓴诫 氤   毵れ ル鸽 ルレ 氤挫欷. DINO 觳错疙鸽 [hub](https://huggingface.co/models?other=dino) 彀眷  惦.\n \n@@ -93,7 +93,7 @@ ViT 於搿 氚 旎れろ 办错办  氙胳 臁办瓿 甏 \n \n 锔 斓\n \n-- [Optimum ╉ 毳 淀 Vision Transformer(ViT) 臧](https://www.philschmid.de/optimizing-vision-transformer)  敫搿攴 ろ \n+- [Optimum ╉ 毳 淀 Vision Transformer(ViT) 臧](https://www.philschmid.de/optimizing-vision-transformer)  敫搿攴 ろ\n \n ★ 於搿\n \n@@ -109,10 +109,6 @@ ViT 於搿 氚 旎れろ 办错办  氙胳 臁办瓿 甏 \n \n [[autodoc]] ViTConfig\n \n-## ViTFeatureExtractor [[transformers.ViTFeatureExtractor]]\n-\n-[[autodoc]] ViTFeatureExtractor\n-    - __call__\n \n ## ViTImageProcessor [[transformers.ViTImageProcessor]]\n \n@@ -139,4 +135,3 @@ ViT 於搿 氚 旎れろ 办错办  氙胳 臁办瓿 甏 \n \n [[autodoc]] ViTForImageClassification\n     - forward\n-"
        },
        {
            "sha": "04a636414204613262119c0a767aa83a5e975814",
            "filename": "docs/source/pt/_toctree.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fpt%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fpt%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2F_toctree.yml?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -20,11 +20,9 @@\n   - local: create_a_model\n     title: Criando uma arquitetura customizada\n   - local: custom_models\n-    title: Compartilhando modelos customizados \n+    title: Compartilhando modelos customizados\n   - local: run_scripts\n     title: Treinamento a partir de um script\n-  - local: serialization\n-    title: Exportando modelos para ONNX\n   - sections:\n     - local: tasks/sequence_classification\n       title: Classifica莽茫o de texto"
        },
        {
            "sha": "9e390f07bde41ded4d80a6d8faf75d563c3a2457",
            "filename": "docs/source/pt/serialization.md",
            "status": "removed",
            "additions": 0,
            "deletions": 502,
            "changes": 502,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/docs%2Fsource%2Fpt%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/docs%2Fsource%2Fpt%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Fserialization.md?ref=bc33fd3fc21f9b69e2b48ab8db890435752ac5fd",
            "patch": "@@ -1,502 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-锔 Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Exportando modelos para ONNX \n-\n-Se voc锚 precisar implantar modelos  Transformers em ambientes de produ莽茫o, recomendamos\n-exporta-los para um formato serializado que pode ser carregado e executado em\n-tempos de execu莽茫o e hardware. Neste guia, mostraremos como exportar modelos  Transformers\n-para [ONNX (Open Neural Network eXchange)](http://onnx.ai).\n-\n-<Tip>\n-\n-Uma vez exportado, um modelo pode ser otimizado para infer锚ncia por meio de t茅cnicas como\n-quantiza莽茫o e poda. Se voc锚 estiver interessado em otimizar seus modelos para serem executados com\n-m谩xima efici锚ncia, confira a biblioteca [ Optimum\n-](https://github.com/huggingface/optimum).\n-\n-</Tip>\n-\n-ONNX 茅 um padr茫o aberto que define um conjunto comum de operadores e um formato de arquivo comum\n-para representar modelos de aprendizado profundo em uma ampla variedade de estruturas, incluindo PyTorch e\n-TensorFlow. Quando um modelo 茅 exportado para o formato ONNX, esses operadores s茫o usados para\n-construir um grafo computacional (muitas vezes chamado de _representa莽茫o intermedi谩ria_) que\n-representa o fluxo de dados atrav茅s da rede neural.\n-\n-Ao expor um grafo com operadores e tipos de dados padronizados, o ONNX facilita a\n-alternar entre os frameworks. Por exemplo, um modelo treinado em PyTorch pode ser exportado para\n-formato ONNX e depois importado no TensorFlow (e vice-versa).\n-\n- Transformers fornece um pacote [`transformers.onnx`](main_classes/onnx) que permite\n-que voc锚 converta os checkpoints do modelo em um grafo ONNX aproveitando os objetos de configura莽茫o.\n-Esses objetos de configura莽茫o v锚m prontos para v谩rias arquiteturas de modelo e s茫o\n-projetado para ser facilmente extens铆vel a outras arquiteturas.\n-\n-As configura莽玫es prontas incluem as seguintes arquiteturas:\n-\n-<!--This table is automatically generated by `make fix-copies`, do not fill manually!-->\n-\n-- ALBERT\n-- BART\n-- BEiT\n-- BERT\n-- BigBird\n-- BigBird-Pegasus\n-- Blenderbot\n-- BlenderbotSmall\n-- BLOOM\n-- CamemBERT\n-- CLIP\n-- CodeGen\n-- Conditional DETR\n-- ConvBERT\n-- ConvNeXT\n-- ConvNeXTV2\n-- Data2VecText\n-- Data2VecVision\n-- DeBERTa\n-- DeBERTa-v2\n-- DeiT\n-- DETR\n-- DistilBERT\n-- ELECTRA\n-- ERNIE\n-- FlauBERT\n-- GPT Neo\n-- GPT-J\n-- GroupViT\n-- I-BERT\n-- LayoutLM\n-- LayoutLMv3\n-- LeViT\n-- Longformer\n-- LongT5\n-- M2M100\n-- Marian\n-- mBART\n-- MobileBERT\n-- MobileViT\n-- MT5\n-- OpenAI GPT-2\n-- OWL-ViT\n-- Perceiver\n-- PLBart\n-- ResNet\n-- RoBERTa\n-- RoFormer\n-- SegFormer\n-- SqueezeBERT\n-- Swin Transformer\n-- T5\n-- Table Transformer\n-- Vision Encoder decoder\n-- ViT\n-- XLM\n-- XLM-RoBERTa\n-- XLM-RoBERTa-XL\n-- YOLOS\n-\n-Nas pr贸ximas duas se莽玫es, mostraremos como:\n-\n-* Exportar um modelo suportado usando o pacote `transformers.onnx`.\n-* Exportar um modelo personalizado para uma arquitetura sem suporte.\n-\n-## Exportando um modelo para ONNX\n-\n-Para exportar um modelo  Transformers para o ONNX, primeiro voc锚 precisa instalar algumas\n-depend锚ncias extras:\n-\n-```bash\n-pip install transformers[onnx]\n-```\n-\n-O pacote `transformers.onnx` pode ent茫o ser usado como um m贸dulo Python:\n-\n-```bash\n-python -m transformers.onnx --help\n-\n-usage: Hugging Face Transformers ONNX exporter [-h] -m MODEL [--feature {causal-lm, ...}] [--opset OPSET] [--atol ATOL] output\n-\n-positional arguments:\n-  output                Path indicating where to store generated ONNX model.\n-\n-optional arguments:\n-  -h, --help            show this help message and exit\n-  -m MODEL, --model MODEL\n-                        Model ID on huggingface.co or path on disk to load model from.\n-  --feature {causal-lm, ...}\n-                        The type of features to export the model with.\n-  --opset OPSET         ONNX opset version to export the model with.\n-  --atol ATOL           Absolute difference tolerance when validating the model.\n-```\n-\n-A exporta莽茫o de um checkpoint usando uma configura莽茫o pronta pode ser feita da seguinte forma:\n-\n-```bash\n-python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/\n-```\n-\n-Voc锚 deve ver os seguintes logs:\n-\n-```bash\n-Validating ONNX model...\n-        -[] ONNX model output names match reference model ({'last_hidden_state'})\n-        - Validating ONNX Model output \"last_hidden_state\":\n-                -[] (2, 8, 768) matches (2, 8, 768)\n-                -[] all values close (atol: 1e-05)\n-All good, model saved at: onnx/model.onnx\n-```\n-\n-Isso exporta um grafo ONNX do ponto de verifica莽茫o definido pelo argumento `--model`. Nisso\n-Por exemplo, 茅 `distilbert/distilbert-base-uncased`, mas pode ser qualquer checkpoint no Hugging\n-Face Hub ou um armazenado localmente.\n-\n-O arquivo `model.onnx` resultante pode ser executado em um dos [muitos\n-aceleradores](https://onnx.ai/supported-tools.html#deployModel) que suportam o ONNX\n-padr茫o. Por exemplo, podemos carregar e executar o modelo com [ONNX\n-Tempo de execu莽茫o](https://onnxruntime.ai/) da seguinte forma:\n-\n-```python\n->>> from transformers import AutoTokenizer\n->>> from onnxruntime import InferenceSession\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> session = InferenceSession(\"onnx/model.onnx\")\n->>> # ONNX Runtime expects NumPy arrays as input\n->>> inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n->>> outputs = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n-```\n-\n-Os nomes de sa铆da necess谩rios (como `[\"last_hidden_state\"]`) podem ser obtidos pegando uma\n- configura莽茫o ONNX de cada modelo. Por exemplo, para DistilBERT temos:\n-\n-```python\n->>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig\n-\n->>> config = DistilBertConfig()\n->>> onnx_config = DistilBertOnnxConfig(config)\n->>> print(list(onnx_config.outputs.keys()))\n-[\"last_hidden_state\"]\n-```\n-\n-O processo 茅 id锚ntico para os checkpoints do TensorFlow no Hub. Por exemplo, podemos\n-exportar um checkpoint TensorFlow puro do [Keras\n-](https://huggingface.co/keras-io) da seguinte forma:\n-\n-```bash\n-python -m transformers.onnx --model=keras-io/transformers-qa onnx/\n-```\n-\n-Para exportar um modelo armazenado localmente, voc锚 precisar谩 ter os pesos e\n-arquivos tokenizer armazenados em um diret贸rio. Por exemplo, podemos carregar e salvar um checkpoint como:\n-\n-```python\n->>> from transformers import AutoTokenizer, AutoModelForSequenceClassification\n-\n->>> # Load tokenizer and PyTorch weights form the Hub\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> pt_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> # Save to disk\n->>> tokenizer.save_pretrained(\"local-pt-checkpoint\")\n->>> pt_model.save_pretrained(\"local-pt-checkpoint\")\n-```\n-\n-Uma vez que o checkpoint 茅 salvo, podemos export谩-lo para o ONNX apontando o `--model`\n-argumento do pacote `transformers.onnx` para o diret贸rio desejado:\n-\n-```bash\n-python -m transformers.onnx --model=local-pt-checkpoint onnx/\n-```\n-\n-```python\n->>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n-\n->>> # Load tokenizer and TensorFlow weights from the Hub\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> # Save to disk\n->>> tokenizer.save_pretrained(\"local-tf-checkpoint\")\n->>> tf_model.save_pretrained(\"local-tf-checkpoint\")\n-```\n-\n-Uma vez que o checkpoint 茅 salvo, podemos export谩-lo para o ONNX apontando o `--model`\n-argumento do pacote `transformers.onnx` para o diret贸rio desejado:\n-\n-```bash\n-python -m transformers.onnx --model=local-tf-checkpoint onnx/\n-```\n-\n-## Selecionando features para diferentes tarefas do modelo\n-\n-Cada configura莽茫o pronta vem com um conjunto de _features_ que permitem exportar\n-modelos para diferentes tipos de tarefas. Conforme mostrado na tabela abaixo, cada recurso 茅\n-associado a uma `AutoClass` diferente:\n-\n-| Feature                              | Auto Class                           |\n-| ------------------------------------ | ------------------------------------ |\n-| `causal-lm`, `causal-lm-with-past`   | `AutoModelForCausalLM`               |\n-| `default`, `default-with-past`       | `AutoModel`                          |\n-| `masked-lm`                          | `AutoModelForMaskedLM`               |\n-| `question-answering`                 | `AutoModelForQuestionAnswering`      |\n-| `seq2seq-lm`, `seq2seq-lm-with-past` | `AutoModelForSeq2SeqLM`              |\n-| `sequence-classification`            | `AutoModelForSequenceClassification` |\n-| `token-classification`               | `AutoModelForTokenClassification`    |\n-\n-Para cada configura莽茫o, voc锚 pode encontrar a lista de recursos suportados por meio do\n-[`~transformers.onnx.FeaturesManager`]. Por exemplo, para DistilBERT temos:\n-\n-```python\n->>> from transformers.onnx.features import FeaturesManager\n-\n->>> distilbert_features = list(FeaturesManager.get_supported_features_for_model_type(\"distilbert\").keys())\n->>> print(distilbert_features)\n-[\"default\", \"masked-lm\", \"causal-lm\", \"sequence-classification\", \"token-classification\", \"question-answering\"]\n-```\n-\n-Voc锚 pode ent茫o passar um desses recursos para o argumento `--feature` no\n-pacote `transformers.onnx`. Por exemplo, para exportar um modelo de classifica莽茫o de texto, podemos\n-escolher um modelo ajustado no Hub e executar:\n-\n-```bash\n-python -m transformers.onnx --model=distilbert/distilbert-base-uncased-finetuned-sst-2-english \\\n-                            --feature=sequence-classification onnx/\n-```\n-\n-Isso exibe os seguintes logs:\n-\n-```bash\n-Validating ONNX model...\n-        -[] ONNX model output names match reference model ({'logits'})\n-        - Validating ONNX Model output \"logits\":\n-                -[] (2, 2) matches (2, 2)\n-                -[] all values close (atol: 1e-05)\n-All good, model saved at: onnx/model.onnx\n-```\n-\n-Observe que, neste caso, os nomes de sa铆da do modelo ajustado s茫o `logits`\n-em vez do `last_hidden_state` que vimos com o checkpoint `distilbert/distilbert-base-uncased`\n-mais cedo. Isso 茅 esperado, pois o modelo ajustado (fine-tuned) possui uma cabe莽a de classifica莽茫o de sequ锚ncia.\n-\n-<Tip>\n-\n-Os recursos que t锚m um sufixo `with-pass` (como `causal-lm-with-pass`) correspondem a\n-classes de modelo com estados ocultos pr茅-computados (chave e valores nos blocos de aten莽茫o)\n-que pode ser usado para decodifica莽茫o autorregressiva r谩pida.\n-\n-</Tip>\n-\n-<Tip>\n-\n-Para modelos do tipo `VisionEncoderDecoder`, as partes do codificador e do decodificador s茫o\n-exportados separadamente como dois arquivos ONNX chamados `encoder_model.onnx` e `decoder_model.onnx` respectivamente.\n-\n-</Tip>\n-\n-## Exportando um modelo para uma arquitetura sem suporte\n-\n-Se voc锚 deseja exportar um modelo cuja arquitetura n茫o 茅 suportada nativamente pela\n-biblioteca, h谩 tr锚s etapas principais a seguir:\n-\n-1. Implemente uma configura莽茫o ONNX personalizada.\n-2. Exporte o modelo para o ONNX.\n-3. Valide as sa铆das do PyTorch e dos modelos exportados.\n-\n-Nesta se莽茫o, veremos como o DistilBERT foi implementado para mostrar o que est谩 envolvido\n-em cada passo.\n-\n-### Implementando uma configura莽茫o ONNX personalizada\n-\n-Vamos come莽ar com o objeto de configura莽茫o ONNX. Fornecemos tr锚s classes abstratas que\n-voc锚 deve herdar, dependendo do tipo de arquitetura de modelo que deseja exportar:\n-\n-* Modelos baseados em codificador herdam de [`~onnx.config.OnnxConfig`]\n-* Modelos baseados em decodificador herdam de [`~onnx.config.OnnxConfigWithPast`]\n-* Os modelos codificador-decodificador herdam de [`~onnx.config.OnnxSeq2SeqConfigWithPast`]\n-\n-<Tip>\n-\n-Uma boa maneira de implementar uma configura莽茫o ONNX personalizada 茅 observar as\n-implementa莽茫o no arquivo `configuration_<model_name>.py` de uma arquitetura semelhante.\n-\n-</Tip>\n-\n-Como o DistilBERT 茅 um modelo baseado em codificador, sua configura莽茫o 茅 herdada de\n-`OnnxConfig`:\n-\n-```python\n->>> from typing import Mapping, OrderedDict\n->>> from transformers.onnx import OnnxConfig\n-\n-\n->>> class DistilBertOnnxConfig(OnnxConfig):\n-...     @property\n-...     def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-...         return OrderedDict(\n-...             [\n-...                 (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n-...                 (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n-...             ]\n-...         )\n-```\n-\n-Todo objeto de configura莽茫o deve implementar a propriedade `inputs` e retornar um mapeamento,\n-onde cada chave corresponde a uma entrada esperada e cada valor indica o eixo \n-dessa entrada. Para o DistilBERT, podemos ver que duas entradas s茫o necess谩rias: `input_ids` e\n-`attention_mask`. Essas entradas t锚m a mesma forma de `(batch_size, sequence_length)`\n-茅 por isso que vemos os mesmos eixos usados na configura莽茫o.\n-\n-<Tip>\n-\n-Notice that `inputs` property for `DistilBertOnnxConfig` returns an `OrderedDict`. This\n-ensures that the inputs are matched with their relative position within the\n-`PreTrainedModel.forward()` method when tracing the graph. We recommend using an\n-`OrderedDict` for the `inputs` and `outputs` properties when implementing custom ONNX\n-configurations.\n-\n-Observe que a propriedade `inputs` para `DistilBertOnnxConfig` retorna um `OrderedDict`. Este\n-garante que as entradas sejam combinadas com sua posi莽茫o relativa dentro do\n-m茅todo `PreTrainedModel.forward()` ao tra莽ar o grafo. Recomendamos o uso de um\n-`OrderedDict` para as propriedades `inputs` e `outputs` ao implementar configura莽玫es personalizadas ONNX.\n-\n-</Tip>\n-\n-Depois de implementar uma configura莽茫o ONNX, voc锚 pode instanci谩-la fornecendo a\n-configura莽茫o do modelo base da seguinte forma:\n-\n-```python\n->>> from transformers import AutoConfig\n-\n->>> config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> onnx_config = DistilBertOnnxConfig(config)\n-```\n-\n-O objeto resultante tem v谩rias propriedades 煤teis. Por exemplo, voc锚 pode visualizar o conjunto de operadores ONNX\n- que ser谩 usado durante a exporta莽茫o:\n-\n-```python\n->>> print(onnx_config.default_onnx_opset)\n-11\n-```\n-\n-Voc锚 tamb茅m pode visualizar as sa铆das associadas ao modelo da seguinte forma:\n-\n-```python\n->>> print(onnx_config.outputs)\n-OrderedDict([(\"last_hidden_state\", {0: \"batch\", 1: \"sequence\"})])\n-```\n-\n-Observe que a propriedade outputs segue a mesma estrutura das entradas; ele retorna um\n-`OrderedDict` de sa铆das nomeadas e suas formas. A estrutura de sa铆da est谩 ligada a\n-escolha do recurso com o qual a configura莽茫o 茅 inicializada. Por padr茫o, a configura莽茫o do ONNX\n-茅 inicializada com o recurso `default` que corresponde  exporta莽茫o de um\n-modelo carregado com a classe `AutoModel`. Se voc锚 deseja exportar um modelo para outra tarefa,\n-apenas forne莽a um recurso diferente para o argumento `task` quando voc锚 inicializar a configura莽茫o ONNX\n-. Por exemplo, se quisermos exportar o DistilBERT com uma sequ锚ncia\n-de classifica莽茫o, poder铆amos usar:\n-\n-```python\n->>> from transformers import AutoConfig\n-\n->>> config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> onnx_config_for_seq_clf = DistilBertOnnxConfig(config, task=\"sequence-classification\")\n->>> print(onnx_config_for_seq_clf.outputs)\n-OrderedDict([('logits', {0: 'batch'})])\n-```\n-\n-<Tip>\n-\n-Todas as propriedades e m茅todos b谩sicos associados a [`~onnx.config.OnnxConfig`] e\n-as outras classes de configura莽茫o podem ser substitu铆das se necess谩rio. Confira [`BartOnnxConfig`]\n-para um exemplo avan莽ado.\n-\n-</Tip>\n-\n-### Exportando um modelo\n-\n-Depois de ter implementado a configura莽茫o do ONNX, o pr贸ximo passo 茅 exportar o modelo.\n-Aqui podemos usar a fun莽茫o `export()` fornecida pelo pacote `transformers.onnx`.\n-Esta fun莽茫o espera a configura莽茫o do ONNX, juntamente com o modelo base e o tokenizer,\n-e o caminho para salvar o arquivo exportado:\n-\n-```python\n->>> from pathlib import Path\n->>> from transformers.onnx import export\n->>> from transformers import AutoTokenizer, AutoModel\n-\n->>> onnx_path = Path(\"model.onnx\")\n->>> model_ckpt = \"distilbert/distilbert-base-uncased\"\n->>> base_model = AutoModel.from_pretrained(model_ckpt)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n-\n->>> onnx_inputs, onnx_outputs = export(tokenizer, base_model, onnx_config, onnx_config.default_onnx_opset, onnx_path)\n-```\n-\n-Os `onnx_inputs` e `onnx_outputs` retornados pela fun莽茫o `export()` s茫o listas de\n- chaves definidas nas propriedades `inputs` e `outputs` da configura莽茫o. Uma vez que o\n-modelo 茅 exportado, voc锚 pode testar se o modelo est谩 bem formado da seguinte forma:\n-\n-```python\n->>> import onnx\n-\n->>> onnx_model = onnx.load(\"model.onnx\")\n->>> onnx.checker.check_model(onnx_model)\n-```\n-\n-<Tip>\n-\n-Se o seu modelo for maior que 2GB, voc锚 ver谩 que muitos arquivos adicionais s茫o criados\n-durante a exporta莽茫o. Isso 茅 _esperado_ porque o ONNX usa [Protocol\n-Buffers](https://developers.google.com/protocol-buffers/) para armazenar o modelo e estes\n-t锚m um limite de tamanho de 2GB. Veja a [ONNX\n-documenta莽茫o](https://github.com/onnx/onnx/blob/master/docs/ExternalData.md) para\n-instru莽玫es sobre como carregar modelos com dados externos.\n-\n-</Tip>\n-\n-### Validando a sa铆da dos modelos\n-\n-A etapa final 茅 validar se as sa铆das do modelo base e exportado concordam\n-dentro de alguma toler芒ncia absoluta. Aqui podemos usar a fun莽茫o `validate_model_outputs()`\n-fornecida pelo pacote `transformers.onnx` da seguinte forma:\n-\n-```python\n->>> from transformers.onnx import validate_model_outputs\n-\n->>> validate_model_outputs(\n-...     onnx_config, tokenizer, base_model, onnx_path, onnx_outputs, onnx_config.atol_for_validation\n-... )\n-```\n-\n-Esta fun莽茫o usa o m茅todo [`~transformers.onnx.OnnxConfig.generate_dummy_inputs`] para\n-gerar entradas para o modelo base e o exportado, e a toler芒ncia absoluta pode ser\n-definida na configura莽茫o. Geralmente encontramos concord芒ncia num茅rica em 1e-6 a 1e-4\n-de alcance, embora qualquer coisa menor que 1e-3 provavelmente esteja OK.\n-\n-## Contribuindo com uma nova configura莽茫o para  Transformers\n-\n-Estamos procurando expandir o conjunto de configura莽玫es prontas e receber contribui莽玫es\n-da comunidade! Se voc锚 gostaria de contribuir para a biblioteca, voc锚\n-precisar谩:\n-\n-* Implemente a configura莽茫o do ONNX no arquivo `configuration_<model_name>.py` correspondente\n-Arquivo\n-* Incluir a arquitetura do modelo e recursos correspondentes em\n-  [`~onnx.features.FeatureManager`]\n-* Adicione sua arquitetura de modelo aos testes em `test_onnx_v2.py`\n-\n-Confira como ficou a configura莽茫o do [IBERT\n-](https://github.com/huggingface/transformers/pull/14868/files) para obter uma\n-id茅ia do que est谩 envolvido."
        },
        {
            "sha": "ef4cee5f9b489642a7e514a95f0da34ca1f878a4",
            "filename": "docs/source/zh/main_classes/onnx.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fzh%2Fmain_classes%2Fonnx.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30/docs%2Fsource%2Fzh%2Fmain_classes%2Fonnx.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fonnx.md?ref=248e7ef8bcc7e8e1990ce6210f03d7c5f6cf4a30",
            "patch": "@@ -43,8 +43,3 @@ rendered properly in your Markdown viewer.\n ## ONNX Features\n \n 姣涓ONNX缃涓涓缁 _规_ 稿宠锛浣挎ㄨ藉涓轰绫诲缁浠诲″煎烘ā\n-\n-### FeaturesManager\n-\n-[[autodoc]] onnx.features.FeaturesManager\n-"
        }
    ],
    "stats": {
        "total": 1908,
        "additions": 9,
        "deletions": 1899
    }
}