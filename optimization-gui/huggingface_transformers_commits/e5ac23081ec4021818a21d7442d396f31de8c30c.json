{
    "author": "gante",
    "message": "[Gemma3] compile ✨  (#37447)",
    "sha": "e5ac23081ec4021818a21d7442d396f31de8c30c",
    "files": [
        {
            "sha": "89a017bfdbe4f967edfc19540049f557f88a32b9",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=e5ac23081ec4021818a21d7442d396f31de8c30c",
            "patch": "@@ -1654,9 +1654,7 @@ class HybridCache(Cache):\n         ```\n     \"\"\"\n \n-    # TODO (joao): dive deeper into gemma2 and paligemma -- there are reports of speed loss with compilation. Revert\n-    # ALL changes from the PR that commented the line below when reactivating it.\n-    # is_compileable = True\n+    is_compileable = True\n \n     def __init__(\n         self,\n@@ -1858,8 +1856,6 @@ class HybridChunkedCache(Cache):\n         ```\n     \"\"\"\n \n-    # TODO (joao): dive deeper into gemma2 and paligemma -- there are reports of speed loss with compilation. Revert\n-    # ALL changes from the PR that commented the line below when reactivating it.\n     is_compileable = True\n \n     def __init__("
        },
        {
            "sha": "1af14d021c7a48236b1b81b4ea72386d55eb972f",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=e5ac23081ec4021818a21d7442d396f31de8c30c",
            "patch": "@@ -42,6 +42,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_cohere2 import Cohere2Config\n \n \n@@ -300,6 +301,7 @@ def __init__(self, config: Cohere2Config, layer_idx: int):\n         self.is_sliding = (layer_idx + 1) % self.config.sliding_window_pattern != 0\n         self.sliding_window = config.sliding_window\n \n+    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -309,7 +311,6 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        last_cache_position: int = 0,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -330,7 +331,6 @@ def forward(\n                 (see `past_key_values`).\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n-            last_cache_position (`int`): equivalent to `cache_position[-1]` but allow indexing without breaking dynamo tracing\n         \"\"\"\n \n         if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n@@ -349,11 +349,16 @@ def forward(\n                 )\n                 attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n                 # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n-                # `last_cache_position` is equivalent to `cache_position[-1]` but without breaking dynamo\n-                offset = last_cache_position - effective_seq_len\n+                offset = cache_position[-1] - effective_seq_len + 1\n                 # Should only be used when beyond the sliding window (i.e. offset > 0)\n                 offset = max(0, offset)\n-                attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]\n+                # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n+                # but without data-dependent slicing (i.e. torch.compile friendly)\n+                mask_indexes = torch.arange(\n+                    min(effective_seq_len, attention_mask.shape[-1]), device=attention_mask.device\n+                )\n+                mask_indexes += offset\n+                attention_mask = attention_mask[:, :, :, mask_indexes]\n \n         residual = hidden_states\n \n@@ -539,6 +544,7 @@ def set_input_embeddings(self, value):\n \n     @can_return_tuple\n     @add_start_docstrings_to_model_forward(COHERE2_INPUTS_DOCSTRING)\n+    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -550,7 +556,6 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -590,16 +595,6 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n-        # (retrieving the same value from `cache_position` later on would crash dynamo)\n-        if last_cache_position is None:\n-            last_cache_position = 0\n-            if attention_mask is not None:\n-                # In case a 4d mask is passed directly without using `generate`, we have to rely on cache_position\n-                # It will break dynamo tracing but there are no way around it (and it should never happen in practice)\n-                last_cache_position = (\n-                    attention_mask.shape[-1] if attention_mask.dim() == 2 else cache_position[-1].item()\n-                )\n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n@@ -627,7 +622,6 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n-                    last_cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -638,7 +632,6 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n-                    last_cache_position=last_cache_position,\n                     **flash_attn_kwargs,\n                 )\n \n@@ -928,10 +921,6 @@ def prepare_inputs_for_generation(\n             # The clone here is for the same reason as for `position_ids`.\n             model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n \n-        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n-        # (retrieving the same value from `cache_position` later on would crash dynamo)\n-        model_inputs[\"last_cache_position\"] = attention_mask.shape[-1] if attention_mask is not None else 0\n-\n         if (\n             isinstance(past_key_values, HybridCache)\n             and attention_mask.ndim == 2"
        },
        {
            "sha": "85a8d04a509188c24e90c536d60654ea4e33c677",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 28,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=e5ac23081ec4021818a21d7442d396f31de8c30c",
            "patch": "@@ -23,15 +23,12 @@\n from ...cache_utils import Cache, HybridCache\n from ...configuration_utils import PretrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-)\n+from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import (\n-    logging,\n-)\n+from ...utils import add_start_docstrings_to_model_forward, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..cohere.modeling_cohere import (\n     CohereAttention,\n     CohereDecoderLayer,\n@@ -45,6 +42,9 @@\n from ..gemma2.modeling_gemma2 import Gemma2Model\n \n \n+COHERE2_INPUTS_DOCSTRING = None  # Will be picked up by modular\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -351,6 +351,7 @@ def __init__(self, config: Cohere2Config, layer_idx: int):\n         self.is_sliding = (layer_idx + 1) % self.config.sliding_window_pattern != 0\n         self.sliding_window = config.sliding_window\n \n+    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -360,7 +361,6 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        last_cache_position: int = 0,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -381,7 +381,6 @@ def forward(\n                 (see `past_key_values`).\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n-            last_cache_position (`int`): equivalent to `cache_position[-1]` but allow indexing without breaking dynamo tracing\n         \"\"\"\n \n         if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n@@ -400,11 +399,16 @@ def forward(\n                 )\n                 attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n                 # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n-                # `last_cache_position` is equivalent to `cache_position[-1]` but without breaking dynamo\n-                offset = last_cache_position - effective_seq_len\n+                offset = cache_position[-1] - effective_seq_len + 1\n                 # Should only be used when beyond the sliding window (i.e. offset > 0)\n                 offset = max(0, offset)\n-                attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]\n+                # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n+                # but without data-dependent slicing (i.e. torch.compile friendly)\n+                mask_indexes = torch.arange(\n+                    min(effective_seq_len, attention_mask.shape[-1]), device=attention_mask.device\n+                )\n+                mask_indexes += offset\n+                attention_mask = attention_mask[:, :, :, mask_indexes]\n \n         residual = hidden_states\n \n@@ -452,6 +456,9 @@ def __init__(self, config: Cohere2Config):\n         self.norm = Cohere2LayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n         self.rotary_emb = Cohere2RotaryEmbedding(config=config)\n \n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(COHERE2_INPUTS_DOCSTRING)\n+    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -463,7 +470,6 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -503,16 +509,6 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n-        # (retrieving the same value from `cache_position` later on would crash dynamo)\n-        if last_cache_position is None:\n-            last_cache_position = 0\n-            if attention_mask is not None:\n-                # In case a 4d mask is passed directly without using `generate`, we have to rely on cache_position\n-                # It will break dynamo tracing but there are no way around it (and it should never happen in practice)\n-                last_cache_position = (\n-                    attention_mask.shape[-1] if attention_mask.dim() == 2 else cache_position[-1].item()\n-                )\n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n@@ -540,7 +536,6 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n-                    last_cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -551,7 +546,6 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n-                    last_cache_position=last_cache_position,\n                     **flash_attn_kwargs,\n                 )\n \n@@ -625,10 +619,6 @@ def prepare_inputs_for_generation(\n             # The clone here is for the same reason as for `position_ids`.\n             model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n \n-        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n-        # (retrieving the same value from `cache_position` later on would crash dynamo)\n-        model_inputs[\"last_cache_position\"] = attention_mask.shape[-1] if attention_mask is not None else 0\n-\n         if (\n             isinstance(past_key_values, HybridCache)\n             and attention_mask.ndim == 2"
        },
        {
            "sha": "353b171042f34b111fa27d57749ec1645a73a5b2",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 20,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=e5ac23081ec4021818a21d7442d396f31de8c30c",
            "patch": "@@ -47,6 +47,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_gemma2 import Gemma2Config\n \n \n@@ -285,6 +286,7 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.sliding_window = config.sliding_window\n \n+    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -295,7 +297,6 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        last_cache_position: int = 0,\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n@@ -314,11 +315,16 @@ def forward(\n                 )\n                 attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n                 # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n-                # `last_cache_position` is equivalent to `cache_position[-1]` but without breaking dynamo\n-                offset = last_cache_position - effective_seq_len\n+                offset = cache_position[-1] - effective_seq_len + 1\n                 # Should only be used when beyond the sliding window (i.e. offset > 0)\n                 offset = max(0, offset)\n-                attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]\n+                # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n+                # but without data-dependent slicing (i.e. torch.compile friendly)\n+                mask_indexes = torch.arange(\n+                    min(effective_seq_len, attention_mask.shape[-1]), device=attention_mask.device\n+                )\n+                mask_indexes += offset\n+                attention_mask = attention_mask[:, :, :, mask_indexes]\n \n         residual = hidden_states\n \n@@ -542,6 +548,7 @@ def set_input_embeddings(self, value):\n \n     @can_return_tuple\n     @add_start_docstrings_to_model_forward(GEMMA2_INPUTS_DOCSTRING)\n+    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -553,7 +560,6 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -594,16 +600,6 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n-        # (retrieving the same value from `cache_position` later on would crash dynamo)\n-        if last_cache_position is None:\n-            last_cache_position = 0\n-            if attention_mask is not None:\n-                # In case a 4d mask is passed directly without using `generate`, we have to rely on cache_position\n-                # It will break dynamo tracing but there are no way around it (and it should never happen in practice)\n-                last_cache_position = (\n-                    attention_mask.shape[-1] if attention_mask.dim() == 2 else cache_position[-1].item()\n-                )\n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n@@ -639,7 +635,6 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n-                    last_cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -651,7 +646,6 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n-                    last_cache_position=last_cache_position,\n                     **flash_attn_kwargs,\n                 )\n \n@@ -922,9 +916,6 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n-        # (retrieving the same value from `cache_position` later on would crash dynamo)\n-        model_inputs[\"last_cache_position\"] = attention_mask.shape[-1] if attention_mask is not None else 0\n         if logits_to_keep is None:\n             _ = model_inputs.pop(\"logits_to_keep\", None)\n "
        },
        {
            "sha": "b219384f34abe7c7b2f189a148c3938df14d87f4",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 25,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=e5ac23081ec4021818a21d7442d396f31de8c30c",
            "patch": "@@ -24,13 +24,11 @@\n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...configuration_utils import PretrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-)\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import is_torch_flex_attn_available, logging\n+from ...utils import add_start_docstrings_to_model_forward, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ..gemma.modeling_gemma import (\n     GemmaAttention,\n     GemmaForCausalLM,\n@@ -45,6 +43,7 @@\n \n \n _CHECKPOINT_FOR_DOC = \"google/gemma2-7b\"\n+GEMMA2_INPUTS_DOCSTRING = None  # Will be picked up by modular\n \n \n if is_torch_flex_attn_available():\n@@ -334,6 +333,7 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.sliding_window = config.sliding_window\n \n+    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -344,7 +344,6 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        last_cache_position: int = 0,\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n@@ -363,11 +362,16 @@ def forward(\n                 )\n                 attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n                 # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n-                # `last_cache_position` is equivalent to `cache_position[-1]` but without breaking dynamo\n-                offset = last_cache_position - effective_seq_len\n+                offset = cache_position[-1] - effective_seq_len + 1\n                 # Should only be used when beyond the sliding window (i.e. offset > 0)\n                 offset = max(0, offset)\n-                attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]\n+                # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n+                # but without data-dependent slicing (i.e. torch.compile friendly)\n+                mask_indexes = torch.arange(\n+                    min(effective_seq_len, attention_mask.shape[-1]), device=attention_mask.device\n+                )\n+                mask_indexes += offset\n+                attention_mask = attention_mask[:, :, :, mask_indexes]\n \n         residual = hidden_states\n \n@@ -409,6 +413,9 @@ def __init__(self, config: Gemma2Config):\n             [Gemma2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n \n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(GEMMA2_INPUTS_DOCSTRING)\n+    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -420,7 +427,6 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -461,16 +467,6 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n-        # (retrieving the same value from `cache_position` later on would crash dynamo)\n-        if last_cache_position is None:\n-            last_cache_position = 0\n-            if attention_mask is not None:\n-                # In case a 4d mask is passed directly without using `generate`, we have to rely on cache_position\n-                # It will break dynamo tracing but there are no way around it (and it should never happen in practice)\n-                last_cache_position = (\n-                    attention_mask.shape[-1] if attention_mask.dim() == 2 else cache_position[-1].item()\n-                )\n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n@@ -506,7 +502,6 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n-                    last_cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -518,7 +513,6 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n-                    last_cache_position=last_cache_position,\n                     **flash_attn_kwargs,\n                 )\n \n@@ -702,9 +696,6 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n-        # (retrieving the same value from `cache_position` later on would crash dynamo)\n-        model_inputs[\"last_cache_position\"] = attention_mask.shape[-1] if attention_mask is not None else 0\n         if logits_to_keep is None:\n             _ = model_inputs.pop(\"logits_to_keep\", None)\n "
        },
        {
            "sha": "170e3d952f319f793e89d7e456742d0d3fbbaca1",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 20,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=e5ac23081ec4021818a21d7442d396f31de8c30c",
            "patch": "@@ -45,6 +45,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_gemma3 import Gemma3Config, Gemma3TextConfig\n \n@@ -377,6 +378,7 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.is_sliding = self.self_attn.is_sliding\n         self.sliding_window = config.sliding_window\n \n+    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -388,7 +390,6 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        last_cache_position: int = 0,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n@@ -407,11 +408,16 @@ def forward(\n                 )\n                 attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n                 # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n-                # `last_cache_position` is equivalent to `cache_position[-1]` but without breaking dynamo\n-                offset = last_cache_position - effective_seq_len\n+                offset = cache_position[-1] - effective_seq_len + 1\n                 # Should only be used when beyond the sliding window (i.e. offset > 0)\n                 offset = max(0, offset)\n-                attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]\n+                # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n+                # but without data-dependent slicing (i.e. torch.compile friendly)\n+                mask_indexes = torch.arange(\n+                    min(effective_seq_len, attention_mask.shape[-1]), device=attention_mask.device\n+                )\n+                mask_indexes += offset\n+                attention_mask = attention_mask[:, :, :, mask_indexes]\n \n         residual = hidden_states\n \n@@ -626,6 +632,7 @@ def set_input_embeddings(self, value):\n \n     @can_return_tuple\n     @add_start_docstrings_to_model_forward(GEMMA3_INPUTS_DOCSTRING)\n+    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -637,7 +644,6 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -678,16 +684,6 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n-        # (retrieving the same value from `cache_position` later on would crash dynamo)\n-        if last_cache_position is None:\n-            last_cache_position = 0\n-            if attention_mask is not None:\n-                # In case a 4d mask is passed directly without using `generate`, we have to rely on cache_position\n-                # It will break dynamo tracing but there are no way around it (and it should never happen in practice)\n-                last_cache_position = (\n-                    attention_mask.shape[-1] if attention_mask.dim() == 2 else cache_position[-1].item()\n-                )\n         causal_mask = self._update_causal_mask(\n             attention_mask,\n             inputs_embeds,\n@@ -723,7 +719,6 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n-                    last_cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -736,7 +731,6 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n-                    last_cache_position=last_cache_position,\n                     **flash_attn_kwargs,\n                 )\n \n@@ -1009,9 +1003,6 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n-        # (retrieving the same value from `cache_position` later on would crash dynamo)\n-        model_inputs[\"last_cache_position\"] = attention_mask.shape[-1] if attention_mask is not None else 0\n         if logits_to_keep is None:\n             _ = model_inputs.pop(\"logits_to_keep\", None)\n "
        },
        {
            "sha": "7c95f63b0e04800ded27eadfeb28018e74b18add",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 14,
            "deletions": 22,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5ac23081ec4021818a21d7442d396f31de8c30c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=e5ac23081ec4021818a21d7442d396f31de8c30c",
            "patch": "@@ -26,11 +26,7 @@\n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...configuration_utils import PretrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    ModelOutput,\n-)\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n@@ -41,6 +37,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n     Gemma2Attention,\n@@ -460,6 +457,7 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.is_sliding = self.self_attn.is_sliding\n         self.sliding_window = config.sliding_window\n \n+    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -471,7 +469,6 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        last_cache_position: int = 0,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n@@ -490,11 +487,16 @@ def forward(\n                 )\n                 attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n                 # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n-                # `last_cache_position` is equivalent to `cache_position[-1]` but without breaking dynamo\n-                offset = last_cache_position - effective_seq_len\n+                offset = cache_position[-1] - effective_seq_len + 1\n                 # Should only be used when beyond the sliding window (i.e. offset > 0)\n                 offset = max(0, offset)\n-                attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]\n+                # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n+                # but without data-dependent slicing (i.e. torch.compile friendly)\n+                mask_indexes = torch.arange(\n+                    min(effective_seq_len, attention_mask.shape[-1]), device=attention_mask.device\n+                )\n+                mask_indexes += offset\n+                attention_mask = attention_mask[:, :, :, mask_indexes]\n \n         residual = hidden_states\n \n@@ -581,6 +583,9 @@ def __init__(self, config: Gemma3TextConfig):\n         config.rope_scaling = {\"rope_type\": \"default\"}\n         self.rotary_emb_local = Gemma3RotaryEmbedding(config=config)\n \n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(GEMMA3_INPUTS_DOCSTRING)\n+    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -592,7 +597,6 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -633,16 +637,6 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        # This is needed to correctly slice the mask without data-dependent slicing later on if using dynamo tracing\n-        # (retrieving the same value from `cache_position` later on would crash dynamo)\n-        if last_cache_position is None:\n-            last_cache_position = 0\n-            if attention_mask is not None:\n-                # In case a 4d mask is passed directly without using `generate`, we have to rely on cache_position\n-                # It will break dynamo tracing but there are no way around it (and it should never happen in practice)\n-                last_cache_position = (\n-                    attention_mask.shape[-1] if attention_mask.dim() == 2 else cache_position[-1].item()\n-                )\n         causal_mask = self._update_causal_mask(\n             attention_mask,\n             inputs_embeds,\n@@ -678,7 +672,6 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n-                    last_cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -691,7 +684,6 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n-                    last_cache_position=last_cache_position,\n                     **flash_attn_kwargs,\n                 )\n "
        },
        {
            "sha": "fa8bd274cce2c33d1b265b13184eb7c86d288fde",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5ac23081ec4021818a21d7442d396f31de8c30c/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5ac23081ec4021818a21d7442d396f31de8c30c/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=e5ac23081ec4021818a21d7442d396f31de8c30c",
            "patch": "@@ -2075,9 +2075,6 @@ def test_generate_compile_model_forward(self):\n         Tests that `.generate` is compatible with torch.compile without graph breaks, keeping the same results.\n         ⚠️ Runs two sequential generations to ensure the cache doesn't get stuck after the first compiled run! ⚠️\n         \"\"\"\n-        # Monkey-patching the HybridCache at test-time to continue testing compilation support\n-        HybridCache.is_compileable = True\n-\n         for model_class in self.all_generative_model_classes:\n             if not model_class._supports_static_cache:\n                 self.skipTest(\"This model doesn't support static cache (= no expectations of compilation support)\")\n@@ -2174,9 +2171,6 @@ def test_generate_compilation_all_outputs(self):\n         Tests that all optional outputs are behaving as expected when compilation is triggered.\n         In essence, it's the same as `test_greedy_generate_dict_outputs`, but with automatic compilation triggered.\n         \"\"\"\n-        # Monkey-patching the HybridCache at test-time to continue testing compilation support\n-        HybridCache.is_compileable = True\n-\n         for model_class in self.all_generative_model_classes:\n             if not model_class._supports_static_cache:\n                 self.skipTest(\"This model doesn't support static cache (= no expectations of compilation support)\")"
        },
        {
            "sha": "d1ba0cbec4e6726f857fde8492388c03d35b42f8",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5ac23081ec4021818a21d7442d396f31de8c30c/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5ac23081ec4021818a21d7442d396f31de8c30c/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=e5ac23081ec4021818a21d7442d396f31de8c30c",
            "patch": "@@ -154,6 +154,10 @@ def test_sdpa_equivalence(self):\n     def test_multi_gpu_data_parallel_forward(self):\n         pass\n \n+    @unittest.skip(\"Gemma2 has HybridCache which auto-compiles. Compile and FA2 don't work together.\")\n+    def test_eager_matches_fa2_generate(self):\n+        pass\n+\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "be83749cf8bc7a1aaf4563d501dcf705ae94f301",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5ac23081ec4021818a21d7442d396f31de8c30c/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5ac23081ec4021818a21d7442d396f31de8c30c/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=e5ac23081ec4021818a21d7442d396f31de8c30c",
            "patch": "@@ -329,6 +329,10 @@ def test_generate_with_static_cache(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n+    @unittest.skip(\"Gemma3 has HybridCache which auto-compiles. Compile and FA2 don't work together.\")\n+    def test_eager_matches_fa2_generate(self):\n+        pass\n+\n     @unittest.skip(\n         reason=\"Siglip (vision backbone) uses the same initialization scheme as the Flax original implementation\"\n     )"
        }
    ],
    "stats": {
        "total": 238,
        "additions": 90,
        "deletions": 148
    }
}