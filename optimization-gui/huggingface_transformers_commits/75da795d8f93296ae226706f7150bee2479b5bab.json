{
    "author": "Cyrilvallez",
    "message": "ðŸš¨ Remove torch.fx support (#41683)\n\n* remove all\n\n* fix comments\n\n* better checks\n\n* doc",
    "sha": "75da795d8f93296ae226706f7150bee2479b5bab",
    "files": [
        {
            "sha": "862171fecec4c2fe2a3754f11967dd86f07e58a0",
            "filename": "docs/source/ja/perf_train_gpu_many.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/docs%2Fsource%2Fja%2Fperf_train_gpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/docs%2Fsource%2Fja%2Fperf_train_gpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_train_gpu_many.md?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -472,8 +472,6 @@ FlexFlowã¯ã€ã‚µãƒ³ãƒ—ãƒ«-ã‚ªãƒšãƒ¬ãƒ¼ã‚¿-å±žæ€§-ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®4Dä¸¦åˆ—åŒ–\n \n ã—ãŸãŒã£ã¦ã€ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ç´„æŸã¯éžå¸¸ã«é­…åŠ›çš„ã§ã™ã€‚é¸æŠžã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ã§30åˆ†é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã€ã“ã®ç‰¹å®šã®ç’°å¢ƒã‚’æœ€é©ã«åˆ©ç”¨ã™ã‚‹ãŸã‚ã®æœ€è‰¯ã®æˆ¦ç•¥ã‚’æä¾›ã—ã¾ã™ã€‚éƒ¨åˆ†ã‚’è¿½åŠ /å‰Šé™¤/ç½®æ›ã™ã‚‹ã¨ã€ãã‚Œã«å¯¾ã—ã¦å®Ÿè¡Œã—ã¦å†æœ€é©åŒ–ãƒ—ãƒ©ãƒ³ã‚’ä½œæˆã—ã¾ã™ã€‚ãã®å¾Œã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã¾ã™ã€‚ç•°ãªã‚‹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«ã¯ç‹¬è‡ªã®æœ€é©åŒ–ãŒã‚ã‚Šã¾ã™ã€‚\n \n-ðŸ¤— Transformersã®ç¾åœ¨ã®çŠ¶æ³: ã¾ã çµ±åˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã™ã§ã«[transformers.utils.fx](https://github.com/huggingface/transformers/blob/master/src/transformers/utils/fx.py)ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ãŒFXãƒˆãƒ¬ãƒ¼ã‚¹å¯èƒ½ã§ã‚ã‚‹ãŸã‚ã€FlexFlowã‚’å‹•ä½œã•ã›ã‚‹ãŸã‚ã«å¿…è¦ãªæ‰‹é †ã‚’èª°ã‹ãŒè¦‹ã¤ã‘ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n-\n ## Which Strategy To Use When\n \n ã“ã“ã§ã¯ã€ã©ã®ä¸¦åˆ—åŒ–æˆ¦ç•¥ã‚’ã„ã¤ä½¿ç”¨ã™ã‚‹ã‹ã®éžå¸¸ã«ãŠãŠã¾ã‹ãªã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã‚’ç¤ºã—ã¾ã™ã€‚å„ãƒªã‚¹ãƒˆã®æœ€åˆãŒé€šå¸¸ã‚ˆã‚Šã‚‚é€Ÿã„ã“ã¨ãŒä¸€èˆ¬çš„ã§ã™ã€‚"
        },
        {
            "sha": "6f01eb22a344b65cbef166ed242bf6ffc339bf53",
            "filename": "docs/source/ko/perf_train_gpu_many.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/docs%2Fsource%2Fko%2Fperf_train_gpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/docs%2Fsource%2Fko%2Fperf_train_gpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperf_train_gpu_many.md?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -476,8 +476,6 @@ https://huggingface.co/papers/2201.11990)\n \n ë”°ë¼ì„œ ì´ í”„ë ˆìž„ì›Œí¬ì˜ ìž¥ì ì€ ì„ íƒí•œ í´ëŸ¬ìŠ¤í„°ì—ì„œ 30ë¶„ ë™ì•ˆ ì‹œë®¬ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ê³  ì´ íŠ¹ì • í™˜ê²½ì„ ìµœì ìœ¼ë¡œ í™œìš©í•˜ê¸° ìœ„í•œ ìµœìƒì˜ ì „ëžµì„ ì œì•ˆí•œë‹¤ëŠ” ê²ƒìž…ë‹ˆë‹¤. ë¶€í’ˆì„ ì¶”ê°€/ì œê±°/êµì²´í•˜ë©´ ì‹¤í–‰í•˜ê³  ê·¸ì— ëŒ€í•œ ê³„íšì„ ë‹¤ì‹œ ìµœì í™”í•œ í›„ í›ˆë ¨í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ì •ì€ ìžì²´ì ì¸ ì‚¬ìš©ìž ì •ì˜ ìµœì í™”ë¥¼ ê°€ì§ˆ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n \n-ðŸ¤— Transformers í˜„í™©: ì•„ì§ í†µí•©ë˜ì§€ ì•ŠìŒ. ì´ë¯¸ [transformers.utils.fx](https://github.com/huggingface/transformers/blob/master/src/transformers/utils/fx.py)ë¥¼ í†µí•´ ëª¨ë¸ì„ FX-ì¶”ì í•  ìˆ˜ ìžˆìœ¼ë©°, ì´ëŠ” FlexFlowì˜ ì„ í–‰ ì¡°ê±´ìž…ë‹ˆë‹¤. ë”°ë¼ì„œ ì–´ë–¤ ìž‘ì—…ì„ ìˆ˜í–‰í•´ì•¼ FlexFlowê°€ ìš°ë¦¬ì˜ ëª¨ë¸ê³¼ í•¨ê»˜ ìž‘ë™í•  ìˆ˜ ìžˆëŠ”ì§€ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤.\n-\n \n ## ì–´ë–¤ ì „ëžµì„ ì‚¬ìš©í•´ì•¼ í• ê¹Œìš”? [[which-strategy-to-use-when]]\n "
        },
        {
            "sha": "8f0a8a04fd600aa7a49648401d1d37817a978920",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -23,7 +23,7 @@\n from .configuration_utils import PreTrainedConfig\n from .utils import is_torch_xpu_available, logging\n from .utils.generic import GeneralInterface\n-from .utils.import_utils import is_torch_flex_attn_available, is_torch_greater_or_equal, is_torchdynamo_compiling\n+from .utils.import_utils import is_torch_flex_attn_available, is_torch_greater_or_equal, is_tracing\n \n \n if is_torch_flex_attn_available():\n@@ -239,7 +239,6 @@ def _ignore_causal_mask_sdpa(\n     allowing to dispatch to the flash attention kernel (that can otherwise not be used if a custom `attn_mask` is\n     passed).\n     \"\"\"\n-    is_tracing = torch.jit.is_tracing() or isinstance(padding_mask, torch.fx.Proxy) or is_torchdynamo_compiling()\n     if padding_mask is not None and padding_mask.shape[-1] > kv_length:\n         mask_indices = torch.arange(kv_length, device=padding_mask.device)\n         mask_indices += kv_offset\n@@ -250,7 +249,7 @@ def _ignore_causal_mask_sdpa(\n     # which is in general wrong (see https://github.com/pytorch/pytorch/issues/108108). Thus, we only set\n     # `ignore_causal_mask = True` if we are not tracing\n     if (\n-        not is_tracing\n+        not is_tracing(padding_mask)\n         # only cases when lower and upper diags are the same, see https://github.com/pytorch/pytorch/issues/108108\n         and (query_length == 1 or (kv_length == query_length or _is_torch_xpu_available))\n         # in this case we need to add special patterns to the mask so cannot be skipped otherwise\n@@ -275,11 +274,9 @@ def _ignore_bidirectional_mask_sdpa(padding_mask: Optional[torch.Tensor]) -> boo\n     Detects whether the bidirectional mask can be ignored in case PyTorch's SDPA is used, i.e. when there is full\n     attention with no padding.\n     \"\"\"\n-    is_tracing = torch.jit.is_tracing() or isinstance(padding_mask, torch.fx.Proxy) or is_torchdynamo_compiling()\n-\n     # When using `torch.export` or `torch.onnx.dynamo_export`, we need to avoid to check the contents of the mask;\n     # otherwise, we will encounter dynamic control flows\n-    if not is_tracing and (padding_mask is None or padding_mask.all()):\n+    if not is_tracing(padding_mask) and (padding_mask is None or padding_mask.all()):\n         return True\n \n     return False"
        },
        {
            "sha": "c2b92c2e5eed086a39322016ed4f4824aca03e8a",
            "filename": "src/transformers/modeling_attn_mask_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodeling_attn_mask_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodeling_attn_mask_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_attn_mask_utils.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -22,7 +22,7 @@\n \n import torch\n \n-from .utils.import_utils import is_torchdynamo_compiling\n+from .utils.import_utils import is_torchdynamo_compiling, is_tracing\n \n \n @dataclass\n@@ -267,7 +267,7 @@ def _ignore_causal_mask_sdpa(\n         _, query_length = inputs_embeds.shape[0], inputs_embeds.shape[1]\n         key_value_length = query_length + past_key_values_length\n \n-        is_tracing = torch.jit.is_tracing() or isinstance(inputs_embeds, torch.fx.Proxy) or is_torchdynamo_compiling()\n+        is_tracing_ = is_tracing(inputs_embeds)\n \n         ignore_causal_mask = False\n \n@@ -283,15 +283,15 @@ def _ignore_causal_mask_sdpa(\n             # Besides, jit.trace can not handle the `q_len > 1` condition for `is_causal`\n             # (\"TypeError: scaled_dot_product_attention(): argument 'is_causal' must be bool, not Tensor\").\n             if (\n-                (is_training or not is_tracing)\n+                (is_training or not is_tracing_)\n                 and (query_length == 1 or key_value_length == query_length)\n                 and (sliding_window is None or key_value_length < sliding_window)\n             ):\n                 ignore_causal_mask = True\n         elif sliding_window is None or key_value_length < sliding_window:\n             if len(attention_mask.shape) == 4:\n                 return False\n-            elif not is_tracing and torch.all(attention_mask == 1):\n+            elif not is_tracing_ and torch.all(attention_mask == 1):\n                 if query_length == 1 or key_value_length == query_length:\n                     # For query_length == 1, causal attention and bi-directional attention are the same.\n                     ignore_causal_mask = True\n@@ -379,7 +379,7 @@ def _prepare_4d_causal_attention_mask_for_sdpa(\n     # torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture the controlflow `is_causal=attention_mask is None and q_len > 1`\n     # used as an SDPA argument. We keep compatibility with these tracing tools by always using SDPA's `attn_mask` argument in case we are tracing.\n     # TODO: For dynamo, rather use a check on fullgraph=True once this is possible (https://github.com/pytorch/pytorch/pull/120400).\n-    is_tracing = torch.jit.is_tracing() or isinstance(inputs_embeds, torch.fx.Proxy) or is_torchdynamo_compiling()\n+    is_tracing_ = is_tracing(inputs_embeds)\n \n     ignore_causal_mask = AttentionMaskConverter._ignore_causal_mask_sdpa(\n         attention_mask=attention_mask,\n@@ -408,7 +408,7 @@ def _prepare_4d_causal_attention_mask_for_sdpa(\n         # Attend to all tokens in masked rows from the causal_mask, for example the relevant first rows when\n         # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n         # Details: https://github.com/pytorch/pytorch/issues/110213\n-        if not is_tracing and expanded_4d_mask.device.type == \"cuda\":\n+        if not is_tracing_ and expanded_4d_mask.device.type == \"cuda\":\n             expanded_4d_mask = AttentionMaskConverter._unmask_unattended(\n                 expanded_4d_mask, min_dtype=torch.finfo(inputs_embeds.dtype).min\n             )\n@@ -448,10 +448,8 @@ def _prepare_4d_attention_mask_for_sdpa(mask: torch.Tensor, dtype: torch.dtype,\n     _, key_value_length = mask.shape\n     tgt_len = tgt_len if tgt_len is not None else key_value_length\n \n-    is_tracing = torch.jit.is_tracing() or isinstance(mask, torch.fx.Proxy) or is_torchdynamo_compiling()\n-\n     # torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture data-dependent controlflows.\n-    if not is_tracing and torch.all(mask == 1):\n+    if not is_tracing(mask) and torch.all(mask == 1):\n         return None\n     else:\n         return AttentionMaskConverter._expand_mask(mask=mask, dtype=dtype, tgt_len=tgt_len)"
        },
        {
            "sha": "4e9db43e1b66d8c4396e12cab41ada7c0707a023",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -120,8 +120,7 @@\n     ENV_VARS_TRUE_VALUES,\n     is_huggingface_hub_greater_or_equal,\n     is_sagemaker_mp_enabled,\n-    is_torch_fx_proxy,\n-    is_torchdynamo_compiling,\n+    is_tracing,\n )\n from .utils.quantization_config import QuantizationMethod\n \n@@ -4946,7 +4945,7 @@ def warn_if_padding_and_no_attention_mask(self, input_ids, attention_mask):\n         \"\"\"\n \n         # Skip the check during tracing.\n-        if is_torch_fx_proxy(input_ids) or torch.jit.is_tracing() or is_torchdynamo_compiling():\n+        if is_tracing(input_ids):\n             return\n \n         if (attention_mask is not None) or (self.config.pad_token_id is None):"
        },
        {
            "sha": "a0aa6c8b5c17607341376920d8a1eb601d72bc8d",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -22,7 +22,7 @@\n from ....cache_utils import Cache\n from ....modeling_outputs import MoECausalLMOutputWithPast, MoEModelOutputWithPastAndCrossAttentions\n from ....modeling_utils import PreTrainedModel\n-from ....utils import DUMMY_INPUTS, DUMMY_MASK, auto_docstring, is_torch_fx_proxy\n+from ....utils import DUMMY_INPUTS, DUMMY_MASK, auto_docstring\n from .configuration_gptsan_japanese import GPTSanJapaneseConfig\n \n \n@@ -593,15 +593,9 @@ def _shift_right(self, input_ids):\n                 \"See T5 docs for more information.\"\n             )\n \n-        # shift inputs to the right\n-        if is_torch_fx_proxy(input_ids):\n-            # Item assignment is not supported natively for proxies.\n-            shifted_input_ids = torch.full(input_ids.shape[:-1] + (1,), decoder_start_token_id)\n-            shifted_input_ids = torch.cat([shifted_input_ids, input_ids[..., :-1]], dim=-1)\n-        else:\n-            shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n-            shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n-            shifted_input_ids[..., 0] = decoder_start_token_id\n+        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n+        shifted_input_ids[..., 0] = decoder_start_token_id\n \n         if pad_token_id is None:\n             raise ValueError(\"self.model.config.pad_token_id has to be defined.\")"
        },
        {
            "sha": "d758b0529d8610dc3306c10deb20256e4ff69045",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -23,7 +23,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -54,11 +54,6 @@\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n-# This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph.\n-# It means that the function will not be traced through and simply appear as a node in the graph.\n-_prepare_4d_causal_attention_mask = torch.fx.wrap(_prepare_4d_causal_attention_mask)\n-\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "24d3322ad658513ded0d6c19d4eaba572abdf515",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 3,
            "deletions": 20,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -17,7 +17,6 @@\n from typing import Optional, Union\n \n import torch\n-import torch.fx\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n@@ -34,12 +33,7 @@\n     SequenceClassifierOutputWithPast,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    auto_docstring,\n-    is_torch_flex_attn_available,\n-    is_torch_fx_proxy,\n-    logging,\n-)\n+from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n from .configuration_gptj import GPTJConfig\n \n \n@@ -62,7 +56,6 @@ def create_sinusoidal_positions(num_pos: int, dim: int) -> torch.Tensor:\n     return torch.cat((torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)), dim=1)\n \n \n-@torch.fx.wrap\n def get_embed_positions(embed_positions, position_ids):\n     return embed_positions.to(position_ids.device).repeat(position_ids.shape[0], 1, 1)\n \n@@ -198,12 +191,7 @@ def forward(\n         key = self._split_heads(key, self.num_attention_heads, self.head_dim, True)\n         value = self._split_heads(value, self.num_attention_heads, self.head_dim, False)\n \n-        if is_torch_fx_proxy(position_ids) or torch.jit.is_tracing():\n-            # The logic to conditionally copy to GPU could not be traced, so we do this\n-            # every time in the torch.fx case\n-            embed_positions = get_embed_positions(self.embed_positions, position_ids)\n-        else:\n-            embed_positions = self._get_embed_positions(position_ids)\n+        embed_positions = self._get_embed_positions(position_ids)\n \n         repeated_position_ids = position_ids.unsqueeze(-1).repeat(1, 1, embed_positions.shape[-1])\n         sincos = torch.gather(embed_positions, 1, repeated_position_ids).to(key.dtype)\n@@ -283,12 +271,7 @@ def forward(\n         key = self._split_heads(key, self.num_attention_heads, self.head_dim, True)\n         value = self._split_heads(value, self.num_attention_heads, self.head_dim, False)\n \n-        if is_torch_fx_proxy(position_ids) or torch.jit.is_tracing():\n-            # The logic to conditionally copy to GPU could not be traced, so we do this\n-            # every time in the torch.fx case\n-            embed_positions = get_embed_positions(self.embed_positions, position_ids)\n-        else:\n-            embed_positions = self._get_embed_positions(position_ids)\n+        embed_positions = self._get_embed_positions(position_ids)\n \n         repeated_position_ids = position_ids.unsqueeze(-1).repeat(1, 1, embed_positions.shape[-1])\n         sincos = torch.gather(embed_positions, 1, repeated_position_ids).to(key.dtype)"
        },
        {
            "sha": "af245b86220bd83186bc094d37e497c953458bea",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -1041,7 +1041,6 @@ def apply_fusion_head(self, head: nn.Module, hidden_states: torch.Tensor) -> tor\n         if isinstance(head, nn.Identity):\n             return hidden_states\n \n-        # Doing explicit to avoid problems with torch.fx\n         batch_size, num_mask_units, mask_unit_height, mask_unit_width, hidden_size = hidden_states.shape\n         # From: [batch_size, num_mask_units, mask_unit_height, mask_unit_width, hidden_size]\n         # To: head([batch_size * num_mask_units, hidden_size, mask_unit_height, mask_unit_width])"
        },
        {
            "sha": "c347f905d5da7f28db8d131325f2f339d36da184",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -39,7 +39,6 @@\n     DUMMY_MASK,\n     auto_docstring,\n     is_torch_flex_attn_available,\n-    is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n )\n@@ -1259,15 +1258,9 @@ def _shift_right(self, input_ids):\n                 \"See LongT5 docs for more information.\"\n             )\n \n-        # shift inputs to the right\n-        if is_torch_fx_proxy(input_ids):\n-            # Item assignment is not supported natively for proxies.\n-            shifted_input_ids = torch.full(input_ids.shape[:-1] + (1,), decoder_start_token_id)\n-            shifted_input_ids = torch.cat([shifted_input_ids, input_ids[..., :-1]], dim=-1)\n-        else:\n-            shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n-            shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n-            shifted_input_ids[..., 0] = decoder_start_token_id\n+        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n+        shifted_input_ids[..., 0] = decoder_start_token_id\n \n         if pad_token_id is None:\n             raise ValueError(\"self.model.config.pad_token_id has to be defined.\")"
        },
        {
            "sha": "d1268c6094461b409f53c764689b5daa21f052bd",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -42,7 +42,6 @@\n     DUMMY_MASK,\n     auto_docstring,\n     is_torch_flex_attn_available,\n-    is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n )\n@@ -631,15 +630,9 @@ def _shift_right(self, input_ids):\n                 \"See MT5 docs for more information.\"\n             )\n \n-        # shift inputs to the right\n-        if is_torch_fx_proxy(input_ids):\n-            # Item assignment is not supported natively for proxies.\n-            shifted_input_ids = torch.full(input_ids.shape[:-1] + (1,), decoder_start_token_id)\n-            shifted_input_ids = torch.cat([shifted_input_ids, input_ids[..., :-1]], dim=-1)\n-        else:\n-            shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n-            shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n-            shifted_input_ids[..., 0] = decoder_start_token_id\n+        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n+        shifted_input_ids[..., 0] = decoder_start_token_id\n \n         if pad_token_id is None:\n             raise ValueError(\"self.model.config.pad_token_id has to be defined.\")"
        },
        {
            "sha": "2d2ae7b7c16b024693cfe678cd6d22049fe266a6",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -38,7 +38,6 @@\n     DUMMY_MASK,\n     auto_docstring,\n     is_torch_flex_attn_available,\n-    is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n )\n@@ -440,15 +439,9 @@ def _shift_right(self, input_ids):\n                 \"See Pix2Struct docs for more information.\"\n             )\n \n-        # shift inputs to the right\n-        if is_torch_fx_proxy(input_ids):\n-            # Item assignment is not supported natively for proxies.\n-            shifted_input_ids = torch.full(input_ids.shape[:-1] + (1,), decoder_start_token_id)\n-            shifted_input_ids = torch.cat([shifted_input_ids, input_ids[..., :-1]], dim=-1)\n-        else:\n-            shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n-            shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n-            shifted_input_ids[..., 0] = decoder_start_token_id\n+        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n+        shifted_input_ids[..., 0] = decoder_start_token_id\n \n         if pad_token_id is None:\n             raise ValueError(\"self.model.config.pad_token_id has to be defined.\")"
        },
        {
            "sha": "8386e7c47727e9ff0367f2bcf1378fc9dcc5466f",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -31,7 +31,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPastAndCrossAttentions, Seq2SeqLMOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, is_torch_flex_attn_available, is_torch_fx_proxy, is_torchdynamo_compiling, logging\n+from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n from .configuration_pop2piano import Pop2PianoConfig\n \n \n@@ -593,15 +593,9 @@ def _shift_right(self, input_ids):\n                 \"self.model.config.decoder_start_token_id has to be defined. In Pop2Piano it is usually set to the pad_token_id.\"\n             )\n \n-        # shift inputs to the right\n-        if is_torch_fx_proxy(input_ids):\n-            # Item assignment is not supported natively for proxies.\n-            shifted_input_ids = torch.full(input_ids.shape[:-1] + (1,), decoder_start_token_id)\n-            shifted_input_ids = torch.cat([shifted_input_ids, input_ids[..., :-1]], dim=-1)\n-        else:\n-            shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n-            shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n-            shifted_input_ids[..., 0] = decoder_start_token_id\n+        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n+        shifted_input_ids[..., 0] = decoder_start_token_id\n \n         if pad_token_id is None:\n             raise ValueError(\"self.model.config.pad_token_id has to be defined.\")"
        },
        {
            "sha": "6abf3a0599caf6dd1be6c254c86b3d40e3fd2d88",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -30,7 +30,7 @@\n from ...modeling_rope_utils import dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n-from ...utils.import_utils import is_torchdynamo_compiling\n+from ...utils.import_utils import is_tracing\n from .configuration_recurrent_gemma import RecurrentGemmaConfig\n \n \n@@ -362,8 +362,7 @@ def forward(\n         # Apply gamma normalization to the input. We need to clip the derivatives of\n         # `sqrt` in order to prevent NaNs during training in bfloat16. TODO a bit annoying\n         multiplier = 1\n-        tracing = isinstance(activations, torch.fx.Proxy) or is_torchdynamo_compiling()\n-        if not torch.jit.is_tracing() and not tracing:\n+        if not is_tracing(activations):\n             multiplier = SqrtBoundDerivative.apply(1 - a_square)\n         multiplier = reset + ~reset * multiplier\n         normalized_x = gated_inputs * multiplier.type(activations.dtype)"
        },
        {
            "sha": "29f5e9c2c99a0b5f1dbd6210c3bb9c78f70fb60e",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -44,7 +44,6 @@\n     TransformersKwargs,\n     auto_docstring,\n     is_torch_flex_attn_available,\n-    is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n )\n@@ -636,15 +635,9 @@ def _shift_right(self, input_ids):\n                 \" to the pad_token_id. See SwitchTransformers docs for more information\"\n             )\n \n-        # shift inputs to the right\n-        if is_torch_fx_proxy(input_ids):\n-            # Item assignment is not supported natively for proxies.\n-            shifted_input_ids = torch.full(input_ids.shape[:-1] + (1,), decoder_start_token_id)\n-            shifted_input_ids = torch.cat([shifted_input_ids, input_ids[..., :-1]], dim=-1)\n-        else:\n-            shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n-            shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n-            shifted_input_ids[..., 0] = decoder_start_token_id\n+        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n+        shifted_input_ids[..., 0] = decoder_start_token_id\n \n         if pad_token_id is None:\n             raise ValueError(\"self.model.config.pad_token_id has to be defined.\")"
        },
        {
            "sha": "274dc6ca44b722b5fa073bdbfe56c19096aa5463",
            "filename": "src/transformers/models/switch_transformers/modular_switch_transformers.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -37,7 +37,6 @@\n     TransformersKwargs,\n     auto_docstring,\n     is_torch_flex_attn_available,\n-    is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n )\n@@ -392,15 +391,9 @@ def _shift_right(self, input_ids):\n                 \" to the pad_token_id. See SwitchTransformers docs for more information\"\n             )\n \n-        # shift inputs to the right\n-        if is_torch_fx_proxy(input_ids):\n-            # Item assignment is not supported natively for proxies.\n-            shifted_input_ids = torch.full(input_ids.shape[:-1] + (1,), decoder_start_token_id)\n-            shifted_input_ids = torch.cat([shifted_input_ids, input_ids[..., :-1]], dim=-1)\n-        else:\n-            shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n-            shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n-            shifted_input_ids[..., 0] = decoder_start_token_id\n+        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n+        shifted_input_ids[..., 0] = decoder_start_token_id\n \n         if pad_token_id is None:\n             raise ValueError(\"self.model.config.pad_token_id has to be defined.\")"
        },
        {
            "sha": "2494964d296b2df3240da99815c56f3ecf4da622",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -42,7 +42,6 @@\n     DUMMY_MASK,\n     auto_docstring,\n     is_torch_flex_attn_available,\n-    is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n )\n@@ -636,15 +635,9 @@ def _shift_right(self, input_ids):\n                 \"See T5 docs for more information.\"\n             )\n \n-        # shift inputs to the right\n-        if is_torch_fx_proxy(input_ids):\n-            # Item assignment is not supported natively for proxies.\n-            shifted_input_ids = torch.full(input_ids.shape[:-1] + (1,), decoder_start_token_id)\n-            shifted_input_ids = torch.cat([shifted_input_ids, input_ids[..., :-1]], dim=-1)\n-        else:\n-            shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n-            shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n-            shifted_input_ids[..., 0] = decoder_start_token_id\n+        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n+        shifted_input_ids[..., 0] = decoder_start_token_id\n \n         if pad_token_id is None:\n             raise ValueError(\"self.model.config.pad_token_id has to be defined.\")"
        },
        {
            "sha": "a1873b99f5cd5fc1241ad2a0626388a0d7ff1e8a",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -42,7 +42,6 @@\n     DUMMY_MASK,\n     auto_docstring,\n     is_torch_flex_attn_available,\n-    is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n )\n@@ -579,15 +578,9 @@ def _shift_right(self, input_ids):\n                 \"See UMT5 docs for more information.\"\n             )\n \n-        # shift inputs to the right\n-        if is_torch_fx_proxy(input_ids):\n-            # Item assignment is not supported natively for proxies.\n-            shifted_input_ids = torch.full(input_ids.shape[:-1] + (1,), decoder_start_token_id)\n-            shifted_input_ids = torch.cat([shifted_input_ids, input_ids[..., :-1]], dim=-1)\n-        else:\n-            shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n-            shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n-            shifted_input_ids[..., 0] = decoder_start_token_id\n+        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n+        shifted_input_ids[..., 0] = decoder_start_token_id\n \n         if pad_token_id is None:\n             raise ValueError(\"self.model.config.pad_token_id has to be defined.\")"
        },
        {
            "sha": "c5a59fe8b3d9ff691da2a58690c4ec196d105879",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -92,7 +92,6 @@ def forward(self, position_ids: Optional[torch.Tensor] = None, past_key_values_l\n         bsz, seq_len = position_ids.size()\n         position_ids += self.offset\n \n-        # Expand embeddings if needed. `position_ids.max()` is NOT used to keep torch.fx compatibility.\n         max_pos = 2 + seq_len + past_key_values_length\n         if max_pos > self.weights.size(0):\n             self.make_weights(max_pos, self.embedding_dim, self.padding_idx)"
        },
        {
            "sha": "ad98aa9e21898403868db954a807f0c092c382c1",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -242,6 +242,7 @@\n     is_torchdynamo_exporting,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n+    is_tracing,\n     is_training_run_on_sagemaker,\n     is_triton_available,\n     is_uroman_available,"
        },
        {
            "sha": "011930f72f4802aa0e6e70eed8f97436bb54710b",
            "filename": "src/transformers/utils/fx.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1503,
            "changes": 1503,
            "blob_url": "https://github.com/huggingface/transformers/blob/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Futils%2Ffx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/080d704af11d8e0cff7fc1156166b7912b198d36/src%2Ftransformers%2Futils%2Ffx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ffx.py?ref=080d704af11d8e0cff7fc1156166b7912b198d36",
            "patch": "@@ -1,1503 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import builtins\n-import collections\n-import contextlib\n-import functools\n-import inspect\n-import math\n-import operator\n-import os\n-import random\n-import sys\n-import warnings\n-from collections.abc import Callable\n-from typing import Any, Literal\n-\n-import torch\n-import torch.utils._pytree as pytree\n-from torch import nn\n-from torch.fx import Graph, GraphModule, Node, Proxy, Tracer\n-from torch.fx._compatibility import compatibility\n-from torch.fx._symbolic_trace import is_fx_tracing\n-from torch.fx.proxy import ParameterProxy\n-\n-from .. import logging\n-from ..cache_utils import Cache, DynamicCache, StaticCache\n-from ..modeling_utils import PreTrainedConfig, PreTrainedModel\n-from ..models.auto import get_values\n-from ..models.auto.modeling_auto import (\n-    MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES,\n-    MODEL_FOR_BACKBONE_MAPPING_NAMES,\n-    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n-    MODEL_FOR_CTC_MAPPING_NAMES,\n-    MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES,\n-    MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES,\n-    MODEL_FOR_IMAGE_MAPPING_NAMES,\n-    MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING_NAMES,\n-    MODEL_FOR_MASKED_LM_MAPPING_NAMES,\n-    MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES,\n-    MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING_NAMES,\n-    MODEL_FOR_PRETRAINING_MAPPING_NAMES,\n-    MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES,\n-    MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES,\n-    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES,\n-    MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES,\n-    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES,\n-    MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES,\n-    MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING_NAMES,\n-    MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING_NAMES,\n-    MODEL_MAPPING_NAMES,\n-)\n-from .import_utils import (\n-    ENV_VARS_TRUE_VALUES,\n-    is_peft_available,\n-)\n-\n-\n-if is_peft_available():\n-    from peft import PeftModel\n-\n-\n-logger = logging.get_logger(__name__)\n-_IS_IN_DEBUG_MODE = os.environ.get(\"FX_DEBUG_MODE\", \"\").upper() in ENV_VARS_TRUE_VALUES\n-\n-\n-def _generate_supported_model_class_names(\n-    model_name: type[PreTrainedConfig],\n-    supported_tasks: str | list[str] | None = None,\n-) -> list[str]:\n-    task_mapping = {\n-        \"default\": MODEL_MAPPING_NAMES,\n-        \"pretraining\": MODEL_FOR_PRETRAINING_MAPPING_NAMES,\n-        \"next-sentence-prediction\": MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING_NAMES,\n-        \"masked-lm\": MODEL_FOR_MASKED_LM_MAPPING_NAMES,\n-        \"causal-lm\": MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n-        \"seq2seq-lm\": MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES,\n-        \"speech-seq2seq\": MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES,\n-        \"multiple-choice\": MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES,\n-        \"document-question-answering\": MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES,\n-        \"question-answering\": MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES,\n-        \"sequence-classification\": MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES,\n-        \"token-classification\": MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES,\n-        \"masked-image-modeling\": MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING_NAMES,\n-        \"image-classification\": MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES,\n-        \"zero-shot-image-classification\": MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING_NAMES,\n-        \"ctc\": MODEL_FOR_CTC_MAPPING_NAMES,\n-        \"audio-classification\": MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES,\n-        \"semantic-segmentation\": MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES,\n-        \"backbone\": MODEL_FOR_BACKBONE_MAPPING_NAMES,\n-        \"image-feature-extraction\": MODEL_FOR_IMAGE_MAPPING_NAMES,\n-    }\n-\n-    if supported_tasks is None:\n-        supported_tasks = task_mapping.keys()\n-    if isinstance(supported_tasks, str):\n-        supported_tasks = [supported_tasks]\n-\n-    model_class_names = []\n-    for task in supported_tasks:\n-        class_name = task_mapping[task].get(model_name, None)\n-        if class_name:\n-            model_class_names.append(class_name)\n-\n-    return model_class_names\n-\n-\n-_REGULAR_SUPPORTED_MODEL_NAMES_AND_TASKS = [\n-    \"altclip\",\n-    \"albert\",\n-    \"bart\",\n-    \"bert\",\n-    \"bitnet\",\n-    \"blenderbot\",\n-    \"blenderbot-small\",\n-    \"bloom\",\n-    \"clip\",\n-    \"convnext\",\n-    \"deberta\",\n-    \"deberta-v2\",\n-    \"dinov2\",\n-    \"dinov3_convnext\",\n-    \"dinov3_vit\",\n-    \"distilbert\",\n-    \"donut-swin\",\n-    \"electra\",\n-    \"gpt2\",\n-    \"gpt_neo\",\n-    \"gptj\",\n-    \"hiera\",\n-    \"hubert\",\n-    \"ijepa\",\n-    \"layoutlm\",\n-    \"llama\",\n-    \"cohere\",\n-    \"lxmert\",\n-    \"m2m_100\",\n-    \"marian\",\n-    \"mbart\",\n-    \"megatron-bert\",\n-    \"ministral\",\n-    \"mistral\",\n-    \"mixtral\",\n-    \"mobilebert\",\n-    \"mt5\",\n-    \"nezha\",\n-    \"opt\",\n-    \"pegasus\",\n-    \"plbart\",\n-    \"qwen2\",\n-    \"qwen2_moe\",\n-    \"qwen3\",\n-    \"qwen3_next\",\n-    \"qwen3_moe\",\n-    \"resnet\",\n-    \"roberta\",\n-    \"segformer\",\n-    \"speech_to_text\",\n-    \"speech_to_text_2\",\n-    \"swin\",\n-    \"t5\",\n-    \"trocr\",\n-    \"vit\",\n-    \"vjepa2\",\n-    \"xglm\",\n-    \"wav2vec2\",\n-    #    \"xlnet\",\n-]\n-\n-_FX_SUPPORTED_MODELS_WITH_KV_CACHE = [\"llama\", \"opt\"]\n-\n-_REGULAR_SUPPORTED_MODELS = []\n-for item in _REGULAR_SUPPORTED_MODEL_NAMES_AND_TASKS:\n-    if isinstance(item, dict):\n-        _REGULAR_SUPPORTED_MODELS.extend(_generate_supported_model_class_names(**item))\n-    else:\n-        _REGULAR_SUPPORTED_MODELS.extend(_generate_supported_model_class_names(item))\n-\n-_SPECIAL_SUPPORTED_MODELS = [\n-    \"CLIPTextModel\",\n-    \"CLIPTextModelWithProjection\",\n-    \"CLIPVisionModel\",\n-    \"CLIPVisionModelWithProjection\",\n-    \"AltCLIPTextModel\",\n-    \"AltCLIPVisionModel\",\n-    \"GitVisionModel\",\n-    \"GPT2DoubleHeadsModel\",\n-    \"Speech2Text2Decoder\",\n-    \"TrOCRDecoder\",\n-    \"PeftModelForCausalLM\",\n-    \"PeftModelForSeq2SeqLM\",\n-    \"VJEPA2ForVideoClassification\",\n-    # TODO: add support for them as it should be quite easy to do so (small blocking issues).\n-    # XLNetForQuestionAnswering,\n-]\n-_SUPPORTED_MODELS = tuple(sorted(set(_REGULAR_SUPPORTED_MODELS + _SPECIAL_SUPPORTED_MODELS)))\n-\n-_CURRENT_TRACER = None\n-\n-\n-def torch_nn_embedding(self, input):\n-    return torch.empty(*input.shape, self.weight.shape[-1], device=\"meta\", dtype=self.weight.dtype)\n-\n-\n-def torch_nn_functional_embedding(\n-    input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False\n-):\n-    return torch.empty(*input.shape, weight.shape[-1], device=\"meta\", dtype=weight.dtype)\n-\n-\n-def torch_nn_layernorm(self, input):\n-    return input\n-\n-\n-def torch_nn_groupnorm(self, input):\n-    return input\n-\n-\n-def torch_nn_linear(self, input):\n-    return torch.empty(input.shape[:-1] + (self.out_features,), device=\"meta\")\n-\n-\n-def torch_relu(x):\n-    return x\n-\n-\n-def torch_nn_relu(self, x):\n-    return x\n-\n-\n-def torch_nn_functional_relu(x, inplace=False):\n-    if not inplace:\n-        raise ValueError(\"Don't support in-place functional.relu for MetaTensor analysis\")\n-    return x\n-\n-\n-def torch_where(condition, x, y):\n-    # torch.where returns the broadcasted tensor of condition, x, and y,\n-    # so hack it by using addition\n-    return condition.to(device=\"meta\") + x.to(device=\"meta\") + y.to(device=\"meta\")\n-\n-\n-def torch_abs(input, *, out=None):\n-    if out is not None:\n-        raise ValueError(\"Don't support in-place abs for MetaTensor analysis\")\n-    return input\n-\n-\n-def torch_arange(*args, **kwargs):\n-    n = len(args)\n-    step = 1\n-    if n == 1:\n-        start = 0\n-        end = args[0]\n-    elif n == 2:\n-        start, end = args\n-    else:\n-        start, end, step = args\n-    if isinstance(start, float):\n-        start = int(start)\n-    if isinstance(end, float):\n-        start = int(end)\n-    if isinstance(step, float):\n-        step = int(step)\n-    step = kwargs.get(\"step\", step)\n-    dtype = kwargs.get(\"dtype\")\n-    return torch.empty((end - start) // step, dtype=dtype, device=\"meta\")\n-\n-\n-def torch_full(*args, **kwargs):\n-    args = list(args)\n-    # We set the fill value to 1 as its value is not important as long as it's not a tensor on the `meta` device.\n-    if len(args) > 1:\n-        args[1] = 1\n-    else:\n-        kwargs[\"fill_value\"] = 1\n-    kwargs_without_device = dict(kwargs)\n-    kwargs_without_device.pop(\"device\", None)\n-    return torch.full(*args, **kwargs_without_device, device=\"meta\")\n-\n-\n-def torch_cat(tensors, dim=None, axis=None, *, out=None):\n-    if dim is None and axis is None:\n-        dim = 0\n-    if dim is None and axis is not None:\n-        dim = axis\n-    if dim < 0:\n-        dim = tensors[0].dim() + dim\n-    shapes = [t.shape for t in tensors]\n-    shape = list(shapes[0])\n-    concatenated_dim = sum(shape[dim] for shape in shapes)\n-    final_shape = shape[:dim] + [concatenated_dim] + shape[dim + 1 :]\n-    return torch.empty(final_shape, device=\"meta\")\n-\n-\n-def torch_stack(tensors, dim=None, axis=None, *, out=None):\n-    if dim is None and axis is None:\n-        dim = 0\n-    if dim is None and axis is not None:\n-        dim = axis\n-    if dim < 0:\n-        dim = tensors[0].dim() + 1 + dim\n-    shape = list(tensors[0].shape)\n-    shape.insert(dim, len(tensors))\n-    return torch.empty(shape, device=\"meta\")\n-\n-\n-def torch_add(input, other, *, alpha=1, out=None):\n-    if not isinstance(input, torch.Tensor):\n-        return torch.empty_like(other, device=\"meta\")\n-    if not isinstance(other, torch.Tensor):\n-        return torch.empty_like(input, device=\"meta\")\n-    max_length = max(input.dim(), other.dim())\n-    input_shape = list(input.shape) + [1] * (max_length - input.dim())\n-    other_shape = list(other.shape) + [1] * (max_length - other.dim())\n-    shape = []\n-    for i in range(max_length):\n-        shape.append(max(input_shape[i], other_shape[i]))\n-    return torch.empty(shape, device=\"meta\")\n-\n-\n-def torch_mul(input, other, *, out=None):\n-    return torch_add(input, other, out=out)\n-\n-\n-def torch_tensor_mul(self, other):\n-    return torch_mul(self, other)\n-\n-\n-def torch_matmul(input, other, *, out=None):\n-    d1 = input.dim()\n-    d2 = other.dim()\n-    shape = None\n-    if d1 == 1 and d2 == 1:\n-        shape = None\n-    elif d1 == 2 and d2 == 2:\n-        shape = (input.size(0), other.size(1))\n-    elif d1 == 1 and d2 == 2:\n-        shape = (other.size(1),)\n-    elif d1 == 2 and d1 == 1:\n-        shape = (input.size(0),)\n-    else:\n-        max_length = max(input.dim(), other.dim())\n-        shape1 = list(input.shape)\n-        shape2 = list(other.shape)\n-        if d1 == 1:\n-            shape1 = [1] + shape1\n-        if d2 == 1:\n-            shape2.append(1)\n-        shape1 = [-1] * (max_length - d1) + list(input.shape)\n-        shape2 = [-1] * (max_length - d2) + list(other.shape)\n-        shape = []\n-        for i in range(max_length):\n-            shape.append(max(shape1[i], shape2[i]))\n-        shape[-2] = shape1[-2]\n-        shape[-1] = shape2[-1]\n-        if d1 == 1:\n-            shape.pop(-2)\n-        if d2 == 1:\n-            shape.pop(-1)\n-    if shape is None:\n-        return torch.tensor(0.0, device=\"meta\")\n-    return torch.empty(*shape, device=\"meta\")\n-\n-\n-def torch_bmm(input, mat2, *, out=None):\n-    if out is not None:\n-        raise ValueError(\"Don't support in-place bmm for MetaTensor analysis\")\n-    batch_size, n, m = input.shape\n-    _, _, p = mat2.shape\n-    return torch.empty(batch_size, n, p, device=\"meta\")\n-\n-\n-def torch_baddbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None):\n-    if out is not None:\n-        raise ValueError(\"Don't support in-place baddbmm for MetaTensor analysis\")\n-    return torch_bmm(batch1, batch2)\n-\n-\n-def torch_tensor_baddbmm(self, batch1, batch2, *, beta=1, alpha=1, out=None):\n-    return torch_baddbmm(self, batch1, batch2, beta=beta, alpha=alpha, out=out)\n-\n-\n-def torch_einsum(equation, *operands):\n-    # TODO: infer shape without performing the computation, this might be quite hard.\n-    concrete_operands = (torch.empty_like(operand, device=\"cpu\") for operand in operands)\n-    return torch.einsum(equation, *concrete_operands).to(\"meta\")\n-\n-\n-def torch_tensor_repeat(self, *sizes):\n-    shape = list(self.shape)\n-    for i, x in enumerate(sizes):\n-        shape[i] *= x\n-    return torch.empty(shape, device=\"meta\")\n-\n-\n-def torch_repeat_interleave(*args, dim=None, output_size=None):\n-    num_args = len(args)\n-    if num_args == 1:\n-        shape = [output_size if output_size is not None else args[0].sum()]\n-    else:\n-        shape = list(args[0].shape)\n-        if dim is None:\n-            if num_args > 2:\n-                dim = args[2]\n-            else:\n-                shape = [sum(shape)]\n-                dim = 0\n-        repeats = args[1]\n-        if isinstance(repeats, int) or torch.numel(repeats) == 1:\n-            shape[dim] *= int(repeats)\n-        else:\n-            shape[dim] = output_size if output_size is not None else repeats.sum()\n-    return torch.empty(*shape, device=\"meta\")\n-\n-\n-def torch_index_select(input, dim, index, *, out=None):\n-    shape = list(input.shape)\n-    shape[dim] = len(index)\n-    return torch.empty(*shape, device=\"meta\")\n-\n-\n-def torch_tensor_index_select(self, dim, index):\n-    return torch_index_select(self, dim, index)\n-\n-\n-def torch_gather(input, dim, index, *, sparse_grad=False, out=None):\n-    shape = list(input.shape)\n-    shape[dim] = index.shape[dim]\n-    return torch.empty(*shape, device=\"meta\")\n-\n-\n-def torch_tensor_gather(self, dim, index):\n-    return torch_gather(self, dim, index)\n-\n-\n-def torch_roll(input, shifts, dims=None):\n-    return input\n-\n-\n-def torch_flip(input, dims):\n-    return input\n-\n-\n-def torch_tensor_flip(self, dims):\n-    return self\n-\n-\n-def torch_nn_conv1d(self, input):\n-    l_in = input.shape[-1]\n-    shape = None\n-    padding = self.padding\n-    if padding == \"valid\":\n-        padding = (0, 0)\n-    if padding == \"same\":\n-        shape = list(input.shape)\n-    if shape is None:\n-        shape = list(input.shape)\n-        l_out = math.floor(\n-            (l_in + 2 * padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1) / self.stride[0] + 1\n-        )\n-        shape[-1] = l_out\n-    shape[-2] = self.out_channels\n-    return torch.empty(shape, device=\"meta\")\n-\n-\n-def torch_nn_conv2d(self, input):\n-    h_in, w_in = input.shape[-2:]\n-    shape = None\n-    padding = self.padding\n-    if padding == \"valid\":\n-        padding = (0, 0)\n-    if padding == \"same\":\n-        shape = list(input.shape)\n-    if shape is None:\n-        shape = list(input.shape)\n-        h_out = math.floor(\n-            (h_in + 2 * padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1) / self.stride[0] + 1\n-        )\n-        w_out = math.floor(\n-            (w_in + 2 * padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1) / self.stride[1] + 1\n-        )\n-        shape[-2:] = [h_out, w_out]\n-    shape[-3] = self.out_channels\n-    return torch.empty(shape, device=\"meta\")\n-\n-\n-def torch_squeeze(input, dim=None):\n-    shape = list(input.shape)\n-    if dim is not None:\n-        if dim < 0:\n-            dim = input.dim() + dim\n-        if shape[dim] == 1:\n-            shape.pop(dim)\n-    else:\n-        new_shape = []\n-        for dim_value in shape:\n-            if dim_value == 1:\n-                continue\n-            new_shape.append(dim_value)\n-        shape = new_shape\n-    return torch.empty(shape, device=\"meta\")\n-\n-\n-def torch_tensor_squeeze(self, dim=None):\n-    return torch_squeeze(self, dim)\n-\n-\n-def torch_unsqueeze(input, dim):\n-    shape = list(input.shape)\n-    if dim < 0:\n-        dim = input.dim() + 1 + dim\n-    shape.insert(dim, 1)\n-    return torch.empty(shape, device=\"meta\")\n-\n-\n-def torch_tensor_unsqueeze(self, dim):\n-    return torch_unsqueeze(self, dim)\n-\n-\n-def torch_unique_consecutive(input, **kwargs):\n-    output = torch.unique_consecutive(torch.zeros_like(input, device=\"cpu\"), **kwargs)\n-    if isinstance(output, torch.Tensor):\n-        return output.to(\"meta\")\n-    else:\n-        return tuple(map(output, lambda x: x.to(\"meta\")))\n-\n-\n-def torch_nn_functional_one_hot(tensor, num_classes=-1):\n-    if num_classes < 0:\n-        raise ValueError(\"Don't support automatic num_classes inference for MetaTensor analysis\")\n-    shape = list(tensor.shape) + [num_classes]\n-    return torch.empty(shape, device=\"meta\")\n-\n-\n-def torch_nn_functional_scaled_dot_product_attention(\n-    query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None\n-):\n-    target_length = query.shape[-2]\n-    head_dim = value.shape[-1]\n-    return torch.empty((*query.shape[:-2], target_length, head_dim), device=\"meta\")\n-\n-\n-def torch_nn_mseloss(self, input, target):\n-    if self.reduction == \"none\":\n-        shape = target.shape\n-    else:\n-        shape = (1,)\n-    return torch.empty(shape, device=\"meta\")\n-\n-\n-def torch_nn_crossentropyloss(self, input, target):\n-    if self.reduction == \"none\":\n-        shape = target.shape\n-    else:\n-        shape = (1,)\n-    return torch.empty(shape, device=\"meta\")\n-\n-\n-def torch_nn_bcewithlogitsloss(self, input, target):\n-    if self.reduction == \"none\":\n-        shape = target.shape\n-    else:\n-        shape = (1,)\n-    return torch.empty(shape, device=\"meta\")\n-\n-\n-def operator_getitem(a, b):\n-    def to_concrete(t):\n-        if isinstance(t, torch.Tensor):\n-            concrete = torch.ones_like(t, device=\"cpu\")\n-            if concrete.dtype in [torch.float16, torch.float32, torch.float64, torch.int32]:\n-                concrete = concrete.to(torch.int64)\n-            return concrete\n-        return t\n-\n-    if isinstance(a, torch.Tensor):\n-        # TODO: infer shape without performing the computation.\n-        if isinstance(b, tuple):\n-            b = tuple(map(to_concrete, b))\n-        else:\n-            b = to_concrete(b)\n-        return operator.getitem(torch.empty_like(a, device=\"cpu\"), b).to(\"meta\")\n-    return operator.getitem(a, b)\n-\n-\n-_MANUAL_META_OVERRIDES: dict[Callable, Callable] = {\n-    torch.nn.Embedding: torch_nn_embedding,\n-    torch.nn.functional.embedding: torch_nn_functional_embedding,\n-    torch.nn.LayerNorm: torch_nn_layernorm,\n-    torch.nn.GroupNorm: torch_nn_groupnorm,\n-    torch.nn.Linear: torch_nn_linear,\n-    torch.relu: torch_relu,\n-    torch.nn.functional.relu: torch_nn_functional_relu,\n-    torch.nn.ReLU: torch_nn_relu,\n-    torch.where: torch_where,\n-    torch.abs: torch_abs,\n-    torch.arange: torch_arange,\n-    torch.full: torch_full,\n-    torch.cat: torch_cat,\n-    torch.stack: torch_stack,\n-    torch.add: torch_add,\n-    torch.mul: torch_mul,\n-    torch.Tensor.mul: torch_tensor_mul,\n-    torch.matmul: torch_matmul,\n-    torch.bmm: torch_bmm,\n-    torch.baddbmm: torch_baddbmm,\n-    torch.Tensor.baddbmm: torch_tensor_baddbmm,\n-    torch.einsum: torch_einsum,\n-    torch.Tensor.repeat: torch_tensor_repeat,\n-    torch.repeat_interleave: torch_repeat_interleave,\n-    torch.roll: torch_roll,\n-    torch.flip: torch_flip,\n-    torch.Tensor.flip: torch_tensor_flip,\n-    torch.index_select: torch_index_select,\n-    torch.Tensor.index_select: torch_tensor_index_select,\n-    torch.gather: torch_gather,\n-    torch.Tensor.gather: torch_tensor_gather,\n-    torch.nn.Conv1d: torch_nn_conv1d,\n-    torch.nn.Conv2d: torch_nn_conv2d,\n-    torch.squeeze: torch_squeeze,\n-    torch.Tensor.squeeze: torch_tensor_squeeze,\n-    torch.unsqueeze: torch_unsqueeze,\n-    torch.Tensor.unsqueeze: torch_tensor_unsqueeze,\n-    torch.unique_consecutive: torch_unique_consecutive,\n-    torch.nn.functional.one_hot: torch_nn_functional_one_hot,\n-    torch.nn.MSELoss: torch_nn_mseloss,\n-    torch.nn.CrossEntropyLoss: torch_nn_crossentropyloss,\n-    torch.nn.BCEWithLogitsLoss: torch_nn_bcewithlogitsloss,\n-    operator.getitem: operator_getitem,\n-}\n-\n-_MANUAL_META_OVERRIDES[torch.nn.functional.scaled_dot_product_attention] = (\n-    torch_nn_functional_scaled_dot_product_attention\n-)\n-\n-\n-class HFProxy(Proxy):\n-    \"\"\"\n-    Proxy that uses metadata to handle data-dependent control-flow.\n-    \"\"\"\n-\n-    def install_metadata(self, metadata):\n-        self._metadata = metadata\n-\n-    @property\n-    def shape(self):\n-        return self.tracer.create_proxy(\"call_method\", \"size\", (self,), {})\n-\n-    @property\n-    def device(self):\n-        # Hack so we can track when devices are used. During meta-tensor propagation,\n-        # replace these values with a constant 'meta'\n-        return MetaDeviceAttribute(self, \"device\")\n-\n-    def __len__(self):\n-        if hasattr(self, \"_metadata\") and self._metadata is not None:\n-            return len(self._metadata)\n-        return super().__len__()\n-\n-    def __bool__(self):\n-        if hasattr(self, \"_metadata\") and self._metadata is not None:\n-            return self._metadata\n-        return super().__bool__()\n-\n-    def __getattr__(self, k):\n-        if k == \"_metadata\":\n-            return self.__getattribute__(k)\n-        # note: not added to the graph yet, if this is a method call\n-        # we peephole optimize to the method invocation\n-        return HFAttribute(self, k)\n-\n-    def __setitem__(self, indices, values):\n-        return self.tracer.create_proxy(\"call_function\", operator.setitem, (self, indices, values), {})\n-\n-    def __contains__(self, key):\n-        if hasattr(self, \"_metadata\") and self._metadata is not None:\n-            return key in self._metadata\n-        return super().__contains__(key)\n-\n-\n-class HFAttribute(HFProxy):\n-    def __init__(self, root, attr: str):\n-        self.root = root\n-        self.attr = attr\n-        self.tracer = root.tracer\n-        self._node = None\n-\n-        if hasattr(self.root, \"_metadata\"):\n-            self.install_metadata(getattr(self.root._metadata, attr))\n-\n-    @property\n-    def node(self):\n-        # the node for attributes is added lazily, since most will just be method calls\n-        # which do not rely on the getitem call\n-        if self._node is None:\n-            self._node = self.tracer.create_proxy(\"call_function\", builtins.getattr, (self.root, self.attr), {}).node\n-        return self._node\n-\n-    def __call__(self, *args, **kwargs):\n-        return self.tracer.create_proxy(\"call_method\", self.attr, (self.root,) + args, kwargs)\n-\n-\n-class MetaDeviceAttribute(HFAttribute):\n-    pass\n-\n-\n-class HFCacheProxy(HFProxy):\n-    \"\"\"\n-    Proxy that represents an instance of `transformers.cache_utils.Cache`.\n-    \"\"\"\n-\n-    def install_orig_cache_cls(self, orig_cache_cls: type[Cache]):\n-        self._orig_cache_cls = orig_cache_cls\n-\n-    @property\n-    def __class__(self):\n-        if not hasattr(self, \"_orig_cache_cls\"):\n-            raise RuntimeError(\"The original Cache class must be installed to the HFCacheProxy.\")\n-        return self.tracer._CLASSES_TO_PATCH[self._orig_cache_cls]\n-\n-\n-def create_wrapper(\n-    function: Callable,\n-    op_type: Literal[\"call_function\"] | Literal[\"call_method\"] | Literal[\"get_attr\"],\n-    proxy_factory_fn: Callable[[Node], Proxy] | None = None,\n-) -> Callable:\n-    @functools.wraps(function)\n-    def wrapper(*args, **kwargs):\n-        if not is_fx_tracing():\n-            return function(*args, **kwargs)\n-\n-        found_proxies = []\n-\n-        def check_proxy(a):\n-            if isinstance(a, Proxy):\n-                found_proxies.append(a)\n-\n-        torch.fx.node.map_aggregate(args, check_proxy)\n-        torch.fx.node.map_aggregate(kwargs, check_proxy)\n-\n-        if len(found_proxies) > 0:\n-            tracer = found_proxies[0].tracer\n-            if op_type == \"call_function\":\n-                target = function\n-            elif op_type == \"call_method\" or op_type == \"get_attr\":\n-                target = function.__name__\n-            else:\n-                raise ValueError(f\"op_type {op_type} not supported.\")\n-            return tracer.create_proxy(op_type, target, args, kwargs, proxy_factory_fn=proxy_factory_fn)\n-        else:\n-            return function(*args, **kwargs)\n-\n-    return wrapper\n-\n-\n-class HFProxyableClassMeta(type):\n-    \"\"\"\n-    Metaclass that creates a class with its main methods wrapped to be proxyable.\n-    \"\"\"\n-\n-    def __new__(\n-        cls,\n-        name: str,\n-        bases: tuple[type, ...],\n-        attrs: dict[str, Any],\n-        proxy_factory_fn: Callable[[Node], Proxy] | None = None,\n-    ):\n-        instance = super().__new__(cls, name, bases, attrs)\n-        for attr_name in dir(instance):\n-            attr = getattr(instance, attr_name, None)\n-            if attr is None:\n-                continue\n-            if attr_name == \"__init__\":\n-                op_type = \"call_function\"\n-            elif attr_name.startswith(\"__\"):\n-                op_type = None\n-            elif inspect.ismethod(attr):\n-                op_type = \"call_function\"\n-            elif inspect.isfunction(attr):\n-                op_type = \"call_method\"\n-            else:\n-                op_type = None\n-            if op_type is not None:\n-                setattr(instance, attr_name, create_wrapper(attr, op_type, proxy_factory_fn=proxy_factory_fn))\n-        return instance\n-\n-\n-def gen_constructor_wrapper(target: Callable) -> tuple[Callable, Callable]:\n-    \"\"\"\n-    Wraps `target` to be proxyable. Used for tensor creators like `torch.ones`, `torch.arange` and so on.\n-    \"\"\"\n-    wrapper = create_wrapper(target, \"call_function\")\n-    return wrapper, target\n-\n-\n-def _proxies_to_metas(v):\n-    \"\"\"Returns the underlying metadata for HFProxies, and behaves like the identity for the others.\"\"\"\n-    if isinstance(v, MetaDeviceAttribute):\n-        return \"meta\"\n-    if isinstance(v, torch.fx.Proxy):\n-        if not (isinstance(v, HFProxy) and hasattr(v, \"_metadata\")):\n-            raise RuntimeError(f\"No metadata was found for {v}\")\n-        return v._metadata\n-    return v\n-\n-\n-def create_cache_proxy_factory_fn(orig_cache_cls: type[Cache]) -> Callable[[Node], HFCacheProxy]:\n-    def cache_proxy_factory_fn(n: Node) -> HFCacheProxy:\n-        if not isinstance(_CURRENT_TRACER, HFTracer):\n-            raise RuntimeError(\"Cannot create HFCacheProxy because there is no HFTracer currently tracing.\")\n-        cache_proxy = HFCacheProxy(n, _CURRENT_TRACER)\n-        cache_proxy.install_orig_cache_cls(orig_cache_cls)\n-        return cache_proxy\n-\n-    return cache_proxy_factory_fn\n-\n-\n-# Proxyable equivalent of the cache classes defined in `transformers.cache_utils`.\n-ProxyableCache = HFProxyableClassMeta(\n-    \"ProxyableCache\", (Cache,), {}, proxy_factory_fn=create_cache_proxy_factory_fn(Cache)\n-)\n-ProxyableDynamicCache = HFProxyableClassMeta(\n-    \"ProxyableDynamicCache\",\n-    (DynamicCache,),\n-    {},\n-    proxy_factory_fn=create_cache_proxy_factory_fn(DynamicCache),\n-)\n-ProxyableStaticCache = HFProxyableClassMeta(\n-    \"ProxyableStaticCache\",\n-    (StaticCache,),\n-    {},\n-    proxy_factory_fn=create_cache_proxy_factory_fn(StaticCache),\n-)\n-\n-\n-def _generate_random_int(low: int = 10, high: int = 20, forbidden_values: list[int] | None = None):\n-    if forbidden_values is None:\n-        forbidden_values = []\n-    value = random.randint(low, high)\n-    while value in forbidden_values:\n-        value = random.randint(low, high)\n-    return value\n-\n-\n-class HFTracer(Tracer):\n-    \"\"\"\n-    Tracer that is able to symbolically trace models from the library. To do that, it uses the HFProxy instead of the\n-    regular PyTorch torch.fx.Proxy.\n-    \"\"\"\n-\n-    # Feature flag for proxying accesses to buffer values\n-    proxy_buffer_attributes: bool = True\n-    allow_insert_stateless_mods: bool = True\n-    _TORCH_METHODS_TO_PATCH = [\n-        \"arange\",\n-        \"zeros\",\n-        \"ones\",\n-        \"full\",\n-        \"full_like\",\n-        \"eye\",\n-        \"empty\",\n-        \"tensor\",\n-        \"clamp\",\n-        \"finfo\",\n-        \"tril\",\n-    ]\n-    _CLASSES_TO_PATCH = {\n-        Cache: ProxyableCache,\n-        DynamicCache: ProxyableDynamicCache,\n-        StaticCache: ProxyableStaticCache,\n-    }\n-\n-    supported_archs = (PreTrainedModel,) if not is_peft_available() else (PreTrainedModel, PeftModel)\n-\n-    def __init__(self, autowrap_modules=(math,), autowrap_functions=()):\n-        super().__init__(autowrap_modules=autowrap_modules, autowrap_functions=autowrap_functions)\n-\n-    def _generate_dummy_input(\n-        self, model: \"PreTrainedModel\", input_name: str, shape: list[int], input_names: list[str]\n-    ) -> dict[str, torch.Tensor]:\n-        \"\"\"Generates dummy input for model inference recording.\"\"\"\n-        # Retrieving the model class, either from the \"class_for_deserialization\" attribute if the model was restored\n-        # from pickle, or from the \"__class__\" attribute in the general case.\n-        model_class_name = getattr(model, \"class_for_deserialization\", model.__class__).__name__\n-        device = model.device\n-        inputs_dict = {}\n-\n-        # when tracing a model with KV cache, we simply need to unsure that the KV cache length is larger than one to\n-        # rightfully pass certain controlflows (Example: https://github.com/huggingface/transformers/blob/5c8d941d66734811d2ef6f57f15b44f7fb7a98c4/src/transformers/modeling_attn_mask_utils.py#L162).\n-        # After tracing, the model can then still be used with arbitrary lengths different than the one used during tracing.\n-        kv_cache_length = 5\n-\n-        if input_name in [\"labels\", \"start_positions\", \"end_positions\"]:\n-            batch_size = shape[0]\n-            if model_class_name in [\n-                *get_values(MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING_NAMES),\n-                *get_values(MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES),\n-                *get_values(MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES),\n-                *get_values(MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING_NAMES),\n-                *get_values(MODEL_FOR_BACKBONE_MAPPING_NAMES),\n-                *get_values(MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES),\n-            ]:\n-                inputs_dict[\"labels\"] = torch.zeros(batch_size, dtype=torch.long, device=device)\n-            elif model_class_name in [\n-                *get_values(MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES),\n-                *get_values(MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES),\n-                \"XLNetForQuestionAnswering\",\n-            ]:\n-                inputs_dict[\"start_positions\"] = torch.zeros(batch_size, dtype=torch.long, device=device)\n-                inputs_dict[\"end_positions\"] = torch.zeros(batch_size, dtype=torch.long, device=device)\n-            elif model_class_name in get_values(MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES):\n-                if not hasattr(model.config, \"problem_type\") or model.config.problem_type is None:\n-                    raise ValueError(\n-                        \"Could not retrieve the problem type for the sequence classification task, please set \"\n-                        'model.config.problem_type to one of the following values: \"regression\", '\n-                        '\"single_label_classification\", or \"multi_label_classification\".'\n-                    )\n-\n-                if model.config.problem_type == \"regression\":\n-                    labels_shape = (batch_size, model.config.num_labels)\n-                    labels_dtype = torch.float32\n-                elif model.config.problem_type == \"single_label_classification\":\n-                    labels_shape = (batch_size,)\n-                    labels_dtype = torch.long\n-                elif model.config.problem_type == \"multi_label_classification\":\n-                    labels_shape = (batch_size, model.config.num_labels)\n-                    labels_dtype = torch.float32\n-                else:\n-                    raise ValueError(\n-                        'Expected model.config.problem_type to be either: \"regression\", \"single_label_classification\"'\n-                        f', or \"multi_label_classification\", but \"{model.config.problem_type}\" was provided.'\n-                    )\n-                inputs_dict[\"labels\"] = torch.zeros(*labels_shape, dtype=labels_dtype, device=device)\n-\n-            elif model_class_name in [\n-                *get_values(MODEL_FOR_PRETRAINING_MAPPING_NAMES),\n-                *get_values(MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES),\n-                *get_values(MODEL_FOR_CAUSAL_LM_MAPPING_NAMES),\n-                *get_values(MODEL_FOR_MASKED_LM_MAPPING_NAMES),\n-                *get_values(MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES),\n-                *get_values(MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES),\n-                \"GPT2DoubleHeadsModel\",\n-                \"PeftModelForCausalLM\",\n-                \"PeftModelForSeq2SeqLM\",\n-            ]:\n-                inputs_dict[\"labels\"] = torch.zeros(shape, dtype=torch.long, device=device)\n-            elif model_class_name in [*get_values(MODEL_FOR_CTC_MAPPING_NAMES)]:\n-                inputs_dict[\"labels\"] = torch.zeros(shape, dtype=torch.float32, device=device)\n-            else:\n-                raise NotImplementedError(\n-                    f\"Generating the dummy input named {input_name} for {model_class_name} is not supported yet.\"\n-                )\n-        elif \"pixel_values\" in input_name:\n-            batch_size = shape[0]\n-            image_size = getattr(model.config, \"image_size\", None)\n-            if image_size is None:\n-                if hasattr(model.config, \"vision_config\"):\n-                    image_size = model.config.vision_config.image_size\n-                elif hasattr(model.config, \"encoder\"):\n-                    image_size = model.config.encoder.image_size\n-                else:\n-                    image_size = (_generate_random_int(), _generate_random_int())\n-\n-            # If no num_channels is in the config, use some arbitrary value.\n-            num_channels = getattr(model.config, \"num_channels\", 3)\n-            if not isinstance(image_size, collections.abc.Iterable):\n-                image_size = (image_size, image_size)\n-            height, width = image_size\n-            inputs_dict[input_name] = torch.zeros(\n-                batch_size, num_channels, height, width, dtype=torch.float32, device=device\n-            )\n-        elif \"bbox\" in input_name:\n-            inputs_dict[input_name] = torch.zeros(*shape, 4, dtype=torch.float, device=device)\n-        elif \"input_features\" in input_name:\n-            inputs_dict[input_name] = torch.zeros(\n-                *shape, model.config.input_feat_per_channel, dtype=torch.float, device=device\n-            )\n-        elif \"inputs_embeds\" in input_name:\n-            batch_size = shape[0]\n-\n-            if (\n-                getattr(model.config, \"embedding_size\", None) is not None\n-                and model.config.model_type != \"megatron-bert\"\n-            ):\n-                embedding_size = model.config.embedding_size\n-            else:\n-                embedding_size = model.config.hidden_size\n-\n-            if len(shape) == 3:\n-                # (batch_size, num_choices, sequence_length, embedding_size)\n-                embedding_shape = (batch_size, shape[1], shape[2], embedding_size)\n-            else:\n-                # (batch_size, sequence_length, embedding_size)\n-                embedding_shape = (batch_size, shape[1], embedding_size)\n-\n-            inputs_dict[input_name] = torch.zeros(embedding_shape, dtype=torch.float, device=device)\n-        elif \"visual_feats\" in input_name:\n-            inputs_dict[input_name] = torch.zeros(\n-                shape\n-                + [\n-                    model.config.visual_feat_dim,\n-                ],\n-                dtype=torch.float,\n-                device=device,\n-            )\n-        elif \"visual_pos\" in input_name:\n-            inputs_dict[input_name] = torch.zeros(\n-                shape\n-                + [\n-                    model.config.visual_pos_dim,\n-                ],\n-                dtype=torch.float,\n-                device=device,\n-            )\n-        elif \"inputs\" in input_name:\n-            inputs_dict[input_name] = torch.zeros(*shape, dtype=torch.float, device=device)\n-        elif \"input_values\" in input_name:\n-            batch_size, _ = shape\n-            # Generating big sequence length for audio inputs.\n-            seq_length = _generate_random_int(low=10000, high=20000)\n-            inputs_dict[input_name] = torch.zeros(batch_size, seq_length, dtype=torch.float, device=device)\n-        elif \"mask\" in input_name:\n-            if \"past_key_values\" in input_names:\n-                mask_shape = [shape[0], shape[1] + kv_cache_length]\n-            else:\n-                mask_shape = shape\n-\n-            inputs_dict[input_name] = torch.zeros(mask_shape, dtype=torch.long, device=device)\n-        elif \"ids\" in input_name:\n-            inputs_dict[input_name] = torch.zeros(shape, dtype=torch.long, device=device)\n-        elif \"past_key_values\" in input_name:\n-            if model.config.model_type not in _FX_SUPPORTED_MODELS_WITH_KV_CACHE:\n-                raise NotImplementedError(\n-                    f\"Symbolic trace with past_key_values input is not supported yet for the model {model.config.model_type}. Please open an issue or a PR in Transformers repository if you would like to see the support added.\"\n-                )\n-            num_heads = model.config.num_attention_heads\n-            head_dim = model.config.hidden_size // model.config.num_attention_heads\n-\n-            cache_shape = (shape[0], num_heads, kv_cache_length, head_dim)\n-            pkv = tuple(\n-                (\n-                    torch.rand(cache_shape, dtype=torch.float, device=device),\n-                    torch.rand(cache_shape, dtype=torch.float, device=device),\n-                )\n-                for i in range(model.config.num_hidden_layers)\n-            )\n-            inputs_dict[input_name] = pkv\n-        else:\n-            shape_with_hidden_size = shape + [model.config.hidden_size]\n-            inputs_dict[input_name] = torch.zeros(shape_with_hidden_size, dtype=torch.float, device=device)\n-\n-        return inputs_dict\n-\n-    def create_proxy(self, kind, target, args, kwargs, name=None, type_expr=None, proxy_factory_fn=None):\n-        rv = super().create_proxy(kind, target, args, kwargs, name, type_expr, proxy_factory_fn)\n-\n-        if kind == \"placeholder\" and target in self.meta_args:\n-            rv.install_metadata(self.meta_args[target])\n-            return rv\n-\n-        if target in self.orig_fns:\n-            # NOTE: tensor constructors in PyTorch define the `device` argument as\n-            # *kwargs-only*. That is why this works. If you add methods to\n-            # _TORCH_METHODS_TO_PATCH that do not define `device` as kwarg-only,\n-            # this will break and you will likely see issues where we cannot infer\n-            # the size of the output.\n-            if \"device\" in kwargs:\n-                kwargs[\"device\"] = \"meta\"\n-\n-        try:\n-            args_metas = torch.fx.node.map_aggregate(args, _proxies_to_metas)\n-            kwargs_metas = torch.fx.node.map_aggregate(kwargs, _proxies_to_metas)\n-\n-            should_install_metadata = True\n-\n-            self._disable_module_getattr = True\n-            self._disable_call_module = True\n-\n-            if kind == \"call_function\":\n-                meta_target = _MANUAL_META_OVERRIDES.get(target, target)\n-                meta_out = meta_target(*args_metas, **kwargs_metas)\n-                if isinstance(meta_out, torch.Tensor):\n-                    meta_out = meta_out.to(device=\"meta\")\n-            elif kind == \"call_method\":\n-                method = getattr(args_metas[0].__class__, target)\n-                meta_target = _MANUAL_META_OVERRIDES.get(method, method)\n-                meta_out = meta_target(*args_metas, **kwargs_metas)\n-            elif kind == \"call_module\":\n-                if not hasattr(self, \"orig_forward\"):\n-                    raise AttributeError(f\"{self} does not have an attribute called orig_forward\")\n-                mod = self.root.get_submodule(target)\n-                mod_type = type(mod)\n-                if mod_type in _MANUAL_META_OVERRIDES:\n-                    meta_out = _MANUAL_META_OVERRIDES[mod_type](mod, *args_metas, **kwargs_metas)\n-                else:\n-                    meta_out = self.orig_forward(*args_metas, **kwargs_metas)\n-            elif kind == \"get_attr\":\n-                attr_itr = self.root\n-                atoms = target.split(\".\")\n-                for atom in atoms:\n-                    attr_itr = getattr(attr_itr, atom)\n-                if isinstance(attr_itr, torch.Tensor):\n-                    meta_out = attr_itr.to(device=\"meta\")\n-                else:\n-                    meta_out = attr_itr\n-            else:\n-                should_install_metadata = False\n-\n-            if should_install_metadata:\n-                if not isinstance(rv, Proxy):\n-                    raise ValueError(\"Don't support composite output yet\")\n-                rv.install_metadata(meta_out)\n-\n-        except Exception as e:\n-            if _IS_IN_DEBUG_MODE:\n-                warnings.warn(f\"Could not compute metadata for {kind} target {target}: {e}\")\n-\n-        self._disable_module_getattr = False\n-        self._disable_call_module = False\n-\n-        return rv\n-\n-    # Replaced by .getattr from PyTorch 1.13\n-    def _module_getattr(self, attr, attr_val, parameter_proxy_cache):\n-        if getattr(self, \"_disable_module_getattr\", False):\n-            return attr_val\n-        else:\n-\n-            def maybe_get_proxy_for_attr(attr_val, collection_to_search, parameter_proxy_cache):\n-                for n, p in collection_to_search:\n-                    if attr_val is p:\n-                        if n not in parameter_proxy_cache:\n-                            kwargs = {}\n-                            if \"proxy_factory_fn\" in inspect.signature(self.create_proxy).parameters:\n-                                kwargs[\"proxy_factory_fn\"] = (\n-                                    None\n-                                    if not self.param_shapes_constant\n-                                    else lambda node: ParameterProxy(self, node, n, attr_val)\n-                                )\n-                            val_proxy = self.create_proxy(\"get_attr\", n, (), {}, **kwargs)  # type: ignore[arg-type]\n-                            parameter_proxy_cache[n] = val_proxy\n-                        return parameter_proxy_cache[n]\n-                return None\n-\n-            if isinstance(attr_val, torch.nn.Parameter):\n-                maybe_parameter_proxy = maybe_get_proxy_for_attr(\n-                    attr_val, self.root.named_parameters(), parameter_proxy_cache\n-                )\n-                if maybe_parameter_proxy is not None:\n-                    return maybe_parameter_proxy\n-\n-            if self.proxy_buffer_attributes and isinstance(attr_val, torch.Tensor):\n-                maybe_buffer_proxy = maybe_get_proxy_for_attr(\n-                    attr_val, self.root.named_buffers(), parameter_proxy_cache\n-                )\n-                if maybe_buffer_proxy is not None:\n-                    return maybe_buffer_proxy\n-\n-            return attr_val\n-\n-    # Needed for PyTorch 1.13+\n-    def getattr(self, attr: str, attr_val: Any, parameter_proxy_cache: dict[str, Any]):\n-        return self._module_getattr(attr, attr_val, parameter_proxy_cache)\n-\n-    def call_module(self, m, forward, args, kwargs):\n-        if getattr(self, \"_disable_call_module\", False):\n-            return forward(*args, **kwargs)\n-        self.orig_forward = forward\n-        return super().call_module(m, forward, args, kwargs)\n-\n-    def proxy(self, node):\n-        return HFProxy(node, self)\n-\n-    @contextlib.contextmanager\n-    def patch_for_tracing(self, root: torch.nn.Module | Callable[..., Any]):\n-        # Patching torch functions\n-        self.patched_torch_methods = {\n-            target: gen_constructor_wrapper(getattr(torch, target)) for target in self._TORCH_METHODS_TO_PATCH\n-        }\n-        self.orig_fns = set()\n-\n-        for name, (wrapper, orig) in self.patched_torch_methods.items():\n-            setattr(torch, name, wrapper)\n-            self.orig_fns.add(orig)\n-\n-        # Patching classes\n-        patched = []\n-        module_of_model = inspect.getmodule(root)\n-        for name, mod in sys.modules.items():\n-            if module_of_model is not None and mod is not module_of_model:\n-                continue\n-            if not name.startswith(\"transformers\"):\n-                continue\n-            for orig_cls, patched_cls in self._CLASSES_TO_PATCH.items():\n-                for attr_name, attr in mod.__dict__.items():\n-                    if attr is orig_cls:\n-                        patched.append((mod, attr_name, orig_cls))\n-                        setattr(mod, attr_name, patched_cls)\n-\n-        yield\n-\n-        # Restoring patched functions and classes.\n-        for name, (_, orig) in self.patched_torch_methods.items():\n-            setattr(torch, name, orig)\n-        self.patched_torch_methods = {}\n-        self.orig_fns = set()\n-\n-        for mod, attr_name, orig_cls in patched:\n-            setattr(mod, attr_name, orig_cls)\n-\n-    def trace(\n-        self,\n-        root: torch.nn.Module | Callable[..., Any],\n-        concrete_args: dict[str, Any] | None = None,\n-        dummy_inputs: dict[str, Any] | None = None,\n-        complete_concrete_args_with_inputs_not_in_dummy_inputs: bool = True,\n-    ) -> Graph:\n-        \"\"\"\n-        Traces `root` and returns the corresponding FX `torch.fx.Graph` representation. `root` can either be a\n-        `torch.nn.Module` instance or a Python callable. Note that after this call, `self.root` may be different from\n-        the `root` passed in here. For example, when a free function is passed to `trace()`, we will create a\n-        `torch.nn.Module` instance to use as the root and add embedded constants to.\n-\n-        Args:\n-            root (`torch.nn.Module` or  `Callable`):\n-                Either a `torch.nn.Module`` or a function to be traced through. If root is not a\n-                [`~transformers.PreTrainedModel`], then `dummy_inputs` must be passed, otherwise tracing will fail.\n-            concrete_args (`dict[str, Any], *optional*):\n-                Concrete arguments that should not be treated as Proxies\n-            dummy_inputs (`dict[str, Any]`, *optional*):\n-                The dummy inputs needed to handle data-dependent control-flow if `root` is not a\n-                [`~transformers.PreTrainedModel`]. It can also be used when `root` is a\n-                [`~transformers.PreTrainedModel`] to specify custom dummy inputs for a subset or all the model inputs.\n-            complete_concrete_args_with_inputs_not_in_dummy_inputs (`bool`, *optional*, defaults to `True`):\n-                If `True`, and `dummy_inputs` is specified, every argument that `root` can take that is not in\n-                `dummy_inputs` and not in `concrete_args` will be added to `concrete_args`, otherwise does nothing.\n-\n-        Returns:\n-            `torch.fx.Graph`:\n-                A FX `torch.fx.Graph` representing the semantics of the passed-in `root`.\n-\n-        \"\"\"\n-        sig = inspect.signature(root.forward if isinstance(root, torch.nn.Module) else root)\n-\n-        if concrete_args is None:\n-            concrete_args = {}\n-\n-        if dummy_inputs is not None and complete_concrete_args_with_inputs_not_in_dummy_inputs:\n-            for param in sig.parameters.values():\n-                if param.name in dummy_inputs:\n-                    continue\n-                if param.default is inspect.Parameter.empty:\n-                    raise ValueError(f\"You need to specify a default value for the parameter {param.name}.\")\n-            concrete_args.update(\n-                {\n-                    p.name: p.default\n-                    for p in sig.parameters.values()\n-                    if (p.name not in dummy_inputs and p.name not in concrete_args)\n-                }\n-            )\n-\n-        input_names = sig.parameters.keys() - concrete_args.keys()\n-\n-        # Creating a random input shape to generate dummy inputs.\n-        batch_size = _generate_random_int()\n-        sequence_length = _generate_random_int()\n-        shape = [batch_size, sequence_length]\n-\n-        if root.__class__.__name__ in get_values(MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES):\n-            num_choices = _generate_random_int(low=2, high=5)\n-            shape.insert(1, num_choices)\n-\n-        inputs = dict(dummy_inputs) if dummy_inputs is not None else {}\n-        for input_name in input_names:\n-            if input_name in inputs:\n-                continue\n-            # We enforce that root must either be a PreTrainedModel or deserialized from a serialized traced model to\n-            # be able to use HFTracer._generate_dummy_input.\n-            if isinstance(root, self.supported_archs) or type(root).__qualname__.startswith(\n-                (\"_deserialize_graph_module\", \"_CodeOnlyModule\")\n-            ):\n-                inputs.update(self._generate_dummy_input(root, input_name, shape, input_names=input_names))\n-            else:\n-                raise RuntimeError(\n-                    f\"Could not generate input named {input_name} for because root is not a\"\n-                    \" transformers.PreTrainedModel.\"\n-                )\n-\n-        def to_meta(value):\n-            if isinstance(value, torch.Tensor):\n-                return value.to(\"meta\")\n-            return value\n-\n-        concrete_metas = pytree.tree_map(to_meta, inputs)\n-\n-        for param in sig.parameters.values():\n-            if param.kind == inspect.Parameter.VAR_KEYWORD and param.name not in input_names:\n-                concrete_metas[f\"**{param.name}\"] = {}\n-        self.meta_args = concrete_metas\n-\n-        global _CURRENT_TRACER\n-        _CURRENT_TRACER = self\n-        with self.patch_for_tracing(root):\n-            try:\n-                self.graph = super().trace(root, concrete_args=concrete_args)\n-            finally:\n-                _CURRENT_TRACER = None\n-\n-        # This is necessary because concrete args are added as input to the traced module since\n-        # https://github.com/pytorch/pytorch/pull/55888.\n-        for node in self.graph.nodes:\n-            if node.op == \"placeholder\":\n-                # Removing default values for inputs as the forward pass will fail with them.\n-                if node.target in input_names:\n-                    node.args = ()\n-                    # Without this, torch.jit.script fails because the inputs type is Optional[torch.Tensor].\n-                    # It cannot infer on the attributes and methods the input should have, and fails.\n-                    node.type = torch.Tensor\n-                # It is a concrete arg so it is not used and should be removed.\n-                else:\n-                    to_visit = [node]\n-                    to_delete = collections.OrderedDict()\n-                    while to_visit:\n-                        n = to_visit.pop(0)\n-                        to_delete[n] = None\n-                        to_visit += list(n.users.keys())\n-\n-                    for user in reversed(to_delete.keys()):\n-                        self.graph.erase_node(user)\n-\n-            # TODO: solves GraphModule creation.\n-            # Without this, return type annotation \"Tuple\" is causing code execution failure.\n-            if node.op == \"output\":\n-                node.type = None\n-\n-        return self.graph\n-\n-    def _stateless_mod_instantiation_depends_on_proxies(self, mod: nn.Module) -> bool:\n-        \"\"\"\n-        Whether the module was instantiated with Proxies. If that is the case, such module cannot be a leaf module\n-        because its attributes are input-dependent.\n-        \"\"\"\n-        return any(isinstance(attr, Proxy) for attr in mod.__dict__.values())\n-\n-    def _insert_module_as_submodule(self, mod: nn.Module) -> str:\n-        \"\"\"\n-        Helper method which tries to insert a module that was not declared as submodule.\n-        \"\"\"\n-        # If one of the module attributes is a Proxy, it means that its instantiation is input-dependent.\n-        # It is not possible to insert such modules, those should be traced through.\n-        if self._stateless_mod_instantiation_depends_on_proxies(mod):\n-            return \"\"\n-        idx = 0\n-        mod_name = mod.__class__.__name__.lower()\n-        path = f\"{mod_name}_{idx}\"\n-        already_inserted = False\n-        while hasattr(self.root, path):\n-            if getattr(self.root, path) is mod:\n-                already_inserted = True\n-                break\n-            path = f\"{mod_name}_{idx}\"\n-            idx += 1\n-\n-        # No need to add multiple instances of the same module.\n-        if not already_inserted:\n-            self.root.add_module(path, mod)\n-        return path\n-\n-    def path_of_module(self, mod: nn.Module) -> str:\n-        \"\"\"\n-        Helper method to find the qualified name of `mod` in the Module hierarchy of `root`. For example, if `root` has\n-        a submodule named `foo`, which has a submodule named `bar`, passing `bar` into this function will return the\n-        string \"foo.bar\".\n-\n-        Args:\n-            mod (str): The `Module` to retrieve the qualified name for.\n-        \"\"\"\n-        try:\n-            return super().path_of_module(mod)\n-        except NameError as e:\n-            if self.allow_insert_stateless_mods and len(list(mod.parameters())) == 0 and len(list(mod.buffers())) == 0:\n-                path = self._insert_module_as_submodule(mod)\n-                return path\n-            raise e\n-\n-    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool:\n-        return (not self._stateless_mod_instantiation_depends_on_proxies(m)) and super().is_leaf_module(\n-            m, module_qualified_name\n-        )\n-\n-    @compatibility(is_backward_compatible=True)\n-    def keys(self, obj: \"Proxy\") -> Any:\n-        \"\"\"Called when a proxy object is has the keys() method called.\n-        This is what happens when ** is called on a proxy. This should return an iterator if ** is supposed to work in\n-        your custom tracer.\n-        \"\"\"\n-        attribute = HFAttribute(obj, \"keys\")()\n-        if obj.node.target.startswith(\"**\"):\n-            return attribute._metadata\n-        return attribute\n-\n-\n-def get_concrete_args(model: nn.Module, input_names: list[str]):\n-    sig = inspect.signature(model.forward)\n-\n-    if not (set(input_names) <= set(sig.parameters.keys())):\n-        formatted_input_names = input_names[0] if len(input_names) == 1 else \", \".join(input_names)\n-        formatted_allowed_input_names = \", \".join(sig.parameters.keys())\n-        raise ValueError(\n-            f\"The model does not have input(s) named: {formatted_input_names}, expected a subset of the following:\"\n-            f\" {formatted_allowed_input_names}\"\n-        )\n-\n-    return {p.name: p.default for p in sig.parameters.values() if p.name not in input_names}\n-\n-\n-def is_model_supported(model: \"PreTrainedModel\"):\n-    return model.__class__.__name__ in _SUPPORTED_MODELS\n-\n-\n-def check_if_model_is_supported(model: \"PreTrainedModel\"):\n-    if not is_model_supported(model):\n-        supported_model_names = \", \".join(_SUPPORTED_MODELS)\n-        raise NotImplementedError(\n-            f\"Model {model.__class__.__name__} is not supported yet, supported models: {supported_model_names}\"\n-        )\n-\n-\n-def symbolic_trace(\n-    model: \"PreTrainedModel\",\n-    input_names: list[str] | None = None,\n-    disable_check: bool = False,\n-    tracer_cls: type[HFTracer] = HFTracer,\n-) -> GraphModule:\n-    \"\"\"\n-    Performs symbolic tracing on the model.\n-\n-    Args:\n-        model ([`PretrainedModel`]):\n-            The model to trace.\n-        input_names (`list[str]`, *optional*):\n-            The names of the inputs of the traced model. If unset, model.dummy_inputs.keys() are used instead.\n-        disable_check (`bool`, *optional*, defaults to `False`):\n-            If `True`, no check is done before trying to trace the model, this is mostly usesul for debugging purposes.\n-        tracer_cls (`Type[HFTracer]`, *optional*, defaults to `HFTracer`):\n-            The tracer class to use for instantiating the tracer. If unset, `HFTracer` is used instead.\n-\n-    Returns:\n-        `torch.fx.GraphModule`: A GraphModule constructed by recording operations seen while tracing the model.\n-\n-    Example:\n-\n-        ```python\n-        from transformers.utils.fx import symbolic_trace\n-\n-        traced_model = symbolic_trace(model, input_names=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])\n-        ```\n-    \"\"\"\n-    if input_names is None:\n-        input_names = model.dummy_inputs.keys()\n-\n-    input_names = list(input_names)\n-    concrete_args = get_concrete_args(model, input_names)\n-\n-    if not disable_check:\n-        check_if_model_is_supported(model)\n-\n-    if \"past_key_values\" in input_names and not getattr(model.config, \"use_cache\", False):\n-        logger.warning(\n-            \"`past_key_values` were specified as input names, but model.config.use_cache = False, this might lead to \"\n-            \"unexpected behavior.\"\n-        )\n-    if \"past_key_values\" not in input_names and getattr(model.config, \"use_cache\", False):\n-        logger.warning(\n-            \"`past_key_values` were not specified as input names, but model.config.use_cache = True. Setting \"\n-            \"model.config.use_cache = False.\"\n-        )\n-        model.config.use_cache = False\n-\n-    # Tracing.\n-    tracer = tracer_cls()\n-    traced_graph = tracer.trace(model, concrete_args=concrete_args)\n-    traced = torch.fx.GraphModule(model, traced_graph)\n-\n-    traced.config = model.config\n-    # The model class must be stored as an attribute to allow model deserialization, which uses trace, and thus\n-    # _generate_dummy_input, where the model class is needed.\n-    traced.class_for_deserialization = model.__class__\n-    traced.device = model.device\n-\n-    return traced"
        },
        {
            "sha": "e2b50ae7054a4355420e52d06f81c04ef93066e2",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -1258,6 +1258,25 @@ def is_torch_fx_proxy(x):\n         return False\n \n \n+def is_jit_tracing() -> bool:\n+    try:\n+        import torch\n+\n+        return torch.jit.is_tracing()\n+    except Exception:\n+        return False\n+\n+\n+def is_tracing(tensor=None) -> bool:\n+    \"\"\"Checks whether we are tracing a graph with dynamo (compile or export), torch.jit, or torch.fx\"\"\"\n+    # Note that `is_torchdynamo_compiling` checks both compiling and exporting (the export check is stricter and\n+    # only checks export)\n+    _is_tracing = is_torchdynamo_compiling() or is_jit_tracing()\n+    if tensor is not None:\n+        _is_tracing |= is_torch_fx_proxy(tensor)\n+    return _is_tracing\n+\n+\n @lru_cache\n def is_in_notebook() -> bool:\n     try:"
        },
        {
            "sha": "698f8bd361c1035097469088173cfc087f2ee423",
            "filename": "tests/models/aimv2/test_modeling_aimv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -179,7 +179,6 @@ class Aimv2VisionModelTest(Aimv2ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (Aimv2VisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -308,7 +307,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Aimv2TextModelTest(Aimv2ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Aimv2TextModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -388,7 +386,6 @@ class Aimv2ModelTest(Aimv2ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "287522aedddef8d896dd33f941a9a2ed7b93db56",
            "filename": "tests/models/albert/test_modeling_albert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -260,7 +260,6 @@ class AlbertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False  # will not be maintained\n \n     # special case for ForPreTraining model\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "9b4e877c30f499e23e3e59dcfd067dd259f4ff7c",
            "filename": "tests/models/align/test_modeling_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_modeling_align.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -130,7 +130,6 @@ class AlignVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (AlignVisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     has_attentions = False\n@@ -333,7 +332,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class AlignTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (AlignTextModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     def setUp(self):\n         self.model_tester = AlignTextModelTester(self)\n@@ -437,7 +435,6 @@ def prepare_config_and_inputs_for_common(self):\n class AlignModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (AlignModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": AlignModel} if is_torch_available() else {}\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "4b7e8bc0ad015b0d367dc27be0f42ca011fcf562",
            "filename": "tests/models/altclip/test_modeling_altclip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -131,7 +131,6 @@ class AltCLIPVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (AltCLIPVisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -293,7 +292,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class AltCLIPTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (AltCLIPTextModel,) if is_torch_available() else ()\n-    fx_compatible = False  # Cannot support if `can_return_tuple`\n \n     # TODO (@SunMarc): Fix me\n     @unittest.skip(reason=\"It's broken.\")\n@@ -407,7 +405,6 @@ def prepare_img():\n class AltCLIPModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (AltCLIPModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": AltCLIPModel} if is_torch_available() else {}\n-    fx_compatible = False  # Cannot support if `can_return_tuple`\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "22304e4c89139a38165efd1764a39e32fe90a48d",
            "filename": "tests/models/arcee/test_modeling_arcee.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Farcee%2Ftest_modeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Farcee%2Ftest_modeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Farcee%2Ftest_modeling_arcee.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -45,7 +45,6 @@ class ArceeModelTester(CausalLMModelTester):\n \n @require_torch\n class ArceeModelTest(CausalLMModelTest, unittest.TestCase):\n-    fx_compatible = False\n     model_tester_class = ArceeModelTester\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "f8560bd47f0ff60e6eb7bed3a136bf5881dac76e",
            "filename": "tests/models/audio_spectrogram_transformer/test_modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -160,7 +160,6 @@ class ASTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n "
        },
        {
            "sha": "acab03ad5b468b349b62853e613ac2b33b53d991",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -170,7 +170,6 @@ class AyaVisionModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     _is_composite = True\n "
        },
        {
            "sha": "832911168b266fc7aec267e59ef0843c2c4880e7",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -288,8 +288,6 @@ class BambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "40991788e3461f4dc1942eba5f42c9e51e996c44",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -520,7 +520,6 @@ class BarkSemanticModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.Te\n     all_generative_model_classes = (BarkCausalModel,) if is_torch_available() else ()\n \n     is_encoder_decoder = False\n-    fx_compatible = False\n     test_missing_keys = False\n \n     test_resize_embeddings = True\n@@ -607,7 +606,6 @@ class BarkCoarseModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.Test\n     all_generative_model_classes = (BarkCausalModel,) if is_torch_available() else ()\n \n     is_encoder_decoder = False\n-    fx_compatible = False\n     test_missing_keys = False\n \n     test_resize_embeddings = True\n@@ -691,7 +689,6 @@ class BarkFineModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (BarkFineModel,) if is_torch_available() else ()\n \n     is_encoder_decoder = False\n-    fx_compatible = False\n     test_missing_keys = False\n \n     test_resize_embeddings = True"
        },
        {
            "sha": "8bee5d9555d35263227af58571f82b02579239eb",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -421,7 +421,6 @@ class BartModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = False  # Fix me Michael\n \n     def setUp(self):\n         self.model_tester = BartModelTester(self)\n@@ -1504,8 +1503,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class BartStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (BartDecoder, BartForCausalLM) if is_torch_available() else ()\n-    fx_comptatible = True\n-\n     is_encoder_decoder = False\n     test_missing_keys = False\n "
        },
        {
            "sha": "c1473fcb0496270ec8291e6d53440cde35f06b5b",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -462,7 +462,6 @@ class BertModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False  # won't be maintained\n     model_split_percents = [0.5, 0.8, 0.9]\n \n     # special case for ForPreTraining model"
        },
        {
            "sha": "17c934351784b450db180c7527bc01a6193200ed",
            "filename": "tests/models/bit/test_modeling_bit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -164,8 +164,6 @@ class BitModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "f55597eddb4343a97668a84514529cf715456619",
            "filename": "tests/models/bitnet/test_modeling_bitnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -142,8 +142,6 @@ class BitNetModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         else {}\n     )\n \n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n-\n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip(\n         self,"
        },
        {
            "sha": "4e906a4dceb87d836b1c642446d88b0dc91c4056",
            "filename": "tests/models/blenderbot/test_modeling_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -224,7 +224,6 @@ class BlenderbotModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = False\n     test_missing_keys = False\n \n     def setUp(self):"
        },
        {
            "sha": "aef6aaa70318f396cd75cdd52108a4092889bd5d",
            "filename": "tests/models/blenderbot_small/test_modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -216,7 +216,6 @@ class BlenderbotSmallModelTest(ModelTesterMixin, GenerationTesterMixin, Pipeline\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = False\n     test_missing_keys = False\n \n     # TODO: Fix the failed tests when this model gets more usage"
        },
        {
            "sha": "5a81b45f71a72801ee4b53fee9763bc66be7884f",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -149,7 +149,6 @@ class BlipVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (BlipVisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -310,7 +309,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class BlipTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (BlipTextModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     def setUp(self):\n         self.model_tester = BlipTextModelTester(self)\n@@ -418,7 +416,6 @@ class BlipModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n@@ -691,7 +688,6 @@ class BlipVQAModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (BlipForQuestionAnswering,) if is_torch_available() else ()\n     # Doesn't run generation tests due to custom generation logic -- won't fix\n     all_generative_model_classes = ()\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n@@ -768,7 +764,6 @@ def test_model_get_set_embeddings(self):\n @require_torch\n class BlipTextRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (BlipForImageTextRetrieval,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n@@ -897,7 +892,6 @@ class BlipTextImageModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (BlipForConditionalGeneration,) if is_torch_available() else ()\n     # Doesn't run generation tests due to custom generation logic -- wont fix\n     all_generative_model_classes = ()\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     test_attention_outputs = False"
        },
        {
            "sha": "52e597b32ecc8a42748234df680127d6b2f5beed",
            "filename": "tests/models/blip/test_modeling_blip_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -125,7 +125,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class BlipTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (BlipTextModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     def setUp(self):\n         self.model_tester = BlipTextModelTester(self)"
        },
        {
            "sha": "9d4ba7b2e733876658ddc3ac31133c93a041303a",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -156,7 +156,6 @@ class Blip2VisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (Blip2VisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -461,7 +460,6 @@ def prepare_config_and_inputs_for_common(self):\n class Blip2ForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2ForConditionalGeneration,) if is_torch_available() else ()\n     additional_model_inputs = [\"input_ids\"]\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False\n@@ -791,7 +789,6 @@ class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixi\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n@@ -1070,7 +1067,6 @@ def create_and_check_model(self, config, input_ids, attention_mask):\n @require_torch\n class Blip2TextModelWithProjectionTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2TextModelWithProjection,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n@@ -1227,7 +1223,6 @@ def create_and_check_model(self, config, pixel_values):\n @require_torch\n class Blip2VisionModelWithProjectionTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2VisionModelWithProjection,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -1375,7 +1370,6 @@ def prepare_config_and_inputs_for_common(self):\n class Blip2TextRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2ForImageTextRetrieval,) if is_torch_available() else ()\n     additional_model_inputs = [\"input_ids\"]\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     test_attention_outputs = False"
        },
        {
            "sha": "7c05340cec811146e8703e8fdbf30f189d440626",
            "filename": "tests/models/bloom/test_modeling_bloom.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -168,7 +168,6 @@ def create_and_check_bloom_weight_initialization(self, config, *args):\n @require_torch\n class BloomModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = BloomModelTester\n-    fx_compatible = True\n     test_missing_keys = False\n \n     def test_bloom_model_past(self):"
        },
        {
            "sha": "c7ca7099582dce181ed8367e70d661e19eff1ff4",
            "filename": "tests/models/blt/test_modeling_blt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -168,7 +168,6 @@ def get_config(self):\n \n @require_torch\n class BltModelTest(CausalLMModelTest, unittest.TestCase):\n-    fx_compatible = False\n     model_tester_class = BltModelTester\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "4237db1107d9e11b2228c6461ed402402272b406",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -205,8 +205,6 @@ class ChameleonModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     def setUp(self):\n         self.model_tester = ChameleonModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=ChameleonConfig, hidden_size=37)\n@@ -258,8 +256,6 @@ class ChameleonVision2SeqModelTest(ModelTesterMixin, GenerationTesterMixin, unit\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     def setUp(self):\n         self.model_tester = ChameleonVision2SeqModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=ChameleonConfig, hidden_size=37)"
        },
        {
            "sha": "f48e8c39e02b99a8229b35e9cd688dba787e11f5",
            "filename": "tests/models/chinese_clip/test_modeling_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -312,7 +312,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class ChineseCLIPTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (ChineseCLIPTextModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     # special case for ForPreTraining model\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n@@ -405,7 +404,6 @@ class ChineseCLIPVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (ChineseCLIPVisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -538,7 +536,6 @@ def prepare_config_and_inputs_for_common(self):\n class ChineseCLIPModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (ChineseCLIPModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": ChineseCLIPModel} if is_torch_available() else {}\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "3075a0df1e58816ea2046d2d54da018b438c2f3b",
            "filename": "tests/models/clap/test_modeling_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -158,7 +158,6 @@ class ClapAudioModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (ClapAudioModel, ClapAudioModelWithProjection) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -377,7 +376,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class ClapTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (ClapTextModel, ClapTextModelWithProjection) if is_torch_available() else ()\n-    fx_compatible = False\n \n     def setUp(self):\n         self.model_tester = ClapTextModelTester(self)\n@@ -487,7 +485,6 @@ def prepare_config_and_inputs_for_common(self):\n class ClapModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (ClapModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": ClapModel} if is_torch_available() else {}\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "4ce1b9faa6aa31b172b9648e6025bf0033b8c4de",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -208,7 +208,6 @@ class CLIPVisionModelTest(CLIPModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (CLIPVisionModel, CLIPVisionModelWithProjection) if is_torch_available() else ()\n-    fx_compatible = True\n \n     test_resize_embeddings = False\n \n@@ -396,7 +395,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class CLIPTextModelTest(CLIPModelTesterMixin, unittest.TestCase):\n     all_model_classes = (CLIPTextModel, CLIPTextModelWithProjection) if is_torch_available() else ()\n-    fx_compatible = True\n \n     model_split_percents = [0.5, 0.8, 0.9]\n \n@@ -524,7 +522,6 @@ class CLIPModelTest(CLIPModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n         {\"feature-extraction\": CLIPModel, \"image-feature-extraction\": CLIPVisionModel} if is_torch_available() else {}\n     )\n     additional_model_inputs = [\"pixel_values\"]\n-    fx_compatible = True\n \n     test_resize_embeddings = False\n     test_attention_outputs = False\n@@ -624,7 +621,6 @@ def prepare_config_and_inputs_for_common(self):\n class CLIPForImageClassificationModelTest(CLIPModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (CLIPForImageClassification,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-classification\": CLIPForImageClassification} if is_torch_available() else {}\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "c3dcf643966c8f0ede23172623f4ded825dcb398",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -136,7 +136,6 @@ class CLIPSegVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (CLIPSegVisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -293,7 +292,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class CLIPSegTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (CLIPSegTextModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     model_split_percents = [0.5, 0.8, 0.9]\n \n@@ -420,7 +418,6 @@ def prepare_config_and_inputs_for_common(self):\n class CLIPSegModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (CLIPSegModel, CLIPSegForImageSegmentation) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": CLIPSegModel} if is_torch_available() else {}\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "815c509b73b09d8b64d8e8f53d6dd363770f62d3",
            "filename": "tests/models/codegen/test_modeling_codegen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -313,7 +313,6 @@ class CodeGenModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n     pipeline_model_mapping = (\n         {\"feature-extraction\": CodeGenModel, \"text-generation\": CodeGenForCausalLM} if is_torch_available() else {}\n     )\n-    fx_compatible = False\n \n     test_missing_keys = False\n "
        },
        {
            "sha": "e76d0aa8c6a118c724ea49c5ac244481776bba43",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -171,8 +171,6 @@ class CohereModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]\n@@ -188,9 +186,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_torch_fx_output_loss(self):\n-        super().test_torch_fx_output_loss()\n-\n \n @require_torch\n @slow"
        },
        {
            "sha": "eaffd4368735b5596166fe05ada7ffa94d675eaf",
            "filename": "tests/models/cohere2_vision/test_modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -157,7 +157,6 @@ class Cohere2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     _is_composite = True\n "
        },
        {
            "sha": "54e45ed7f31e4ecb7dfc926b4b1d12f4cadd9b1f",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -185,8 +185,6 @@ class ColPaliForRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (ColPaliForRetrieval,) if is_torch_available() else ()\n-    fx_compatible = False\n-\n     test_resize_embeddings = True\n     additional_model_inputs = [\"token_type_ids\"]\n "
        },
        {
            "sha": "790cf639c985e040861d11f243f78450a0ab1b73",
            "filename": "tests/models/colqwen2/test_modeling_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -200,7 +200,6 @@ class ColQwen2ForRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (ColQwen2ForRetrieval,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n "
        },
        {
            "sha": "ce90bb53779468b63771a5f41030584c6bd1b1d2",
            "filename": "tests/models/convnext/test_modeling_convnext.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -175,8 +175,6 @@ class ConvNextModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "79fd07b098fc371eafdf0b42bcce54a84112b6e9",
            "filename": "tests/models/convnextv2/test_modeling_convnextv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -154,8 +154,6 @@ class ConvNextV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "8ed6e335c62d57f1c3b39d07f91d394625254e9c",
            "filename": "tests/models/cwm/test_modeling_cwm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fcwm%2Ftest_modeling_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fcwm%2Ftest_modeling_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcwm%2Ftest_modeling_cwm.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -76,7 +76,6 @@ class CwmModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n     model_tester_class = CwmModelTester\n \n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "915d7472f5fbffad710d81e3b3d7e7959588e56a",
            "filename": "tests/models/deberta/test_modeling_deberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -237,8 +237,6 @@ class DebertaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n         else {}\n     )\n \n-    fx_compatible = True\n-\n     is_encoder_decoder = False\n \n     def setUp(self):\n@@ -274,14 +272,6 @@ def test_model_from_pretrained(self):\n         model = DebertaModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @unittest.skip(\"This test was broken by the refactor in #22105, TODO @ArthurZucker\")\n-    def test_torch_fx_output_loss(self):\n-        pass\n-\n-    @unittest.skip(\"This test was broken by the refactor in #22105, TODO @ArthurZucker\")\n-    def test_torch_fx(self):\n-        pass\n-\n \n @require_torch\n @require_sentencepiece"
        },
        {
            "sha": "7236e740ed830ef513fda24e04838ef6d6e8ffe9",
            "filename": "tests/models/deberta_v2/test_modeling_deberta_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -251,8 +251,6 @@ class DebertaV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n         else {}\n     )\n \n-    fx_compatible = True\n-\n     is_encoder_decoder = False\n \n     def setUp(self):\n@@ -292,14 +290,6 @@ def test_model_from_pretrained(self):\n         model = DebertaV2Model.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @unittest.skip(\"This test was broken by the refactor in #22105, TODO @ArthurZucker\")\n-    def test_torch_fx_output_loss(self):\n-        pass\n-\n-    @unittest.skip(\"This test was broken by the refactor in #22105, TODO @ArthurZucker\")\n-    def test_torch_fx(self):\n-        pass\n-\n \n @require_torch\n @require_sentencepiece"
        },
        {
            "sha": "8f7da870e700f44766ad90d9d81680dd888a32c6",
            "filename": "tests/models/deepseek_v2/test_modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -54,7 +54,6 @@ def __init__(\n \n @require_torch\n class DeepseekV2ModelTest(CausalLMModelTest, unittest.TestCase):\n-    fx_compatible = False\n     test_all_params_have_gradient = False\n     model_tester_class = DeepseekV2ModelTester\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "618aa2f4ca66201ad286812d52849b33b99b1a29",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -233,8 +233,6 @@ class DeepseekV3ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "f18c75cb3febafb21a3747825e1dd15855c33635",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -194,8 +194,6 @@ class DiffLlamaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "bddb82bafbc01e2ec6e18646926b8ffd277bd032",
            "filename": "tests/models/dinat/test_modeling_dinat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -210,7 +210,6 @@ class DinatModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_torch_exportable = True"
        },
        {
            "sha": "a3624493c5fb50954d480b513b79df4815b0de9b",
            "filename": "tests/models/dinov2/test_modeling_dinov2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -227,7 +227,6 @@ class Dinov2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False  # broken by output recording refactor\n \n     test_resize_embeddings = False\n "
        },
        {
            "sha": "dc5fc3b12d666000bcd8292567104095c8d89750",
            "filename": "tests/models/dinov2_with_registers/test_modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -232,7 +232,6 @@ class Dinov2WithRegistersModelTest(ModelTesterMixin, PipelineTesterMixin, unitte\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_torch_exportable = True"
        },
        {
            "sha": "ccb84f2111d97bfa2876d93b09e9c112effc53ff",
            "filename": "tests/models/dinov3_convnext/test_modeling_dinov3_convnext.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -165,8 +165,6 @@ class DINOv3ConvNextModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Te\n     all_model_classes = (DINOv3ConvNextModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-feature-extraction\": DINOv3ConvNextModel} if is_torch_available() else {}\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "09f76e066840c419b5c1fb50c2b52ead162db85f",
            "filename": "tests/models/dinov3_vit/test_modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -150,7 +150,6 @@ class Dinov3ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_torch_exportable = True"
        },
        {
            "sha": "f6a01a5620e175a2ee822487db2ebcc5b9543f51",
            "filename": "tests/models/distilbert/test_modeling_distilbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -223,7 +223,6 @@ class DistilBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False  # won't be maintained\n     test_resize_embeddings = True\n     test_resize_position_embeddings = True\n "
        },
        {
            "sha": "f01e22fb794a2fb30cd95045d2f7bc76d8aaf5f1",
            "filename": "tests/models/doge/test_modeling_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -275,8 +275,6 @@ class DogeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     )\n     has_attentions = False\n \n-    fx_compatible = False\n-\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "1f983ecdff43076a019695b3c82990377c92c620",
            "filename": "tests/models/donut/test_modeling_donut_swin.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -166,7 +166,6 @@ class DonutSwinModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = True\n \n     test_resize_embeddings = False\n "
        },
        {
            "sha": "4ec9e16c4db202dbcee7920db5e163389103312d",
            "filename": "tests/models/edgetam/test_modeling_edgetam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fedgetam%2Ftest_modeling_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fedgetam%2Ftest_modeling_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fedgetam%2Ftest_modeling_edgetam.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -234,7 +234,6 @@ class EdgeTamModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     pipeline_model_mapping = (\n         {\"feature-extraction\": EdgeTamModel, \"mask-generation\": EdgeTamModel} if is_torch_available() else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     _is_composite = True"
        },
        {
            "sha": "6eab2ca2d29b1dbbaa3913cdcfd9158b02da98c5",
            "filename": "tests/models/efficientnet/test_modeling_efficientnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -134,8 +134,6 @@ class EfficientNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Test\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "7bf778aa4d9edcb7df0b065e2057bb5626364bff",
            "filename": "tests/models/electra/test_modeling_electra.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -405,7 +405,6 @@ class ElectraModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False  # won't be maintained\n \n     # Overwriting to add `is_decoder` flag\n     def prepare_config_and_inputs_for_generate(self, batch_size=2):"
        },
        {
            "sha": "0feaddb0231897a37618684113bff1e4778ac48f",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -131,8 +131,6 @@ class Emu3Text2TextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTe\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     def setUp(self):\n         self.model_tester = Emu3Text2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Emu3TextConfig, hidden_size=37)\n@@ -288,8 +286,6 @@ class Emu3Vision2TextModelTest(ModelTesterMixin, GenerationTesterMixin, Pipeline\n     )\n     pipeline_model_mapping = {}\n \n-    fx_compatible = False\n-\n     def setUp(self):\n         self.model_tester = Emu3Vision2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Emu3Config, has_text_modality=False, hidden_size=37)"
        },
        {
            "sha": "6ca4d53eeea1c9d60a19155ee935cd4db5505aa9",
            "filename": "tests/models/ernie/test_modeling_ernie.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -455,7 +455,6 @@ class ErnieModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     # Overwriting to add `is_decoder` flag\n     def prepare_config_and_inputs_for_generate(self, batch_size=2):"
        },
        {
            "sha": "7339a9d5c89246c0640ca30e30dff2c1548627da",
            "filename": "tests/models/ernie4_5/test_modeling_ernie4_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -45,7 +45,6 @@ class Ernie4_5ModelTester(CausalLMModelTester):\n \n @require_torch\n class Ernie4_5ModelTest(CausalLMModelTest, unittest.TestCase):\n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = Ernie4_5ModelTester\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "3bef0a07cae1b2ca78ae9ef7de52b701574b95db",
            "filename": "tests/models/exaone4/test_modeling_exaone4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -52,7 +52,6 @@ class Exaone4ModelTester(CausalLMModelTester):\n \n @require_torch\n class Exaone4ModelTest(CausalLMModelTest, unittest.TestCase):\n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = Exaone4ModelTester\n     model_split_percents = [0.5, 0.6]\n "
        },
        {
            "sha": "966580496c98f09996b9519778f7437b77ef212d",
            "filename": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -260,8 +260,6 @@ def create_and_check_decoder_model_past_large_inputs(\n class FalconH1ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (FalconH1Model, FalconH1ForCausalLM) if is_torch_available() else ()\n \n-    fx_compatible = False\n-\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "f2e042c117483d3dc9c78cdcf8478d62f085b4a4",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -263,7 +263,6 @@ def prepare_config_and_inputs_for_common(self):\n class FalconMambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (FalconMambaModel, FalconMambaForCausalLM) if is_torch_available() else ()\n     has_attentions = False  # FalconMamba does not support attentions\n-    fx_compatible = False  # FIXME let's try to support this @ArthurZucker\n     test_missing_keys = False\n \n     pipeline_model_mapping = ("
        },
        {
            "sha": "6f5f27bc4b6f383e023e666f10138c9925776350",
            "filename": "tests/models/flex_olmo/test_modeling_flex_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -47,7 +47,6 @@ class FlexOlmoModelTester(CausalLMModelTester):\n \n @require_torch\n class FlexOlmoModelTest(CausalLMModelTest, unittest.TestCase):\n-    fx_compatible = False\n     test_all_params_have_gradient = False\n     model_tester_class = FlexOlmoModelTester\n "
        },
        {
            "sha": "b39c87721bcf89adfaf1c8cd4d3a0b68fff70dcf",
            "filename": "tests/models/focalnet/test_modeling_focalnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ffocalnet%2Ftest_modeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ffocalnet%2Ftest_modeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffocalnet%2Ftest_modeling_focalnet.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -241,7 +241,6 @@ class FocalNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     has_attentions = False"
        },
        {
            "sha": "19831b427181a9f3353cfa3037d8ee1cec2f1649",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -125,7 +125,6 @@ class GitVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (GitVisionModel,) if is_torch_available() else ()\n-    fx_compatible = True\n \n     test_resize_embeddings = False\n \n@@ -381,7 +380,6 @@ class GitModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     # special case for GitForCausalLM model\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "d4cebbd02983ab9d62e86437c60e0f76c117fe26",
            "filename": "tests/models/glm4_moe/test_modeling_glm4_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -58,7 +58,6 @@ def __init__(\n \n @require_torch\n class Glm4MoeModelTest(CausalLMModelTest, unittest.TestCase):\n-    fx_compatible = False\n     model_tester_class = Glm4MoeModelTester\n     # used in `test_torch_compile_for_training`. Skip as \"Dynamic control flow in MoE\"\n     _torch_compile_train_cls = None"
        },
        {
            "sha": "af045509a9a4d1596197267b9187e25b6ef985b2",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -169,7 +169,6 @@ class GPT2ModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     test_missing_keys = False\n     model_tester_class = GPT2ModelTester\n "
        },
        {
            "sha": "cfd56f199cf31e17efeec68f72753d4e204d3356",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -393,7 +393,6 @@ class GPTBigCodeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n     test_missing_keys = False\n \n     multi_query = True"
        },
        {
            "sha": "193f05d56d01cb48ad4866941010af166764a4a4",
            "filename": "tests/models/gpt_neo/test_modeling_gpt_neo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -375,7 +375,6 @@ class GPTNeoModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = True\n     test_missing_keys = False\n \n     # special case for DoubleHeads model"
        },
        {
            "sha": "3f6bab0a827f4581a5ec54a40a88814568182726",
            "filename": "tests/models/gptj/test_modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -340,16 +340,8 @@ class GPTJModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = True\n-\n     test_missing_keys = False\n \n-    def test_torch_fx(self):\n-        super().test_torch_fx()\n-\n-    def test_torch_fx_output_loss(self):\n-        super().test_torch_fx_output_loss()\n-\n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n         self,"
        },
        {
            "sha": "d02923f5fe3d698faedeb039dfb3801f92895259",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -175,8 +175,6 @@ class GraniteModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "96ec52e5b2c2391b1a7cd40c729412afd5f257c6",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -174,8 +174,6 @@ class GraniteMoeModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.Test\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "0deca05a7eb553fa2bf206b4301b2eed0cd06197",
            "filename": "tests/models/granitemoeshared/test_modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -177,8 +177,6 @@ class GraniteMoeSharedModelTest(ModelTesterMixin, GenerationTesterMixin, unittes\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "5f7301c3bd5a2f48cf18e13a828e2efbf9a849f9",
            "filename": "tests/models/hgnet_v2/test_modeling_hgnet_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fhgnet_v2%2Ftest_modeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fhgnet_v2%2Ftest_modeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhgnet_v2%2Ftest_modeling_hgnet_v2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -178,8 +178,6 @@ class HGNetV2ForImageClassificationTest(ModelTesterMixin, PipelineTesterMixin, u\n     all_model_classes = (HGNetV2ForImageClassification, HGNetV2Backbone) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-classification\": HGNetV2ForImageClassification} if is_torch_available() else {}\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     test_torch_exportable = True\n     has_attentions = False"
        },
        {
            "sha": "f945b756936ed0c09e24188ec243fb715d98e8f0",
            "filename": "tests/models/hiera/test_modeling_hiera.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -243,7 +243,6 @@ class HieraModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = True\n \n     test_resize_embeddings = False\n     test_torch_exportable = True"
        },
        {
            "sha": "f47d20239f2afa02abe2d966c481628852519214",
            "filename": "tests/models/hubert/test_modeling_hubert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 121,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -14,9 +14,6 @@\n \"\"\"Testing suite for the PyTorch Hubert model.\"\"\"\n \n import math\n-import os\n-import pickle\n-import tempfile\n import unittest\n \n import pytest\n@@ -27,7 +24,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -47,8 +43,6 @@\n     )\n     from transformers.models.hubert.modeling_hubert import _compute_mask_indices\n \n-from transformers.utils.fx import symbolic_trace\n-\n \n class HubertModelTester:\n     def __init__(\n@@ -312,7 +306,6 @@ class HubertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = True\n \n     def setUp(self):\n         self.model_tester = HubertModelTester(self)\n@@ -406,120 +399,6 @@ def test_retain_grad_hidden_states_attentions(self):\n         self.assertIsNotNone(hidden_states.grad)\n         self.assertIsNotNone(attentions.grad)\n \n-    # Hubert cannot be TorchScripted because of torch.nn.utils.weight_norm\n-    def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=False):\n-        # TODO: fix it\n-        self.skipTest(reason=\"torch 2.1 breaks torch fx tests for wav2vec2/hubert.\")\n-\n-        if not self.fx_compatible:\n-            self.skipTest(reason=\"torch fx is not compatible with this model\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.return_dict = False\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=output_loss)\n-\n-            try:\n-                if model.config.is_encoder_decoder:\n-                    model.config.use_cache = False  # FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward\n-                    labels = inputs.get(\"labels\", None)\n-                    input_names = [\n-                        \"attention_mask\",\n-                        \"decoder_attention_mask\",\n-                        \"decoder_input_ids\",\n-                        \"input_features\",\n-                        \"input_ids\",\n-                        \"input_values\",\n-                    ]\n-                    if labels is not None:\n-                        input_names.append(\"labels\")\n-\n-                    filtered_inputs = {k: v for (k, v) in inputs.items() if k in input_names}\n-                    input_names = list(filtered_inputs.keys())\n-\n-                    model_output = model(**filtered_inputs)\n-\n-                    traced_model = symbolic_trace(model, input_names)\n-                    traced_output = traced_model(**filtered_inputs)\n-                else:\n-                    input_names = [\n-                        \"attention_mask\",\n-                        \"bbox\",\n-                        \"input_features\",\n-                        \"input_ids\",\n-                        \"input_values\",\n-                        \"pixel_values\",\n-                        \"token_type_ids\",\n-                        \"visual_feats\",\n-                        \"visual_pos\",\n-                    ]\n-\n-                    labels = inputs.get(\"labels\", None)\n-                    start_positions = inputs.get(\"start_positions\", None)\n-                    end_positions = inputs.get(\"end_positions\", None)\n-                    if labels is not None:\n-                        input_names.append(\"labels\")\n-                    if start_positions is not None:\n-                        input_names.append(\"start_positions\")\n-                    if end_positions is not None:\n-                        input_names.append(\"end_positions\")\n-\n-                    filtered_inputs = {k: v for (k, v) in inputs.items() if k in input_names}\n-                    input_names = list(filtered_inputs.keys())\n-\n-                    model_output = model(**filtered_inputs)\n-\n-                    traced_model = symbolic_trace(model, input_names)\n-                    traced_output = traced_model(**filtered_inputs)\n-\n-            except Exception as e:\n-                self.fail(f\"Couldn't trace module: {e}\")\n-\n-            def flatten_output(output):\n-                flatten = []\n-                for x in output:\n-                    if isinstance(x, (tuple, list)):\n-                        flatten += flatten_output(x)\n-                    elif not isinstance(x, torch.Tensor):\n-                        continue\n-                    else:\n-                        flatten.append(x)\n-                return flatten\n-\n-            model_output = flatten_output(model_output)\n-            traced_output = flatten_output(traced_output)\n-            num_outputs = len(model_output)\n-\n-            for i in range(num_outputs):\n-                self.assertTrue(\n-                    torch.allclose(model_output[i], traced_output[i]),\n-                    f\"traced {i}th output doesn't match model {i}th output for {model_class}\",\n-                )\n-\n-            # Test that the model can be serialized and restored properly\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pkl_file_name = os.path.join(tmp_dir_name, \"model.pkl\")\n-                try:\n-                    with open(pkl_file_name, \"wb\") as f:\n-                        pickle.dump(traced_model, f)\n-                    with open(pkl_file_name, \"rb\") as f:\n-                        loaded = pickle.load(f)\n-                except Exception as e:\n-                    self.fail(f\"Couldn't serialize / deserialize the traced model: {e}\")\n-\n-                loaded_output = loaded(**filtered_inputs)\n-                loaded_output = flatten_output(loaded_output)\n-\n-                for i in range(num_outputs):\n-                    self.assertTrue(\n-                        torch.allclose(model_output[i], loaded_output[i]),\n-                        f\"serialized model {i}th output doesn't match model {i}th output for {model_class}\",\n-                    )\n-\n     # overwrite from test_modeling_common\n     def _mock_init_weights(self, module):\n         if hasattr(module, \"weight\") and module.weight is not None:"
        },
        {
            "sha": "a345f5f375748436145385bf6f80a852e575b80b",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -177,7 +177,6 @@ class Idefics2ModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (Idefics2Model,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     _is_composite = True\n@@ -368,7 +367,6 @@ class Idefics2ForConditionalGenerationModelTest(GenerationTesterMixin, ModelTest\n \n     all_model_classes = (Idefics2ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": Idefics2ForConditionalGeneration} if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n "
        },
        {
            "sha": "4642444be23727dcddbfb793ed18b5274bff9e93",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -167,7 +167,6 @@ class Idefics3ModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (Idefics3Model,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n \n@@ -333,7 +332,6 @@ class Idefics3ForConditionalGenerationModelTest(GenerationTesterMixin, ModelTest\n \n     all_model_classes = (Idefics3ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": Idefics3ForConditionalGeneration} if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n "
        },
        {
            "sha": "bf52907d587d4b81be93dbb4cb87571ef63cf720",
            "filename": "tests/models/ijepa/test_modeling_ijepa.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -201,7 +201,6 @@ class IJepaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False  # broken by output recording refactor\n \n     test_resize_embeddings = False\n     test_torch_exportable = True"
        },
        {
            "sha": "4882c14dba362232706fa948727bcab4f52787e2",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -149,7 +149,6 @@ class InstructBlipVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (InstructBlipVisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -475,7 +474,6 @@ class InstructBlipForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, Gene\n     )\n     pipeline_model_mapping = {\"image-text-to-text\": InstructBlipForConditionalGeneration}\n     additional_model_inputs = [\"qformer_input_ids\", \"input_ids\"]\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     test_attention_outputs = False"
        },
        {
            "sha": "747469de3ffaf617851e813a4488a186685120f9",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -153,7 +153,6 @@ class InstructBlipVideoVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (InstructBlipVideoVisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -490,7 +489,6 @@ class InstructBlipVideoForConditionalGenerationDecoderOnlyTest(\n         (InstructBlipVideoForConditionalGeneration, InstructBlipVideoModel) if is_torch_available() else ()\n     )\n     additional_model_inputs = [\"qformer_input_ids\", \"input_ids\"]\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     test_attention_outputs = False"
        },
        {
            "sha": "76e49179fdb5bcfd87808ef79ad963d95dda9b47",
            "filename": "tests/models/janus/test_modeling_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -193,7 +193,6 @@ def prepare_config_and_inputs_for_common(self):\n class JanusVisionText2TextModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (JanusModel, JanusForConditionalGeneration) if is_torch_available() else ()\n     all_generative_model_classes = (JanusForConditionalGeneration,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     _is_composite = True\n \n@@ -354,7 +353,6 @@ def prepare_config_and_inputs_for_common(self):\n class JanusVQModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (JanusVQVAE,) if is_torch_available() else ()\n \n-    fx_compatible = False\n     has_attentions = False\n     test_resize_embeddings = False\n "
        },
        {
            "sha": "b63076e8f2b4fb8b86f12c6749ab8f41b160851c",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -268,7 +268,6 @@ class Kosmos2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "ac8be1982721d567ab940a166b95bff63f28dca3",
            "filename": "tests/models/kosmos2_5/test_modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -300,7 +300,6 @@ class Kosmos2_5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "83ff0d9ec223b77e0f9b7bd1246bea710e39fd3a",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -253,8 +253,6 @@ class KyutaiSpeechToTextModelTest(ModelTesterMixin, GenerationTesterMixin, Pipel\n         else {}\n     )\n \n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n-\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "6c3d45bd2acb7ce04dcec01ab37913534137b295",
            "filename": "tests/models/layoutlm/test_modeling_layoutlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_layoutlm.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -243,7 +243,6 @@ class LayoutLMModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False  # Cannot support if `can_return_tuple`\n \n     def setUp(self):\n         self.model_tester = LayoutLMModelTester(self)"
        },
        {
            "sha": "d3c27409562b46aeaca4848e4ce913aa3abfb011",
            "filename": "tests/models/lfm2/test_modeling_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -49,7 +49,6 @@ def __init__(\n \n @require_torch\n class Lfm2ModelTest(CausalLMModelTest, unittest.TestCase):\n-    fx_compatible = False\n     model_tester_class = Lfm2ModelTester\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = Lfm2ForCausalLM if is_torch_available() else None"
        },
        {
            "sha": "55a9f4e40c197940035547c089eb1cc1425bafcd",
            "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -62,7 +62,6 @@ class Lfm2MoeModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n     model_tester_class = Lfm2MoeModelTester\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = Lfm2MoeForCausalLM if is_torch_available() else None"
        },
        {
            "sha": "906a7aa54e6a50820b09c14d86d62088f4cbc5f4",
            "filename": "tests/models/lfm2_vl/test_modeling_lfm2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -162,7 +162,6 @@ class Lfm2VlModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase\n         else {}\n     )\n \n-    fx_compatible = False\n     model_tester_class = Lfm2VlModelTester\n     _is_composite = True\n "
        },
        {
            "sha": "33230c0128ded0235192da3ef264636597494338",
            "filename": "tests/models/lilt/test_modeling_lilt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -238,7 +238,6 @@ class LiltModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "b4e1e2b1c6728a6222f07b9a504b258f664643a6",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -51,7 +51,6 @@ class LlamaModelTester(CausalLMModelTester):\n \n @require_torch\n class LlamaModelTest(CausalLMModelTest, unittest.TestCase):\n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = LlamaModelTester\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "25b769e715b723e6fe06a826cd5e47e6cbfba066",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -506,7 +506,6 @@ class LongT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     is_encoder_decoder = True"
        },
        {
            "sha": "bdcd2f5ab33b99f4205720651e0596ed0933ff0a",
            "filename": "tests/models/lxmert/test_modeling_lxmert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -529,8 +529,6 @@ class LxmertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    fx_compatible = True\n-\n     # overwrite function because qa models takes different input label shape\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         inputs_dict = copy.deepcopy(inputs_dict)"
        },
        {
            "sha": "718b5cca2956ef08c16cc6553bca532ef18d42ec",
            "filename": "tests/models/m2m_100/test_modeling_m2m_100.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -237,7 +237,6 @@ class M2M100ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = False\n     test_missing_keys = False\n \n     # TODO: Fix the failed tests"
        },
        {
            "sha": "99a20aea5eecaa5afbe91884af31aed2aee66ac1",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -235,7 +235,6 @@ def prepare_config_and_inputs_for_common(self):\n class MambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (MambaModel, MambaForCausalLM) if is_torch_available() else ()\n     has_attentions = False  # Mamba does not support attentions\n-    fx_compatible = False  # FIXME let's try to support this @ArthurZucker\n     test_missing_keys = False\n \n     pipeline_model_mapping = ("
        },
        {
            "sha": "be1d0a351034a8c719d95be5f7b08800f34fa24d",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -238,7 +238,6 @@ def create_and_check_mamba2_slow_vs_fast_forward(self, config, input_ids, *args,\n class Mamba2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Mamba2Model, Mamba2ForCausalLM) if is_torch_available() else ()\n     has_attentions = False  # Mamba does not support attentions\n-    fx_compatible = False  # FIXME let's try to support this @molbap\n     test_missing_keys = False\n \n     pipeline_model_mapping = ("
        },
        {
            "sha": "b897cb76c6d8f0d1987a4ab3510862c7ca8b588d",
            "filename": "tests/models/marian/test_modeling_marian.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -229,7 +229,6 @@ class MarianModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = False\n     test_missing_keys = False\n \n     def setUp(self):"
        },
        {
            "sha": "e73077dd93effd10d83775aeab97073103db09c2",
            "filename": "tests/models/maskformer/test_modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer_swin.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -174,7 +174,6 @@ class MaskFormerSwinModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Te\n         else ()\n     )\n     pipeline_model_mapping = {\"feature-extraction\": MaskFormerSwinModel} if is_torch_available() else {}\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_torch_exportable = True"
        },
        {
            "sha": "73c28e9ed573c7a30914ddc29aa0e3abf7159054",
            "filename": "tests/models/mbart/test_modeling_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -231,7 +231,6 @@ class MBartModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = False  # Fix me Michael\n \n     test_missing_keys = False\n "
        },
        {
            "sha": "a2a53f7281853f4ebd73e67a148049d7776149f8",
            "filename": "tests/models/megatron_bert/test_modeling_megatron_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmegatron_bert%2Ftest_modeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmegatron_bert%2Ftest_modeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmegatron_bert%2Ftest_modeling_megatron_bert.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -288,7 +288,6 @@ class MegatronBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Test\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = True\n     # test_resize_embeddings = False\n \n     # special case for ForPreTraining model"
        },
        {
            "sha": "40e17e652098014a195dfdac66d5fbe7f543659c",
            "filename": "tests/models/metaclip_2/test_modeling_metaclip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -207,7 +207,6 @@ class MetaClip2VisionModelTest(MetaClip2ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (MetaClip2VisionModel, MetaClip2VisionModelWithProjection) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -402,7 +401,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class MetaClip2TextModelTest(MetaClip2ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (MetaClip2TextModel, MetaClip2TextModelWithProjection) if is_torch_available() else ()\n-    fx_compatible = False\n \n     model_split_percents = [0.5, 0.8, 0.9]\n \n@@ -534,7 +532,6 @@ class MetaClip2ModelTest(MetaClip2ModelTesterMixin, PipelineTesterMixin, unittes\n         else {}\n     )\n     additional_model_inputs = [\"pixel_values\"]\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False\n@@ -635,7 +632,6 @@ def prepare_config_and_inputs_for_common(self):\n class MetaClip2ForImageClassificationModelTest(MetaClip2ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (MetaClip2ForImageClassification,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-classification\": MetaClip2ForImageClassification} if is_torch_available() else {}\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "1ceef103dfac9ffa864a2fc181c2343d8f84cc55",
            "filename": "tests/models/mgp_str/test_modeling_mgp_str.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmgp_str%2Ftest_modeling_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmgp_str%2Ftest_modeling_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmgp_str%2Ftest_modeling_mgp_str.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -122,7 +122,6 @@ class MgpstrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "95bf2b654379e4a58c70e98839a4861413fa0c89",
            "filename": "tests/models/mobilebert/test_modeling_mobilebert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -283,7 +283,6 @@ class MobileBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False  # won't be maintained\n \n     # special case for ForPreTraining model\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "a07b7010f9938219e62bad75570bebeaa8045ec4",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -260,7 +260,6 @@ class ModernBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     model_split_percents = [0.5, 0.8, 0.9]\n "
        },
        {
            "sha": "61aeeb5d7ce514f38e8dece2ddba2348ab07eb40",
            "filename": "tests/models/mpt/test_modeling_mpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -350,7 +350,6 @@ class MptModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         else ()\n     )\n \n-    fx_compatible = False\n     test_missing_keys = False\n \n     pipeline_model_mapping = ("
        },
        {
            "sha": "b5fd56813845b15e582604aee5743c003632b994",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 119,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -13,25 +13,21 @@\n # limitations under the License.\n \n import copy\n-import os\n-import pickle\n import tempfile\n import unittest\n \n from transformers import MT5Config, is_torch_available\n-from transformers.models.auto.modeling_auto import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n from transformers.testing_utils import (\n     require_sentencepiece,\n     require_tokenizers,\n     require_torch,\n     slow,\n     torch_device,\n )\n-from transformers.utils.fx import symbolic_trace\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -557,7 +553,6 @@ class MT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = True\n \n     test_resize_embeddings = True\n     is_encoder_decoder = True\n@@ -587,119 +582,6 @@ def is_pipeline_test_to_skip(\n \n         return False\n \n-    def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=False):\n-        if not self.fx_compatible:\n-            self.skipTest(reason=\"torch.fx is not compatible with this model\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.return_dict = False\n-\n-        for model_class in self.all_model_classes:\n-            if model_class.__name__ == \"MT5ForSequenceClassification\":\n-                continue\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=output_loss)\n-\n-            try:\n-                if model.config.is_encoder_decoder:\n-                    model.config.use_cache = False  # FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward\n-                    labels = inputs.get(\"labels\", None)\n-                    input_names = [\n-                        \"attention_mask\",\n-                        \"decoder_attention_mask\",\n-                        \"decoder_input_ids\",\n-                        \"input_features\",\n-                        \"input_ids\",\n-                        \"input_values\",\n-                    ]\n-                    if labels is not None:\n-                        input_names.append(\"labels\")\n-                    filtered_inputs = {k: v for (k, v) in inputs.items() if k in input_names}\n-                    input_names = list(filtered_inputs.keys())\n-                    model_output = model(**filtered_inputs)\n-                    traced_model = symbolic_trace(model, input_names)\n-                    traced_output = traced_model(**filtered_inputs)\n-                else:\n-                    input_names = [\n-                        \"attention_mask\",\n-                        \"bbox\",\n-                        \"input_features\",\n-                        \"input_ids\",\n-                        \"input_values\",\n-                        \"pixel_values\",\n-                        \"token_type_ids\",\n-                        \"visual_feats\",\n-                        \"visual_pos\",\n-                    ]\n-                    labels = inputs.get(\"labels\", None)\n-                    start_positions = inputs.get(\"start_positions\", None)\n-                    end_positions = inputs.get(\"end_positions\", None)\n-                    if labels is not None:\n-                        input_names.append(\"labels\")\n-                    if start_positions is not None:\n-                        input_names.append(\"start_positions\")\n-                    if end_positions is not None:\n-                        input_names.append(\"end_positions\")\n-                    filtered_inputs = {k: v for (k, v) in inputs.items() if k in input_names}\n-                    input_names = list(filtered_inputs.keys())\n-                    if model.__class__.__name__ in set(MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES.values()) and (\n-                        not hasattr(model.config, \"problem_type\") or model.config.problem_type is None\n-                    ):\n-                        model.config.problem_type = \"single_label_classification\"\n-                    traced_model = symbolic_trace(model, input_names)\n-                    traced_output = traced_model(**filtered_inputs)\n-                    model_output = model(**filtered_inputs)\n-\n-            except Exception as e:\n-                self.fail(f\"Couldn't trace module: {e}\")\n-\n-            def flatten_output(output):\n-                flatten = []\n-                for x in output:\n-                    if isinstance(x, (tuple, list)):\n-                        flatten += flatten_output(x)\n-                    elif not isinstance(x, torch.Tensor):\n-                        continue\n-                    else:\n-                        flatten.append(x)\n-                return flatten\n-\n-            model_output = flatten_output(model_output)\n-            traced_output = flatten_output(traced_output)\n-            num_outputs = len(model_output)\n-\n-            for i in range(num_outputs):\n-                self.assertTrue(\n-                    torch.allclose(model_output[i], traced_output[i]),\n-                    f\"traced {i}th output doesn't match model {i}th output for {model_class}\",\n-                )\n-\n-            # Test that the model can be serialized and restored properly\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pkl_file_name = os.path.join(tmp_dir_name, \"model.pkl\")\n-                try:\n-                    with open(pkl_file_name, \"wb\") as f:\n-                        pickle.dump(traced_model, f)\n-                    with open(pkl_file_name, \"rb\") as f:\n-                        loaded = pickle.load(f)\n-                except Exception as e:\n-                    self.fail(f\"Couldn't serialize / deserialize the traced model: {e}\")\n-\n-                loaded_output = loaded(**filtered_inputs)\n-                loaded_output = flatten_output(loaded_output)\n-\n-                for i in range(num_outputs):\n-                    self.assertTrue(\n-                        torch.allclose(model_output[i], loaded_output[i]),\n-                        f\"serialized model {i}th output doesn't match model {i}th output for {model_class}\",\n-                    )\n-\n-            # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n-            # (Even with this call, there are still memory leak by ~0.04MB)\n-            self.clear_torch_jit_class_registry()\n-\n     # overwrite because MT5 doesn't accept position ids as input and expects `decoder_input_ids`\n     def test_custom_4d_attention_mask(self):\n         for model_class in self.all_generative_model_classes:"
        },
        {
            "sha": "e50039d68fe6e40f57a605ec1149e53e58c63a52",
            "filename": "tests/models/mvp/test_modeling_mvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -421,7 +421,6 @@ class MvpModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = False\n \n     test_missing_keys = False\n \n@@ -789,8 +788,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class MvpStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (MvpDecoder, MvpForCausalLM) if is_torch_available() else ()\n-    fx_comptatible = True\n-\n     is_encoder_decoder = False\n \n     def setUp("
        },
        {
            "sha": "f1fa2d66f4220789b3d990db88a6207d1dd601af",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -50,7 +50,6 @@ class NemotronModelTest(CausalLMModelTest, unittest.TestCase):\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]\n-    fx_compatible = False\n \n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = NemotronForCausalLM if is_torch_available() else None"
        },
        {
            "sha": "2040b5ca435a8a16232867250ecc4707f6af2396",
            "filename": "tests/models/nllb_moe/test_modeling_nllb_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -242,7 +242,6 @@ class NllbMoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = False\n \n     test_missing_keys = True\n "
        },
        {
            "sha": "22fc890b1e929460765814f04e7b5868f87c2709",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -172,8 +172,6 @@ class OlmoModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "cf9c84b950a604b0c818801b5bf27e71321b1364",
            "filename": "tests/models/olmo2/test_modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -173,8 +173,6 @@ class Olmo2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "d892509a42b90bb5d6f3574accbdaf62d75a31e6",
            "filename": "tests/models/olmo3/test_modeling_olmo3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -52,7 +52,6 @@ class Olmo3ModelTester(CausalLMModelTester):\n \n @require_torch\n class Olmo3ModelTest(CausalLMModelTest, unittest.TestCase):\n-    fx_compatible = False\n     test_all_params_have_gradient = False\n     model_tester_class = Olmo3ModelTester\n "
        },
        {
            "sha": "9c56629089d7852a5911349e08dae73ba08cc84d",
            "filename": "tests/models/olmoe/test_modeling_olmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -183,8 +183,6 @@ class OlmoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "d195385ecdd51575f8a1ae0423467b37c71b4a7c",
            "filename": "tests/models/opt/test_modeling_opt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -215,7 +215,6 @@ class OPTModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         else {}\n     )\n     is_encoder_decoder = False\n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n \n     test_missing_keys = False\n "
        },
        {
            "sha": "2a14a35cefa39ebb23d535c1a89c5cfef7265156",
            "filename": "tests/models/owlv2/test_modeling_owlv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -141,7 +141,6 @@ class Owlv2VisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (Owlv2VisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -304,7 +303,6 @@ def prepare_config_and_inputs_for_common(self):\n # Copied from tests.models.owlvit.test_modeling_owlvit.OwlViTTextModelTest with OwlViT->Owlv2, OWL-ViT->OwlV2, OWLVIT->OWLV2, owlvit-base-patch32->owlv2-base-patch16-ensemble\n class Owlv2TextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Owlv2TextModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     def setUp(self):\n         self.model_tester = Owlv2TextModelTester(self)\n@@ -421,7 +419,6 @@ class Owlv2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False\n@@ -546,7 +543,6 @@ def prepare_config_and_inputs_for_common(self):\n # Copied from tests.models.owlvit.test_modeling_owlvit.OwlViTForObjectDetectionTest with OwlViT->Owlv2, OWL-ViT->OwlV2, OWLVIT->OWLV2, owlvit-base-patch32->owlv2-base-patch16-ensemble\n class Owlv2ForObjectDetectionTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Owlv2ForObjectDetection,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "5ca710cf66b2295f507252bdaec219920a398fe7",
            "filename": "tests/models/owlvit/test_modeling_owlvit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -139,7 +139,6 @@ class OwlViTVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (OwlViTVisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -300,7 +299,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class OwlViTTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (OwlViTTextModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     def setUp(self):\n         self.model_tester = OwlViTTextModelTester(self)\n@@ -416,7 +414,6 @@ class OwlViTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False\n@@ -539,7 +536,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class OwlViTForObjectDetectionTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (OwlViTForObjectDetection,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "efc663f9bb71e583e19f8b8c07bbc9dd1a3a4d4c",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -190,7 +190,6 @@ class PaliGemmaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n     )\n     pipeline_model_mapping = {\"image-text-to-text\": PaliGemmaForConditionalGeneration}\n     additional_model_inputs = [\"token_type_ids\"]\n-    fx_compatible = False\n \n     _is_composite = True\n "
        },
        {
            "sha": "a000d68d20a148d19fcf6546f48ad7d13d249b76",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -169,7 +169,6 @@ class PaliGemma2ForConditionalGenerationModelTest(ModelTesterMixin, GenerationTe\n \n     all_model_classes = (PaliGemmaForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": PaliGemmaForConditionalGeneration}\n-    fx_compatible = False\n \n     _is_composite = True\n "
        },
        {
            "sha": "1289acefd315ede3362c20369fefcc127f23ed7e",
            "filename": "tests/models/pegasus/test_modeling_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -234,7 +234,6 @@ class PegasusModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = False\n     test_resize_position_embeddings = True\n \n     test_missing_keys = False"
        },
        {
            "sha": "04bbcd28c5be55fc1e9bba3b66362e2c5936d3bd",
            "filename": "tests/models/pix2struct/test_modeling_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -142,7 +142,6 @@ class Pix2StructVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (Pix2StructVisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -310,7 +309,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Pix2StructTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Pix2StructTextModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     def setUp(self):\n         self.model_tester = Pix2StructTextModelTester(self)\n@@ -408,7 +406,6 @@ class Pix2StructModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     test_attention_outputs = False"
        },
        {
            "sha": "5da6225b03d522d094a1c7bd612545763f982894",
            "filename": "tests/models/plbart/test_modeling_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -225,7 +225,6 @@ class PLBartModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = False  # Fix me Michael\n \n     test_missing_keys = False\n "
        },
        {
            "sha": "3177df3ca89cdb63d74c05013412bccc68a8f38c",
            "filename": "tests/models/pop2piano/test_modeling_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -489,7 +489,6 @@ class Pop2PianoModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n     pipeline_model_mapping = (\n         {\"automatic-speech-recognition\": Pop2PianoForConditionalGeneration} if is_torch_available() else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     is_encoder_decoder = True"
        },
        {
            "sha": "b8fb99916e42730de9cca0497684ddbc03b61b89",
            "filename": "tests/models/resnet/test_modeling_resnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fresnet%2Ftest_modeling_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fresnet%2Ftest_modeling_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fresnet%2Ftest_modeling_resnet.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -172,8 +172,6 @@ class ResNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    fx_compatible = True\n-\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "e381631ea839b8b653cc59ac6150dd4e4f16de2b",
            "filename": "tests/models/roberta/test_modeling_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -393,7 +393,6 @@ class RobertaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False  # won't be maintained\n     model_split_percents = [0.5, 0.8, 0.9]\n \n     # Overwriting to add `is_decoder` flag"
        },
        {
            "sha": "9394d3a8dcfd248943492197bba48a17766e4253",
            "filename": "tests/models/roberta_prelayernorm/test_modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -390,7 +390,6 @@ class RobertaPreLayerNormModelTest(ModelTesterMixin, GenerationTesterMixin, Pipe\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n     model_split_percents = [0.5, 0.8, 0.9]\n \n     # Overwriting to add `is_decoder` flag"
        },
        {
            "sha": "e923183c51301365c7462314947a23d3cdcab501",
            "filename": "tests/models/rwkv/test_modeling_rwkv.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -215,7 +215,6 @@ class RwkvModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     pipeline_model_mapping = (\n         {\"feature-extraction\": RwkvModel, \"text-generation\": RwkvForCausalLM} if is_torch_available() else {}\n     )\n-    fx_compatible = False\n     test_missing_keys = False\n \n     def setUp(self):"
        },
        {
            "sha": "935126f57907faa6cc912a77c4a3161a23b56416",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -158,7 +158,6 @@ class SamVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (SamVisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_torch_exportable = True\n@@ -512,7 +511,6 @@ class SamModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     pipeline_model_mapping = (\n         {\"feature-extraction\": SamModel, \"mask-generation\": SamModel} if is_torch_available() else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     _is_composite = True"
        },
        {
            "sha": "15dd3039e76a172ae7ce51d289b114e35502cad3",
            "filename": "tests/models/sam2/test_modeling_sam2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -141,7 +141,6 @@ class Sam2VisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (Sam2VisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_torch_exportable = True\n@@ -463,7 +462,6 @@ class Sam2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     pipeline_model_mapping = (\n         {\"feature-extraction\": Sam2Model, \"mask-generation\": Sam2Model} if is_torch_available() else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     _is_composite = True"
        },
        {
            "sha": "08e95a9ad8a8f0c019047f45fe642190928f27c0",
            "filename": "tests/models/sam_hq/test_modeling_sam_hq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -166,7 +166,6 @@ class SamHQVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (SamHQVisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_torch_exportable = True\n@@ -544,7 +543,6 @@ class SamHQModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     pipeline_model_mapping = (\n         {\"feature-extraction\": SamHQModel, \"mask-generation\": SamHQModel} if is_torch_available() else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_cpu_offload = False"
        },
        {
            "sha": "95f858df2f5b60f359ac74cf4126f41f67ba7910",
            "filename": "tests/models/seamless_m4t/test_modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -336,7 +336,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class SeamlessM4TModelWithSpeechInputTest(ModelTesterMixin, unittest.TestCase):\n     is_encoder_decoder = True\n-    fx_compatible = False\n     test_missing_keys = False\n \n     test_resize_embeddings = False\n@@ -529,7 +528,6 @@ def test_retain_grad_hidden_states_attentions(self):\n @require_torch\n class SeamlessM4TModelWithTextInputTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     is_encoder_decoder = True\n-    fx_compatible = False\n     test_missing_keys = False\n \n     test_resize_embeddings = True"
        },
        {
            "sha": "3c68901fe98a964d59395b633067e3cd985ac6c8",
            "filename": "tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -362,7 +362,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class SeamlessM4Tv2ModelWithSpeechInputTest(ModelTesterMixin, unittest.TestCase):\n     is_encoder_decoder = True\n-    fx_compatible = False\n     test_missing_keys = False\n \n     test_resize_embeddings = False\n@@ -554,7 +553,6 @@ def test_retain_grad_hidden_states_attentions(self):\n @require_torch\n class SeamlessM4Tv2ModelWithTextInputTest(ModelTesterMixin, unittest.TestCase):\n     is_encoder_decoder = True\n-    fx_compatible = False\n     test_missing_keys = False\n \n     test_resize_embeddings = True"
        },
        {
            "sha": "b455c42bbb81475ae48510ece9e853bacf2d7525",
            "filename": "tests/models/segformer/test_modeling_segformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsegformer%2Ftest_modeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsegformer%2Ftest_modeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsegformer%2Ftest_modeling_segformer.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -175,8 +175,6 @@ class SegformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n         else {}\n     )\n \n-    fx_compatible = True\n-\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "88dccbaa7ab5b641276568ea215db8db96a0f184",
            "filename": "tests/models/seggpt/test_modeling_seggpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fseggpt%2Ftest_modeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fseggpt%2Ftest_modeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseggpt%2Ftest_modeling_seggpt.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -167,7 +167,6 @@ class SegGptModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (SegGptModel, SegGptForImageSegmentation) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_torch_exportable = True"
        },
        {
            "sha": "dc58a75373615ba64e2d1f7520f670db26b3937e",
            "filename": "tests/models/siglip/test_modeling_siglip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -173,7 +173,6 @@ class SiglipVisionModelTest(SiglipModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (SiglipVisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     # MP works but offload doesn't work when the MultiheadAttention is offloaded\n@@ -339,7 +338,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class SiglipTextModelTest(SiglipModelTesterMixin, unittest.TestCase):\n     all_model_classes = (SiglipTextModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     model_split_percents = [0.5, 0.8, 0.9]\n \n@@ -441,7 +439,6 @@ class SiglipModelTest(SiglipModelTesterMixin, PipelineTesterMixin, unittest.Test\n     additional_model_inputs = [\"pixel_values\"]\n     all_model_classes = (SiglipModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": SiglipModel} if is_torch_available() else {}\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False\n@@ -533,7 +530,6 @@ def prepare_config_and_inputs_for_common(self):\n class SiglipForImageClassificationModelTest(SiglipModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (SiglipForImageClassification,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-classification\": SiglipForImageClassification} if is_torch_available() else {}\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "7b634ca7acb24bc9de0a6395c20420bfab11460b",
            "filename": "tests/models/siglip2/test_modeling_siglip2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -267,7 +267,6 @@ class Siglip2VisionModelTest(Siglip2ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (Siglip2VisionModel,) if is_torch_available() else ()\n     additional_model_inputs = [\"pixel_attention_mask\", \"spatial_shapes\"]\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     # MP works but offload doesn't work when the MultiheadAttention is offloaded\n@@ -432,7 +431,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Siglip2TextModelTest(Siglip2ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Siglip2TextModel,) if is_torch_available() else ()\n-    fx_compatible = False\n     test_resize_embeddings = False\n \n     model_split_percents = [0.5, 0.8, 0.9]\n@@ -541,7 +539,6 @@ class Siglip2ModelTest(Siglip2ModelTesterMixin, PipelineTesterMixin, unittest.Te\n         \"pixel_attention_mask\",\n         \"spatial_shapes\",\n     ]\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False\n@@ -638,7 +635,6 @@ class Siglip2ForImageClassificationModelTest(Siglip2ModelTesterMixin, PipelineTe\n     all_model_classes = (Siglip2ForImageClassification,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-classification\": Siglip2ForImageClassification} if is_torch_available() else {}\n     additional_model_inputs = [\"pixel_values\", \"pixel_attention_mask\", \"spatial_shapes\"]\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "5d2383ded88d525536dae137975837e3e95605e4",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -167,7 +167,6 @@ class SmolVLMModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (SmolVLMModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n \n@@ -330,7 +329,6 @@ class SmolVLMForConditionalGenerationModelTest(GenerationTesterMixin, ModelTeste\n     all_model_classes = (SmolVLMForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (SmolVLMForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": SmolVLMForConditionalGeneration} if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n "
        },
        {
            "sha": "26262363bd4810851e513af27513069625c0d2a6",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -264,7 +264,6 @@ class Speech2TextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTest\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = False\n     test_missing_keys = False\n \n     def setUp(self):"
        },
        {
            "sha": "c5412db98a47cb9674eebf3058c22a9a7df44daf",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -46,7 +46,6 @@ class StableLmModelTester(CausalLMModelTester):\n \n @require_torch\n class StableLmModelTest(CausalLMModelTest, unittest.TestCase):\n-    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = StableLmModelTester\n \n "
        },
        {
            "sha": "10909709318a73f0e08319ed9c608591b0be3d86",
            "filename": "tests/models/superglue/test_modeling_superglue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsuperglue%2Ftest_modeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsuperglue%2Ftest_modeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperglue%2Ftest_modeling_superglue.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -120,8 +120,6 @@ def prepare_config_and_inputs_for_common(self):\n class SuperGlueModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (SuperGlueForKeypointMatching,) if is_torch_available() else ()\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     has_attentions = True\n "
        },
        {
            "sha": "09f159c571fd9c4621feefdf7389851f247f98ba",
            "filename": "tests/models/superpoint/test_modeling_superpoint.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsuperpoint%2Ftest_modeling_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fsuperpoint%2Ftest_modeling_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperpoint%2Ftest_modeling_superpoint.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -114,8 +114,6 @@ def prepare_config_and_inputs_for_common(self):\n class SuperPointModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (SuperPointForKeypointDetection,) if is_torch_available() else ()\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     has_attentions = False\n     from_pretrained_id = \"magic-leap-community/superpoint\""
        },
        {
            "sha": "f231599df1d7886514a61c21742812305edfebc9",
            "filename": "tests/models/swiftformer/test_modeling_swiftformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fswiftformer%2Ftest_modeling_swiftformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fswiftformer%2Ftest_modeling_swiftformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswiftformer%2Ftest_modeling_swiftformer.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -141,8 +141,6 @@ class SwiftFormerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "c8d39802512b29fb268f1b7a237fad6e20f1b155",
            "filename": "tests/models/swin/test_modeling_swin.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -235,7 +235,6 @@ class SwinModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = True\n \n     test_resize_embeddings = False\n     test_torch_exportable = True"
        },
        {
            "sha": "9710c1a4c4afc74cdb311f9a7cbe84440fecad9f",
            "filename": "tests/models/swin2sr/test_modeling_swin2sr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -166,8 +166,6 @@ class Swin2SRModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "c80b0f95e801ed4997efc5823d7f3cfa9688066f",
            "filename": "tests/models/swinv2/test_modeling_swinv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -222,8 +222,6 @@ class Swinv2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "65eb103c1fc42d059ab57aeb390be0b49d1dfa65",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -566,7 +566,6 @@ class SwitchTransformersModelTest(ModelTesterMixin, GenerationTesterMixin, Pipel\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     is_encoder_decoder = True"
        },
        {
            "sha": "8345cd63b036ada6b18d72050fa831fc1e2c464d",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 119,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -14,16 +14,13 @@\n \n \n import copy\n-import os\n-import pickle\n import tempfile\n import unittest\n from functools import cached_property\n \n import pytest\n \n from transformers import T5Config, is_torch_available\n-from transformers.models.auto.modeling_auto import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4\n from transformers.testing_utils import (\n     Expectations,\n@@ -36,11 +33,10 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils.fx import symbolic_trace\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -565,7 +561,6 @@ class T5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = True\n \n     test_resize_embeddings = True\n     is_encoder_decoder = True\n@@ -595,119 +590,6 @@ def is_pipeline_test_to_skip(\n \n         return False\n \n-    def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=False):\n-        if not self.fx_compatible:\n-            self.skipTest(reason=\"torch.fx is not compatible with this model\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.return_dict = False\n-\n-        for model_class in self.all_model_classes:\n-            if model_class.__name__ == \"T5ForSequenceClassification\":\n-                continue\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=output_loss)\n-\n-            try:\n-                if model.config.is_encoder_decoder:\n-                    model.config.use_cache = False  # FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward\n-                    labels = inputs.get(\"labels\", None)\n-                    input_names = [\n-                        \"attention_mask\",\n-                        \"decoder_attention_mask\",\n-                        \"decoder_input_ids\",\n-                        \"input_features\",\n-                        \"input_ids\",\n-                        \"input_values\",\n-                    ]\n-                    if labels is not None:\n-                        input_names.append(\"labels\")\n-                    filtered_inputs = {k: v for (k, v) in inputs.items() if k in input_names}\n-                    input_names = list(filtered_inputs.keys())\n-                    model_output = model(**filtered_inputs)\n-                    traced_model = symbolic_trace(model, input_names)\n-                    traced_output = traced_model(**filtered_inputs)\n-                else:\n-                    input_names = [\n-                        \"attention_mask\",\n-                        \"bbox\",\n-                        \"input_features\",\n-                        \"input_ids\",\n-                        \"input_values\",\n-                        \"pixel_values\",\n-                        \"token_type_ids\",\n-                        \"visual_feats\",\n-                        \"visual_pos\",\n-                    ]\n-                    labels = inputs.get(\"labels\", None)\n-                    start_positions = inputs.get(\"start_positions\", None)\n-                    end_positions = inputs.get(\"end_positions\", None)\n-                    if labels is not None:\n-                        input_names.append(\"labels\")\n-                    if start_positions is not None:\n-                        input_names.append(\"start_positions\")\n-                    if end_positions is not None:\n-                        input_names.append(\"end_positions\")\n-                    filtered_inputs = {k: v for (k, v) in inputs.items() if k in input_names}\n-                    input_names = list(filtered_inputs.keys())\n-                    if model.__class__.__name__ in set(MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES.values()) and (\n-                        not hasattr(model.config, \"problem_type\") or model.config.problem_type is None\n-                    ):\n-                        model.config.problem_type = \"single_label_classification\"\n-                    traced_model = symbolic_trace(model, input_names)\n-                    traced_output = traced_model(**filtered_inputs)\n-                    model_output = model(**filtered_inputs)\n-\n-            except Exception as e:\n-                self.fail(f\"Couldn't trace module: {e}\")\n-\n-            def flatten_output(output):\n-                flatten = []\n-                for x in output:\n-                    if isinstance(x, (tuple, list)):\n-                        flatten += flatten_output(x)\n-                    elif not isinstance(x, torch.Tensor):\n-                        continue\n-                    else:\n-                        flatten.append(x)\n-                return flatten\n-\n-            model_output = flatten_output(model_output)\n-            traced_output = flatten_output(traced_output)\n-            num_outputs = len(model_output)\n-\n-            for i in range(num_outputs):\n-                self.assertTrue(\n-                    torch.allclose(model_output[i], traced_output[i]),\n-                    f\"traced {i}th output doesn't match model {i}th output for {model_class}\",\n-                )\n-\n-            # Test that the model can be serialized and restored properly\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pkl_file_name = os.path.join(tmp_dir_name, \"model.pkl\")\n-                try:\n-                    with open(pkl_file_name, \"wb\") as f:\n-                        pickle.dump(traced_model, f)\n-                    with open(pkl_file_name, \"rb\") as f:\n-                        loaded = pickle.load(f)\n-                except Exception as e:\n-                    self.fail(f\"Couldn't serialize / deserialize the traced model: {e}\")\n-\n-                loaded_output = loaded(**filtered_inputs)\n-                loaded_output = flatten_output(loaded_output)\n-\n-                for i in range(num_outputs):\n-                    self.assertTrue(\n-                        torch.allclose(model_output[i], loaded_output[i]),\n-                        f\"serialized model {i}th output doesn't match model {i}th output for {model_class}\",\n-                    )\n-\n-            # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n-            # (Even with this call, there are still memory leak by ~0.04MB)\n-            self.clear_torch_jit_class_registry()\n-\n     # overwrite because T5 doesn't accept position ids as input and expects `decoder_input_ids`\n     def test_custom_4d_attention_mask(self):\n         for model_class in self.all_generative_model_classes:"
        },
        {
            "sha": "e5babc76693ae58c795e4ca7247b2d843c2058a6",
            "filename": "tests/models/textnet/test_modeling_textnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -211,8 +211,6 @@ class TextNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n         else {}\n     )\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     test_torch_exportable = True\n     has_attentions = False"
        },
        {
            "sha": "62fb00a7f0943f32e785124188b4d32393b57717",
            "filename": "tests/models/timesfm/test_modeling_timesfm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ftimesfm%2Ftest_modeling_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ftimesfm%2Ftest_modeling_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimesfm%2Ftest_modeling_timesfm.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -123,7 +123,6 @@ def prepare_config_and_inputs_for_common(self):\n class TimesFmModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (TimesFmModelForPrediction,) if is_torch_available() else ()\n     all_generative_model_classes = ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     is_encoder_decoder = False"
        },
        {
            "sha": "b975f39390674946968381f69d95da2bd9f362f7",
            "filename": "tests/models/trocr/test_modeling_trocr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ftrocr%2Ftest_modeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Ftrocr%2Ftest_modeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftrocr%2Ftest_modeling_trocr.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -161,7 +161,6 @@ def prepare_config_and_inputs_for_common(self):\n class TrOCRStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (TrOCRDecoder, TrOCRForCausalLM) if is_torch_available() else ()\n     pipeline_model_mapping = {\"text-generation\": TrOCRForCausalLM} if is_torch_available() else {}\n-    fx_compatible = True\n \n     def setUp(self):\n         self.model_tester = TrOCRStandaloneDecoderModelTester(self, is_training=False)"
        },
        {
            "sha": "2444ed9f5e783ab87d01c24d54d0ec143b80bfb9",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -273,7 +273,6 @@ class UdopModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     is_encoder_decoder = True"
        },
        {
            "sha": "58e6e923e8df536d7998361652049e26cab9b53d",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 127,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -13,25 +13,20 @@\n # limitations under the License.\n \n import copy\n-import os\n-import pickle\n-import tempfile\n import unittest\n \n from transformers import UMT5Config, is_torch_available\n-from transformers.models.auto.modeling_auto import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n from transformers.testing_utils import (\n     require_sentencepiece,\n     require_tokenizers,\n     require_torch,\n     slow,\n     torch_device,\n )\n-from transformers.utils.fx import symbolic_trace\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -249,7 +244,6 @@ class UMT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = False\n \n     test_missing_keys = True\n     # The small UMT5 model needs higher percentages for CPU/MP tests\n@@ -275,126 +269,6 @@ def is_pipeline_test_to_skip(\n \n         return False\n \n-    def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=False):\n-        if not self.fx_compatible:\n-            self.skipTest(reason=\"torch fx is not compatible with this model\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.return_dict = False\n-\n-        for model_class in self.all_model_classes:\n-            if model_class.__name__ == \"UMT5ForSequenceClassification\":\n-                continue\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=output_loss)\n-\n-            try:\n-                if model.config.is_encoder_decoder:\n-                    model.config.use_cache = False  # FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward\n-                    labels = inputs.get(\"labels\", None)\n-                    input_names = [\n-                        \"attention_mask\",\n-                        \"decoder_attention_mask\",\n-                        \"decoder_input_ids\",\n-                        \"input_features\",\n-                        \"input_ids\",\n-                        \"input_values\",\n-                    ]\n-                    if labels is not None:\n-                        input_names.append(\"labels\")\n-\n-                    filtered_inputs = {k: v for (k, v) in inputs.items() if k in input_names}\n-                    input_names = list(filtered_inputs.keys())\n-\n-                    model_output = model(**filtered_inputs)\n-\n-                    traced_model = symbolic_trace(model, input_names)\n-                    traced_output = traced_model(**filtered_inputs)\n-                else:\n-                    input_names = [\n-                        \"attention_mask\",\n-                        \"bbox\",\n-                        \"input_features\",\n-                        \"input_ids\",\n-                        \"input_values\",\n-                        \"pixel_values\",\n-                        \"token_type_ids\",\n-                        \"visual_feats\",\n-                        \"visual_pos\",\n-                    ]\n-\n-                    labels = inputs.get(\"labels\", None)\n-                    start_positions = inputs.get(\"start_positions\", None)\n-                    end_positions = inputs.get(\"end_positions\", None)\n-                    if labels is not None:\n-                        input_names.append(\"labels\")\n-                    if start_positions is not None:\n-                        input_names.append(\"start_positions\")\n-                    if end_positions is not None:\n-                        input_names.append(\"end_positions\")\n-\n-                    filtered_inputs = {k: v for (k, v) in inputs.items() if k in input_names}\n-                    input_names = list(filtered_inputs.keys())\n-\n-                    if model.__class__.__name__ in set(MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES.values()) and (\n-                        not hasattr(model.config, \"problem_type\") or model.config.problem_type is None\n-                    ):\n-                        model.config.problem_type = \"single_label_classification\"\n-\n-                    traced_model = symbolic_trace(model, input_names)\n-                    traced_output = traced_model(**filtered_inputs)\n-                    model_output = model(**filtered_inputs)\n-\n-            except Exception as e:\n-                self.fail(f\"Couldn't trace module: {e}\")\n-\n-            def flatten_output(output):\n-                flatten = []\n-                for x in output:\n-                    if isinstance(x, (tuple, list)):\n-                        flatten += flatten_output(x)\n-                    elif not isinstance(x, torch.Tensor):\n-                        continue\n-                    else:\n-                        flatten.append(x)\n-                return flatten\n-\n-            model_output = flatten_output(model_output)\n-            traced_output = flatten_output(traced_output)\n-            num_outputs = len(model_output)\n-\n-            for i in range(num_outputs):\n-                self.assertTrue(\n-                    torch.allclose(model_output[i], traced_output[i]),\n-                    f\"traced {i}th output doesn't match model {i}th output for {model_class}\",\n-                )\n-\n-            # Test that the model can be serialized and restored properly\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pkl_file_name = os.path.join(tmp_dir_name, \"model.pkl\")\n-                try:\n-                    with open(pkl_file_name, \"wb\") as f:\n-                        pickle.dump(traced_model, f)\n-                    with open(pkl_file_name, \"rb\") as f:\n-                        loaded = pickle.load(f)\n-                except Exception as e:\n-                    self.fail(f\"Couldn't serialize / deserialize the traced model: {e}\")\n-\n-                loaded_output = loaded(**filtered_inputs)\n-                loaded_output = flatten_output(loaded_output)\n-\n-                for i in range(num_outputs):\n-                    self.assertTrue(\n-                        torch.allclose(model_output[i], loaded_output[i]),\n-                        f\"serialized model {i}th output doesn't match model {i}th output for {model_class}\",\n-                    )\n-\n-            # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n-            # (Even with this call, there are still memory leak by ~0.04MB)\n-            self.clear_torch_jit_class_registry()\n-\n     # UMT5ForSequenceClassification does not support inputs_embeds\n     def test_inputs_embeds(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "1ac09d8e20060cf376c893415e01e9d0e2f696b2",
            "filename": "tests/models/upernet/test_modeling_upernet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -149,7 +149,6 @@ class UperNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n \n     all_model_classes = (UperNetForSemanticSegmentation,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-segmentation\": UperNetForSemanticSegmentation} if is_torch_available() else {}\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     has_attentions = False"
        },
        {
            "sha": "90efb9a43fa61caaec7e2d93c69cf385efc9e579",
            "filename": "tests/models/video_llama_3/test_modeling_video_llama_3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -348,7 +348,6 @@ class VideoLlama3VisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (VideoLlama3VisionModel,) if is_torch_available() else ()\n     additional_model_inputs = [\"grid_thw\", \"merge_sizes\"]\n-    # fx_compatible = False\n     test_resize_embeddings = False\n     test_head_masking = False\n     test_cpu_offload = False"
        },
        {
            "sha": "3bcea272dd5102ad388b3a34c1fec7e8876a4e7e",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -201,7 +201,6 @@ class VideoLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTe\n         if is_torch_available()\n         else ()\n     )\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     _is_composite = True"
        },
        {
            "sha": "b158d37dd5748b270260e5f8efb02ec896daf54c",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -177,7 +177,6 @@ class VipLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTest\n         else ()\n     )\n     pipeline_model_mapping = {\"image-text-to-text\": VipLlavaForConditionalGeneration} if is_torch_available() else {}\n-    fx_compatible = False\n \n     test_resize_embeddings = True\n     _is_composite = True"
        },
        {
            "sha": "5e454c097c74dd0f8cd1f46cdb29c83553453001",
            "filename": "tests/models/vit/test_modeling_vit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvit%2Ftest_modeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvit%2Ftest_modeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit%2Ftest_modeling_vit.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -202,7 +202,6 @@ class ViTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False  # broken by output recording refactor\n \n     test_resize_embeddings = False\n     test_torch_exportable = True"
        },
        {
            "sha": "25c6d93fd1d0dbcc2480f78e3b421860db600c3b",
            "filename": "tests/models/vitdet/test_modeling_vitdet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvitdet%2Ftest_modeling_vitdet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvitdet%2Ftest_modeling_vitdet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitdet%2Ftest_modeling_vitdet.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -164,8 +164,6 @@ class VitDetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (VitDetModel, VitDetBackbone) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": VitDetModel} if is_torch_available() else {}\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "64ff2e582b771eb39caa5e648cb8ce9252a38c29",
            "filename": "tests/models/vitmatte/test_modeling_vitmatte.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -139,8 +139,6 @@ class VitMatteModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     all_model_classes = (VitMatteForImageMatting,) if is_torch_available() else ()\n     pipeline_model_mapping = {}\n \n-    fx_compatible = False\n-\n     test_resize_embeddings = False\n     test_torch_exportable = True\n     test_torch_exportable_strictly = get_torch_major_and_minor_version() != \"2.7\""
        },
        {
            "sha": "645204331bf7b27160e865a5d923c3e6d4fcd373",
            "filename": "tests/models/vitpose/test_modeling_vitpose.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -150,7 +150,6 @@ class VitPoseModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (VitPoseForPoseEstimation,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_torch_exportable = True"
        },
        {
            "sha": "a3671b61b4772a8f0670781ae76e106673513806",
            "filename": "tests/models/vitpose_backbone/test_modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -124,7 +124,6 @@ class VitPoseBackboneModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (VitPoseBackbone,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_torch_exportable = True"
        },
        {
            "sha": "dc3034f7a246c1959da5aa89f2ba54901616c911",
            "filename": "tests/models/vjepa2/test_modeling_vjepa2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvjepa2%2Ftest_modeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fvjepa2%2Ftest_modeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvjepa2%2Ftest_modeling_vjepa2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -155,8 +155,6 @@ class VJEPA2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     all_model_classes = (VJEPA2Model, VJEPA2ForVideoClassification) if is_torch_available() else ()\n \n-    fx_compatible = False\n-\n     pipeline_model_mapping = {}\n \n     test_resize_embeddings = False"
        },
        {
            "sha": "e645070ffa3130c3ef33405dab05c4cccd461ca7",
            "filename": "tests/models/wav2vec2/test_modeling_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 109,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -16,7 +16,6 @@\n import math\n import multiprocessing\n import os\n-import pickle\n import tempfile\n import traceback\n import unittest\n@@ -46,7 +45,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -89,9 +87,6 @@\n     from transformers.models.wav2vec2_with_lm import processing_wav2vec2_with_lm\n \n \n-from transformers.utils.fx import symbolic_trace\n-\n-\n def _test_wav2vec2_with_lm_invalid_pool(in_queue, out_queue, timeout):\n     error = None\n     try:\n@@ -497,7 +492,6 @@ class Wav2Vec2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = True\n \n     def setUp(self):\n         self.model_tester = Wav2Vec2ModelTester(self)\n@@ -675,109 +669,6 @@ def test_model_from_pretrained(self):\n         model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n         self.assertIsNotNone(model)\n \n-    # Wav2Vec2 cannot be torchscripted because of group norm.\n-    def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=False):\n-        # TODO: fix it\n-        self.skipTest(reason=\"torch 2.1 breaks torch fx tests for wav2vec2/hubert.\")\n-\n-        if not self.fx_compatible:\n-            self.skipTest(reason=\"torch fx is not compatible with this model\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.return_dict = False\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=output_loss)\n-\n-            try:\n-                input_names = [\n-                    \"attention_mask\",\n-                    \"bbox\",\n-                    \"input_features\",\n-                    \"input_ids\",\n-                    \"input_values\",\n-                    \"pixel_values\",\n-                    \"token_type_ids\",\n-                    \"visual_feats\",\n-                    \"visual_pos\",\n-                ]\n-\n-                labels = inputs.get(\"labels\", None)\n-                start_positions = inputs.get(\"start_positions\", None)\n-                end_positions = inputs.get(\"end_positions\", None)\n-                if labels is not None:\n-                    input_names.append(\"labels\")\n-                if start_positions is not None:\n-                    input_names.append(\"start_positions\")\n-                if end_positions is not None:\n-                    input_names.append(\"end_positions\")\n-\n-                filtered_inputs = {k: v for (k, v) in inputs.items() if k in input_names}\n-                input_names = list(filtered_inputs.keys())\n-\n-                model_output = model(**filtered_inputs)\n-\n-                if (\n-                    isinstance(model, Wav2Vec2ForSequenceClassification)\n-                    and not hasattr(model.config, \"problem_type\")\n-                    or model.config.problem_type is None\n-                ):\n-                    model.config.problem_type = \"single_label_classification\"\n-\n-                traced_model = symbolic_trace(model, input_names)\n-                traced_output = traced_model(**filtered_inputs)\n-\n-            except Exception as e:\n-                self.fail(f\"Couldn't trace module: {e}\")\n-\n-            def flatten_output(output):\n-                flatten = []\n-                for x in output:\n-                    if isinstance(x, (tuple, list)):\n-                        flatten += flatten_output(x)\n-                    elif not isinstance(x, torch.Tensor):\n-                        continue\n-                    else:\n-                        flatten.append(x)\n-                return flatten\n-\n-            model_output = flatten_output(model_output)\n-            traced_output = flatten_output(traced_output)\n-            num_outputs = len(model_output)\n-\n-            for i in range(num_outputs):\n-                self.assertTrue(\n-                    torch.allclose(model_output[i], traced_output[i]),\n-                    f\"traced {i}th output doesn't match model {i}th output for {model_class}\",\n-                )\n-\n-            # Test that the model can be serialized and restored properly\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pkl_file_name = os.path.join(tmp_dir_name, \"model.pkl\")\n-                try:\n-                    with open(pkl_file_name, \"wb\") as f:\n-                        pickle.dump(traced_model, f)\n-                    with open(pkl_file_name, \"rb\") as f:\n-                        loaded = pickle.load(f)\n-                except Exception as e:\n-                    self.fail(f\"Couldn't serialize / deserialize the traced model: {e}\")\n-\n-                loaded_output = loaded(**filtered_inputs)\n-                loaded_output = flatten_output(loaded_output)\n-\n-                for i in range(num_outputs):\n-                    self.assertTrue(\n-                        torch.allclose(model_output[i], loaded_output[i]),\n-                        f\"serialized model {i}th output doesn't match model {i}th output for {model_class}\",\n-                    )\n-\n-            # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n-            # (Even with this call, there are still memory leak by ~0.04MB)\n-            self.clear_torch_jit_class_registry()\n-\n \n @require_torch\n class Wav2Vec2RobustModelTest(ModelTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "35d4a8ffd3caefcc930640d3de77f70f26134947",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -362,7 +362,6 @@ class WhisperModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = False\n \n     test_missing_keys = False\n     # Needs higher percentages after model tester's vocab_size is changed to 200 (PR #21222)\n@@ -3235,7 +3234,6 @@ def create_and_check_model_forward(self, config, inputs_dict, use_weighted_layer\n class WhisperEncoderModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (WhisperForAudioClassification,) if is_torch_available() else ()\n     is_encoder_decoder = False\n-    fx_compatible = False\n \n     test_missing_keys = False\n \n@@ -3503,8 +3501,6 @@ def create_and_check_decoder_model_attention_mask_past(self, config, input_ids):\n @require_torch\n class WhisperStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (WhisperDecoder, WhisperForCausalLM) if is_torch_available() else ()\n-    fx_comptatible = False\n-\n     is_encoder_decoder = False\n     test_missing_keys = False\n "
        },
        {
            "sha": "dfa7084403f8a8e338d4fbcbb4452772c7ef6e9b",
            "filename": "tests/models/x_clip/test_modeling_x_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -147,7 +147,6 @@ class XCLIPVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (XCLIPVisionModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n \n@@ -397,7 +396,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class XCLIPTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (XCLIPTextModel,) if is_torch_available() else ()\n-    fx_compatible = False\n \n     def setUp(self):\n         self.model_tester = XCLIPTextModelTester(self)\n@@ -517,7 +515,6 @@ def prepare_config_and_inputs_for_common(self):\n class XCLIPModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (XCLIPModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": XCLIPModel} if is_torch_available() else {}\n-    fx_compatible = False\n \n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "1eafa5cf535a388390a9fd4067180862023f3425",
            "filename": "tests/models/xglm/test_modeling_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -279,7 +279,6 @@ class XGLMModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     pipeline_model_mapping = (\n         {\"feature-extraction\": XGLMModel, \"text-generation\": XGLMForCausalLM} if is_torch_available() else {}\n     )\n-    fx_compatible = True\n     test_missing_keys = False\n \n     def setUp(self):"
        },
        {
            "sha": "54b59c55d4ccc99d55c2d111f1b8d80002a7b21d",
            "filename": "tests/models/xlnet/test_modeling_xlnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fxlnet%2Ftest_modeling_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fxlnet%2Ftest_modeling_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlnet%2Ftest_modeling_xlnet.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -531,7 +531,6 @@ class XLNetModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else {}\n     )\n-    fx_compatible = False\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "918734c7f8ac51a6767f47dcb471a2f7ddf654da",
            "filename": "tests/models/xlstm/test_modeling_xlstm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -150,7 +150,6 @@ class xLSTMModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     all_model_classes = (xLSTMModel, xLSTMForCausalLM) if is_torch_available() else ()\n     all_generative_model_classes = (xLSTMForCausalLM,) if is_torch_available() else ()\n     has_attentions = False  # xLSTM does not support attentions\n-    fx_compatible = False\n \n     pipeline_model_mapping = (\n         {\"feature-extraction\": xLSTMModel, \"text-generation\": xLSTMForCausalLM} if is_torch_available() else {}"
        },
        {
            "sha": "2d6d470d43ef30c53edf898a53aefeb8833bf230",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 176,
            "changes": 176,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -121,13 +121,9 @@\n     from torch import nn\n \n     from transformers import MODEL_MAPPING\n-    from transformers.cache_utils import DynamicCache\n     from transformers.modeling_utils import load_state_dict\n     from transformers.pytorch_utils import id_tensor_storage\n \n-from transformers.utils.fx import _FX_SUPPORTED_MODELS_WITH_KV_CACHE, symbolic_trace\n-\n-\n if is_deepspeed_available():\n     import deepspeed\n \n@@ -565,7 +561,6 @@ def sdpa_kernel(enable_flash, enable_math, enable_mem_efficient):\n class ModelTesterMixin:\n     model_tester = None\n     all_model_classes = ()\n-    fx_compatible = False\n     test_resize_embeddings = True\n     test_resize_position_embeddings = False\n     test_mismatched_shapes = True\n@@ -1357,177 +1352,6 @@ def clear_torch_jit_class_registry(self):\n         if hasattr(torch.jit._state, \"_clear_class_state\"):\n             torch.jit._state._clear_class_state()\n \n-    def test_torch_fx(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        self._create_and_check_torch_fx_tracing(config, inputs_dict)\n-\n-    def test_torch_fx_output_loss(self):\n-        if self.all_model_classes[0].__name__ == \"BloomModel\":\n-            self.skipTest(reason=\"Bloom currently has issues, @michaelbenayoun\")\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        self._create_and_check_torch_fx_tracing(config, inputs_dict, output_loss=True)\n-\n-    def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=False):\n-        if not self.fx_compatible:\n-            self.skipTest(f\"The model type {config.model_type} is not compatible with torch.fx\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.return_dict = False\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=output_loss)\n-\n-            # We may want to test several inputs (various shapes, etc.).\n-            inputs_to_test = [inputs]\n-\n-            if model.config.is_encoder_decoder:\n-                model.config.use_cache = False  # FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward\n-                labels = inputs.get(\"labels\", None)\n-                input_names = [\n-                    \"attention_mask\",\n-                    \"decoder_attention_mask\",\n-                    \"decoder_input_ids\",\n-                    \"input_features\",\n-                    \"input_ids\",\n-                    \"input_values\",\n-                ]\n-                if labels is not None:\n-                    input_names.append(\"labels\")\n-            else:\n-                input_names = [\n-                    \"attention_mask\",\n-                    \"bbox\",\n-                    \"input_features\",\n-                    \"input_ids\",\n-                    \"input_values\",\n-                    \"inputs_embeds\",\n-                    \"pixel_values\",\n-                    \"pixel_values_videos\",\n-                    \"token_type_ids\",\n-                    \"visual_feats\",\n-                    \"visual_pos\",\n-                    \"noise\",\n-                ]\n-\n-                labels = inputs.get(\"labels\", None)\n-                start_positions = inputs.get(\"start_positions\", None)\n-                end_positions = inputs.get(\"end_positions\", None)\n-                if labels is not None:\n-                    input_names.append(\"labels\")\n-                if start_positions is not None:\n-                    input_names.append(\"start_positions\")\n-                if end_positions is not None:\n-                    input_names.append(\"end_positions\")\n-\n-                if model.config.model_type in _FX_SUPPORTED_MODELS_WITH_KV_CACHE:\n-                    input_names.append(\"past_key_values\")\n-\n-                    # Generally model_tester.prepare_config_and_inputs_for_common seem not to generate past key values inputs.\n-                    if \"past_key_values\" not in inputs:\n-                        batch_size = inputs[next(iter(inputs))].shape[0]\n-                        num_heads = model.config.num_attention_heads\n-                        head_dim = model.config.hidden_size // model.config.num_attention_heads\n-\n-                        cache_shape = (batch_size, num_heads, 0, head_dim)\n-                        empty_pkv = DynamicCache(config=model.config)\n-\n-                        cache_length = 9\n-                        cache_shape = (batch_size, num_heads, cache_length, head_dim)\n-                        non_empty_pkv = tuple(\n-                            (\n-                                None,\n-                                torch.rand(cache_shape, dtype=torch.float, device=torch_device),\n-                                torch.rand(cache_shape, dtype=torch.float, device=torch_device),\n-                            )\n-                            for i in range(model.config.num_hidden_layers)\n-                        )\n-                        non_empty_pkv = DynamicCache(non_empty_pkv)\n-\n-                        inps = copy.deepcopy(inputs_to_test[0])\n-\n-                        inputs_to_test[0][\"past_key_values\"] = empty_pkv\n-\n-                        inps[\"past_key_values\"] = non_empty_pkv\n-                        inputs_to_test.append(inps)\n-\n-                        past_mask = torch.ones(batch_size, cache_length, device=torch_device, dtype=torch.float)\n-                        inputs_to_test[1][\"attention_mask\"] = torch.cat(\n-                            (past_mask, inputs_to_test[1][\"attention_mask\"]), dim=1\n-                        )\n-\n-                forward_parameters = inspect.signature(model.forward).parameters\n-                if \"input_ids\" in forward_parameters and \"inputs_embeds\" in forward_parameters:\n-                    inps = copy.deepcopy(inputs_to_test[0])\n-\n-                    embedding_size = (\n-                        model.config.embedding_size\n-                        if getattr(model.config, \"embedding_size\", None) is not None\n-                        and model.config.model_type != \"megatron-bert\"\n-                        else model.config.hidden_size\n-                    )\n-\n-                    if (\n-                        model.config.model_type in MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES\n-                        and model.__class__.__name__\n-                        == MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES[model.config.model_type]\n-                    ):\n-                        batch_size, num_choices, sequence_length = inputs[\"input_ids\"].shape\n-                        shape = (batch_size, num_choices, sequence_length, embedding_size)\n-                    elif inps[\"input_ids\"].ndim == 2:\n-                        batch_size, sequence_length = inputs[\"input_ids\"].shape\n-                        shape = (batch_size, sequence_length, embedding_size)\n-                    else:\n-                        self.skipTest(\"Unknown case\")\n-\n-                    del inps[\"input_ids\"]\n-                    inps[\"inputs_embeds\"] = torch.rand(shape, dtype=torch.float, device=torch_device)\n-                    inputs_to_test.append(inps)\n-\n-            for inps in inputs_to_test:\n-                filtered_inputs = {k: v for (k, v) in inps.items() if k in input_names}\n-                input_names_to_trace = list(filtered_inputs.keys())\n-\n-                if model.__class__.__name__ in set(MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES.values()) and (\n-                    not hasattr(model.config, \"problem_type\") or model.config.problem_type is None\n-                ):\n-                    model.config.problem_type = \"single_label_classification\"\n-\n-                model.config.use_cache = \"past_key_values\" in input_names_to_trace\n-\n-                traced_model = symbolic_trace(model, input_names_to_trace)\n-\n-                with torch.no_grad():\n-                    traced_output = traced_model(**filtered_inputs)\n-                    model_output = model(**filtered_inputs)\n-\n-                def flatten_output(output):\n-                    flatten = []\n-                    for x in output:\n-                        if isinstance(x, (tuple, list)):\n-                            flatten += flatten_output(x)\n-                        elif not isinstance(x, torch.Tensor):\n-                            continue\n-                        else:\n-                            flatten.append(x)\n-                    return flatten\n-\n-                model_output = flatten_output(model_output)\n-                traced_output = flatten_output(traced_output)\n-                num_outputs = len(model_output)\n-\n-                for i in range(num_outputs):\n-                    self.assertTrue(\n-                        torch.allclose(model_output[i], traced_output[i]),\n-                        f\"traced {i}th output doesn't match model {i}th output for {model_class}\",\n-                    )\n-\n-                # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n-                # (Even with this call, there are still memory leak by ~0.04MB)\n-                self.clear_torch_jit_class_registry()\n-\n     def test_hidden_states_output(self):\n         def check_hidden_states_output(inputs_dict, config, model_class):\n             model = model_class(copy.deepcopy(config))"
        },
        {
            "sha": "3b95f810729d827509ae551e017804ad0dfca43a",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75da795d8f93296ae226706f7150bee2479b5bab/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/75da795d8f93296ae226706f7150bee2479b5bab/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=75da795d8f93296ae226706f7150bee2479b5bab",
            "patch": "@@ -797,7 +797,6 @@ src/transformers/utils/dummy_sentencepiece_objects.py\n src/transformers/utils/dummy_speech_objects.py\n src/transformers/utils/dummy_tokenizers_objects.py\n src/transformers/utils/dummy_vision_objects.py\n-src/transformers/utils/fx.py\n src/transformers/utils/generic.py\n src/transformers/utils/hp_naming.py\n src/transformers/utils/hub.py"
        }
    ],
    "stats": {
        "total": 2775,
        "additions": 70,
        "deletions": 2705
    }
}