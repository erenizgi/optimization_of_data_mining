{
    "author": "HMJ0628",
    "message": "Translating \"translate perf_infer_gpu_multi.md\" to Chinese (#35271)\n\nadd \"translate perf_infer_gpu_multi\"",
    "sha": "886f690e76cdf647bb38851abca7b59add27dd95",
    "files": [
        {
            "sha": "2cce86b6592484acedc2bc10b7dae7e01381ee97",
            "filename": "docs/source/zh/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/886f690e76cdf647bb38851abca7b59add27dd95/docs%2Fsource%2Fzh%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/886f690e76cdf647bb38851abca7b59add27dd95/docs%2Fsource%2Fzh%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2F_toctree.yml?ref=886f690e76cdf647bb38851abca7b59add27dd95",
            "patch": "@@ -69,6 +69,8 @@\n       title: 完全分片数据并行\n     - local: perf_train_special\n       title: 在 Apple silicon 芯片上进行 PyTorch 训练\n+    - local: perf_infer_gpu_multi\n+      title: 多GPU推理\n     - local: perf_train_cpu\n       title: 在CPU上进行高效训练\n     - local: perf_hardware"
        },
        {
            "sha": "ee523bc604c2043cafad544921f01a19de12cba8",
            "filename": "docs/source/zh/perf_infer_gpu_multi.md",
            "status": "added",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/886f690e76cdf647bb38851abca7b59add27dd95/docs%2Fsource%2Fzh%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/886f690e76cdf647bb38851abca7b59add27dd95/docs%2Fsource%2Fzh%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fperf_infer_gpu_multi.md?ref=886f690e76cdf647bb38851abca7b59add27dd95",
            "patch": "@@ -0,0 +1,68 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# 多GPU推理\n+\n+某些模型现已支持内置的**张量并行**（Tensor Parallelism, TP），并通过 PyTorch 实现。张量并行技术将模型切分到多个 GPU 上，从而支持更大的模型尺寸，并对诸如矩阵乘法等计算任务进行并行化。\n+\n+要启用张量并行，只需在调用 [`~AutoModelForCausalLM.from_pretrained`] 时传递参数 `tp_plan=\"auto\"`：\n+\n+```python\n+import os\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n+\n+# 初始化分布式环境\n+rank = int(os.environ[\"RANK\"])\n+device = torch.device(f\"cuda:{rank}\")\n+torch.distributed.init_process_group(\"nccl\", device_id=device)\n+\n+# 获取支持张量并行的模型\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_id,\n+    tp_plan=\"auto\",\n+)\n+\n+# 准备输入tokens\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+prompt = \"Can I help\"\n+inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n+\n+# 分布式运行\n+outputs = model(inputs)\n+```\n+\n+您可以使用 `torchrun` 命令启动上述脚本，多进程模式会自动将每个进程映射到一张 GPU：\n+\n+```\n+torchrun --nproc-per-node 4 demo.py\n+```\n+\n+目前，PyTorch 张量并行支持以下模型：\n+* [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)\n+\n+如果您希望对其他模型添加张量并行支持，可以通过提交 GitHub Issue 或 Pull Request 来提出请求。\n+\n+### 预期性能提升\n+\n+对于推理场景（尤其是处理大批量或长序列的输入），张量并行可以显著提升计算速度。\n+\n+以下是 [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel) 模型在序列长度为 512 且不同批量大小情况下的单次前向推理的预期加速效果：\n+\n+<div style=\"text-align: center\">\n+<img src=\"huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Meta-Llama-3-8B-Instruct, seqlen = 512, python, w_ compile.png\">\n+</div>"
        }
    ],
    "stats": {
        "total": 70,
        "additions": 70,
        "deletions": 0
    }
}