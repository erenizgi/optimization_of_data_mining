{
    "author": "Cyrilvallez",
    "message": "Remove random flag (#42282)\n\n* remove random flag\n\n* style",
    "sha": "e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
    "files": [
        {
            "sha": "078f78fff2a9f8ec0f1c1abc9fb99facdb48ee94",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -1535,8 +1535,6 @@ class BigBirdPegasusPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BigBirdPegasusEncoderLayer\", \"BigBirdPegasusDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_param_buffer_assignment = False\n-\n     _can_compile_fullgraph = True\n \n     @property"
        },
        {
            "sha": "1bf2179deec6a4993235a228db00de3d08ab540f",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -780,7 +780,6 @@ class ChameleonPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n \n     _can_compile_fullgraph = True\n-    _supports_param_buffer_assignment = False\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "98f1462231af3a5d15d1f1dfd53179b0b78fc691",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -130,7 +130,6 @@ class DeepseekVLPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n \n     _can_compile_fullgraph = True\n-    _supports_param_buffer_assignment = False\n \n \n @auto_docstring"
        },
        {
            "sha": "7eab9ea059f41e56b09d52ae77b02ea42df731a1",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -213,7 +213,6 @@ class DeepseekVLHybridPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n \n     _can_compile_fullgraph = True\n-    _supports_param_buffer_assignment = False\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "a0675a6e329da112aa83db6585b6c9d4ed08ad2c",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -1105,7 +1105,6 @@ class Emu3PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n \n     _can_compile_fullgraph = True\n-    _supports_param_buffer_assignment = False\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "a08601235a6206c41af3370e254d579c896c8588",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -76,7 +76,6 @@ class EncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     base_model_prefix = \"encoder_decoder\"\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True\n-    _supports_param_buffer_assignment = False\n     _supports_flash_attn = True\n     _supports_sdpa = True\n "
        },
        {
            "sha": "1ada68552066bb6927f91d4d6f0791b46dbe8783",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -544,8 +544,6 @@ class GPT2DoubleHeadsModelOutput(ModelOutput):\n \n @auto_docstring\n class GPT2Model(GPT2PreTrainedModel):\n-    _supports_param_buffer_assignment = False\n-\n     def __init__(self, config):\n         super().__init__(config)\n "
        },
        {
            "sha": "338ac9524e60ee2713c3c810ed55b3d7adf70471",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -442,7 +442,6 @@ class GPTJPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _can_compile_fullgraph = True\n-    _supports_param_buffer_assignment = False\n \n \n @auto_docstring"
        },
        {
            "sha": "a5a930453567ed5fda7d0814fb09e35e6c95c837",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -57,7 +57,6 @@ class JanusPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n \n     _can_compile_fullgraph = True\n-    _supports_param_buffer_assignment = False\n \n \n @dataclass"
        },
        {
            "sha": "4791a8b14951cd25b1420aca43194d2ee0f6a9cb",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -390,7 +390,6 @@ class JanusPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n \n     _can_compile_fullgraph = True\n-    _supports_param_buffer_assignment = False\n \n \n @dataclass"
        },
        {
            "sha": "a88dc32f8ff0a9006a00d64be47737a01e15a442",
            "filename": "src/transformers/models/lxmert/modeling_lxmert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -673,7 +673,6 @@ class LxmertPreTrainedModel(PreTrainedModel):\n     config: LxmertConfig\n     base_model_prefix = \"lxmert\"\n     input_modalities = [\"image\", \"text\"]\n-    _supports_param_buffer_assignment = False\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "99d140708078564935ceffa23ab48f46b5ef782f",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -67,7 +67,6 @@ class SpeechEncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     main_input_name = \"inputs\"\n     input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n-    _supports_param_buffer_assignment = False\n     _supports_flash_attn = True\n     _supports_sdpa = True\n "
        },
        {
            "sha": "e1b40e1d06d9363424921ff111d183164f5a0090",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -507,7 +507,6 @@ class TapasPreTrainedModel(PreTrainedModel):\n     config: TapasConfig\n     base_model_prefix = \"tapas\"\n     supports_gradient_checkpointing = True\n-    _supports_param_buffer_assignment = False\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "5bf7b956cad3827f629243d339cf6d1cf62c03de",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=e5c8a06fdd8ed7766bc4ea8952dba8b6fca9367f",
            "patch": "@@ -67,7 +67,6 @@ class VisionEncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     main_input_name = \"pixel_values\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n-    _supports_param_buffer_assignment = False\n     _supports_flash_attn = True\n     _supports_sdpa = True\n "
        }
    ],
    "stats": {
        "total": 16,
        "additions": 0,
        "deletions": 16
    }
}