{
    "author": "ngazagna-qc",
    "message": "Fix loaded data order bug when resuming from epoch >= 1 (#40691)\n\n* fix resume from epoch >= 1\n\n* add test checking order of sampled data points\n\n* add require_torch_non_multi_accelerator decorator to test method\n\n* move the epoch setting of epoch_dataloader before iterating over it\n\n* make fixup",
    "sha": "629c0da46d10d6cc9101d5042c0cdf670d1299e2",
    "files": [
        {
            "sha": "3266f35450b4e8acf809488cb5a8d0ec1c177e92",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/629c0da46d10d6cc9101d5042c0cdf670d1299e2/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/629c0da46d10d6cc9101d5042c0cdf670d1299e2/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=629c0da46d10d6cc9101d5042c0cdf670d1299e2",
            "patch": "@@ -2428,8 +2428,6 @@ def _inner_training_loop(\n \n         for epoch in range(epochs_trained, num_train_epochs):\n             epoch_dataloader = train_dataloader\n-            if hasattr(epoch_dataloader, \"set_epoch\"):\n-                epoch_dataloader.set_epoch(epoch)\n \n             steps_in_epoch = (\n                 len(epoch_dataloader)\n@@ -2450,6 +2448,9 @@ def _inner_training_loop(\n                 elif steps_trained_in_current_epoch == 0:\n                     self._load_rng_state(resume_from_checkpoint)\n \n+            if hasattr(epoch_dataloader, \"set_epoch\"):\n+                epoch_dataloader.set_epoch(epoch)\n+\n             epoch_iterator = iter(epoch_dataloader)\n             # We chunkify the epoch iterator into gradient accumulation steps `n` batches\n             remainder = steps_in_epoch % args.gradient_accumulation_steps"
        },
        {
            "sha": "85f967ac03cb815913b0353d6127429a06a1d4bd",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 143,
            "deletions": 1,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/629c0da46d10d6cc9101d5042c0cdf670d1299e2/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/629c0da46d10d6cc9101d5042c0cdf670d1299e2/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=629c0da46d10d6cc9101d5042c0cdf670d1299e2",
            "patch": "@@ -104,7 +104,12 @@\n     slow,\n     torch_device,\n )\n-from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR, HPSearchBackend, check_target_module_exists\n+from transformers.trainer_utils import (\n+    PREFIX_CHECKPOINT_DIR,\n+    HPSearchBackend,\n+    check_target_module_exists,\n+    get_last_checkpoint,\n+)\n from transformers.training_args import OptimizerNames\n from transformers.utils import (\n     SAFE_WEIGHTS_INDEX_NAME,\n@@ -5111,6 +5116,143 @@ def create_dummy_dataset():\n         final_model_path = os.path.join(final_checkpoint_path, SAFE_WEIGHTS_NAME)\n         self.assertTrue(os.path.exists(final_model_path), \"Final model checkpoint was not saved!\")\n \n+    @require_torch_non_multi_accelerator\n+    def test_resume_batch_order(self):\n+        \"\"\"\n+        Test that verifies dataloader order is reproducible when resuming from partial checkpoints.\n+        Tests resuming from checkpoint 7 (within epoch 1).\n+        \"\"\"\n+\n+        # --- Helper classes and functions defined locally for this test ---\n+        class DummyDataset(torch.utils.data.Dataset):\n+            def __init__(self, size: int = 32):\n+                self.size = size\n+                self.data = torch.randn((size, 10))\n+                self.data[:, 0] = torch.arange(0, size)  # Encode the data order\n+                self.labels = torch.randint(0, 10, (size,))\n+\n+            def __len__(self) -> int:\n+                return self.size\n+\n+            def __getitem__(self, idx: int):\n+                return {\"input_ids\": self.data[idx], \"labels\": self.labels[idx]}\n+\n+        class DummyModel(nn.Module):\n+            def __init__(self, size: int):\n+                super().__init__()\n+                self.fc = nn.Linear(10, 10, bias=False)\n+                # data_order logs the order of data points seen by the model\n+                self.register_buffer(\"data_order\", torch.empty(0, dtype=torch.long))\n+\n+            def load_state_dict(self, state_dict, strict=True):\n+                # Handle data_order buffer size mismatch during checkpoint loading\n+                if \"data_order\" in state_dict:\n+                    saved_data_order = state_dict[\"data_order\"]\n+                    if hasattr(self, \"data_order\") and self.data_order.shape != saved_data_order.shape:\n+                        # Resize the buffer to match the saved state\n+                        self.data_order = saved_data_order.clone()\n+\n+                return super().load_state_dict(state_dict, strict=strict)\n+\n+            def forward(self, input_ids: torch.Tensor, labels: torch.Tensor = None):\n+                logits = self.fc(input_ids)\n+                loss = None\n+                if labels is not None:\n+                    loss_fn = nn.CrossEntropyLoss()\n+                    loss = loss_fn(logits, labels)\n+\n+                # Log the data order for verification\n+                data_indices = input_ids[:, 0].int()\n+                self.data_order = torch.cat([self.data_order, data_indices.detach().clone()])\n+\n+                return {\"loss\": loss, \"logits\": logits}\n+\n+        # Scenario 1: Run baseline training to completion\n+        # 1.1 Run training to completion\n+        set_seed(42)\n+        train_dataset = DummyDataset(size=10)\n+        model_baseline = DummyModel(size=10)\n+\n+        exp_dir_baseline = self.get_auto_remove_tmp_dir()\n+        args_baseline = TrainingArguments(\n+            output_dir=str(exp_dir_baseline),\n+            seed=42,\n+            learning_rate=0.1,\n+            per_device_train_batch_size=2,\n+            gradient_accumulation_steps=1,\n+            save_strategy=\"steps\",\n+            save_steps=1,\n+            num_train_epochs=3,\n+            optim=\"sgd\",\n+            disable_tqdm=True,\n+            dataloader_num_workers=0,  # Ensures that main process loads the data\n+            report_to=[],  # Disable wandb/tensorboard and other loggers\n+        )\n+\n+        trainer_baseline = Trainer(\n+            model=model_baseline,\n+            args=args_baseline,\n+            train_dataset=train_dataset,\n+        )\n+\n+        trainer_baseline.train()\n+\n+        # 1.2 Get the data order from the last saved checkpoint for the full run\n+        last_checkpoint_path = get_last_checkpoint(exp_dir_baseline)\n+        last_ckpt_num = int(os.path.basename(last_checkpoint_path).split(\"-\")[1])  # Must be 15\n+\n+        baseline_state_dict = safetensors.torch.load_file(\n+            os.path.join(exp_dir_baseline, f\"checkpoint-{last_ckpt_num}\", \"model.safetensors\")\n+        )\n+        baseline_data_order = baseline_state_dict[\"data_order\"]\n+\n+        # Scenario 2: Resume training from checkpoint in the middle of the second epoch\n+        # 2.1 Resume training from the second batch of epoch 1 (target_ckpt_num = 7)\n+        # 1 epoch consists of 10 points, so 5 steps with batch size 2\n+        target_ckpt_num = 7\n+        checkpoint_path = os.path.join(exp_dir_baseline, f\"checkpoint-{target_ckpt_num - 1}\")\n+\n+        set_seed(42)\n+        model_resume = DummyModel(size=10)\n+\n+        exp_dir_resume = self.get_auto_remove_tmp_dir()\n+        args_resume = TrainingArguments(\n+            output_dir=str(exp_dir_resume),\n+            seed=42,\n+            learning_rate=0.1,\n+            per_device_train_batch_size=2,\n+            gradient_accumulation_steps=1,\n+            save_strategy=\"steps\",\n+            save_steps=1,\n+            num_train_epochs=3,\n+            optim=\"sgd\",\n+            disable_tqdm=True,\n+            dataloader_num_workers=0,  # Ensures that main process loads the data\n+            report_to=[],  # Disable wandb/tensorboard and other loggers\n+        )\n+\n+        trainer_resume = Trainer(\n+            model=model_resume,\n+            args=args_resume,\n+            train_dataset=train_dataset,\n+        )\n+\n+        trainer_resume.train(resume_from_checkpoint=checkpoint_path)\n+\n+        # 2.2 Get the data order from the last saved checkpoint for the resumed run\n+        resumed_state_dict = safetensors.torch.load_file(\n+            os.path.join(exp_dir_resume, f\"checkpoint-{last_ckpt_num}\", \"model.safetensors\")\n+        )\n+        resumed_data_order = resumed_state_dict[\"data_order\"]\n+\n+        # 3. Compare results: the data order should be identical\n+        self.assertTrue(\n+            torch.equal(baseline_data_order, resumed_data_order),\n+            f\"Data order mismatch after checkpoint deletion and resume.\\n\"\n+            f\"Baseline: {baseline_data_order}\\n\"\n+            f\"Resumed: {resumed_data_order}\",\n+        )\n+\n \n @require_torch\n @is_staging_test"
        }
    ],
    "stats": {
        "total": 149,
        "additions": 146,
        "deletions": 3
    }
}