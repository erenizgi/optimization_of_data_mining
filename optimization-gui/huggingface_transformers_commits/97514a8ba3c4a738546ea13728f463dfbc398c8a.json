{
    "author": "wanxiangchwng",
    "message": "chore: fix some typos (#34891)\n\nSigned-off-by: wanxiangchwng <cui.shuang@foxmail.com>",
    "sha": "97514a8ba3c4a738546ea13728f463dfbc398c8a",
    "files": [
        {
            "sha": "d106c32defa4db3577d3a72158550d355aa5ece7",
            "filename": "src/transformers/generation/flax_logits_process.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fgeneration%2Fflax_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fgeneration%2Fflax_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fflax_logits_process.py?ref=97514a8ba3c4a738546ea13728f463dfbc398c8a",
            "patch": "@@ -273,7 +273,7 @@ class FlaxSuppressTokensAtBeginLogitsProcessor(FlaxLogitsProcessor):\n     r\"\"\"\n     [`FlaxLogitsProcessor`] supressing a list of tokens as soon as the `generate` function starts generating using\n     `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens` are not sampled at the\n-    begining of the generation.\n+    beginning of the generation.\n \n     Args:\n         begin_suppress_tokens (`List[int]`):"
        },
        {
            "sha": "39a38f9139ec1b4cc752c67fecac018edf7cbf79",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=97514a8ba3c4a738546ea13728f463dfbc398c8a",
            "patch": "@@ -1782,7 +1782,7 @@ class SuppressTokensAtBeginLogitsProcessor(LogitsProcessor):\n     r\"\"\"\n     [`SuppressTokensAtBeginLogitsProcessor`] supresses a list of tokens as soon as the `generate` function starts\n     generating using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens` are\n-    not generated at the begining. Originally created for\n+    not generated at the beginning. Originally created for\n     [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).\n \n     Examples:"
        },
        {
            "sha": "f70655fb7c1d0b502dfc8f1c4f2c491867aa7e8c",
            "filename": "src/transformers/generation/tf_logits_process.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py?ref=97514a8ba3c4a738546ea13728f463dfbc398c8a",
            "patch": "@@ -512,7 +512,7 @@ class TFSuppressTokensAtBeginLogitsProcessor(TFLogitsProcessor):\n     r\"\"\"\n     [`TFSuppressTokensAtBeginLogitsProcessor`] suppresses a list of tokens as soon as the `generate` function starts\n     generating using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens` at not\n-    sampled at the begining of the generation.\n+    sampled at the beginning of the generation.\n     \"\"\"\n \n     def __init__(self, begin_suppress_tokens, begin_index):"
        },
        {
            "sha": "3aabe979d8ef2fc620d30946be3ee387f3a9a991",
            "filename": "src/transformers/models/jamba/configuration_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py?ref=97514a8ba3c4a738546ea13728f463dfbc398c8a",
            "patch": "@@ -114,7 +114,7 @@ class JambaConfig(PretrainedConfig):\n         mamba_expand (`int`, *optional*, defaults to 2):\n             Expanding factor (relative to hidden_size) used to determine the mamba intermediate size\n         mamba_dt_rank (`Union[int,str]`, *optional*, defaults to `\"auto\"`):\n-            Rank of the the mamba discretization projection matrix. `\"auto\"` means that it will default to `math.ceil(self.hidden_size / 16)`\n+            Rank of the mamba discretization projection matrix. `\"auto\"` means that it will default to `math.ceil(self.hidden_size / 16)`\n         mamba_conv_bias (`bool`, *optional*, defaults to `True`):\n             Flag indicating whether or not to use bias in the convolution layer of the mamba mixer block.\n         mamba_proj_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "a185d5ebc6e86c768d30e7db86f1d43dee05ab8d",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=97514a8ba3c4a738546ea13728f463dfbc398c8a",
            "patch": "@@ -852,7 +852,7 @@ class JambaSparseMoeBlock(nn.Module):\n     This implementation is\n     strictly equivalent to standard MoE with full capacity (no\n     dropped tokens). It's faster since it formulates MoE operations\n-    in terms of block-sparse operations to accomodate imbalanced\n+    in terms of block-sparse operations to accommodate imbalanced\n     assignments of tokens to experts, whereas standard MoE either\n     (1) drop tokens at the cost of reduced performance or (2) set\n     capacity factor to number of experts and thus waste computation"
        },
        {
            "sha": "0f04ef255c431d9841608f2d3ae9eae23c9602ac",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=97514a8ba3c4a738546ea13728f463dfbc398c8a",
            "patch": "@@ -607,7 +607,7 @@ class MixtralSparseMoeBlock(nn.Module):\n     This implementation is\n     strictly equivalent to standard MoE with full capacity (no\n     dropped tokens). It's faster since it formulates MoE operations\n-    in terms of block-sparse operations to accomodate imbalanced\n+    in terms of block-sparse operations to accommodate imbalanced\n     assignments of tokens to experts, whereas standard MoE either\n     (1) drop tokens at the cost of reduced performance or (2) set\n     capacity factor to number of experts and thus waste computation"
        },
        {
            "sha": "82abfa66c2e8373f0fd6b52134f8dd18a35fa452",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=97514a8ba3c4a738546ea13728f463dfbc398c8a",
            "patch": "@@ -2527,7 +2527,7 @@ def build_delay_pattern_mask(\n         - [ B, -1, -1, -1, -1, -1]\n         - [ B, -1, -1, -1, -1, -1]\n         - [ B, -1, -1, -1, -1, -1]\n-        where B is the begining-of-sentence token, P is the special padding token id and -1 indicates that the token is valid for prediction. If we include\n+        where B is the beginning-of-sentence token, P is the special padding token id and -1 indicates that the token is valid for prediction. If we include\n         a prompt (input ids), the -1 positions indicate where new tokens should be predicted. Otherwise, the\n         mask is set to the value in the prompt:\n         - [ a0, a1, -1, -1, -1,  P]"
        },
        {
            "sha": "82763ccea62e4c3d647e915feb7aeb36fd90e8ec",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=97514a8ba3c4a738546ea13728f463dfbc398c8a",
            "patch": "@@ -735,7 +735,7 @@ class PhimoeSparseMoeBlock(nn.Module):\n     This implementation is\n     strictly equivalent to standard MoE with full capacity (no\n     dropped tokens). It's faster since it formulates MoE operations\n-    in terms of block-sparse operations to accomodate imbalanced\n+    in terms of block-sparse operations to accommodate imbalanced\n     assignments of tokens to experts, whereas standard MoE either\n     (1) drop tokens at the cost of reduced performance or (2) set\n     capacity factor to number of experts and thus waste computation"
        },
        {
            "sha": "a28730371631978d5d2513a65d51e76c0a1c59ce",
            "filename": "src/transformers/models/whisper/modeling_tf_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py?ref=97514a8ba3c4a738546ea13728f463dfbc398c8a",
            "patch": "@@ -1646,7 +1646,7 @@ def generate(\n             prompt_ids = prompt_ids.tolist()\n             decoder_start_token_id, *text_prompt_ids = prompt_ids\n             # Slicing the text prompt ids in a manner consistent with the OpenAI implementation\n-            # to accomodate context space for the prefix (see https://github.com/openai/whisper/blob/c09a7ae299c4c34c5839a76380ae407e7d785914/whisper/decoding.py#L599)\n+            # to accommodate context space for the prefix (see https://github.com/openai/whisper/blob/c09a7ae299c4c34c5839a76380ae407e7d785914/whisper/decoding.py#L599)\n             text_prompt_ids = text_prompt_ids[-self.config.max_length // 2 - 1 :]\n             # Set the decoder_start_token_id to <|startofprev|>\n             kwargs.update({\"decoder_start_token_id\": decoder_start_token_id})"
        },
        {
            "sha": "77aa940141f295f59368127c28b44bc943b88df6",
            "filename": "src/transformers/models/zamba/configuration_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fzamba%2Fconfiguration_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Fmodels%2Fzamba%2Fconfiguration_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fconfiguration_zamba.py?ref=97514a8ba3c4a738546ea13728f463dfbc398c8a",
            "patch": "@@ -106,7 +106,7 @@ class ZambaConfig(PretrainedConfig):\n         mamba_expand (`int`, *optional*, defaults to 2):\n             Expanding factor (relative to hidden_size) used to determine the mamba intermediate size\n         mamba_dt_rank (`Union[int,str]`, *optional*, defaults to `\"auto\"`):\n-            Rank of the the mamba discretization projection matrix. `\"auto\"` means that it will default to `math.ceil(self.hidden_size / 16)`\n+            Rank of the mamba discretization projection matrix. `\"auto\"` means that it will default to `math.ceil(self.hidden_size / 16)`\n         time_step_min (`float`, *optional*, defaults to 0.001):\n             Minimum `time_step` used to bound `dt_proj_bias`.\n         time_step_max (`float`, *optional*, defaults to 0.1):"
        },
        {
            "sha": "ca5a3bb9c20412fffc819421f9f993a6c31e2f29",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97514a8ba3c4a738546ea13728f463dfbc398c8a/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=97514a8ba3c4a738546ea13728f463dfbc398c8a",
            "patch": "@@ -1000,7 +1000,7 @@ def add_tokens(\n     ) -> int:\n         \"\"\"\n         Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n-        it with indices starting from length of the current vocabulary and and will be isolated before the tokenization\n+        it with indices starting from length of the current vocabulary and will be isolated before the tokenization\n         algorithm is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm are therefore\n         not treated in the same way.\n "
        }
    ],
    "stats": {
        "total": 22,
        "additions": 11,
        "deletions": 11
    }
}