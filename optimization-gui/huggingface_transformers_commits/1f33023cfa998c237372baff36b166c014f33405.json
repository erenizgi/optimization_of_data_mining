{
    "author": "Cyrilvallez",
    "message": "Flash-attn performance: remove cuda sync during inference (#33570)\n\nSwitch conditions to use short-circuit during inference",
    "sha": "1f33023cfa998c237372baff36b166c014f33405",
    "files": [
        {
            "sha": "da961c6060e4992a5fd0b836f1a9c26c777699b9",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1f33023cfa998c237372baff36b166c014f33405/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1f33023cfa998c237372baff36b166c014f33405/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=1f33023cfa998c237372baff36b166c014f33405",
            "patch": "@@ -267,7 +267,8 @@ def _flash_attention_forward(\n     # If position_ids is provided and check all examples do not contain only 1 sequence, If tensor in increasing\n     # then we probably have one sequence, otherwise it is packed. Additionally check we are in pre-fill/training stage.\n     # Use `flash_attn_varlen_func` to prevent cross-example attention and also allow padding free approach\n-    elif position_ids is not None and not (torch.diff(position_ids, dim=-1) >= 0).all() and query_length != 1:\n+    # Note: the `torch.diff(...)` condition is last to use short-circuit and avoid the cuda synchronization it incurs during inference (query_length == 1 always)\n+    elif position_ids is not None and query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all():\n         batch_size = query_states.size(0)\n         query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = prepare_fa2_from_position_ids(\n             query_states, key_states, value_states, position_ids"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
    }
}