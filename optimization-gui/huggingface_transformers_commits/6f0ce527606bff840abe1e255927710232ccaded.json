{
    "author": "gante",
    "message": "VLM Generate: tag `test_static_cache_matches_dynamic` as flaky (#33630)\n\nflaky",
    "sha": "6f0ce527606bff840abe1e255927710232ccaded",
    "files": [
        {
            "sha": "705631a460d8895cd4af59e57a076ce52c47b770",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f0ce527606bff840abe1e255927710232ccaded/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f0ce527606bff840abe1e255927710232ccaded/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=6f0ce527606bff840abe1e255927710232ccaded",
            "patch": "@@ -4714,6 +4714,7 @@ def test_custom_4d_attention_mask(self):\n             normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n             torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n \n+    @is_flaky(max_attempts=10)  # TODO @raushan: this test is VERY flaky on some VLMs, like paligemma\n     def test_static_cache_matches_dynamic(self):\n         \"\"\"\n         Tests that generating with static cache give almost same results as with dynamic cache.\n@@ -4749,7 +4750,7 @@ def test_static_cache_matches_dynamic(self):\n                 output_logits=True,\n                 return_dict_in_generate=True,\n             )\n-            self.assertTrue(torch.allclose(dynamic_out.logits[0], static_out.logits[0], rtol=1e-3, atol=1e-3))\n+            self.assertTrue(torch.allclose(dynamic_out.logits[0], static_out.logits[0], rtol=1e-3, atol=1e-4))\n \n     # For now, Let's focus only on GPU for `torch.compile`\n     @slow"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
    }
}