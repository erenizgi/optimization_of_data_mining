{
    "author": "SunMarc",
    "message": "Fix minor bugs in ConversionOps and WeightConverter (#42479)\n\n* style\n\n* fix\n\n* fix\n\n* Revert \"fix\"\n\nThis reverts commit b7fc6aa2688f4a8ca14047411c78f25822da7499.\n\n* remove reverse for now as not needed\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "4abfd165e6175012a6509284b3ad482235eab1d7",
    "files": [
        {
            "sha": "4505f2a84376bff1871a54e38c75de05cc9821e3",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abfd165e6175012a6509284b3ad482235eab1d7/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abfd165e6175012a6509284b3ad482235eab1d7/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=4abfd165e6175012a6509284b3ad482235eab1d7",
            "patch": "@@ -105,7 +105,10 @@ class ConversionOps:\n     \"\"\"Base class for weight conversion operations.\"\"\"\n \n     def __repr__(self):\n-        return f\"{self.__class__.__name__}(dim={self.dim})\"\n+        if hasattr(self, \"dim\"):\n+            return f\"{self.__class__.__name__}(dim={self.dim})\"\n+        else:\n+            return f\"{self.__class__.__name__}\"\n \n     @abstractmethod\n     def convert(\n@@ -435,6 +438,7 @@ def convert(\n                     source_patterns=self.source_patterns,\n                     target_patterns=self.target_patterns,\n                     # Additional kwargs, ususally not used\n+                    full_layer_name=layer_name,\n                     model=model,\n                     config=config,\n                     missing_keys=missing_keys,\n@@ -448,7 +452,6 @@ def convert(\n         prefix, _, suffix = next(full_name.partition(k) for k in collected_tensors.keys() if k in full_name)\n         # Rename the tensors\n         collected_tensors = {prefix + k + suffix: v for k, v in collected_tensors.items()}\n-\n         if hf_quantizer is not None and self.quantization_operation is not None:\n             with log_to_misc(layer_name, misc, (collected_tensors, layer_name), self.quantization_operation):\n                 collected_tensors = self.quantization_operation.convert("
        },
        {
            "sha": "9df71fd70d3bf2aa51d00fbe4e951d4950404707",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abfd165e6175012a6509284b3ad482235eab1d7/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abfd165e6175012a6509284b3ad482235eab1d7/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=4abfd165e6175012a6509284b3ad482235eab1d7",
            "patch": "@@ -75,15 +75,14 @@ def convert(\n         Deserialization of bnb keys. We need 6 keys to recreate the quantized weights\n         \"\"\"\n         if len(input_dict) == 1:\n-            # special case when we only fetched the weight\n-            # since we collected keys, we need to return it like that\n-            return {full_layer_name: input_dict[\"weight\"]}\n+            return input_dict\n \n         for key, value in input_dict.items():\n             if isinstance(value, list):\n                 input_dict[key] = value[0]\n \n-        weight = input_dict.pop(\"weight\")\n+        key_weight = \"weight\"\n+        weight = input_dict.pop(key_weight)\n         module, _ = get_module_from_name(model, full_layer_name)\n         new_value = bnb.nn.Params4bit.from_prequantized(\n             data=weight,\n@@ -93,7 +92,7 @@ def convert(\n             module=module,\n         )\n         module._is_hf_initialized = True\n-        return {full_layer_name: new_value}\n+        return {key_weight: new_value}\n \n \n class Bnb8bitQuantize(ConversionOps):\n@@ -140,20 +139,21 @@ def convert(\n         if len(input_dict) == 1:\n             # special case when we only fetched the weight\n             # since we collected keys, we need to return it like that\n-            return {full_layer_name: input_dict[\"weight\"]}\n+            return input_dict\n \n         for key, value in input_dict.items():\n             if isinstance(value, list):\n                 input_dict[key] = value[0]\n \n         module, _ = get_module_from_name(model, full_layer_name)\n \n-        weight = input_dict[\"weight\"]\n+        key_weight = \"weight\"\n+        weight = input_dict[key_weight]\n         kwargs = model.get_parameter_or_buffer(full_layer_name).__dict__\n         kwargs[\"SCB\"] = input_dict[\"SCB\"]\n         new_value = bnb.nn.Int8Params(weight, requires_grad=False, **kwargs).to(weight.device)\n         module._is_hf_initialized = True\n-        return {full_layer_name: new_value}\n+        return {key_weight: new_value}\n \n \n def _replace_with_bnb_linear("
        },
        {
            "sha": "d35aa9919dfba67fb5cd836475522fd2e6fbebe2",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4abfd165e6175012a6509284b3ad482235eab1d7/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4abfd165e6175012a6509284b3ad482235eab1d7/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=4abfd165e6175012a6509284b3ad482235eab1d7",
            "patch": "@@ -579,7 +579,6 @@ class Fp8Quantize(ConversionOps):\n \n     def __init__(self, hf_quantizer):\n         self.hf_quantizer = hf_quantizer\n-        self.reverse_op = Fp8Dequantize\n \n     def convert(self, input_dict: torch.Tensor, **kwargs) -> dict[str, torch.Tensor]:\n         # Unpack single key/value (value may be wrapped in a list)\n@@ -655,7 +654,6 @@ class Fp8Dequantize(ConversionOps):\n \n     def __init__(self, block_size: tuple[int, int] | None = None):\n         self.block_size = block_size\n-        self.reverse_op = Fp8Quantize\n \n     def convert(\n         self,"
        }
    ],
    "stats": {
        "total": 25,
        "additions": 13,
        "deletions": 12
    }
}