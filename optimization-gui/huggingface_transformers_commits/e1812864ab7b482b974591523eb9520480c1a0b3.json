{
    "author": "jcaip",
    "message": "[docs] Add int4wo + 2:4 sparsity example to TorchAO README (#38592)\n\n* update quantization readme\n\n* update\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "e1812864ab7b482b974591523eb9520480c1a0b3",
    "files": [
        {
            "sha": "6269294a332dc250fcf52420df47e43f0f3b16fc",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 63,
            "deletions": 0,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1812864ab7b482b974591523eb9520480c1a0b3/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1812864ab7b482b974591523eb9520480c1a0b3/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=e1812864ab7b482b974591523eb9520480c1a0b3",
            "patch": "@@ -38,6 +38,7 @@ torchao supports the [quantization techniques](https://github.com/pytorch/ao/blo\n - A8W8 Int8 Dynamic Quantization\n - A16W8 Int8 Weight Only Quantization\n - A16W4 Int4 Weight Only Quantization\n+- A16W4 Int4 Weight Only Quantization + 2:4 Sparsity\n - Autoquantization\n \n torchao also supports module level configuration by specifying a dictionary from fully qualified name of module and its corresponding quantization config. This allows skip quantizing certain layers and using different quantization config for different modules.\n@@ -147,6 +148,37 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n </hfoption>\n </hfoptions>\n \n+</hfoption>\n+<hfoption id=\"int4-weight-only-24sparse\">\n+\n+```py\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Int4WeightOnlyConfig\n+from torchao.dtypes import MarlinSparseLayout\n+\n+quant_config = Int4WeightOnlyConfig(layout=MarlinSparseLayout())\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+\n+# Load and quantize the model with sparsity. A sparse checkpoint is needed to accelerate without accuraccy loss\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"RedHatAI/Sparse-Llama-3.1-8B-2of4\",\n+    torch_dtype=torch.float16,\n+    device_map=\"cuda\",\n+    quantization_config=quantization_config\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"RedHatAI/Sparse-Llama-3.1-8B-2of4\")\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+\n+# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n+output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+</hfoption>\n+</hfoptions>\n+\n ### A100 GPU\n <hfoptions id=\"examples-A100-GPU\">\n <hfoption id=\"int8-dynamic-and-weight-only\">\n@@ -215,6 +247,37 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n </hfoption>\n </hfoptions>\n \n+</hfoption>\n+<hfoption id=\"int4-weight-only-24sparse\">\n+\n+```py\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Int4WeightOnlyConfig\n+from torchao.dtypes import MarlinSparseLayout\n+\n+quant_config = Int4WeightOnlyConfig(layout=MarlinSparseLayout())\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+\n+# Load and quantize the model with sparsity. A sparse checkpoint is needed to accelerate without accuraccy loss\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"RedHatAI/Sparse-Llama-3.1-8B-2of4\",\n+    torch_dtype=torch.float16,\n+    device_map=\"cuda\",\n+    quantization_config=quantization_config\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"RedHatAI/Sparse-Llama-3.1-8B-2of4\")\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+\n+# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n+output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+</hfoption>\n+</hfoptions>\n+\n ### CPU\n <hfoptions id=\"examples-CPU\">\n <hfoption id=\"int8-dynamic-and-weight-only\">"
        }
    ],
    "stats": {
        "total": 63,
        "additions": 63,
        "deletions": 0
    }
}