{
    "author": "SunMarc",
    "message": "Remove all instances of `is_safetensors_available` (#41233)\n\n* safetensors is a core dep\n\n* fix\n\n* ok\n\n* simplify branching\n\n* keep it for now\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "d848a3953aeb7debdbfbc6bfbf788c528cd40e46",
    "files": [
        {
            "sha": "328e42fb2ea986606289205966442ae9c415317c",
            "filename": "src/transformers/commands/env.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Fcommands%2Fenv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Fcommands%2Fenv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fenv.py?ref=d848a3953aeb7debdbfbc6bfbf788c528cd40e46",
            "patch": "@@ -14,7 +14,6 @@\n \n \n import contextlib\n-import importlib.util\n import io\n import os\n import platform\n@@ -26,7 +25,6 @@\n from ..integrations.deepspeed import is_deepspeed_available\n from ..utils import (\n     is_accelerate_available,\n-    is_safetensors_available,\n     is_torch_available,\n     is_torch_hpu_available,\n     is_torch_npu_available,\n@@ -59,18 +57,13 @@ def __init__(self, accelerate_config_file, *args) -> None:\n         self._accelerate_config_file = accelerate_config_file\n \n     def run(self):\n-        safetensors_version = \"not installed\"\n-        if is_safetensors_available():\n-            import safetensors\n+        import safetensors\n \n-            safetensors_version = safetensors.__version__\n-        elif importlib.util.find_spec(\"safetensors\") is not None:\n-            import safetensors\n-\n-            safetensors_version = f\"{safetensors.__version__} but is ignored because of PyTorch version too old.\"\n+        safetensors_version = safetensors.__version__\n \n         accelerate_version = \"not installed\"\n         accelerate_config = accelerate_config_str = \"not found\"\n+\n         if is_accelerate_available():\n             import accelerate\n             from accelerate.commands.config import default_config_file, load_config_from_file"
        },
        {
            "sha": "ba75731507e6ecc08f95a37b3cdb6a976df2c218",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 29,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=d848a3953aeb7debdbfbc6bfbf788c528cd40e46",
            "patch": "@@ -37,6 +37,9 @@\n import torch\n from huggingface_hub import split_torch_state_dict_into_shards\n from packaging import version\n+from safetensors import safe_open\n+from safetensors.torch import load_file as safe_load_file\n+from safetensors.torch import save_file as safe_save_file\n from torch import Tensor, nn\n from torch.distributions import constraints\n from torch.utils.checkpoint import checkpoint\n@@ -97,7 +100,6 @@\n     is_optimum_available,\n     is_peft_available,\n     is_remote_url,\n-    is_safetensors_available,\n     is_torch_flex_attn_available,\n     is_torch_greater_or_equal,\n     is_torch_mlu_available,\n@@ -134,11 +136,6 @@\n     if accelerate_version >= version.parse(\"0.31\"):\n         from accelerate.utils.modeling import get_state_dict_from_offload\n \n-if is_safetensors_available():\n-    from safetensors import safe_open\n-    from safetensors.torch import load_file as safe_load_file\n-    from safetensors.torch import save_file as safe_save_file\n-\n if is_peft_available():\n     from .utils import find_adapter_config_file\n \n@@ -403,24 +400,11 @@ def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n     index_present = os.path.isfile(index_file)\n     safe_index_present = os.path.isfile(safe_index_file)\n \n-    if not index_present and not (safe_index_present and is_safetensors_available()):\n-        filenames = (\n-            (WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_INDEX_NAME) if is_safetensors_available() else (WEIGHTS_INDEX_NAME,)\n-        )\n+    if not index_present and not safe_index_present:\n+        filenames = (WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_INDEX_NAME)\n         raise ValueError(f\"Can't find a checkpoint index ({' or '.join(filenames)}) in {folder}.\")\n \n-    load_safe = False\n-    if safe_index_present:\n-        if prefer_safe:\n-            if is_safetensors_available():\n-                load_safe = True  # load safe due to preference\n-            else:\n-                logger.warning(\n-                    f\"Cannot load sharded checkpoint at {folder} safely since safetensors is not installed!\"\n-                )\n-        elif not index_present:\n-            load_safe = True  # load safe since we have no other choice\n-\n+    load_safe = safe_index_present and (prefer_safe or not index_present)\n     load_index = safe_index_file if load_safe else index_file\n \n     with open(load_index, \"r\", encoding=\"utf-8\") as f:\n@@ -493,7 +477,7 @@ def load_state_dict(\n     Reads a `safetensor` or a `.bin` checkpoint file. We load the checkpoint on \"cpu\" by default.\n     \"\"\"\n     # Use safetensors if possible\n-    if checkpoint_file.endswith(\".safetensors\") and is_safetensors_available():\n+    if checkpoint_file.endswith(\".safetensors\"):\n         with safe_open(checkpoint_file, framework=\"pt\") as f:\n             state_dict = {}\n             for k in f.keys():\n@@ -3744,8 +3728,6 @@ def save_pretrained(\n                 \"`save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\"\n             )\n             is_main_process = kwargs.pop(\"save_config\")\n-        if safe_serialization and not is_safetensors_available():\n-            raise ImportError(\"`safe_serialization` requires the `safetensors library: `pip install safetensors`.\")\n \n         # we need to check against tp_size, not tp_plan, as tp_plan is substituted to the class one\n         if self._tp_size is not None and not is_huggingface_hub_greater_or_equal(\"0.31.4\"):\n@@ -4584,9 +4566,6 @@ def from_pretrained(\n         if token is not None and adapter_kwargs is not None and \"token\" not in adapter_kwargs:\n             adapter_kwargs[\"token\"] = token\n \n-        if use_safetensors is None and not is_safetensors_available():\n-            use_safetensors = False\n-\n         if gguf_file is not None and not is_accelerate_available():\n             raise ValueError(\"accelerate is required when loading a GGUF file `pip install accelerate`.\")\n \n@@ -4779,7 +4758,7 @@ def from_pretrained(\n         is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None\n \n         # Just a helpful message in case we try to load safetensors files coming from old Transformers tf/flax classes\n-        if is_safetensors_available() and is_from_file and checkpoint_files[0].endswith(\".safetensors\"):\n+        if is_from_file and checkpoint_files[0].endswith(\".safetensors\"):\n             with safe_open(checkpoint_files[0], framework=\"pt\") as f:\n                 metadata = f.metadata()\n             if metadata is not None and metadata.get(\"format\") in [\"tf\", \"flax\"]:"
        },
        {
            "sha": "c4d73ec30d48864c3fea6374aad120a5b762808e",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=d848a3953aeb7debdbfbc6bfbf788c528cd40e46",
            "patch": "@@ -21,6 +21,7 @@\n \n import numpy as np\n import torch\n+from safetensors.torch import load_file as safe_load_file\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n@@ -50,7 +51,6 @@\n     cached_file,\n     check_torch_load_is_safe,\n     is_peft_available,\n-    is_safetensors_available,\n     is_torch_flex_attn_available,\n     logging,\n )\n@@ -60,10 +60,6 @@\n WAV2VEC2_ADAPTER_PT_FILE = \"adapter.{}.bin\"\n WAV2VEC2_ADAPTER_SAFE_FILE = \"adapter.{}.safetensors\"\n \n-if is_safetensors_available():\n-    from safetensors.torch import load_file as safe_load_file\n-\n-\n if is_torch_flex_attn_available():\n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n@@ -1214,7 +1210,7 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n         token = kwargs.pop(\"token\", None)\n         use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         revision = kwargs.pop(\"revision\", None)\n-        use_safetensors = kwargs.pop(\"use_safetensors\", None if is_safetensors_available() else False)\n+        use_safetensors = kwargs.pop(\"use_safetensors\", None)\n \n         if use_auth_token is not None:\n             warnings.warn("
        },
        {
            "sha": "62357c8e0dcb34bf597e414c567491b38fff8e1d",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=d848a3953aeb7debdbfbc6bfbf788c528cd40e46",
            "patch": "@@ -27,11 +27,7 @@\n     XVectorOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    ModelOutput,\n-    auto_docstring,\n-    is_peft_available,\n-)\n+from ...utils import ModelOutput, auto_docstring, is_peft_available\n from .configuration_wav2vec2_conformer import Wav2Vec2ConformerConfig\n \n "
        },
        {
            "sha": "6538d0c6122b3d37a9fb3b242034eb830c1688f6",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=d848a3953aeb7debdbfbc6bfbf788c528cd40e46",
            "patch": "@@ -26,12 +26,9 @@\n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n \n+from safetensors import safe_open\n \n-from ..utils import is_safetensors_available, is_torch_available, is_torchao_available, logging\n-\n-\n-if is_safetensors_available():\n-    from safetensors import safe_open\n+from ..utils import is_torch_available, is_torchao_available, logging\n \n \n if is_torch_available():\n@@ -436,7 +433,7 @@ def is_compileable(self) -> bool:\n         return True\n \n     def set_metadata(self, checkpoint_files: list[str]):\n-        if checkpoint_files[0].endswith(\".safetensors\") and is_safetensors_available():\n+        if checkpoint_files[0].endswith(\".safetensors\"):\n             metadata = {}\n             for checkpoint in checkpoint_files:\n                 with safe_open(checkpoint, framework=\"pt\") as f:"
        },
        {
            "sha": "2802fd5d25fad218b75630cc6e64ab6806130844",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=d848a3953aeb7debdbfbc6bfbf788c528cd40e46",
            "patch": "@@ -47,6 +47,7 @@\n \n import huggingface_hub.utils as hf_hub_utils\n import numpy as np\n+import safetensors.torch\n import torch\n import torch.distributed as dist\n from huggingface_hub import CommitInfo, ModelCard, create_repo, upload_folder\n@@ -156,7 +157,6 @@\n     is_liger_kernel_available,\n     is_lomo_available,\n     is_peft_available,\n-    is_safetensors_available,\n     is_sagemaker_dp_enabled,\n     is_sagemaker_mp_enabled,\n     is_schedulefree_available,\n@@ -206,14 +206,9 @@\n \n     from .trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat\n \n-\n-if is_safetensors_available():\n-    import safetensors.torch\n-\n if is_peft_available():\n     from peft import PeftModel\n \n-\n if is_accelerate_available():\n     from accelerate import Accelerator, skip_first_batches\n     from accelerate import __version__ as accelerate_version"
        },
        {
            "sha": "e168c1b98a5ce8632430ec3ca7e6a445a9280d27",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=d848a3953aeb7debdbfbc6bfbf788c528cd40e46",
            "patch": "@@ -36,7 +36,6 @@\n     ExplicitEnum,\n     is_accelerate_available,\n     is_ipex_available,\n-    is_safetensors_available,\n     is_sagemaker_dp_enabled,\n     is_sagemaker_mp_enabled,\n     is_torch_available,\n@@ -390,12 +389,12 @@ class TrainingArguments:\n             Whether or not to use PyTorch jit trace for inference.\n         bf16 (`bool`, *optional*, defaults to `False`):\n             Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\n-            NVIDIA architecture or Intel XPU or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\n+            NVIDIA architecture or Intel XPU or using CPU (use_cpu) or Ascend NPU.\n         fp16 (`bool`, *optional*, defaults to `False`):\n             Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n         bf16_full_eval (`bool`, *optional*, defaults to `False`):\n             Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n-            metric values. This is an experimental API and it may change.\n+            metric values.\n         fp16_full_eval (`bool`, *optional*, defaults to `False`):\n             Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n             metric values.\n@@ -1542,10 +1541,7 @@ def __post_init__(self):\n                         f\"steps, but found {self.save_steps}, which is not a round multiple of {self.eval_steps}.\"\n                     )\n \n-        safetensors_available = is_safetensors_available()\n-        if self.save_safetensors and not safetensors_available:\n-            raise ValueError(f\"--save_safetensors={self.save_safetensors} requires safetensors to be installed!\")\n-        if not self.save_safetensors and safetensors_available:\n+        if not self.save_safetensors:\n             logger.info(\n                 f\"Found safetensors installation, but --save_safetensors={self.save_safetensors}. \"\n                 f\"Safetensors should be a preferred weights saving format due to security and performance reasons. \""
        },
        {
            "sha": "bc48a7b005ab35cc1494663380e8aa55b7187490",
            "filename": "tests/models/auto/test_modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py?ref=d848a3953aeb7debdbfbc6bfbf788c528cd40e46",
            "patch": "@@ -22,7 +22,7 @@\n import pytest\n \n import transformers\n-from transformers import BertConfig, GPT2Model, is_safetensors_available, is_torch_available\n+from transformers import BertConfig, GPT2Model, is_torch_available\n from transformers.models.auto.configuration_auto import CONFIG_MAPPING\n from transformers.testing_utils import (\n     DUMMY_UNKNOWN_IDENTIFIER,\n@@ -107,7 +107,7 @@ def test_model_from_pretrained(self):\n         self.assertEqual(len(loading_info[\"missing_keys\"]), 0)\n         # When using PyTorch checkpoint, the expected value is `8`. With `safetensors` checkpoint (if it is\n         # installed), the expected value becomes `7`.\n-        EXPECTED_NUM_OF_UNEXPECTED_KEYS = 7 if is_safetensors_available() else 8\n+        EXPECTED_NUM_OF_UNEXPECTED_KEYS = 7\n         self.assertEqual(len(loading_info[\"unexpected_keys\"]), EXPECTED_NUM_OF_UNEXPECTED_KEYS)\n         self.assertEqual(len(loading_info[\"mismatched_keys\"]), 0)\n         self.assertEqual(len(loading_info[\"error_msgs\"]), 0)"
        },
        {
            "sha": "bc4df3cbfab9efaea73543541ae361533485490f",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=d848a3953aeb7debdbfbc6bfbf788c528cd40e46",
            "patch": "@@ -84,7 +84,6 @@\n     require_flash_attn_3,\n     require_kernels,\n     require_non_hpu,\n-    require_safetensors,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n@@ -2450,7 +2449,6 @@ def check_same_values(layer_1, layer_2):\n             params_tied_2 = list(model_tied.parameters())\n             self.assertEqual(len(params_tied_2), len(params_tied))\n \n-    @require_safetensors\n     def test_can_use_safetensors(self):\n         for model_class in self.all_model_classes:\n             config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "9a80f0032e7afadf44e4033bb8316a7719c89167",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=d848a3953aeb7debdbfbc6bfbf788c528cd40e46",
            "patch": "@@ -87,7 +87,6 @@\n     require_optuna,\n     require_peft,\n     require_ray,\n-    require_safetensors,\n     require_schedulefree,\n     require_sentencepiece,\n     require_sigopt,\n@@ -123,7 +122,6 @@\n     is_accelerate_available,\n     is_apex_available,\n     is_bitsandbytes_available,\n-    is_safetensors_available,\n     is_torchao_available,\n     is_torchdistx_available,\n )\n@@ -138,6 +136,7 @@\n     ATOL = 1e-5\n \n if is_torch_available():\n+    import safetensors.torch\n     import torch\n     from torch import nn\n     from torch.utils.data import IterableDataset\n@@ -160,9 +159,6 @@\n     )\n     from transformers.trainer_pt_utils import AcceleratorConfig\n \n-    if is_safetensors_available():\n-        import safetensors.torch\n-\n if is_datasets_available():\n     import datasets\n \n@@ -3177,7 +3173,6 @@ def test_save_checkpoints(self):\n         trainer.train()\n         self.check_saved_checkpoints(tmp_dir, 5, int(self.n_epochs * 64 / self.batch_size), False)\n \n-    @require_safetensors\n     def test_safe_checkpoints(self):\n         for save_safetensors in [True, False]:\n             tmp_dir = self.get_auto_remove_tmp_dir()\n@@ -3628,7 +3623,6 @@ def test_resume_training_with_shard_checkpoint(self):\n             self.assertEqual(b, b1)\n             self.check_trainer_state_are_the_same(state, state1)\n \n-    @require_safetensors\n     @require_torch_up_to_2_accelerators\n     def test_resume_training_with_safe_checkpoint(self):\n         # This test will fail for more than 2 GPUs since the batch size will get bigger and with the number of\n@@ -3813,7 +3807,6 @@ def test_load_best_model_at_end(self):\n             self.check_saved_checkpoints(tmpdir, 5, total, is_pretrained=False)\n             self.check_best_model_has_been_loaded(tmpdir, 5, total, trainer, \"eval_loss\", is_pretrained=False)\n \n-    @require_safetensors\n     def test_load_best_model_from_safetensors(self):\n         total = int(self.n_epochs * 64 / self.batch_size)\n         for save_safetensors, pretrained in product([False, True], [False, True]):\n@@ -5203,7 +5196,6 @@ def test_trainer_works_without_model_config(self):\n             )\n             trainer.train()\n \n-    @require_safetensors\n     def test_resume_from_interrupted_training(self):\n         \"\"\"\n         Tests resuming training from a checkpoint after a simulated interruption."
        },
        {
            "sha": "ca55e59549b748829c017b5f00e57e5168483bcd",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d848a3953aeb7debdbfbc6bfbf788c528cd40e46/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=d848a3953aeb7debdbfbc6bfbf788c528cd40e46",
            "patch": "@@ -70,7 +70,6 @@\n     require_accelerate,\n     require_non_hpu,\n     require_read_token,\n-    require_safetensors,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_multi_accelerator,\n@@ -875,7 +874,6 @@ def test_checkpoint_variant_local_sharded_bin(self):\n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n             torch.testing.assert_close(p1, p2)\n \n-    @require_safetensors\n     def test_checkpoint_variant_local_safe(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n \n@@ -896,7 +894,6 @@ def test_checkpoint_variant_local_safe(self):\n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n             torch.testing.assert_close(p1, p2)\n \n-    @require_safetensors\n     def test_checkpoint_variant_local_sharded_safe(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n \n@@ -1007,7 +1004,6 @@ def test_checkpoint_variant_hub_sharded(self):\n             )\n         self.assertIsNotNone(model)\n \n-    @require_safetensors\n     def test_checkpoint_variant_hub_safe(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             with self.assertRaises(EnvironmentError):\n@@ -1017,7 +1013,6 @@ def test_checkpoint_variant_hub_safe(self):\n             )\n         self.assertIsNotNone(model)\n \n-    @require_safetensors\n     def test_checkpoint_variant_hub_sharded_safe(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             with self.assertRaises(EnvironmentError):\n@@ -1276,7 +1271,6 @@ def test_save_offloaded_model_dynamic_tied_weights_keys(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             model.save_pretrained(tmp_dir)\n \n-    @require_safetensors\n     def test_use_safetensors(self):\n         # Should not raise anymore\n         AutoModel.from_pretrained(\"hf-internal-testing/tiny-random-RobertaModel\", use_safetensors=True)\n@@ -1333,7 +1327,6 @@ def test_use_safetensors(self):\n             \"Error no file named model.safetensors, or pytorch_model.bin\" in str(missing_model_file_error.exception)\n         )\n \n-    @require_safetensors\n     def test_safetensors_save_and_load(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n         with tempfile.TemporaryDirectory() as tmp_dir:\n@@ -1348,7 +1341,6 @@ def test_safetensors_save_and_load(self):\n             for p1, p2 in zip(model.parameters(), new_model.parameters()):\n                 torch.testing.assert_close(p1, p2)\n \n-    @require_safetensors\n     def test_safetensors_load_from_hub(self):\n         safetensors_model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert-safetensors\")\n         pytorch_model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n@@ -1357,7 +1349,6 @@ def test_safetensors_load_from_hub(self):\n         for p1, p2 in zip(safetensors_model.parameters(), pytorch_model.parameters()):\n             torch.testing.assert_close(p1, p2)\n \n-    @require_safetensors\n     def test_safetensors_save_and_load_sharded(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n         with tempfile.TemporaryDirectory() as tmp_dir:\n@@ -1375,7 +1366,6 @@ def test_safetensors_save_and_load_sharded(self):\n             for p1, p2 in zip(model.parameters(), new_model.parameters()):\n                 torch.testing.assert_close(p1, p2)\n \n-    @require_safetensors\n     def test_safetensors_load_from_hub_sharded(self):\n         safetensors_model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert-sharded-safetensors\")\n         pytorch_model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert-sharded\")\n@@ -1589,7 +1579,6 @@ def test_generation_config_is_loaded_with_model(self):\n         model = AutoModelForCausalLM.from_pretrained(TINY_MISTRAL, device_map=\"auto\")\n         self.assertEqual(model.generation_config.bos_token_id, 1)\n \n-    @require_safetensors\n     def test_safetensors_torch_from_torch(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-only\")\n \n@@ -1600,7 +1589,6 @@ def test_safetensors_torch_from_torch(self):\n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n             self.assertTrue(torch.equal(p1, p2))\n \n-    @require_safetensors\n     def test_safetensors_torch_from_torch_sharded(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-only\")\n \n@@ -1636,7 +1624,6 @@ def test_modifying_model_config_gets_moved_to_generation_config(self):\n         self.assertTrue(\"Moving the following attributes\" in str(warning_list[0].message))\n         self.assertTrue(\"repetition_penalty\" in str(warning_list[0].message))\n \n-    @require_safetensors\n     def test_model_from_pretrained_from_mlx(self):\n         from safetensors import safe_open\n "
        }
    ],
    "stats": {
        "total": 119,
        "additions": 24,
        "deletions": 95
    }
}