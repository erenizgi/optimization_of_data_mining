{
    "author": "zucchini-nlp",
    "message": "Load sub-configs from composite configs (#34410)\n\n* save/load sub-configs\r\n\r\n* nit forgot these\r\n\r\n* fix copies\r\n\r\n* move test to common\r\n\r\n* use dict for sub-configs\r\n\r\n* add load-save-laod test\r\n\r\n* clean up modeling check\r\n\r\n* oops this are correct keys\r\n\r\n* fix some tests, missed some composite configs\r\n\r\n* this model was missed",
    "sha": "893ad04fad145904ccb71e4e858e4134c32226b6",
    "files": [
        {
            "sha": "60f9f34cf861c9cdb918a0453621daa5639a73a7",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 4,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -190,6 +190,8 @@ class PretrainedConfig(PushToHubMixin):\n     \"\"\"\n \n     model_type: str = \"\"\n+    base_config_key: str = \"\"\n+    sub_configs: Dict[str, \"PretrainedConfig\"] = {}\n     is_composition: bool = False\n     attribute_map: Dict[str, str] = {}\n     _auto_class: Optional[str] = None\n@@ -543,11 +545,22 @@ def from_pretrained(\n         cls._set_token_in_kwargs(kwargs, token)\n \n         config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n+        if cls.base_config_key and cls.base_config_key in config_dict:\n+            config_dict = config_dict[cls.base_config_key]\n+\n         if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n+            # sometimes the config has no `base_config_key` if the config is used in several composite models\n+            # e.g. LlamaConfig. In that case we try to see if there is match in `model_type` before raising a warning\n+            for k, v in config_dict.items():\n+                if isinstance(v, dict) and v.get(\"model_type\") == cls.model_type:\n+                    config_dict = v\n+\n+            # raise warning only if we still can't see a match in `model_type`\n+            if config_dict[\"model_type\"] != cls.model_type:\n+                logger.warning(\n+                    f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n+                    f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n+                )\n \n         return cls.from_dict(config_dict, **kwargs)\n "
        },
        {
            "sha": "0df59d1db8e05b7dd5bb74638b82fd222ffd1e27",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -1608,15 +1608,14 @@ def _autoset_attn_implementation(\n         # Below we check if a config is composite and manually prepare a dict of attn impl if not already passed as a dict.\n         # Later each sub-module will dispatch with its own attn impl, by calling `XXXModel._from_config(config.text_config)`\n         # If any of sub-modules doesn't support requested attn, an error will be raised. See https://github.com/huggingface/transformers/pull/32238\n-        for key in config:\n-            if isinstance(getattr(config, key), PretrainedConfig):\n-                sub_config = getattr(config, key)\n-                curr_attn_implementation = (\n-                    requested_attn_implementation\n-                    if not isinstance(requested_attn_implementation, dict)\n-                    else requested_attn_implementation.get(key, None)\n-                )\n-                sub_config._attn_implementation_internal = curr_attn_implementation\n+        for key in config.sub_configs.keys():\n+            sub_config = getattr(config, key)\n+            curr_attn_implementation = (\n+                requested_attn_implementation\n+                if not isinstance(requested_attn_implementation, dict)\n+                else requested_attn_implementation.get(key, None)\n+            )\n+            sub_config._attn_implementation_internal = curr_attn_implementation\n \n         if use_flash_attention_2:\n             logger.warning_once("
        },
        {
            "sha": "a22ab1dc40f8d0d147e5f98f122a64e53bf18fcc",
            "filename": "src/transformers/models/align/configuration_align.py",
            "status": "modified",
            "additions": 4,
            "deletions": 38,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,8 +14,7 @@\n # limitations under the License.\n \"\"\"ALIGN model configuration\"\"\"\n \n-import os\n-from typing import TYPE_CHECKING, List, Union\n+from typing import TYPE_CHECKING, List\n \n \n if TYPE_CHECKING:\n@@ -95,6 +94,7 @@ class AlignTextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"align_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -133,24 +133,6 @@ def __init__(\n         self.use_cache = use_cache\n         self.pad_token_id = pad_token_id\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from AlignConfig\n-        if config_dict.get(\"model_type\") == \"align\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class AlignVisionConfig(PretrainedConfig):\n     r\"\"\"\n@@ -223,6 +205,7 @@ class AlignVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"align_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -272,24 +255,6 @@ def __init__(\n         self.drop_connect_rate = drop_connect_rate\n         self.num_hidden_layers = sum(num_block_repeats) * 4\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from AlignConfig\n-        if config_dict.get(\"model_type\") == \"align\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class AlignConfig(PretrainedConfig):\n     r\"\"\"\n@@ -340,6 +305,7 @@ class AlignConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"align\"\n+    sub_configs = {\"text_config\": AlignTextConfig, \"vision_config\": AlignVisionConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "3c8e91bd473533c37f37a6dda58f550a0f9cfeda",
            "filename": "src/transformers/models/altclip/configuration_altclip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 21,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,9 +14,6 @@\n # limitations under the License.\n \"\"\"AltCLIP model configuration\"\"\"\n \n-import os\n-from typing import Union\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n \n@@ -199,6 +196,7 @@ class AltCLIPVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"altclip_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -233,24 +231,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.hidden_act = hidden_act\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from AltCLIPConfig\n-        if config_dict.get(\"model_type\") == \"altclip\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class AltCLIPConfig(PretrainedConfig):\n     r\"\"\"\n@@ -298,6 +278,7 @@ class AltCLIPConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"altclip\"\n+    sub_configs = {\"text_config\": AltCLIPTextConfig, \"vision_config\": AltCLIPVisionConfig}\n \n     def __init__(\n         self, text_config=None, vision_config=None, projection_dim=768, logit_scale_init_value=2.6592, **kwargs"
        },
        {
            "sha": "a498d1dd19371d3e88d4b815d2a13c39ff8c1390",
            "filename": "src/transformers/models/bark/configuration_bark.py",
            "status": "modified",
            "additions": 11,
            "deletions": 36,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,12 +14,11 @@\n # limitations under the License.\n \"\"\"BARK model configuration\"\"\"\n \n-import os\n-from typing import Dict, Optional, Union\n+from typing import Dict\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import add_start_docstrings, logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -64,7 +63,6 @@\n \n \n class BarkSubModelConfig(PretrainedConfig):\n-    model_type = \"bark_module\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n     attribute_map = {\n@@ -101,38 +99,6 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n-    @classmethod\n-    def from_pretrained(\n-        cls,\n-        pretrained_model_name_or_path: Union[str, os.PathLike],\n-        cache_dir: Optional[Union[str, os.PathLike]] = None,\n-        force_download: bool = False,\n-        local_files_only: bool = False,\n-        token: Optional[Union[str, bool]] = None,\n-        revision: str = \"main\",\n-        **kwargs,\n-    ) -> \"PretrainedConfig\":\n-        kwargs[\"cache_dir\"] = cache_dir\n-        kwargs[\"force_download\"] = force_download\n-        kwargs[\"local_files_only\"] = local_files_only\n-        kwargs[\"revision\"] = revision\n-\n-        cls._set_token_in_kwargs(kwargs, token)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the config dict if we are loading from Bark\n-        if config_dict.get(\"model_type\") == \"bark\":\n-            config_dict = config_dict[f\"{cls.model_type}_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n @add_start_docstrings(\n     BARK_SUBMODELCONFIG_START_DOCSTRING.format(config=\"BarkSemanticConfig\", model=\"BarkSemanticModel\"),\n@@ -154,6 +120,7 @@ def from_pretrained(\n )\n class BarkSemanticConfig(BarkSubModelConfig):\n     model_type = \"semantic\"\n+    base_config_key = \"semantic_config\"\n \n \n @add_start_docstrings(\n@@ -176,6 +143,7 @@ class BarkSemanticConfig(BarkSubModelConfig):\n )\n class BarkCoarseConfig(BarkSubModelConfig):\n     model_type = \"coarse_acoustics\"\n+    base_config_key = \"coarse_acoustics_config\"\n \n \n @add_start_docstrings(\n@@ -203,6 +171,7 @@ class BarkCoarseConfig(BarkSubModelConfig):\n )\n class BarkFineConfig(BarkSubModelConfig):\n     model_type = \"fine_acoustics\"\n+    base_config_key = \"fine_acoustics_config\"\n \n     def __init__(self, tie_word_embeddings=True, n_codes_total=8, n_codes_given=1, **kwargs):\n         self.n_codes_total = n_codes_total\n@@ -265,6 +234,12 @@ class BarkConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"bark\"\n+    sub_configs = {\n+        \"semantic_config\": BarkSemanticConfig,\n+        \"coarse_acoustics_config\": BarkCoarseConfig,\n+        \"fine_acoustics_config\": BarkFineConfig,\n+        \"codec_config\": AutoConfig,\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "18db71eb14890bc520f48d8820931cced57abd8d",
            "filename": "src/transformers/models/blip/configuration_blip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 39,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,9 +14,6 @@\n # limitations under the License.\n \"\"\"Blip model configuration\"\"\"\n \n-import os\n-from typing import Union\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n \n@@ -96,6 +93,7 @@ class BlipTextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"blip_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -146,24 +144,6 @@ def __init__(\n         self.use_cache = use_cache\n         self.label_smoothing = label_smoothing\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from BlipConfig\n-        if config_dict.get(\"model_type\") == \"blip\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class BlipVisionConfig(PretrainedConfig):\n     r\"\"\"\n@@ -215,6 +195,7 @@ class BlipVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"blip_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -245,24 +226,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.hidden_act = hidden_act\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from BlipConfig\n-        if config_dict.get(\"model_type\") == \"blip\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class BlipConfig(PretrainedConfig):\n     r\"\"\"\n@@ -316,6 +279,7 @@ class BlipConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"blip\"\n+    sub_configs = {\"text_config\": BlipTextConfig, \"vision_config\": BlipVisionConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "d690d22338a687ae139c3480c7e443f6fc9ed7b5",
            "filename": "src/transformers/models/blip_2/configuration_blip_2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 39,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,13 +14,12 @@\n # limitations under the License.\n \"\"\"BLIP-2 model configuration\"\"\"\n \n-import os\n-from typing import Optional, Union\n+from typing import Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -76,6 +75,7 @@ class Blip2VisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"blip_2_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -106,24 +106,6 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.qkv_bias = qkv_bias\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from Blip2Config\n-        if config_dict.get(\"model_type\") == \"blip-2\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class Blip2QFormerConfig(PretrainedConfig):\n     r\"\"\"\n@@ -190,6 +172,7 @@ class Blip2QFormerConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"blip_2_qformer\"\n+    base_config_key = \"qformer_config\"\n \n     def __init__(\n         self,\n@@ -229,24 +212,6 @@ def __init__(\n         self.encoder_hidden_size = encoder_hidden_size\n         self.use_qformer_text_input = use_qformer_text_input\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the qformer config dict if we are loading from Blip2Config\n-        if config_dict.get(\"model_type\") == \"blip-2\":\n-            config_dict = config_dict[\"qformer_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class Blip2Config(PretrainedConfig):\n     r\"\"\"\n@@ -306,6 +271,7 @@ class Blip2Config(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"blip-2\"\n+    sub_configs = {\"text_config\": AutoConfig, \"qformer_config\": Blip2QFormerConfig, \"vision_config\": Blip2VisionConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "de49283493b63f134d9557b1f642f81e0f76ee21",
            "filename": "src/transformers/models/bridgetower/configuration_bridgetower.py",
            "status": "modified",
            "additions": 3,
            "deletions": 33,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,9 +14,6 @@\n # limitations under the License.\n \"\"\"BridgeTower model configuration\"\"\"\n \n-import os\n-from typing import Union\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n \n@@ -68,6 +65,7 @@ class BridgeTowerVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"bridgetower_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -95,21 +93,6 @@ def __init__(\n         self.share_layernorm = share_layernorm\n         self.remove_last_layer = remove_last_layer\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        if config_dict.get(\"model_type\") == \"bridgetower\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class BridgeTowerTextConfig(PretrainedConfig):\n     r\"\"\"\n@@ -175,6 +158,7 @@ class BridgeTowerTextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"bridgetower_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -217,21 +201,6 @@ def __init__(\n         self.bos_token_id = bos_token_id\n         self.eos_token_id = eos_token_id\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        if config_dict.get(\"model_type\") == \"bridgetower\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class BridgeTowerConfig(PretrainedConfig):\n     r\"\"\"\n@@ -288,6 +257,7 @@ class BridgeTowerConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"bridgetower\"\n+    sub_configs = {\"text_config\": BridgeTowerTextConfig, \"vision_config\": BridgeTowerVisionConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "9842127e7bb48f4f4eb0ded883f46cb825a1dc77",
            "filename": "src/transformers/models/chameleon/configuration_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -62,6 +62,7 @@ class ChameleonVQVAEConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"chameleon_vqgan\"\n+    base_config_key = \"vq_config\"\n \n     def __init__(\n         self,\n@@ -187,6 +188,7 @@ class ChameleonConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"chameleon\"\n+    sub_configs = {\"vq_config\": ChameleonVQVAEConfig}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n     def __init__("
        },
        {
            "sha": "d50d6c842b313c84efb124a8baae981d9c7bfbed",
            "filename": "src/transformers/models/chinese_clip/configuration_chinese_clip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 38,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,9 +14,8 @@\n # limitations under the License.\n \"\"\"Chinese-CLIP model configuration\"\"\"\n \n-import os\n from collections import OrderedDict\n-from typing import TYPE_CHECKING, Any, Mapping, Optional, Union\n+from typing import TYPE_CHECKING, Any, Mapping, Optional\n \n \n if TYPE_CHECKING:\n@@ -102,6 +101,7 @@ class ChineseCLIPTextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"chinese_clip_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -141,24 +141,6 @@ def __init__(\n         self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from ChineseCLIPConfig\n-        if config_dict.get(\"model_type\") == \"chinese_clip\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class ChineseCLIPVisionConfig(PretrainedConfig):\n     r\"\"\"\n@@ -215,6 +197,7 @@ class ChineseCLIPVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"chinese_clip_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -249,24 +232,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.hidden_act = hidden_act\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from ChineseCLIPConfig\n-        if config_dict.get(\"model_type\") == \"chinese_clip\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class ChineseCLIPConfig(PretrainedConfig):\n     r\"\"\"\n@@ -316,6 +281,7 @@ class ChineseCLIPConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"chinese_clip\"\n+    sub_configs = {\"text_config\": ChineseCLIPTextConfig, \"vision_config\": ChineseCLIPVisionConfig}\n \n     def __init__(\n         self, text_config=None, vision_config=None, projection_dim=512, logit_scale_init_value=2.6592, **kwargs"
        },
        {
            "sha": "b2added7f0e073de54cc44dfad92431d32b7fb27",
            "filename": "src/transformers/models/clap/configuration_clap.py",
            "status": "modified",
            "additions": 3,
            "deletions": 39,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,9 +14,6 @@\n # limitations under the License.\n \"\"\"CLAP model configuration\"\"\"\n \n-import os\n-from typing import Union\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n \n@@ -94,6 +91,7 @@ class ClapTextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"clap_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -137,24 +135,6 @@ def __init__(\n         self.projection_hidden_act = projection_hidden_act\n         self.projection_dim = projection_dim\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from ClapConfig\n-        if config_dict.get(\"model_type\") == \"clap\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class ClapAudioConfig(PretrainedConfig):\n     r\"\"\"\n@@ -245,6 +225,7 @@ class ClapAudioConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"clap_audio_model\"\n+    base_config_key = \"audio_config\"\n \n     def __init__(\n         self,\n@@ -307,24 +288,6 @@ def __init__(\n         self.initializer_factor = initializer_factor\n         self.projection_hidden_act = projection_hidden_act\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the audio config dict if we are loading from ClapConfig\n-        if config_dict.get(\"model_type\") == \"clap\":\n-            config_dict = config_dict[\"audio_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class ClapConfig(PretrainedConfig):\n     r\"\"\"\n@@ -377,6 +340,7 @@ class ClapConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"clap\"\n+    sub_configs = {\"text_config\": ClapTextConfig, \"audio_config\": ClapAudioConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "2e1f2deede00c91da0fd86c845fb250b5ef602dd",
            "filename": "src/transformers/models/clip/configuration_clip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 38,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,9 +14,8 @@\n # limitations under the License.\n \"\"\"CLIP model configuration\"\"\"\n \n-import os\n from collections import OrderedDict\n-from typing import TYPE_CHECKING, Any, Mapping, Optional, Union\n+from typing import TYPE_CHECKING, Any, Mapping, Optional\n \n \n if TYPE_CHECKING:\n@@ -93,6 +92,7 @@ class CLIPTextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"clip_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -130,24 +130,6 @@ def __init__(\n         self.initializer_factor = initializer_factor\n         self.attention_dropout = attention_dropout\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from CLIPConfig\n-        if config_dict.get(\"model_type\") == \"clip\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class CLIPVisionConfig(PretrainedConfig):\n     r\"\"\"\n@@ -205,6 +187,7 @@ class CLIPVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"clip_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -239,24 +222,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.hidden_act = hidden_act\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from CLIPConfig\n-        if config_dict.get(\"model_type\") == \"clip\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class CLIPConfig(PretrainedConfig):\n     r\"\"\"\n@@ -305,6 +270,7 @@ class CLIPConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"clip\"\n+    sub_configs = {\"text_config\": CLIPTextConfig, \"vision_config\": CLIPVisionConfig}\n \n     def __init__(\n         self, text_config=None, vision_config=None, projection_dim=512, logit_scale_init_value=2.6592, **kwargs"
        },
        {
            "sha": "5474840f357a3430d5840a181f527df3373cf744",
            "filename": "src/transformers/models/clipseg/configuration_clipseg.py",
            "status": "modified",
            "additions": 3,
            "deletions": 39,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,9 +14,6 @@\n # limitations under the License.\n \"\"\"CLIPSeg model configuration\"\"\"\n \n-import os\n-from typing import Union\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n \n@@ -84,6 +81,7 @@ class CLIPSegTextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"clipseg_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -117,24 +115,6 @@ def __init__(\n         self.initializer_factor = initializer_factor\n         self.attention_dropout = attention_dropout\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from CLIPSegConfig\n-        if config_dict.get(\"model_type\") == \"clipseg\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class CLIPSegVisionConfig(PretrainedConfig):\n     r\"\"\"\n@@ -190,6 +170,7 @@ class CLIPSegVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"clipseg_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -222,24 +203,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.hidden_act = hidden_act\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from CLIPSegConfig\n-        if config_dict.get(\"model_type\") == \"clipseg\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class CLIPSegConfig(PretrainedConfig):\n     r\"\"\"\n@@ -306,6 +269,7 @@ class CLIPSegConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"clipseg\"\n+    sub_configs = {\"text_config\": CLIPSegTextConfig, \"vision_config\": CLIPSegVisionConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "8fd0e150801a6689fc3d87fbb463184d216243be",
            "filename": "src/transformers/models/clvp/configuration_clvp.py",
            "status": "modified",
            "additions": 8,
            "deletions": 20,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -91,6 +91,7 @@ class ClvpEncoderConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"clvp_encoder\"\n+    base_config_key = [\"text_config\", \"speech_config\"]\n \n     def __init__(\n         self,\n@@ -141,7 +142,7 @@ def from_pretrained(\n \n         # make sure to have the config_type be either \"text_config\" or \"speech_config\"\n         # this is to make sure that we can load only text or speech configs from the nested ClvpConfig.\n-        if config_type not in [\"text_config\", \"speech_config\"]:\n+        if config_type not in cls.base_config_key:\n             raise ValueError(\n                 f\"We can only load either 'text_config' or 'speech_config' but you are trying to load\" f\"{config_type}\"\n             )\n@@ -253,6 +254,7 @@ class ClvpDecoderConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"clvp_decoder\"\n+    base_config_key = \"decoder_config\"\n \n     def __init__(\n         self,\n@@ -314,24 +316,6 @@ def __init__(\n \n         super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the speech config dict if we are loading from ClvpConfig\n-        if config_dict.get(\"model_type\") == \"clvp\":\n-            config_dict = config_dict[\"decoder_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class ClvpConfig(PretrainedConfig):\n     r\"\"\"\n@@ -386,7 +370,11 @@ class ClvpConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"clvp\"\n-    is_composition = True\n+    sub_configs = {\n+        \"text_config\": ClvpEncoderConfig,\n+        \"speech_config\": ClvpEncoderConfig,\n+        \"decoder_config\": ClvpDecoderConfig,\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "302b5e6a55821d5f6ff04a3313f00af566ff0bb1",
            "filename": "src/transformers/models/dbrx/configuration_dbrx.py",
            "status": "modified",
            "additions": 7,
            "deletions": 36,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -41,6 +41,8 @@ class DbrxAttentionConfig(PretrainedConfig):\n         rope_theta (`float`, *optional*, defaults to 10000.0): The base frequency for rope.\n     \"\"\"\n \n+    base_config_key = \"attn_config\"\n+\n     def __init__(\n         self,\n         attn_pdrop: float = 0.0,\n@@ -55,29 +57,12 @@ def __init__(\n         self.kv_n_heads = kv_n_heads\n         self.rope_theta = rope_theta\n \n-        for k in [\"model_type\"]:\n+        for k in [\"model_type\", \"attn_implementation\", \"transformers_version\", \"_commit_hash\"]:\n             if k in kwargs:\n                 kwargs.pop(k)\n         if len(kwargs) != 0:\n             raise ValueError(f\"Found unknown {kwargs=}\")\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: str, **kwargs: Any) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        if config_dict.get(\"model_type\") == \"dbrx\":\n-            config_dict = config_dict[\"attn_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                + f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class DbrxFFNConfig(PretrainedConfig):\n     \"\"\"Configuration class for Dbrx FFN.\n@@ -100,6 +85,8 @@ class DbrxFFNConfig(PretrainedConfig):\n         moe_normalize_expert_weights (`float`, *optional*, defaults to 1.0): The normalization factor for the expert weights.\n     \"\"\"\n \n+    base_config_key = \"ffn_config\"\n+\n     def __init__(\n         self,\n         ffn_act_fn: dict = None,\n@@ -122,29 +109,12 @@ def __init__(\n         self.moe_loss_weight = moe_loss_weight\n         self.moe_normalize_expert_weights = moe_normalize_expert_weights\n \n-        for k in [\"model_type\"]:\n+        for k in [\"model_type\", \"attn_implementation\", \"transformers_version\", \"_commit_hash\"]:\n             if k in kwargs:\n                 kwargs.pop(k)\n         if len(kwargs) != 0:\n             raise ValueError(f\"Found unknown {kwargs=}\")\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: str, **kwargs: Any) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        if config_dict.get(\"model_type\") == \"dbrx\":\n-            config_dict = config_dict[\"ffn_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                + f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class DbrxConfig(PretrainedConfig):\n     r\"\"\"\n@@ -202,6 +172,7 @@ class DbrxConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"dbrx\"\n+    sub_configs = {\"attn_config\": DbrxAttentionConfig, \"ffn_config\": DbrxFFNConfig}\n     attribute_map = {\n         \"num_attention_heads\": \"n_heads\",\n         \"hidden_size\": \"d_model\","
        },
        {
            "sha": "5190ed51ffd3503531aa815ee88b025fd986e19d",
            "filename": "src/transformers/models/encoder_decoder/configuration_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -17,6 +17,7 @@\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n+from ..auto import AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -70,6 +71,7 @@ class EncoderDecoderConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"encoder-decoder\"\n+    sub_configs = {\"encoder\": AutoConfig, \"decoder\": AutoConfig}\n     is_composition = True\n \n     def __init__(self, **kwargs):\n@@ -84,8 +86,6 @@ def __init__(self, **kwargs):\n         decoder_config = kwargs.pop(\"decoder\")\n         decoder_model_type = decoder_config.pop(\"model_type\")\n \n-        from ..auto.configuration_auto import AutoConfig\n-\n         self.encoder = AutoConfig.for_model(encoder_model_type, **encoder_config)\n         self.decoder = AutoConfig.for_model(decoder_model_type, **decoder_config)\n         self.is_encoder_decoder = True"
        },
        {
            "sha": "59a1b029751646b15c088432d631ca5137f80d98",
            "filename": "src/transformers/models/fastspeech2_conformer/configuration_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -164,6 +164,7 @@ class FastSpeech2ConformerConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"fastspeech2_conformer\"\n+    base_config_key = \"model_config\"\n     attribute_map = {\"num_hidden_layers\": \"encoder_layers\", \"num_attention_heads\": \"encoder_num_attention_heads\"}\n \n     def __init__(\n@@ -377,6 +378,7 @@ class FastSpeech2ConformerHifiGanConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"hifigan\"\n+    base_config_key = \"vocoder_config\"\n \n     def __init__(\n         self,\n@@ -453,7 +455,7 @@ class FastSpeech2ConformerWithHifiGanConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"fastspeech2_conformer_with_hifigan\"\n-    is_composition = True\n+    sub_configs = {\"model_config\": FastSpeech2ConformerConfig, \"vocoder_config\": FastSpeech2ConformerHifiGanConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "47cdb488a2eb5d66cba153093fa46b2253a9e4fa",
            "filename": "src/transformers/models/flava/configuration_flava.py",
            "status": "modified",
            "additions": 11,
            "deletions": 74,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,8 +14,7 @@\n # limitations under the License.\n \"\"\"FLAVA model configurations\"\"\"\n \n-import os\n-from typing import Any, Dict, Union\n+from typing import Any, Dict\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n@@ -86,6 +85,7 @@ class FlavaImageConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"flava_image_model\"\n+    base_config_key = \"image_config\"\n \n     def __init__(\n         self,\n@@ -124,24 +124,6 @@ def __init__(\n         self.mask_token = mask_token\n         self.vocab_size = vocab_size\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the image config dict if we are loading from FlavaConfig\n-        if config_dict.get(\"model_type\") == \"flava\":\n-            config_dict = config_dict[\"image_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class FlavaTextConfig(PretrainedConfig):\n     r\"\"\"\n@@ -216,6 +198,7 @@ class FlavaTextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"flava_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -254,24 +237,6 @@ def __init__(\n         self.qkv_bias = qkv_bias\n         self.pad_token_id = pad_token_id\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from FlavaConfig\n-        if config_dict.get(\"model_type\") == \"flava\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class FlavaMultimodalConfig(PretrainedConfig):\n     r\"\"\"\n@@ -327,6 +292,7 @@ class FlavaMultimodalConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"flava_multimodal_model\"\n+    base_config_key = \"multimodal_config\"\n \n     def __init__(\n         self,\n@@ -357,27 +323,10 @@ def __init__(\n         self.qkv_bias = qkv_bias\n         self.use_cls_token = use_cls_token\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the multimodal config dict if we are loading from FlavaConfig\n-        if config_dict.get(\"model_type\") == \"flava\":\n-            config_dict = config_dict[\"multimodal_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class FlavaImageCodebookConfig(PretrainedConfig):\n     model_type = \"flava_image_codebook\"\n+    base_config_key = \"image_codebook_config\"\n \n     r\"\"\"\n     [`FlavaImageCodebookConfig`] is the configuration class to store the configuration of a [`FlavaImageCodebook`]. It\n@@ -442,24 +391,6 @@ def __init__(\n         self.freeze = freeze\n         self.initializer_range = initializer_range\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the image codebook config dict if we are loading from FlavaConfig\n-        if config_dict.get(\"model_type\") == \"flava\":\n-            config_dict = config_dict[\"image_codebook_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class FlavaConfig(PretrainedConfig):\n     r\"\"\"\n@@ -532,6 +463,12 @@ class FlavaConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"flava\"\n+    sub_configs = {\n+        \"text_config\": FlavaTextConfig,\n+        \"image_config\": FlavaImageConfig,\n+        \"multimodal_config\": FlavaMultimodalConfig,\n+        \"image_codebook_config\": FlavaImageCodebookConfig,\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "1be3e7067bdfcf987a9ebc495eb0a44d11524838",
            "filename": "src/transformers/models/git/configuration_git.py",
            "status": "modified",
            "additions": 2,
            "deletions": 20,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -13,8 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import os\n-from typing import Union\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n@@ -72,6 +70,7 @@ class GitVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"git_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -102,24 +101,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.hidden_act = hidden_act\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from GITConfig\n-        if config_dict.get(\"model_type\") == \"git\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class GitConfig(PretrainedConfig):\n     r\"\"\"\n@@ -186,6 +167,7 @@ class GitConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"git\"\n+    sub_configs = {\"vision_config\": GitVisionConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "e85e4fc918437156756802a0d79a10afddbd471a",
            "filename": "src/transformers/models/groupvit/configuration_groupvit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 38,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,9 +14,8 @@\n # limitations under the License.\n \"\"\"GroupViT model configuration\"\"\"\n \n-import os\n from collections import OrderedDict\n-from typing import TYPE_CHECKING, Any, Mapping, Optional, Union\n+from typing import TYPE_CHECKING, Any, Mapping, Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...onnx import OnnxConfig\n@@ -86,6 +85,7 @@ class GroupViTTextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"groupvit_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -121,24 +121,6 @@ def __init__(\n         self.initializer_factor = initializer_factor\n         self.attention_dropout = attention_dropout\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from GroupViTConfig\n-        if config_dict.get(\"model_type\") == \"groupvit\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class GroupViTVisionConfig(PretrainedConfig):\n     r\"\"\"\n@@ -197,6 +179,7 @@ class GroupViTVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"groupvit_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -246,24 +229,6 @@ def __init__(\n         self.assign_eps = assign_eps\n         self.assign_mlp_ratio = assign_mlp_ratio\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from GroupViTConfig\n-        if config_dict.get(\"model_type\") == \"groupvit\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class GroupViTConfig(PretrainedConfig):\n     r\"\"\"\n@@ -292,6 +257,7 @@ class GroupViTConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"groupvit\"\n+    sub_configs = {\"text_config\": GroupViTTextConfig, \"vision_config\": GroupViTVisionConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "e34a5764400196e12aab33f40fa88ea1d30e9f86",
            "filename": "src/transformers/models/idefics/configuration_idefics.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fidefics%2Fconfiguration_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fidefics%2Fconfiguration_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fconfiguration_idefics.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -38,7 +38,7 @@ class IdeficsVisionConfig(PretrainedConfig):\n     documentation from [`PretrainedConfig`] for more information.\n \n     Args:\n-        hidden_size (`int`, *optional*, defaults to 768):\n+        embed_dim (`int`, *optional*, defaults to 768):\n             Dimensionality of the encoder layers and the pooler layer. (elsewhere referred to as `hidden_size`)\n         image_size (`int`, *optional*, defaults to 224):\n             The size (resolution) of each image.\n@@ -50,12 +50,12 @@ class IdeficsVisionConfig(PretrainedConfig):\n             Number of hidden layers in the Transformer encoder.\n         num_attention_heads (`int`, *optional*, defaults to 16):\n             Number of attention heads for each attention layer in the Transformer encoder.\n-        image_num_channels (`int`, *optional*, defaults to `3`):\n+        num_channels (`int`, *optional*, defaults to 3):\n             Number of image channels.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n             The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n             `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n-        layer_norm_eps (`float`, *optional*, defaults to 1e-5):\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n             The epsilon used by the layer normalization layers.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n@@ -64,11 +64,9 @@ class IdeficsVisionConfig(PretrainedConfig):\n         initializer_factor (`float`, *optional*, defaults to 1.0):\n             A factor for initializing all weight matrices (should be kept to 1.0, used internally for initialization\n             testing).\n-        initializer_range (`float`, *optional*, defaults to 0.02):\n-            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n     \"\"\"\n \n-    model_type = \"idefics\"\n+    model_type = \"idefics_vision\"\n     attribute_map = {\n         \"hidden_size\": \"embed_dim\",\n     }\n@@ -119,7 +117,7 @@ class IdeficsPerceiverConfig(PretrainedConfig):\n     Args:\n         use_resampler (`bool`, *optional*, defaults to `False`):\n             Whether or not to use the resampler\n-        resampler_n_latents (`int`, *optional*, defaults to ):\n+        resampler_n_latents (`int`, *optional*, defaults to 64):\n             Number of latent embeddings to resample (\"compress\") the input sequence to (usually < 128).\n         resampler_depth (`int`, *optional*, defaults to 6):\n             Depth of the Perceiver Resampler (Transformer w/ cross attention). Should be shallow (< 3).\n@@ -131,7 +129,7 @@ class IdeficsPerceiverConfig(PretrainedConfig):\n             Whether or not to use qk layer norms in perceiver\n     \"\"\"\n \n-    model_type = \"idefics\"\n+    model_type = \"idefics_perciever\"\n \n     def __init__(\n         self,\n@@ -235,7 +233,7 @@ class IdeficsConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"idefics\"\n-    is_composition = False\n+    sub_configs = {\"perceiver_config\": IdeficsPerceiverConfig, \"vision_config\": IdeficsVisionConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "408d374c77f7eba33123c2c5c7cbe7faa0771f00",
            "filename": "src/transformers/models/idefics2/configuration_idefics2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 25,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -13,12 +13,9 @@\n # limitations under the License.\n \"\"\"Idefics2 model configuration\"\"\"\n \n-import os\n-from typing import Union\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -76,7 +73,8 @@ class Idefics2VisionConfig(PretrainedConfig):\n     >>> configuration = model.config\n     ```\"\"\"\n \n-    model_type = \"idefics2\"\n+    model_type = \"idefics2_vision\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -107,24 +105,6 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.initializer_range = initializer_range\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from Idefics2Config\n-        if config_dict.get(\"model_type\") == \"idefics2\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class Idefics2PerceiverConfig(PretrainedConfig):\n     r\"\"\"\n@@ -152,7 +132,7 @@ class Idefics2PerceiverConfig(PretrainedConfig):\n             The dropout ratio for the attention probabilities.\n     \"\"\"\n \n-    model_type = \"idefics2\"\n+    model_type = \"idefics2_perceiver\"\n \n     def __init__(\n         self,\n@@ -220,7 +200,11 @@ class Idefics2Config(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"idefics2\"\n-    is_composition = True\n+    sub_configs = {\n+        \"text_config\": AutoConfig,\n+        \"perceiver_config\": Idefics2PerceiverConfig,\n+        \"vision_config\": Idefics2VisionConfig,\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "4b10d8d2d03a81d632528d74401effa2b5f9cde9",
            "filename": "src/transformers/models/idefics3/configuration_idefics3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 27,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconfiguration_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconfiguration_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconfiguration_idefics3.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -13,12 +13,9 @@\n # limitations under the License.\n \"\"\"Idefics3 model configuration\"\"\"\n \n-import os\n-from typing import Union\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -57,8 +54,7 @@ class Idefics3VisionConfig(PretrainedConfig):\n             The epsilon used by the layer normalization layers.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        intializer_range (`float`, *optional*, defaults to 0.02):\n-            The standard deviation for initializing all weight matrices in the model.\n+        initializer_range (`<fill_type>`, *optional*, defaults to 0.02): <fill_docstring>\n \n     Example:\n \n@@ -76,7 +72,8 @@ class Idefics3VisionConfig(PretrainedConfig):\n     >>> configuration = model.config\n     ```\"\"\"\n \n-    model_type = \"idefics3\"\n+    model_type = \"idefics3_vision\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -107,24 +104,6 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.initializer_range = initializer_range\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from Idefics3Config\n-        if config_dict.get(\"model_type\") == \"idefics3\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class Idefics3Config(PretrainedConfig):\n     r\"\"\"\n@@ -165,7 +144,7 @@ class Idefics3Config(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"idefics3\"\n-    is_composition = True\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": Idefics3VisionConfig}\n \n     def __init__(\n         self,\n@@ -204,4 +183,4 @@ def __init__(\n         self.text_config = text_config\n         self.scale_factor = scale_factor\n \n-        super().__init__(**kwargs, tie_word_embeddings=tie_word_embeddings)\n+        super().__init__(**kwargs, pad_token_id=pad_token_id, tie_word_embeddings=tie_word_embeddings)"
        },
        {
            "sha": "6124dba3a08efe9b4670d668023cc6ab479e6683",
            "filename": "src/transformers/models/instructblip/configuration_instructblip.py",
            "status": "modified",
            "additions": 8,
            "deletions": 40,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,13 +14,10 @@\n # limitations under the License.\n \"\"\"InstructBLIP model configuration\"\"\"\n \n-import os\n-from typing import Union\n-\n from ...configuration_utils import PretrainedConfig\n from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -78,6 +75,7 @@ class InstructBlipVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"instructblip_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -108,24 +106,6 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.qkv_bias = qkv_bias\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from InstructBlipConfig\n-        if config_dict.get(\"model_type\") == \"instructblip\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class InstructBlipQFormerConfig(PretrainedConfig):\n     r\"\"\"\n@@ -192,6 +172,7 @@ class InstructBlipQFormerConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"instructblip_qformer\"\n+    base_config_key = \"qformer_config\"\n \n     def __init__(\n         self,\n@@ -229,24 +210,6 @@ def __init__(\n         self.cross_attention_frequency = cross_attention_frequency\n         self.encoder_hidden_size = encoder_hidden_size\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the qformer config dict if we are loading from InstructBlipConfig\n-        if config_dict.get(\"model_type\") == \"instructblip\":\n-            config_dict = config_dict[\"qformer_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class InstructBlipConfig(PretrainedConfig):\n     r\"\"\"\n@@ -305,6 +268,11 @@ class InstructBlipConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"instructblip\"\n+    sub_configs = {\n+        \"text_config\": AutoConfig,\n+        \"qformer_config\": InstructBlipQFormerConfig,\n+        \"vision_config\": InstructBlipVisionConfig,\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "14687a96e54f371e205a3df6c4a0b126573e54f1",
            "filename": "src/transformers/models/instructblipvideo/configuration_instructblipvideo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 39,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -19,13 +19,11 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import os\n-from typing import Union\n \n from ...configuration_utils import PretrainedConfig\n from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -83,6 +81,7 @@ class InstructBlipVideoVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"instructblipvideo_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -113,24 +112,6 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.qkv_bias = qkv_bias\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from InstructBlipVideoConfig\n-        if config_dict.get(\"model_type\") == \"instructblipvideo\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class InstructBlipVideoQFormerConfig(PretrainedConfig):\n     r\"\"\"\n@@ -197,6 +178,7 @@ class InstructBlipVideoQFormerConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"instructblipvideo_qformer\"\n+    base_config_key = \"qformer_config\"\n \n     def __init__(\n         self,\n@@ -234,24 +216,6 @@ def __init__(\n         self.cross_attention_frequency = cross_attention_frequency\n         self.encoder_hidden_size = encoder_hidden_size\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the qformer config dict if we are loading from InstructBlipVideoConfig\n-        if config_dict.get(\"model_type\") == \"instructblipvideo\":\n-            config_dict = config_dict[\"qformer_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class InstructBlipVideoConfig(PretrainedConfig):\n     r\"\"\"\n@@ -310,6 +274,11 @@ class InstructBlipVideoConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"instructblipvideo\"\n+    sub_configs = {\n+        \"text_config\": AutoConfig,\n+        \"qformer_config\": InstructBlipVideoQFormerConfig,\n+        \"vision_config\": InstructBlipVideoVisionConfig,\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "b0dc8a215740f14d6b3ab810bf90531aff732485",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -32,7 +32,7 @@\n from ...configuration_utils import PretrainedConfig\n from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -103,6 +103,11 @@ class InstructBlipVideoConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"instructblipvideo\"\n+    sub_configs = {\n+        \"text_config\": AutoConfig,\n+        \"qformer_config\": InstructBlipVideoQFormerConfig,\n+        \"vision_config\": InstructBlipVideoVisionConfig,\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "921ec336c0be8023bdde0fae0f15f96e0a518e2c",
            "filename": "src/transformers/models/kosmos2/configuration_kosmos2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 43,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fconfiguration_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fconfiguration_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fconfiguration_kosmos2.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,9 +14,6 @@\n # limitations under the License.\n \"\"\"KOSMOS-2 model configuration\"\"\"\n \n-import os\n-from typing import Union\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n \n@@ -61,17 +58,24 @@ class Kosmos2TextConfig(PretrainedConfig):\n         layerdrop (`float`, *optional*, defaults to 0.0):\n             The LayerDrop probability for the decoder. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)\n             for more details.\n-        layer_norm_eps (`float`, *optional*, defaults to 1e-5):\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n             The epsilon used by the layer normalization layers.\n         init_std (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         scale_embedding (`bool`, *optional*, defaults to `True`):\n             Scale embeddings by diving by sqrt(embed_dim).\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models).\n+        pad_token_id (`int`, *optional*, defaults to 1):\n+            Token id used for padding.\n+        bos_token_id (`int`, *optional*, defaults to 0):\n+            Token id used for beginning of string.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            Token id used for end of string.\n     ```\"\"\"\n \n     model_type = \"kosmos_2_text_model\"\n+    base_config_key = \"text_config\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n     attribute_map = {\n         \"num_attention_heads\": \"attention_heads\",\n@@ -124,24 +128,6 @@ def __init__(\n         self.scale_embedding = scale_embedding\n         self.use_cache = use_cache\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from Kosmos2Config\n-        if config_dict.get(\"model_type\") == \"kosmos-2\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class Kosmos2VisionConfig(PretrainedConfig):\n     r\"\"\"\n@@ -171,18 +157,19 @@ class Kosmos2VisionConfig(PretrainedConfig):\n         hidden_act (`str` or `function`, *optional*, defaults to `\"quick_gelu\"`):\n             The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n             `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n-        layer_norm_eps (`float`, *optional*, defaults to 1e-5):\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n             The epsilon used by the layer normalization layers.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n-        initializer_factor (`float`, *optional*, defaults to 1):\n+        initializer_factor (`float`, *optional*, defaults to 1.0):\n             A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n             testing).\n     ```\"\"\"\n \n     model_type = \"kosmos_2_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -215,24 +202,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.hidden_act = hidden_act\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from Kosmos2Config\n-        if config_dict.get(\"model_type\") == \"kosmos-2\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class Kosmos2Config(PretrainedConfig):\n     r\"\"\"\n@@ -267,7 +236,7 @@ class Kosmos2Config(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"kosmos-2\"\n-    is_composition = True\n+    sub_configs = {\"text_config\": Kosmos2TextConfig, \"vision_config\": Kosmos2VisionConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "05034f5cfcf6f8075559a9e82fc05bbed3deeca4",
            "filename": "src/transformers/models/llava/configuration_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -15,7 +15,7 @@\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -73,7 +73,7 @@ class LlavaConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"llava\"\n-    is_composition = True\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "54616edbf96dce14abb739af1b6bd9310e7cf2f3",
            "filename": "src/transformers/models/llava_next/configuration_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -15,7 +15,7 @@\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -78,7 +78,7 @@ class LlavaNextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"llava_next\"\n-    is_composition = False\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "2fe889da60336b2c5762cb6a20a8b11e3420bbeb",
            "filename": "src/transformers/models/llava_next_video/configuration_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -21,7 +21,7 @@\n \n \n from ...configuration_utils import PretrainedConfig\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n class LlavaNextVideoConfig(PretrainedConfig):\n@@ -86,7 +86,7 @@ class LlavaNextVideoConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"llava_next_video\"\n-    is_composition = True\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "002b450c2af232f7c3c6152b35321baeb4670f80",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -31,7 +31,7 @@\n from ...utils import (\n     logging,\n )\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -99,7 +99,7 @@ class LlavaNextVideoConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"llava_next_video\"\n-    is_composition = True\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "46b65b35b1a5cbce7187d74b2ff612a9151c608e",
            "filename": "src/transformers/models/llava_onevision/configuration_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -18,7 +18,7 @@\n from ...utils import (\n     logging,\n )\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -81,7 +81,7 @@ class LlavaOnevisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"llava_onevision\"\n-    is_composition = False\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "635ca503205f5f00c2457cf4ea31e756820f7c04",
            "filename": "src/transformers/models/mllama/configuration_mllama.py",
            "status": "modified",
            "additions": 7,
            "deletions": 40,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -13,8 +13,7 @@\n # limitations under the License.\n \"\"\"Mllama model configuration\"\"\"\n \n-import os\n-from typing import Dict, List, Optional, Union\n+from typing import Dict, List, Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n@@ -59,7 +58,7 @@ class MllamaVisionConfig(PretrainedConfig):\n             The size (resolution) of each image *tile*.\n         patch_size (`int`, *optional*, defaults to 14):\n             The size (resolution) of each patch.\n-        norm_eps (`float`, *optional*, defaults to 1e-5):\n+        norm_eps (`float`, *optional*, defaults to 1e-05):\n             The epsilon used by the layer normalization layers.\n         max_num_tiles (`int`, *optional*, defaults to 4):\n             Maximum number of tiles for image splitting.\n@@ -88,6 +87,7 @@ class MllamaVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"mllama_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -137,23 +137,6 @@ def __init__(\n     def max_aspect_ratio_id(self) -> int:\n         return len(self.supported_aspect_ratios)\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        if config_dict.get(\"model_type\") == \"mllama\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class MllamaTextConfig(PretrainedConfig):\n     r\"\"\"\n@@ -178,12 +161,12 @@ class MllamaTextConfig(PretrainedConfig):\n             Number of hidden layers in the Transformer encoder.\n         num_attention_heads (`int`, *optional*, defaults to 32):\n             Number of attention heads for each attention layer in the Transformer encoder.\n-        num_key_value_heads (`int`, *optional*):\n+        num_key_value_heads (`int`, *optional*, defaults to 8):\n             This is the number of key_value heads that should be used to implement Grouped Query Attention. If not\n             specified, will default to `num_attention_heads`.\n         intermediate_size (`int`, *optional*, defaults to 14336):\n             Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n-        rope_theta (`float`, *optional*, defaults to 500000.0):\n+        rope_theta (`float`, *optional*, defaults to `500000.0`):\n             The base period of the RoPE embeddings.\n         rope_scaling (`Dict`, *optional*):\n             Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n@@ -259,6 +242,7 @@ class MllamaTextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"mllama_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -311,23 +295,6 @@ def __init__(\n             **kwargs,\n         )\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        if config_dict.get(\"model_type\") == \"mllama\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class MllamaConfig(PretrainedConfig):\n     r\"\"\"\n@@ -370,7 +337,7 @@ class MllamaConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"mllama\"\n-    is_composition = True\n+    sub_configs = {\"text_config\": MllamaTextConfig, \"vision_config\": MllamaVisionConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "1b31141f020db575ea0d6667968dc419d828b139",
            "filename": "src/transformers/models/moshi/configuration_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -235,8 +235,8 @@ class MoshiConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"moshi\"\n-    is_composition = True\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    sub_configs = {\"audio_encoder_config\": AutoConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "8ee3f8c0c07428652415dc65183336a063bb4ed9",
            "filename": "src/transformers/models/mpt/configuration_mpt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 21,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fmpt%2Fconfiguration_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fmpt%2Fconfiguration_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fconfiguration_mpt.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -41,22 +41,22 @@ class MptAttentionConfig(PretrainedConfig):\n     Args:\n         attn_type (`str`, *optional*, defaults to `\"multihead_attention\"`):\n             type of attention to use. Options: `\"multihead_attention\"`, `\"multiquery_attention\"`.\n-        attn_pdrop (`float`, *optional*, defaults to 0.0):\n+        attn_pdrop (`float`, *optional*, defaults to `0.0`):\n             The dropout probability for the attention layers.\n         attn_impl (`str`, *optional*, defaults to `\"torch\"`):\n             The attention implementation to use. One of `\"torch\"`, `\"flash\"`, or `\"triton\"`.\n         clip_qkv (`float`, *optional*):\n             If not `None`, clip the queries, keys, and values in the attention layer to this value.\n-        softmax_scale (`float`, *optional*, defaults to `None`):\n+        softmax_scale (`float`, *optional*):\n             If not `None`, scale the softmax in the attention layer by this value. If `None`, will default to\n             `1/sqrt(hidden_size)`.\n-        prefix_lm (`bool`, *optional*, defaults to `False`)):\n+        prefix_lm (`bool`, *optional*, defaults to `False`):\n             Whether the model should operate as a Prefix LM. This requires passing an extra `prefix_mask` argument\n             which indicates which tokens belong to the prefix. Tokens in the prefix can attend to one another\n             bi-directionally. Tokens outside the prefix use causal attention.\n         qk_ln (`bool`, *optional*, defaults to `False`):\n             Whether to apply layer normalization to the queries and keys in the attention layer.\n-        attn_uses_sequence_id (`bool`, *optional*, defaults to `False`)):\n+        attn_uses_sequence_id (`bool`, *optional*, defaults to `False`):\n             Whether to restrict attention to tokens that have the same token_type_ids. When the model is in `train`\n             mode, this requires passing an extra *token_type_ids* argument which indicates which sub-sequence each\n             token belongs to. Defaults to `False` meaning any provided *token_type_ids* will be ignored.\n@@ -66,6 +66,8 @@ class MptAttentionConfig(PretrainedConfig):\n             The maximum value of the alibi bias.\n     \"\"\"\n \n+    base_config_key = \"attn_config\"\n+\n     def __init__(\n         self,\n         attn_type=\"multihead_attention\",\n@@ -97,23 +99,6 @@ def __init__(\n                 f\"`attn_type` has to be either `multihead_attention` or `multiquery_attention`. Received: {attn_type}\"\n             )\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        if config_dict.get(\"model_type\") == \"mpt\":\n-            config_dict = config_dict[\"attn_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class MptConfig(PretrainedConfig):\n     \"\"\"\n@@ -188,6 +173,7 @@ class MptConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"mpt\"\n+    sub_configs = {\"attn_config\": MptAttentionConfig}\n     attribute_map = {\n         \"num_attention_heads\": \"n_heads\",\n         \"hidden_size\": \"d_model\","
        },
        {
            "sha": "00c03072198092ebd2a859f7784420e3a31fed0a",
            "filename": "src/transformers/models/musicgen/configuration_musicgen.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -76,6 +76,7 @@ class MusicgenDecoderConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"musicgen_decoder\"\n+    base_config_key = \"decoder_config\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n     def __init__(\n@@ -189,6 +190,11 @@ class MusicgenConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"musicgen\"\n+    sub_configs = {\n+        \"text_encoder\": AutoConfig,\n+        \"audio_encoder\": AutoConfig,\n+        \"decoder\": MusicgenDecoderConfig,\n+    }\n     is_composition = True\n \n     def __init__(self, **kwargs):"
        },
        {
            "sha": "e65ad50021c3abf343b4da54396bad58aded0385",
            "filename": "src/transformers/models/musicgen_melody/configuration_musicgen_melody.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -78,6 +78,7 @@ class MusicgenMelodyDecoderConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"musicgen_melody_decoder\"\n+    base_config_key = \"decoder_config\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n     def __init__(\n@@ -195,6 +196,11 @@ class MusicgenMelodyConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"musicgen_melody\"\n+    sub_configs = {\n+        \"text_encoder\": AutoConfig,\n+        \"audio_encoder\": AutoConfig,\n+        \"decoder\": MusicgenMelodyDecoderConfig,\n+    }\n     is_composition = True\n \n     def __init__("
        },
        {
            "sha": "f9085eaf9c154625784b8997718308425cddefd4",
            "filename": "src/transformers/models/owlv2/configuration_owlv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 52,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fowlv2%2Fconfiguration_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fowlv2%2Fconfiguration_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fconfiguration_owlv2.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,8 +14,7 @@\n # limitations under the License.\n \"\"\"OWLv2 model configuration\"\"\"\n \n-import os\n-from typing import TYPE_CHECKING, Dict, Union\n+from typing import TYPE_CHECKING, Dict\n \n \n if TYPE_CHECKING:\n@@ -90,6 +89,7 @@ class Owlv2TextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"owlv2_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -123,24 +123,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.initializer_factor = initializer_factor\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from Owlv2Config\n-        if config_dict.get(\"model_type\") == \"owlv2\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n # Copied from transformers.models.owlvit.configuration_owlvit.OwlViTVisionConfig with OwlViT->Owlv2, owlvit-base-patch32->owlv2-base-patch16, owlvit->owlv2, OWL-ViT->OWLv2, 32->16\n class Owlv2VisionConfig(PretrainedConfig):\n@@ -197,6 +179,7 @@ class Owlv2VisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"owlv2_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -229,24 +212,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.initializer_factor = initializer_factor\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from Owlv2Config\n-        if config_dict.get(\"model_type\") == \"owlv2\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n # Copied from transformers.models.owlvit.configuration_owlvit.OwlViTConfig with OwlViT->Owlv2, owlvit-base-patch32->owlv2-base-patch16, owlvit->owlv2, OWL-ViT->OWLv2\n class Owlv2Config(PretrainedConfig):\n@@ -276,6 +241,7 @@ class Owlv2Config(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"owlv2\"\n+    sub_configs = {\"text_config\": Owlv2TextConfig, \"vision_config\": Owlv2VisionConfig}\n \n     def __init__(\n         self,\n@@ -304,20 +270,6 @@ def __init__(\n         self.return_dict = return_dict\n         self.initializer_factor = 1.0\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n     @classmethod\n     def from_text_vision_configs(cls, text_config: Dict, vision_config: Dict, **kwargs):\n         r\"\"\""
        },
        {
            "sha": "8be707ce99a1c6e89563592d790ea1f34f426086",
            "filename": "src/transformers/models/owlvit/configuration_owlvit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 52,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconfiguration_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconfiguration_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconfiguration_owlvit.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,9 +14,8 @@\n # limitations under the License.\n \"\"\"OWL-ViT model configuration\"\"\"\n \n-import os\n from collections import OrderedDict\n-from typing import TYPE_CHECKING, Any, Dict, Mapping, Optional, Union\n+from typing import TYPE_CHECKING, Any, Dict, Mapping, Optional\n \n \n if TYPE_CHECKING:\n@@ -92,6 +91,7 @@ class OwlViTTextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"owlvit_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -125,24 +125,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.initializer_factor = initializer_factor\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from OwlViTConfig\n-        if config_dict.get(\"model_type\") == \"owlvit\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class OwlViTVisionConfig(PretrainedConfig):\n     r\"\"\"\n@@ -198,6 +180,7 @@ class OwlViTVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"owlvit_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -230,24 +213,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.initializer_factor = initializer_factor\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from OwlViTConfig\n-        if config_dict.get(\"model_type\") == \"owlvit\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class OwlViTConfig(PretrainedConfig):\n     r\"\"\"\n@@ -276,6 +241,7 @@ class OwlViTConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"owlvit\"\n+    sub_configs = {\"text_config\": OwlViTTextConfig, \"vision_config\": OwlViTVisionConfig}\n \n     def __init__(\n         self,\n@@ -304,20 +270,6 @@ def __init__(\n         self.return_dict = return_dict\n         self.initializer_factor = 1.0\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n     @classmethod\n     def from_text_vision_configs(cls, text_config: Dict, vision_config: Dict, **kwargs):\n         r\"\"\""
        },
        {
            "sha": "de60c501292b30a8ab7272da77209996e0916b92",
            "filename": "src/transformers/models/paligemma/configuration_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -17,7 +17,7 @@\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -73,7 +73,7 @@ class PaliGemmaConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"paligemma\"\n-    is_composition = False\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "925aa60a8dc6dee8ead8370f40e4ee8f36347b60",
            "filename": "src/transformers/models/qwen2_audio/configuration_qwen2_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fconfiguration_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fconfiguration_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fconfiguration_qwen2_audio.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -15,7 +15,7 @@\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -157,7 +157,7 @@ class Qwen2AudioConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"qwen2_audio\"\n-    is_composition = False\n+    sub_configs = {\"text_config\": AutoConfig, \"audio_config\": AutoConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "55042327de4ec379b1476621d45088191ab3d49a",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 20,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,9 +14,6 @@\n # limitations under the License.\n \"\"\"Qwen2VL model configuration\"\"\"\n \n-import os\n-from typing import Union\n-\n from ...configuration_utils import PretrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n@@ -27,6 +24,7 @@\n \n class Qwen2VLVisionConfig(PretrainedConfig):\n     model_type = \"qwen2_vl\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -55,23 +53,6 @@ def __init__(\n         self.spatial_merge_size = spatial_merge_size\n         self.temporal_patch_size = temporal_patch_size\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        if config_dict.get(\"model_type\") == \"qwen2_vl\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class Qwen2VLConfig(PretrainedConfig):\n     r\"\"\"\n@@ -180,6 +161,7 @@ class Qwen2VLConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"qwen2_vl\"\n+    sub_configs = {\"vision_config\": Qwen2VLVisionConfig}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n     def __init__("
        },
        {
            "sha": "cc8fae93cdb25b5136762b555b260e710db88fc1",
            "filename": "src/transformers/models/siglip/configuration_siglip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 39,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconfiguration_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconfiguration_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconfiguration_siglip.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,9 +14,6 @@\n # limitations under the License.\n \"\"\"Siglip model configuration\"\"\"\n \n-import os\n-from typing import Union\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n \n@@ -79,6 +76,7 @@ class SiglipTextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"siglip_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -110,24 +108,6 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.attention_dropout = attention_dropout\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from SiglipConfig\n-        if config_dict.get(\"model_type\") == \"siglip\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class SiglipVisionConfig(PretrainedConfig):\n     r\"\"\"\n@@ -178,6 +158,7 @@ class SiglipVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"siglip_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -206,24 +187,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.hidden_act = hidden_act\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from SiglipConfig\n-        if config_dict.get(\"model_type\") == \"siglip\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class SiglipConfig(PretrainedConfig):\n     r\"\"\"\n@@ -268,6 +231,7 @@ class SiglipConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"siglip\"\n+    sub_configs = {\"text_config\": SiglipTextConfig, \"vision_config\": SiglipVisionConfig}\n \n     def __init__(self, text_config=None, vision_config=None, **kwargs):\n         super().__init__(**kwargs)"
        },
        {
            "sha": "d7e0211610b6572f111781fd7dd2c5ad8392bd1a",
            "filename": "src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconfiguration_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconfiguration_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconfiguration_speech_encoder_decoder.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -71,6 +71,7 @@ class SpeechEncoderDecoderConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"speech-encoder-decoder\"\n+    sub_configs = {\"encoder\": AutoConfig, \"decoder\": AutoConfig}\n     is_composition = True\n \n     def __init__(self, **kwargs):"
        },
        {
            "sha": "87d96ca24ffdb48dca353642c07da2f8c8b9ea33",
            "filename": "src/transformers/models/video_llava/configuration_video_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -15,7 +15,7 @@\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -78,7 +78,7 @@ class VideoLlavaConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"video_llava\"\n-    is_composition = False\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "f26c2b2f50fb6a3850a3c06cc675f29cfd9a81e3",
            "filename": "src/transformers/models/vipllava/configuration_vipllava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -15,7 +15,7 @@\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -72,7 +72,7 @@ class VipLlavaConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"vipllava\"\n-    is_composition = False\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "59678f2573ff0e1614d8cd34d26b1d5e88eaf570",
            "filename": "src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fconfiguration_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fconfiguration_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fconfiguration_vision_encoder_decoder.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -78,6 +78,7 @@ class VisionEncoderDecoderConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"vision-encoder-decoder\"\n+    sub_configs = {\"encoder\": AutoConfig, \"decoder\": AutoConfig}\n     is_composition = True\n \n     def __init__(self, **kwargs):"
        },
        {
            "sha": "0d79720e1aa8d227db6b6aded073cec19b566642",
            "filename": "src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fconfiguration_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fconfiguration_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fconfiguration_vision_text_dual_encoder.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -75,6 +75,7 @@ class VisionTextDualEncoderConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"vision-text-dual-encoder\"\n+    sub_configs = {\"vision_config\": AutoConfig, \"text_config\": AutoConfig}\n     is_composition = True\n \n     def __init__(self, projection_dim=512, logit_scale_init_value=2.6592, **kwargs):"
        },
        {
            "sha": "3d3b92d2c8c02e1d2274a018aa83d7de317782ac",
            "filename": "src/transformers/models/x_clip/configuration_x_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 39,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -14,9 +14,6 @@\n # limitations under the License.\n \"\"\"X-CLIP model configuration\"\"\"\n \n-import os\n-from typing import Union\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n \n@@ -79,6 +76,7 @@ class XCLIPTextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"xclip_text_model\"\n+    base_config_key = \"text_config\"\n \n     def __init__(\n         self,\n@@ -112,24 +110,6 @@ def __init__(\n         self.initializer_factor = initializer_factor\n         self.attention_dropout = attention_dropout\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the text config dict if we are loading from XCLIPConfig\n-        if config_dict.get(\"model_type\") == \"xclip\":\n-            config_dict = config_dict[\"text_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class XCLIPVisionConfig(PretrainedConfig):\n     r\"\"\"\n@@ -195,6 +175,7 @@ class XCLIPVisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"xclip_vision_model\"\n+    base_config_key = \"vision_config\"\n \n     def __init__(\n         self,\n@@ -239,24 +220,6 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.drop_path_rate = drop_path_rate\n \n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n-        cls._set_token_in_kwargs(kwargs)\n-\n-        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n-\n-        # get the vision config dict if we are loading from XCLIPConfig\n-        if config_dict.get(\"model_type\") == \"xclip\":\n-            config_dict = config_dict[\"vision_config\"]\n-\n-        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n-            logger.warning(\n-                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n-                f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n-            )\n-\n-        return cls.from_dict(config_dict, **kwargs)\n-\n \n class XCLIPConfig(PretrainedConfig):\n     r\"\"\"\n@@ -295,6 +258,7 @@ class XCLIPConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"xclip\"\n+    sub_configs = {\"text_config\": XCLIPTextConfig, \"vision_config\": XCLIPVisionConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "3c7e679686f6178655bc68e480226565f5d9226f",
            "filename": "tests/models/align/test_modeling_align.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_modeling_align.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -457,11 +457,20 @@ class AlignModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     def setUp(self):\n         self.model_tester = AlignModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=AlignConfig,\n+            has_text_modality=False,\n+            common_properties=[\"projection_dim\", \"temperature_init_value\"],\n+        )\n \n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n     @unittest.skip(reason=\"Start to fail after using torch `cu118`.\")\n     def test_multi_gpu_data_parallel_forward(self):\n         super().test_multi_gpu_data_parallel_forward()"
        },
        {
            "sha": "658e2e38d9adb51c17b3887b6b488c8337d99968",
            "filename": "tests/models/altclip/test_modeling_altclip.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -452,11 +452,20 @@ def is_pipeline_test_to_skip(\n \n     def setUp(self):\n         self.model_tester = AltCLIPModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=AltCLIPConfig,\n+            has_text_modality=False,\n+            common_properties=[\"projection_dim\", \"logit_scale_init_value\"],\n+        )\n \n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n     @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n     def test_hidden_states_output(self):\n         pass"
        },
        {
            "sha": "7e1dbbe6bb9cb02357a0a0beb0b15fe0070da6b5",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -449,11 +449,18 @@ class BlipModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     def setUp(self):\n         self.model_tester = BlipModelTester(self)\n+        common_properties = [\"logit_scale_init_value\", \"image_text_hidden_size\", \"projection_dim\", \"label_smoothing\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=BlipConfig, has_text_modality=False, common_properties=common_properties\n+        )\n \n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n     @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n     def test_hidden_states_output(self):\n         pass"
        },
        {
            "sha": "0943661b96666c94078cb763bb46ed2c7320ce97",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -482,6 +482,13 @@ class Blip2ForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationT\n \n     def setUp(self):\n         self.model_tester = Blip2ForConditionalGenerationDecoderOnlyModelTester(self)\n+        common_properties = [\"image_token_index\", \"num_query_tokens\", \"image_text_hidden_size\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=Blip2Config, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     def test_for_conditional_generation(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()"
        },
        {
            "sha": "60b77d0efa4b7ba3906e3126de9e34ebf7270d6b",
            "filename": "tests/models/clap/test_modeling_clap.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -515,11 +515,18 @@ class ClapModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     def setUp(self):\n         self.model_tester = ClapModelTester(self)\n+        common_properties = [\"logit_scale_init_value\", \"projection_hidden_act\", \"projection_dim\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=ClapConfig, has_text_modality=False, common_properties=common_properties\n+        )\n \n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n     @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n     def test_hidden_states_output(self):\n         pass"
        },
        {
            "sha": "fa5de84e06205f482ea6e4ec7fdfef652ef350cf",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -745,11 +745,18 @@ class CLIPModelTest(CLIPModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n \n     def setUp(self):\n         self.model_tester = CLIPModelTester(self)\n+        common_properties = [\"projection_dim\", \"logit_scale_init_value\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=CLIPConfig, has_text_modality=False, common_properties=common_properties\n+        )\n \n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n     @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n     def test_hidden_states_output(self):\n         pass"
        },
        {
            "sha": "b2b047bb502ccebc19e1687681b61d902536f2f7",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -472,11 +472,18 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n \n     def setUp(self):\n         self.model_tester = CLIPSegModelTester(self)\n+        common_properties = [\"projection_dim\", \"logit_scale_init_value\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=CLIPSegConfig, has_text_modality=False, common_properties=common_properties\n+        )\n \n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n     def test_model_for_image_segmentation(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_for_image_segmentation(*config_and_inputs)"
        },
        {
            "sha": "a212b4781d0a78e779e0119f8baee13dd8294463",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -414,7 +414,13 @@ class ClvpModelForConditionalGenerationTest(ModelTesterMixin, unittest.TestCase)\n \n     def setUp(self):\n         self.model_tester = ClvpModelForConditionalGenerationTester(self)\n-        self.clvp_config_tester = ConfigTester(self, config_class=ClvpConfig, hidden_size=32)\n+        common_properties = [\"projection_dim\", \"logit_scale_init_value\"]\n+        self.clvp_config_tester = ConfigTester(\n+            self, config_class=ClvpConfig, has_text_modality=False, common_properties=common_properties, hidden_size=32\n+        )\n+\n+    def test_config(self):\n+        self.clvp_config_tester.run_common_tests()\n \n     def tearDown(self):\n         super().tearDown()"
        },
        {
            "sha": "1c35fd705ccd876d7aabe286ff66816d22dc8a40",
            "filename": "tests/models/flava/test_modeling_flava.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -931,11 +931,18 @@ class FlavaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     def setUp(self):\n         self.model_tester = self.class_for_tester(self)\n+        common_properties = [\"projection_dim\", \"logit_scale_init_value\", \"init_codebook\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=FlavaConfig, has_text_modality=False, common_properties=common_properties\n+        )\n \n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_common()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n     @unittest.skip(reason=\"tested in individual model tests\")\n     def test_hidden_states_output(self):\n         pass"
        },
        {
            "sha": "88b55ec56d8233004af0705a78410c992df6053f",
            "filename": "tests/models/groupvit/test_modeling_groupvit.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -559,11 +559,18 @@ class GroupViTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n \n     def setUp(self):\n         self.model_tester = GroupViTModelTester(self)\n+        common_properties = [\"projection_dim\", \"projection_intermediate_dim\", \"logit_scale_init_value\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=GroupViTConfig, has_text_modality=False, common_properties=common_properties\n+        )\n \n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n     @unittest.skip(reason=\"hidden_states are tested in individual model tests\")\n     def test_hidden_states_output(self):\n         pass"
        },
        {
            "sha": "ae8c91f29d4d46390ae8e59a849c2c035a5830e2",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -185,7 +185,12 @@ class Idefics2ModelTest(ModelTesterMixin, unittest.TestCase):\n \n     def setUp(self):\n         self.model_tester = Idefics2VisionText2TextModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Idefics2Config, has_text_modality=False)\n+        self.config_tester = ConfigTester(\n+            self, config_class=Idefics2Config, has_text_modality=False, common_properties=[\"image_token_id\"]\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     @unittest.skip(reason=\"input_embeds cannot be passed in without input_ids\")\n     def test_inputs_embeds():"
        },
        {
            "sha": "5bfd4c3f3c0e834e9de8ed0b0b168249aee1adc9",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -168,7 +168,12 @@ class Idefics3ModelTest(ModelTesterMixin, unittest.TestCase):\n \n     def setUp(self):\n         self.model_tester = Idefics3VisionText2TextModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Idefics3Config, has_text_modality=False)\n+        self.config_tester = ConfigTester(\n+            self, config_class=Idefics3Config, has_text_modality=False, common_properties=[\"image_token_id\"]\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     @unittest.skip(reason=\"input_embeds cannot be passed in without input_ids\")\n     def test_inputs_embeds():"
        },
        {
            "sha": "e77577dad7877b07974f4eca235513849c289f6f",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -486,6 +486,15 @@ class InstructBlipForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, Gene\n \n     def setUp(self):\n         self.model_tester = InstructBlipForConditionalGenerationDecoderOnlyModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=InstructBlipConfig,\n+            has_text_modality=False,\n+            common_properties=[\"num_query_tokens\", \"image_token_index\"],\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     def test_for_conditional_generation(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()"
        },
        {
            "sha": "3be5f89325cf386be9c7e5ffa07e06049d463cf3",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -510,11 +510,18 @@ class InstructBlipVideoForConditionalGenerationDecoderOnlyTest(\n \n     def setUp(self):\n         self.model_tester = InstructBlipVideoForConditionalGenerationDecoderOnlyModelTester(self)\n+        common_properties = [\"num_query_tokens\", \"video_token_index\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=InstructBlipVideoConfig, has_text_modality=False, common_properties=common_properties\n+        )\n \n     def test_for_conditional_generation(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_conditional_generation(*config_and_inputs)\n \n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n     @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n     def test_hidden_states_output(self):\n         pass"
        },
        {
            "sha": "7ede47a348d55b9f90069e41cf7ba9e4b9d57a10",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -304,7 +304,12 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n \n     def setUp(self):\n         self.model_tester = Kosmos2ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Kosmos2Config, hidden_size=37)\n+        self.config_tester = ConfigTester(\n+            self, config_class=Kosmos2Config, has_text_modality=False, common_properties=[\"latent_query_num\"]\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     # overwrite from common to skip `image_to_text_projection.latent_query`\n     def test_initialization(self):"
        },
        {
            "sha": "1359e16a3d7b03e2c4aa346d38b1d0914ebf9dfe",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -194,7 +194,13 @@ class LlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterM\n \n     def setUp(self):\n         self.model_tester = LlavaVisionText2TextModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=LlavaConfig, has_text_modality=False)\n+        common_properties = [\"image_token_index\", \"vision_feature_layer\", \"image_seq_length\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=LlavaConfig, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n     def test_inputs_embeds(self):"
        },
        {
            "sha": "7ce57dcba3eb438a1cadc017781a0ddc5cbfd973",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -223,7 +223,13 @@ class LlavaNextForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n \n     def setUp(self):\n         self.model_tester = LlavaNextVisionText2TextModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=LlavaNextConfig, has_text_modality=False)\n+        common_properties = [\"image_token_index\", \"vision_feature_layer\", \"image_seq_length\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=LlavaNextConfig, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "3ebb5752bd8ded1a3eaa7b1bc4533648f181e24b",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -240,7 +240,13 @@ class LlavaNextVideoForConditionalGenerationModelTest(ModelTesterMixin, Generati\n \n     def setUp(self):\n         self.model_tester = LlavaNextVideoVisionText2TextModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=LlavaNextVideoConfig, has_text_modality=False)\n+        common_properties = [\"image_token_index\", \"video_token_index\", \"vision_feature_layer\", \"image_seq_length\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=LlavaNextVideoConfig, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "a217eee2c706716c632b5b7563a59b3154781bc1",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -226,7 +226,13 @@ class LlavaOnevisionForConditionalGenerationModelTest(ModelTesterMixin, Generati\n \n     def setUp(self):\n         self.model_tester = LlavaOnevisionVisionText2TextModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=LlavaOnevisionConfig, has_text_modality=False)\n+        common_properties = [\"image_token_index\", \"video_token_index\", \"vision_feature_layer\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=LlavaOnevisionConfig, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "9ed5d678225b18f3177a90f805d3b257464619ff",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -272,7 +272,12 @@ class MllamaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTester\n \n     def setUp(self):\n         self.model_tester = MllamaVisionText2TextModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=MllamaConfig, has_text_modality=False)\n+        self.config_tester = ConfigTester(\n+            self, config_class=MllamaConfig, has_text_modality=False, common_properties=[\"image_token_index\"]\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n     def test_inputs_embeds(self):"
        },
        {
            "sha": "df763aed48c749381a387ab44fea1dacd1716131",
            "filename": "tests/models/owlv2/test_modeling_owlv2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -447,6 +447,13 @@ class Owlv2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     def setUp(self):\n         self.model_tester = Owlv2ModelTester(self)\n+        common_properties = [\"projection_dim\", \"logit_scale_init_value\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=Owlv2Config, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()"
        },
        {
            "sha": "e0599a50fb98b407fad1e13aff04b11fa6e9f083",
            "filename": "tests/models/owlvit/test_modeling_owlvit.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -442,6 +442,13 @@ class OwlViTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     def setUp(self):\n         self.model_tester = OwlViTModelTester(self)\n+        common_properties = [\"projection_dim\", \"logit_scale_init_value\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=OwlViTConfig, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()"
        },
        {
            "sha": "afd45dc0167a5b34edd2849baba71ec454cc7288",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -232,6 +232,9 @@ def setUp(self):\n         self.model_tester = Qwen2VLVisionText2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Qwen2VLConfig, has_text_modality=False)\n \n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "61ac78f102994a93c610b11190c110bebe23ea46",
            "filename": "tests/models/siglip/test_modeling_siglip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -667,9 +667,12 @@ class SiglipModelTest(SiglipModelTesterMixin, PipelineTesterMixin, unittest.Test\n     test_disk_offload_bin = False\n     _is_composite = True\n \n-    # Copied from tests.models.clip.test_modeling_clip.CLIPModelTest.setUp with CLIP->Siglip\n     def setUp(self):\n         self.model_tester = SiglipModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=SiglipConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     # Copied from tests.models.clip.test_modeling_clip.CLIPModelTest.test_model\n     def test_model(self):"
        },
        {
            "sha": "4da6dc19addd4abff45758e099cb2fd15404a118",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -217,7 +217,13 @@ class VideoLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTe\n \n     def setUp(self):\n         self.model_tester = VideoLlavaVisionText2TextModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=VideoLlavaConfig, has_text_modality=False)\n+        common_properties = [\"image_token_index\", \"video_token_index\", \"vision_feature_layer\", \"image_seq_length\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=VideoLlavaConfig, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     @unittest.skip(\n         reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\""
        },
        {
            "sha": "25670d782a987e831828ddf24868d42de775926b",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -179,7 +179,13 @@ class VipLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTest\n \n     def setUp(self):\n         self.model_tester = VipLlavaVisionText2TextModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=VipLlavaConfig, has_text_modality=False)\n+        common_properties = [\"image_token_index\", \"vision_feature_layers\", \"image_seq_length\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=VipLlavaConfig, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n     def test_inputs_embeds(self):"
        },
        {
            "sha": "04dd2d9d29687a98d9aefe3094da75fd1b2c39aa",
            "filename": "tests/models/x_clip/test_modeling_x_clip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -547,6 +547,13 @@ class XCLIPModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     def setUp(self):\n         self.model_tester = XCLIPModelTester(self)\n+        common_properties = [\"projection_dim\", \"prompt_layers\", \"prompt_num_attention_heads\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=XCLIPConfig, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n \n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()"
        },
        {
            "sha": "4dbbdedbbc2eb7102e8c0da4d73edb4c2f88f349",
            "filename": "tests/test_configuration_common.py",
            "status": "modified",
            "additions": 49,
            "deletions": 1,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Ftest_configuration_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Ftest_configuration_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_configuration_common.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -17,12 +17,17 @@\n import json\n import os\n import tempfile\n+from pathlib import Path\n \n from transformers import is_torch_available\n+from transformers.utils import direct_transformers_import\n \n from .utils.test_configuration_utils import config_common_kwargs\n \n \n+transformers_module = direct_transformers_import(Path(__file__).parent)\n+\n+\n class ConfigTester:\n     def __init__(self, parent, config_class=None, has_text_modality=True, common_properties=None, **kwargs):\n         self.parent = parent\n@@ -35,9 +40,10 @@ def create_and_test_config_common_properties(self):\n         config = self.config_class(**self.inputs_dict)\n         common_properties = (\n             [\"hidden_size\", \"num_attention_heads\", \"num_hidden_layers\"]\n-            if self.common_properties is None\n+            if self.common_properties is None and not self.config_class.sub_configs\n             else self.common_properties\n         )\n+        common_properties = [] if common_properties is None else common_properties\n \n         # Add common fields for text models\n         if self.has_text_modality:\n@@ -110,6 +116,44 @@ def create_and_test_config_from_and_save_pretrained_subfolder(self):\n \n         self.parent.assertEqual(config_second.to_dict(), config_first.to_dict())\n \n+    def create_and_test_config_from_and_save_pretrained_composite(self):\n+        \"\"\"\n+        Tests that composite or nested cofigs can be loaded and saved correctly. In case the config\n+        has a sub-config, we should be able to call `sub_config.from_pretrained('general_config_file')`\n+        and get a result same as if we loaded the whole config and obtained `config.sub_config` from it.\n+        \"\"\"\n+        config = self.config_class(**self.inputs_dict)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            config.save_pretrained(tmpdirname)\n+            general_config_loaded = self.config_class.from_pretrained(tmpdirname)\n+            general_config_dict = config.to_dict()\n+\n+            # Iterate over all sub_configs if there are any and load them with their own classes\n+            sub_configs = self.config_class.sub_configs\n+            for sub_config_key, sub_class in sub_configs.items():\n+                if sub_class.__name__ == \"AutoConfig\":\n+                    sub_class = sub_class.for_model(**general_config_dict[sub_config_key]).__class__\n+                    sub_config_loaded = sub_class.from_pretrained(tmpdirname)\n+                else:\n+                    sub_config_loaded = sub_class.from_pretrained(tmpdirname)\n+\n+                # Pop `transformers_version`, it never exists when a config is part of a general composite config\n+                # Verify that loading with subconfig class results in same dict as if we loaded with general composite config class\n+                sub_config_loaded_dict = sub_config_loaded.to_dict()\n+                sub_config_loaded_dict.pop(\"transformers_version\", None)\n+                self.parent.assertEqual(sub_config_loaded_dict, general_config_dict[sub_config_key])\n+\n+                # Verify that the loaded config type is same as in the general config\n+                type_from_general_config = type(getattr(general_config_loaded, sub_config_key))\n+                self.parent.assertTrue(isinstance(sub_config_loaded, type_from_general_config))\n+\n+                # Now save only the sub-config and load it back to make sure the whole load-save-load pipeline works\n+                with tempfile.TemporaryDirectory() as tmpdirname2:\n+                    sub_config_loaded.save_pretrained(tmpdirname2)\n+                    sub_config_loaded_2 = sub_class.from_pretrained(tmpdirname2)\n+                    self.parent.assertEqual(sub_config_loaded.to_dict(), sub_config_loaded_2.to_dict())\n+\n     def create_and_test_config_with_num_labels(self):\n         config = self.config_class(**self.inputs_dict, num_labels=5)\n         self.parent.assertEqual(len(config.id2label), 5)\n@@ -128,6 +172,9 @@ def check_config_can_be_init_without_params(self):\n             self.parent.assertIsNotNone(config)\n \n     def check_config_arguments_init(self):\n+        if self.config_class.sub_configs:\n+            return  # TODO: @raushan composite models are not consistent in how they set general params\n+\n         kwargs = copy.deepcopy(config_common_kwargs)\n         config = self.config_class(**kwargs)\n         wrong_values = []\n@@ -153,6 +200,7 @@ def run_common_tests(self):\n         self.create_and_test_config_to_json_file()\n         self.create_and_test_config_from_and_save_pretrained()\n         self.create_and_test_config_from_and_save_pretrained_subfolder()\n+        self.create_and_test_config_from_and_save_pretrained_composite()\n         self.create_and_test_config_with_num_labels()\n         self.check_config_can_be_init_without_params()\n         self.check_config_arguments_init()"
        },
        {
            "sha": "c7a11ff0ac8a242a6e3aadf3dbbef82d7726036d",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/893ad04fad145904ccb71e4e858e4134c32226b6/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=893ad04fad145904ccb71e4e858e4134c32226b6",
            "patch": "@@ -3802,22 +3802,18 @@ def test_attn_implementation_composite_models(self):\n                 self.skipTest(\"Model is not a composite model.\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            sub_configs = {\n-                key: getattr(config, key) for key in config if isinstance(getattr(config, key), PretrainedConfig)\n-            }\n \n             # set eager as it will be the one supported in all models\n             # we just need to test if passing 'attn_implementation' as a dict fails or not\n             attn_implementation_per_subconfig = {}\n-            for key, sub_config in sub_configs.items():\n+            for key in config.sub_configs.keys():\n                 attn_implementation_per_subconfig[key] = \"eager\"\n \n             config._attn_implementation = attn_implementation_per_subconfig\n             model = model_class(config)\n-            for key in model.config:\n-                if isinstance(getattr(model.config, key), PretrainedConfig):\n-                    sub_config = getattr(model.config, key)\n-                    self.assertTrue(sub_config._attn_implementation == \"eager\")\n+            for key in config.sub_configs.keys():\n+                sub_config = getattr(model.config, key)\n+                self.assertTrue(sub_config._attn_implementation == \"eager\")\n \n             for name, submodule in model.named_modules():\n                 class_name = submodule.__class__.__name__\n@@ -3826,7 +3822,7 @@ def test_attn_implementation_composite_models(self):\n                     or \"SdpaSelfAttention\" in class_name\n                     or \"FlashAttention\" in class_name\n                 ):\n-                    raise ValueError(\"The eager model should not have SDPA/FA2 attention layers\")\n+                    raise ValueError(f\"The eager model should not have SDPA/FA2 attention layers but got {class_name}\")\n \n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_non_composite_models(self):"
        }
    ],
    "stats": {
        "total": 1516,
        "additions": 464,
        "deletions": 1052
    }
}