{
    "author": "DeVikingMark",
    "message": "fix typos (#38336)\n\n* Update video_processor.md\n\n* Update deepseek_v3.md",
    "sha": "63964b7c67b22177741908d7f0f516e71a050fe8",
    "files": [
        {
            "sha": "4ff973d2ed29c41fbdd03a2cd14380ad4cdff31f",
            "filename": "docs/source/en/main_classes/video_processor.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63964b7c67b22177741908d7f0f516e71a050fe8/docs%2Fsource%2Fen%2Fmain_classes%2Fvideo_processor.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/63964b7c67b22177741908d7f0f516e71a050fe8/docs%2Fsource%2Fen%2Fmain_classes%2Fvideo_processor.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fvideo_processor.md?ref=63964b7c67b22177741908d7f0f516e71a050fe8",
            "patch": "@@ -21,7 +21,7 @@ A **Video Processor** is a utility responsible for preparing input features for\n \n The video processor extends the functionality of image processors by allowing Vision Large Language Models (VLMs) to handle videos with a distinct set of arguments compared to images. It serves as the bridge between raw video data and the model, ensuring that input features are optimized for the VLM.\n \n-When adding a new VLM or updating an existing one to enable distinct video preprocessing, saving and reloading the processor configuration will store the video related arguments in a dedicated file named `video_preprocessing_config.json`. Don't worry if you haven't upadted your VLM, the processor will try to load video related configurations from a file named `preprocessing_config.json`.\n+When adding a new VLM or updating an existing one to enable distinct video preprocessing, saving and reloading the processor configuration will store the video related arguments in a dedicated file named `video_preprocessing_config.json`. Don't worry if you haven't updated your VLM, the processor will try to load video related configurations from a file named `preprocessing_config.json`.\n \n \n ### Usage Example"
        },
        {
            "sha": "ae2bb42a625e1c83eb2670bde389e699a1132d87",
            "filename": "docs/source/en/model_doc/deepseek_v3.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/63964b7c67b22177741908d7f0f516e71a050fe8/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/63964b7c67b22177741908d7f0f516e71a050fe8/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md?ref=63964b7c67b22177741908d7f0f516e71a050fe8",
            "patch": "@@ -28,8 +28,8 @@ We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 67\n We are super happy to make this code community-powered, and would love to see how you can best optimize the following: \n \n - current implementation uses the \"naive\" attention compution (so not really MLA)\n-- current implementation loops through the experts. This should be replaced. Pointers to use `get_packed_weights` from `intetrations/tensor_parallel`. \n-- current implementation uses the eleuther formula for ROPE, using the orginal one would be more efficient! (should still follow our API)\n+- current implementation loops through the experts. This should be replaced. Pointers to use `get_packed_weights` from `integrations/tensor_parallel`. \n+- current implementation uses the eleuther formula for ROPE, using the original one would be more efficient! (should still follow our API)\n - static cache is not supported (this should be just a generation config issue / config shape issues)\n \n ### Usage tips"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}