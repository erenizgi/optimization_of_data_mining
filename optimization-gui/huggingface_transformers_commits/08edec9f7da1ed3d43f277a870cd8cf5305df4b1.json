{
    "author": "zucchini-nlp",
    "message": "[processors] Unbloating simple processors (#40377)\n\n* modularize processor - step 1\n\n* typos\n\n* why raise error, super call check it also\n\n* tiny update\n\n* fix copies\n\n* fix style and test\n\n* lost an import / fix copies\n\n* fix tests\n\n* oops deleted accidentally",
    "sha": "08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
    "files": [
        {
            "sha": "fbca27b2ff397f808d956f97d3c93369924140ae",
            "filename": "src/transformers/models/align/processing_align.py",
            "status": "modified",
            "additions": 2,
            "deletions": 70,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -16,11 +16,7 @@\n Image/Text processor class for ALIGN\n \"\"\"\n \n-from typing import Optional, Union\n-\n-from ...image_utils import ImageInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin\n \n \n class AlignProcessorKwargs(ProcessingKwargs, total=False):\n@@ -66,74 +62,10 @@ class AlignProcessor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"EfficientNetImageProcessor\"\n     tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n+    valid_processor_kwargs = AlignProcessorKwargs\n \n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n \n-    def __call__(\n-        self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n-        **kwargs: Unpack[AlignProcessorKwargs],\n-    ) -> BatchEncoding:\n-        \"\"\"\n-        Main method to prepare text(s) and image(s) to be fed as input to the model. This method forwards the `text`\n-        arguments to BertTokenizerFast's [`~BertTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` arguments to\n-        EfficientNetImageProcessor's [`~EfficientNetImageProcessor.__call__`] if `images` is not `None`. Please refer\n-        to the docstring of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                    - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                    - `'np'`: Return NumPy `np.ndarray` objects.\n-                    - `'jax'`: Return JAX `jnp.ndarray` objects.\n-        Returns:\n-            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n-\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n-            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n-        \"\"\"\n-        if text is None and images is None:\n-            raise ValueError(\"You must specify either text or images.\")\n-\n-        output_kwargs = self._merge_kwargs(\n-            AlignProcessorKwargs,\n-            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n-            **kwargs,\n-        )\n-        # then, we can pass correct kwargs to each processor\n-        if text is not None:\n-            encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-\n-        if images is not None:\n-            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n-\n-        # BC for explicit return_tensors\n-        if \"return_tensors\" in output_kwargs[\"common_kwargs\"]:\n-            return_tensors = output_kwargs[\"common_kwargs\"].pop(\"return_tensors\", None)\n-\n-        if text is not None and images is not None:\n-            encoding[\"pixel_values\"] = image_features.pixel_values\n-            return encoding\n-        elif text is not None:\n-            return encoding\n-        else:\n-            return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n-\n \n __all__ = [\"AlignProcessor\"]"
        },
        {
            "sha": "24631ecacbd72782ea5592159ddc4aae22094276",
            "filename": "src/transformers/models/altclip/processing_altclip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 82,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -16,18 +16,10 @@\n Image/Text processor class for AltCLIP\n \"\"\"\n \n-from typing import Optional, Union\n-\n-from ...image_utils import ImageInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n+from ...processing_utils import ProcessorMixin\n from ...utils.deprecation import deprecate_kwarg\n \n \n-class AltClipProcessorKwargs(ProcessingKwargs, total=False):\n-    _defaults = {}\n-\n-\n class AltCLIPProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a AltCLIP processor which wraps a CLIP image processor and a XLM-Roberta tokenizer into a single\n@@ -49,80 +41,7 @@ class AltCLIPProcessor(ProcessorMixin):\n \n     @deprecate_kwarg(old_name=\"feature_extractor\", version=\"5.0.0\", new_name=\"image_processor\")\n     def __init__(self, image_processor=None, tokenizer=None):\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n-\n         super().__init__(image_processor, tokenizer)\n \n-    def __call__(\n-        self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n-        **kwargs: Unpack[AltClipProcessorKwargs],\n-    ) -> BatchEncoding:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to XLMRobertaTokenizerFast's [`~XLMRobertaTokenizerFast.__call__`] if `text` is not\n-        `None` to encode the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-\n-            images (`ImageInput`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                    - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                    - `'np'`: Return NumPy `np.ndarray` objects.\n-                    - `'jax'`: Return JAX `jnp.ndarray` objects.\n-        Returns:\n-            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n-\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n-            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n-        \"\"\"\n-\n-        if text is None and images is None:\n-            raise ValueError(\"You must specify either text or images.\")\n-\n-        if text is None and images is None:\n-            raise ValueError(\"You must specify either text or images.\")\n-        output_kwargs = self._merge_kwargs(\n-            AltClipProcessorKwargs,\n-            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n-            **kwargs,\n-        )\n-\n-        if text is not None:\n-            encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-        if images is not None:\n-            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n-\n-        # BC for explicit return_tensors\n-        if \"return_tensors\" in output_kwargs[\"common_kwargs\"]:\n-            return_tensors = output_kwargs[\"common_kwargs\"].pop(\"return_tensors\", None)\n-\n-        if text is not None and images is not None:\n-            encoding[\"pixel_values\"] = image_features.pixel_values\n-            return encoding\n-        elif text is not None:\n-            return encoding\n-        else:\n-            return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n-\n \n __all__ = [\"AltCLIPProcessor\"]"
        },
        {
            "sha": "030c578c49cda3fdd630d869898cc49f94d9430d",
            "filename": "src/transformers/models/bridgetower/processing_bridgetower.py",
            "status": "modified",
            "additions": 2,
            "deletions": 30,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -16,10 +16,7 @@\n Processor class for BridgeTower.\n \"\"\"\n \n-from typing import Union\n-\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin\n \n \n class BridgeTowerProcessorKwargs(ProcessingKwargs, total=False):\n@@ -60,35 +57,10 @@ class BridgeTowerProcessor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"BridgeTowerImageProcessor\"\n     tokenizer_class = (\"RobertaTokenizer\", \"RobertaTokenizerFast\")\n+    valid_processor_kwargs = BridgeTowerProcessorKwargs\n \n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n \n-    def __call__(\n-        self,\n-        images,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n-        **kwargs: Unpack[BridgeTowerProcessorKwargs],\n-    ) -> BatchEncoding:\n-        \"\"\"\n-        This method uses [`BridgeTowerImageProcessor.__call__`] method to prepare image(s) for the model, and\n-        [`RobertaTokenizerFast.__call__`] to prepare text for the model.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n-        output_kwargs = self._merge_kwargs(\n-            BridgeTowerProcessorKwargs,\n-            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n-            **kwargs,\n-        )\n-        encoding = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"])\n-        # add pixel_values + pixel_mask\n-        encoding_image_processor = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n-        encoding.update(encoding_image_processor)\n-\n-        return encoding\n-\n \n __all__ = [\"BridgeTowerProcessor\"]"
        },
        {
            "sha": "8de0a1c49b0d0adcb3650bb8fba55582d4fa9588",
            "filename": "src/transformers/models/bros/processing_bros.py",
            "status": "modified",
            "additions": 16,
            "deletions": 49,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -16,11 +16,22 @@\n Processor class for Bros.\n \"\"\"\n \n-from typing import Optional, Union\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin\n \n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType\n+\n+class BrosProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"add_special_tokens\": True,\n+            \"padding\": False,\n+            \"stride\": 0,\n+            \"return_overflowing_tokens\": False,\n+            \"return_special_tokens_mask\": False,\n+            \"return_offsets_mapping\": False,\n+            \"return_length\": False,\n+            \"verbose\": True,\n+        },\n+    }\n \n \n class BrosProcessor(ProcessorMixin):\n@@ -37,57 +48,13 @@ class BrosProcessor(ProcessorMixin):\n \n     attributes = [\"tokenizer\"]\n     tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n+    valid_processor_kwargs = BrosProcessorKwargs\n \n     def __init__(self, tokenizer=None, **kwargs):\n         if tokenizer is None:\n             raise ValueError(\"You need to specify a `tokenizer`.\")\n \n         super().__init__(tokenizer)\n \n-    def __call__(\n-        self,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_overflowing_tokens: bool = False,\n-        return_special_tokens_mask: bool = False,\n-        return_offsets_mapping: bool = False,\n-        return_length: bool = False,\n-        verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        **kwargs,\n-    ) -> BatchEncoding:\n-        \"\"\"\n-        This method uses [`BertTokenizerFast.__call__`] to prepare text for the model.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n-        encoding = self.tokenizer(\n-            text=text,\n-            add_special_tokens=add_special_tokens,\n-            padding=padding,\n-            truncation=truncation,\n-            max_length=max_length,\n-            stride=stride,\n-            pad_to_multiple_of=pad_to_multiple_of,\n-            return_token_type_ids=return_token_type_ids,\n-            return_attention_mask=return_attention_mask,\n-            return_overflowing_tokens=return_overflowing_tokens,\n-            return_special_tokens_mask=return_special_tokens_mask,\n-            return_offsets_mapping=return_offsets_mapping,\n-            return_length=return_length,\n-            verbose=verbose,\n-            return_tensors=return_tensors,\n-            **kwargs,\n-        )\n-\n-        return encoding\n-\n \n __all__ = [\"BrosProcessor\"]"
        },
        {
            "sha": "d481a62b6fc6608bd3088e4766b91ff843680ea0",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -99,7 +99,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "c6fb9f9df247555b2946cf471c650db2974450df",
            "filename": "src/transformers/models/chinese_clip/processing_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 77,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -17,15 +17,8 @@\n \"\"\"\n \n import warnings\n-from typing import Optional, Union\n \n-from ...image_utils import ImageInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n-\n-\n-class ChineseClipProcessorKwargs(ProcessingKwargs, total=False):\n-    _defaults = {}\n+from ...processing_utils import ProcessorMixin\n \n \n class ChineseCLIPProcessor(ProcessorMixin):\n@@ -58,79 +51,10 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n \n         super().__init__(image_processor, tokenizer)\n         self.current_processor = self.image_processor\n \n-    def __call__(\n-        self,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: Optional[ImageInput] = None,\n-        audio=None,\n-        videos=None,\n-        **kwargs: Unpack[ChineseClipProcessorKwargs],\n-    ) -> BatchEncoding:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to BertTokenizerFast's [`~BertTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                    - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                    - `'np'`: Return NumPy `np.ndarray` objects.\n-                    - `'jax'`: Return JAX `jnp.ndarray` objects.\n-        Returns:\n-            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n-\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n-            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n-        \"\"\"\n-\n-        if text is None and images is None:\n-            raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n-        output_kwargs = self._merge_kwargs(\n-            ChineseClipProcessorKwargs,\n-            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n-            **kwargs,\n-        )\n-\n-        if text is not None:\n-            encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-        if images is not None:\n-            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n-\n-        # BC for explicit return_tensors\n-        if \"return_tensors\" in output_kwargs[\"common_kwargs\"]:\n-            return_tensors = output_kwargs[\"common_kwargs\"].pop(\"return_tensors\", None)\n-\n-        if text is not None and images is not None:\n-            encoding[\"pixel_values\"] = image_features.pixel_values\n-            return encoding\n-        elif text is not None:\n-            return encoding\n-        else:\n-            return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "6524a87158418206b8b96a7b57f6c1b7392e56cf",
            "filename": "src/transformers/models/clap/processing_clap.py",
            "status": "modified",
            "additions": 28,
            "deletions": 53,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -16,8 +16,16 @@\n Audio/Text processor class for CLAP\n \"\"\"\n \n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding\n+from typing import Optional, Union\n+\n+from ...audio_utils import AudioInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n+from ...utils.deprecation import deprecate_kwarg\n+\n+\n+logger = logging.get_logger(__name__)\n \n \n class ClapProcessor(ProcessorMixin):\n@@ -40,61 +48,28 @@ class ClapProcessor(ProcessorMixin):\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n-    def __call__(self, text=None, audios=None, return_tensors=None, **kwargs):\n+    @deprecate_kwarg(\"audios\", version=\"v4.59.0\", new_name=\"audio\")\n+    def __call__(\n+        self,\n+        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        audios: Optional[AudioInput] = None,\n+        audio: Optional[AudioInput] = None,\n+        **kwargs: Unpack[ProcessingKwargs],\n+    ):\n         \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `text`\n-        and `kwargs` arguments to RobertaTokenizerFast's [`~RobertaTokenizerFast.__call__`] if `text` is not `None` to\n-        encode the text. To prepare the audio(s), this method forwards the `audios` and `kwrags` arguments to\n-        ClapFeatureExtractor's [`~ClapFeatureExtractor.__call__`] if `audios` is not `None`. Please refer to the\n-        docstring of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            audios (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The audio or batch of audios to be prepared. Each audio can be NumPy array or PyTorch tensor. In case\n-                of a NumPy array/PyTorch tensor, each audio should be of shape (C, T), where C is a number of channels,\n-                and T the sample length of the audio.\n-\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-                - `'jax'`: Return JAX `jnp.ndarray` objects.\n-\n-        Returns:\n-            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n-\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n-            - **audio_features** -- Audio features to be fed to a model. Returned when `audios` is not `None`.\n+        Forwards the `audio` and `sampling_rate` arguments to [`~ClapFeatureExtractor.__call__`] and the `text`\n+        argument to [`~RobertaTokenizerFast.__call__`]. Please refer to the docstring of the above two methods for more\n+        information.\n         \"\"\"\n-        sampling_rate = kwargs.pop(\"sampling_rate\", None)\n-\n-        if text is None and audios is None:\n-            raise ValueError(\"You have to specify either text or audios. Both cannot be none.\")\n-\n-        if text is not None:\n-            encoding = self.tokenizer(text, return_tensors=return_tensors, **kwargs)\n-\n-        if audios is not None:\n-            audio_features = self.feature_extractor(\n-                audios, sampling_rate=sampling_rate, return_tensors=return_tensors, **kwargs\n+        # The `deprecate_kwarg` will not work if the inputs are passed as arguments, so we check\n+        # again that the correct naming is used\n+        if audios is not None and audio is None:\n+            logger.warning(\n+                \"Using `audios` keyword argument is deprecated when calling ClapProcessor, instead use `audio`.\"\n             )\n+            audio = audios\n \n-        if text is not None and audios is not None:\n-            encoding.update(audio_features)\n-            return encoding\n-        elif text is not None:\n-            return encoding\n-        else:\n-            return BatchEncoding(data=dict(**audio_features), tensor_type=return_tensors)\n+        return super().__call__(text=text, audio=audio, **kwargs)\n \n \n __all__ = [\"ClapProcessor\"]"
        },
        {
            "sha": "893346e798bb2df38e9f05e2692327fd316fb0ab",
            "filename": "src/transformers/models/clip/processing_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 63,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -19,7 +19,6 @@\n import warnings\n \n from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding\n \n \n class CLIPProcessor(ProcessorMixin):\n@@ -51,71 +50,9 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n \n         super().__init__(image_processor, tokenizer)\n \n-    def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-                - `'jax'`: Return JAX `jnp.ndarray` objects.\n-\n-        Returns:\n-            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n-\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n-            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n-        \"\"\"\n-        tokenizer_kwargs, image_processor_kwargs = {}, {}\n-        if kwargs:\n-            tokenizer_kwargs = {k: v for k, v in kwargs.items() if k not in self.image_processor._valid_processor_keys}\n-            image_processor_kwargs = {\n-                k: v for k, v in kwargs.items() if k in self.image_processor._valid_processor_keys\n-            }\n-\n-        if text is None and images is None:\n-            raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n-\n-        if text is not None:\n-            encoding = self.tokenizer(text, return_tensors=return_tensors, **tokenizer_kwargs)\n-\n-        if images is not None:\n-            image_features = self.image_processor(images, return_tensors=return_tensors, **image_processor_kwargs)\n-\n-        if text is not None and images is not None:\n-            encoding[\"pixel_values\"] = image_features.pixel_values\n-            return encoding\n-        elif text is not None:\n-            return encoding\n-        else:\n-            return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "0af4c8ee12fcb47e659b6c05fffb50d63e299a7e",
            "filename": "src/transformers/models/clipseg/processing_clipseg.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -51,18 +51,14 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n \n         super().__init__(image_processor, tokenizer)\n \n     def __call__(self, text=None, images=None, visual_prompt=None, return_tensors=None, **kwargs):\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         ViTImageProcessor's [`~ViTImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring of\n         the above two methods for more information.\n "
        },
        {
            "sha": "8fad43cd2f30e1bde11e3f94e8393449d7026126",
            "filename": "src/transformers/models/clvp/processing_clvp.py",
            "status": "modified",
            "additions": 9,
            "deletions": 24,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -18,6 +18,10 @@\n \"\"\"\n \n from ...processing_utils import ProcessorMixin\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n \n \n class ClvpProcessor(ProcessorMixin):\n@@ -36,11 +40,6 @@ class ClvpProcessor(ProcessorMixin):\n \n     feature_extractor_class = \"ClvpFeatureExtractor\"\n     tokenizer_class = \"ClvpTokenizer\"\n-    model_input_names = [\n-        \"input_ids\",\n-        \"input_features\",\n-        \"attention_mask\",\n-    ]\n \n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n@@ -51,27 +50,13 @@ def __call__(self, *args, **kwargs):\n         argument to [`~ClvpTokenizer.__call__`]. Please refer to the docstring of the above two methods for more\n         information.\n         \"\"\"\n-\n         raw_speech = kwargs.pop(\"raw_speech\", None)\n-        sampling_rate = kwargs.pop(\"sampling_rate\", None)\n-        text = kwargs.pop(\"text\", None)\n-\n-        if raw_speech is None and text is None:\n-            raise ValueError(\"You need to specify either an `raw_speech` or `text` input to process.\")\n-\n         if raw_speech is not None:\n-            inputs = self.feature_extractor(raw_speech, sampling_rate=sampling_rate, **kwargs)\n-        if text is not None:\n-            encodings = self.tokenizer(text, **kwargs)\n-\n-        if text is None:\n-            return inputs\n-        elif raw_speech is None:\n-            return encodings\n-        else:\n-            inputs[\"input_ids\"] = encodings[\"input_ids\"]\n-            inputs[\"attention_mask\"] = encodings[\"attention_mask\"]\n-            return inputs\n+            logger.warning(\n+                \"Using `raw_speech` keyword argument is deprecated when calling ClvpProcessor, instead use `audio`.\"\n+            )\n+        kwargs[\"audio\"] = raw_speech\n+        return super().__call__(*args, **kwargs)\n \n \n __all__ = [\"ClvpProcessor\"]"
        },
        {
            "sha": "b3f758a00006cb8abec225b3308d2a30594a781a",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -106,10 +106,6 @@ def __init__(\n         query_prefix: str = \"Question: \",\n     ):\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n         if not hasattr(image_processor, \"image_seq_length\"):\n             raise ValueError(\"Image processor is missing an `image_seq_length` attribute.\")\n "
        },
        {
            "sha": "33f1cf26bb65106ad553657db916727bc9f720cf",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -249,7 +249,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         DeepseekVLImageProcessor's [`~DeepseekVLImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "26d59d85a2955022c2a428509dcd29eab5c7c5d4",
            "filename": "src/transformers/models/deepseek_vl/processing_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -78,7 +78,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         DeepseekVLImageProcessor's [`~DeepseekVLImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "6c36cfa50daa4956dd5924963a0ab982bd8a15ee",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -926,7 +926,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         DeepseekVLHybridImageProcessor's [`~DeepseekVLHybridImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "538fea5a6b322ddf0b8a0902915e726eab7420f0",
            "filename": "src/transformers/models/deepseek_vl_hybrid/processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -78,7 +78,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         DeepseekVLHybridImageProcessor's [`~DeepseekVLHybridImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "c75e2fcaa54203bdb4f3d64b977cc160c11c11fc",
            "filename": "src/transformers/models/donut/processing_donut.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -65,10 +65,6 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n \n         super().__init__(image_processor, tokenizer)\n         self.current_processor = self.image_processor"
        },
        {
            "sha": "67ccab795733563359dbba53e17591cab2878506",
            "filename": "src/transformers/models/emu3/processing_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -102,7 +102,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to Emu3TokenizerFast's [`~Emu3TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "ceebdb6efa49134c3d078a8dd03b08da7c59fb9d",
            "filename": "src/transformers/models/flava/processing_flava.py",
            "status": "modified",
            "additions": 32,
            "deletions": 77,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -17,12 +17,39 @@\n \"\"\"\n \n import warnings\n+from collections.abc import Iterable\n from typing import Optional, Union\n \n-from ...image_utils import ImageInput\n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin\n+\n+\n+class FlavaImagesKwargs(ImagesKwargs):\n+    # Mask related params\n+    return_image_mask: Optional[bool]\n+    input_size_patches: Optional[int]\n+    total_mask_patches: Optional[int]\n+    mask_group_min_patches: Optional[int]\n+    mask_group_max_patches: Optional[int]\n+    mask_group_min_aspect_ratio: Optional[float]\n+    mask_group_max_aspect_ratio: Optional[float]\n+    # Codebook related params\n+    return_codebook_pixels: Optional[bool]\n+    codebook_do_resize: Optional[bool]\n+    codebook_size: Optional[bool]\n+    codebook_resample: Optional[int]\n+    codebook_do_center_crop: Optional[bool]\n+    codebook_crop_size: Optional[int]\n+    codebook_do_rescale: Optional[bool]\n+    codebook_rescale_factor: Optional[Union[int, float]]\n+    codebook_do_map_pixels: Optional[bool]\n+    codebook_do_normalize: Optional[bool]\n+    codebook_image_mean: Optional[Union[float, Iterable[float]]]\n+    codebook_image_std: Optional[Union[float, Iterable[float]]]\n+\n+\n+class FlavaProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: FlavaImagesKwargs\n+    _defaults = {}\n \n \n class FlavaProcessor(ProcessorMixin):\n@@ -40,6 +67,7 @@ class FlavaProcessor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"FlavaImageProcessor\"\n     tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n+    valid_processor_kwargs = FlavaProcessorKwargs\n \n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         feature_extractor = None\n@@ -52,82 +80,9 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n-\n         super().__init__(image_processor, tokenizer)\n         self.current_processor = self.image_processor\n \n-    def __call__(\n-        self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = False,\n-        max_length: Optional[int] = None,\n-        stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_image_mask: Optional[bool] = None,\n-        return_codebook_pixels: Optional[bool] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_overflowing_tokens: bool = False,\n-        return_special_tokens_mask: bool = False,\n-        return_offsets_mapping: bool = False,\n-        return_length: bool = False,\n-        verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        This method uses [`FlavaImageProcessor.__call__`] method to prepare image(s) for the model, and\n-        [`BertTokenizerFast.__call__`] to prepare text for the model.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n-\n-        if text is None and images is None:\n-            raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n-\n-        if text is not None:\n-            encoding = self.tokenizer(\n-                text=text,\n-                add_special_tokens=add_special_tokens,\n-                padding=padding,\n-                truncation=truncation,\n-                max_length=max_length,\n-                stride=stride,\n-                pad_to_multiple_of=pad_to_multiple_of,\n-                return_token_type_ids=return_token_type_ids,\n-                return_attention_mask=return_attention_mask,\n-                return_overflowing_tokens=return_overflowing_tokens,\n-                return_special_tokens_mask=return_special_tokens_mask,\n-                return_offsets_mapping=return_offsets_mapping,\n-                return_length=return_length,\n-                verbose=verbose,\n-                return_tensors=return_tensors,\n-                **kwargs,\n-            )\n-        if images is not None:\n-            image_features = self.image_processor(\n-                images,\n-                return_image_mask=return_image_mask,\n-                return_codebook_pixels=return_codebook_pixels,\n-                return_tensors=return_tensors,\n-                **kwargs,\n-            )\n-\n-        if text is not None and images is not None:\n-            encoding.update(image_features)\n-            return encoding\n-        elif text is not None:\n-            return encoding\n-        else:\n-            return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "03e0fd0535cf93c7d2b317b8eba82de6ae7704ac",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -357,7 +357,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to BartTokenizerFast's [`~BartTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "53e3d562aa29371cd5185a17aeadf96d755af11e",
            "filename": "src/transformers/models/florence2/processing_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -158,7 +158,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to BartTokenizerFast's [`~BartTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "20fae1a9f36ba5ac0d0821a862667fa60940d713",
            "filename": "src/transformers/models/git/processing_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 74,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -16,20 +16,7 @@\n Image/Text processor class for GIT\n \"\"\"\n \n-from typing import Optional, Union\n-\n-from ...feature_extraction_utils import BatchFeature\n-from ...image_utils import ImageInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n-\n-\n-class GitProcessorKwargs(ProcessingKwargs, total=False):\n-    _defaults = {}\n-\n-\n-logger = logging.get_logger(__name__)\n+from ...processing_utils import ProcessorMixin\n \n \n class GitProcessor(ProcessorMixin):\n@@ -54,65 +41,5 @@ def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n         self.current_processor = self.image_processor\n \n-    def __call__(\n-        self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        audio=None,\n-        videos=None,\n-        **kwargs: Unpack[GitProcessorKwargs],\n-    ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to BertTokenizerFast's [`~BertTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`, *optional*):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-                - `'jax'`: Return JAX `jnp.ndarray` objects.\n-\n-        Returns:\n-            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n-\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n-            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n-        \"\"\"\n-        if text is None and images is None:\n-            raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n-\n-        output_kwargs = self._merge_kwargs(\n-            GitProcessorKwargs,\n-            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n-            **kwargs,\n-        )\n-\n-        data = {}\n-        if text is not None:\n-            text_features = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-            data.update(text_features)\n-        if images is not None:\n-            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n-            data.update(image_features)\n-\n-        return BatchFeature(data=data, tensor_type=output_kwargs[\"common_kwargs\"].get(\"return_tensors\"))\n-\n \n __all__ = [\"GitProcessor\"]"
        },
        {
            "sha": "16c062ec63ade1310971f8797291439b59bad5ac",
            "filename": "src/transformers/models/got_ocr2/processing_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -144,7 +144,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode the text if `text`\n         is not `None`, otherwise encode default OCR queries which depends on the `format`, `box`, `color`, `multi_page` and\n-        `crop_to_patches` arguments. To prepare the vision inputs, this method forwards the `images` and `kwrags` arguments to\n+        `crop_to_patches` arguments. To prepare the vision inputs, this method forwards the `images` and `kwargs` arguments to\n         GotOcr2ImageProcessor's [`~GotOcr2ImageProcessor.__call__`] if `images` is not `None`.\n \n         Args:"
        },
        {
            "sha": "ea0e288f3eecf473149e7751c563906236d8811a",
            "filename": "src/transformers/models/grounding_dino/processing_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 28,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -20,7 +20,6 @@\n import warnings\n from typing import TYPE_CHECKING, Optional, Union\n \n-from ...image_processing_utils import BatchFeature\n from ...image_transforms import center_to_corners_format\n from ...image_utils import AnnotationFormat, ImageInput\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n@@ -144,6 +143,7 @@ class GroundingDinoProcessor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"GroundingDinoImageProcessor\"\n     tokenizer_class = \"AutoTokenizer\"\n+    valid_processor_kwargs = GroundingDinoProcessorKwargs\n \n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n@@ -152,8 +152,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n         **kwargs: Unpack[GroundingDinoProcessorKwargs],\n     ) -> BatchEncoding:\n         \"\"\"\n@@ -170,33 +168,9 @@ def __call__(\n                 - A merged candidate labels string to be detected on the image, separated by \".\" (e.g. \"a cat. a dog.\").\n                 - A batch of merged candidate labels text to be detected on the batch of images (e.g. [\"a cat. a dog.\", \"a car. a person.\"]).\n         \"\"\"\n-        if images is None and text is None:\n-            raise ValueError(\"You must specify either text or images.\")\n-\n-        output_kwargs = self._merge_kwargs(\n-            GroundingDinoProcessorKwargs,\n-            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n-            **kwargs,\n-        )\n-\n-        # Get only text\n-        if images is not None:\n-            encoding_image_processor = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n-        else:\n-            encoding_image_processor = BatchFeature()\n-\n         if text is not None:\n             text = self._preprocess_input_text(text)\n-            text_encoding = self.tokenizer(\n-                text=text,\n-                **output_kwargs[\"text_kwargs\"],\n-            )\n-        else:\n-            text_encoding = BatchEncoding()\n-\n-        text_encoding.update(encoding_image_processor)\n-\n-        return text_encoding\n+        return super().__call__(images=images, text=text, **kwargs)\n \n     def _preprocess_input_text(self, text):\n         \"\"\""
        },
        {
            "sha": "59c8078ad84adf69463c4850f03999f9eac2db57",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -214,11 +214,6 @@ class IdeficsProcessor(ProcessorMixin):\n     tokenizer_class = \"LlamaTokenizerFast\"\n \n     def __init__(self, image_processor, tokenizer=None, image_size=224, add_end_of_utterance_token=None, **kwargs):\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n-\n         super().__init__(image_processor, tokenizer)\n         self.current_processor = self.image_processor\n         self.image_token_id = ("
        },
        {
            "sha": "550ca877409504ac3a608e5f8ce99259cf46b501",
            "filename": "src/transformers/models/idefics2/processing_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -90,11 +90,6 @@ class Idefics2Processor(ProcessorMixin):\n     def __init__(\n         self, image_processor, tokenizer=None, image_seq_len: int = 64, chat_template: Optional[str] = None, **kwargs\n     ):\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n-\n         if not hasattr(tokenizer, \"image_token\"):\n             self.fake_image_token = AddedToken(\"<fake_token_around_image>\", normalized=False, special=True).content\n             self.image_token = AddedToken(\"<image>\", normalized=False, special=True).content"
        },
        {
            "sha": "ab9eaac8e8b2abbfedf26db5069f7f13d1e9b659",
            "filename": "src/transformers/models/idefics3/processing_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -138,11 +138,6 @@ class Idefics3Processor(ProcessorMixin):\n     def __init__(\n         self, image_processor, tokenizer=None, image_seq_len: int = 169, chat_template: Optional[str] = None, **kwargs\n     ):\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n-\n         self.fake_image_token = AddedToken(\"<fake_token_around_image>\", normalized=False, special=True).content\n         self.image_token = AddedToken(\"<image>\", normalized=False, special=True).content\n         self.end_of_utterance_token = AddedToken(\"<end_of_utterance>\", normalized=False, special=True).content"
        },
        {
            "sha": "a13457886bafeb0a7f989a7b9b6bc49346fd6510",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -165,7 +165,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode the text if `text`\n         is not `None`, otherwise encode default OCR queries which depends on the `format`, `box`, `color`, `multi_page` and\n-        `crop_to_patches` arguments. To prepare the vision inputs, this method forwards the `images` and `kwrags` arguments to\n+        `crop_to_patches` arguments. To prepare the vision inputs, this method forwards the `images` and `kwargs` arguments to\n         GotOcr2ImageProcessor's [`~GotOcr2ImageProcessor.__call__`] if `images` is not `None`.\n \n         Args:"
        },
        {
            "sha": "2de97400043ffda6e969f2bb638aec34c6f4debb",
            "filename": "src/transformers/models/janus/processing_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -88,7 +88,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         JanusImageProcessor's [`~JanusImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "91a588857377431e605bf6b45041e8d7e7795c6c",
            "filename": "src/transformers/models/kyutai_speech_to_text/processing_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 50,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fprocessing_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fprocessing_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fprocessing_kyutai_speech_to_text.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -13,10 +13,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional\n \n-from ...audio_utils import AudioInput, make_list_of_audio\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin\n \n \n class KyutaiSpeechToTextProcessorKwargs(ProcessingKwargs, total=False):\n@@ -38,53 +36,7 @@ class KyutaiSpeechToTextProcessor(ProcessorMixin):\n \n     feature_extractor_class = \"KyutaiSpeechToTextFeatureExtractor\"\n     tokenizer_class = \"PreTrainedTokenizerFast\"\n-\n-    def __call__(\n-        self,\n-        audio: Optional[AudioInput] = None,\n-        **kwargs: Unpack[KyutaiSpeechToTextProcessorKwargs],\n-    ):\n-        r\"\"\"\n-        Main method to prepare audio to be fed as input to the model. This method forwards the `audio`\n-        arguments to KyutaiSpeechToTextFeatureExtractor's [`~KyutaiSpeechToTextFeatureExtractor.__call__`]. Please refer\n-        to the docstring of the above method for more information.\n-\n-        Args:\n-            audio (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The audio or batch of audio to be prepared. Each audio can be a NumPy array or PyTorch\n-                tensor.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                    - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                    - `'np'`: Return NumPy `np.ndarray` objects.\n-                    - `'jax'`: Return JAX `jnp.ndarray` objects.\n-        Returns:\n-            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n-\n-            - **input_values** -- List of audio values to be fed to a model. Returned when `audio` is not `None`.\n-            - **padding_mask** -- List of indices specifying which input values should be ignored by the model.\n-        \"\"\"\n-\n-        if audio is None:\n-            raise ValueError(\"`audio` is required.\")\n-\n-        output_kwargs = self._merge_kwargs(\n-            KyutaiSpeechToTextProcessorKwargs,\n-            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n-            **kwargs,\n-        )\n-        audio_kwargs = output_kwargs[\"audio_kwargs\"]\n-\n-        # ensure audio in correct format\n-        audio = make_list_of_audio(audio)\n-\n-        inputs = self.feature_extractor(\n-            audio,\n-            **audio_kwargs,\n-        )\n-\n-        return inputs\n+    valid_processor_kwargs = KyutaiSpeechToTextProcessorKwargs\n \n \n __all__ = [\"KyutaiSpeechToTextProcessor\"]"
        },
        {
            "sha": "603cdf4df4e93e615fbb127bdf2057ae8d1d3b2c",
            "filename": "src/transformers/models/layoutlmv2/processing_layoutlmv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -59,10 +59,6 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n \n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "1f1b6cead6077b391a993e346b6ab572f73e5380",
            "filename": "src/transformers/models/layoutlmv3/processing_layoutlmv3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -59,10 +59,6 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n \n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "e3ece89f434ba1ab69a1ec2ded35722559c32611",
            "filename": "src/transformers/models/layoutxlm/processing_layoutxlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -58,10 +58,6 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n \n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "63c07c20cbb9974fb6d2597e88e5224cd0111a52",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -101,7 +101,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "2574fc443519f928a1d8f14bed08ae15466950c5",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -111,7 +111,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "b9dbc6650b635875548940dd7e8c05d12eca5dd8",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -123,9 +123,9 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. To prepare the video(s),\n-        this method forwards the `videos` and `kwrags` arguments to LlavaNextVideoImageProcessor's\n+        this method forwards the `videos` and `kwargs` arguments to LlavaNextVideoImageProcessor's\n         [`~LlavaNextVideoImageProcessor.__call__`] if `videos` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "9fadc6af3067e3478c3f21626e4f9d3f99e64b00",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -121,7 +121,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "8f7d32388748b2c8d9ea3bbf35edacc464408a69",
            "filename": "src/transformers/models/mgp_str/processing_mgp_str.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -67,10 +67,6 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n \n         self.char_tokenizer = tokenizer\n         self.bpe_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")"
        },
        {
            "sha": "e2bcbf373d489a79eaaec6451e90f092439f3f9f",
            "filename": "src/transformers/models/musicgen/processing_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 26,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -60,33 +60,9 @@ def __call__(self, *args, **kwargs):\n         if self._in_target_context_manager:\n             return self.current_processor(*args, **kwargs)\n \n-        audio = kwargs.pop(\"audio\", None)\n-        sampling_rate = kwargs.pop(\"sampling_rate\", None)\n-        text = kwargs.pop(\"text\", None)\n         if len(args) > 0:\n-            audio = args[0]\n-            args = args[1:]\n-\n-        if audio is None and text is None:\n-            raise ValueError(\"You need to specify either an `audio` or `text` input to process.\")\n-\n-        if text is not None:\n-            inputs = self.tokenizer(text, **kwargs)\n-\n-        if audio is not None:\n-            audio_inputs = self.feature_extractor(audio, *args, sampling_rate=sampling_rate, **kwargs)\n-\n-        if audio is None:\n-            return inputs\n-\n-        elif text is None:\n-            return audio_inputs\n-\n-        else:\n-            inputs[\"input_values\"] = audio_inputs[\"input_values\"]\n-            if \"padding_mask\" in audio_inputs:\n-                inputs[\"padding_mask\"] = audio_inputs[\"padding_mask\"]\n-            return inputs\n+            kwargs[\"audio\"] = args[0]\n+        return super().__call__(*args, **kwargs)\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "1c2bbcb6e4a8ad7572f8ad507a71bde18d981711",
            "filename": "src/transformers/models/musicgen_melody/processing_musicgen_melody.py",
            "status": "modified",
            "additions": 7,
            "deletions": 40,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -51,49 +51,16 @@ def __init__(self, feature_extractor, tokenizer):\n     def get_decoder_prompt_ids(self, task=None, language=None, no_timestamps=True):\n         return self.tokenizer.get_decoder_prompt_ids(task=task, language=language, no_timestamps=no_timestamps)\n \n-    def __call__(self, audio=None, text=None, **kwargs):\n+    def __call__(self, *args, **kwargs):\n         \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `audio`\n-        and `kwargs` arguments to MusicgenMelodyFeatureExtractor's [`~MusicgenMelodyFeatureExtractor.__call__`] if `audio` is not\n-        `None` to pre-process the audio. It also forwards the `text` and `kwargs` arguments to\n-        PreTrainedTokenizer's [`~PreTrainedTokenizer.__call__`] if `text` is not `None`. Please refer to the docstring of the above two methods for more information.\n-\n-        Args:\n-            audio (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The audio or batch of audios to be prepared. Each audio can be NumPy array or PyTorch tensor. In case\n-                of a NumPy array/PyTorch tensor, each audio should be a mono-stereo signal of shape (T), where T is the sample length of the audio.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            kwargs (*optional*):\n-                Remaining dictionary of keyword arguments that will be passed to the feature extractor and/or the\n-                tokenizer.\n-        Returns:\n-            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **input_features** -- Audio input features to be fed to a model. Returned when `audio` is not `None`.\n-            - **attention_mask** -- List of token indices specifying which tokens should be attended to by the model when `text` is not `None`.\n-            When only `audio` is specified, returns the timestamps attention mask.\n+        Forwards the `audio` argument to EncodecFeatureExtractor's [`~EncodecFeatureExtractor.__call__`] and the `text`\n+        argument to [`~T5Tokenizer.__call__`]. Please refer to the docstring of the above two methods for more\n+        information.\n         \"\"\"\n \n-        sampling_rate = kwargs.pop(\"sampling_rate\", None)\n-\n-        if audio is None and text is None:\n-            raise ValueError(\"You need to specify either an `audio` or `text` input to process.\")\n-\n-        if text is not None:\n-            inputs = self.tokenizer(text, **kwargs)\n-        if audio is not None:\n-            audio_inputs = self.feature_extractor(audio, sampling_rate=sampling_rate, **kwargs)\n-\n-        if text is None:\n-            return audio_inputs\n-        elif audio is None:\n-            return inputs\n-        else:\n-            inputs[\"input_features\"] = audio_inputs[\"input_features\"]\n-            return inputs\n+        if len(args) > 0:\n+            kwargs[\"audio\"] = args[0]\n+        return super().__call__(*args, **kwargs)\n \n     # Copied from transformers.models.musicgen.processing_musicgen.MusicgenProcessor.batch_decode with padding_mask->attention_mask\n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "de5a0474e26a8d27dae1a20ef4db8b7eb19f6d0f",
            "filename": "src/transformers/models/oneformer/processing_oneformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Foneformer%2Fprocessing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Foneformer%2Fprocessing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fprocessing_oneformer.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -48,11 +48,6 @@ class OneFormerProcessor(ProcessorMixin):\n     def __init__(\n         self, image_processor=None, tokenizer=None, max_seq_length: int = 77, task_seq_length: int = 77, **kwargs\n     ):\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n-\n         self.max_seq_length = max_seq_length\n         self.task_seq_length = task_seq_length\n "
        },
        {
            "sha": "efb79409da2bd3edc5e9a89d0d0cabc3b76c53e0",
            "filename": "src/transformers/models/ovis2/processing_ovis2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -85,7 +85,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         Ovis2ImageProcessor's [`~Ovis2ImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "2e69379af73f022b909c4b05577d44ff0c7ccf35",
            "filename": "src/transformers/models/owlv2/processing_owlv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -86,7 +86,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several text(s) and image(s). This method forwards the `text` and\n         `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode:\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "0e0c59d555f2f56da112d7ee44659797d9af0b14",
            "filename": "src/transformers/models/owlvit/processing_owlvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -82,10 +82,6 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n \n         super().__init__(image_processor, tokenizer)\n \n@@ -100,7 +96,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several text(s) and image(s). This method forwards the `text` and\n         `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode:\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "242627a0eb71a98f71ca0aadf260cc51df3fb085",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -125,10 +125,6 @@ def __init__(\n         chat_template=None,\n         **kwargs,\n     ):\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n         if not hasattr(image_processor, \"image_seq_length\"):\n             raise ValueError(\"Image processor is missing an `image_seq_length` attribute.\")\n \n@@ -161,7 +157,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to GemmaTokenizerFast's [`~GemmaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         SiglipImageProcessor's [`~SiglipImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "4a1af1d6bb780bf22d7ba0d74bdf98bf0e41daac",
            "filename": "src/transformers/models/phi4_multimodal/processing_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -86,7 +86,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forards the `text`\n         and `kwargs` arguments to GPT2Tokenizer's [`~GPT2Tokenizer.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         Phi4MultimodalImageProcessorFast's [`~Phi4MultimodalImageProcessorFast.__call__`] if `images` is not `None`. Please refer to the doctsring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "bb868156fb40b4f6874124f3c210dee48b5abc36",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -127,7 +127,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "d62f94f37678a047d952411464d3d3e4f702960d",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -893,7 +893,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwrags` arguments to\n+        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n         Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n \n         Args:"
        },
        {
            "sha": "b357ba850debac6afbcff27f349d47083529d750",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -105,7 +105,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwrags` arguments to\n+        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n         Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n \n         Args:"
        },
        {
            "sha": "e591d5fb8d916f0bda7a8afc28e13e64eec15baf",
            "filename": "src/transformers/models/qwen2_audio/processing_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -87,7 +87,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `text`\n         and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the audio(s), this method forwards the `audios` and `kwrags` arguments to\n+        the text. To prepare the audio(s), this method forwards the `audios` and `kwargs` arguments to\n         WhisperFeatureExtractor's [`~WhisperFeatureExtractor.__call__`] if `audios` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "5bbbf6ac1aecb5a4a11039d068eea726e84ed192",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -100,7 +100,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwrags` arguments to\n+        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n         Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n \n         Args:"
        },
        {
            "sha": "85e1968d7d43c006b5495b5729031abfd5026010",
            "filename": "src/transformers/models/seamless_m4t/processing_seamless_m4t.py",
            "status": "modified",
            "additions": 37,
            "deletions": 28,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -16,7 +16,26 @@\n Audio/Text processor class for SeamlessM4T\n \"\"\"\n \n-from ...processing_utils import ProcessorMixin\n+from typing import Optional, Union\n+\n+from ...audio_utils import AudioInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n+from ...utils.deprecation import deprecate_kwarg\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class SeamlessM4TTextKwargs(TextKwargs):\n+    src_lang: Optional[str]\n+    tgt_lang: Optional[str]\n+\n+\n+class SeamlessM4TProcessorKwargs(ProcessingKwargs, total=False):\n+    text_kwargs: SeamlessM4TTextKwargs\n+    _defaults = {}\n \n \n class SeamlessM4TProcessor(ProcessorMixin):\n@@ -37,15 +56,23 @@ class SeamlessM4TProcessor(ProcessorMixin):\n \n     feature_extractor_class = \"SeamlessM4TFeatureExtractor\"\n     tokenizer_class = (\"SeamlessM4TTokenizer\", \"SeamlessM4TTokenizerFast\")\n+    valid_processor_kwargs = SeamlessM4TProcessorKwargs\n \n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n-    def __call__(self, text=None, audios=None, src_lang=None, tgt_lang=None, **kwargs):\n+    @deprecate_kwarg(\"audios\", version=\"v4.59.0\", new_name=\"audio\")\n+    def __call__(\n+        self,\n+        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        audios: Optional[AudioInput] = None,\n+        audio: Optional[AudioInput] = None,\n+        **kwargs: Unpack[ProcessingKwargs],\n+    ):\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `text`\n         and `kwargs` arguments to SeamlessM4TTokenizerFast's [`~SeamlessM4TTokenizerFast.__call__`] if `text` is not\n-        `None` to encode the text. To prepare the audio(s), this method forwards the `audios` and `kwrags` arguments to\n+        `None` to encode the text. To prepare the audio(s), this method forwards the `audios` and `kwargs` arguments to\n         SeamlessM4TFeatureExtractor's [`~SeamlessM4TFeatureExtractor.__call__`] if `audios` is not `None`. Please refer\n         to the docstring of the above two methods for more information.\n \n@@ -58,14 +85,6 @@ def __call__(self, text=None, audios=None, src_lang=None, tgt_lang=None, **kwarg\n                 The audio or batch of audios to be prepared. Each audio can be NumPy array or PyTorch tensor. In case\n                 of a NumPy array/PyTorch tensor, each audio should be of shape (C, T), where C is a number of channels,\n                 and T the sample length of the audio.\n-            src_lang (`str`, *optional*):\n-                The language code of the input texts/audios. If not specified, the last `src_lang` specified will be\n-                used.\n-            tgt_lang (`str`, *optional*):\n-                The code of the target language. If not specified, the last `tgt_lang` specified will be used.\n-            kwargs (*optional*):\n-                Remaining dictionary of keyword arguments that will be passed to the feature extractor and/or the\n-                tokenizer.\n         Returns:\n             [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n \n@@ -75,26 +94,16 @@ def __call__(self, text=None, audios=None, src_lang=None, tgt_lang=None, **kwarg\n               `None`).\n             - **input_features** -- Audio input features to be fed to a model. Returned when `audios` is not `None`.\n         \"\"\"\n-        sampling_rate = kwargs.pop(\"sampling_rate\", None)\n-\n-        if text is None and audios is None:\n-            raise ValueError(\"You have to specify either text or audios. Both cannot be none.\")\n-        elif text is not None and audios is not None:\n+        if text is not None and audios is not None:\n             raise ValueError(\n                 \"Text and audios are mututally exclusive when passed to `SeamlessM4T`. Specify one or another.\"\n             )\n-        elif text is not None:\n-            if tgt_lang is not None:\n-                self.tokenizer.tgt_lang = tgt_lang\n-            if src_lang is not None:\n-                self.tokenizer.src_lang = src_lang\n-            encoding = self.tokenizer(text, **kwargs)\n-\n-            return encoding\n-\n-        else:\n-            encoding = self.feature_extractor(audios, sampling_rate=sampling_rate, **kwargs)\n-            return encoding\n+        if audio is None and audios is not None:\n+            logger.warning(\n+                \"Passing `audios` as keyword argument is deprecated and will be removed in v4.63, please pass `audio` instead.\"\n+            )\n+            audio = audios\n+        return super().__call__(text=text, audio=audio, **kwargs)\n \n \n __all__ = [\"SeamlessM4TProcessor\"]"
        },
        {
            "sha": "f0284ae66670768ac399082c30eae997fbddf0cf",
            "filename": "src/transformers/models/siglip/processing_siglip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 80,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -16,13 +16,7 @@\n Image/Text processor class for SigLIP.\n \"\"\"\n \n-from typing import Optional, Union\n-\n-from ...feature_extraction_utils import BatchFeature\n-from ...image_utils import ImageInput\n from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType\n \n \n class SiglipProcessor(ProcessorMixin):\n@@ -46,79 +40,5 @@ class SiglipProcessor(ProcessorMixin):\n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n \n-    def __call__(\n-        self,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        images: Optional[ImageInput] = None,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n-    ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to SiglipTokenizer's [`~SiglipTokenizer.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` argument to\n-        SiglipImageProcessor's [`~SiglipImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n-                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n-                index) among:\n-                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n-                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-                  acceptable input length for the model if that argument is not provided.\n-                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-                  lengths).\n-            max_length (`int`, *optional*):\n-                Maximum length of the returned list and optionally padding length (see above).\n-            truncation (`bool`, *optional*):\n-                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-                - `'jax'`: Return JAX `jnp.ndarray` objects.\n-\n-        Returns:\n-            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n-\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n-            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n-        \"\"\"\n-\n-        if text is None and images is None:\n-            raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n-\n-        if text is not None:\n-            encoding = self.tokenizer(\n-                text, return_tensors=return_tensors, padding=padding, truncation=truncation, max_length=max_length\n-            )\n-\n-        if images is not None:\n-            image_features = self.image_processor(images, return_tensors=return_tensors)\n-\n-        if text is not None and images is not None:\n-            encoding.update(image_features)\n-            return encoding\n-        elif text is not None:\n-            return encoding\n-        else:\n-            return BatchFeature(data=dict(**image_features), tensor_type=return_tensors)\n-\n \n __all__ = [\"SiglipProcessor\"]"
        },
        {
            "sha": "8e177b237b10633cc6181f07d54d24fbd1600d5c",
            "filename": "src/transformers/models/siglip2/processing_siglip2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 85,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -16,12 +16,9 @@\n Image/Text processor class for SigLIP2.\n \"\"\"\n \n-from typing import Optional, Union\n+from typing import Optional\n \n-from ...feature_extraction_utils import BatchFeature\n-from ...image_utils import ImageInput\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin\n \n \n class Siglip2ImagesKwargs(ImagesKwargs, total=False):\n@@ -63,89 +60,10 @@ class Siglip2Processor(ProcessorMixin):\n \n     image_processor_class = \"AutoImageProcessor\"\n     tokenizer_class = \"AutoTokenizer\"\n+    valid_processor_kwargs = Siglip2ProcessorKwargs\n \n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n \n-    def __call__(\n-        self,\n-        images: Optional[Union[ImageInput, list[ImageInput], list[list[ImageInput]]]] = None,\n-        text: Optional[Union[TextInput, \"PreTokenizedInput\", list[TextInput], list[\"PreTokenizedInput\"]]] = None,\n-        audio=None,\n-        videos=None,\n-        **kwargs: Unpack[Siglip2ProcessorKwargs],\n-    ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to GemmaTokenizerFast's [`~GemmaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` argument to\n-        Siglip2ImageProcessor's [`~Siglip2ImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `max_length`):\n-                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n-                index) among:\n-                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-                  acceptable input length for the model if that argument is not provided.\n-                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n-                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-                  lengths).\n-            max_length (`int`, *optional*, defaults to 64):\n-                Maximum length of the returned list and optionally padding length (see above).\n-            truncation (`bool`, *optional*, defaults to `True`):\n-                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*, defaults to `'pt'`):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-                - `'jax'`: Return JAX `jnp.ndarray` objects.\n-\n-        Returns:\n-            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n-\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n-            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n-            - **pixel_attention_mask** -- Attention mask for the pixel values. Returned when `images` is not `None`.\n-            - **spatial_shapes** -- The number of horizontal and vertical patches per image.\n-              Returned when `images` is not `None`.\n-        \"\"\"\n-        output_kwargs = self._merge_kwargs(\n-            Siglip2ProcessorKwargs,\n-            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n-            **kwargs,\n-        )\n-\n-        if text is None and images is None:\n-            raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n-\n-        if text is not None:\n-            encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-\n-        if images is not None:\n-            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n-\n-        if text is not None and images is not None:\n-            encoding.update(image_features)\n-            return encoding\n-        elif text is not None:\n-            return encoding\n-        else:\n-            return_tensors = output_kwargs[\"common_kwargs\"][\"return_tensors\"]\n-            return BatchFeature(data=dict(**image_features), tensor_type=return_tensors)\n-\n \n __all__ = [\"Siglip2Processor\"]"
        },
        {
            "sha": "049ed96c1749c033b0cb0142067a52f64e80985c",
            "filename": "src/transformers/models/trocr/processing_trocr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -60,10 +60,6 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n \n         super().__init__(image_processor, tokenizer)\n         self.current_processor = self.image_processor"
        },
        {
            "sha": "7cec0f14ab765b2cfe9d4bca97dd565fb904e9ac",
            "filename": "src/transformers/models/tvp/processing_tvp.py",
            "status": "modified",
            "additions": 13,
            "deletions": 69,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -16,8 +16,18 @@\n Processor class for TVP.\n \"\"\"\n \n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin\n+\n+\n+class TvpProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"truncation\": True,\n+            \"padding\": \"max_length\",\n+            \"pad_to_max_length\": True,\n+            \"return_token_type_ids\": False,\n+        },\n+    }\n \n \n class TvpProcessor(ProcessorMixin):\n@@ -39,74 +49,8 @@ class TvpProcessor(ProcessorMixin):\n     tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n \n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n-\n         super().__init__(image_processor, tokenizer)\n-\n-    def __call__(self, text=None, videos=None, return_tensors=None, **kwargs):\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to BertTokenizerFast's [`~BertTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `videos` and `kwargs` arguments to\n-        TvpImageProcessor's [`~TvpImageProcessor.__call__`] if `videos` is not `None`. Please refer to the docstring of\n-        the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`, `list[list[PIL.Image.Image]]`, `list[list[np.ndarray]]`,:\n-                `list[list[torch.Tensor]]`): The video or batch of videos to be prepared. Each video should be a list\n-                of frames, which can be either PIL images or NumPy arrays. In case of NumPy arrays/PyTorch tensors,\n-                each frame should be of shape (H, W, C), where H and W are frame height and width, and C is a number of\n-                channels.\n-\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-                - `'jax'`: Return JAX `jnp.ndarray` objects.\n-\n-        Returns:\n-            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n-\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n-            - **pixel_values** -- Pixel values to be fed to a model. Returned when `videos` is not `None`.\n-        \"\"\"\n-\n-        max_text_length = kwargs.pop(\"max_text_length\", None)\n-\n-        if text is None and videos is None:\n-            raise ValueError(\"You have to specify either text or videos. Both cannot be none.\")\n-\n-        encoding = {}\n-        if text is not None:\n-            textual_input = self.tokenizer.batch_encode_plus(\n-                text,\n-                truncation=True,\n-                padding=\"max_length\",\n-                max_length=max_text_length,\n-                pad_to_max_length=True,\n-                return_tensors=return_tensors,\n-                return_token_type_ids=False,\n-                **kwargs,\n-            )\n-            encoding.update(textual_input)\n-\n-        if videos is not None:\n-            image_features = self.image_processor(videos, return_tensors=return_tensors, **kwargs)\n-            encoding.update(image_features)\n-\n-        return BatchEncoding(data=encoding, tensor_type=return_tensors)\n+        self.video_processor = image_processor\n \n     def post_process_video_grounding(self, logits, video_durations):\n         \"\"\""
        },
        {
            "sha": "fc7a396853a8a64054952a2ff321b46a951ba1bd",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -100,7 +100,7 @@ def __call__(\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         VideoLlavaImageProcessor's [`~VideoLlavaImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n "
        },
        {
            "sha": "5b5126ad4a85a0e170e6934c61d77ce7fbb9081f",
            "filename": "src/transformers/models/vilt/processing_vilt.py",
            "status": "modified",
            "additions": 17,
            "deletions": 59,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -17,11 +17,23 @@\n \"\"\"\n \n import warnings\n-from typing import Optional, Union\n \n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin\n+\n+\n+class ViltProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"add_special_tokens\": True,\n+            \"padding\": False,\n+            \"stride\": 0,\n+            \"return_overflowing_tokens\": False,\n+            \"return_special_tokens_mask\": False,\n+            \"return_offsets_mapping\": False,\n+            \"return_length\": False,\n+            \"verbose\": True,\n+        },\n+    }\n \n \n class ViltProcessor(ProcessorMixin):\n@@ -41,6 +53,7 @@ class ViltProcessor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"ViltImageProcessor\"\n     tokenizer_class = (\"BertTokenizer\", \"BertTokenizerFast\")\n+    valid_processor_kwargs = ViltProcessorKwargs\n \n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         feature_extractor = None\n@@ -53,64 +66,9 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n-\n         super().__init__(image_processor, tokenizer)\n         self.current_processor = self.image_processor\n \n-    def __call__(\n-        self,\n-        images,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_overflowing_tokens: bool = False,\n-        return_special_tokens_mask: bool = False,\n-        return_offsets_mapping: bool = False,\n-        return_length: bool = False,\n-        verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        **kwargs,\n-    ) -> BatchEncoding:\n-        \"\"\"\n-        This method uses [`ViltImageProcessor.__call__`] method to prepare image(s) for the model, and\n-        [`BertTokenizerFast.__call__`] to prepare text for the model.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n-        encoding = self.tokenizer(\n-            text=text,\n-            add_special_tokens=add_special_tokens,\n-            padding=padding,\n-            truncation=truncation,\n-            max_length=max_length,\n-            stride=stride,\n-            pad_to_multiple_of=pad_to_multiple_of,\n-            return_token_type_ids=return_token_type_ids,\n-            return_attention_mask=return_attention_mask,\n-            return_overflowing_tokens=return_overflowing_tokens,\n-            return_special_tokens_mask=return_special_tokens_mask,\n-            return_offsets_mapping=return_offsets_mapping,\n-            return_length=return_length,\n-            verbose=verbose,\n-            return_tensors=return_tensors,\n-            **kwargs,\n-        )\n-        # add pixel_values + pixel_mask\n-        encoding_image_processor = self.image_processor(images, return_tensors=return_tensors)\n-        encoding.update(encoding_image_processor)\n-\n-        return encoding\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "7bfe3e980ef061703c693604bd00ee973fd7f20d",
            "filename": "src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 77,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -17,11 +17,8 @@\n \"\"\"\n \n import warnings\n-from typing import Optional, Union\n \n-from ...image_utils import ImageInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin\n \n \n class VisionTextDualEncoderProcessorKwargs(ProcessingKwargs, total=False):\n@@ -59,82 +56,9 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You have to specify an image_processor.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You have to specify a tokenizer.\")\n-\n         super().__init__(image_processor, tokenizer)\n         self.current_processor = self.image_processor\n \n-    def __call__(\n-        self,\n-        images: Optional[ImageInput] = None,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        audio=None,\n-        videos=None,\n-        **kwargs: Unpack[VisionTextDualEncoderProcessorKwargs],\n-    ) -> BatchEncoding:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to VisionTextDualEncoderTokenizer's [`~PreTrainedTokenizer.__call__`] if `text` is not\n-        `None` to encode the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        AutoImageProcessor's [`~AutoImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-                - `'jax'`: Return JAX `jnp.ndarray` objects.\n-\n-        Returns:\n-            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n-\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n-            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n-        \"\"\"\n-\n-        if text is None and images is None:\n-            raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n-\n-        output_kwargs = self._merge_kwargs(\n-            VisionTextDualEncoderProcessorKwargs,\n-            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n-            **kwargs,\n-        )\n-\n-        if text is not None:\n-            encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-\n-        if images is not None:\n-            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n-\n-        if text is not None and images is not None:\n-            encoding[\"pixel_values\"] = image_features.pixel_values\n-            return encoding\n-        elif text is not None:\n-            return encoding\n-        else:\n-            return BatchEncoding(\n-                data=dict(**image_features),\n-                tensor_type=output_kwargs[\"common_kwargs\"].get(\"return_tensors\"),\n-            )\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "581dabc6d8b598d48789ba2bcc3f396672dff408",
            "filename": "src/transformers/models/x_clip/processing_x_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 60,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -19,7 +19,6 @@\n import warnings\n \n from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding\n \n \n class XCLIPProcessor(ProcessorMixin):\n@@ -51,68 +50,10 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n             feature_extractor = kwargs.pop(\"feature_extractor\")\n \n         image_processor = image_processor if image_processor is not None else feature_extractor\n-        if image_processor is None:\n-            raise ValueError(\"You need to specify an `image_processor`.\")\n-        if tokenizer is None:\n-            raise ValueError(\"You need to specify a `tokenizer`.\")\n-\n         super().__init__(image_processor, tokenizer)\n+        self.video_processor = self.image_processor\n         self.current_processor = self.image_processor\n \n-    def __call__(self, text=None, videos=None, return_tensors=None, **kwargs):\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `videos` and `kwargs` arguments to\n-        VideoMAEImageProcessor's [`~VideoMAEImageProcessor.__call__`] if `videos` is not `None`. Please refer to the\n-        docstring of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`, `list[list[PIL.Image.Image]]`, `list[list[np.ndarray]]`,:\n-                `list[list[torch.Tensor]]`): The video or batch of videos to be prepared. Each video should be a list\n-                of frames, which can be either PIL images or NumPy arrays. In case of NumPy arrays/PyTorch tensors,\n-                each frame should be of shape (H, W, C), where H and W are frame height and width, and C is a number of\n-                channels.\n-\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-                - `'jax'`: Return JAX `jnp.ndarray` objects.\n-\n-        Returns:\n-            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n-\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n-            - **pixel_values** -- Pixel values to be fed to a model. Returned when `videos` is not `None`.\n-        \"\"\"\n-\n-        if text is None and videos is None:\n-            raise ValueError(\"You have to specify either text or videos. Both cannot be none.\")\n-\n-        if text is not None:\n-            encoding = self.tokenizer(text, return_tensors=return_tensors, **kwargs)\n-\n-        if videos is not None:\n-            image_features = self.image_processor(videos, return_tensors=return_tensors, **kwargs)\n-\n-        if text is not None and videos is not None:\n-            encoding[\"pixel_values\"] = image_features.pixel_values\n-            return encoding\n-        elif text is not None:\n-            return encoding\n-        else:\n-            return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "3130d0ded34f4d741fe49d9964b3b8c124092462",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 70,
            "deletions": 4,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -31,12 +31,12 @@\n import typing_extensions\n from huggingface_hub.errors import EntryNotFoundError\n \n-from .audio_utils import load_audio\n+from .audio_utils import AudioInput, load_audio\n from .dynamic_module_utils import custom_object_save\n from .feature_extraction_utils import BatchFeature\n-from .image_utils import ChannelDimension, is_vision_available\n+from .image_utils import ChannelDimension, ImageInput, is_vision_available\n from .utils.chat_template_utils import render_jinja_template\n-from .video_utils import VideoMetadata\n+from .video_utils import VideoInput, VideoMetadata\n \n \n if is_vision_available():\n@@ -335,7 +335,8 @@ class CommonKwargs(TypedDict, total=False):\n class ProcessingKwargs(TypedDict, total=False):\n     \"\"\"\n     Base class for kwargs passing to processors.\n-    A model should have its own `ModelProcessorKwargs` class that inherits from `ProcessingKwargs` to provide:\n+    In case a model has specific kwargs that are not present in the base class or default values for existing keys,\n+    it should have its own `ModelProcessorKwargs` class that inherits from `ProcessingKwargs` to provide:\n         1) Additional typed keys and that this model requires to process inputs.\n         2) Default values for existing keys under a `_defaults` attribute.\n     New keys have to be defined as follows to ensure type hinting is done correctly.\n@@ -370,6 +371,8 @@ class CustomProcessorKwargs(ProcessingKwargs, total=False):\n \n     \"\"\"\n \n+    _defaults = {}\n+\n     common_kwargs: CommonKwargs = {\n         **CommonKwargs.__annotations__,\n     }\n@@ -499,6 +502,7 @@ class ProcessorMixin(PushToHubMixin):\n     feature_extractor_class = None\n     tokenizer_class = None\n     _auto_class = None\n+    valid_processor_kwargs = ProcessingKwargs\n \n     # args have to match the attributes class attribute\n     def __init__(self, *args, **kwargs):\n@@ -539,6 +543,68 @@ def __init__(self, *args, **kwargs):\n             self.check_argument_for_proper_class(attribute_name, arg)\n             setattr(self, attribute_name, arg)\n \n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n+        videos: Optional[VideoInput] = None,\n+        audio: Optional[AudioInput] = None,\n+        **kwargs: Unpack[ProcessingKwargs],\n+    ):\n+        \"\"\"\n+        Main method to prepare for model inputs. This method forwards the each modality argument to its own processor\n+        along with `kwargs`. Please refer to the docstring of the each processor attributes for more information.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`, *optional*):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            videos (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The video or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n+                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n+            audio (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                The audio or batch of audio to be prepared. Each audio can be a NumPy array or PyTorch\n+                tensor.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] object with processed inputs in a dict format.\n+        \"\"\"\n+        if images is None and text is None and videos is None and audio is None:\n+            raise ValueError(f\"You need to provide at least one input to call {self.__class__.__name__}\")\n+\n+        kwargs = self._merge_kwargs(\n+            self.valid_processor_kwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs if hasattr(self, \"tokenizer\") else {},\n+            **kwargs,\n+        )\n+\n+        attribute_to_kwargs = {\n+            \"tokenizer\": (text, \"text_kwargs\"),\n+            \"image_processor\": (images, \"images_kwargs\"),\n+            \"video_processor\": (videos, \"videos_kwargs\"),\n+            \"feature_extractor\": (audio, \"audio_kwargs\"),\n+        }\n+        outputs = {}\n+        for attribute_name in self.attributes:\n+            attribute = getattr(self, attribute_name, None)\n+            input_data, input_kwargs = attribute_to_kwargs[attribute_name]\n+            if input_data is not None and attribute is not None:\n+                attribute_output = attribute(input_data, **kwargs[input_kwargs])\n+                outputs.update(attribute_output)\n+\n+        return BatchFeature(outputs)\n+\n     def check_argument_for_proper_class(self, argument_name, argument):\n         \"\"\"\n         Checks the passed argument's class against the expected transformers class. In case of an unexpected"
        },
        {
            "sha": "5aef3d06c15b65f34be2869c1c434f127c898d94",
            "filename": "tests/models/chinese_clip/test_processing_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/tests%2Fmodels%2Fchinese_clip%2Ftest_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/tests%2Fmodels%2Fchinese_clip%2Ftest_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_processing_chinese_clip.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -184,7 +184,7 @@ def test_processor(self):\n \n         inputs = processor(text=input_str, images=image_input)\n \n-        self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"token_type_ids\", \"attention_mask\", \"pixel_values\"])\n+        self.assertSetEqual(set(inputs.keys()), {\"input_ids\", \"token_type_ids\", \"attention_mask\", \"pixel_values\"})\n \n         # test if it raises when no input is passed\n         with pytest.raises(ValueError):"
        },
        {
            "sha": "10a00a869915fcdb9a047426e58ed88409a3fc7e",
            "filename": "tests/models/flava/test_processing_flava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/tests%2Fmodels%2Fflava%2Ftest_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/tests%2Fmodels%2Fflava%2Ftest_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_processing_flava.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -186,21 +186,21 @@ def test_processor(self):\n \n         inputs = processor(text=input_str, images=image_input)\n \n-        self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"token_type_ids\", \"attention_mask\", \"pixel_values\"])\n+        self.assertSetEqual(set(inputs.keys()), {\"input_ids\", \"token_type_ids\", \"attention_mask\", \"pixel_values\"})\n \n         # add extra args\n         inputs = processor(text=input_str, images=image_input, return_codebook_pixels=True, return_image_mask=True)\n \n-        self.assertListEqual(\n-            list(inputs.keys()),\n-            [\n+        self.assertSetEqual(\n+            set(inputs.keys()),\n+            {\n                 \"input_ids\",\n                 \"token_type_ids\",\n                 \"attention_mask\",\n                 \"pixel_values\",\n                 \"codebook_pixel_values\",\n                 \"bool_masked_pos\",\n-            ],\n+            },\n         )\n \n         # test if it raises when no input is passed"
        },
        {
            "sha": "5e06636007bcc6f1573a277ded336c30eb8ffc9d",
            "filename": "tests/models/git/test_processing_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/tests%2Fmodels%2Fgit%2Ftest_processing_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/tests%2Fmodels%2Fgit%2Ftest_processing_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_processing_git.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -112,7 +112,7 @@ def test_processor(self):\n \n         inputs = processor(text=input_str, images=image_input)\n \n-        self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values\"])\n+        self.assertSetEqual(set(inputs.keys()), {\"input_ids\", \"attention_mask\", \"pixel_values\"})\n \n         # test if it raises when no input is passed\n         with pytest.raises(ValueError):"
        },
        {
            "sha": "088f240eee7308d53d7227e1d3d18c5311e92232",
            "filename": "tests/models/grounding_dino/test_processing_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -236,8 +236,8 @@ def test_processor(self):\n \n         inputs = processor(text=input_str, images=image_input)\n \n-        self.assertListEqual(\n-            list(inputs.keys()), [\"input_ids\", \"token_type_ids\", \"attention_mask\", \"pixel_values\", \"pixel_mask\"]\n+        self.assertSetEqual(\n+            set(inputs.keys()), {\"input_ids\", \"token_type_ids\", \"attention_mask\", \"pixel_values\", \"pixel_mask\"}\n         )\n \n         # test if it raises when no input is passed"
        },
        {
            "sha": "7e1b025721dcad6f6fad7fd7bc898d654fbae897",
            "filename": "tests/models/janus/test_processing_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -38,6 +38,7 @@ def setUp(self):\n         processor = self.processor_class.from_pretrained(\n             \"deepseek-community/Janus-Pro-1B\",\n             extra_special_tokens=special_image_tokens,\n+            **self.prepare_processor_dict(),\n         )\n         # Set the processor to use the default system prompt to False as it's used based on input modality.\n         # Hence set to False to avoid any issues in the test irrespective of inputs."
        },
        {
            "sha": "2e0c21e6334210f26fe4d9fdf87f7a54cbe4dd18",
            "filename": "tests/models/vision_text_dual_encoder/test_processing_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_processing_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08edec9f7da1ed3d43f277a870cd8cf5305df4b1/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_processing_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_processing_vision_text_dual_encoder.py?ref=08edec9f7da1ed3d43f277a870cd8cf5305df4b1",
            "patch": "@@ -149,7 +149,7 @@ def test_processor(self):\n \n         inputs = processor(text=input_str, images=image_input)\n \n-        self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"token_type_ids\", \"attention_mask\", \"pixel_values\"])\n+        self.assertListEqual(list(inputs.keys()), [\"pixel_values\", \"input_ids\", \"token_type_ids\", \"attention_mask\"])\n \n         # test if it raises when no input is passed\n         with self.assertRaises(ValueError):"
        }
    ],
    "stats": {
        "total": 1589,
        "additions": 286,
        "deletions": 1303
    }
}