{
    "author": "MekkCyber",
    "message": "[quantization] Skip Fp8 tests when hardware capability < 8.9 (#41785)\n\n* skipping tests\n\n* style\n\n* nit",
    "sha": "18a3349a9f953b2d9ca34c03558b2de201f9a0e9",
    "files": [
        {
            "sha": "a6edcbf542784f4d73fb3b76fc9fa4c8e0a3bf28",
            "filename": "tests/quantization/finegrained_fp8/test_fp8.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/18a3349a9f953b2d9ca34c03558b2de201f9a0e9/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18a3349a9f953b2d9ca34c03558b2de201f9a0e9/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py?ref=18a3349a9f953b2d9ca34c03558b2de201f9a0e9",
            "patch": "@@ -64,6 +64,11 @@ def test_from_dict(self):\n @require_accelerate\n @require_read_token\n @require_torch_accelerator\n+@unittest.skipIf(\n+    get_device_properties()[0] == \"cuda\"\n+    and (get_device_properties()[1] < 8 or (get_device_properties()[1] == 8 and get_device_properties()[2] < 9)),\n+    \"Skipping FP8QuantizerTest because it is not supported on GPU with capability < 8.9\",\n+)\n class FP8QuantizerTest(unittest.TestCase):\n     model_name = \"meta-llama/Llama-3.2-1B\"\n     input_text = \"Once upon a time\"\n@@ -251,13 +256,14 @@ def test_save_pretrained_offload(self):\n \n \n @require_torch_accelerator\n+@unittest.skipIf(\n+    get_device_properties()[0] == \"cuda\"\n+    and (get_device_properties()[1] < 8 or (get_device_properties()[1] == 8 and get_device_properties()[2] < 9)),\n+    \"Skipping FP8LinearTest because it is not supported on GPU with capability < 8.9\",\n+)\n class FP8LinearTest(unittest.TestCase):\n     device = torch_device\n \n-    @unittest.skipIf(\n-        get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 9,\n-        \"Skipping FP8LinearTest because it is not supported on GPU with capability < 9.0\",\n-    )\n     def test_linear_preserves_shape(self):\n         \"\"\"\n         Test that FP8Linear preserves shape when in_features == out_features.\n@@ -270,10 +276,6 @@ def test_linear_preserves_shape(self):\n         x_ = linear(x)\n         self.assertEqual(x_.shape, x.shape)\n \n-    @unittest.skipIf(\n-        get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 9,\n-        \"Skipping FP8LinearTest because it is not supported on GPU with capability < 9.0\",\n-    )\n     def test_linear_with_diff_feature_size_preserves_shape(self):\n         \"\"\"\n         Test that FP8Linear generates the correct shape when in_features != out_features."
        }
    ],
    "stats": {
        "total": 18,
        "additions": 10,
        "deletions": 8
    }
}