{
    "author": "ArthurZucker",
    "message": "ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ default to `\"auto\"` dtype (#34919)\n\n* default to `\"auto\"` dtype\n\n* the actual change?\n\n* up?\n\n* style\n\n* up?\n\n* only sam models were broken with this",
    "sha": "f5aa90d0fbb7b6f9dd07ac534319d973cd29a123",
    "files": [
        {
            "sha": "5fa8f807625550e99667396b39881bbe8bc06e35",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5aa90d0fbb7b6f9dd07ac534319d973cd29a123/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5aa90d0fbb7b6f9dd07ac534319d973cd29a123/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=f5aa90d0fbb7b6f9dd07ac534319d973cd29a123",
            "patch": "@@ -341,6 +341,9 @@ def lazy_load_kernel(kernel_name: str, mapping: dict[str, ModuleType | None] = _\n             mapping[kernel_name] = kernel\n         except FileNotFoundError:\n             mapping[kernel_name] = None\n+        except AssertionError:\n+            # Happens when torch is built without an accelerator backend; fall back to slow path.\n+            mapping[kernel_name] = None\n \n     else:\n         # Try to import is_{kernel_name}_available from ..utils"
        },
        {
            "sha": "e7fc56ffac70e53de397fed88831da29360bb743",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 28,
            "deletions": 7,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5aa90d0fbb7b6f9dd07ac534319d973cd29a123/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5aa90d0fbb7b6f9dd07ac534319d973cd29a123/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=f5aa90d0fbb7b6f9dd07ac534319d973cd29a123",
            "patch": "@@ -268,16 +268,24 @@ def get_torch_context_manager_or_global_device():\n     return device_in_context\n \n \n-def get_state_dict_dtype(state_dict):\n+def get_state_dict_dtype(state_dict, config_dtype: Optional[torch.dtype] = None):\n     \"\"\"\n     Returns the first found floating dtype in `state_dict` if there is one, otherwise returns the first dtype.\n+\n+    If `config_dtype` is provided (for instance when `dtype=\"auto\"` and the config already carries a dtype), it is used.\n     \"\"\"\n+    if config_dtype is not None:\n+        return config_dtype\n+\n+    if len(state_dict) == 0:\n+        return torch.get_default_dtype()\n+\n     for t in state_dict.values():\n         if t.is_floating_point():\n             return t.dtype\n \n     # if no floating dtype was found return whatever the first dtype is\n-    return next(state_dict.values()).dtype\n+    return next(iter(state_dict.values())).dtype\n \n \n str_to_torch_dtype = {\n@@ -722,12 +730,16 @@ def _get_dtype(\n                     if is_sharded and \"dtype\" in sharded_metadata:\n                         dtype = sharded_metadata[\"dtype\"]\n                     elif state_dict is not None:\n-                        dtype = get_state_dict_dtype(state_dict)\n+                        dtype = get_state_dict_dtype(state_dict, getattr(config, \"dtype\", None))\n                     else:\n                         state_dict = load_state_dict(\n                             checkpoint_files[0], map_location=\"meta\", weights_only=weights_only\n                         )\n-                        dtype = get_state_dict_dtype(state_dict)\n+                        dtype = get_state_dict_dtype(state_dict, getattr(config, \"dtype\", None))\n+                    config.dtype = dtype\n+                    for sub_config_key in config.sub_configs:\n+                        if (sub_config := getattr(config, sub_config_key)) is not None:\n+                            sub_config.dtype = dtype\n                     logger.info(\n                         \"Since the `dtype` attribute can't be found in model's config object, \"\n                         \"will use dtype={dtype} as derived from model's weights\"\n@@ -1219,6 +1231,14 @@ def __init__(self, config: PreTrainedConfig, *inputs, **kwargs):\n                 f\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n             )\n         self.config = config\n+        if getattr(self.config, \"dtype\", None) is None:\n+            default_dtype = torch.get_default_dtype()\n+            self.config.dtype = default_dtype\n+            for sub_config_key in self.config.sub_configs:\n+                if (sub_config := getattr(self.config, sub_config_key)) is not None and getattr(\n+                    sub_config, \"dtype\", None\n+                ) is None:\n+                    sub_config.dtype = default_dtype\n \n         # Check the attention implementation is supported, or set it if not yet set (on the internal attr, to avoid\n         # setting it recursively)\n@@ -3789,7 +3809,8 @@ def from_pretrained(\n         output_loading_info = kwargs.pop(\"output_loading_info\", False)\n         from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n         from_auto_class = kwargs.pop(\"_from_auto\", False)\n-        dtype = kwargs.pop(\"dtype\", None)\n+        dtype_kwarg_provided = \"dtype\" in kwargs\n+        dtype = kwargs.pop(\"dtype\", \"auto\")\n         torch_dtype = kwargs.pop(\"torch_dtype\", None)  # kept for BC\n         device_map = kwargs.pop(\"device_map\", None)\n         max_memory = kwargs.pop(\"max_memory\", None)\n@@ -3820,8 +3841,8 @@ def from_pretrained(\n             _ = kwargs.pop(name, None)\n \n         # For BC on torch_dtype argument\n-        if torch_dtype is not None:\n-            dtype = dtype if dtype is not None else torch_dtype\n+        if torch_dtype is not None and (not dtype_kwarg_provided or dtype is None):\n+            dtype = torch_dtype\n \n         if is_offline_mode() and not local_files_only:\n             local_files_only = True"
        }
    ],
    "stats": {
        "total": 38,
        "additions": 31,
        "deletions": 7
    }
}