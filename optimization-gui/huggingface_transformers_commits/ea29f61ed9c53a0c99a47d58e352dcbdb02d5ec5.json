{
    "author": "techkang",
    "message": "fix bug in distributed loss test (#38166)\n\n* fix bug in distributed loss test and change some config to pass at both 2&8 gpus\n\n* fix doc",
    "sha": "ea29f61ed9c53a0c99a47d58e352dcbdb02d5ec5",
    "files": [
        {
            "sha": "0ca6f6af6e5a76633e34692aeb887b5bde22b4f1",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea29f61ed9c53a0c99a47d58e352dcbdb02d5ec5/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea29f61ed9c53a0c99a47d58e352dcbdb02d5ec5/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=ea29f61ed9c53a0c99a47d58e352dcbdb02d5ec5",
            "patch": "@@ -3784,7 +3784,7 @@ def training_step(\n             with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                 scaled_loss.backward()\n         else:\n-            # Finally we need to normalize the loss for reporting\n+            # Finally we need to normalize the loss for reporting if GA loss bug is not fixed during compute loss\n             if not self.model_accepts_loss_kwargs and self.compute_loss_func is None:\n                 loss = loss / self.args.gradient_accumulation_steps\n "
        },
        {
            "sha": "405763125ecbd926cc2cf0541578d2ddf06f2bdb",
            "filename": "tests/trainer/test_trainer_distributed_loss.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea29f61ed9c53a0c99a47d58e352dcbdb02d5ec5/tests%2Ftrainer%2Ftest_trainer_distributed_loss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea29f61ed9c53a0c99a47d58e352dcbdb02d5ec5/tests%2Ftrainer%2Ftest_trainer_distributed_loss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_distributed_loss.py?ref=ea29f61ed9c53a0c99a47d58e352dcbdb02d5ec5",
            "patch": "@@ -26,7 +26,7 @@ class TestTrainerDistributedLoss(TestCasePlus):\n     @require_torch_multi_accelerator\n     def test_trainer(self):\n         device_count = backend_device_count(torch_device)\n-        min_bs = 1\n+        min_bs = 2\n         output_dir = self.get_auto_remove_tmp_dir()\n         for gpu_num, enable, bs, name in (\n             (1, True, min_bs * device_count, \"base\"),\n@@ -50,9 +50,10 @@ def test_trainer(self):\n         broken_diff = [abs(base_loss[i] - broken_loss[i]) for i in range(len(base_loss))]\n         fixed_diff = [abs(base_loss[i] - fixed_loss[i]) for i in range(len(base_loss))]\n         sum_base = sum(base_loss)\n-        sum_broken = sum(broken_diff)\n+        sum_broken = sum(broken_loss)\n         relative_broken = abs(sum_base - sum_broken) / max(sum_base, sum_broken)\n \n+        # the gap may be smaller for other models, but it still ok.\n         self.assertGreater(max(broken_diff), 0.5)\n         self.assertLess(max(fixed_diff), 0.005)\n         self.assertLess(relative_broken, 0.1)\n@@ -63,7 +64,7 @@ def run_distributed_training(training_args):\n     model_name = \"nickypro/tinyllama-15M\"\n     dataset_name = \"wikitext\"\n     dataset_config = \"wikitext-2-raw-v1\"\n-    dataset = datasets.load_dataset(dataset_name, dataset_config, split=\"train[:17]\")\n+    dataset = datasets.load_dataset(dataset_name, dataset_config, split=\"train[:100]\")\n     tokenizer = AutoTokenizer.from_pretrained(model_name)\n     tokenizer.pad_token = tokenizer.eos_token\n "
        }
    ],
    "stats": {
        "total": 9,
        "additions": 5,
        "deletions": 4
    }
}