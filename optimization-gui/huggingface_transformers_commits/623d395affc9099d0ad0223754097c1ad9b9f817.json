{
    "author": "alex-jw-brooks",
    "message": "Add Granite Speech Support (#36801)\n\n* First pass at speech granite\n\nAdd encoder / projector, rename things\n\n* Combine into one model file with causal lm outputs for forward\n\n* Add loss calc\n\n* Fix config loading\n\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\n\n* Split new / old loading logic\n\n* Use transformers integration for loading peft adapters\n\n* Add generation wrapper for selective lora enablement\n\n* Add note for qformer encoder automodel\n\n* Guard torch/audio imports in feature extractor\n\n* Handle granite speech autoclasses\n\n* Handle optional deps in package structure for granite speech\n\n* Add granite pretrained model def for init\n\n* Add dummy objects for torch/torchaudio\n\n* Add tests for granite speech processor\n\n* Minor formatting fixes and refactoring\n\n* Add options for falling back to config in forward\n\n* Tentative model docstrings for granite speech\n\n* Fix config type\n\n* Remove legacy load\n\n* Allow non-lora variants for granite speech\n\n* Override weight tying for llm\n\n* Use text config instead of llm config\n\n* Add output embeddings getter to fix weight tying\n\n* Fix relative imports\n\n* computing the number of audio features, based on the raw audio sequence.\n\n* collating audio inputs, and keeping the original lengths.\n\n* asserted we have text. otherwise we can't specify the audio special token.\n\n* assering the number of audio-symbols/audios match correctly.\nrunning get validated_audios only when audio is present\n\n* indentation bugfix + supporting different feature lengths when expanding audio.\n\n* redundant, done in _get_validated_text\n\n* adapting the tests:\n- we must have text (not either audio or text)\n- _get_num_audio_features takes a list of raw lengths, provided it insetad.\n\n* Minor cleanup, remove unused import\n\n* Add more tests for batch feature processing\n\n* Allow setting offset in rel position embeddings\n\n* Add config option for warning if peft is not installed w/ lora\n\n* Port blip2 qformer code into granite speech\n\n* Add sad test for numpy arr processing\n\n* Allow numpy arrays / tuples in granite speech processor\n\n* Fix config type for projector\n\n* - pad instead of creating a zeros tensor, to keep the original dtype/device (support bfloat16)\n- cast input_features to the model dtype (support bfloat16)\n\n* merge Blip2QFormerConfig to GraniteSpeechProjectorConfig\n\n* prevent a crash when re-saving/loading the model (line 109)\n\n* consider additional edge cases during preprocessing.\n\n* consider additional edge cases during preprocessing.\n\n* add features mask for batched inference (bugfix)\n\n* Minor refactor, remove multiaudio processor tests\n\n* Add set input/output embeddings for granite speech\n\n* Fix feature dim check in processor test\n\n* Pop input features in embed test for granite speech\n\n* Small fixes for test edge cases\n\nAdd granite speech to seq2seq causal lm mapping names\n\n* Add small tests for granite speech model\n\n* Fix data parallelism test\n\n* Standardize model class names\n\n* Fix check for copies\n\n* Fix misaligned init check\n\n* Skip granite speech in checkpoint check\n\n* Use default for tie_word_embeddings in granite speech\n\n* Fix non documentation granite speech repo issues\n\n* Fix comments and docstring checks\n\n* Add placeholder docs for granite speech\n\n* Fix test naming collision\n\n* Code formatting\n\n* Rerun torch dummy obj regen\n\n* Fix save pretrained for granite speech\n\n* Import sorting\n\n* Fix tests typo\n\n* Remove offset hack\n\n* Pass args through encoder config\n\n* Remove unused prune heads from blip2\n\n* removing einsum. replaced with explicit multiplication (relative positional encodings) and sdpa attention.\n\n* remove Sequential from ConformerFeedForward and ConformerConvModule. + fix for sdpa attention\n\n* remove GraniteSpeechConformerScale\n\n* rename to hidden_states\n\n* rename conformer layers to self.layers, remove the first linear from the list to keep the list homogenous.\n\n* move pre-norm to the attention/feedforward blocks (avoid complex module wrapping)\n\n* adding pre_norm into forward\n\n* feature extractor refactoring to resemble how it's done in phi4multimodal.\n\n* rename feature_extractor to audio_processor\n\n* bugfix: input_feature_mask fix to get the exact number tokens.\n\n* Fix pytest decorator in processor test\n\n* Add (disabled) integration tests for granite speech\n\n* Fix handling of optional feature masking\n\n* Loosen validation in processing for vLLM compatability\n\n* Formatting fixes\n\n* Update init structure to mirror llama\n\n* Make granite speech projector generic\n\n* Update test config to reflect generic projector\n\n* Formatting fixes\n\n* Fix typos, add license\n\n* Fix undefined var in input processing\n\n* Cleanup and expose ctc encoder\n\n* Add missing config docstrings\n\n* Better var names, type hints, etc\n\n* Set attn context size in init\n\n* Add max pos emb to encoder config\n\n* Cleanup feature extractor\n\n* Add granite speech architecture details\n\n* Remove granite speech qformer ref\n\n* Add paper link, explicit calc for qkv\n\n* Calculate padding directly in depthwise conv1d init\n\n* Raise value error instead of asserting\n\n* Reorder class defs (classes used at top)\n\n* Precompute relpos distances\n\n* Run formatting\n\n* Pass attention distances through forward\n\n* Apply suggestions from code review\n\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>\n\n* Add todo for using common batch feature extraction\n\n* Rename audios/features\n\n* Ensure chat template may be provided to processor\n\n* Move granite speech docs to audio models\n\n* Add todos for input proc refactoring\n\n* Fix import order\n\n* Guard torch import\n\n* Use relative imports\n\n* Require torch backend for processor in granite speech\n\n* Add backend guards in feature extractor\n\n---------\n\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\nCo-authored-by: Avihu Dekel <avihu.dekel@ibm.com>\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>",
    "sha": "623d395affc9099d0ad0223754097c1ad9b9f817",
    "files": [
        {
            "sha": "fa1aa7491c2d19d4bf5a2fa7c1927bea08ca0a12",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -823,6 +823,8 @@\n         title: EnCodec\n       - local: model_doc/fastspeech2_conformer\n         title: FastSpeech2Conformer\n+      - local: model_doc/granite_speech\n+        title: GraniteSpeech\n       - local: model_doc/hubert\n         title: Hubert\n       - local: model_doc/mctct"
        },
        {
            "sha": "212c3d1499354cccf3d313063cea643878daaa66",
            "filename": "docs/source/en/model_doc/granite_speech.md",
            "status": "added",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -0,0 +1,68 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Granite Speech\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+## Overview\n+The Granite Speech model is a multimodal language model, consisting of a speech encoder, speech projector, large language model, and LoRA adapter(s). More details regarding each component for the current (Granite 3.2 Speech) model architecture may be found below.\n+\n+1. Speech Encoder: A [Conformer](https://arxiv.org/abs/2005.08100) encoder trained with Connectionist Temporal Classification (CTC) on character-level targets on ASR corpora. The encoder uses block-attention and self-conditioned CTC from the middle layer.\n+\n+2. Speech Projector: A query transformer (q-former) operating on the outputs of the last encoder block. The encoder and projector temporally downsample the audio features to be merged into the multimodal embeddings to be processed by the llm.\n+\n+3. Large Language Model: The Granite Speech model leverages Granite LLMs, which were originally proposed in [this paper](https://arxiv.org/abs/2408.13359).\n+\n+4. LoRA adapter(s): The Granite Speech model contains a modality specific LoRA, which will be enabled when audio features are provided, and disabled otherwise.\n+\n+\n+Note that most of the aforementioned components are implemented generically to enable compatability and potential integration with other model architectures in transformers.\n+\n+\n+This model was contributed by [Alexander Brooks](https://huggingface.co/abrooks9944), [Avihu Dekel](https://huggingface.co/Avihu), and [George Saon](https://huggingface.co/gsaon).\n+\n+## Usage tips\n+- This model bundles its own LoRA adapter, which will be automatically loaded and enabled/disabled as needed during inference calls. Be sure to install [PEFT](https://github.com/huggingface/peft) to ensure the LoRA is correctly applied!\n+\n+<!-- TODO (@alex-jw-brooks) Add an example here once the model compatible with the transformers implementation is released -->\n+\n+## GraniteSpeechConfig\n+\n+[[autodoc]] GraniteSpeechConfig\n+\n+\n+## GraniteSpeechEncoderConfig\n+\n+[[autodoc]] GraniteSpeechEncoderConfig\n+\n+\n+## GraniteSpeechProcessor\n+\n+[[autodoc]] GraniteSpeechProcessor\n+\n+\n+## GraniteSpeechFeatureExtractor\n+\n+[[autodoc]] GraniteSpeechFeatureExtractor\n+\n+\n+## GraniteSpeechForConditionalGeneration\n+\n+[[autodoc]] GraniteSpeechForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "43eebbcb3a5a3007176b3226ffcf16639c01e5fd",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -125,6 +125,7 @@\n     from .gpt_sw3 import *\n     from .gptj import *\n     from .granite import *\n+    from .granite_speech import *\n     from .granitemoe import *\n     from .granitemoeshared import *\n     from .grounding_dino import *"
        },
        {
            "sha": "0d6e02606634b7a14835a73d12dcd6a598756744",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -142,6 +142,7 @@\n         (\"gptj\", \"GPTJConfig\"),\n         (\"gptsan-japanese\", \"GPTSanJapaneseConfig\"),\n         (\"granite\", \"GraniteConfig\"),\n+        (\"granite_speech\", \"GraniteSpeechConfig\"),\n         (\"granitemoe\", \"GraniteMoeConfig\"),\n         (\"granitemoeshared\", \"GraniteMoeSharedConfig\"),\n         (\"granitevision\", \"LlavaNextConfig\"),\n@@ -491,6 +492,7 @@\n         (\"gptj\", \"GPT-J\"),\n         (\"gptsan-japanese\", \"GPTSAN-japanese\"),\n         (\"granite\", \"Granite\"),\n+        (\"granite_speech\", \"GraniteSpeech\"),\n         (\"granitemoe\", \"GraniteMoeMoe\"),\n         (\"granitemoeshared\", \"GraniteMoeSharedMoe\"),\n         (\"granitevision\", \"LLaVA-NeXT\"),"
        },
        {
            "sha": "86dc8703c426ab9d953bccfa622b45d5d346b9d7",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -61,6 +61,7 @@\n         (\"encodec\", \"EncodecFeatureExtractor\"),\n         (\"flava\", \"FlavaFeatureExtractor\"),\n         (\"glpn\", \"GLPNFeatureExtractor\"),\n+        (\"granite_speech\", \"GraniteSpeechFeatureExtractor\"),\n         (\"groupvit\", \"CLIPFeatureExtractor\"),\n         (\"hubert\", \"Wav2Vec2FeatureExtractor\"),\n         (\"imagegpt\", \"ImageGPTFeatureExtractor\"),"
        },
        {
            "sha": "3f2fb425932dd501b9c3f2774b4d921a707ce9c3",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -973,6 +973,7 @@\n         (\"encoder-decoder\", \"EncoderDecoderModel\"),\n         (\"fsmt\", \"FSMTForConditionalGeneration\"),\n         (\"gptsan-japanese\", \"GPTSanJapaneseForConditionalGeneration\"),\n+        (\"granite_speech\", \"GraniteSpeechForConditionalGeneration\"),\n         (\"led\", \"LEDForConditionalGeneration\"),\n         (\"longt5\", \"LongT5ForConditionalGeneration\"),\n         (\"m2m_100\", \"M2M100ForConditionalGeneration\"),\n@@ -997,6 +998,7 @@\n \n MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES = OrderedDict(\n     [\n+        (\"granite_speech\", \"GraniteSpeechForConditionalGeneration\"),\n         (\"moonshine\", \"MoonshineForConditionalGeneration\"),\n         (\"pop2piano\", \"Pop2PianoForConditionalGeneration\"),\n         (\"seamless_m4t\", \"SeamlessM4TForSpeechToText\"),"
        },
        {
            "sha": "864634965afe3d5846f2f3f7c21eff8f141f620f",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -66,6 +66,7 @@\n         (\"gemma3\", \"Gemma3Processor\"),\n         (\"git\", \"GitProcessor\"),\n         (\"got_ocr2\", \"GotOcr2Processor\"),\n+        (\"granite_speech\", \"GraniteSpeechProcessor\"),\n         (\"grounding-dino\", \"GroundingDinoProcessor\"),\n         (\"groupvit\", \"CLIPProcessor\"),\n         (\"hubert\", \"Wav2Vec2Processor\"),"
        },
        {
            "sha": "d6122581855250911b0bf951a42e22d9c354240a",
            "filename": "src/transformers/models/granite_speech/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fgranite_speech%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fgranite_speech%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2F__init__.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_granite_speech import *\n+    from .feature_extraction_granite_speech import *\n+    from .modeling_granite_speech import *\n+    from .processing_granite_speech import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e1355db41fdc6f335ade28152e0adb18e06b43f8",
            "filename": "src/transformers/models/granite_speech/configuration_granite_speech.py",
            "status": "added",
            "additions": 197,
            "deletions": 0,
            "changes": 197,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fconfiguration_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fconfiguration_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fconfiguration_granite_speech.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -0,0 +1,197 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Config class for Granite Speech.\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class GraniteSpeechEncoderConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`GraniteSpeechCTCEncoder`]. It is used to instantiate\n+    a Granite Speech audio encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the dfefaults will yield a similar configuration to that of the audio encoder of the Granite Speech\n+    architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        input_dim (`int`, *optional*, defaults to 160):\n+            Dimension of the first hidden layer of the encoder.\n+        num_layers (`int`, *optional*, defaults to 10):\n+            Number of encoder blocks.\n+        hidden_dim (`int`, *optional*, defaults to 1024):\n+            The size of the intermediate layers in the conformer encoder.\n+        feedforward_mult (`int`, *optional*, defaults to 4):\n+            Multiplier for the up/down projections in the encoder's feedforward layers;\n+            The projections will have intermediate dim of size `hidden_dim * feedforward_mult`.\n+        num_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        dim_head (`int`, *optional*, defaults to 128):\n+            Dimension of attention heads for each attention layer in the Transformer encoder.\n+        output_dim (`int`, *optional*, defaults to 42):\n+            Intermediate dimension of the feedforward projections in the conformer\n+            to be added to every other encoder block's output.\n+        context_size (`int`, *optional*, defaults to 200):\n+            Context size to be used in conformer attention.\n+        max_pos_emb (`int`, *optional*, defaults to 512):\n+            Max pos embeds to be used in attention (shaw's relative positional encoding).\n+        dropout (`float`, *optional*, defaults to 0.1):\n+            The dropout probability for fully connected layers in the encoder.\n+        conv_kernel_size (`int`, *optional*, defaults to 15):\n+            Kernel size to be used for 1D convolution in each conformer block.\n+        conv_expansion_factor (`int`, *optional*, defaults to 2):\n+            Intermediate dimension to be used in conformer convolutions.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import GraniteSpeechEncoderConfig, GraniteSpeechCTCEncoder\n+\n+    >>> # Initializing a GraniteSpeechEncoderConfig\n+    >>> configuration = GraniteSpeechEncoderConfig()\n+\n+    >>> # Initializing a GraniteSpeechCTCEncoder (with random weights)\n+    >>> model = GraniteSpeechCTCEncoder(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"granite_speech_encoder\"\n+\n+    def __init__(\n+        self,\n+        input_dim=160,\n+        num_layers=10,\n+        hidden_dim=1024,\n+        feedforward_mult=4,\n+        num_heads=8,\n+        dim_head=128,\n+        output_dim=42,\n+        context_size=200,\n+        max_pos_emb=512,\n+        dropout=0.1,\n+        conv_kernel_size=15,\n+        conv_expansion_factor=2,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.input_dim = input_dim\n+        self.num_layers = num_layers\n+        self.hidden_dim = hidden_dim\n+        self.feedforward_mult = feedforward_mult\n+        self.num_heads = num_heads\n+        self.dim_head = dim_head\n+        self.output_dim = output_dim\n+        self.context_size = context_size\n+        self.dropout = dropout\n+        self.conv_kernel_size = conv_kernel_size\n+        self.conv_expansion_factor = conv_expansion_factor\n+        self.max_pos_emb = max_pos_emb\n+\n+\n+class GraniteSpeechConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`GraniteSpeechForConditionalGeneration`]. It is used to instantiate an\n+    Granite Speech model according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `GraniteConfig`):\n+            The config object or dictionary of the text backbone.\n+        encoder_config (`GraniteSpeechEncoderConfig`, *optional*):\n+            The config object or dictionary of the Granite Speech CTC Encoder.\n+        projector_config (`Union[AutoConfig, dict]`, *optional*, defaults to `Blip2QFormerConfig`):\n+            The config object or dictionary of the audio projector.\n+        audio_token_index (`int`, *optional*, defaults to 49155):\n+            The audio token index to encode the audio prompt.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        has_lora_adapter (`bool`, *optional*, defaults to `True`):\n+            Indicates whether or not the model has a lora adapter that should only\n+            be activate when processing audio inputs.\n+        downsample_rate (`int`, *optional*, defaults to 5):\n+            Downsample rate for the audio feature extractor.\n+        window_size (`int`, *optional*, defaults to 15):\n+            Window size for the audio feature projector.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import GraniteSpeechConfig, GraniteSpeechForConditionalGeneration\n+\n+    >>> # Initializing a GraniteSpeechConfig\n+    >>> configuration = GraniteSpeechConfig()\n+\n+    >>> # Initializing a GraniteSpeechForConditionalGeneration (with random weights)\n+    >>> model = GraniteSpeechForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"granite_speech\"\n+    sub_configs = {\n+        \"text_config\": AutoConfig,\n+        \"encoder_config\": GraniteSpeechEncoderConfig,\n+        \"projector_config\": AutoConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        text_config=None,\n+        encoder_config=None,\n+        projector_config=None,\n+        audio_token_index=49155,\n+        initializer_range=0.02,\n+        has_lora_adapter=True,\n+        downsample_rate=5,\n+        window_size=15,\n+        **kwargs,\n+    ):\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"granite\"\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"granite\"]()\n+\n+        if isinstance(projector_config, dict):\n+            projector_config[\"model_type\"] = (\n+                projector_config[\"model_type\"] if \"model_type\" in projector_config else \"blip_2_qformer\"\n+            )\n+            projector_config = CONFIG_MAPPING[projector_config[\"model_type\"]](**projector_config)\n+        elif projector_config is None:\n+            projector_config = CONFIG_MAPPING[\"blip_2_qformer\"]()\n+\n+        if not isinstance(encoder_config, GraniteSpeechEncoderConfig):\n+            encoder_config = {} if encoder_config is None else encoder_config\n+            encoder_config = GraniteSpeechEncoderConfig(**encoder_config)\n+\n+        self.text_config = text_config\n+        self.encoder_config = encoder_config\n+        self.projector_config = projector_config\n+        self.audio_token_index = audio_token_index\n+        self.initializer_range = initializer_range\n+        self.has_lora_adapter = has_lora_adapter\n+        self.downsample_rate = downsample_rate\n+        self.window_size = window_size\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"GraniteSpeechEncoderConfig\", \"GraniteSpeechConfig\"]"
        },
        {
            "sha": "14b4bb10c4330832196f2606c5169002ee2fd572",
            "filename": "src/transformers/models/granite_speech/feature_extraction_granite_speech.py",
            "status": "added",
            "additions": 208,
            "deletions": 0,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Ffeature_extraction_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Ffeature_extraction_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Ffeature_extraction_granite_speech.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -0,0 +1,208 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Feature extractor class for Granite Speech.\"\"\"\n+\n+import math\n+from collections.abc import Sequence\n+from typing import Optional\n+\n+import numpy as np\n+\n+from ...feature_extraction_utils import BatchFeature, FeatureExtractionMixin\n+from ...tokenization_utils_base import AudioInput\n+from ...utils import is_torch_available, is_torchaudio_available, logging\n+from ...utils.import_utils import requires_backends\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchaudio_available():\n+    import torchaudio\n+\n+\n+class GraniteSpeechFeatureExtractor(FeatureExtractionMixin):\n+    model_input_names = [\"input_features\"]\n+\n+    def __init__(\n+        self,\n+        sampling_rate: int = 16000,\n+        n_fft: int = 512,\n+        win_length: int = 400,\n+        hop_length: int = 160,\n+        n_mels: int = 80,\n+        projector_window_size: int = 15,\n+        projector_downsample_rate: int = 5,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.melspec_kwargs = {\n+            \"sample_rate\": sampling_rate,\n+            \"n_fft\": n_fft,\n+            \"win_length\": win_length,\n+            \"hop_length\": hop_length,\n+            \"n_mels\": n_mels,\n+        }\n+        # Currently lazily initialized\n+        self.melspec = None\n+        self.projector_window_size = projector_window_size\n+        self.projector_downsample_rate = projector_downsample_rate\n+\n+    def __call__(\n+        self,\n+        audios: AudioInput,\n+        device: Optional[str] = \"cpu\",\n+    ) -> BatchFeature:\n+        requires_backends(self, [\"torchaudio\"])\n+\n+        speech_inputs = {}\n+        batched_audio, audio_lengths = self._get_audios_and_audio_lengths(audios)\n+        speech_inputs[\"input_features\"] = self._extract_mel_spectrograms(\n+            batched_audio,\n+            device=device,\n+        )\n+        audio_embed_sizes = self._get_num_audio_features(audio_lengths)\n+        speech_inputs[\"audio_embed_sizes\"] = audio_embed_sizes\n+        # TODO (@alex-jw-brooks): Currently input_features_mask is not\n+        # a great name, because input_features and input_features_mask\n+        # have different shapes (before/after the projector).\n+        #\n+        # We should align this with other multimodal models, e.g,. llava\n+        # and qwen2audio and refactor this to ensure input_feature_mask\n+        # has the same dimensionality as input_features, or compute it in\n+        # the model based on the audio embedding sizes (since we do not\n+        # have an attention mask for the audio features to infer padding from).\n+        speech_inputs[\"input_features_mask\"] = torch.arange(max(audio_embed_sizes)).view(1, -1) < torch.tensor(\n+            audio_embed_sizes\n+        ).view(-1, 1)\n+        return BatchFeature(data=speech_inputs)\n+\n+    def _ensure_melspec_transform_is_initialized(self):\n+        \"\"\"\n+        Ensures the mel spectrogram transform on this instance is initialized.\n+\n+        We do this for now since some logging explodes since the mel spectrogram\n+        transform is not JSON serializable.\n+        \"\"\"\n+        requires_backends(self, [\"torchaudio\"])\n+\n+        if self.melspec is None:\n+            # TODO (@alex-jw-brooks / @eustlb) move this to common batch\n+            # feature extraction in audio utils once they are written!\n+            self.melspec = torchaudio.transforms.MelSpectrogram(**self.melspec_kwargs)\n+\n+    def _extract_mel_spectrograms(self, audio: \"torch.Tensor\", device=\"cpu\"):\n+        \"\"\"\n+        Compute the Mel features to be passed to the conformer encoder.\n+        \"\"\"\n+        requires_backends(self, [\"torchaudio\"])\n+\n+        # Initialize the mel spectrogram if isn't not already and\n+        # move the melspec / audio to the computation device.\n+        self._ensure_melspec_transform_is_initialized()\n+        if device is not None:\n+            melspec = self.melspec.to(device)\n+            audio = audio.to(device)\n+        else:\n+            melspec = self.melspec\n+\n+        bsz = audio.shape[0]\n+        with torch.no_grad():\n+            # Compute mel features\n+            mel = melspec(audio.float())\n+            logmel = mel.transpose(-1, -2).clip_(min=1e-10).log10_()\n+            mx = logmel.amax(dim=(-2, -1), keepdim=True)\n+            logmel = torch.maximum(logmel, mx - 8.0).div_(4).add_(1)\n+            # remove last frame if odd\n+            if logmel.shape[1] % 2 == 1:\n+                logmel = logmel[:, :-1]\n+\n+            # stacking and skipping by 2\n+            audio = logmel.reshape(bsz, -1, 2 * logmel.shape[-1])\n+\n+        if audio.device != \"cpu\":\n+            return audio.detach().cpu()\n+        return audio\n+\n+    def _get_num_audio_features(self, audio_lengths: Sequence[int]) -> Sequence[int]:\n+        \"\"\"\n+        Gets the (variable length) number of features (i.e., projector output) for the sequences\n+        being considered.\n+\n+        Args:\n+            audio_lengths (`Sequence[int]`):\n+                Sequence of one or more raw audio lengths.\n+        \"\"\"\n+        hop_length = self.melspec_kwargs[\"hop_length\"]\n+        effective_window_size = self.projector_window_size // self.projector_downsample_rate\n+\n+        projector_lengths = []\n+        for raw_length in audio_lengths:\n+            # mel sequence length computation\n+            mel_length = raw_length // hop_length + 1\n+            # encoder frame takes two mel features\n+            encoder_length = mel_length // 2\n+            nblocks = math.ceil(encoder_length / self.projector_window_size)\n+            # projector output length\n+            projector_length = nblocks * effective_window_size\n+            projector_lengths.append(projector_length)\n+\n+        return projector_lengths\n+\n+    def _get_audios_and_audio_lengths(self, audios: AudioInput) -> Sequence[\"torch.Tensor\", Sequence[int]]:\n+        \"\"\"\n+        Coerces audio inputs to torch tensors and extracts audio lengths prior to stacking.\n+\n+        Args:\n+            audios (`AudioInput`):\n+                Audio sequence, numpy array, or torch tensor.\n+        \"\"\"\n+        requires_backends(self, [\"torch\"])\n+\n+        # Coerce to PyTorch tensors if we have numpy arrays, since\n+        # currently we have a dependency on torch/torchaudio anyway\n+        if isinstance(audios, np.ndarray):\n+            audios = torch.from_numpy(audios)\n+        elif isinstance(audios, Sequence) and isinstance(audios[0], np.ndarray):\n+            audios = [torch.from_numpy(arr) for arr in audios]\n+\n+        if isinstance(audios, torch.Tensor):\n+            if audios.ndim == 1:\n+                audios = audios.unsqueeze(0)\n+            if not torch.is_floating_point(audios):\n+                raise ValueError(\"Invalid audio provided. Audio should be a floating point between 0 and 1\")\n+\n+            if audios.shape[0] > 1:\n+                logger.warning(\"Audio samples are already collated; assuming they all have the same length\")\n+            lengths = [audios.shape[-1]] * audios.shape[0]\n+            return audios, lengths\n+\n+        elif isinstance(audios, Sequence) and isinstance(audios[0], torch.Tensor):\n+            if not torch.is_floating_point(audios[0]):\n+                raise ValueError(\"Invalid audio provided. Audio should be a floating point between 0 and 1\")\n+            lengths = [audio.shape[-1] for audio in audios]\n+            padding = [max(lengths) - length for length in lengths]\n+            # ensure all audios have a batch dimension:\n+            audios = [audio.view(1, -1) for audio in audios]\n+            padded = [torch.nn.functional.pad(audio, (0, pad)) for audio, pad in zip(audios, padding)]\n+            audios = torch.cat(padded, dim=0)\n+            return audios, lengths\n+\n+        raise TypeError(\"Invalid audio provided. Audio should be a one or more torch tensors or numpy arrays\")\n+\n+\n+__all__ = [\"GraniteSpeechFeatureExtractor\"]"
        },
        {
            "sha": "821539d4163985bfcdaee524e213862a63be5c20",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "added",
            "additions": 673,
            "deletions": 0,
            "changes": 673,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -0,0 +1,673 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from dataclasses import dataclass\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import ModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_peft_available,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ..auto import AutoModel, AutoModelForCausalLM\n+from .configuration_granite_speech import (\n+    GraniteSpeechConfig,\n+    GraniteSpeechEncoderConfig,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CONFIG_FOR_DOC = \"GraniteSpeechConfig\"\n+\n+\n+@dataclass\n+class GraniteSpeechCausalLMOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for LlavaNext causal language model (or autoregressive) outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: torch.FloatTensor = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+### Projector\n+class GraniteSpeechEncoderProjector(nn.Module):\n+    def __init__(self, config: GraniteSpeechConfig):\n+        super().__init__()\n+        self.hidden_size = config.projector_config.hidden_size\n+        self.downsample_rate = config.downsample_rate\n+        self.window_size = config.window_size\n+        self.num_queries = config.window_size // config.downsample_rate\n+\n+        self.query = nn.Parameter(torch.zeros(1, self.num_queries, config.projector_config.hidden_size))\n+        self.query.data.normal_(mean=0.0, std=1.0)\n+\n+        # By default, this will be a blip_2_qformer config\n+        self.qformer = AutoModel.from_config(config.projector_config)\n+        self.linear = nn.Linear(config.projector_config.hidden_size, config.text_config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, seq_len, dim = hidden_states.size()\n+        nblocks = math.ceil(seq_len / self.window_size)\n+        pad = nblocks * self.window_size - seq_len\n+        hidden_states = nn.functional.pad(hidden_states, (0, 0, 0, pad), \"constant\", 0)\n+        hidden_states = hidden_states.view(batch_size * nblocks, self.window_size, dim)\n+\n+        query_output = self.qformer(\n+            query_embeds=self.query.data,\n+            encoder_hidden_states=hidden_states,\n+            encoder_attention_mask=None,\n+            return_dict=True,\n+        )\n+        query_proj = self.linear(\n+            query_output.last_hidden_state.view(batch_size, nblocks * self.window_size // self.downsample_rate, -1)\n+        )\n+        return query_proj\n+\n+\n+### Encoder - conformer is adapted from: https://github.com/lucidrains/conformer.git\n+class GraniteSpeechConformerFeedForward(nn.Module):\n+    \"\"\"Feedforward module for conformer encoder blocks.\"\"\"\n+\n+    def __init__(self, config: GraniteSpeechEncoderConfig):\n+        super().__init__()\n+        self.pre_norm = nn.LayerNorm(config.hidden_dim)\n+        self.up_proj = nn.Linear(config.hidden_dim, config.hidden_dim * config.feedforward_mult)\n+        self.silu = nn.SiLU()\n+        self.dropout = nn.Dropout(config.dropout)\n+        self.down_proj = nn.Linear(config.hidden_dim * config.feedforward_mult, config.hidden_dim)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.pre_norm(hidden_states)\n+        hidden_states = self.up_proj(hidden_states)\n+        hidden_states = self.dropout(self.silu(hidden_states))\n+        hidden_states = self.down_proj(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        return hidden_states\n+\n+\n+class GraniteSpeechConformerAttention(nn.Module):\n+    \"\"\"Attention for conformer blocks using Shaw's relative positional embeddings.\n+    See the following [paper](https://arxiv.org/pdf/1803.02155) for more details.\n+    \"\"\"\n+\n+    def __init__(self, config: GraniteSpeechEncoderConfig):\n+        super().__init__()\n+\n+        inner_dim = config.dim_head * config.num_heads\n+        self.max_pos_emb = config.max_pos_emb\n+        self.context_size = config.context_size\n+        self.num_heads = config.num_heads\n+        self.dim_head = config.dim_head\n+        self.scale = self.dim_head**-0.5\n+        self.pre_norm = nn.LayerNorm(config.hidden_dim)\n+        self.to_q = nn.Linear(config.hidden_dim, inner_dim, bias=False)\n+        self.to_kv = nn.Linear(config.hidden_dim, inner_dim * 2, bias=False)\n+        self.to_out = nn.Linear(inner_dim, config.hidden_dim)\n+        self.rel_pos_emb = nn.Embedding(2 * self.max_pos_emb + 1, self.dim_head)\n+        self.dropout = nn.Dropout(config.dropout)\n+\n+        if self.context_size <= 0 or self.context_size > self.max_pos_emb:\n+            raise ValueError(\"Context size is either less than 0 or exceeds the max_pos_emb\")\n+\n+    def forward(self, hidden_states: torch.Tensor, attention_dists: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.pre_norm(hidden_states)\n+        bsz, num_features, _ = hidden_states.shape\n+\n+        num_blocks = math.ceil(num_features / self.context_size)\n+        remainder = num_features % self.context_size\n+        if remainder > 0:\n+            # right padding to reach block size\n+            hidden_states = torch.nn.functional.pad(hidden_states, (0, 0, 0, self.context_size - remainder))\n+\n+        query_states = self.to_q(hidden_states)\n+        key_states, value_states = self.to_kv(hidden_states).chunk(2, dim=-1)\n+\n+        query_states = query_states.reshape(bsz, num_blocks, self.context_size, self.num_heads, -1).transpose(2, 3)\n+        key_states = key_states.reshape(bsz, num_blocks, self.context_size, self.num_heads, -1).transpose(2, 3)\n+        value_states = value_states.reshape(bsz, num_blocks, self.context_size, self.num_heads, -1).transpose(2, 3)\n+\n+        # shaw's relative positional embedding\n+        dist = attention_dists.to(hidden_states.device)\n+        rel_pos_emb = self.rel_pos_emb(dist)\n+        rel_pos_emb_expanded = rel_pos_emb.view([1, 1, 1] + list(rel_pos_emb.shape))\n+        pos_attn = torch.sum(query_states.unsqueeze(-2) * rel_pos_emb_expanded, dim=-1) * self.scale\n+\n+        if remainder > 0:\n+            # masked attention in the extended block\n+            mask = torch.ones(self.context_size, self.context_size, dtype=bool, device=hidden_states.device)\n+            mask[:remainder, :remainder] = 0\n+            mask_value = -torch.finfo(pos_attn.dtype).max\n+            pos_attn[:, -1, :].masked_fill_(mask, mask_value)\n+\n+        with torch.nn.attention.sdpa_kernel(torch.nn.attention.SDPBackend.MATH):\n+            out = F.scaled_dot_product_attention(\n+                query_states, key_states, value_states, attn_mask=pos_attn, scale=self.scale\n+            )\n+        out = out.transpose(2, 3).reshape(bsz, hidden_states.shape[1], -1)\n+        out = self.to_out(out[:, :num_features, :])\n+        return self.dropout(out)\n+\n+\n+class GraniteSpeechConformerDepthWiseConv1d(nn.Module):\n+    \"\"\"Wrapper for padded 1D pointwise convolution.\"\"\"\n+\n+    def __init__(self, chan_in: int, chan_out: int, kernel_size: int):\n+        super().__init__()\n+        # Padding for the 1D conv is symmetric or close (i.e., offset by one).\n+        pad = kernel_size // 2\n+        pad_offset = (kernel_size + 1) % 2\n+        self.padding = (pad, pad - pad_offset)\n+\n+        self.conv = nn.Conv1d(chan_in, chan_out, kernel_size, groups=chan_in, bias=False)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = F.pad(hidden_states, self.padding)\n+        return self.conv(hidden_states)\n+\n+\n+class GraniteSpeechConformerConvModule(nn.Module):\n+    \"\"\"Conformer conv module consisting of several 1D/depthwise 1D convolutional layers.\"\"\"\n+\n+    def __init__(self, config: GraniteSpeechEncoderConfig):\n+        super().__init__()\n+        inner_dim = config.hidden_dim * config.conv_expansion_factor\n+\n+        self.norm = nn.LayerNorm(config.hidden_dim)\n+        self.up_conv = nn.Conv1d(config.hidden_dim, inner_dim * 2, 1)\n+        self.glu = nn.GLU(dim=1)\n+        self.depth_conv = GraniteSpeechConformerDepthWiseConv1d(\n+            inner_dim,\n+            inner_dim,\n+            kernel_size=config.conv_kernel_size,\n+        )\n+        self.silu = nn.SiLU()\n+        self.batch_norm = nn.BatchNorm1d(inner_dim)\n+        self.down_conv = nn.Conv1d(inner_dim, config.hidden_dim, 1)\n+        self.dropout = nn.Dropout(config.dropout)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.norm(hidden_states)\n+        hidden_states = self.up_conv(hidden_states.permute(0, 2, 1))\n+        hidden_states = self.glu(hidden_states)\n+        hidden_states = self.depth_conv(hidden_states)\n+        hidden_states = self.silu(self.batch_norm(hidden_states))\n+        hidden_states = self.down_conv(hidden_states).permute(0, 2, 1)\n+        hidden_states = self.dropout(hidden_states)\n+        return hidden_states\n+\n+\n+class GraniteSpeechConformerBlock(nn.Module):\n+    \"\"\"Conformer block, consisting largely of linear layers, attention, and convolutional layers.\"\"\"\n+\n+    def __init__(self, config: GraniteSpeechEncoderConfig):\n+        super().__init__()\n+        self.ff1 = GraniteSpeechConformerFeedForward(config)\n+        self.attn = GraniteSpeechConformerAttention(config)\n+        self.conv = GraniteSpeechConformerConvModule(config)\n+        self.ff2 = GraniteSpeechConformerFeedForward(config)\n+        self.post_norm = nn.LayerNorm(config.hidden_dim)\n+\n+    def forward(self, hidden_states: torch.Tensor, attention_dists: torch.Tensor) -> torch.Tensor:\n+        hidden_states = 0.5 * self.ff1(hidden_states) + hidden_states\n+        hidden_states = self.attn(hidden_states, attention_dists=attention_dists) + hidden_states\n+        hidden_states = self.conv(hidden_states) + hidden_states\n+        hidden_states = 0.5 * self.ff2(hidden_states) + hidden_states\n+        hidden_states = self.post_norm(hidden_states)\n+        return hidden_states\n+\n+\n+class GraniteSpeechCTCEncoder(nn.Module):\n+    def __init__(self, config: GraniteSpeechEncoderConfig):\n+        super().__init__()\n+        self.config = config\n+\n+        # Precompute clamped relative positional encoding distances\n+        seq = torch.arange(config.context_size)\n+        relpos_dist = seq.view(-1, 1) - seq.view(1, -1)\n+        self.attention_dists = torch.clamp(relpos_dist, -config.context_size, config.context_size) + config.max_pos_emb\n+\n+        self.input_linear = nn.Linear(config.input_dim, config.hidden_dim, bias=True)\n+        self.layers = nn.ModuleList([GraniteSpeechConformerBlock(config) for _ in range(config.num_layers)])\n+\n+        self.out = nn.Linear(config.hidden_dim, config.output_dim, bias=True)\n+        self.out_mid = nn.Linear(config.output_dim, config.hidden_dim, bias=True)\n+        self.num_layers = config.num_layers\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        hidden_states = self.input_linear(hidden_states)\n+        for idx, layer in enumerate(self.layers, start=1):\n+            hidden_states = layer(hidden_states, attention_dists=self.attention_dists)\n+\n+            if idx == self.num_layers // 2:\n+                hidden_states_mid = hidden_states.clone()\n+                hidden_states_mid = self.out(hidden_states_mid)\n+                hidden_states += self.out_mid(nn.Softmax(dim=-1)(hidden_states_mid))\n+        return hidden_states\n+\n+\n+GRANITE_SPEECH_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config (`GraniteSpeechConfig`):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Granite Speech Model outputting raw hidden-states without any specific head on top.\",\n+    GRANITE_SPEECH_START_DOCSTRING,\n+)\n+class GraniteSpeechPreTrainedModel(PreTrainedModel):\n+    config_class = GraniteSpeechConfig\n+    _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module: nn.Module):\n+        \"\"\"Initialize the weights.\"\"\"\n+        std = self.config.initializer_range\n+\n+        if isinstance(module, (nn.Linear, nn.Conv1d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+GRANITE_SPEECH_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        input_features (`torch.FloatTensor` of shape `(batch_size, audio seq len, mel feat dim)):\n+            The tensors corresponding to the input audios. input features can be obtained using\n+            [`AutoFeatureExtractor`]. See [`GraniteSpeechFeatureExtractor.__call__`] for details.\n+            [`GraniteSpeechProcessor`] uses [`GraniteSpeechFeatureExtractor`] for processing audio.\n+        input_mask (`torch.Tensor`, *optional*)\n+            Mask for extracted audio features that should should be ignored when creating the merged\n+            multimodal representation (i.e., due to padding).\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n+            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The Granite Speech model, which consists of an audio encoder, projector, and language model.\"\"\",\n+    GRANITE_SPEECH_START_DOCSTRING,\n+)\n+class GraniteSpeechForConditionalGeneration(GraniteSpeechPreTrainedModel, GenerationMixin):\n+    def __init__(self, config: GraniteSpeechConfig):\n+        super().__init__(config)\n+        # NOTE: It doesn't matter when we initialize from config, but we should be careful\n+        # to make sure this does not pick up the adapter_config if in the future we use\n+        # from_pretrained or something similar, since that should be set by the composite\n+        # model; don't need to consider it twice\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n+        self.encoder = GraniteSpeechCTCEncoder(config.encoder_config)\n+        self.projector = GraniteSpeechEncoderProjector(config)\n+\n+        if config.has_lora_adapter and not is_peft_available():\n+            logger.warning(\n+                \"Config indicates that a lora adapter should be present, but \"\n+                \"peft is not installed; this will cause the model to perform \"\n+                \"incorrectly when audio inputs are provided. Please install \"\n+                \"peft and reload the model!\"\n+            )\n+\n+        self.post_init()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.language_model.set_output_embeddings(new_embeddings)\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def get_output_embeddings(self):\n+        return self.language_model.get_output_embeddings()\n+\n+    def get_audio_features(self, input_features: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Get the audio features to merged into the multimodal embeddings.\"\"\"\n+        encoder_embeds = self.encoder(input_features)\n+        projected_embeds = self.projector(encoder_embeds)\n+        return projected_embeds\n+\n+    @add_start_docstrings_to_model_forward(GRANITE_SPEECH_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=GraniteSpeechCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        input_features: torch.FloatTensor = None,\n+        input_features_mask: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n+    ) -> Union[Tuple[torch.Tensor], GraniteSpeechCausalLMOutputWithPast]:\n+        r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+        Returns:\n+        \"\"\"\n+        # TODO (@alex-jw-brooks) add an example to this docstring once models are released\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if input_features is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both input_features and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            # Get the base embeddings; set all audio tokens to 0 index\n+            # to avoid out of vocabulary issues with the LLM embedding.\n+            # Audio features will be masked into is_audio_idx indices later.\n+            is_audio_idx = input_ids == self.config.audio_token_index\n+            llm_input_ids = input_ids.clone()\n+            llm_input_ids[is_audio_idx] = 0\n+            inputs_embeds = self.get_input_embeddings()(llm_input_ids)\n+\n+        if input_features is not None:\n+            if input_features.dtype != self.dtype:\n+                input_features = input_features.to(self.dtype)\n+            # Get the audio features from the encoder / projector\n+            audio_embeds = self.get_audio_features(input_features)\n+\n+            # Merge the audio features into the LLM embeddings\n+            inputs_embeds = self.get_merged_audio_embeddings(\n+                input_ids=input_ids,\n+                audio_features=audio_embeds,\n+                input_features_mask=input_features_mask,\n+            )\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n+        )\n+        logits = outputs[0]\n+\n+        loss = None\n+        if labels is not None:\n+            # Shift so that tokens < n predict n\n+            if attention_mask is not None:\n+                # we use the input attention mask to shift the logits and labels, because it is 2D.\n+                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n+                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n+                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n+                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n+            else:\n+                shift_logits = logits[..., :-1, :].contiguous()\n+                shift_labels = labels[..., 1:].contiguous()\n+            # Flatten the tokens\n+            loss_fct = nn.CrossEntropyLoss()\n+            loss = loss_fct(\n+                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n+            )\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return GraniteSpeechCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        input_features=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward audio inputs to the model\n+\n+        model_inputs = self.language_model.prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        # If we're in cached decoding stage, input_features should be None because\n+        # input ids do not contain special audio token anymore Otherwise we need\n+        # input feature values to be passed to the model\n+        if cache_position[0] == 0:\n+            model_inputs[\"input_features\"] = input_features\n+        return model_inputs\n+\n+    def get_merged_audio_embeddings(\n+        self, input_ids: torch.Tensor, audio_features: torch.Tensor, input_features_mask: Optional[torch.Tensor] = None\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Adds the audio token to the model's LLM vocabulary so that we can pass it\n+        through the tokenizer; it's assumed that the embeddings corresponding to the\n+        <|audio|> token will be clobbered with speech features.\n+\n+        Args:\n+            input_ids (`torch.Tensor`):\n+                Input IDs containing one or more audio tokens.\n+            audio_features (`torch.Tensor`):\n+                Audio features to be masked into the language embeddings to form multimodal embeddings.\n+            input_features_mask (`torch.Tensor`, *optional*, defaults to `None`)\n+                Mask to be applied to audio features prior to scattering into the language embeddings.\n+        \"\"\"\n+        is_audio_index = input_ids == self.config.audio_token_index\n+        llm_input_ids = torch.where(is_audio_index, 0, input_ids)\n+        inputs_embeds = self.language_model.get_input_embeddings()(llm_input_ids)  # [bsz, # features, hidden size]\n+\n+        # Mask the audio features into the text embeddings\n+        special_audio_mask = is_audio_index.unsqueeze(-1)\n+        audio_features = audio_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+        if input_features_mask is not None:\n+            if torch.all(is_audio_index.int().sum(dim=1) != input_features_mask.int().sum(dim=1)).item():\n+                raise ValueError(\"Number of audio tokens does not match number of audio features\")\n+\n+            audio_features = audio_features[input_features_mask]\n+\n+        inputs_embeds = inputs_embeds.masked_scatter(\n+            special_audio_mask,\n+            audio_features,\n+        )\n+        return inputs_embeds\n+\n+    def generate(self, *args, **kwargs) -> torch.LongTensor:\n+        # This model is expected to have a lora adapter, which is only\n+        # enabled when considering audio inputs. As such, we override generate\n+        # to conditionally enable / disable the lora adapter based on whether\n+        # or not any input features were provided.\n+\n+        input_features = kwargs.pop(\"input_features\", None)\n+        if is_peft_available and self._hf_peft_config_loaded:\n+            if input_features is not None:\n+                self.enable_adapters()\n+            else:\n+                self.disable_adapters()\n+        return super().generate(*args, input_features=input_features, **kwargs)\n+\n+    def save_pretrained(self, *args, **kwargs):\n+        # overwrite save_pretrained to first save the adapter if we have one\n+        # NOTE - this will use the base model path we are exporting in the lora\n+        # adapter, which may not necessarily be the best behavior, but for now\n+        # we keep this for portability, since using the local dir causes problems\n+        # if the model is loaded from outside of the current working dir.\n+        if is_peft_available and self._hf_peft_config_loaded:\n+            super().save_pretrained(*args, **kwargs)\n+        # Then save the base model afterwards\n+        self._hf_peft_config_loaded = False\n+        super().save_pretrained(*args, **kwargs)\n+\n+\n+__all__ = [\n+    \"GraniteSpeechCTCEncoder\",\n+    \"GraniteSpeechForConditionalGeneration\",\n+    \"GraniteSpeechPreTrainedModel\",\n+]"
        },
        {
            "sha": "ec36eb497031116e97ec7535a0c2eec9c1bdd48a",
            "filename": "src/transformers/models/granite_speech/processing_granite_speech.py",
            "status": "added",
            "additions": 104,
            "deletions": 0,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -0,0 +1,104 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Processor class for Granite Speech.\"\"\"\n+\n+from typing import List, Union\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...processing_utils import ProcessorMixin\n+from ...tokenization_utils import PreTokenizedInput, TextInput\n+from ...utils import is_torch_available, logging\n+from ...utils.import_utils import requires_backends\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class GraniteSpeechProcessor(ProcessorMixin):\n+    attributes = [\"audio_processor\", \"tokenizer\"]\n+    valid_kwargs = [\"audio_token\"]\n+\n+    audio_processor_class = \"GraniteSpeechFeatureExtractor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        audio_processor,\n+        tokenizer,\n+        audio_token=\"<|audio|>\",\n+        chat_template=None,\n+    ):\n+        self.audio_token = tokenizer.audio_token if hasattr(tokenizer, \"audio_token\") else audio_token\n+        super().__init__(audio_processor, tokenizer, chat_template=chat_template)\n+\n+    def __call__(\n+        self,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n+        audio: Union[\"torch.Tensor\", List[\"torch.Tensor\"]] = None,\n+        device: str = \"cpu\",\n+        images=None,\n+        videos=None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        requires_backends(self, [\"torch\"])\n+\n+        text = self._get_validated_text(text)\n+        prompt_strings = text\n+\n+        if audio is not None:\n+            # NOTE - we intentionally avoid throwing for potentially misaligned\n+            # text / audio inputs here because some inference engines will\n+            # trigger the conditions due to the way they call multimodal\n+            # processors, e.g., vLLM.\n+            audio_inputs = self.audio_processor(audio, device=device)\n+\n+            # TODO (@alex-jw-brooks); we should add a util to get_num_audio_tokens\n+            # from feature lengths and call it here, rather than returning it\n+            # from the feature extractor.\n+            audio_embed_sizes = audio_inputs.pop(\"audio_embed_sizes\")\n+\n+            # Expand the audio placeholders to match the feature dims; this\n+            # is similar to how many VLMs handle image tokens, e.g., llava next\n+            prompt_strings = []\n+            num_replaced = 0\n+            for sample in text:\n+                while self.audio_token in sample:\n+                    sample = sample.replace(\n+                        self.audio_token,\n+                        \"<placeholder>\" * audio_embed_sizes[num_replaced],\n+                        1,\n+                    )\n+                    num_replaced += 1\n+                prompt_strings.append(sample)\n+\n+            prompt_strings = [sample.replace(\"<placeholder>\", self.audio_token) for sample in prompt_strings]\n+        else:\n+            audio_inputs = {}\n+\n+        text_inputs = self.tokenizer(prompt_strings, padding=True, **kwargs)\n+        return BatchFeature(data={**text_inputs, **audio_inputs})\n+\n+    def _get_validated_text(self, text: Union[str, list]) -> List[str]:\n+        if isinstance(text, str):\n+            return [text]\n+        elif isinstance(text, list) and isinstance(text[0], str):\n+            return text\n+        raise TypeError(\"Invalid text provided! Text should be a string or list of strings.\")\n+\n+\n+__all__ = [\"GraniteSpeechProcessor\"]"
        },
        {
            "sha": "f7aea4a70207a2ce26385868b4437ea478c9efc5",
            "filename": "src/transformers/utils/dummy_torchaudio_objects.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Futils%2Fdummy_torchaudio_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/src%2Ftransformers%2Futils%2Fdummy_torchaudio_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_torchaudio_objects.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -2,6 +2,20 @@\n from ..utils import DummyObject, requires_backends\n \n \n+class GraniteSpeechFeatureExtractor(metaclass=DummyObject):\n+    _backends = [\"torchaudio\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torchaudio\"])\n+\n+\n+class GraniteSpeechProcessor(metaclass=DummyObject):\n+    _backends = [\"torchaudio\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torchaudio\"])\n+\n+\n class MusicgenMelodyFeatureExtractor(metaclass=DummyObject):\n     _backends = [\"torchaudio\"]\n "
        },
        {
            "sha": "384c47fef47e0b946ecce9e9b98078bc8674e172",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -1659,6 +1659,12 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n                 inputs_dict.pop(\"pixel_values\", None)\n                 inputs_dict.pop(\"pixel_values_videos\", None)\n                 inputs_dict.pop(\"pixel_values_images\", None)\n+            # HACK - in the case of granite speech, input_features and inputs_embeds are mutually exclusive;\n+            # this is similar to VLMs and should likely be standardized for similar audio models in the future,\n+            # then made generic here.\n+            if \"granitespeech\" in model_class.__name__.lower():\n+                inputs_dict.pop(\"input_features\", None)\n+\n             #   2.C - No easy fix, let's skip the check that compares the outputs from `input_ids` and `inputs_embeds`\n             has_complex_embeds_computation = any(\n                 model_name in model_class.__name__.lower() for model_name in [\"moshi\"]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/granite_speech/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/tests%2Fmodels%2Fgranite_speech%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/tests%2Fmodels%2Fgranite_speech%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2F__init__.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817"
        },
        {
            "sha": "02b1c4600b9ebf7421cac36870d8c533fe463036",
            "filename": "tests/models/granite_speech/test_modeling_granite_speech.py",
            "status": "added",
            "additions": 393,
            "deletions": 0,
            "changes": 393,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -0,0 +1,393 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the IBM Granite Speech model.\"\"\"\n+\n+import tempfile\n+import unittest\n+\n+import pytest\n+\n+from transformers import (\n+    AutoProcessor,\n+    GraniteSpeechConfig,\n+    GraniteSpeechForConditionalGeneration,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_torch,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import (\n+    is_datasets_available,\n+    is_torch_available,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import (\n+    ModelTesterMixin,\n+    _config_zero_init,\n+    floats_tensor,\n+    ids_tensor,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_datasets_available():\n+    from datasets import load_dataset\n+\n+\n+class GraniteSpeechForConditionalGenerationModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        seq_length=7,\n+        encoder_config={\n+            \"model_type\": \"granite_speech_encoder\",\n+            \"context_size\": 200,\n+            \"conv_expansion_factor\": 2,\n+            \"conv_kernel_size\": 15,\n+            \"dim_head\": 32,\n+            \"dropout\": 0.1,\n+            \"feedforward_mult\": 4,\n+            \"hidden_dim\": 32,\n+            \"input_dim\": 160,\n+            \"num_heads\": 4,\n+            \"num_layers\": 2,\n+            \"output_dim\": 42,\n+        },\n+        text_config={\n+            \"model_type\": \"granite\",\n+            \"is_training\": True,\n+            \"seq_length\": 7,\n+            \"use_token_type_ids\": False,\n+            \"use_labels\": True,\n+            \"vocab_size\": 99,\n+            \"hidden_size\": 32,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"intermediate_size\": 37,\n+            \"hidden_act\": \"gelu\",\n+            \"hidden_dropout_prob\": 0.1,\n+            \"attention_probs_dropout_prob\": 0.1,\n+            \"max_position_embeddings\": 580,\n+            \"type_vocab_size\": 16,\n+            \"type_sequence_label_size\": 2,\n+            \"initializer_range\": 0.02,\n+            \"num_labels\": 3,\n+            \"num_choices\": 4,\n+            \"pad_token_id\": 1,\n+        },\n+        projector_config={\n+            \"attention_probs_dropout_prob\": 0.1,\n+            \"cross_attention_frequency\": 1,\n+            \"encoder_hidden_size\": 32,\n+            \"hidden_act\": \"gelu\",\n+            \"hidden_dropout_prob\": 0.1,\n+            \"hidden_size\": 32,\n+            \"initializer_range\": 0.02,\n+            \"intermediate_size\": 256,\n+            \"layer_norm_eps\": 1e-12,\n+            \"max_position_embeddings\": 2048,\n+            \"model_type\": \"blip_2_qformer\",\n+            \"num_attention_heads\": 4,\n+            \"num_hidden_layers\": 2,\n+            \"position_embedding_type\": \"absolute\",\n+            \"use_qformer_text_input\": False,\n+            \"vocab_size\": 30522,\n+        },\n+        audio_token_index=0,\n+        tie_word_embeddings=True,\n+        initializer_range=0.02,\n+        has_lora_adapter=True,\n+        downsample_rate=5,\n+        window_size=15,\n+        is_training=True,\n+    ):\n+        self.parent = parent\n+        self.encoder_config = encoder_config\n+        self.text_config = text_config\n+        self.projector_config = projector_config\n+        self.audio_token_index = audio_token_index\n+        self.tie_word_embeddings = tie_word_embeddings\n+        self.initializer_range = initializer_range\n+        self.has_lora_adapater = has_lora_adapter\n+        self.downsample_rate = downsample_rate\n+        self.window_size = window_size\n+        self.is_training = is_training\n+\n+        # Dims for audio features\n+        self.sequence_dim = 844\n+        self.feature_dim = 160\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.batch_size = 3\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n+        self.seq_len = 7\n+        self.num_audio_tokens = 2\n+        self.seq_length = seq_length + self.num_audio_tokens\n+\n+    def get_config(self):\n+        return GraniteSpeechConfig(\n+            encoder_config=self.encoder_config,\n+            text_config=self.text_config,\n+            projector_config=self.projector_config,\n+            audio_token_index=self.audio_token_index,\n+            tie_word_embeddings=self.tie_word_embeddings,\n+            initializer_range=self.initializer_range,\n+            has_lora_adapter=self.has_lora_adapater,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        input_features = floats_tensor(\n+            [self.batch_size, self.sequence_dim, self.feature_dim],\n+        )\n+        config = self.get_config()\n+        return config, input_features\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_features = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 2) + 2\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(torch_device)\n+        input_ids[input_ids == config.audio_token_index] = self.pad_token_id\n+\n+        input_ids[:, : self.num_audio_tokens] = config.audio_token_index\n+\n+        inputs_dict = {\n+            \"input_features\": input_features,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+    def create_and_check_granite_speech_model_fp16_forward(self, config, input_ids, input_features, attention_mask):\n+        model = GraniteSpeechForConditionalGeneration(config=config)\n+        model.to(torch_device)\n+        model.half()\n+        model.eval()\n+        logits = model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            input_features=input_features,\n+            return_dict=True,\n+        )[\"logits\"]\n+        self.parent.assertFalse(torch.isnan(logits).any().item())\n+\n+    def create_and_check_granite_speech_model_fp16_autocast_forward(\n+        self,\n+        config,\n+        input_ids,\n+        input_features,\n+        attention_mask,\n+    ):\n+        config.torch_dtype = torch.float16\n+        model = GraniteSpeechForConditionalGeneration(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n+            logits = model(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                input_features=input_features.to(torch.bfloat16),\n+                return_dict=True,\n+            )[\"logits\"]\n+        self.parent.assertFalse(torch.isnan(logits).any().item())\n+\n+\n+@require_torch\n+class GraniteSpeechForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `GraniteSpeechForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (GraniteSpeechForConditionalGeneration,) if is_torch_available() else ()\n+    test_pruning = False\n+    test_head_masking = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = GraniteSpeechForConditionalGenerationModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=GraniteSpeechConfig,\n+            has_text_modality=False,\n+        )\n+\n+    def test_inputs_embeds(self):\n+        # overwrite inputs_embeds tests because we need to delete \"input features\" for the audio model\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"input_features\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    def test_initialization(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if name == \"projector.query\":\n+                    continue\n+                elif param.requires_grad:\n+                    self.assertIn(\n+                        ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        # overwrite because Granite Speech is audio+text model (not vision+text)\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            # NOTE - currently we only enable alternate attention implementations on\n+            # the encapsulated LLM; in the future, this should be added for the conformer\n+            # encoder as well.\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                text_attn = \"sdpa\" if model.language_model._supports_sdpa else \"eager\"\n+\n+                # `None` as it is the requested one which will be assigned to each sub-config\n+                # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model.language_model.config._attn_implementation == text_attn)\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.language_model.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+\n+class GraniteSpeechForConditionalGenerationIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        # TODO - use the actual model path on HF hub after release.\n+        self.model_path = \"ibm-granite/granite-speech\"\n+        self.processor = AutoProcessor.from_pretrained(self.model_path)\n+        self.prompt = self._get_prompt(self.processor.tokenizer)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def _get_prompt(self, tokenizer):\n+        chat = [\n+            {\n+                \"role\": \"system\",\n+                \"content\": \"Knowledge Cutoff Date: April 2024.\\nToday's Date: December 19, 2024.\\nYou are Granite, developed by IBM. You are a helpful AI assistant\",\n+            },\n+            {\n+                \"role\": \"user\",\n+                \"content\": \"<|audio|>can you transcribe the speech into a written format?\",\n+            },\n+        ]\n+        return tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n+\n+    def _load_datasamples(self, num_samples):\n+        ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        # automatic decoding with librispeech\n+        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+\n+        return [x[\"array\"] for x in speech_samples]\n+\n+    @slow\n+    @pytest.mark.skip(\"Public models not yet available\")\n+    def test_small_model_integration_test_single(self):\n+        model = GraniteSpeechForConditionalGeneration.from_pretrained(self.model_path).to(torch_device)\n+        input_speech = self._load_datasamples(1)\n+\n+        # Verify feature sizes; note that the feature mask refers to the size of\n+        # features that are masked into the LLM, not the output of the processor,\n+        # which is why we inspect the mask instead of the `num_features` tensor.\n+        inputs = self.processor(self.prompt, input_speech, return_tensors=\"pt\").to(torch_device)\n+\n+        num_computed_features = self.processor.audio_processor._get_num_audio_features(\n+            [speech_arr.shape[-1] for speech_arr in input_speech],\n+        )[0]\n+        num_actual_features = torch.sum(inputs[\"input_features_mask\"]).item()\n+        assert num_actual_features == num_computed_features\n+\n+        # verify generation\n+        output = model.generate(**inputs, max_new_tokens=32)\n+        EXPECTED_DECODED_TEXT = \"systemKnowledge Cutoff Date: April 2024.\\nToday's Date: December 19, 2024.\\nYou are Granite, developed by IBM. You are a helpful AI assistant\\nusercan you transcribe the speech into a written format?\\nassistantmister quilter is the apostle of the middle classes and we are glad to welcome his gospel\"  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.tokenizer.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @pytest.mark.skip(\"Public models not yet available\")\n+    def test_small_model_integration_test_batch(self):\n+        model = GraniteSpeechForConditionalGeneration.from_pretrained(self.model_path)\n+        input_speech = self._load_datasamples(2)\n+        prompts = [self.prompt, self.prompt]\n+\n+        # Verify feature sizes & padding\n+        inputs = self.processor(prompts, input_speech, return_tensors=\"pt\").to(model.device)\n+        num_computed_features = self.processor.audio_processor._get_num_audio_features(\n+            [speech_arr.shape[-1] for speech_arr in input_speech],\n+        )\n+        num_actual_features = torch.sum(inputs[\"input_features_mask\"], dim=-1)\n+        for e_feats, a_feats in zip(num_computed_features, num_actual_features):\n+            assert e_feats == a_feats.item()\n+\n+        # verify generation\n+        output = model.generate(**inputs, max_new_tokens=32)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            \"systemKnowledge Cutoff Date: April 2024.\\nToday's Date: December 19, 2024.\\nYou are Granite, developed by IBM. You are a helpful AI assistant\\nusercan you transcribe the speech into a written format?\\nassistantmister quilter is the apostle of the middle classes and we are glad to welcome his gospel\",\n+            \"systemKnowledge Cutoff Date: April 2024.\\nToday's Date: December 19, 2024.\\nYou are Granite, developed by IBM. You are a helpful AI assistant\\nusercan you transcribe the speech into a written format?\\nassistantnor is mister quilter's manner less interesting than his matter\"\n+        ]  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.tokenizer.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )"
        },
        {
            "sha": "a566658f63dfb386cdb071f830b6194b5641c318",
            "filename": "tests/models/granite_speech/test_processor_granite_speech.py",
            "status": "added",
            "additions": 222,
            "deletions": 0,
            "changes": 222,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/tests%2Fmodels%2Fgranite_speech%2Ftest_processor_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/tests%2Fmodels%2Fgranite_speech%2Ftest_processor_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2Ftest_processor_granite_speech.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -0,0 +1,222 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import shutil\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import pytest\n+import torch\n+from parameterized import parameterized\n+\n+from transformers import AutoTokenizer, GPT2TokenizerFast\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_gpu,\n+    require_torchaudio,\n+)\n+from transformers.utils import is_torchaudio_available\n+\n+\n+if is_torchaudio_available():\n+    from transformers import GraniteSpeechFeatureExtractor, GraniteSpeechProcessor\n+\n+\n+@pytest.skip(\"Public models not yet available\", allow_module_level=True)\n+@require_torch\n+@require_torchaudio\n+class GraniteSpeechProcessorTest(unittest.TestCase):\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        # TODO - use the actual model path on HF hub after release.\n+        self.checkpoint = \"ibm-granite/granite-speech\"\n+        processor = GraniteSpeechProcessor.from_pretrained(self.checkpoint)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoTokenizer.from_pretrained(self.checkpoint, **kwargs)\n+\n+    def get_audio_processor(self, **kwargs):\n+        return GraniteSpeechFeatureExtractor.from_pretrained(self.checkpoint, **kwargs)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def test_save_load_pretrained_default(self):\n+        \"\"\"Ensure we can save / reload a processor correctly.\"\"\"\n+        tokenizer = self.get_tokenizer()\n+        audio_processor = self.get_audio_processor()\n+        processor = GraniteSpeechProcessor(\n+            tokenizer=tokenizer,\n+            audio_processor=audio_processor,\n+        )\n+\n+        processor.save_pretrained(self.tmpdirname)\n+        processor = GraniteSpeechProcessor.from_pretrained(self.tmpdirname)\n+\n+        self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n+        self.assertIsInstance(processor.tokenizer, GPT2TokenizerFast)\n+\n+        self.assertEqual(processor.audio_processor.to_json_string(), audio_processor.to_json_string())\n+        self.assertIsInstance(processor.audio_processor, GraniteSpeechFeatureExtractor)\n+\n+    def test_requires_text(self):\n+        \"\"\"Ensure we require text\"\"\"\n+        tokenizer = self.get_tokenizer()\n+        audio_processor = self.get_audio_processor()\n+        processor = GraniteSpeechProcessor(\n+            tokenizer=tokenizer,\n+            audio_processor=audio_processor,\n+        )\n+\n+        with pytest.raises(TypeError):\n+            processor(text=None)\n+\n+    def test_bad_text_fails(self):\n+        \"\"\"Ensure we gracefully fail if text is the wrong type.\"\"\"\n+        tokenizer = self.get_tokenizer()\n+        audio_processor = self.get_audio_processor()\n+\n+        processor = GraniteSpeechProcessor(tokenizer=tokenizer, audio_processor=audio_processor)\n+        with pytest.raises(TypeError):\n+            processor(text=424, audio=None)\n+\n+    def test_bad_nested_text_fails(self):\n+        \"\"\"Ensure we gracefully fail if text is the wrong nested type.\"\"\"\n+        tokenizer = self.get_tokenizer()\n+        audio_processor = self.get_audio_processor()\n+        processor = GraniteSpeechProcessor(\n+            tokenizer=tokenizer,\n+            audio_processor=audio_processor,\n+        )\n+\n+        with pytest.raises(TypeError):\n+            processor(text=[424], audio=None)\n+\n+    def test_bad_audio_fails(self):\n+        \"\"\"Ensure we gracefully fail if audio is the wrong type.\"\"\"\n+        tokenizer = self.get_tokenizer()\n+        audio_processor = self.get_audio_processor()\n+        processor = GraniteSpeechProcessor(\n+            tokenizer=tokenizer,\n+            audio_processor=audio_processor,\n+        )\n+\n+        with pytest.raises(TypeError):\n+            processor(text=None, audio=\"foo\")\n+\n+    def test_nested_bad_audio_fails(self):\n+        \"\"\"Ensure we gracefully fail if audio is the wrong nested type.\"\"\"\n+        tokenizer = self.get_tokenizer()\n+        audio_processor = self.get_audio_processor()\n+        processor = GraniteSpeechProcessor(\n+            tokenizer=tokenizer,\n+            audio_processor=audio_processor,\n+        )\n+\n+        with pytest.raises(TypeError):\n+            processor(text=None, audio=[\"foo\"])\n+\n+    @parameterized.expand(\n+        [\n+            ([1, 269920], [171], torch.rand),\n+            ([1, 269920], [171], np.random.rand),\n+        ]\n+    )\n+    def test_audio_token_filling_same_len_feature_tensors(self, vec_dims, num_expected_features, random_func):\n+        \"\"\"Ensure audio token filling is handled correctly when we have\n+        one or more audio inputs whose features are all the same length\n+        stacked into a tensor / numpy array.\n+\n+        NOTE: Currently we enforce that each sample can only have one audio.\n+        \"\"\"\n+        tokenizer = self.get_tokenizer()\n+        audio_processor = self.get_audio_processor()\n+        processor = GraniteSpeechProcessor(\n+            tokenizer=tokenizer,\n+            audio_processor=audio_processor,\n+        )\n+        audio = random_func(*vec_dims) - 0.5\n+\n+        audio_tokens = processor.audio_token * vec_dims[0]\n+        inputs = processor(text=f\"{audio_tokens} Can you compare this audio?\", audio=audio, return_tensors=\"pt\")\n+\n+        # Check the number of audio tokens\n+        audio_token_id = tokenizer.get_vocab()[processor.audio_token]\n+\n+        # Make sure the number of audio tokens matches the number of features\n+        num_computed_features = processor.audio_processor._get_num_audio_features(\n+            [vec_dims[1] for _ in range(vec_dims[0])],\n+        )\n+        num_audio_tokens = int(torch.sum(inputs[\"input_ids\"] == audio_token_id))\n+        assert list(inputs[\"input_features\"].shape) == [vec_dims[0], 844, 160]\n+        assert sum(num_computed_features) == num_audio_tokens\n+\n+    def test_audio_token_filling_varying_len_feature_list(self):\n+        \"\"\"Ensure audio token filling is handled correctly when we have\n+        multiple varying len audio sequences passed as a list.\n+        \"\"\"\n+        tokenizer = self.get_tokenizer()\n+        audio_processor = self.get_audio_processor()\n+        processor = GraniteSpeechProcessor(\n+            tokenizer=tokenizer,\n+            audio_processor=audio_processor,\n+        )\n+        vec_dims = [[1, 142100], [1, 269920]]\n+        num_expected_features = [90, 171]\n+        audio = [torch.rand(dims) - 0.5 for dims in vec_dims]\n+\n+        inputs = processor(\n+            text=[\n+                f\"{processor.audio_token} Can you describe this audio?\",\n+                f\"{processor.audio_token} How does it compare with this audio?\",\n+            ],\n+            audio=audio,\n+            return_tensors=\"pt\",\n+        )\n+\n+        # Check the number of audio tokens\n+        audio_token_id = tokenizer.get_vocab()[processor.audio_token]\n+\n+        # Make sure the number of audio tokens matches the number of features\n+        num_calculated_features = processor.audio_processor._get_num_audio_features(\n+            [dims[1] for dims in vec_dims],\n+        )\n+        num_audio_tokens = int(torch.sum(inputs[\"input_ids\"] == audio_token_id))\n+        assert num_calculated_features == [90, 171]\n+        assert sum(num_expected_features) == num_audio_tokens\n+\n+    @require_torch_gpu\n+    def test_device_override(self):\n+        \"\"\"Ensure that we regardless of the processing device, the tensors\n+        produced are on the CPU.\n+        \"\"\"\n+        tokenizer = self.get_tokenizer()\n+        audio_processor = self.get_audio_processor()\n+        processor = GraniteSpeechProcessor(\n+            tokenizer=tokenizer,\n+            audio_processor=audio_processor,\n+        )\n+\n+        vec_dims = [1, 269920]\n+        wav = torch.rand(vec_dims) - 0.5\n+\n+        inputs = processor(\n+            text=f\"{processor.audio_token} Can you transcribe this audio?\",\n+            audio=wav,\n+            return_tensors=\"pt\",\n+            device=\"cuda\",\n+        )\n+\n+        assert inputs[\"input_features\"].device.type == \"cpu\""
        },
        {
            "sha": "7de82aff81af263d4f2bbfb976543fa0f3c1c8d5",
            "filename": "utils/check_config_docstrings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/623d395affc9099d0ad0223754097c1ad9b9f817/utils%2Fcheck_config_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623d395affc9099d0ad0223754097c1ad9b9f817/utils%2Fcheck_config_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_docstrings.py?ref=623d395affc9099d0ad0223754097c1ad9b9f817",
            "patch": "@@ -48,6 +48,7 @@\n     \"GraniteConfig\",\n     \"GraniteMoeConfig\",\n     \"Qwen3MoeConfig\",\n+    \"GraniteSpeechConfig\",\n }\n \n "
        }
    ],
    "stats": {
        "total": 1924,
        "additions": 1924,
        "deletions": 0
    }
}