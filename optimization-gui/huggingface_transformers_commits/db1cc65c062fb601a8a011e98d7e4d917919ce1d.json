{
    "author": "zucchini-nlp",
    "message": "Video processor accepts single frames on cuda (#41218)\n\n* fix\n\n* why was is np if input is in torch",
    "sha": "db1cc65c062fb601a8a011e98d7e4d917919ce1d",
    "files": [
        {
            "sha": "253113c1fd4412258501c396058f5f0254b4ad44",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/db1cc65c062fb601a8a011e98d7e4d917919ce1d/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/db1cc65c062fb601a8a011e98d7e4d917919ce1d/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=db1cc65c062fb601a8a011e98d7e4d917919ce1d",
            "patch": "@@ -196,7 +196,9 @@ def make_batched_videos(videos) -> list[Union[np.ndarray, \"torch.Tensor\", \"URL\",\n         return convert_pil_frames_to_video([videos])\n     # only one frame passed, thus we unsqueeze time dim\n     elif is_valid_image(videos):\n-        return [np.array(videos)[None, ...]]\n+        if isinstance(videos, PIL.Image.Image):\n+            videos = np.array(videos)\n+        return [videos[None, ...]]\n     elif not isinstance(videos, list):\n         raise ValueError(\n             f\"Invalid video input. Expected either a list of video frames or an input of 4 or 5 dimensions, but got\""
        },
        {
            "sha": "447c61d1ecb2b3b024f7eacb2c2fda99eb63aaa3",
            "filename": "tests/utils/test_video_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/db1cc65c062fb601a8a011e98d7e4d917919ce1d/tests%2Futils%2Ftest_video_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/db1cc65c062fb601a8a011e98d7e4d917919ce1d/tests%2Futils%2Ftest_video_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_video_utils.py?ref=db1cc65c062fb601a8a011e98d7e4d917919ce1d",
            "patch": "@@ -122,7 +122,7 @@ def test_make_batched_videos_torch(self):\n         torch_video = torch.from_numpy(video)\n         videos_list = make_batched_videos(torch_video)\n         self.assertIsInstance(videos_list, list)\n-        self.assertIsInstance(videos_list[0], np.ndarray)\n+        self.assertIsInstance(videos_list[0], torch.Tensor)\n         self.assertEqual(videos_list[0].shape, (1, 16, 32, 3))\n         self.assertTrue(np.array_equal(videos_list[0][0], video))\n "
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}