{
    "author": "gante",
    "message": "[CI] revert device in `test_export_static_cache` (#39662)\n\n* revert device\n\n* add todo",
    "sha": "5d0ba3e479839f3a385799cecc3cf42b4e970797",
    "files": [
        {
            "sha": "71335c37075e943bec283d315b0560f3fa2c194b",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=5d0ba3e479839f3a385799cecc3cf42b4e970797",
            "patch": "@@ -248,7 +248,7 @@ def test_export_static_cache(self):\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id, pad_token=\"<PAD>\", padding_side=\"right\")\n         # Load model\n-        device = torch_device\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n         dtype = torch.bfloat16\n         cache_implementation = \"static\"\n         attn_implementation = \"sdpa\""
        },
        {
            "sha": "f58fbf569a956d822f84bfab50afaabdaec378f1",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=5d0ba3e479839f3a385799cecc3cf42b4e970797",
            "patch": "@@ -423,7 +423,7 @@ def test_export_static_cache(self):\n         ].shape[-1]\n \n         # Load model\n-        device = torch_device\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n         dtype = torch.bfloat16\n         cache_implementation = \"static\"\n         attn_implementation = \"sdpa\""
        },
        {
            "sha": "589e08dd1d98c43e54f282784e9a5637835e7d51",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=5d0ba3e479839f3a385799cecc3cf42b4e970797",
            "patch": "@@ -335,7 +335,7 @@ def test_export_static_cache(self):\n         ].shape[-1]\n \n         # Load model\n-        device = torch_device\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n         dtype = torch.bfloat16\n         cache_implementation = \"static\"\n         attn_implementation = \"sdpa\""
        },
        {
            "sha": "136f76f48c9ad746a162bba09c58032517b2e54d",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=5d0ba3e479839f3a385799cecc3cf42b4e970797",
            "patch": "@@ -322,7 +322,7 @@ def test_export_static_cache(self):\n             ].shape[-1]\n \n             # Load model\n-            device = torch_device\n+            device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n             dtype = torch.bfloat16\n             cache_implementation = \"static\"\n             attn_implementation = \"sdpa\""
        },
        {
            "sha": "86913f254fbb26aedb40ce268b0f2934ed18f681",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=5d0ba3e479839f3a385799cecc3cf42b4e970797",
            "patch": "@@ -347,7 +347,7 @@ def test_export_static_cache(self):\n         ].shape[-1]\n \n         # Load model\n-        device = torch_device\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n         dtype = torch.bfloat16\n         cache_implementation = \"static\"\n         attn_implementation = \"sdpa\""
        },
        {
            "sha": "20b0c49d3f0b63747a21eda4a1aa78306cfb58fd",
            "filename": "tests/models/olmo2/test_modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py?ref=5d0ba3e479839f3a385799cecc3cf42b4e970797",
            "patch": "@@ -348,7 +348,7 @@ def test_export_static_cache(self):\n         ].shape[-1]\n \n         # Load model\n-        device = torch_device\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n         dtype = torch.bfloat16\n         cache_implementation = \"static\"\n         attn_implementation = \"sdpa\""
        },
        {
            "sha": "387eb6c4df79139d53fa5f4fda2b69ab3cdf0a7b",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=5d0ba3e479839f3a385799cecc3cf42b4e970797",
            "patch": "@@ -384,7 +384,7 @@ def test_export_static_cache(self):\n             config.rope_scaling[\"type\"] = \"default\"\n \n         # Load model\n-        device = torch_device\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n         dtype = torch.bfloat16\n         cache_implementation = \"static\"\n         attn_implementation = \"sdpa\""
        },
        {
            "sha": "d48226394c335aac005ae8c050e24a79339b7585",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=5d0ba3e479839f3a385799cecc3cf42b4e970797",
            "patch": "@@ -270,7 +270,7 @@ def test_export_static_cache(self):\n         ].shape[-1]\n \n         # Load model\n-        device = torch_device\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n         dtype = torch.bfloat16\n         cache_implementation = \"static\"\n         attn_implementation = \"sdpa\""
        },
        {
            "sha": "a37df40ed4a871e0bf3542768b7e8e09ab9b838a",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=5d0ba3e479839f3a385799cecc3cf42b4e970797",
            "patch": "@@ -261,7 +261,7 @@ def test_export_static_cache(self):\n         max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n             \"input_ids\"\n         ].shape[-1]\n-        device = torch_device\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n         dtype = torch.bfloat16\n         cache_implementation = \"static\"\n         attn_implementation = \"sdpa\""
        },
        {
            "sha": "f855e0b36a5f06bcce1ecc4876b141c81b54fa47",
            "filename": "tests/models/smollm3/test_modeling_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d0ba3e479839f3a385799cecc3cf42b4e970797/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py?ref=5d0ba3e479839f3a385799cecc3cf42b4e970797",
            "patch": "@@ -191,7 +191,7 @@ def test_export_static_cache(self):\n         ].shape[-1]\n \n         # Load model\n-        device = \"cpu\"\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n         dtype = torch.bfloat16\n         cache_implementation = \"static\"\n         attn_implementation = \"sdpa\""
        }
    ],
    "stats": {
        "total": 20,
        "additions": 10,
        "deletions": 10
    }
}