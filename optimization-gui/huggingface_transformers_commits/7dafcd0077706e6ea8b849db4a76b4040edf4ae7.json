{
    "author": "Cyrilvallez",
    "message": "More appropriate cuda warmup in resource-constrained hardware (#37550)\n\n* better allocation in resource constrained env\n\n* Update modeling_utils.py\n\n* CIs",
    "sha": "7dafcd0077706e6ea8b849db4a76b4040edf4ae7",
    "files": [
        {
            "sha": "bd63aad88b2b657c8ba7da7f21e1443a6cde653b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/7dafcd0077706e6ea8b849db4a76b4040edf4ae7/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7dafcd0077706e6ea8b849db4a76b4040edf4ae7/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=7dafcd0077706e6ea8b849db4a76b4040edf4ae7",
            "patch": "@@ -21,7 +21,6 @@\n import inspect\n import itertools\n import json\n-import math\n import os\n import re\n import shutil\n@@ -5872,7 +5871,7 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict,\n     for param_name, device in accelerator_device_map.items():\n         param = model.get_parameter_or_buffer(param_name)\n         # The dtype of different parameters may be different with composite models or `keep_in_fp32_modules`\n-        param_byte_count = math.prod(param.shape) * param.element_size()\n+        param_byte_count = param.numel() * param.element_size()\n \n         if tp_plan_regex is not None:\n             generic_name = re.sub(r\"\\.\\d+\\.\", \".*.\", param_name)\n@@ -5885,8 +5884,14 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict,\n         if device.type == \"cuda\":\n             index = device.index if device.index is not None else torch.cuda.current_device()\n             device_memory = torch.cuda.mem_get_info(index)[0]\n-            # Allow up to 95% of max device memory\n-            byte_count = min(byte_count, int(0.95 * device_memory))\n+            # Allow up to (max device memory - 1.2 GiB) in resource-constrained hardware configurations. Trying to reserve more\n+            # than that amount might sometimes lead to unecesary cuda OOM, if the last parameter to be loaded on the device is large,\n+            # and the remaining reserved memory portion is smaller than the param size -> torch will then try to fully re-allocate all\n+            # the param size, instead of using the remaining reserved part, and allocating only the difference, which can lead\n+            # to OOM. See https://github.com/huggingface/transformers/issues/37436#issuecomment-2808982161 for more details.\n+            # Note that we use an absolute value instead of device proportion here, as a 8GiB device could still allocate too much\n+            # if using e.g. 90% of device size, while a 140GiB device would allocate too little\n+            byte_count = min(byte_count, int(device_memory - 1.2 * 1024**3))\n         # Allocate memory\n         _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)\n "
        }
    ],
    "stats": {
        "total": 13,
        "additions": 9,
        "deletions": 4
    }
}