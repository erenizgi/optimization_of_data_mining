{
    "author": "tanuj-rai",
    "message": "Update phi4_multimodal.md (#38830)\n\n* Update phi4_multimodal.md\n\n* Update docs/source/en/model_doc/phi4_multimodal.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/phi4_multimodal.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/phi4_multimodal.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/phi4_multimodal.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/phi4_multimodal.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update phi4_multimodal.md\n\n* Update phi4_multimodal.md\n\n* Update phi4_multimodal.md\n\n* Update phi4_multimodal.md\n\n* Update phi4_multimodal.md\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "8d40ca5749dff9e0dcdc18eba4165691fe951f42",
    "files": [
        {
            "sha": "f7d93d2617d891fe47136d3b00348cefe75144c9",
            "filename": "docs/source/en/model_doc/phi4_multimodal.md",
            "status": "modified",
            "additions": 48,
            "deletions": 22,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d40ca5749dff9e0dcdc18eba4165691fe951f42/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d40ca5749dff9e0dcdc18eba4165691fe951f42/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md?ref=8d40ca5749dff9e0dcdc18eba4165691fe951f42",
            "patch": "@@ -9,44 +9,53 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n -->\n \n-# Phi4 Multimodal\n+<div style=\"float: right;\">\n+  <div class=\"flex flex-wrap space-x-1\">\n+    <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-EE4C2C?logo=pytorch&logoColor=white&style=flat\">\n+  </div>\n+</div>\n \n-## Overview\n+## Phi4 Multimodal\n \n-Phi4 Multimodal is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, generating text outputs, and comes with 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning, direct preference optimization and RLHF (Reinforcement Learning from Human Feedback) to support precise instruction adherence and safety measures. The languages that each modal supports are the following:\n+[Phi4 Multimodal](https://huggingface.co/papers/2503.01743) is a multimodal model capable of text, image, and speech and audio inputs or any combination of these. It features a mixture of LoRA adapters for handling different inputs, and each input is routed to the appropriate encoder.\n \n-- Text: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian\n-- Vision: English\n-- Audio: English, Chinese, German, French, Italian, Japanese, Spanish, Portuguese\n+You can find all the original Phi4 Multimodal checkpoints under the [Phi4](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4) collection.\n \n-This model was contributed by [Cyril Vallez](https://huggingface.co/cyrilvallez). The most recent code can be\n-found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py).\n+> [!TIP]\n+> This model was contributed by [cyrilvallez](https://huggingface.co/cyrilvallez).\n+>\n+> Click on the Phi-4 Multimodal in the right sidebar for more examples of how to apply Phi-4 Multimodal to different tasks.\n \n+The example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n \n-## Usage tips\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n-`Phi4-multimodal-instruct` can be found on the [Huggingface Hub](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)\n+```python\n+from transformers import pipeline\n+generator = pipeline(\"text-generation\", model=\"microsoft/Phi-4-multimodal-instruct\", torch_dtype=\"auto\", device=0)\n+\n+prompt = \"Explain the concept of multimodal AI in simple terms.\"\n+\n+result = generator(prompt, max_length=50)\n+print(result[0]['generated_text'])\n+```\n \n-In the following, we demonstrate how to use it for inference depending on the input modalities (text, image, audio).\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n ```python\n import torch\n from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n \n-\n-# Define model path\n model_path = \"microsoft/Phi-4-multimodal-instruct\"\n device = \"cuda:0\"\n \n-# Load model and processor\n processor = AutoProcessor.from_pretrained(model_path)\n-model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device,  torch_dtype=torch.float16)\n+model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, torch_dtype=torch.float16)\n \n-# Optional: load the adapters (note that without them, the base model will very likely not work well)\n-model.load_adapter(model_path, adapter_name=\"speech\", device_map=device, adapter_kwargs={\"subfolder\": 'speech-lora'})\n model.load_adapter(model_path, adapter_name=\"vision\", device_map=device, adapter_kwargs={\"subfolder\": 'vision-lora'})\n \n-# Part : Image Processing\n messages = [\n     {\n         \"role\": \"user\",\n@@ -57,7 +66,7 @@ messages = [\n     },\n ]\n \n-model.set_adapter(\"vision\") # if loaded, activate the vision adapter\n+model.set_adapter(\"vision\")\n inputs = processor.apply_chat_template(\n     messages,\n     add_generation_prompt=True,\n@@ -66,7 +75,6 @@ inputs = processor.apply_chat_template(\n     return_tensors=\"pt\",\n ).to(device)\n \n-# Generate response\n generate_ids = model.generate(\n     **inputs,\n     max_new_tokens=1000,\n@@ -77,10 +85,27 @@ response = processor.batch_decode(\n     generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n )[0]\n print(f'>>> Response\\n{response}')\n+```\n+\n+</hfoption>\n+</hfoptions>\n \n+## Notes\n \n-# Part 2: Audio Processing\n-model.set_adapter(\"speech\") # if loaded, activate the speech adapter\n+The example below demonstrates inference with an audio and text input.\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n+\n+model_path = \"microsoft/Phi-4-multimodal-instruct\"\n+device = \"cuda:0\"\n+\n+processor = AutoProcessor.from_pretrained(model_path)\n+model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device,  torch_dtype=torch.float16)\n+\n+model.load_adapter(model_path, adapter_name=\"speech\", device_map=device, adapter_kwargs={\"subfolder\": 'speech-lora'})\n+model.set_adapter(\"speech\")\n audio_url = \"https://upload.wikimedia.org/wikipedia/commons/b/b0/Barbara_Sahakian_BBC_Radio4_The_Life_Scientific_29_May_2012_b01j5j24.flac\"\n messages = [\n     {\n@@ -110,6 +135,7 @@ response = processor.batch_decode(\n     generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n )[0]\n print(f'>>> Response\\n{response}')\n+\n ```\n \n ## Phi4MultimodalFeatureExtractor"
        }
    ],
    "stats": {
        "total": 70,
        "additions": 48,
        "deletions": 22
    }
}