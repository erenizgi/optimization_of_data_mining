{
    "author": "Rocketknight1",
    "message": "Update chat template docs to remove Blenderbot (#33254)\n\n* Update docs to remove obsolete Blenderbot\r\n\r\n* Remove another reference to Blenderbot",
    "sha": "0d86727354ad8b1ff23ab66380e949fe0d842590",
    "files": [
        {
            "sha": "6564912a8534be5638a71bb67fb6529c218a7bfe",
            "filename": "docs/source/en/chat_templating.md",
            "status": "modified",
            "additions": 46,
            "deletions": 64,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d86727354ad8b1ff23ab66380e949fe0d842590/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d86727354ad8b1ff23ab66380e949fe0d842590/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=0d86727354ad8b1ff23ab66380e949fe0d842590",
            "patch": "@@ -26,26 +26,7 @@ Much like tokenization, different models expect very different input formats for\n **chat templates** as a feature. Chat templates are part of the tokenizer. They specify how to convert conversations, \n represented as lists of messages, into a single tokenizable string in the format that the model expects. \n \n-Let's make this concrete with a quick example using the `BlenderBot` model. BlenderBot has an extremely simple default \n-template, which mostly just adds whitespace between rounds of dialogue:\n-\n-```python\n->>> from transformers import AutoTokenizer\n->>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n-\n->>> chat = [\n-...    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n-...    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n-...    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n-... ]\n-\n->>> tokenizer.apply_chat_template(chat, tokenize=False)\n-\" Hello, how are you?  I'm doing great. How can I help you today?   I'd like to show off how chat templating works!</s>\"\n-```\n-\n-Notice how the entire chat is condensed into a single string. If we use `tokenize=True`, which is the default setting,\n-that string will also be tokenized for us. To see a more complex template in action, though, let's use the \n-`mistralai/Mistral-7B-Instruct-v0.1` model.\n+Let's make this concrete with a quick example using the `mistralai/Mistral-7B-Instruct-v0.1` model:\n \n ```python\n >>> from transformers import AutoTokenizer\n@@ -61,8 +42,26 @@ that string will also be tokenized for us. To see a more complex template in act\n \"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n ```\n \n-Note that this time, the tokenizer has added the control tokens [INST] and [/INST] to indicate the start and end of \n-user messages (but not assistant messages!). Mistral-instruct was trained with these tokens, but BlenderBot was not.\n+Notice how the tokenizer has added the control tokens [INST] and [/INST] to indicate the start and end of \n+user messages (but not assistant messages!), and the entire chat is condensed into a single string. \n+If we use `tokenize=True`, which is the default setting, that string will also be tokenized for us.\n+\n+Now, try the same code, but swap in the `HuggingFaceH4/zephyr-7b-beta` model instead, and you should get:\n+\n+```text\n+<|user|>\n+Hello, how are you?</s>\n+<|assistant|>\n+I'm doing great. How can I help you today?</s>\n+<|user|>\n+I'd like to show off how chat templating works!</s>\n+```\n+\n+Both Zephyr and Mistral-Instruct were fine-tuned from the same base model, `Mistral-7B-v0.1`. However, they were trained\n+with totally different chat formats. Without chat templates, you would have to write manual formatting code for each\n+model, and it's very easy to make minor errors that hurt performance! Chat templates handle the details of formatting \n+for you, allowing you to write universal code that works for any model.\n+\n \n ## How do I use chat templates?\n \n@@ -71,7 +70,7 @@ and `content` keys, and then pass it to the [`~PreTrainedTokenizer.apply_chat_te\n you'll get output that's ready to go! When using chat templates as input for model generation, it's also a good idea\n to use `add_generation_prompt=True` to add a [generation prompt](#what-are-generation-prompts). \n \n-Here's an example of preparing input for `model.generate()`, using the `Zephyr` assistant model:\n+Here's an example of preparing input for `model.generate()`, using `Zephyr` again:\n \n ```python\n from transformers import AutoModelForCausalLM, AutoTokenizer\n@@ -160,7 +159,7 @@ messages = [\n ]\n ```\n \n-Here's what this will look like without a generation prompt, using the ChatML template we saw in the Zephyr example:\n+Here's what this will look like without a generation prompt, for a model that uses standard \"ChatML\" formatting:\n \n ```python\n tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n@@ -193,7 +192,7 @@ message. Remember, chat models are still just language models - they're trained\n special kind of text to them! You need to guide them with appropriate control tokens, so they know what they're \n supposed to be doing.\n \n-Not all models require generation prompts. Some models, like BlenderBot and LLaMA, don't have any\n+Not all models require generation prompts. Some models, like LLaMA, don't have any\n special tokens before bot responses. In these cases, the `add_generation_prompt` argument will have no effect. The exact\n effect that `add_generation_prompt` has will depend on the template being used.\n \n@@ -630,58 +629,41 @@ model_input = tokenizer.apply_chat_template(\n ## Advanced: How do chat templates work?\n \n The chat template for a model is stored on the `tokenizer.chat_template` attribute. If no chat template is set, the\n-default template for that model class is used instead. Let's take a look at the template for `BlenderBot`:\n-\n-```python\n-\n->>> from transformers import AutoTokenizer\n->>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n-\n->>> tokenizer.chat_template\n-\"{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}\"\n-```\n-\n-That's kind of intimidating. Let's clean it up a little to make it more readable. In the process, though, we also make\n-sure that the newlines and indentation we add don't end up being included in the template output - see the tip on\n-[trimming whitespace](#trimming-whitespace) below!\n+default template for that model class is used instead. Let's take a look at a `Zephyr` chat template, though note this\n+one is a little simplified from the actual one!\n \n ```\n {%- for message in messages %}\n-    {%- if message['role'] == 'user' %}\n-        {{- ' ' }}\n-    {%- endif %}\n-    {{- message['content'] }}\n-    {%- if not loop.last %}\n-        {{- '  ' }}\n-    {%- endif %}\n+    {{- '<|' + message['role'] + |>\\n' }}\n+    {{- message['content'] + eos_token }}\n {%- endfor %}\n-{{- eos_token }}\n+{%- if add_generation_prompt %}\n+    {{- '<|assistant|>\\n' }}\n+{%- endif %}\n ```\n \n If you've never seen one of these before, this is a [Jinja template](https://jinja.palletsprojects.com/en/3.1.x/templates/).\n Jinja is a templating language that allows you to write simple code that generates text. In many ways, the code and\n syntax resembles Python. In pure Python, this template would look something like this:\n \n ```python\n-for idx, message in enumerate(messages):\n-    if message['role'] == 'user':\n-        print(' ')\n-    print(message['content'])\n-    if not idx == len(messages) - 1:  # Check for the last message in the conversation\n-        print('  ')\n-print(eos_token)\n+for message in messages:\n+    print(f'<|{message[\"role\"]}|>')\n+    print(message['content'] + eos_token)\n+if add_generation_prompt:\n+    print('<|assistant|>')\n ```\n \n Effectively, the template does three things:\n-1. For each message, if the message is a user message, add a blank space before it, otherwise print nothing.\n-2. Add the message content\n-3. If the message is not the last message, add two spaces after it. After the final message, print the EOS token.\n+1. For each message, print the role enclosed in `<|` and `|>`, like `<|user|>` or `<|assistant|>`.\n+2. Next, print the content of the message, followed by the end-of-sequence token.\n+3. Finally, if `add_generation_prompt` is set, print the assistant token, so that the model knows to start generating\n+   an assistant response.\n \n-This is a pretty simple template - it doesn't add any control tokens, and it doesn't support \"system\" messages, which \n-are a common way to give the model directives about how it should behave in the subsequent conversation.\n-But Jinja gives you a lot of flexibility to do those things! Let's see a Jinja template that can format inputs\n-similarly to the way LLaMA formats them (note that the real LLaMA template includes handling for default system\n-messages and slightly different system message handling in general - don't use this one in your actual code!)\n+This is a pretty simple template but Jinja gives you a lot of flexibility to do more complex things! Let's see a Jinja\n+template that can format inputs similarly to the way LLaMA formats them (note that the real LLaMA template includes \n+handling for default system messages and slightly different system message handling in general - don't use this one \n+in your actual code!)\n \n ```\n {%- for message in messages %}\n@@ -695,8 +677,8 @@ messages and slightly different system message handling in general - don't use t\n {%- endfor %}\n ```\n \n-Hopefully if you stare at this for a little bit you can see what this template is doing - it adds specific tokens based\n-on the \"role\" of each message, which represents who sent it. User, assistant and system messages are clearly\n+Hopefully if you stare at this for a little bit you can see what this template is doing - it adds specific tokens like\n+`[INST]` and `[/INST]` based on the role of each message. User, assistant and system messages are clearly\n distinguishable to the model because of the tokens they're wrapped in.\n \n ## Advanced: Adding and editing chat templates"
        }
    ],
    "stats": {
        "total": 110,
        "additions": 46,
        "deletions": 64
    }
}