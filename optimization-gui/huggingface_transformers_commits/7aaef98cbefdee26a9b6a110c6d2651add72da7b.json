{
    "author": "gante",
    "message": "rm src/transformers/convert_pytorch_checkpoint_to_tf2.py (#40718)\n\n* rm src/transformers/convert_pytorch_checkpoint_to_tf2.py\n\n* doctest skip",
    "sha": "7aaef98cbefdee26a9b6a110c6d2651add72da7b",
    "files": [
        {
            "sha": "24e1f8506490bae3e37d807edc27d31493d07ffd",
            "filename": "src/transformers/convert_pytorch_checkpoint_to_tf2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 440,
            "changes": 440,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fconvert_pytorch_checkpoint_to_tf2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fconvert_pytorch_checkpoint_to_tf2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_pytorch_checkpoint_to_tf2.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -1,440 +0,0 @@\n-# Copyright 2018 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Convert pytorch checkpoints to TensorFlow\"\"\"\n-\n-import argparse\n-import os\n-\n-from . import (\n-    AlbertConfig,\n-    BartConfig,\n-    BertConfig,\n-    CamembertConfig,\n-    CTRLConfig,\n-    DistilBertConfig,\n-    DPRConfig,\n-    ElectraConfig,\n-    FlaubertConfig,\n-    GPT2Config,\n-    LayoutLMConfig,\n-    LxmertConfig,\n-    OpenAIGPTConfig,\n-    RobertaConfig,\n-    T5Config,\n-    TFAlbertForPreTraining,\n-    TFBartForConditionalGeneration,\n-    TFBartForSequenceClassification,\n-    TFBertForPreTraining,\n-    TFBertForQuestionAnswering,\n-    TFBertForSequenceClassification,\n-    TFCamembertForMaskedLM,\n-    TFCTRLLMHeadModel,\n-    TFDistilBertForMaskedLM,\n-    TFDistilBertForQuestionAnswering,\n-    TFDPRContextEncoder,\n-    TFDPRQuestionEncoder,\n-    TFDPRReader,\n-    TFElectraForPreTraining,\n-    TFFlaubertWithLMHeadModel,\n-    TFGPT2LMHeadModel,\n-    TFLayoutLMForMaskedLM,\n-    TFLxmertForPreTraining,\n-    TFLxmertVisualFeatureEncoder,\n-    TFOpenAIGPTLMHeadModel,\n-    TFRobertaForCausalLM,\n-    TFRobertaForMaskedLM,\n-    TFRobertaForSequenceClassification,\n-    TFT5ForConditionalGeneration,\n-    TFTransfoXLLMHeadModel,\n-    TFWav2Vec2Model,\n-    TFXLMRobertaForMaskedLM,\n-    TFXLMWithLMHeadModel,\n-    TFXLNetLMHeadModel,\n-    TransfoXLConfig,\n-    Wav2Vec2Config,\n-    Wav2Vec2Model,\n-    XLMConfig,\n-    XLMRobertaConfig,\n-    XLNetConfig,\n-    is_torch_available,\n-    load_pytorch_checkpoint_in_tf2_model,\n-)\n-from .utils import CONFIG_NAME, WEIGHTS_NAME, cached_file, logging\n-\n-\n-if is_torch_available():\n-    import numpy as np\n-    import torch\n-\n-    from . import (\n-        AlbertForPreTraining,\n-        BartForConditionalGeneration,\n-        BertForPreTraining,\n-        BertForQuestionAnswering,\n-        BertForSequenceClassification,\n-        CamembertForMaskedLM,\n-        CTRLLMHeadModel,\n-        DistilBertForMaskedLM,\n-        DistilBertForQuestionAnswering,\n-        DPRContextEncoder,\n-        DPRQuestionEncoder,\n-        DPRReader,\n-        ElectraForPreTraining,\n-        FlaubertWithLMHeadModel,\n-        GPT2LMHeadModel,\n-        LayoutLMForMaskedLM,\n-        LxmertForPreTraining,\n-        LxmertVisualFeatureEncoder,\n-        OpenAIGPTLMHeadModel,\n-        RobertaForMaskedLM,\n-        RobertaForSequenceClassification,\n-        T5ForConditionalGeneration,\n-        TransfoXLLMHeadModel,\n-        XLMRobertaForMaskedLM,\n-        XLMWithLMHeadModel,\n-        XLNetLMHeadModel,\n-    )\n-\n-\n-logging.set_verbosity_info()\n-\n-MODEL_CLASSES = {\n-    \"bart\": (\n-        BartConfig,\n-        TFBartForConditionalGeneration,\n-        TFBartForSequenceClassification,\n-        BartForConditionalGeneration,\n-    ),\n-    \"bert\": (\n-        BertConfig,\n-        TFBertForPreTraining,\n-        BertForPreTraining,\n-    ),\n-    \"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\": (\n-        BertConfig,\n-        TFBertForQuestionAnswering,\n-        BertForQuestionAnswering,\n-    ),\n-    \"google-bert/bert-large-cased-whole-word-masking-finetuned-squad\": (\n-        BertConfig,\n-        TFBertForQuestionAnswering,\n-        BertForQuestionAnswering,\n-    ),\n-    \"google-bert/bert-base-cased-finetuned-mrpc\": (\n-        BertConfig,\n-        TFBertForSequenceClassification,\n-        BertForSequenceClassification,\n-    ),\n-    \"dpr\": (\n-        DPRConfig,\n-        TFDPRQuestionEncoder,\n-        TFDPRContextEncoder,\n-        TFDPRReader,\n-        DPRQuestionEncoder,\n-        DPRContextEncoder,\n-        DPRReader,\n-    ),\n-    \"openai-community/gpt2\": (\n-        GPT2Config,\n-        TFGPT2LMHeadModel,\n-        GPT2LMHeadModel,\n-    ),\n-    \"xlnet\": (\n-        XLNetConfig,\n-        TFXLNetLMHeadModel,\n-        XLNetLMHeadModel,\n-    ),\n-    \"xlm\": (\n-        XLMConfig,\n-        TFXLMWithLMHeadModel,\n-        XLMWithLMHeadModel,\n-    ),\n-    \"xlm-roberta\": (\n-        XLMRobertaConfig,\n-        TFXLMRobertaForMaskedLM,\n-        XLMRobertaForMaskedLM,\n-    ),\n-    \"transfo-xl\": (\n-        TransfoXLConfig,\n-        TFTransfoXLLMHeadModel,\n-        TransfoXLLMHeadModel,\n-    ),\n-    \"openai-community/openai-gpt\": (\n-        OpenAIGPTConfig,\n-        TFOpenAIGPTLMHeadModel,\n-        OpenAIGPTLMHeadModel,\n-    ),\n-    \"roberta\": (\n-        RobertaConfig,\n-        TFRobertaForCausalLM,\n-        TFRobertaForMaskedLM,\n-        RobertaForMaskedLM,\n-    ),\n-    \"layoutlm\": (\n-        LayoutLMConfig,\n-        TFLayoutLMForMaskedLM,\n-        LayoutLMForMaskedLM,\n-    ),\n-    \"FacebookAI/roberta-large-mnli\": (\n-        RobertaConfig,\n-        TFRobertaForSequenceClassification,\n-        RobertaForSequenceClassification,\n-    ),\n-    \"camembert\": (\n-        CamembertConfig,\n-        TFCamembertForMaskedLM,\n-        CamembertForMaskedLM,\n-    ),\n-    \"flaubert\": (\n-        FlaubertConfig,\n-        TFFlaubertWithLMHeadModel,\n-        FlaubertWithLMHeadModel,\n-    ),\n-    \"distilbert\": (\n-        DistilBertConfig,\n-        TFDistilBertForMaskedLM,\n-        DistilBertForMaskedLM,\n-    ),\n-    \"distilbert-base-distilled-squad\": (\n-        DistilBertConfig,\n-        TFDistilBertForQuestionAnswering,\n-        DistilBertForQuestionAnswering,\n-    ),\n-    \"lxmert\": (\n-        LxmertConfig,\n-        TFLxmertForPreTraining,\n-        LxmertForPreTraining,\n-    ),\n-    \"lxmert-visual-feature-encoder\": (\n-        LxmertConfig,\n-        TFLxmertVisualFeatureEncoder,\n-        LxmertVisualFeatureEncoder,\n-    ),\n-    \"Salesforce/ctrl\": (\n-        CTRLConfig,\n-        TFCTRLLMHeadModel,\n-        CTRLLMHeadModel,\n-    ),\n-    \"albert\": (\n-        AlbertConfig,\n-        TFAlbertForPreTraining,\n-        AlbertForPreTraining,\n-    ),\n-    \"t5\": (\n-        T5Config,\n-        TFT5ForConditionalGeneration,\n-        T5ForConditionalGeneration,\n-    ),\n-    \"electra\": (\n-        ElectraConfig,\n-        TFElectraForPreTraining,\n-        ElectraForPreTraining,\n-    ),\n-    \"wav2vec2\": (\n-        Wav2Vec2Config,\n-        TFWav2Vec2Model,\n-        Wav2Vec2Model,\n-    ),\n-}\n-\n-\n-def convert_pt_checkpoint_to_tf(\n-    model_type, pytorch_checkpoint_path, config_file, tf_dump_path, compare_with_pt_model=False, use_cached_models=True\n-):\n-    if model_type not in MODEL_CLASSES:\n-        raise ValueError(f\"Unrecognized model type, should be one of {list(MODEL_CLASSES.keys())}.\")\n-\n-    config_class, model_class, pt_model_class, aws_config_map = MODEL_CLASSES[model_type]\n-\n-    # Initialise TF model\n-    if config_file in aws_config_map:\n-        config_file = cached_file(config_file, CONFIG_NAME, force_download=not use_cached_models)\n-    config = config_class.from_json_file(config_file)\n-    config.output_hidden_states = True\n-    config.output_attentions = True\n-    print(f\"Building TensorFlow model from configuration: {config}\")\n-    tf_model = model_class(config)\n-\n-    # Load weights from tf checkpoint\n-    if pytorch_checkpoint_path in aws_config_map:\n-        pytorch_checkpoint_path = cached_file(\n-            pytorch_checkpoint_path, WEIGHTS_NAME, force_download=not use_cached_models\n-        )\n-    # Load PyTorch checkpoint in tf2 model:\n-    tf_model = load_pytorch_checkpoint_in_tf2_model(tf_model, pytorch_checkpoint_path)\n-\n-    if compare_with_pt_model:\n-        tfo = tf_model(tf_model.dummy_inputs, training=False)  # build the network\n-\n-        state_dict = torch.load(pytorch_checkpoint_path, map_location=\"cpu\", weights_only=True)\n-        pt_model = pt_model_class.from_pretrained(\n-            pretrained_model_name_or_path=None, config=config, state_dict=state_dict\n-        )\n-\n-        with torch.no_grad():\n-            pto = pt_model(**pt_model.dummy_inputs)\n-\n-        np_pt = pto[0].numpy()\n-        np_tf = tfo[0].numpy()\n-        diff = np.amax(np.abs(np_pt - np_tf))\n-        print(f\"Max absolute difference between models outputs {diff}\")\n-        assert diff <= 2e-2, f\"Error, model absolute difference is >2e-2: {diff}\"\n-\n-    # Save pytorch-model\n-    print(f\"Save TensorFlow model to {tf_dump_path}\")\n-    tf_model.save_weights(tf_dump_path, save_format=\"h5\")\n-\n-\n-def convert_all_pt_checkpoints_to_tf(\n-    args_model_type,\n-    tf_dump_path,\n-    model_shortcut_names_or_path=None,\n-    config_shortcut_names_or_path=None,\n-    compare_with_pt_model=False,\n-    use_cached_models=False,\n-    remove_cached_files=False,\n-    only_convert_finetuned_models=False,\n-):\n-    if args_model_type is None:\n-        model_types = list(MODEL_CLASSES.keys())\n-    else:\n-        model_types = [args_model_type]\n-\n-    for j, model_type in enumerate(model_types, start=1):\n-        print(\"=\" * 100)\n-        print(f\" Converting model type {j}/{len(model_types)}: {model_type}\")\n-        print(\"=\" * 100)\n-        if model_type not in MODEL_CLASSES:\n-            raise ValueError(f\"Unrecognized model type {model_type}, should be one of {list(MODEL_CLASSES.keys())}.\")\n-\n-        config_class, model_class, pt_model_class, aws_model_maps, aws_config_map = MODEL_CLASSES[model_type]\n-\n-        if model_shortcut_names_or_path is None:\n-            model_shortcut_names_or_path = list(aws_model_maps.keys())\n-        if config_shortcut_names_or_path is None:\n-            config_shortcut_names_or_path = model_shortcut_names_or_path\n-\n-        for i, (model_shortcut_name, config_shortcut_name) in enumerate(\n-            zip(model_shortcut_names_or_path, config_shortcut_names_or_path), start=1\n-        ):\n-            print(\"-\" * 100)\n-            if \"-squad\" in model_shortcut_name or \"-mrpc\" in model_shortcut_name or \"-mnli\" in model_shortcut_name:\n-                if not only_convert_finetuned_models:\n-                    print(f\"    Skipping finetuned checkpoint {model_shortcut_name}\")\n-                    continue\n-                model_type = model_shortcut_name\n-            elif only_convert_finetuned_models:\n-                print(f\"    Skipping not finetuned checkpoint {model_shortcut_name}\")\n-                continue\n-            print(\n-                f\"    Converting checkpoint {i}/{len(aws_config_map)}: {model_shortcut_name} - model_type {model_type}\"\n-            )\n-            print(\"-\" * 100)\n-\n-            if config_shortcut_name in aws_config_map:\n-                config_file = cached_file(config_shortcut_name, CONFIG_NAME, force_download=not use_cached_models)\n-            else:\n-                config_file = config_shortcut_name\n-\n-            if model_shortcut_name in aws_model_maps:\n-                model_file = cached_file(model_shortcut_name, WEIGHTS_NAME, force_download=not use_cached_models)\n-            else:\n-                model_file = model_shortcut_name\n-\n-            if os.path.isfile(model_shortcut_name):\n-                model_shortcut_name = \"converted_model\"\n-\n-            convert_pt_checkpoint_to_tf(\n-                model_type=model_type,\n-                pytorch_checkpoint_path=model_file,\n-                config_file=config_file,\n-                tf_dump_path=os.path.join(tf_dump_path, model_shortcut_name + \"-tf_model.h5\"),\n-                compare_with_pt_model=compare_with_pt_model,\n-            )\n-            if remove_cached_files:\n-                os.remove(config_file)\n-                os.remove(model_file)\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser()\n-    # Required parameters\n-    parser.add_argument(\n-        \"--tf_dump_path\", default=None, type=str, required=True, help=\"Path to the output Tensorflow dump file.\"\n-    )\n-    parser.add_argument(\n-        \"--model_type\",\n-        default=None,\n-        type=str,\n-        help=(\n-            f\"Model type selected in the list of {list(MODEL_CLASSES.keys())}. If not given, will download and \"\n-            \"convert all the models from AWS.\"\n-        ),\n-    )\n-    parser.add_argument(\n-        \"--pytorch_checkpoint_path\",\n-        default=None,\n-        type=str,\n-        help=(\n-            \"Path to the PyTorch checkpoint path or shortcut name to download from AWS. \"\n-            \"If not given, will download and convert all the checkpoints from AWS.\"\n-        ),\n-    )\n-    parser.add_argument(\n-        \"--config_file\",\n-        default=None,\n-        type=str,\n-        help=(\n-            \"The config json file corresponding to the pre-trained model. \\n\"\n-            \"This specifies the model architecture. If not given and \"\n-            \"--pytorch_checkpoint_path is not given or is a shortcut name \"\n-            \"use the configuration associated to the shortcut name on the AWS\"\n-        ),\n-    )\n-    parser.add_argument(\n-        \"--compare_with_pt_model\", action=\"store_true\", help=\"Compare Tensorflow and PyTorch model predictions.\"\n-    )\n-    parser.add_argument(\n-        \"--use_cached_models\",\n-        action=\"store_true\",\n-        help=\"Use cached models if possible instead of updating to latest checkpoint versions.\",\n-    )\n-    parser.add_argument(\n-        \"--remove_cached_files\",\n-        action=\"store_true\",\n-        help=\"Remove pytorch models after conversion (save memory when converting in batches).\",\n-    )\n-    parser.add_argument(\"--only_convert_finetuned_models\", action=\"store_true\", help=\"Only convert finetuned models.\")\n-    args = parser.parse_args()\n-\n-    # if args.pytorch_checkpoint_path is not None:\n-    #     convert_pt_checkpoint_to_tf(args.model_type.lower(),\n-    #                                 args.pytorch_checkpoint_path,\n-    #                                 args.config_file if args.config_file is not None else args.pytorch_checkpoint_path,\n-    #                                 args.tf_dump_path,\n-    #                                 compare_with_pt_model=args.compare_with_pt_model,\n-    #                                 use_cached_models=args.use_cached_models)\n-    # else:\n-    convert_all_pt_checkpoints_to_tf(\n-        args.model_type.lower() if args.model_type is not None else None,\n-        args.tf_dump_path,\n-        model_shortcut_names_or_path=[args.pytorch_checkpoint_path]\n-        if args.pytorch_checkpoint_path is not None\n-        else None,\n-        config_shortcut_names_or_path=[args.config_file] if args.config_file is not None else None,\n-        compare_with_pt_model=args.compare_with_pt_model,\n-        use_cached_models=args.use_cached_models,\n-        remove_cached_files=args.remove_cached_files,\n-        only_convert_finetuned_models=args.only_convert_finetuned_models,\n-    )"
        },
        {
            "sha": "36226d2891912e732b79a01261eb1c33660fc1e4",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aaef98cbefdee26a9b6a110c6d2651add72da7b/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aaef98cbefdee26a9b6a110c6d2651add72da7b/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=7aaef98cbefdee26a9b6a110c6d2651add72da7b",
            "patch": "@@ -324,7 +324,6 @@ src/transformers/commands/train.py\n src/transformers/commands/transformers_cli.py\n src/transformers/configuration_utils.py\n src/transformers/convert_graph_to_onnx.py\n-src/transformers/convert_pytorch_checkpoint_to_tf2.py\n src/transformers/convert_slow_tokenizer.py\n src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py\n src/transformers/convert_tf_hub_seq_to_seq_bert_to_pytorch.py"
        }
    ],
    "stats": {
        "total": 441,
        "additions": 0,
        "deletions": 441
    }
}