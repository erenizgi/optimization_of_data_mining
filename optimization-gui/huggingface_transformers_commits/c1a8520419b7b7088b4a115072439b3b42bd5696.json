{
    "author": "zucchini-nlp",
    "message": "Cache: init empty cache when `use_cache` (#34274)\n\n* fix\r\n\r\n* fix tests\r\n\r\n* fix copies\r\n\r\n* add docs\r\n\r\n* Revert \"add docs\"\r\n\r\nThis reverts commit 32d35634f12ba02781d2ebdee0c8dcfbe992a7b9.\r\n\r\n* qwen move deltas\r\n\r\n* mllama can potentiall fullgraph compile\r\n\r\n* enable mllama compile and fix tests\r\n\r\n* remove mllama fixes",
    "sha": "c1a8520419b7b7088b4a115072439b3b42bd5696",
    "files": [
        {
            "sha": "3255b6f44c05fbc372e4e2f1d3a99ef8f8acd58d",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1a8520419b7b7088b4a115072439b3b42bd5696/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1a8520419b7b7088b4a115072439b3b42bd5696/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=c1a8520419b7b7088b4a115072439b3b42bd5696",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward\n@@ -1300,6 +1300,10 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        # torch.jit.trace() doesn't support cache objects in the output\n+        if use_cache and past_key_values is None and not torch.jit.is_tracing():\n+            past_key_values = DynamicCache()\n+\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange("
        },
        {
            "sha": "3ce5d0b7aa0b5de3eaf7bce1650a42c604fb7336",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1a8520419b7b7088b4a115072439b3b42bd5696/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1a8520419b7b7088b4a115072439b3b42bd5696/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=c1a8520419b7b7088b4a115072439b3b42bd5696",
            "patch": "@@ -24,7 +24,7 @@\n \n from ... import PreTrainedModel\n from ...activations import ACT2FN\n-from ...cache_utils import Cache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -1618,6 +1618,9 @@ def forward(\n \n         hidden_states = inputs_embeds\n \n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n@@ -1845,7 +1848,7 @@ def __init__(self, config):\n         super().__init__(config.get_text_config())\n         self.text_config = config.get_text_config()\n         self.vocab_size = self.text_config.vocab_size\n-        self.model = MllamaTextModel._from_config(self.text_config, attn_implementation=config._attn_implementation)\n+        self.model = MllamaTextModel._from_config(self.text_config)\n         self.lm_head = nn.Linear(self.text_config.hidden_size, self.vocab_size, bias=False)\n \n         self.post_init()"
        },
        {
            "sha": "76275778c49a76e7774aa70bf8ae52fed3d27663",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1a8520419b7b7088b4a115072439b3b42bd5696/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1a8520419b7b7088b4a115072439b3b42bd5696/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=c1a8520419b7b7088b4a115072439b3b42bd5696",
            "patch": "@@ -780,6 +780,9 @@ def forward(\n             )\n             use_cache = False\n \n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n "
        },
        {
            "sha": "cc05baca2f044c063bcf7034cd704a0151a4c4a5",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 30,
            "deletions": 60,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1a8520419b7b7088b4a115072439b3b42bd5696/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1a8520419b7b7088b4a115072439b3b42bd5696/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=c1a8520419b7b7088b4a115072439b3b42bd5696",
            "patch": "@@ -21,7 +21,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -30,7 +30,7 @@\n from torch.nn import CrossEntropyLoss, LayerNorm\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -549,10 +549,6 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += cache_position[0] + 1\n-\n         if position_embeddings is None:\n             logger.warning_once(\n                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n@@ -646,16 +642,6 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-\n         # Because the input can be padded, the absolute sequence length depends on the max position id.\n         if position_embeddings is None:\n             logger.warning_once(\n@@ -784,9 +770,6 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n         if position_embeddings is None:\n             logger.warning_once(\n                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n@@ -1116,6 +1099,10 @@ def forward(\n                 )\n                 use_cache = False\n \n+        # torch.jit.trace() doesn't support cache objects in the output\n+        if use_cache and past_key_values is None and not torch.jit.is_tracing():\n+            past_key_values = DynamicCache()\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -1428,7 +1415,7 @@ def __init__(self, config):\n         self.model = Qwen2VLModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-        self.padding_side = \"left\"  # set it to left by default, user can use setter to change padding_sides\n+        self.rope_deltas = None  # cache rope_deltas here\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1507,7 +1494,7 @@ def get_rope_index(\n         video_token_id = self.config.video_token_id\n         vision_start_token_id = self.config.vision_start_token_id\n         mrope_position_deltas = []\n-        if image_grid_thw is not None or video_grid_thw is not None:\n+        if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):\n             total_input_ids = input_ids\n             if attention_mask is None:\n                 attention_mask = torch.ones_like(total_input_ids)\n@@ -1600,25 +1587,6 @@ def get_rope_index(\n \n             return position_ids, mrope_position_deltas\n \n-    def _update_model_kwargs_for_generation(\n-        self,\n-        outputs: ModelOutput,\n-        model_kwargs: Dict[str, Any],\n-        is_encoder_decoder: bool = False,\n-        num_new_tokens: int = 1,\n-    ) -> Dict[str, Any]:\n-        model_kwargs = super()._update_model_kwargs_for_generation(\n-            outputs=outputs,\n-            model_kwargs=model_kwargs,\n-            is_encoder_decoder=is_encoder_decoder,\n-            num_new_tokens=num_new_tokens,\n-        )\n-\n-        if getattr(outputs, \"rope_deltas\", None) is not None:\n-            model_kwargs[\"rope_deltas\"] = outputs.rope_deltas\n-\n-        return model_kwargs\n-\n     @add_start_docstrings_to_model_forward(QWEN2_VL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Qwen2VLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1638,6 +1606,7 @@ def forward(\n         image_grid_thw: Optional[torch.LongTensor] = None,\n         video_grid_thw: Optional[torch.LongTensor] = None,\n         rope_deltas: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Qwen2VLCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1726,8 +1695,24 @@ def forward(\n             if attention_mask is not None:\n                 attention_mask = attention_mask.to(inputs_embeds.device)\n \n-        if position_ids is None and input_ids is not None:\n-            position_ids, _ = self.get_rope_index(input_ids, image_grid_thw, video_grid_thw, attention_mask)\n+        # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme\n+        if position_ids is None and input_ids is not None and (attention_mask is None or attention_mask.ndim == 2):\n+            # calculate RoPE index once per generation in the pre-fill stage only\n+            if (cache_position is not None and cache_position[0] == 0) or self.rope_deltas is None:\n+                position_ids, rope_deltas = self.get_rope_index(\n+                    input_ids, image_grid_thw, video_grid_thw, attention_mask\n+                )\n+                self.rope_deltas = rope_deltas\n+            # then use the prev pre-calculated rope-deltas to get the correct position ids\n+            else:\n+                batch_size, seq_length, _ = inputs_embeds.shape\n+                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0\n+                position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n+                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n+                if cache_position is not None:  # otherwise `deltas` is an int `0`\n+                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n+                position_ids = position_ids.add(delta)\n+                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n \n         outputs = self.model(\n             input_ids=None,\n@@ -1739,6 +1724,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         hidden_states = outputs[0]\n@@ -1769,7 +1755,7 @@ def forward(\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            rope_deltas=rope_deltas,\n+            rope_deltas=self.rope_deltas,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -1798,22 +1784,6 @@ def prepare_inputs_for_generation(\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]\n \n-        rope_deltas = kwargs.get(\"rope_deltas\", None)\n-        if attention_mask is not None and position_ids is None:\n-            if cache_position is None or (cache_position is not None and cache_position[0] == 0):\n-                position_ids, rope_deltas = self.get_rope_index(\n-                    input_ids, image_grid_thw, video_grid_thw, attention_mask\n-                )\n-            else:\n-                batch_size, seq_length = input_ids.shape\n-                delta = (\n-                    cache_position[0] + rope_deltas if cache_position is not None and rope_deltas is not None else 0\n-                )\n-                position_ids = torch.arange(seq_length, device=input_ids.device)\n-                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n-                position_ids = position_ids.add(delta)\n-                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n-\n         if cache_position[0] != 0:\n             pixel_values = None\n             pixel_values_videos = None\n@@ -1854,7 +1824,7 @@ def prepare_inputs_for_generation(\n                 \"pixel_values_videos\": pixel_values_videos,\n                 \"image_grid_thw\": image_grid_thw,\n                 \"video_grid_thw\": video_grid_thw,\n-                \"rope_deltas\": rope_deltas,\n+                \"cache_position\": cache_position,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "6c9a4801b65bba9f411905edee8e424fd0aa1949",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1a8520419b7b7088b4a115072439b3b42bd5696/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1a8520419b7b7088b4a115072439b3b42bd5696/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=c1a8520419b7b7088b4a115072439b3b42bd5696",
            "patch": "@@ -1531,6 +1531,14 @@ def test_past_key_values_format(self):\n             embed_dim = getattr(text_config, \"d_model\", text_config.hidden_size)\n             per_head_embed_dim = embed_dim // num_attention_heads\n \n+            # some models have diffent num-head for query vs key/value so we need to assign correct value\n+            # BUT only after `per_head_embed_dim` is set\n+            num_attention_heads = (\n+                text_config.num_key_value_heads\n+                if getattr(text_config, \"num_key_value_heads\", None) is not None\n+                else num_attention_heads\n+            )\n+\n             past_kv = outputs[\"past_key_values\"]\n             self.assertEqual(len(past_kv), num_hidden_layers)\n "
        },
        {
            "sha": "93ed33ae77445890a289786aba6d877f61c86530",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1a8520419b7b7088b4a115072439b3b42bd5696/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1a8520419b7b7088b4a115072439b3b42bd5696/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=c1a8520419b7b7088b4a115072439b3b42bd5696",
            "patch": "@@ -333,6 +333,10 @@ def test_beam_search_low_memory(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n+    @unittest.skip(reason=\"Can't compile fullgraph due to dynamic control flow in `prepare_inputs_for_generate`\")\n+    def test_generate_compile_fullgraph(self):\n+        pass\n+\n \n @require_torch\n class Qwen2VLIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "fe06e223586bf35fac3c6718c19b82d7371d0367",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c1a8520419b7b7088b4a115072439b3b42bd5696/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c1a8520419b7b7088b4a115072439b3b42bd5696/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=c1a8520419b7b7088b4a115072439b3b42bd5696",
            "patch": "@@ -2343,7 +2343,8 @@ def recursive_check(tuple_object, dict_object):\n                             recursive_check(tuple_iterable_value, dict_iterable_value)\n                     elif tuple_object is None:\n                         return\n-                    else:\n+                    # model might return non-tensors objects (e.g. Cache class)\n+                    elif isinstance(tuple_object, torch.Tensor):\n                         self.assertTrue(\n                             torch.allclose(\n                                 set_nan_tensor_to_zero(tuple_object), set_nan_tensor_to_zero(dict_object), atol=1e-5"
        }
    ],
    "stats": {
        "total": 121,
        "additions": 57,
        "deletions": 64
    }
}