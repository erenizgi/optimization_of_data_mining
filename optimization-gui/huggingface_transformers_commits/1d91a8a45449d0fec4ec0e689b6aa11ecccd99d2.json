{
    "author": "cyyever",
    "message": "Use max/min (#41280)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2",
    "files": [
        {
            "sha": "1025fdf75fb46a0d350359a66c09315bd42effd3",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/tokenization_gptsan_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py?ref=1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2",
            "patch": "@@ -495,7 +495,7 @@ def checku2e(x):\n                         candidates.append((self.vocab[wd], wd, e))\n             if len(candidates) > 0:\n                 # the smallest token_id is adopted\n-                _, wd, e = sorted(candidates, key=lambda x: x[0])[0]\n+                _, wd, e = min(candidates, key=lambda x: x[0])\n                 result.append(wd)\n                 pos = e\n             else:"
        },
        {
            "sha": "584e74a8123e7cfdf31c4738a656a8417085e9a1",
            "filename": "src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py?ref=1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2",
            "patch": "@@ -318,7 +318,7 @@ def checku2e(x):\n                         candidates.append((self.vocab[wd], wd, e))\n             if len(candidates) > 0:\n                 # the smallest token_id is adopted\n-                _, wd, e = sorted(candidates, key=lambda x: x[0])[0]\n+                _, wd, e = min(candidates, key=lambda x: x[0])\n                 result.append(wd)\n                 pos = e\n             else:"
        },
        {
            "sha": "633a7fdee46c90546c9ccd54b993eb30c982f769",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py?ref=1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2",
            "patch": "@@ -169,10 +169,10 @@ def get_min_tile_covering_grid(\n \n     if sufficient_covering_grids:\n         # Prefer fewer tiles and higher covering ratio\n-        return sorted(sufficient_covering_grids, key=lambda x: (x[0][0] * x[0][1], -x[1]))[0][0]\n+        return min(sufficient_covering_grids, key=lambda x: (x[0][0] * x[0][1], -x[1]))[0]\n     else:\n         # Fallback: prefer higher covering even if below threshold\n-        return sorted(evaluated_grids, key=lambda x: (-x[1], x[0][0] * x[0][1]))[0][0]\n+        return min(evaluated_grids, key=lambda x: (-x[1], x[0][0] * x[0][1]))[0]\n \n \n class Ovis2ImageProcessor(BaseImageProcessor):"
        },
        {
            "sha": "f388f46cb9d8f11ccdca1e97d9266fc9ea59b324",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2",
            "patch": "@@ -1789,7 +1789,7 @@ def from_pretrained(\n                 if \"tekken.json\" in valid_tokenizer_files:\n                     tokenizer_file = \"tekken.json\"\n                 else:\n-                    tokenizer_file = sorted(valid_tokenizer_files)[-1]\n+                    tokenizer_file = max(valid_tokenizer_files)\n                 logger.warning(\n                     f\"Multiple tokenizer files found in directory: {pretrained_model_name_or_path}. Using {tokenizer_file}.\"\n                 )"
        },
        {
            "sha": "f5d8a5dfbf6e42dc0f29c29ac7fcd86047733adc",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2",
            "patch": "@@ -3478,7 +3478,7 @@ def test_resume_training_with_randomness(self):\n             checkpoints = [d for d in os.listdir(tmp_dir) if d.startswith(\"checkpoint-\")]\n             # There should be one checkpoint per epoch.\n             self.assertEqual(len(checkpoints), 3)\n-            checkpoint_dir = sorted(checkpoints, key=lambda x: int(x.replace(\"checkpoint-\", \"\")))[0]\n+            checkpoint_dir = min(checkpoints, key=lambda x: int(x.replace(\"checkpoint-\", \"\")))\n \n             trainer.train(resume_from_checkpoint=os.path.join(tmp_dir, checkpoint_dir))\n             (a1, b1) = trainer.model.a.item(), trainer.model.b.item()"
        },
        {
            "sha": "ffca3b0d51a94284ddd634ce93da1a62d5a99fb0",
            "filename": "utils/add_pipeline_model_mapping_to_test.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/utils%2Fadd_pipeline_model_mapping_to_test.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/utils%2Fadd_pipeline_model_mapping_to_test.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fadd_pipeline_model_mapping_to_test.py?ref=1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2",
            "patch": "@@ -134,7 +134,7 @@ def find_test_class(test_file):\n             break\n     # Take the test class with the shortest name (just a heuristic)\n     if target_test_class is None and len(test_classes) > 0:\n-        target_test_class = sorted(test_classes, key=lambda x: (len(x.__name__), x.__name__))[0]\n+        target_test_class = min(test_classes, key=lambda x: (len(x.__name__), x.__name__))\n \n     return target_test_class\n "
        },
        {
            "sha": "4cdb30ff2b40933d009b12748b4e74476ed60586",
            "filename": "utils/create_dummy_models.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/utils%2Fcreate_dummy_models.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/utils%2Fcreate_dummy_models.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcreate_dummy_models.py?ref=1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2",
            "patch": "@@ -389,7 +389,7 @@ def get_tiny_config(config_class, model_class=None, **model_tester_kwargs):\n             # This is to avoid `T5EncoderOnlyModelTest` is used instead of `T5ModelTest`, which has\n             # `is_encoder_decoder=False` and causes some pipeline tests failing (also failures in `Optimum` CI).\n             # TODO: More fine grained control of the desired tester class.\n-            model_tester_class = sorted(tester_classes, key=lambda x: (len(x.__name__), x.__name__))[0]\n+            model_tester_class = min(tester_classes, key=lambda x: (len(x.__name__), x.__name__))\n     except ModuleNotFoundError:\n         error = f\"Tiny config not created for {model_type} - cannot find the testing module from the model name.\"\n         raise ValueError(error)"
        },
        {
            "sha": "faf25f9e5c3be4dabd0a4c436329451affe1d7e7",
            "filename": "utils/deprecate_models.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/utils%2Fdeprecate_models.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2/utils%2Fdeprecate_models.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fdeprecate_models.py?ref=1d91a8a45449d0fec4ec0e689b6aa11ecccd99d2",
            "patch": "@@ -37,7 +37,7 @@ def get_last_stable_minor_release():\n     last_stable_minor_releases = [\n         release for release in release_data[\"releases\"] if release.startswith(last_major_minor)\n     ]\n-    last_stable_release = sorted(last_stable_minor_releases, key=version.parse)[-1]\n+    last_stable_release = max(last_stable_minor_releases, key=version.parse)\n \n     return last_stable_release\n "
        }
    ],
    "stats": {
        "total": 18,
        "additions": 9,
        "deletions": 9
    }
}