{
    "author": "Cyrilvallez",
    "message": "Fix configs and doc for the Qwens (#38808)\n\nfix doc and configs",
    "sha": "5f59a9b43991d7be4859f8285ade3e1674878654",
    "files": [
        {
            "sha": "fcba4b4a66f89853ce033c412a3b4c5f6af0f272",
            "filename": "src/transformers/models/qwen2/configuration_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py?ref=5f59a9b43991d7be4859f8285ade3e1674878654",
            "patch": "@@ -109,7 +109,8 @@ class Qwen2Config(PretrainedConfig):\n         sliding_window (`int`, *optional*, defaults to 4096):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 28):\n-            The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+            The number of layers using full attention. The first `max_window_layers` layers will use full attention, while any\n+            additional layer afterwards will use SWA (Sliding Window Attention).\n         layer_types (`list`, *optional*):\n             Attention pattern for each layer.\n         attention_dropout (`float`, *optional*, defaults to 0.0):"
        },
        {
            "sha": "197ab8e78aacf5e429cc3886d809edb41f69e780",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=5f59a9b43991d7be4859f8285ade3e1674878654",
            "patch": "@@ -256,7 +256,8 @@ class Qwen2_5OmniTextConfig(PretrainedConfig):\n         sliding_window (`int`, *optional*, defaults to 32768):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 28):\n-            The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+            The number of layers using full attention. The first `max_window_layers` layers will use full attention, while any\n+            additional layer afterwards will use SWA (Sliding Window Attention).\n         layer_types (`list`, *optional*):\n             Attention pattern for each layer.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -606,7 +607,8 @@ class Qwen2_5OmniTalkerConfig(PretrainedConfig):\n         sliding_window (`int`, *optional*, defaults to 32768):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 28):\n-            The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+            The number of layers using full attention. The first `max_window_layers` layers will use full attention, while any\n+            additional layer afterwards will use SWA (Sliding Window Attention).\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_scaling (`Dict`, *optional*):"
        },
        {
            "sha": "2f18921df2c9644af6cacf649597a4900d1f5f5f",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=5f59a9b43991d7be4859f8285ade3e1674878654",
            "patch": "@@ -295,7 +295,8 @@ class Qwen2_5OmniTextConfig(PretrainedConfig):\n         sliding_window (`int`, *optional*, defaults to 32768):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 28):\n-            The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+            The number of layers using full attention. The first `max_window_layers` layers will use full attention, while any\n+            additional layer afterwards will use SWA (Sliding Window Attention).\n         layer_types (`list`, *optional*):\n             Attention pattern for each layer.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n@@ -645,7 +646,8 @@ class Qwen2_5OmniTalkerConfig(PretrainedConfig):\n         sliding_window (`int`, *optional*, defaults to 32768):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 28):\n-            The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+            The number of layers using full attention. The first `max_window_layers` layers will use full attention, while any\n+            additional layer afterwards will use SWA (Sliding Window Attention).\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_scaling (`Dict`, *optional*):"
        },
        {
            "sha": "be91b5b4ee86a95db73312385b3241005d41e4cc",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=5f59a9b43991d7be4859f8285ade3e1674878654",
            "patch": "@@ -116,7 +116,8 @@ class Qwen2_5_VLTextConfig(PretrainedConfig):\n         sliding_window (`int`, *optional*, defaults to 4096):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 80):\n-            The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+            The number of layers using full attention. The first `max_window_layers` layers will use full attention, while any\n+            additional layer afterwards will use SWA (Sliding Window Attention).\n         layer_types (`list`, *optional*):\n             Attention pattern for each layer.\n         attention_dropout (`float`, *optional*, defaults to 0.0):"
        },
        {
            "sha": "c279463e9d6b2ce693049102f18be627e722217d",
            "filename": "src/transformers/models/qwen2_moe/configuration_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py?ref=5f59a9b43991d7be4859f8285ade3e1674878654",
            "patch": "@@ -108,7 +108,8 @@ class Qwen2MoeConfig(PretrainedConfig):\n         sliding_window (`int`, *optional*, defaults to 4096):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 28):\n-            The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+            The number of layers using full attention. The first `max_window_layers` layers will use full attention, while any\n+            additional layer afterwards will use SWA (Sliding Window Attention).\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         decoder_sparse_step (`int`, *optional*, defaults to 1):"
        },
        {
            "sha": "db1c089608b7cf3fd2a9fe06c25972c8afe5f577",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=5f59a9b43991d7be4859f8285ade3e1674878654",
            "patch": "@@ -105,7 +105,8 @@ class Qwen2VLTextConfig(PretrainedConfig):\n         sliding_window (`int`, *optional*, defaults to 4096):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 80):\n-            The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+            The number of layers using full attention. The first `max_window_layers` layers will use full attention, while any\n+            additional layer afterwards will use SWA (Sliding Window Attention).\n         layer_types (`list`, *optional*):\n             Attention pattern for each layer.\n         attention_dropout (`float`, *optional*, defaults to 0.0):"
        },
        {
            "sha": "032c611f86764712c1826f4e8f9ecc04e728c1d4",
            "filename": "src/transformers/models/qwen3/configuration_qwen3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py?ref=5f59a9b43991d7be4859f8285ade3e1674878654",
            "patch": "@@ -113,7 +113,8 @@ class Qwen3Config(PretrainedConfig):\n         sliding_window (`int`, *optional*, defaults to 4096):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 28):\n-            The number of layers that use SWA (Sliding Window Attention). The top layers use SWA while the bottom ones use full attention.\n+            The number of layers using full attention. The first `max_window_layers` layers will use full attention, while any\n+            additional layer afterwards will use SWA (Sliding Window Attention).\n         layer_types (`list`, *optional*):\n             Attention pattern for each layer.\n         attention_dropout (`float`, *optional*, defaults to 0.0):"
        },
        {
            "sha": "e0d6978047df80550c24fcdcbcef8001ae51f234",
            "filename": "src/transformers/models/qwen3_moe/configuration_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f59a9b43991d7be4859f8285ade3e1674878654/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py?ref=5f59a9b43991d7be4859f8285ade3e1674878654",
            "patch": "@@ -109,8 +109,6 @@ class Qwen3MoeConfig(PretrainedConfig):\n             Whether to use sliding window attention.\n         sliding_window (`int`, *optional*, defaults to 4096):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n-        max_window_layers (`int`, *optional*, defaults to 28):\n-            The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         decoder_sparse_step (`int`, *optional*, defaults to 1):\n@@ -187,7 +185,6 @@ def __init__(\n         attention_bias=False,\n         use_sliding_window=False,\n         sliding_window=4096,\n-        max_window_layers=28,\n         attention_dropout=0.0,\n         decoder_sparse_step=1,\n         moe_intermediate_size=768,\n@@ -207,7 +204,6 @@ def __init__(\n         self.num_attention_heads = num_attention_heads\n         self.use_sliding_window = use_sliding_window\n         self.sliding_window = sliding_window if use_sliding_window else None\n-        self.max_window_layers = max_window_layers\n \n         self.num_key_value_heads = num_key_value_heads\n         self.hidden_act = hidden_act"
        }
    ],
    "stats": {
        "total": 31,
        "additions": 18,
        "deletions": 13
    }
}