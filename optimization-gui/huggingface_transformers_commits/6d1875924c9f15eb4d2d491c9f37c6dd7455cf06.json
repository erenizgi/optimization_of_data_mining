{
    "author": "Szustarol",
    "message": "Fixed loading LongT5 from legacy checkpoints (#40724)\n\n* Fixed loading LongT5 from legacy checkpoints\n\n* Adapted the fix to work with missing lm_head",
    "sha": "6d1875924c9f15eb4d2d491c9f37c6dd7455cf06",
    "files": [
        {
            "sha": "534b148b956d51ecc6e1f24248a76afa2eccbb4e",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d1875924c9f15eb4d2d491c9f37c6dd7455cf06/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d1875924c9f15eb4d2d491c9f37c6dd7455cf06/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=6d1875924c9f15eb4d2d491c9f37c6dd7455cf06",
            "patch": "@@ -1267,6 +1267,36 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n+    def _try_load_missing_tied_module(self, key):\n+        module = self\n+        if key.endswith(\".weight\"):\n+            key = key[: -len(\".weight\")]\n+        for sub_key in key.split(\".\"):\n+            if not hasattr(module, sub_key):\n+                return\n+            module = getattr(module, sub_key)\n+\n+        self._tie_or_clone_weights(module, self.shared)\n+\n+    @classmethod\n+    def from_pretrained(self, *args, **kwargs):\n+        requested_loading_info = kwargs.get(\"output_loading_info\", False)\n+        kwargs[\"output_loading_info\"] = True\n+        model, loading_info = super().from_pretrained(*args, **kwargs)\n+        missing_keys = loading_info.get(\"missing_keys\", [])\n+\n+        if hasattr(model, \"shared\") and hasattr(model, \"_tied_weights_keys\"):\n+            for missing_key in missing_keys:\n+                logger.warning(\n+                    f\"Recovering a missing tied weight {missing_key} from a legacy LongT5 checkpoint. \"\n+                    f\"Consider saving {missing_key} in your checkpoint or updating the config (tie_word_embeddings=true).\"\n+                )\n+                model._try_load_missing_tied_module(missing_key)\n+\n+        if requested_loading_info:\n+            return model, loading_info\n+        return model\n+\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor  # Used for testing weights initialization"
        }
    ],
    "stats": {
        "total": 30,
        "additions": 30,
        "deletions": 0
    }
}