{
    "author": "eustlb",
    "message": "[Qwen2Audio] handle input ids expansion during processing (#35534)\n\n* add audio_token attribute to proc\r\n\r\n* expand input_ids\r\n\r\n* and legacy and expanded input_ids\r\n\r\n* test update\r\n\r\n* split lines\r\n\r\n* add possibility not to provide eos and bos audio tokens\r\n\r\n* raise errors\r\n\r\n* test incorrect number of audio tokens\r\n\r\n* add example\r\n\r\n* fmt\r\n\r\n* typo",
    "sha": "7f7677307cfc8722ccc4680b993811098379844c",
    "files": [
        {
            "sha": "2ef947ce430d40d4d055fd9567c141cb61b06c50",
            "filename": "docs/source/en/model_doc/qwen2_audio.md",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f7677307cfc8722ccc4680b993811098379844c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f7677307cfc8722ccc4680b993811098379844c/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md?ref=7f7677307cfc8722ccc4680b993811098379844c",
            "patch": "@@ -34,6 +34,37 @@ The abstract from the paper is the following:\n \n `Qwen2-Audio-7B` and `Qwen2-Audio-7B-Instruct` can be found on the [Huggingface Hub](https://huggingface.co/Qwen)\n \n+### Inference\n+\n+```python\n+from io import BytesIO\n+from urllib.request import urlopen\n+import librosa\n+from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n+\n+model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\", trust_remote_code=True, device_map=\"auto\")\n+processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\", trust_remote_code=True)\n+\n+prompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Generate the caption in English:\"\n+url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Audio/glass-breaking-151256.mp3\"\n+audio, sr = librosa.load(BytesIO(urlopen(url).read()), sr=processor.feature_extractor.sampling_rate)\n+inputs = processor(text=prompt, audios=audio, return_tensors=\"pt\").to(model.device)\n+\n+generate_ids = model.generate(**inputs, max_length=256)\n+generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n+\n+response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+\n+# We can also omit the audio_bos and audio_eos tokens\n+prompt = \"<|AUDIO|>Generate the caption in English:\"\n+inputs = processor(text=prompt, audios=audio, return_tensors=\"pt\").to(model.device)\n+\n+generate_ids = model.generate(**inputs, max_length=256)\n+generate_ids = generate_ids[:, inputs.input_ids.size(1):]\n+\n+response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+```\n+\n In the following, we demonstrate how to use `Qwen2-Audio-7B-Instruct` for the inference, supporting both voice chat and audio analysis modes. Note that we have used the ChatML format for dialog, in this demo we show how to leverage `apply_chat_template` for this purpose.\n \n ### Voice Chat Inference"
        },
        {
            "sha": "421bb3801dfd37aa590023d65aa919f3e4c43ce0",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 28,
            "deletions": 3,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f7677307cfc8722ccc4680b993811098379844c/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f7677307cfc8722ccc4680b993811098379844c/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=7f7677307cfc8722ccc4680b993811098379844c",
            "patch": "@@ -1197,9 +1197,34 @@ def forward(\n                 selected_audio_feature = audio_outputs.last_hidden_state\n                 audio_features = self.multi_modal_projector(selected_audio_feature)\n \n-                inputs_embeds, attention_mask, labels, position_ids, _ = self._merge_input_ids_with_audio_features(\n-                    audio_features, audio_output_lengths, inputs_embeds, input_ids, attention_mask, labels\n-                )\n+                # if we have consecutive audio tokens, then it means we expanded input_ids in processing\n+                audio_tokens = input_ids == self.config.audio_token_index\n+                legacy_processing = (audio_tokens[:, :-1] & audio_tokens[:, 1:]).sum() == 0\n+\n+                if legacy_processing:\n+                    logger.warning_once(\n+                        \"Expanding inputs for audio tokens in Qwen2Audio should be done in processing.\"\n+                    )\n+                    inputs_embeds, attention_mask, labels, position_ids, _ = self._merge_input_ids_with_audio_features(\n+                        audio_features, audio_output_lengths, inputs_embeds, input_ids, attention_mask, labels\n+                    )\n+                else:\n+                    num_audios, max_audio_tokens, embed_dim = audio_features.shape\n+                    audio_features_mask = torch.arange(max_audio_tokens, device=audio_output_lengths.device)[None, :]\n+                    audio_features_mask = audio_features_mask < audio_output_lengths[:, None]\n+                    audio_features = audio_features[audio_features_mask]\n+\n+                    n_audio_tokens = (input_ids == self.config.audio_token_index).sum().item()\n+                    n_audio_features = audio_features.shape[0]\n+\n+                    if n_audio_tokens != n_audio_features:\n+                        raise ValueError(\n+                            f\"Audio features and audio tokens do not match: tokens: {n_audio_tokens}, features {n_audio_features}\"\n+                        )\n+                    special_audio_mask = (input_ids == self.config.audio_token_index).to(inputs_embeds.device)\n+                    special_audio_mask = special_audio_mask.unsqueeze(-1).expand_as(inputs_embeds)\n+                    audio_features = audio_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+                    inputs_embeds = inputs_embeds.masked_scatter(special_audio_mask, audio_features)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,"
        },
        {
            "sha": "5eee95398b363d41219aef934df59cc09e0c2d99",
            "filename": "src/transformers/models/qwen2_audio/processing_qwen2_audio.py",
            "status": "modified",
            "additions": 70,
            "deletions": 2,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f7677307cfc8722ccc4680b993811098379844c/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f7677307cfc8722ccc4680b993811098379844c/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py?ref=7f7677307cfc8722ccc4680b993811098379844c",
            "patch": "@@ -40,15 +40,32 @@ class Qwen2AudioProcessor(ProcessorMixin):\n         chat_template (`Optional[str]`, *optional*):\n                 The Jinja template to use for formatting the conversation. If not provided, the default chat template\n                 is used.\n+        audio_token (`str`, *optional*, defaults to `\"<|AUDIO|>\"`):\n+            The token to use for audio tokens.\n+        audio_bos_token (`str`, *optional*, defaults to `\"<|audio_bos|>\"`):\n+            The token to use for audio bos tokens.\n+        audio_eos_token (`str`, *optional*, defaults to `\"<|audio_eos|>\"`):\n+            The token to use for audio eos tokens.\n     \"\"\"\n \n     attributes = [\"feature_extractor\", \"tokenizer\"]\n     feature_extractor_class = \"WhisperFeatureExtractor\"\n     tokenizer_class = \"AutoTokenizer\"\n \n-    def __init__(self, feature_extractor=None, tokenizer=None, chat_template=None):\n+    def __init__(\n+        self,\n+        feature_extractor=None,\n+        tokenizer=None,\n+        chat_template=None,\n+        audio_token=\"<|AUDIO|>\",\n+        audio_bos_token=\"<|audio_bos|>\",\n+        audio_eos_token=\"<|audio_eos|>\",\n+    ):\n         if chat_template is None:\n             chat_template = self.default_chat_template\n+        self.audio_token = tokenizer.audio_token if hasattr(tokenizer, \"audio_token\") else audio_token\n+        self.audio_bos_token = tokenizer.audio_bos_token if hasattr(tokenizer, \"audio_bos_token\") else audio_bos_token\n+        self.audio_eos_token = tokenizer.audio_eos_token if hasattr(tokenizer, \"audio_eos_token\") else audio_eos_token\n         super().__init__(feature_extractor, tokenizer, chat_template=chat_template)\n \n     def __call__(\n@@ -88,7 +105,18 @@ def __call__(\n \n         if text is None:\n             raise ValueError(\"You need to specify either a `text` input to process.\")\n-        inputs = self.tokenizer(text, padding=padding, **kwargs)\n+        elif isinstance(text, str):\n+            text = [text]\n+        elif not isinstance(text, list) and not isinstance(text[0], str):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        # ensure we have as much audios as audio tokens\n+        num_audio_tokens = sum(sample.count(self.audio_token) for sample in text)\n+        num_audios = 1 if type(audios) == np.ndarray else len(audios)\n+        if num_audio_tokens != num_audios:\n+            raise ValueError(\n+                f\"Found {num_audio_tokens} {self.audio_token} token{'s' if num_audio_tokens > 1 else ''} in provided text but received {num_audios} audio{'s' if num_audios > 1 else ''}\"\n+            )\n \n         if audios is not None:\n             audio_inputs = self.feature_extractor(\n@@ -97,6 +125,46 @@ def __call__(\n             audio_inputs[\"feature_attention_mask\"] = audio_inputs.pop(\n                 \"attention_mask\"\n             )  # rename attention_mask to prevent conflicts later on\n+\n+            expanded_text = []\n+            audio_lengths = audio_inputs[\"feature_attention_mask\"].sum(-1).tolist()\n+\n+            for sample in text:\n+                replace_str = []\n+                while self.audio_token in sample:\n+                    audio_length = audio_lengths.pop(0)\n+                    input_length = (audio_length - 1) // 2 + 1\n+                    num_audio_tokens = (input_length - 2) // 2 + 1\n+\n+                    expanded_audio_token = self.audio_token * num_audio_tokens\n+\n+                    audio_token_start_idx = sample.find(self.audio_token)\n+                    audio_token_end_idx = audio_token_start_idx + len(self.audio_token)\n+\n+                    has_bos = (\n+                        sample[audio_token_start_idx - len(self.audio_bos_token) : audio_token_start_idx]\n+                        == self.audio_bos_token\n+                    )\n+                    has_eos = (\n+                        sample[audio_token_end_idx : audio_token_end_idx + len(self.audio_eos_token)]\n+                        == self.audio_eos_token\n+                    )\n+\n+                    # Check if this audio token is surrounded by bos/eos tokens\n+                    if not has_bos and not has_eos:\n+                        expanded_audio_token = self.audio_bos_token + expanded_audio_token + self.audio_eos_token\n+\n+                    replace_str.append(expanded_audio_token)\n+                    sample = sample.replace(self.audio_token, \"<placeholder>\", 1)\n+\n+                while \"<placeholder>\" in sample:\n+                    sample = sample.replace(\"<placeholder>\", replace_str.pop(0), 1)\n+                expanded_text.append(sample)\n+            text = expanded_text\n+\n+        inputs = self.tokenizer(text, padding=padding, **kwargs)\n+\n+        if audios is not None:\n             inputs.update(audio_inputs)\n \n         return BatchFeature(data={**inputs})"
        },
        {
            "sha": "ef8def3caef29c923d9b668615973f3047bbe0e3",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 30,
            "deletions": 43,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f7677307cfc8722ccc4680b993811098379844c/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f7677307cfc8722ccc4680b993811098379844c/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=7f7677307cfc8722ccc4680b993811098379844c",
            "patch": "@@ -49,7 +49,7 @@ def __init__(\n         parent,\n         ignore_index=-100,\n         audio_token_index=0,\n-        seq_length=7,\n+        seq_length=25,\n         feat_seq_length=60,\n         text_config={\n             \"model_type\": \"qwen2\",\n@@ -91,7 +91,7 @@ def __init__(\n         self.is_training = is_training\n \n         self.batch_size = 3\n-        self.encoder_seq_length = audio_config[\"max_source_positions\"] // 2 + seq_length - 1\n+        self.encoder_seq_length = seq_length\n \n     def get_config(self):\n         return Qwen2AudioConfig(\n@@ -116,11 +116,13 @@ def prepare_config_and_inputs(self):\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         config, input_features_values, feature_attention_mask = config_and_inputs\n+        input_length = (input_features_values.shape[-1] - 1) // 2 + 1\n+        num_audio_tokens = (input_length - 2) // 2 + 1\n         input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n         attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(torch_device)\n         attention_mask[:, :1] = 0\n         # we are giving 3 audios let's make sure we pass in 3 audios tokens\n-        input_ids[:, 1] = config.audio_token_index\n+        input_ids[:, 1 : 1 + num_audio_tokens] = config.audio_token_index\n         inputs_dict = {\n             \"input_features\": input_features_values,\n             \"feature_attention_mask\": feature_attention_mask,\n@@ -237,54 +239,39 @@ def test_small_model_integration_test_single(self):\n \n         output = model.generate(**inputs, max_new_tokens=32)\n \n-        EXPECTED_INPUT_IDS = torch.tensor(\n-            [\n-                [\n-                    151644,\n-                    8948,\n-                    198,\n-                    2610,\n-                    525,\n-                    264,\n-                    10950,\n-                    17847,\n-                    13,\n-                    151645,\n-                    198,\n-                    151644,\n-                    872,\n-                    198,\n-                    14755,\n-                    220,\n-                    16,\n-                    25,\n-                    220,\n-                    151647,\n-                    151646,\n-                    151648,\n-                    198,\n-                    3838,\n-                    594,\n-                    429,\n-                    5112,\n-                    30,\n-                    151645,\n-                    198,\n-                    151644,\n-                    77091,\n-                    198,\n-                ]\n-            ]\n-        )\n+        # fmt: off\n+        EXPECTED_INPUT_IDS = torch.tensor([[\n+            151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 14755, 220, 16, 25, 220, 151647,\n+            *[151646] * 101,\n+            151648, 198, 3838, 594, 429, 5112, 30, 151645, 198, 151644, 77091, 198,\n+        ]])\n+        # fmt: on\n         self.assertTrue(torch.equal(inputs[\"input_ids\"], EXPECTED_INPUT_IDS))\n \n-        EXPECTED_DECODED_TEXT = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nAudio 1: <|audio_bos|><|AUDIO|><|audio_eos|>\\nWhat's that sound?<|im_end|>\\n<|im_start|>assistant\\nIt is the sound of glass breaking.<|im_end|>\"\n+        EXPECTED_DECODED_TEXT = (\n+            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nAudio 1: <|audio_bos|>\"\n+            + \"<|AUDIO|>\" * 101\n+            + \"<|audio_eos|>\\nWhat's that sound?<|im_end|>\\n<|im_start|>assistant\\nIt is the sound of glass breaking.<|im_end|>\"\n+        )\n \n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=False),\n             EXPECTED_DECODED_TEXT,\n         )\n \n+        # test the error when incorrect number of audio tokens\n+        # fmt: off\n+        inputs[\"input_ids\"] = torch.tensor([[\n+            151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 14755, 220, 16, 25, 220, 151647,\n+            *[151646] * 200,\n+            151648, 198, 3838, 594, 429, 5112, 30, 151645, 198, 151644, 77091, 198,\n+        ]])\n+        # fmt: on\n+        with self.assertRaisesRegex(\n+            ValueError, \"Audio features and audio tokens do not match: tokens: 200, features 101\"\n+        ):\n+            model.generate(**inputs, max_new_tokens=32)\n+\n     @slow\n     def test_small_model_integration_test_batch(self):\n         # Let' s make sure we test the preprocessing to replace what is used"
        }
    ],
    "stats": {
        "total": 207,
        "additions": 159,
        "deletions": 48
    }
}