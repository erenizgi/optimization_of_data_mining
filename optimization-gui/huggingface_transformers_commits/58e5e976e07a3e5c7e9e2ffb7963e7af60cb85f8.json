{
    "author": "Cyrilvallez",
    "message": "Small fix on context manager detection (#37562)\n\n* small fixes\n\n* Update modeling_utils.py\n\n* test\n\n* Update test_modeling_common.py\n\n* Update test_modeling_timm_backbone.py\n\n* more general\n\n* simpler",
    "sha": "58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8",
    "files": [
        {
            "sha": "da2f07c0a66bf30eb688dcce375a01a66e0d817e",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 9,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8",
            "patch": "@@ -4167,15 +4167,14 @@ def from_pretrained(\n             _adapter_model_path = None\n \n         # Potentially detect context manager or global device, and use it (only if no device_map was provided)\n-        if device_map is None:\n+        if device_map is None and not is_deepspeed_zero3_enabled():\n             device_in_context = get_torch_context_manager_or_global_device()\n             if device_in_context == torch.device(\"meta\"):\n-                raise ValueError(\n-                    (\n-                        \"`from_pretrained` is not compatible with a meta device context manager or `torch.set_default_device('meta')` \"\n-                        \"as its purpose is to load weights. If you want to initialize a model on the meta device, use the context manager \"\n-                        \"or global device with `from_config`, or `ModelClass(config)`\"\n-                    )\n+                # TODO Cyril: raise an error instead of the warning in v4.53 (and change the test to check for raise instead of success)\n+                logger.warning(\n+                    \"We detected that you are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`\\n\"\n+                    \"This is an anti-pattern and will raise an Error in version v4.53\\nIf you want to initialize a model on the meta device, use \"\n+                    \"the context manager or global device with `from_config`, or `ModelClass(config)`\"\n                 )\n             device_map = device_in_context\n \n@@ -5834,6 +5833,16 @@ def expand_device_map(device_map, param_names):\n     return new_device_map\n \n \n+def is_accelerator_device(device: Union[str, int, torch.device]) -> bool:\n+    \"\"\"Check if the device is an accelerator. We need to function, as device_map can be \"disk\" as well, which is not\n+    a proper `torch.device`.\n+    \"\"\"\n+    if device == \"disk\":\n+        return False\n+    else:\n+        return torch.device(device).type not in [\"meta\", \"cpu\"]\n+\n+\n def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict, factor=2):\n     \"\"\"This function warm-ups the caching allocator based on the size of the model tensors that will reside on each\n     device. It allows to have one large call to Malloc, instead of recursively calling it later when loading\n@@ -5853,9 +5862,9 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict,\n     - Loading speed bottleneck is now almost only tensor copy (i.e. changing the dtype) and moving the tensors to the devices.\n     However, we cannot really improve on those aspects obviously, as the data needs to be moved/copied in the end.\n     \"\"\"\n-    # Remove disk and cpu devices, and cast to proper torch.device\n+    # Remove disk, cpu and meta devices, and cast to proper torch.device\n     accelerator_device_map = {\n-        param: torch.device(device) for param, device in expanded_device_map.items() if device not in [\"cpu\", \"disk\"]\n+        param: torch.device(device) for param, device in expanded_device_map.items() if is_accelerator_device(device)\n     }\n     if not len(accelerator_device_map):\n         return"
        },
        {
            "sha": "70ecffcf51ea9afa07baf52b32181d9a803b9871",
            "filename": "src/transformers/models/deprecated/nat/modeling_nat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py?ref=58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8",
            "patch": "@@ -545,7 +545,7 @@ def __init__(self, config):\n         super().__init__()\n         self.num_levels = len(config.depths)\n         self.config = config\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")]\n         self.levels = nn.ModuleList(\n             [\n                 NatStage("
        },
        {
            "sha": "1da03cb544d467d9fdbe9b5258fabaccdedd7eff",
            "filename": "src/transformers/models/deprecated/van/modeling_van.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py?ref=58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8",
            "patch": "@@ -311,7 +311,9 @@ def __init__(self, config: VanConfig):\n         hidden_sizes = config.hidden_sizes\n         depths = config.depths\n         mlp_ratios = config.mlp_ratios\n-        drop_path_rates = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n+        drop_path_rates = [\n+            x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")\n+        ]\n \n         for num_stage, (patch_size, stride, hidden_size, depth, mlp_expantion, drop_path_rate) in enumerate(\n             zip(patch_sizes, strides, hidden_sizes, depths, mlp_ratios, drop_path_rates)"
        },
        {
            "sha": "8837372a84c463af5a92ad23627c9336ec8be24c",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8",
            "patch": "@@ -553,7 +553,7 @@ def __init__(self, config):\n         super().__init__()\n         self.num_levels = len(config.depths)\n         self.config = config\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")]\n         self.levels = nn.ModuleList(\n             [\n                 DinatStage("
        },
        {
            "sha": "d060ab38886ad08e96bb72a6dd5c0fb691109036",
            "filename": "tests/models/timm_backbone/test_modeling_timm_backbone.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py?ref=58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8",
            "patch": "@@ -177,7 +177,7 @@ def test_can_load_with_global_device_set(self):\n         pass\n \n     @unittest.skip(reason=\"TimmBackbone uses its own `from_pretrained` without device_map support\")\n-    def test_cannot_load_with_meta_device_context_manager(self):\n+    def test_can_load_with_meta_device_context_manager(self):\n         pass\n \n     @unittest.skip(reason=\"model weights aren't tied in TimmBackbone.\")"
        },
        {
            "sha": "fca89147f4288ab3fe72fc84fbf2c78fdfa5b2d0",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 12,
            "deletions": 5,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=58e5e976e07a3e5c7e9e2ffb7963e7af60cb85f8",
            "patch": "@@ -4590,7 +4590,7 @@ def test_can_load_with_global_device_set(self):\n                 unique_devices, {device}, f\"All parameters should be on {device}, but found {unique_devices}.\"\n             )\n \n-    def test_cannot_load_with_meta_device_context_manager(self):\n+    def test_can_load_with_meta_device_context_manager(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             # Need to deepcopy here as it is modified in-place in save_pretrained (it sets sdpa for default attn, which\n@@ -4600,10 +4600,17 @@ def test_cannot_load_with_meta_device_context_manager(self):\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n \n-                # This should raise an error with meta device\n-                with self.assertRaises(ValueError, msg=\"`from_pretrained` is not compatible with a meta device\"):\n-                    with torch.device(\"meta\"):\n-                        _ = model_class.from_pretrained(tmpdirname)\n+                with torch.device(\"meta\"):\n+                    new_model = model_class.from_pretrained(tmpdirname)\n+                unique_devices = {param.device for param in new_model.parameters()} | {\n+                    buffer.device for buffer in new_model.buffers()\n+                }\n+\n+            self.assertEqual(\n+                unique_devices,\n+                {torch.device(\"meta\")},\n+                f\"All parameters should be on meta device, but found {unique_devices}.\",\n+            )\n \n \n global_rng = random.Random()"
        }
    ],
    "stats": {
        "total": 54,
        "additions": 36,
        "deletions": 18
    }
}