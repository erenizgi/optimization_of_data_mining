{
    "author": "3outeille",
    "message": "fix Dtensor and tensor mismatch for Col/RowRep (#42924)\n\n* begin Moe test tensor parallel\n\n* create tiny moe model + fix test tensor parallel Moe\n\neaeaae\n\n* create tiny moe model + fix test tensor parallel Moe\n\neaeaae\n\nfix tensor parallel MoE test\nfix tensor parallel MoE test\n\n* fix backward pass test in tensor parallel for Dense model (#42811)\n\n* fix\n\n* linting\n\n* use mixtral instead for testing\n\n* fix dtensor and tensor mismatch\n\n* linting\n\n* checkout test tensor parallel to be like main\n\n* avoid hack and create class instead\n\n* fix loading ep\n\n* add moe test\n\n* now EP inference works again but pass still fails\n\n* Add ColwiseParallelReplicate and RowwiseParallelReplicate classes for replicated layouts\n\n* clean\n\n* eaza\n\n* aeaeaea\n\n* eaeaa\n\n* linting",
    "sha": "99be81e77cb8aebfc52671611df3144bd2f03dc4",
    "files": [
        {
            "sha": "0e710f2cc3a84ace3529740400509d7986d6252a",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 20,
            "deletions": 2,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/99be81e77cb8aebfc52671611df3144bd2f03dc4/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99be81e77cb8aebfc52671611df3144bd2f03dc4/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=99be81e77cb8aebfc52671611df3144bd2f03dc4",
            "patch": "@@ -767,6 +767,15 @@ def __init__(self, **kwargs):\n         super().__init__(use_dtensor=False, **kwargs)\n \n \n+class ColwiseParallelReplicate(ColwiseParallel):\n+    \"\"\"\n+    Colwise parallel with output layouts replicated.\n+    \"\"\"\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(output_layouts=Replicate(), **kwargs)\n+\n+\n class RowwiseParallel(TensorParallelLayer):\n     \"\"\"\n     Partition a compatible nn.Module in a row-wise fashion. Currently supports nn.Linear and nn.Embedding.\n@@ -940,6 +949,15 @@ def __init__(self, **kwargs):\n         super().__init__(use_dtensor=False, **kwargs)\n \n \n+class RowwiseParallelReplicate(RowwiseParallel):\n+    \"\"\"\n+    Rowwise parallel with input layouts replicated.\n+    \"\"\"\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(input_layouts=Replicate(), **kwargs)\n+\n+\n class SequenceParallel(TensorParallelLayer):\n     \"\"\"\n     SequenceParallel replicates a compatible ``nn.Module`` parameters and runs the sharded computation with\n@@ -1196,8 +1214,8 @@ class ParallelInterface(GeneralInterface):\n         {\n             \"colwise\": ColwiseParallel(),\n             \"rowwise\": RowwiseParallel(),\n-            \"colwise_rep\": ColwiseParallel(output_layouts=Replicate()),\n-            \"rowwise_rep\": RowwiseParallel(input_layouts=Replicate()),\n+            \"colwise_rep\": ColwiseParallelReplicate(),\n+            \"rowwise_rep\": RowwiseParallelReplicate(),\n             \"local_colwise\": LocalColwiseParallel(),\n             \"local_rowwise\": LocalRowwiseParallel(),\n             \"local\": IsolatedParallel(),"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 20,
        "deletions": 2
    }
}