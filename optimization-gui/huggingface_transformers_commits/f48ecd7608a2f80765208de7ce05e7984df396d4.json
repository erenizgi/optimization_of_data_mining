{
    "author": "Cyrilvallez",
    "message": "Fix TP initialization (#35860)\n\n* fix tp\r\n\r\n* Update modeling_utils.py\r\n\r\n* style\r\n\r\n* style\r\n\r\n* Update test_tp.py\r\n\r\n* Update test_tp.py\r\n\r\n* style\r\n\r\n* Update test_tp.py\r\n\r\n* Update test_tp.py\r\n\r\n* Update test_tp.py\r\n\r\n* Update test_tp.py",
    "sha": "f48ecd7608a2f80765208de7ce05e7984df396d4",
    "files": [
        {
            "sha": "b5df36e12a944192686b67794453c275c117ab4d",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 49,
            "deletions": 43,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/f48ecd7608a2f80765208de7ce05e7984df396d4/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f48ecd7608a2f80765208de7ce05e7984df396d4/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=f48ecd7608a2f80765208de7ce05e7984df396d4",
            "patch": "@@ -3443,6 +3443,29 @@ def from_pretrained(\n             # TODO: we can relax this check when we support taking tp_plan from a json file, for example.\n             raise ValueError(f\"tp_plan supports 'auto' only for now but got {tp_plan}.\")\n \n+        if tp_plan is not None and device_map is not None:\n+            raise ValueError(\n+                \"`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization.\"\n+            )\n+\n+        # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple\n+        # `device_map` pointing to the correct device. If we don't, torch will use the default device (index 0) for all\n+        # childs processes at parallelization time, resulting in excessive memory usage on device 0 and OOMs.\n+        # And temporarily setting the default device to current process rank result in the following error\n+        # `torch.distributed.DistBackendError: Attempt to perform collective on tensor not on device passed to init_process_group`\n+        tp_device = None\n+        if tp_plan is not None:\n+            if not torch.distributed.is_initialized():\n+                raise ValueError(\"Tensor Parallel requires torch.distributed to be initialized first.\")\n+\n+            # Detect the accelerator on the machine. If no accelerator is available, it returns CPU.\n+            device_type = torch._C._get_accelerator().type\n+            device_module = torch.get_device_module(device_type)\n+            # Get device with index assuming equal number of devices per host\n+            tp_device = torch.device(device_type, torch.distributed.get_rank() % device_module.device_count())\n+            # This is the easiest way to dispatch to the current process device\n+            device_map = tp_device\n+\n         if is_fsdp_enabled():\n             low_cpu_mem_usage = True\n \n@@ -4090,7 +4113,6 @@ def from_pretrained(\n \n         # Instantiate model.\n         init_contexts = [no_init_weights(_enable=_fast_init)]\n-        tp_device = None\n \n         if is_deepspeed_zero3_enabled() and not is_quantized and not _is_ds_init_called:\n             import deepspeed\n@@ -4106,16 +4128,6 @@ def from_pretrained(\n                     f\"Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n                 )\n             init_contexts.append(init_empty_weights())\n-        elif tp_plan is not None:\n-            if not torch.distributed.is_initialized():\n-                raise ValueError(\"Tensor Parallel requires torch.distributed to be initialized first.\")\n-\n-            # Detect the accelerator on the machine. If no accelerator is available, it returns CPU.\n-            device_type = torch._C._get_accelerator().type\n-            device_module = torch.get_device_module(device_type)\n-            # Get device with index assuming equal number of devices per host\n-            tp_device = torch.device(device_type, torch.distributed.get_rank() % device_module.device_count())\n-            init_contexts.append(tp_device)\n \n         if is_deepspeed_zero3_enabled() and is_quantized:\n             init_contexts.append(set_quantized_state())\n@@ -4249,38 +4261,32 @@ def from_pretrained(\n             if dtype_orig is not None:\n                 torch.set_default_dtype(dtype_orig)\n \n-            load_contexts = []\n-            # Make sure we load onto targeted device\n-            if tp_device is not None:\n-                load_contexts.append(tp_device)\n-\n-            with ContextManagers(load_contexts):\n-                (\n-                    model,\n-                    missing_keys,\n-                    unexpected_keys,\n-                    mismatched_keys,\n-                    offload_index,\n-                    error_msgs,\n-                ) = cls._load_pretrained_model(\n-                    model,\n-                    state_dict,\n-                    loaded_state_dict_keys,  # XXX: rename?\n-                    resolved_archive_file,\n-                    pretrained_model_name_or_path,\n-                    ignore_mismatched_sizes=ignore_mismatched_sizes,\n-                    sharded_metadata=sharded_metadata,\n-                    _fast_init=_fast_init,\n-                    low_cpu_mem_usage=low_cpu_mem_usage,\n-                    device_map=device_map,\n-                    offload_folder=offload_folder,\n-                    offload_state_dict=offload_state_dict,\n-                    dtype=torch_dtype,\n-                    hf_quantizer=hf_quantizer,\n-                    keep_in_fp32_modules=keep_in_fp32_modules,\n-                    gguf_path=gguf_path,\n-                    weights_only=weights_only,\n-                )\n+            (\n+                model,\n+                missing_keys,\n+                unexpected_keys,\n+                mismatched_keys,\n+                offload_index,\n+                error_msgs,\n+            ) = cls._load_pretrained_model(\n+                model,\n+                state_dict,\n+                loaded_state_dict_keys,  # XXX: rename?\n+                resolved_archive_file,\n+                pretrained_model_name_or_path,\n+                ignore_mismatched_sizes=ignore_mismatched_sizes,\n+                sharded_metadata=sharded_metadata,\n+                _fast_init=_fast_init,\n+                low_cpu_mem_usage=low_cpu_mem_usage,\n+                device_map=device_map,\n+                offload_folder=offload_folder,\n+                offload_state_dict=offload_state_dict,\n+                dtype=torch_dtype,\n+                hf_quantizer=hf_quantizer,\n+                keep_in_fp32_modules=keep_in_fp32_modules,\n+                gguf_path=gguf_path,\n+                weights_only=weights_only,\n+            )\n \n         # make sure token embedding weights are still tied if needed\n         model.tie_weights()"
        },
        {
            "sha": "3df57c5f955cc2e030af05953ca1cb8c1ee4497c",
            "filename": "tests/tp/test_tp.py",
            "status": "modified",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/f48ecd7608a2f80765208de7ce05e7984df396d4/tests%2Ftp%2Ftest_tp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f48ecd7608a2f80765208de7ce05e7984df396d4/tests%2Ftp%2Ftest_tp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftp%2Ftest_tp.py?ref=f48ecd7608a2f80765208de7ce05e7984df396d4",
            "patch": "@@ -13,6 +13,9 @@\n # limitations under the License.\n \n import os\n+import subprocess\n+import tempfile\n+import textwrap\n \n from transformers import is_torch_available\n from transformers.models.llama.configuration_llama import LlamaConfig\n@@ -30,6 +33,22 @@\n \n \n class TestTensorParallel(TestCasePlus):\n+    def torchrun(self, script: str):\n+        \"\"\"Run the `script` using `torchrun` command for multi-processing in a subprocess. Captures errors as necesary.\"\"\"\n+        with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".py\") as tmp:\n+            tmp.write(script)\n+            tmp.flush()\n+            tmp.seek(0)\n+            cmd = (\n+                f\"torchrun --nproc_per_node {torch.cuda.device_count()} --master_port {get_torch_dist_unique_port()} {tmp.name}\"\n+            ).split()\n+\n+            # Note that the subprocess will be waited for here, and raise an error if not successful\n+            try:\n+                _ = subprocess.run(cmd, capture_output=True, env=self.get_env(), text=True, check=True)\n+            except subprocess.CalledProcessError as e:\n+                raise Exception(f\"The following error was captured: {e.stderr}\")\n+\n     @require_torch_multi_gpu\n     def test_tp(self):\n         distributed_args = f\"\"\"--nproc_per_node={torch.cuda.device_count()}\n@@ -43,6 +62,42 @@ def test_tp(self):\n         execute_subprocess_async(cmd, env=self.get_env())\n         # successful return here == success - any errors would have caused an error in the sub-call\n \n+    @require_torch_multi_gpu\n+    def test_loading_memory_consumption(self):\n+        script_to_run = textwrap.dedent(\n+            \"\"\"\n+            import torch\n+            import os\n+            from transformers import AutoModelForCausalLM\n+\n+            model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n+\n+            rank = int(os.environ[\"RANK\"])\n+            world_size = int(os.environ[\"WORLD_SIZE\"])\n+            device = torch.device(f\"cuda:{rank}\")\n+            torch.distributed.init_process_group(\"nccl\", device_id=device)\n+\n+            model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, tp_plan=\"auto\")\n+            torch.distributed.barrier()\n+\n+            # The expected full model memory footprint\n+            expected_model_memory = 16\n+            overhead_factor = 1.2\n+\n+            # Assert we did not use more than the full model expected memory (with some overhead)\n+            if not torch.cuda.max_memory_allocated(device) / 1024**3 < expected_model_memory * overhead_factor:\n+                raise ValueError(\"Loading the model used more than the full model size\")\n+\n+            # Assert we correctly handled the sharding between devices\n+            if not torch.cuda.memory_allocated(device) / 1024**3 < (expected_model_memory / world_size) * overhead_factor:\n+                raise ValueError(\"Each model shard is larger than what is expected.\")\n+\n+            torch.distributed.barrier()\n+            torch.distributed.destroy_process_group()\n+            \"\"\"\n+        )\n+        self.torchrun(script_to_run)\n+\n \n if __name__ == \"__main__\":\n     # The script below is meant to be run under torch.distributed, on a machine with multiple GPUs:"
        }
    ],
    "stats": {
        "total": 147,
        "additions": 104,
        "deletions": 43
    }
}