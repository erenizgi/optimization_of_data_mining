{
    "author": "mgoin",
    "message": "[Docs] Update compressed_tensors.md (#33961)\n\n* Update compressed_tensors.md\r\n\r\nFix some unfinished sections\r\n\r\n* Update docs/source/en/quantization/compressed_tensors.md\r\n\r\nCo-authored-by: Xiao Yuan <yuanx749@gmail.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Xiao Yuan <yuanx749@gmail.com>",
    "sha": "b2f09fb90fc2ea532eade76ca9b552dd4a6a01ef",
    "files": [
        {
            "sha": "77b8986bf8ca4cb5e3362513657b812714fad12a",
            "filename": "docs/source/en/quantization/compressed_tensors.md",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f09fb90fc2ea532eade76ca9b552dd4a6a01ef/docs%2Fsource%2Fen%2Fquantization%2Fcompressed_tensors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f09fb90fc2ea532eade76ca9b552dd4a6a01ef/docs%2Fsource%2Fen%2Fquantization%2Fcompressed_tensors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fcompressed_tensors.md?ref=b2f09fb90fc2ea532eade76ca9b552dd4a6a01ef",
            "patch": "@@ -19,23 +19,20 @@ The [`compressed-tensors`](https://github.com/neuralmagic/compressed-tensors) li\n \n Some of the supported formats include:\n 1. `dense`\n-2. `int-quantized`: INT8 quantized models\n-    - sample [model/config](https://huggingface.co/nm-testing/tinyllama-w8a8-compressed-hf-quantizer)\n-3. `float-quantized`: FP8 quantized models; currently support E4M3\n-    - sample [model/config](https://huggingface.co/nm-testing/Meta-Llama-3-8B-Instruct-fp8-hf_compat/tree/main)\n-4. `pack-quantized`: INT4 or INT8 weight-quantized models, packed into INT32. For INT4, the weights have an INT4 range but are stored as INT8 and   then packed into INT32.\n-    - sample [model/config](nm-testing/tinyllama-w4a16-compressed-hf-quantizer)\n+2. `int-quantized` ([sample](https://huggingface.co/nm-testing/tinyllama-w8a8-compressed-hf-quantizer)): INT8 quantized models\n+3. `float-quantized` ([sample](https://huggingface.co/nm-testing/Meta-Llama-3-8B-Instruct-fp8-hf_compat)): FP8 quantized models; currently support E4M3\n+4. `pack-quantized` ([sample](https://huggingface.co/nm-testing/tinyllama-w4a16-compressed-hf-quantizer)): INT4 or INT8 weight-quantized models, packed into INT32. For INT4, the weights have an INT4 range but are stored as INT8 and then packed into INT32.\n \n Compressed models can be easily created using [llm-compressor](https://github.com/vllm-project/llm-compressor).\n-Alternatively models can be created indepedenty and serialized with a compressed tensors config.\n+Alternatively models can be created independently and serialized with a compressed tensors config.\n \n To find existing models on the Hugging Face Model Hub, search for the [`compressed-tensors` tag](https://huggingface.co/models?other=compressed-tensors).\n \n #### Features:\n  - Weight and activation precisions: FP8, INT4, INT8 (for Q/DQ arbitrary precision is allowed for INT)\n  - Quantization scales and zero-points strategies: [tensor, channel, group, block, token](https://github.com/neuralmagic/compressed-tensors/blob/83b2e7a969d70606421a76b9a3d112646077c8de/src/compressed_tensors/quantization/quant_args.py#L43-L52)\n  - Dynamic per-token activation quantization (or any static strategy)\n- - Sparsity can be \n+ - Sparsity in weights (unstructured or semi-structured like 2:4) can be composed with quantization for extreme compression\n  - Supports quantization of arbitrary modules, not just Linear modules\n  - Targeted support or ignoring of modules by name or class\n "
        }
    ],
    "stats": {
        "total": 13,
        "additions": 5,
        "deletions": 8
    }
}