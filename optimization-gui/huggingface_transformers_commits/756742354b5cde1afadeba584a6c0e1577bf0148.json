{
    "author": "JJJYmmm",
    "message": "[Bugfix] fix qwen3vl expand generation with video (#42089)\n\nfix qwen3vl expand generation with video and add",
    "sha": "756742354b5cde1afadeba584a6c0e1577bf0148",
    "files": [
        {
            "sha": "05c6204d051d95b189f9c3d12de6576ff63ef9ed",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 15,
            "deletions": 7,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/756742354b5cde1afadeba584a6c0e1577bf0148/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/756742354b5cde1afadeba584a6c0e1577bf0148/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=756742354b5cde1afadeba584a6c0e1577bf0148",
            "patch": "@@ -1535,15 +1535,16 @@ def _expand_inputs_for_generation(\n         input_ids: Optional[torch.LongTensor] = None,\n         **model_kwargs,\n     ) -> tuple[torch.LongTensor, dict[str, Any]]:\n-        # Overwritten -- Support for expanding tensors without a batch size dimension\n-        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw, second_per_grid_t\n+        # Overwritten -- Qwen3VL use timestamps and remove second_per_grid_ts\n+        # Support for expanding tensors without a batch size dimension\n+        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw\n         # pixel_values.shape[0] is sum(seqlen_images for samples)\n         # image_grid_thw.shape[0] is sum(num_images for samples)\n \n         if expand_size == 1:\n             return input_ids, model_kwargs\n \n-        visual_keys = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\", \"second_per_grid_ts\"]\n+        visual_keys = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n \n         def _expand_dict_for_generation_visual(dict_to_expand):\n             image_grid_thw = model_kwargs.get(\"image_grid_thw\", None)\n@@ -1552,6 +1553,17 @@ def _expand_dict_for_generation_visual(dict_to_expand):\n                 input_ids, inputs_embeds=model_kwargs.get(\"inputs_embeds\", None)\n             )\n \n+            # video_nums: (batch_size,)\n+            # since video_nums is the number of videos in the input dependent on the input_ids(vision_start),\n+            # but qwen3vl append vision_start to each frame of each video, so we need to recover the real video_nums according to video_grid_thw\n+            if video_grid_thw is not None:\n+                cumulative_frame_counts = torch.cumsum(video_grid_thw[:, 0], dim=0)\n+                cumulative_token_video_counts = torch.cumsum(video_nums, dim=0)\n+                # Find video boundaries in cumulative_frame_counts\n+                video_boundary_indices = torch.searchsorted(cumulative_frame_counts, cumulative_token_video_counts)\n+                # example: video_boundary_indices = [3, 5] means video_nums = [4, 2]\n+                video_nums = torch.diff(torch.cat([-video_boundary_indices.new_ones(1), video_boundary_indices]))\n+\n             def _repeat_interleave_samples(x, lengths, repeat_times):\n                 samples = torch.split(x, lengths)\n                 repeat_args = [repeat_times] + [1] * (x.dim() - 1)\n@@ -1584,10 +1596,6 @@ def _repeat_interleave_samples(x, lengths, repeat_times):\n                     dict_to_expand[key] = _repeat_interleave_samples(\n                         dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n                     )\n-                elif key == \"second_per_grid_ts\":\n-                    dict_to_expand[key] = _repeat_interleave_samples(\n-                        dict_to_expand[key], lengths=list(video_nums), repeat_times=expand_size\n-                    )\n             return dict_to_expand\n \n         def _expand_dict_for_generation(dict_to_expand):"
        },
        {
            "sha": "09944202e5e22f7f21ab73411758fdf25ad29a24",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 96,
            "deletions": 1,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/756742354b5cde1afadeba584a6c0e1577bf0148/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/756742354b5cde1afadeba584a6c0e1577bf0148/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=756742354b5cde1afadeba584a6c0e1577bf0148",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"PyTorch Qwen3-VL model.\"\"\"\n \n from collections.abc import Callable\n-from typing import Optional, Union\n+from typing import Any, Optional, Union\n \n import numpy as np\n import torch\n@@ -1242,6 +1242,101 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n+    def _expand_inputs_for_generation(\n+        self,\n+        expand_size: int = 1,\n+        is_encoder_decoder: bool = False,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        **model_kwargs,\n+    ) -> tuple[torch.LongTensor, dict[str, Any]]:\n+        # Overwritten -- Qwen3VL use timestamps and remove second_per_grid_ts\n+        # Support for expanding tensors without a batch size dimension\n+        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw\n+        # pixel_values.shape[0] is sum(seqlen_images for samples)\n+        # image_grid_thw.shape[0] is sum(num_images for samples)\n+\n+        if expand_size == 1:\n+            return input_ids, model_kwargs\n+\n+        visual_keys = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n+\n+        def _expand_dict_for_generation_visual(dict_to_expand):\n+            image_grid_thw = model_kwargs.get(\"image_grid_thw\", None)\n+            video_grid_thw = model_kwargs.get(\"video_grid_thw\", None)\n+            image_nums, video_nums = self._get_image_nums_and_video_nums(\n+                input_ids, inputs_embeds=model_kwargs.get(\"inputs_embeds\", None)\n+            )\n+\n+            # video_nums: (batch_size,)\n+            # since video_nums is the number of videos in the input dependent on the input_ids(vision_start),\n+            # but qwen3vl append vision_start to each frame of each video, so we need to recover the real video_nums according to video_grid_thw\n+            if video_grid_thw is not None:\n+                cumulative_frame_counts = torch.cumsum(video_grid_thw[:, 0], dim=0)\n+                cumulative_token_video_counts = torch.cumsum(video_nums, dim=0)\n+                # Find video boundaries in cumulative_frame_counts\n+                video_boundary_indices = torch.searchsorted(cumulative_frame_counts, cumulative_token_video_counts)\n+                # example: video_boundary_indices = [3, 5] means video_nums = [4, 2]\n+                video_nums = torch.diff(torch.cat([-video_boundary_indices.new_ones(1), video_boundary_indices]))\n+\n+            def _repeat_interleave_samples(x, lengths, repeat_times):\n+                samples = torch.split(x, lengths)\n+                repeat_args = [repeat_times] + [1] * (x.dim() - 1)\n+                result = torch.cat([sample.repeat(*repeat_args) for sample in samples], dim=0)\n+                return result\n+\n+            for key in dict_to_expand:\n+                if key == \"pixel_values\":\n+                    # split images into samples\n+                    samples = torch.split(image_grid_thw, list(image_nums))\n+                    # compute the sequence length of images for each sample\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"image_grid_thw\":\n+                    # get the num of images for each sample\n+                    lengths = list(image_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"pixel_values_videos\":\n+                    samples = torch.split(video_grid_thw, list(video_nums))\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"video_grid_thw\":\n+                    lengths = list(video_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+            return dict_to_expand\n+\n+        def _expand_dict_for_generation(dict_to_expand):\n+            for key in dict_to_expand:\n+                if (\n+                    key != \"cache_position\"\n+                    and dict_to_expand[key] is not None\n+                    and isinstance(dict_to_expand[key], torch.Tensor)\n+                    and key not in visual_keys\n+                ):\n+                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n+            return dict_to_expand\n+\n+        model_kwargs = _expand_dict_for_generation_visual(model_kwargs)\n+\n+        if input_ids is not None:\n+            input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n+\n+        model_kwargs = _expand_dict_for_generation(model_kwargs)\n+\n+        if is_encoder_decoder:\n+            if model_kwargs.get(\"encoder_outputs\") is None:\n+                raise ValueError(\"If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.\")\n+            model_kwargs[\"encoder_outputs\"] = _expand_dict_for_generation(model_kwargs[\"encoder_outputs\"])\n+\n+        return input_ids, model_kwargs\n+\n \n class Qwen3VLProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {"
        },
        {
            "sha": "6476eb9150ed82c7e7130f21ba54b6c0402f5f7e",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 15,
            "deletions": 7,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/756742354b5cde1afadeba584a6c0e1577bf0148/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/756742354b5cde1afadeba584a6c0e1577bf0148/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=756742354b5cde1afadeba584a6c0e1577bf0148",
            "patch": "@@ -1740,15 +1740,16 @@ def _expand_inputs_for_generation(\n         input_ids: Optional[torch.LongTensor] = None,\n         **model_kwargs,\n     ) -> tuple[torch.LongTensor, dict[str, Any]]:\n-        # Overwritten -- Support for expanding tensors without a batch size dimension\n-        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw, second_per_grid_t\n+        # Overwritten -- Qwen3VLMoe use timestamps and remove second_per_grid_ts\n+        # Support for expanding tensors without a batch size dimension\n+        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw\n         # pixel_values.shape[0] is sum(seqlen_images for samples)\n         # image_grid_thw.shape[0] is sum(num_images for samples)\n \n         if expand_size == 1:\n             return input_ids, model_kwargs\n \n-        visual_keys = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\", \"second_per_grid_ts\"]\n+        visual_keys = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n \n         def _expand_dict_for_generation_visual(dict_to_expand):\n             image_grid_thw = model_kwargs.get(\"image_grid_thw\", None)\n@@ -1757,6 +1758,17 @@ def _expand_dict_for_generation_visual(dict_to_expand):\n                 input_ids, inputs_embeds=model_kwargs.get(\"inputs_embeds\", None)\n             )\n \n+            # video_nums: (batch_size,)\n+            # since video_nums is the number of videos in the input dependent on the input_ids(vision_start),\n+            # but Qwen3VLMoe append vision_start to each frame of each video, so we need to recover the real video_nums according to video_grid_thw\n+            if video_grid_thw is not None:\n+                cumulative_frame_counts = torch.cumsum(video_grid_thw[:, 0], dim=0)\n+                cumulative_token_video_counts = torch.cumsum(video_nums, dim=0)\n+                # Find video boundaries in cumulative_frame_counts\n+                video_boundary_indices = torch.searchsorted(cumulative_frame_counts, cumulative_token_video_counts)\n+                # example: video_boundary_indices = [3, 5] means video_nums = [4, 2]\n+                video_nums = torch.diff(torch.cat([-video_boundary_indices.new_ones(1), video_boundary_indices]))\n+\n             def _repeat_interleave_samples(x, lengths, repeat_times):\n                 samples = torch.split(x, lengths)\n                 repeat_args = [repeat_times] + [1] * (x.dim() - 1)\n@@ -1789,10 +1801,6 @@ def _repeat_interleave_samples(x, lengths, repeat_times):\n                     dict_to_expand[key] = _repeat_interleave_samples(\n                         dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n                     )\n-                elif key == \"second_per_grid_ts\":\n-                    dict_to_expand[key] = _repeat_interleave_samples(\n-                        dict_to_expand[key], lengths=list(video_nums), repeat_times=expand_size\n-                    )\n             return dict_to_expand\n \n         def _expand_dict_for_generation(dict_to_expand):"
        },
        {
            "sha": "fcbb38260b8eb13cc765699a63d637911f56f457",
            "filename": "tests/models/qwen3_vl_moe/test_modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 33,
            "deletions": 1,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/756742354b5cde1afadeba584a6c0e1577bf0148/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/756742354b5cde1afadeba584a6c0e1577bf0148/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py?ref=756742354b5cde1afadeba584a6c0e1577bf0148",
            "patch": "@@ -305,7 +305,6 @@ def test_video_forward(self):\n \n \n @require_torch\n-@unittest.skip(\"The checkpoint is not yet released\")\n class Qwen3VLMoeIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         cleanup(torch_device, gc_collect=True)\n@@ -336,6 +335,18 @@ def setUp(self):\n                 ],\n             }\n         ]\n+        self.message3 = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"video\",\n+                        \"url\": \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/sample_demo_1.mp4\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the video in short.\"},\n+                ],\n+            }\n+        ]\n \n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n@@ -455,6 +466,27 @@ def test_small_model_integration_test_expand(self):\n             EXPECTED_DECODED_TEXT,\n         )\n \n+    @slow\n+    def test_small_model_integration_test_expand_with_video(self):\n+        model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen3-VL-30B-A3B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n+        )\n+        inputs = self.processor.apply_chat_template(\n+            self.message3, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, num_beams=2, num_return_sequences=2)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            \"user\\n<0.3 seconds><1.3 seconds><2.4 seconds><3.5 seconds><4.6 seconds><5.6 seconds><6.7 seconds><7.8 seconds><8.9 seconds><9.7 seconds>Describe the video in short.\\nassistant\\nA baby wearing glasses sits on a bed and flips through a book.\",\n+            \"user\\n<0.3 seconds><1.3 seconds><2.4 seconds><3.5 seconds><4.6 seconds><5.6 seconds><6.7 seconds><7.8 seconds><8.9 seconds><9.7 seconds>Describe the video in short.\\nassistant\\nA baby wearing glasses sits on a bed and flips through the pages of a book.\"\n+        ]  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n     @slow\n     def test_small_model_integration_test_batch_wo_image(self):\n         model = Qwen3VLMoeForConditionalGeneration.from_pretrained("
        }
    ],
    "stats": {
        "total": 175,
        "additions": 159,
        "deletions": 16
    }
}