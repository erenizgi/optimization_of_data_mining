{
    "author": "yao-matrix",
    "message": "enable 2 llama UT cases on xpu (#37126)\n\n* enable tests/models/llama/test_modeling_llama.py::LlamaIntegrationTest::test_model_7b_logits and tests/models/llama/test_modeling_llama.py::LlamaIntegrationTest::test_model_7b_logits_bf16 on xpu\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* switch to use Expectations\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* extract gen bits from architecture and use it\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* add cross refererence\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "12bf24d6ae249add2e94559a63fc85785ef96326",
    "files": [
        {
            "sha": "fbcaddc4c20cf191074931aaec731a955d6a8aca",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/12bf24d6ae249add2e94559a63fc85785ef96326/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12bf24d6ae249add2e94559a63fc85785ef96326/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=12bf24d6ae249add2e94559a63fc85785ef96326",
            "patch": "@@ -202,9 +202,11 @@\n \n     IS_ROCM_SYSTEM = torch.version.hip is not None\n     IS_CUDA_SYSTEM = torch.version.cuda is not None\n+    IS_XPU_SYSTEM = torch.version.xpu is not None\n else:\n     IS_ROCM_SYSTEM = False\n     IS_CUDA_SYSTEM = False\n+    IS_XPU_SYSTEM = False\n \n logger = transformers_logging.get_logger(__name__)\n \n@@ -3097,6 +3099,14 @@ def get_device_properties() -> DeviceProperties:\n             return (\"rocm\", major)\n         else:\n             return (\"cuda\", major)\n+    elif IS_XPU_SYSTEM:\n+        import torch\n+\n+        # To get more info of the architecture meaning and bit allocation, refer to https://github.com/intel/llvm/blob/sycl/sycl/include/sycl/ext/oneapi/experimental/device_architecture.def\n+        arch = torch.xpu.get_device_capability()[\"architecture\"]\n+        gen_mask = 0x000000FF00000000\n+        gen = (arch & gen_mask) >> 32\n+        return (\"xpu\", gen)\n     else:\n         return (torch_device, None)\n "
        },
        {
            "sha": "202a8687f8b4851241b05539b819f2776f253ddf",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 35,
            "deletions": 33,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/12bf24d6ae249add2e94559a63fc85785ef96326/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12bf24d6ae249add2e94559a63fc85785ef96326/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=12bf24d6ae249add2e94559a63fc85785ef96326",
            "patch": "@@ -22,6 +22,7 @@\n from transformers import AutoTokenizer, LlamaConfig, StaticCache, is_torch_available, set_seed\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n     require_read_token,\n     require_torch,\n@@ -429,16 +430,6 @@ def _reinitialize_config(base_config, new_kwargs):\n \n @require_torch_accelerator\n class LlamaIntegrationTest(unittest.TestCase):\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n-\n     def tearDown(self):\n         # TODO (joao): automatic compilation, i.e. compilation when `cache_implementation=\"static\"` is used, leaves\n         # some memory allocated in the cache, which means some object is not being released properly. This causes some\n@@ -490,30 +481,35 @@ def test_model_7b_logits_bf16(self):\n         # Expected mean on dim = -1\n \n         # fmt: off\n-        EXPECTED_MEAN = {\n-            7: torch.tensor([[-6.5061, -4.1147, -4.9669, -3.2038, 0.8069, -2.9694, 1.2864, -3.3786]]),\n-            8: torch.tensor([[-6.5208, -4.1218, -4.9377, -3.2536,  0.8127, -2.9811,  1.2918, -3.3848]])\n-        }\n-\n+        expected_means = Expectations(\n+            {\n+            (\"xpu\", 3): torch.tensor([[-6.5208, -4.1218, -4.9377, -3.2536,  0.8127, -2.9811,  1.2918, -3.3848]]),\n+            (\"cuda\", 7): torch.tensor([[-6.5061, -4.1147, -4.9669, -3.2038, 0.8069, -2.9694, 1.2864, -3.3786]]),\n+            (\"cuda\", 8): torch.tensor([[-6.5208, -4.1218, -4.9377, -3.2536,  0.8127, -2.9811,  1.2918, -3.3848]])\n+         })\n+\n+        expected_mean = expected_means.get_expectation()\n         self.assertTrue(\n             torch.allclose(\n-                EXPECTED_MEAN[self.cuda_compute_capability_major_version].to(torch_device),\n+                expected_mean.to(torch_device),\n                 out.logits.float().mean(-1),\n                 atol=1e-2,\n                 rtol=1e-2\n             )\n         )\n \n         # slicing logits[0, 0, 0:15]\n-        EXPECTED_SLICE = {\n-            7: torch.tensor([[-12.5000, -7.0625, -0.6289, -7.8750, -6.9688, -7.8125, -6.4688, -7.4375, -7.6875, -6.9375, -6.0312, -7.0000, -1.8594, 1.8438, -8.5000]]),\n-            8: torch.tensor([[-12.5625,  -7.1250,  -0.6289,  -7.8750,  -6.9688,  -7.8125,  -6.5000, -7.4375,  -7.6562,  -6.9688,  -6.0312,  -7.0312,  -1.8203,   1.8750, -8.5000]])\n-        }\n+        expected_slices = Expectations(\n+            {\n+            (\"xpu\", 3): torch.tensor([[-12.5625,  -7.1250,  -0.6289,  -7.8750,  -6.9688,  -7.8125,  -6.5000, -7.4375,  -7.6562,  -6.9688,  -6.0312,  -7.0312,  -1.8203,   1.8750, -8.5000]]),\n+            (\"cuda\", 7): torch.tensor([[-12.5000, -7.0625, -0.6289, -7.8750, -6.9688, -7.8125, -6.4688, -7.4375, -7.6875, -6.9375, -6.0312, -7.0000, -1.8594, 1.8438, -8.5000]]),\n+            (\"cuda\", 8): torch.tensor([[-12.5625,  -7.1250,  -0.6289,  -7.8750,  -6.9688,  -7.8125,  -6.5000, -7.4375,  -7.6562,  -6.9688,  -6.0312,  -7.0312,  -1.8203,   1.8750, -8.5000]])\n+        })\n         # fmt: on\n-\n+        expected_slice = expected_slices.get_expectation()\n         self.assertTrue(\n             torch.allclose(\n-                EXPECTED_SLICE[self.cuda_compute_capability_major_version].to(torch_device),\n+                expected_slice.to(torch_device),\n                 out.logits[0, 0, :15].float(),\n                 atol=1e-2,\n                 rtol=1e-2,\n@@ -534,30 +530,36 @@ def test_model_7b_logits(self):\n \n         # fmt: off\n         # Expected mean on dim = -1\n-        EXPECTED_MEAN = {\n-            7: torch.tensor([[-6.6420, -4.1227, -4.9809, -3.2041, 0.8261, -3.0052, 1.2957, -3.3648]]),\n-            8: torch.tensor([[-6.6544, -4.1259, -4.9840, -3.2456,  0.8261, -3.0124,  1.2971, -3.3641]])\n-        }\n-\n+        expected_means = Expectations(\n+          {\n+            (\"xpu\", 3): torch.tensor([[-6.6544, -4.1259, -4.9840, -3.2456,  0.8261, -3.0124,  1.2971, -3.3641]]),\n+            (\"cuda\", 7): torch.tensor([[-6.6420, -4.1227, -4.9809, -3.2041, 0.8261, -3.0052, 1.2957, -3.3648]]),\n+            (\"cuda\", 8): torch.tensor([[-6.6544, -4.1259, -4.9840, -3.2456,  0.8261, -3.0124,  1.2971, -3.3641]]),\n+        })\n+\n+        expected_mean = expected_means.get_expectation()\n         self.assertTrue(\n             torch.allclose(\n-                EXPECTED_MEAN[self.cuda_compute_capability_major_version].to(torch_device),\n+                expected_mean.to(torch_device),\n                 out.logits.float().mean(-1),\n                 atol=1e-2,\n                 rtol=1e-2\n             )\n         )\n \n         # slicing logits[0, 0, 0:15]\n-        EXPECTED_SLICE = {\n-            7: torch.tensor([-12.8125, -7.3359, -0.4846, -8.0234, -7.2383, -7.9922, -6.4805, -7.7344, -7.8125, -7.0078, -6.1797, -7.1094, -1.8633, 1.9736, -8.6016]),\n-            8: torch.tensor([-12.8281,  -7.4609,  -0.4668,  -8.0703,  -7.2539,  -8.0078,  -6.4961, -7.7734,  -7.8516,  -7.0352,  -6.2188,  -7.1367,  -1.8564,   1.9922, -8.6328])\n-        }\n+        expected_slices = Expectations(\n+            {\n+              (\"xpu\", 3): torch.tensor([-12.8281,  -7.4609,  -0.4668,  -8.0703,  -7.2539,  -8.0078,  -6.4961, -7.7734,  -7.8516,  -7.0352,  -6.2188,  -7.1367,  -1.8564,   1.9922, -8.6328]),\n+              (\"cuda\", 7): torch.tensor([-12.8125, -7.3359, -0.4846, -8.0234, -7.2383, -7.9922, -6.4805, -7.7344, -7.8125, -7.0078, -6.1797, -7.1094, -1.8633, 1.9736, -8.6016]),\n+              (\"cuda\", 8): torch.tensor([-12.8281,  -7.4609,  -0.4668,  -8.0703,  -7.2539,  -8.0078,  -6.4961, -7.7734,  -7.8516,  -7.0352,  -6.2188,  -7.1367,  -1.8564,   1.9922, -8.6328])\n+        })\n         # fmt: on\n \n+        expected_slice = expected_slices.get_expectation()\n         self.assertTrue(\n             torch.allclose(\n-                EXPECTED_SLICE[self.cuda_compute_capability_major_version].to(torch_device),\n+                expected_slice.to(torch_device),\n                 out.logits[0, 0, :15].float(),\n                 atol=1e-2,\n                 rtol=1e-2,"
        },
        {
            "sha": "230aabc7acc7b4406e3611a7b354fa13ead5e2cc",
            "filename": "tests/utils/test_expectations.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/12bf24d6ae249add2e94559a63fc85785ef96326/tests%2Futils%2Ftest_expectations.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12bf24d6ae249add2e94559a63fc85785ef96326/tests%2Futils%2Ftest_expectations.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_expectations.py?ref=12bf24d6ae249add2e94559a63fc85785ef96326",
            "patch": "@@ -13,14 +13,16 @@ def test_expectations(self):\n                 (\"rocm\", 8): 4,\n                 (\"rocm\", None): 5,\n                 (\"cpu\", None): 6,\n+                (\"xpu\", 3): 7,\n             }\n         )\n \n         def check(value, key):\n             assert expectations.find_expectation(key) == value\n \n-        # xpu has no matches so should find default expectation\n-        check(1, (\"xpu\", None))\n+        # npu has no matches so should find default expectation\n+        check(1, (\"npu\", None))\n+        check(7, (\"xpu\", 3))\n         check(2, (\"cuda\", 8))\n         check(3, (\"cuda\", 7))\n         check(4, (\"rocm\", 9))"
        }
    ],
    "stats": {
        "total": 84,
        "additions": 49,
        "deletions": 35
    }
}