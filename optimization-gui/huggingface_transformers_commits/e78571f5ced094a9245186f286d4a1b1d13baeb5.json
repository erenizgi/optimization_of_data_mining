{
    "author": "manueldeprada",
    "message": "`decoding_method` argument in generate (#40085)\n\n* factor out expand inputs\n\n* callable arg\n\n* improve docs, add test\n\n* Update docs/source/en/generation_strategies.md\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "e78571f5ced094a9245186f286d4a1b1d13baeb5",
    "files": [
        {
            "sha": "ba59c1228609b35e4e9e5bfe0c2bbfdb9d42c8e2",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e78571f5ced094a9245186f286d4a1b1d13baeb5/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e78571f5ced094a9245186f286d4a1b1d13baeb5/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=e78571f5ced094a9245186f286d4a1b1d13baeb5",
            "patch": "@@ -504,6 +504,31 @@ Recommended practices:\n - Add self-contained examples to enable quick experimentation.\n - Describe soft-requirements such as if the method only works well with a certain family of models.\n \n+### Reusing `generate`’s input preparation\n+\n+If you're adding a new decoding loop, you might want to preserve the input preparation present in `generate` (batch expansion, attention masks, logits processors, stopping criteria, etc.). You can also pass a **callable** to `custom_generate` to reuse [`~GenerationMixin.generate`]’s full preparation pipeline while overriding only the decoding loop.\n+\n+```py\n+def custom_loop(model, input_ids, attention_mask, logits_processor, stopping_criteria, generation_config, **model_kwargs):\n+    next_tokens = input_ids\n+    while input_ids.shape[1] < stopping_criteria[0].max_length:\n+        logits = model(next_tokens, attention_mask=attention_mask, **model_kwargs).logits\n+        next_token_logits = logits_processor(input_ids, logits[:, -1, :])\n+        next_tokens = torch.argmax(next_token_logits, dim=-1)[:, None]\n+        input_ids = torch.cat((input_ids, next_tokens), dim=-1)\n+        attention_mask = torch.cat((attention_mask, torch.ones_like(next_tokens)), dim=-1)\n+    return input_ids\n+\n+output = model.generate(\n+    **inputs,\n+    custom_generate=custom_loop,\n+    max_new_tokens=10,\n+)\n+```\n+\n+> [!TIP]\n+> If you publish a `custom_generate` repository, your `generate` implementation can itself define a callable and pass it to `model.generate()`. This lets you customize the decoding loop while still benefiting from Transformers’ built-in input preparation logic.\n+\n ### Finding custom generation methods\n \n You can find all custom generation methods by [searching for their custom tag.](https://huggingface.co/models?other=custom_generate), `custom_generate`. In addition to the tag, we curate two collections of `custom_generate` methods:"
        },
        {
            "sha": "0ae0f333c4a6a16c7a43c8506083648b66f7f2fa",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 34,
            "deletions": 41,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/e78571f5ced094a9245186f286d4a1b1d13baeb5/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e78571f5ced094a9245186f286d4a1b1d13baeb5/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=e78571f5ced094a9245186f286d4a1b1d13baeb5",
            "patch": "@@ -2165,7 +2165,7 @@ def generate(\n         negative_prompt_ids: Optional[torch.Tensor] = None,\n         negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n         use_model_defaults: Optional[bool] = None,\n-        custom_generate: Optional[str] = None,\n+        custom_generate: Optional[Union[str, Callable]] = None,\n         **kwargs,\n     ) -> Union[GenerateOutput, torch.LongTensor]:\n         r\"\"\"\n@@ -2235,11 +2235,15 @@ def generate(\n                 generation configuration (`model.generation_config`), as opposed to the global defaults\n                 (`GenerationConfig()`). If unset, models saved starting from `v4.50` will consider this flag to be\n                 `True`.\n-            custom_generate (`str`, *optional*):\n-                A string containing the name of a huggingface.co repository. If provided, the custom `generate`\n-                function defined in that reposity's `custom_generate/generate.py` file will be executed instead of the\n-                standard `generate` method. Note that the logic is for generation is entirely defined in that\n-                repository, and the return type may be different from the standard `generate` method.\n+            custom_generate (`str` or `Callable`, *optional*):\n+                One of the following:\n+                - `str` (Hugging Face Hub repository name): runs the custom `generate` function defined at\n+                  `custom_generate/generate.py` in that repository instead of the standard `generate` method. The\n+                  repository fully replaces the generation logic, and the return type may differ.\n+                - `str` (local repository path): same as above but from a local path, `trust_remote_code` not required.\n+                - `Callable`: `generate` will perform the usual input preparation steps, then call the provided callable to\n+                  run the decoding loop.\n+                For more information, see [the docs](../../generation_strategies#custom-generation-methods).\n             kwargs (`dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n@@ -2263,7 +2267,7 @@ def generate(\n         \"\"\"\n         # 0. If requested, load an arbitrary generation recipe from the Hub and run it instead\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n-        if custom_generate is not None:\n+        if custom_generate is not None and isinstance(custom_generate, str):\n             # Get all `generate` arguments in a single variable. Custom functions are responsible for handling them:\n             # they receive the same inputs as `generate`, with `model` instead of `self` and excluding the arguments to\n             # trigger the custom generation. They can access to methods from `GenerationMixin` through `model`.\n@@ -2360,6 +2364,14 @@ def generate(\n         else:\n             input_ids = inputs_tensor if model_input_name == \"input_ids\" else model_kwargs.pop(\"input_ids\")\n \n+        # Expand inputs depending on the generation mode\n+        input_ids, model_kwargs = self._expand_inputs_for_generation(\n+            input_ids=input_ids,\n+            expand_size=max(generation_config.num_beams, generation_config.num_return_sequences),\n+            is_encoder_decoder=self.config.is_encoder_decoder,\n+            **model_kwargs,\n+        )\n+\n         if generation_config.token_healing:\n             input_ids = self.heal_tokens(input_ids, tokenizer)\n \n@@ -2441,7 +2453,18 @@ def generate(\n         model_kwargs[\"use_cache\"] = generation_config.use_cache\n \n         # 10. go into different generation modes\n-        if generation_mode == GenerationMode.ASSISTED_GENERATION:\n+        if isinstance(custom_generate, Callable):\n+            result = custom_generate(\n+                self,\n+                input_ids,\n+                logits_processor=prepared_logits_processor,\n+                stopping_criteria=prepared_stopping_criteria,\n+                generation_config=generation_config,\n+                synced_gpus=synced_gpus,\n+                streamer=streamer,\n+                **model_kwargs,\n+            )\n+        elif generation_mode == GenerationMode.ASSISTED_GENERATION:\n             if generation_config.num_return_sequences > 1:\n                 raise ValueError(\n                     \"num_return_sequences has to be 1 when doing assisted generate, \"\n@@ -2530,15 +2553,7 @@ def generate(\n             )\n \n         elif generation_mode in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n-            # 11. expand input_ids with `num_return_sequences` additional sequences per batch\n-            input_ids, model_kwargs = self._expand_inputs_for_generation(\n-                input_ids=input_ids,\n-                expand_size=generation_config.num_return_sequences,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-                **model_kwargs,\n-            )\n-\n-            # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n+            # 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n             result = self._sample(\n                 input_ids,\n                 logits_processor=prepared_logits_processor,\n@@ -2550,14 +2565,7 @@ def generate(\n             )\n \n         elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n-            # 11. interleave input_ids with `num_beams` additional sequences per batch\n-            input_ids, model_kwargs = self._expand_inputs_for_generation(\n-                input_ids=input_ids,\n-                expand_size=generation_config.num_beams,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-                **model_kwargs,\n-            )\n-            # 12. run beam sample\n+            # 11. run beam sample\n             result = self._beam_search(\n                 input_ids,\n                 logits_processor=prepared_logits_processor,\n@@ -2583,14 +2591,6 @@ def generate(\n                 num_beam_groups=generation_config.num_beam_groups,\n                 max_length=generation_config.max_length,\n             )\n-            # 12. interleave input_ids with `num_beams` additional sequences per batch\n-            input_ids, model_kwargs = self._expand_inputs_for_generation(\n-                input_ids=input_ids,\n-                expand_size=generation_config.num_beams,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-                **model_kwargs,\n-            )\n-            # 13. run beam search\n             result = self._group_beam_search(\n                 input_ids,\n                 beam_scorer,\n@@ -2657,14 +2657,7 @@ def typeerror():\n                 num_beam_hyps_to_keep=generation_config.num_return_sequences,\n                 max_length=generation_config.max_length,\n             )\n-            # 12. interleave input_ids with `num_beams` additional sequences per batch\n-            input_ids, model_kwargs = self._expand_inputs_for_generation(\n-                input_ids=input_ids,\n-                expand_size=generation_config.num_beams,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-                **model_kwargs,\n-            )\n-            # 13. run beam search\n+            # 12. run beam search\n             result = self._constrained_beam_search(\n                 input_ids,\n                 constrained_beam_scorer=constrained_beam_scorer,"
        },
        {
            "sha": "c9d20e692e921ca9795afb2385db45c005ab938c",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/e78571f5ced094a9245186f286d4a1b1d13baeb5/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e78571f5ced094a9245186f286d4a1b1d13baeb5/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=e78571f5ced094a9245186f286d4a1b1d13baeb5",
            "patch": "@@ -5044,6 +5044,26 @@ def test_custom_generate_local_directory(self):\n             )\n             assert value == \"success\"\n \n+    def test_custom_generate_callable(self):\n+        \"\"\"Tests that passing a callable to `custom_generate` executes the callable decoding loop\"\"\"\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"hf-internal-testing/tiny-random-MistralForCausalLM\", device_map=\"auto\"\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n+        model_inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\").to(model.device)\n+\n+        def custom_loop(model, input_ids, logits_processor, stopping_criteria, generation_config, **model_kwargs):\n+            # Check that generate() correctly prepares the stopping criteria\n+            assert stopping_criteria[0].max_length == input_ids.shape[1] + 3\n+            return \"callable_success\"\n+\n+        value = model.generate(\n+            **model_inputs,\n+            max_new_tokens=3,\n+            custom_generate=custom_loop,\n+        )\n+        self.assertEqual(value, \"callable_success\")\n+\n     @pytest.mark.generate\n     def test_generate_custom_cache_position(self):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 120,
        "additions": 79,
        "deletions": 41
    }
}