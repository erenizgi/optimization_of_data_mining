{
    "author": "zucchini-nlp",
    "message": "Update composition flag usage (#36263)\n\n* update composition flag usage\n\n* remove print\n\n* fix tests\n\n* actually fix\n\n* oh c'mon\n\n* now should be fixed right?\n\n* fix copies",
    "sha": "6f4058aee3dd2b3b4cba290a008692e399efbd72",
    "files": [
        {
            "sha": "651c4e7dcaf9da026af4dd144d4903ab77582fd6",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -1195,9 +1195,7 @@ def __init__(\n         self.max_cache_len = config.max_position_embeddings if max_cache_len is None else max_cache_len\n \n         # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n-        self.head_dim = (\n-            config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n-        )\n+        self.head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n \n         self._dtype = dtype\n         self.num_key_value_heads = ("
        },
        {
            "sha": "8fa8dc46c3edcc9c389638577067368aa89f0f3c",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -61,9 +61,10 @@ class PretrainedConfig(PushToHubMixin):\n \n     - **model_type** (`str`) -- An identifier for the model type, serialized into the JSON file, and used to recreate\n       the correct object in [`~transformers.AutoConfig`].\n-    - **is_composition** (`bool`) -- Whether the config class is composed of multiple sub-configs. In this case the\n-      config has to be initialized from two or more configs of type [`~transformers.PretrainedConfig`] like:\n-      [`~transformers.EncoderDecoderConfig`] or [`~RagConfig`].\n+    - **has_no_defaults_at_init** (`bool`) -- Whether the config class can be initialized without providing input arguments.\n+      Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,\n+      (but not necessarily) such as [`~transformers.EncoderDecoderConfig`] or [`~RagConfig`]. They have to be initialized from\n+      two or more configs of type [`~transformers.PretrainedConfig`].\n     - **keys_to_ignore_at_inference** (`List[str]`) -- A list of keys to ignore by default when looking at dictionary\n       outputs of the model during inference.\n     - **attribute_map** (`Dict[str, str]`) -- A dict that maps model specific attribute names to the standardized\n@@ -193,7 +194,7 @@ class PretrainedConfig(PushToHubMixin):\n     model_type: str = \"\"\n     base_config_key: str = \"\"\n     sub_configs: dict[str, \"PretrainedConfig\"] = {}\n-    is_composition: bool = False\n+    has_no_defaults_at_init: bool = False\n     attribute_map: dict[str, str] = {}\n     base_model_tp_plan: Optional[dict[str, Any]] = None\n     base_model_pp_plan: Optional[dict[str, tuple[list[str]]]] = None\n@@ -813,8 +814,8 @@ def to_diff_dict(self) -> dict[str, Any]:\n         # Get the default config dict (from a fresh PreTrainedConfig instance)\n         default_config_dict = PretrainedConfig().to_dict()\n \n-        # Get class-specific config dict if not part of a composition\n-        class_config_dict = self.__class__().to_dict() if not self.is_composition else {}\n+        # get class specific config dict\n+        class_config_dict = self.__class__().to_dict() if not self.has_no_defaults_at_init else {}\n \n         serializable_config_dict = {}\n "
        },
        {
            "sha": "ccb961edea9d6e297b14531188320d72bbf4ef66",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -121,7 +121,7 @@ def _compute_default_rope_parameters(\n     elif config is not None:\n         base = config.rope_theta\n         partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n-        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         dim = int(head_dim * partial_rotary_factor)\n \n     attention_factor = 1.0  # Unused in this type of RoPE"
        },
        {
            "sha": "00ff22c8b894be11110b5a5ec9f20cece58e3788",
            "filename": "src/transformers/models/bark/generation_configuration_bark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -240,7 +240,6 @@ def validate(self, **kwargs):\n \n class BarkGenerationConfig(GenerationConfig):\n     model_type = \"bark\"\n-    is_composition = True\n \n     # TODO (joao): nested from_dict\n "
        },
        {
            "sha": "a5eff83e55824786d70c821ecfa210e07c27da2e",
            "filename": "src/transformers/models/encoder_decoder/configuration_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -72,7 +72,7 @@ class EncoderDecoderConfig(PretrainedConfig):\n \n     model_type = \"encoder-decoder\"\n     sub_configs = {\"encoder\": AutoConfig, \"decoder\": AutoConfig}\n-    is_composition = True\n+    has_no_defaults_at_init = True\n \n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)"
        },
        {
            "sha": "f476591b2eb6ff50b1e47cb7e22ca8b7d804466a",
            "filename": "src/transformers/models/llava/configuration_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -76,7 +76,6 @@ class LlavaConfig(PretrainedConfig):\n \n     model_type = \"llava\"\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n-    is_composition = True\n \n     def __init__(\n         self,"
        },
        {
            "sha": "8a5055be658a348e276f2ff1ef5ebedd69ae0125",
            "filename": "src/transformers/models/mistral/configuration_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -143,7 +143,7 @@ def __init__(\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n         self.sliding_window = sliding_window\n-        self.head_dim = head_dim or hidden_size // num_attention_heads\n+        self.head_dim = head_dim\n \n         # for backward compatibility\n         if num_key_value_heads is None:"
        },
        {
            "sha": "5639c3bbb6de6df2e69742a951e7d09fbc206678",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -139,7 +139,7 @@ def __init__(self, config: MistralConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout"
        },
        {
            "sha": "4f36181cd7228e516c0ea6d2bb38974a7e910bce",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -42,6 +42,7 @@ def __init__(self, config):\n class MistralAttention(LlamaAttention):\n     def __init__(self, config: MistralConfig, layer_idx: int):\n         super().__init__()\n+        self.head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)"
        },
        {
            "sha": "4f11077c194310521885078b90c0d7065270f0ef",
            "filename": "src/transformers/models/mixtral/configuration_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -172,7 +172,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.rope_theta = rope_theta\n         self.attention_dropout = attention_dropout\n-        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        self.head_dim = head_dim\n \n         self.num_experts_per_tok = num_experts_per_tok\n         self.num_local_experts = num_local_experts"
        },
        {
            "sha": "51604dec3f92d978128eb095af18c638126f5d1c",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -251,7 +251,7 @@ def __init__(self, config: MixtralConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout"
        },
        {
            "sha": "e5079eb1edb7758b40addec8082e93f677a7a6ff",
            "filename": "src/transformers/models/musicgen/configuration_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -195,7 +195,7 @@ class MusicgenConfig(PretrainedConfig):\n         \"audio_encoder\": AutoConfig,\n         \"decoder\": MusicgenDecoderConfig,\n     }\n-    is_composition = True\n+    has_no_defaults_at_init = True\n \n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)"
        },
        {
            "sha": "e35c7bd3c8a8c28a911967323b1711841b852402",
            "filename": "src/transformers/models/musicgen_melody/configuration_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -201,7 +201,7 @@ class MusicgenMelodyConfig(PretrainedConfig):\n         \"audio_encoder\": AutoConfig,\n         \"decoder\": MusicgenMelodyDecoderConfig,\n     }\n-    is_composition = True\n+    has_no_defaults_at_init = True\n \n     def __init__(\n         self,"
        },
        {
            "sha": "da5af9fb344bbdbb4c2bb8c3fae56538854f3419",
            "filename": "src/transformers/models/pixtral/convert_pixtral_weights_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -158,7 +158,6 @@ def convert_mistral_model(input_dir, output_dir):\n             hidden_act=\"silu\",\n             sliding_window=None,\n             tie_word_embeddings=False,\n-            is_composition=True,\n             rms_norm_eps=1e-5,\n         )\n     else:"
        },
        {
            "sha": "fd0c9bb9cce447f041ed3ccc9398ef93376e42e8",
            "filename": "src/transformers/models/rag/configuration_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -79,7 +79,7 @@\n @add_start_docstrings(RAG_CONFIG_DOC)\n class RagConfig(PretrainedConfig):\n     model_type = \"rag\"\n-    is_composition = True\n+    has_no_defaults_at_init = True\n \n     def __init__(\n         self,"
        },
        {
            "sha": "14dfab7eaa64e28f981536157c4af7d077acc835",
            "filename": "src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconfiguration_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconfiguration_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconfiguration_speech_encoder_decoder.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -72,7 +72,7 @@ class SpeechEncoderDecoderConfig(PretrainedConfig):\n \n     model_type = \"speech-encoder-decoder\"\n     sub_configs = {\"encoder\": AutoConfig, \"decoder\": AutoConfig}\n-    is_composition = True\n+    has_no_defaults_at_init = True\n \n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)"
        },
        {
            "sha": "089cb00dceb475f3def63a81711fa3de0ec3e7ac",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -158,7 +158,7 @@ def __init__(self, config: Starcoder2Config, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout"
        },
        {
            "sha": "09b324af24785c4563c030c01ef94d247e3e8a47",
            "filename": "src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fconfiguration_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fconfiguration_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fconfiguration_vision_encoder_decoder.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -79,7 +79,7 @@ class VisionEncoderDecoderConfig(PretrainedConfig):\n \n     model_type = \"vision-encoder-decoder\"\n     sub_configs = {\"encoder\": AutoConfig, \"decoder\": AutoConfig}\n-    is_composition = True\n+    has_no_defaults_at_init = True\n \n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)"
        },
        {
            "sha": "3f544e9eaf05061d1a362059edc5fea654952c5f",
            "filename": "src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fconfiguration_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fconfiguration_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fconfiguration_vision_text_dual_encoder.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -76,7 +76,7 @@ class VisionTextDualEncoderConfig(PretrainedConfig):\n \n     model_type = \"vision-text-dual-encoder\"\n     sub_configs = {\"vision_config\": AutoConfig, \"text_config\": AutoConfig}\n-    is_composition = True\n+    has_no_defaults_at_init = True\n \n     def __init__(self, projection_dim=512, logit_scale_init_value=2.6592, **kwargs):\n         super().__init__(**kwargs)"
        },
        {
            "sha": "68539b71e16a20ab413356968bcf0a8ec6566c7d",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -1755,9 +1755,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n \n             text_config = model.config.get_text_config()\n             head_dim = (\n-                text_config.head_dim\n-                if hasattr(text_config, \"head_dim\")\n-                else text_config.hidden_size // text_config.num_attention_heads\n+                getattr(text_config, \"head_dim\", None) or text_config.hidden_size // text_config.num_attention_heads\n             )\n             num_key_value_heads = (\n                 text_config.num_attention_heads\n@@ -2008,9 +2006,8 @@ def test_generate_with_static_cache(self):\n                 max_cache_len = seq_length + max_new_tokens - 1  # cache len = gen len - 1, the last token has no cache\n                 text_config = config.text_config if hasattr(config, \"text_config\") else config\n                 head_dim = (\n-                    text_config.head_dim\n-                    if hasattr(text_config, \"head_dim\")\n-                    else text_config.hidden_size // text_config.num_attention_heads\n+                    getattr(text_config, \"head_dim\", None)\n+                    or text_config.hidden_size // text_config.num_attention_heads\n                 )\n                 num_key_value_heads = (\n                     text_config.num_attention_heads"
        },
        {
            "sha": "f329d1b2110911933238a226acc628d5732b5a37",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -184,13 +184,6 @@ def setUp(self):\n         )\n \n     def test_config(self):\n-        # overwritten from `tests/test_configuration_common.py::ConfigTester` after #36077\n-        # TODO: avoid overwritten once there is a better fix for #36077\n-        def check_config_can_be_init_without_params():\n-            config = self.config_tester.config_class()\n-            self.config_tester.parent.assertIsNotNone(config)\n-\n-        self.config_tester.check_config_can_be_init_without_params = check_config_can_be_init_without_params\n         self.config_tester.run_common_tests()\n \n     # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs"
        },
        {
            "sha": "4d4ce3a3f165d63ff474966b8a964f408f734d1b",
            "filename": "tests/test_configuration_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f4058aee3dd2b3b4cba290a008692e399efbd72/tests%2Ftest_configuration_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f4058aee3dd2b3b4cba290a008692e399efbd72/tests%2Ftest_configuration_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_configuration_common.py?ref=6f4058aee3dd2b3b4cba290a008692e399efbd72",
            "patch": "@@ -163,7 +163,7 @@ def create_and_test_config_with_num_labels(self):\n         self.parent.assertEqual(len(config.label2id), 3)\n \n     def check_config_can_be_init_without_params(self):\n-        if self.config_class.is_composition:\n+        if self.config_class.has_no_defaults_at_init:\n             with self.parent.assertRaises(ValueError):\n                 config = self.config_class()\n         else:"
        }
    ],
    "stats": {
        "total": 65,
        "additions": 26,
        "deletions": 39
    }
}